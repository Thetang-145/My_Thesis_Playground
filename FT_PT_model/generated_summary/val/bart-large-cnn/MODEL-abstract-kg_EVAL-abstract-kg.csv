,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,Federated learning ( FL ) USED-FOR machine learning models. decentralized data sources USED-FOR machine learning models. local differential privacy constraints USED-FOR FL. communication efficiency CONJUNCTION highdimensional compatibility. highdimensional compatibility CONJUNCTION communication efficiency. sqSGD ( selective quantized stochastic gradient descent ) HYPONYM-OF gradient - based learning algorithm. privacy - preserving quantization scheme USED-FOR algorithm. training performance CONJUNCTION communication costs. communication costs CONJUNCTION training performance. fixed privacy budget USED-FOR gradient subsampling strategy. communication costs EVALUATE-FOR gradient subsampling strategy. training performance EVALUATE-FOR gradient subsampling strategy. randomized rotation USED-FOR quantization error. quantization CONJUNCTION perturbation. perturbation CONJUNCTION quantization. perturbation USED-FOR FL algorithm. quantization USED-FOR FL algorithm. privacy and communication constraints FEATURE-OF FL algorithm. benchmark datasets EVALUATE-FOR framework. LeNet CONJUNCTION ResNet. ResNet CONJUNCTION LeNet. sqSGD USED-FOR large models. local privacy constraints FEATURE-OF large models. ResNet HYPONYM-OF large models. LeNet HYPONYM-OF large models. sqSGD COMPARE baseline algorithms. baseline algorithms COMPARE sqSGD. fixed privacy and communication level FEATURE-OF sqSGD. OtherScientificTerm is sensitive data disclosures. Method is privacy - preserving FL algorithms. Generic is base algorithm. ,This paper proposes a new gradient-based learning algorithm called sqSGD (selective quantized stochastic gradient descent) for federated learning with local differential privacy constraints. The authors propose a privacy-preserving quantization scheme to improve the training performance and communication costs. The proposed method is based on a gradient subsampling strategy that uses randomized rotation to reduce the quantization error. Theoretical analysis shows that the proposed method achieves better performance than existing FL algorithms with fixed privacy budget and communication level.,"This paper proposes sqSGD (selective quantized stochastic gradient descent), a privacy-preserving gradient-based learning algorithm for federated learning (FL) with local differential privacy constraints. The authors propose a new quantization scheme for the gradient subsampling strategy, which is based on a randomized rotation of the quantization error. The proposed method is evaluated on a variety of benchmark datasets, and it is shown to be competitive with existing FL algorithms."
9,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,Self - attention networks ( SANs ) USED-FOR natural language processing tasks. it USED-FOR language representation. language knowledge USED-FOR it. prior knowledge USED-FOR language representation. prior knowledge USED-FOR general representation method. method USED-FOR SANs. prior knowledge USED-FOR SANs. it USED-FOR language representation. it USED-FOR prior word frequency knowledge. prior word frequency knowledge CONJUNCTION prior translation lexicon knowledge. prior translation lexicon knowledge CONJUNCTION prior word frequency knowledge. it USED-FOR prior translation lexicon knowledge. prior translation lexicon knowledge USED-FOR bilingual data. prior word frequency knowledge CONJUNCTION monolingual data. monolingual data CONJUNCTION prior word frequency knowledge. method COMPARE Transformer - based baseline. Transformer - based baseline COMPARE method. Method is neural networks. ,"-based self-attention networks (SANs) have been shown to be effective in natural language processing tasks. This paper proposes to use language knowledge to improve the generalization ability of SANs. Specifically, the authors propose to use the prior knowledge from bilingual data and monolingual data to learn the language representation. The proposed method is evaluated on a variety of tasks.   ",This paper proposes a general representation method for self-attention networks (SANs) that uses language knowledge to learn language representations. The method is based on the Transformer-based representation method. The authors show that their method is able to learn a language representation that is similar to the language representation learned by a Transformer. They also show that the method can be applied to bilingual data and monolingual data.
18,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,"cat - and - mouse game USED-FOR cybersecurity. Moving Target Defense ( MTD ) HYPONYM-OF proactive defense methods. leader - follower games USED-FOR MTD. models USED-FOR sequential settings. incomplete information FEATURE-OF rational adversary. learning defense policies USED-FOR cyber - security. learning defense policies USED-FOR sequential settings. sequential settings USED-FOR cyber - security. optimal movement policy USED-FOR BSMGs. interaction USED-FOR optimal movement policy. Bayesian Stackelberg Markov Games ( BSMGs ) HYPONYM-OF game - theoretic model. BSMGs PART-OF landscape of incomplete - information Markov games. Strong Stackelberg Equilibrium ( SSE ) FEATURE-OF them. learning approach USED-FOR SSE. learning approach USED-FOR BSMG. MTD USED-FOR web - application security. optimal policy USED-FOR MTD domains. movement policy USED-FOR optimal policy. SSE FEATURE-OF BSMG. incomplete information FEATURE-OF MTD domains. MTD EVALUATE-FOR movement policy. OtherScientificTerm are reconnaissance, sub - optimal movement strategies, incomplete - information Markov games, and prior information. Method are movement strategies, defense policies, single - agent reinforcement learning techniques, and MTD system. Generic is they. ","This paper proposes a Bayesian Stackelberg Markov game model for moving target defense (MTD) in leader-following games, where the goal is to defend against an adversary that has sub-optimal movement strategies. The proposed model is based on Bayesian Markov games (BSMG), which is a model of Bayesian stochastic games with incomplete information. The authors show that BSMGs can be used to learn an optimal movement policy in MTD, which is then used to train a single-agent reinforcement learning algorithm. Experiments show that the proposed model outperforms the state-of-the-art methods in terms of MTD performance. ","This paper proposes a Bayesian Stackelberg Markov game (BSM) model for the moving target defense (MTD) problem. The model is based on Bayesian Markov games (BSMGs), which are incomplete-information Markov Games (MMGs) where the goal is to defend against a rational adversary. The authors show that the optimal movement policy in BSMGs can be learned using a single-agent reinforcement learning (SRL) approach. They also show that SRL can be used to learn the optimal BSMG in the context of the MTD problem."
27,SP:97911e02bf06b34d022e7548beb5169a1d825903,"unsupervised disentangled representation learning EVALUATE-FOR Variational Autoencoder ( VAE ) based frameworks. VAE im3 plementation choices USED-FOR PCA - like behavior. PCA - like behavior FEATURE-OF data sam4 ples. local orthogonality CONJUNCTION data re6 construction. data re6 construction CONJUNCTION local orthogonality. models USED-FOR entangled representations. architecture CONJUNCTION hyperparameter 7 setting. hyperparameter 7 setting CONJUNCTION architecture. architecture USED-FOR models. hyperparameter 7 setting FEATURE-OF models. multi9 ple VAEs PART-OF VAE ensemble framework. VAE ensemble 15 objective USED-FOR linear transformations. approach COMPARE unsupervised disen18 tangled representation learning approaches. unsupervised disen18 tangled representation learning approaches COMPARE approach. OtherScientificTerm are model identifiability, disentangled representations, signed permutation transformation, pair - wise linear transformations, VAEs, triv16 ial transformations, and latent representations. Method are VAE based disentanglement 5 frameworks, and VAE ensemble. Generic is It. ","This paper proposes a VAE-based method for unsupervised disentanglement. The method is based on the observation that VAE im-plementation choices can lead to PCA-like behavior in data samples. The authors propose to use multi-plea VAEs to learn disentangled representations, where each VAE encodes a pair of pair-wise linear transformations. The proposed method is evaluated on a set of synthetic and real-world datasets. ",This paper proposes a VAE ensemble-based disentangled representation learning framework for unsupervised VAE disentanglement learning. The authors propose a new VAE im3 plementation scheme that can be applied to any VAE-implemented representation learning task. They show that the proposed method outperforms the state-of-the-art in terms of performance on a variety of datasets. 
36,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,zero - shot approach USED-FOR automated machine learning ( AutoML ). model USED-FOR supervised learning task. zero - shot approach USED-FOR model. approach USED-FOR AutoML. meta - feature extractor USED-FOR data. free - text descriptions CONJUNCTION meta - feature extractor. meta - feature extractor CONJUNCTION free - text descriptions. transformer - based language embedding USED-FOR algorithms. meta - feature extractor USED-FOR method. free - text descriptions USED-FOR algorithms. transformer - based language embedding USED-FOR method. meta - feature extractor USED-FOR algorithms. graph neural network USED-FOR machine learning pipeline. approach USED-FOR AutoML. unsupervised representation learning USED-FOR AutoML. unsupervised representation learning USED-FOR natural language processing. unsupervised representation learning USED-FOR approach. Method is AutoML systems. Metric is running time. OtherScientificTerm is prediction time. ,This paper proposes a zero-shot approach for automated machine learning (AutoML) that uses a meta-feature extractor to extract the data from a set of free-text descriptions and uses a transformer-based language embedding to learn the representation of the data. The proposed method is able to achieve state-of-the-art performance in terms of prediction time and inference time.  ,"This paper proposes a zero-shot approach for automated machine learning (AutoML) with a meta-feature extractor, where the meta-features are extracted from the data using a transformer-based language embedding. The proposed method is based on a graph neural network (GNN) that is trained on a set of free-text descriptions. The authors show that the proposed method outperforms the state-of-the-art in terms of prediction time. "
45,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,optimization process USED-FOR non - compositional solutions. compositionality learning approaches USED-FOR compositionality. model architecture design USED-FOR compositionality learning approaches. compositional learning CONJUNCTION gradient descent. gradient descent CONJUNCTION compositional learning. machine learning models USED-FOR human - level intelligence. Method is neural network optimization. Task is compositional generalization. ,": This paper studies the problem of compositional generalization in deep neural networks. The authors propose to use compositionality as a measure of generalization ability, and show that compositional learning can be used to improve generalization performance. They also show that gradient descent is a good way to learn compositionality. ","This paper studies the problem of compositional generalization in neural network optimization. The authors propose a new approach to generalize compositional learning, which is based on the notion of ""compositional generalisation"". The main idea is to learn a compositional model that can generalize well across different tasks. They show that this generalization can be achieved by learning a model architecture that is compositional in nature. They also show that the compositionality learning approach can be combined with gradient descent to improve generalization. "
54,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"Knowledge graph ( KG ) representation learning USED-FOR entity alignment. machine translation CONJUNCTION feature extraction. feature extraction CONJUNCTION machine translation. methods COMPARE embeddingbased ones. embeddingbased ones COMPARE methods. embedding spaces PART-OF KGs. pre - aligned entities USED-FOR embedding spaces. scoring function USED-FOR embedding learning. margin FEATURE-OF scoring function. margin USED-FOR representation discrepancy. approach USED-FOR KG - invariant and principled entity representations. feature distribution CONJUNCTION ontology knowledge. ontology knowledge CONJUNCTION feature distribution. neural ontologies PART-OF KGs. state - of - the - art ones HYPONYM-OF embedding - based entity alignment methods. Generic are they, paradigm, and model. Method is alignment learning. OtherScientificTerm is geometric distance. ","This paper proposes a method for entity alignment in Knowledge Graphs (KGs) based on pre-alignment of entities in the embedding space. The main idea is to use a scoring function to measure the distance between the embeddings of two entities in a KG. The margin of the scoring function is defined as the difference between the two embedding spaces with respect to a set of pre-aligned entities. The authors show that the margin can be used to measure representation discrepancy between two entities, which is a measure of how close the two entities are to each other in terms of geometric distance.   The authors then show that their method is able to learn entity representations that are KG-invariant and principled. The method is evaluated on a variety of tasks, including machine translation and feature extraction. ","This paper proposes a method for entity alignment in Knowledge Graph (KG) representation learning. In particular, the authors propose a scoring function for the embedding learning of KG embeddings. The scoring function is based on the geometric distance between two embedding spaces. The authors show that the margin of the scoring function can be used to measure the representation discrepancy between two KG representations. They also show that this margin can be applied to the representation of a KG-invariant and principled entity representations."
63,SP:0e42de72d10040289283516ec1bd324788f7d371,"Convolutional Neural Networks ( CNNs ) powered functionalities USED-FOR ubiquitous intelligent “ IoT cameras ”. medicineand wearable - related ones HYPONYM-OF applications. CNNs COMPARE IoT devices. IoT devices COMPARE CNNs. limited resources FEATURE-OF IoT devices. storage and energy cost USED-FOR CNNs. form factor FEATURE-OF PhlatCam. compression techniques USED-FOR storage and energy reduction. Sensor Algorithm Co - Design framework USED-FOR CNN - powered PhlatCam. SACoD USED-FOR CNN - powered PhlatCam. SACoD HYPONYM-OF Sensor Algorithm Co - Design framework. mask CONJUNCTION backend CNN model. backend CNN model CONJUNCTION mask. PhlatCam sensor CONJUNCTION backend CNN model. backend CNN model CONJUNCTION PhlatCam sensor. mask PART-OF PhlatCam sensor. differential neural architecture search USED-FOR mask. model compression CONJUNCTION energy savings. energy savings CONJUNCTION model compression. energy savings EVALUATE-FOR SACoD framework. model compression EVALUATE-FOR SACoD framework. task accuracy EVALUATE-FOR SACoD framework. PhlatCam imaging system EVALUATE-FOR SACoD. Method are IoT systems, CNN algorithm, and SOTA ) designs. Metric is camera form factor. OtherScientificTerm is model parameters. Generic is tasks. ",This paper proposes a method to reduce the storage and energy cost of CNNs in IoT devices. The proposed method is based on differential neural architecture search (DNNS) to find the mask in the PhlatCam sensor that can be used to compress the model parameters and reduce the energy consumption. The method is evaluated on a variety of image classification tasks and compared with the state of the art. ,"This paper proposes a sensor-based method to reduce the energy and storage cost of a CNN-powered PhlatCam. The proposed method is based on the SACoD framework, which proposes a new sensor algorithm that can be applied to the Phlatcam sensor and the backend CNN model. The key idea is to use a differential neural architecture search (DNA) to find the mask of the sensor that best matches the parameters of the CNN model, which can be used to compress the model parameters and reduce the storage and energy cost of the camera. The method is evaluated on a variety of tasks, and the proposed method outperforms the state-of-the-art in terms of accuracy. "
72,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"Honey bees USED-FOR complex social systems. dataset USED-FOR temporal matrix factorization model. temporal matrix factorization model USED-FOR average developmental path. lifetime trajectories PART-OF dataset. social sciences CONJUNCTION neuroscience. neuroscience CONJUNCTION social sciences. behavioral biology CONJUNCTION social sciences. social sciences CONJUNCTION behavioral biology. neuroscience CONJUNCTION information science. information science CONJUNCTION neuroscience. method USED-FOR behavioral heterogeneity. behavioral heterogeneity FEATURE-OF complex social systems. information science HYPONYM-OF fields. behavioral biology HYPONYM-OF fields. social sciences HYPONYM-OF fields. neuroscience HYPONYM-OF fields. OtherScientificTerm are global behavior, and social network. Material is honey bee colonies. ","This paper proposes a method to estimate the average developmental path of a population of honey bees using a dataset of lifetime trajectories. The dataset consists of a set of trajectories from the same population over multiple years. The authors use a temporal matrix factorization model to model the average trajectory of the population. The model is trained using a combination of two approaches: (1) a population-level model that estimates the average trajectories of the entire population, and (2) an ensemble of individual trajectories that are estimated using the average of the individual lifetime trajectory.  The authors show that their method is able to capture behavioral heterogeneity in the population, which is important for understanding complex social systems.  ","This paper proposes a method to estimate the average developmental path of a population of honey bees. The method is based on a temporal matrix factorization model that estimates the average trajectories of the bees' lifetime trajectories. The authors show that the average trajectory of a honey bee is determined by the average of its lifetime trajectory across a dataset of bees. They also show that their method can be applied to other complex social systems, such as neuroscience, neuroscience, and social sciences."
81,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"Deep neural networks USED-FOR image restoration and reconstruction tasks. noisy or corrupted measurement USED-FOR networks. pipeline USED-FOR data augmentation. Data Augmentation ( DA ) USED-FOR classification problems. data augmentation USED-FOR image reconstruction tasks. medical imaging FEATURE-OF image reconstruction tasks. invariances FEATURE-OF medical imaging measurements. naive DA strategies USED-FOR DA pipeline. invariances USED-FOR DA pipeline. problem regimes EVALUATE-FOR DA. fastMRI dataset EVALUATE-FOR DA. training data USED-FOR single - coil reconstruction. training data USED-FOR multi - coil reconstruction. multi - coil reconstruction CONJUNCTION single - coil reconstruction. single - coil reconstruction CONJUNCTION multi - coil reconstruction. training data CONJUNCTION training data. training data CONJUNCTION training data. Task is accelerated magnetic resonance imaging. OtherScientificTerm are under - sampled linear measurements, and high - data regime. Method is data augmentation pipeline. ","This paper proposes a data augmentation method for image reconstruction in medical imaging. The proposed method is based on the idea of data augmentations, i.e. adding invariances to a set of measurements to improve the reconstruction performance. The method is evaluated on the accelerated magnetic resonance imaging (AMIR) dataset, where it achieves state-of-the-art results.   ",This paper proposes a data augmentation (DA) method for image reconstruction and restoration tasks. The authors propose a pipeline to augment the training data with invariances to improve the performance of deep neural networks. They show that the proposed method outperforms the state-of-the-art methods in the high-data regime. They also show that their method can be applied to the fastMRI dataset.
90,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"deep repulsive clustering ( DRC ) algorithm USED-FOR order learning. ordered data USED-FOR deep repulsive clustering ( DRC ) algorithm. order - related feature CONJUNCTION identity feature. identity feature CONJUNCTION order - related feature. facial age estimation CONJUNCTION aesthetic score regression. aesthetic score regression CONJUNCTION facial age estimation. aesthetic score regression CONJUNCTION historical color image classification. historical color image classification CONJUNCTION aesthetic score regression. algorithm USED-FOR ordered data. historical color image classification EVALUATE-FOR algorithm. facial age estimation EVALUATE-FOR algorithm. rank estimation EVALUATE-FOR algorithm. Method is order - identity decomposition ( ORID ) network. OtherScientificTerm are identity features, repulsive term, and rank. ","This paper proposes a deep repulsive clustering (DRC) algorithm for order learning. The proposed method is based on order-identity decomposition (ORID) network, where the order-related features are repulsive and the identity features are identity features. The main contribution of this paper is to introduce a repulsive term in the order learning algorithm, which is used for rank estimation. Experiments are conducted on facial age estimation, aesthetic score regression, and color image classification.","This paper proposes a novel deep repulsive clustering (DRC) algorithm for order learning. The proposed method is based on the order-identity decomposition (ORID) network, which decomposes the data into order-related features and identity features. The ORID network is trained on a set of ordered data, where the identity feature is the repulsive term, and the identity features are the rank of the data. The authors show that the proposed method outperforms the state-of-the-art DRC algorithm in terms of rank estimation and aesthetic score regression. "
99,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"Exploration PART-OF model - free reinforcement learning. sparse reward USED-FOR Exploration. intrinsic rewards USED-FOR state - of - the - art methods. methods USED-FOR procedurally - generated environments. intrinsic rewards USED-FOR methods. episode - level exploration method USED-FOR procedurally - generated environments. RAPID HYPONYM-OF episode - level exploration method. per - episode and long - term views FEATURE-OF episodic exploration score. episodic exploration score EVALUATE-FOR RAPID. sparse MuJoCo tasks EVALUATE-FOR method. procedurally - generated MiniGrid environments EVALUATE-FOR method. RAPID COMPARE intrinsic reward strategies. intrinsic reward strategies COMPARE RAPID. sample efficiency EVALUATE-FOR intrinsic reward strategies. sample efficiency EVALUATE-FOR RAPID. OtherScientificTerm are uncertain environment dynamics, and ranking buffer. Material is MiniWorld. ","This paper proposes an exploration method for model-free reinforcement learning in sparse MuJoCo. The proposed method, called RAPID, is an episode-level exploration method that uses a ranking buffer to guide exploration in each episode. The exploration score is computed based on both per-episode and long-term views of the environment. Experiments on MiniGrid environments show that the proposed method outperforms the state-of-the-art methods in terms of exploration efficiency. ","This paper proposes an episodic exploration method for model-free reinforcement learning with sparse MuJoCo. The authors propose an episode-level exploration method, called RAPID, for procedurally-generated environments. The method is based on a ranking buffer, where each episode is represented as a set of episodes, and the exploration score is a sum of the per-episode and long-term views of the environment.  The authors demonstrate the effectiveness of the proposed method on a variety of MuJCo tasks, including MiniGrid environments. "
108,SP:30024ac5aef153ae24c893a53bad93ead2526476,"semantic space of class attributes CONJUNCTION visual space of images. visual space of images CONJUNCTION semantic space of class attributes. Isometric Propagation Network ( IPN ) USED-FOR class dependency. IPN USED-FOR class representations. auto - generated graph USED-FOR class representations. ZSL benchmarks EVALUATE-FOR IPN. them USED-FOR IPN. Method are Zero - shot learning ( ZSL ), static representation, and dynamic propagation procedures. OtherScientificTerm are class attributes, imbalanced supervision, and semantic and the visual space. Generic are representations, and mapping. Task is ZSL settings. Material is seen - class data. Metric is consistency loss. ","This paper proposes a novel method for zero-shot learning in the presence of imbalanced supervision. The proposed method is based on the Isometric Propagation Network (IPN), which maps the semantic space of class attributes and the visual space of images into a single representation. The authors show that the proposed method can learn the class representations from the seen-class data. The method is evaluated on a variety of benchmark datasets.","This paper proposes a new method for zero-shot learning (ZSL) with imbalanced supervision. The proposed method is based on the Isometric Propagation Network (IPN), which is an auto-generated graph that maps the semantic and the visual space of images into a single class representation. The authors show that the proposed method outperforms the state-of-the-art methods in terms of consistency loss.  "
117,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"multi - task model USED-FOR tasks. HyperGrid Transformers HYPONYM-OF Transformer architecture. task - conditioned hyper networks USED-FOR feed - forward layers. task - conditioned hyper networks USED-FOR Transformer architecture. decomposable hypernetwork USED-FOR grid - wise projections. global ( task - agnostic ) state CONJUNCTION local task - specific state. local task - specific state CONJUNCTION global ( task - agnostic ) state. method USED-FOR hypernetwork. SuperGLUE test set EVALUATE-FOR state - of - the - art. fine - tuning CONJUNCTION multi - task learning approaches. multi - task learning approaches CONJUNCTION fine - tuning. method USED-FOR fine - tuning. method USED-FOR multi - task learning approaches. Task is natural language understanding tasks. Generic are model, and approach. OtherScientificTerm is weight matrices. Material is GLUE / SuperGLUE. ","This paper proposes a new architecture for multi-task learning in NLP tasks. The proposed architecture is based on a task-conditioned hypernetwork, which is a decomposable hypernetwork with a grid-wise projections. The authors show that the proposed architecture achieves state-of-the-art performance on GLUE and SuperGLUE tasks. ",This paper proposes a new Transformer architecture for multi-task learning. The main idea is to use a task-conditioned hypernetwork to learn the global and local task-specific states of the model. The model is trained using the SuperGLUE test set. The proposed method is evaluated on the GLUE and Super-GLUE datasets. 
126,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"lighting CONJUNCTION weather. weather CONJUNCTION lighting. weather CONJUNCTION visibility conditions. visibility conditions CONJUNCTION weather. image input USED-FOR autonomous driving. image input USED-FOR learning algorithm. algorithm USED-FOR task. sensitivity analysis USED-FOR algorithm. sensitivity analysis USED-FOR task. approach USED-FOR learning outcomes. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. neural network training USED-FOR self - driving cars. approach COMPARE techniques. techniques COMPARE approach. algorithm USED-FOR neural network training. robustness EVALUATE-FOR algorithm. adversarial training HYPONYM-OF techniques. data augmentation HYPONYM-OF techniques. OtherScientificTerm are external and environmental factors, and sensors. Task is perceptual data processing. ","This paper proposes a method to improve the performance of self-driving cars in the presence of external and environmental factors in the image input. The authors propose to use the sensitivity analysis to estimate the sensitivity of the input image to external factors, and then use this information to train a neural network to predict the outcome of the image. The proposed method is evaluated on a variety of image inputs, and is shown to outperform existing methods in terms of accuracy and robustness.   ","This paper proposes a new method for self-driving autonomous driving. The main idea is to learn a learning algorithm that is robust to external and environmental factors such as lighting, weather, and visibility conditions. The method is based on a sensitivity analysis of the input image and the learning algorithm. The authors show that their method outperforms the state-of-the-art in terms of robustness. "
135,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"deep networks USED-FOR approximate solvers. hard constraints FEATURE-OF Large optimization problems. hard constraints FEATURE-OF problems. method USED-FOR feasibility. gradient - based corrections USED-FOR inequality constraints. differentiable procedure USED-FOR method. differentiable procedure USED-FOR feasibility. DC3 USED-FOR AC optimal power flow. DC3 USED-FOR synthetic optimization tasks. synthetic optimization tasks CONJUNCTION AC optimal power flow. AC optimal power flow CONJUNCTION synthetic optimization tasks. hard constraints FEATURE-OF physics of the electrical grid. DC3 USED-FOR near - optimal objective values. feasibility FEATURE-OF DC3. Method are classical solvers, deep learning approaches, and Deep Constraint Completion and Correction ( DC3 ). OtherScientificTerm are infeasible solutions, and equality constraints. Generic is algorithm. ","This paper proposes Deep Constraint Completion and Correction (DC3), a method for solving large-scale optimization problems with hard constraints. The main idea is to use gradient-based corrections to solve the inequality constraints in the problem, and then use a differentiable procedure to estimate the feasibility of the solution. The proposed method is shown to achieve near-optimal objective values in a variety of experiments. ","This paper proposes Deep Constraint Completion and Correction (DC3), a method for solving large-scale optimization problems with hard constraints. The main idea of DC3 is to use gradient-based corrections for inequality constraints to improve the feasibility of the solver. The method is based on a differentiable procedure that can be applied to any problem with inequality constraints. It is shown that DC3 can achieve near-optimal objective values for a variety of optimization problems, and it can be used to solve the AC optimal power flow."
144,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"Regularization USED-FOR sparsity. Regularization USED-FOR deep neural network pruning. pruning schedule CONJUNCTION weight importance scoring. weight importance scoring CONJUNCTION pruning schedule. problems PART-OF pruning. weight importance scoring HYPONYM-OF pruning. pruning schedule HYPONYM-OF pruning. weight importance scoring HYPONYM-OF problems. pruning schedule HYPONYM-OF problems. it COMPARE one - shot counterpart. one - shot counterpart COMPARE it. L2 regularization variant COMPARE one - shot counterpart. one - shot counterpart COMPARE L2 regularization variant. rising penalty factors FEATURE-OF L2 regularization variant. Hessian information USED-FOR pruning. growing penalty scheme USED-FOR approach. approach USED-FOR Hessian information. networks USED-FOR structured and unstructured pruning. algorithms USED-FOR large datasets. large datasets CONJUNCTION networks. networks CONJUNCTION large datasets. networks EVALUATE-FOR algorithms. CIFAR and ImageNet datasets EVALUATE-FOR deep neural networks. OtherScientificTerm are small penalty strength regime, and regularization. Task is Hessian approximation problems. Generic is state - of - the - art algorithms. ","This paper studies the problem of regularization in deep neural network pruning. The authors propose a growing penalty scheme for regularization, which is based on the Hessian approximation problem. They show that the growing penalty is equivalent to a regularization method that uses Hessian information for pruning, and show that it can be used for both structured and unstructured pruning for both large datasets and networks.  ","This paper proposes a new regularization method for deep neural network pruning. The main idea is to use the Hessian information of the pruning process to improve the performance of the proposed method. The proposed method is based on the L2 regularization, which is a growing penalty scheme. The authors show that their method outperforms the state-of-the-art methods on CIFAR and ImageNet datasets. "
153,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"Model - based planning USED-FOR deep, careful reasoning. deep, careful reasoning CONJUNCTION generalization. generalization CONJUNCTION deep, careful reasoning. Model - based planning USED-FOR generalization. generalization USED-FOR artificial agents. deep, careful reasoning USED-FOR artificial agents. deep function approximation USED-FOR model - based reinforcement learning ( MBRL ). planning USED-FOR MBRL agents. planning USED-FOR generalization. MuZero HYPONYM-OF MBRL algorithm. MuZero COMPARE MBRL algorithms. MBRL algorithms COMPARE MuZero. overlapping components CONJUNCTION MBRL algorithms. MBRL algorithms CONJUNCTION overlapping components. MBRL algorithm COMPARE MBRL algorithms. MBRL algorithms COMPARE MBRL algorithm. overlapping components PART-OF MuZero. overlapping components PART-OF MBRL algorithm. control tasks CONJUNCTION Atari. Atari CONJUNCTION control tasks. Atari CONJUNCTION 9x9 Go. 9x9 Go CONJUNCTION Atari. Planning USED-FOR learning process. Planning USED-FOR policy updates. Planning USED-FOR data distribution. Monte - Carlo rollouts USED-FOR shallow trees. Planning USED-FOR generalization. planning USED-FOR reinforcement learning settings. zeroand few - shot learning CONJUNCTION strategic thinking. strategic thinking CONJUNCTION zeroand few - shot learning. Model - based reinforcement learning ( MBRL ) COMPARE model - free methods. model - free methods COMPARE Model - based reinforcement learning ( MBRL ). data efficiency CONJUNCTION zeroand few - shot learning. zeroand few - shot learning CONJUNCTION data efficiency. zeroand few - shot learning EVALUATE-FOR model - free methods. data efficiency EVALUATE-FOR model - free methods. planning CONJUNCTION learning. learning CONJUNCTION planning. learning PART-OF methods. planning PART-OF methods. models USED-FOR intelligent artificial agents. models USED-FOR discrete search. planning PART-OF MBRL algorithm. MuZero HYPONYM-OF MBRL algorithm. value estimation CONJUNCTION policy optimization. policy optimization CONJUNCTION value estimation. learned model CONJUNCTION value estimation. value estimation CONJUNCTION learned model. search - based planning CONJUNCTION","This paper proposes a model-based reinforcement learning algorithm, MuZero, that combines planning and model-free reinforcement learning. The main idea is to use a deep function approximation to model the data distribution and use Monte-Carlo rollouts to train a shallow tree. The proposed method is evaluated on a variety of tasks, including zero-shot and few-shot learning.   ","This paper proposes MuZero, a model-based reinforcement learning (MBRL) algorithm for zero-shot and few-shot learning. The main idea of MuZero is to use a deep function approximation to estimate the value of the learned model, which is then used to guide the planning of the agent. The authors show that MuZero outperforms other model-free MBRL algorithms in terms of data efficiency and generalization. They also show that their algorithm is able to outperform the state-of-the-art zero-and-few-shot RL algorithms."
162,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"long - range reasoning CONJUNCTION understanding of environment dynamics. understanding of environment dynamics CONJUNCTION long - range reasoning. Value Iteration Networks ( VINs ) USED-FOR implicit planning. long - range reasoning USED-FOR tasks. deep reinforcement learning USED-FOR implicit planning. understanding of environment dynamics USED-FOR tasks. graph representation learning CONJUNCTION neural algorithmic reasoning. neural algorithmic reasoning CONJUNCTION graph representation learning. contrastive self - supervised learning CONJUNCTION graph representation learning. graph representation learning CONJUNCTION contrastive self - supervised learning. generic environments USED-FOR VIN - style models. XLVINs COMPARE VIN - like models. VIN - like models COMPARE XLVINs. XLVINs COMPARE model - free baselines. model - free baselines COMPARE XLVINs. MDP USED-FOR VIN - like models. Generic is model. Task is planning computations. OtherScientificTerm are state space, and Markov decision process ( MDP ). Method is Latent Value Iteration Networks ( XLVINs ). ","This paper proposes Latent Value Iteration Networks (XLVINs), a method for learning to model the dynamics of a Markov decision process (MDP) in the state space. The method is based on contrastive self-supervised learning and graph representation learning, where the goal is to learn a representation of the MDP. The proposed method is evaluated on a set of synthetic and real-world tasks, where it is shown that the proposed method outperforms baselines.","This paper proposes a new method for learning value-iteration networks (VINs) for long-range planning. The key idea is to learn a graph representation of the state space, which is then used to model a Markov decision process (MDP). The authors show that the proposed method outperforms the state-of-the-art VIN-like models on a variety of tasks."
171,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"Learning functions PART-OF machine learning. Boolean variables FEATURE-OF Learning functions. neural networks USED-FOR functions. distribution free setting FEATURE-OF functions. networks USED-FOR they. read - once DNFs HYPONYM-OF functions. convex neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION convex neural network. convex neural network USED-FOR functions. gradient descent USED-FOR functions. inductive bias FEATURE-OF learning process. ones HYPONYM-OF networks. networks USED-FOR risk. gradient descent USED-FOR compact representation. process USED-FOR DNF. it USED-FOR process. computer assisted proof USED-FOR inductive bias. inductive bias FEATURE-OF DNFs. computer assisted proof USED-FOR DNFs. network USED-FOR process. network USED-FOR DNF. optimization USED-FOR inductive bias. learning process CONJUNCTION optimization. optimization CONJUNCTION learning process. network USED-FOR l2 norm. network USED-FOR DNF terms. margin constraints FEATURE-OF l2 norm. OtherScientificTerm are uniform distribution, neurons, logical formulas, and high dimensional DNFs. Method is zero - error networks. Metric is population risk. Material is tabular datasets. ","This paper studies the problem of learning learning functions in the distribution-free setting. The authors show that learning functions with read-once DNFs, i.e., functions that can be represented by a convex neural network, can be approximated by gradient descent. They show that the learning process can be viewed as a process of learning a compact representation of the DNF, which is then used to compute the loss function. They also show that this process is computationally tractable.   ","This paper studies the problem of zero-error neural networks (ZNNs) in the distribution-free setting. The authors show that ZNNs can be read-once DNFs (read-only, read-only-read-one DNF). They show that the inductive bias of the learning process is due to the compact representation of the DNF. They also provide a computer-assisted proof of this phenomenon.   "
180,SP:6e600bedbf995375fd41cc0b517ddefb918318af,exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. sparse environment FEATURE-OF map. graph structure USED-FOR exploration directions. Graph Structured Reinforcement Learning ( GSRL ) USED-FOR value function estimation. graph structure USED-FOR value function estimation. graph structure PART-OF historical trajectories. graph structure USED-FOR Graph Structured Reinforcement Learning ( GSRL ). state transitions PART-OF replay buffer. GSRL USED-FOR dynamic graph. attention strategy USED-FOR map. state transitions USED-FOR dynamic graph. historical trajectories USED-FOR dynamic graph. graph structure USED-FOR value learning. GSRL COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE GSRL. sample efficiency EVALUATE-FOR state - of - the - art algorithms. sample efficiency EVALUATE-FOR GSRL. Task is reinforcement learning. OtherScientificTerm is sparse reward functions. ,"This paper proposes a novel method for reinforcement learning with sparse reward functions. The proposed method is based on graph structured reinforcement learning (GSRL), which uses a replay buffer to store historical trajectories and a dynamic graph to learn the exploration directions in the sparse environment. The method is evaluated on a variety of tasks and achieves state-of-the-art performance. ","This paper proposes a graph structured reinforcement learning (GSRL) framework for value learning in sparse environments with sparse reward functions. The main idea of GSRL is to use a replay buffer to store historical trajectories of the state transitions in the replay buffer, which are then used to learn the dynamics of the dynamic graph. The authors show that GSRL can achieve state-of-the-art sample efficiency in terms of the number of exploration and exploitation attempts.  "
189,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"Simulated environments EVALUATE-FOR systematic generalization of reinforcement learning agents. procedurally generated content FEATURE-OF Simulated environments. positions of entities CONJUNCTION asset appearances. asset appearances CONJUNCTION positions of entities. layout CONJUNCTION positions of entities. positions of entities CONJUNCTION layout. asset appearances CONJUNCTION rules. rules CONJUNCTION asset appearances. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR test levels. robustness EVALUATE-FOR test levels. levels USED-FOR learning progress. framework USED-FOR future learning potential. Prioritized Level Replay HYPONYM-OF framework. Prioritized Level Replay USED-FOR future learning potential. sample - efficiency CONJUNCTION generalization. generalization CONJUNCTION sample - efficiency. Procgen Benchmark environments CONJUNCTION MiniGrid environments. MiniGrid environments CONJUNCTION Procgen Benchmark environments. Prioritized Level Replay USED-FOR implicit curriculum. Generic is environment. OtherScientificTerm are environment transitions, training levels, agent, and temporal - difference ( TD ) errors. ","This paper studies the problem of systematic generalization in simulated environments with procedurally generated content. The authors propose an implicit curriculum for training RL agents to learn to generalize to new environments. The proposed method is based on prioritized level replay, where the agent is replayed at a fixed number of training levels, and the goal is to minimize the temporal difference (TD) errors between training and test levels.  The authors show that the proposed method improves sample efficiency, generalization, and robustness.  ","This paper proposes a new method to improve the generalization performance of reinforcement learning agents in simulated environments. The method is based on the idea of implicit curriculum, where the agent is given a set of training levels and the goal is to learn to generalize well across training levels. The authors propose a new approach called Prioritized Level Replay (PL Replay), which is a method to replay the training levels of the agent at different training levels to improve sample-efficiency and generalization. The proposed method is evaluated on Procgen and MiniGrid environments. "
198,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"pre - training CONJUNCTION multitask learning. multitask learning CONJUNCTION pre - training. deep learning USED-FOR data - rich settings. pre - training USED-FOR tasks. multitask learning USED-FOR tasks. automatic differentiation procedures CONJUNCTION randomized singular value decomposition. randomized singular value decomposition CONJUNCTION automatic differentiation procedures. randomized singular value decomposition USED-FOR scalability. scalability EVALUATE-FOR method. automatic differentiation procedures USED-FOR method. randomized singular value decomposition USED-FOR method. approach COMPARE baselines. baselines COMPARE approach. out - of - distribution data USED-FOR Text and Image classification tasks. Text and Image classification tasks EVALUATE-FOR approach. out - of - distribution data EVALUATE-FOR baselines. out - of - distribution data EVALUATE-FOR approach. OtherScientificTerm are model parameterizations, auxiliary tasks, auxiliary task gradients, auxiliary updates, and primary task loss. Method is modelagnostic framework. Generic are algorithm, and framework. ",This paper proposes a model-agnostic multi-task pre-training approach for multi-class classification tasks. The main idea is to use auxiliary tasks as auxiliary gradients to update the model parameters during training. The auxiliary task gradients are computed by using the gradients of the auxiliary tasks and the primary task loss. The authors show that the proposed method is model-free and can be applied to a wide range of tasks.  ,"This paper proposes a model-agnostic multi-task learning framework for pre-training and multitask learning in data-rich settings. The main idea of the paper is to train a model that can handle multiple tasks at the same time. The authors propose a novel method to do so, which is based on a randomized singular value decomposition (RV decomposition). The authors show that the proposed method can be applied to out-of-distribution data and outperforms other baselines. "
207,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"large text - based neural language models USED-FOR one - shot learning. RL algorithms USED-FOR one - shot word learning. short - term, within - episode knowledge CONJUNCTION long - term lexical and motor knowledge. long - term lexical and motor knowledge CONJUNCTION short - term, within - episode knowledge. memory writing mechanism USED-FOR one - shot word - object binding. dual - coding memory USED-FOR intrinsic motivation. deep neural networks USED-FOR fast - mapping. episodic memory CONJUNCTION multi - modal environment. multi - modal environment CONJUNCTION episodic memory. meta - learning CONJUNCTION episodic memory. episodic memory CONJUNCTION meta - learning. multi - modal environment USED-FOR fast - mapping. transformative capacity FEATURE-OF artificial agents. human cognitive development CONJUNCTION transformative capacity. transformative capacity CONJUNCTION human cognitive development. fast - mapping HYPONYM-OF human cognitive development. meta - learning USED-FOR deep neural networks. multi - modal environment USED-FOR deep neural networks. episodic memory USED-FOR deep neural networks. Material is simulated 3D world. OtherScientificTerm are dual - coding external memory, visual perception and language, and ShapeNet category. ","This paper proposes a novel method for one-shot word-object binding in a large text-based neural language models. The proposed method is based on a meta-learning approach where the model is trained to learn a memory writing mechanism to encode short-term, within-episode knowledge, and long-term lexical and motor knowledge. The paper also proposes a dual-coding memory for intrinsic motivation to guide the learning process. Experiments show that the proposed method achieves state-of-the-art performance on the ShapeNet task. ","This paper proposes a novel approach to one-shot word-object binding in the context of a large text-based neural language model. The authors propose a novel memory-writing mechanism for the task, which is based on the dual-coding of the episodic memory. They also propose a meta-learning framework to improve the performance of deep neural networks in the multi-modal environment. The proposed method is evaluated on a ShapeNet environment, where the authors show that the proposed method can improve performance on the task of fast-mapping."
216,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"Few - shot learning USED-FOR models. support set USED-FOR setup. class - imbalance HYPONYM-OF dynamic nature of the real world. backbones USED-FOR few - shot learning methods. strategies USED-FOR imbalance. feature - transfer CONJUNCTION metric - based methods. metric - based methods CONJUNCTION feature - transfer. strategies USED-FOR few - shot case. balanced task COMPARE class - imbalance counterparts. class - imbalance counterparts COMPARE balanced task. imbalance FEATURE-OF supervised learning. strategies USED-FOR supervised learning. imbalance COMPARE support set level. support set level COMPARE imbalance. imbalance FEATURE-OF dataset level. dataset level COMPARE support set level. support set level COMPARE dataset level. class - imbalance counterparts COMPARE optimization - based methods. optimization - based methods COMPARE class - imbalance counterparts. OtherScientificTerm are few - shot class - imbalance, dataset vs. support set imbalance, and imbalance distributions. Method is rebalancing techniques. ","This paper studies the problem of class-imbalance in few-shot learning. The authors show that there is a large class imbalance in the support set, which is caused by the fact that there are too many classes in the training set and not enough examples in the test set. To address this problem, the authors propose two strategies: (1) feature-transfer and (2) metric-based methods. The proposed strategies are shown to improve the performance of the proposed methods in terms of accuracy and training time.   ","This paper studies the problem of few-shot class-imbalance in supervised learning, where the class imbalance is defined as the difference between the dataset vs. support set imbalance and the imbalance between the two sets of data. The authors show that the imbalance can be controlled by a number of strategies, including feature-transfer, metric-based methods, and rebalancing strategies. They also show that these strategies can be used to improve the performance of the model. "
225,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"convolution operators USED-FOR representations of graphs. Graph Convolutional Neural Networks ( GCNs ) USED-FOR representations of graphs. convolution operators USED-FOR Graph Convolutional Neural Networks ( GCNs ). neighborhood aggregating scheme USED-FOR convolution operators. local topological information USED-FOR convolution operators. decoupled representations USED-FOR them. graph convolution layer USED-FOR neighbouring nodes. topological distances FEATURE-OF neighbouring nodes. readout layers USED-FOR representations. convolution operators CONJUNCTION linear stacking. linear stacking CONJUNCTION convolution operators. Polynomial Graph Convolution ( PGC ) layer COMPARE convolution operators. convolution operators COMPARE Polynomial Graph Convolution ( PGC ) layer. Polynomial Graph Convolution ( PGC ) layer USED-FOR strategy. receptive field FEATURE-OF convolution operator. single PGC layer USED-FOR Graph Neural Network architecture. graph classification benchmarks EVALUATE-FOR Graph Neural Network architecture. OtherScientificTerm are wider topological receptive fields, and GC parameters. Method is non - linear graph convolutions. Metric is neural network expressiveness. ","This paper proposes a novel neighborhood aggregation scheme for graph convolutional neural networks (GCNs) based on local topological information. The proposed method is based on decoupled representations for neighbouring nodes and readout layers, where the topological distances between neighbouring nodes are computed using a neighborhood aggregating scheme. The method is evaluated on several graph classification tasks and achieves state-of-the-art performance. ","This paper proposes Polynomial Graph Convolution (PGC) layer for graph convolutional neural networks (GCNs). The idea is to use the local topological information of the graph as a decoupled representation for each node in the graph, and use the topological distance between neighbouring nodes as a readout layer. The authors show that the PGC layer can improve the expressiveness of GCNs on a variety of graph classification tasks. "
234,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,labeled datasets USED-FOR SG generation techniques. neural network models COMPARE real data. real data COMPARE neural network models. synthetic data USED-FOR neural network models. scalable technique USED-FOR sim - to - real transfer. sim - to - real transfer USED-FOR scene graph generation. Sim2SG HYPONYM-OF scalable technique. scalable technique USED-FOR scene graph generation. Sim2SG USED-FOR domain gap. supervision PART-OF real - world dataset. supervision USED-FOR Sim2SG. baselines USED-FOR domain gap. toy simulators CONJUNCTION realistic simulators. realistic simulators CONJUNCTION toy simulators. real - world data USED-FOR realistic simulators. realistic simulators EVALUATE-FOR approach. toy simulators EVALUATE-FOR approach. Task is Scene graph ( SG ) generation. Material is Synthetic data. OtherScientificTerm is appearance. Generic is discrepancies. ,"This paper proposes Sim2SG, a method for scene graph (SG) generation from synthetic data. The main idea is to use synthetic data to train a scene graph model that can be used for scene generation from real data. To do so, the authors propose to use a set of simulated data generated from a real-world dataset and use it to train the model on the synthetic data, and then use the generated data from the real data to improve the performance of the model.   The main contribution of the paper is to propose a new method for sim-to-real transfer of scene graph generation. The method is based on the observation that the difference between real data and synthetic data can lead to discrepancies in the appearance of the generated scenes. To address this discrepancy, the paper proposes to use simulated data from toy simulators and real data from simulated environments to train scene graph models. The proposed method is evaluated on toy and simulated environments, and shows improved performance compared to baselines. ","This paper proposes Sim2SG, a new method for scene graph generation from synthetic data to real-world data. The main idea is to use a synthetic dataset to generate a scene graph, and then use the generated scene graph to train a neural network model on the real data to generate the scene graph. The model is trained on the synthetic dataset, and the real dataset is used to train the neural network models. The method is evaluated on toy simulators and realistic simulators. "
243,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,model - based methods COMPARE model - free methods. model - free methods COMPARE model - based methods. sample efficiency EVALUATE-FOR model - free methods. model - free methods USED-FOR continuous - action DRL benchmarks. continuous - action DRL benchmarks EVALUATE-FOR model - based methods. sample efficiency EVALUATE-FOR model - based methods. modelbased algorithm USED-FOR MuJoCo benchmark. REDQ COMPARE model - based method. model - based method COMPARE REDQ. parameters USED-FOR model - based method. wall - clock run time EVALUATE-FOR REDQ. parameters USED-FOR REDQ. random subset of Q functions PART-OF ensemble. REDQ USED-FOR it. ensemble of Q functions CONJUNCTION in - target minimization. in - target minimization CONJUNCTION ensemble of Q functions. random subset of Q functions USED-FOR in - target minimization. REDQ CONJUNCTION model - free algorithms. model - free algorithms CONJUNCTION REDQ. model - free DRL algorithm USED-FOR continuous - action spaces. REDQ HYPONYM-OF model - free DRL algorithm. REDQ USED-FOR continuous - action spaces. UTD ratio 1 USED-FOR model - free DRL algorithm. UTD ratio 1 USED-FOR REDQ. Method is modelfree algorithm. OtherScientificTerm is UTD ratio. ,"This paper proposes a model-free algorithm for continuous-action reinforcement learning (DRL) in the MuJoCo setting. The proposed algorithm, called REDQ, is based on an ensemble of Q functions that is trained to minimize the in-target minimization and a random subset of the Q functions in the ensemble. Theoretical analysis is provided to show that the proposed algorithm is efficient in terms of the UTD ratio. Experiments are conducted to demonstrate the effectiveness of the proposed method. ",This paper proposes a new model-free DRL algorithm REDQ for continuous-action reinforcement learning. The main idea is to use an ensemble of Q functions to perform in-target minimization and ensemble-based ensemble minimization. The authors show that REDQ outperforms the state-of-the-art model-based DRL algorithms on MuJoCo benchmark. They also provide a theoretical analysis of the performance of REDQ.
252,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"deep learning USED-FOR classification or regression. probability distributions USED-FOR deep learning. distribution samples USED-FOR classification or regression. Lipschitz - bounded transformations of the input distribution FEATURE-OF robustness. tasks EVALUATE-FOR approach. DIDA USED-FOR meta - features. DIDA USED-FOR labelled ) dataset. meta - features USED-FOR labelled ) dataset. DIDA USED-FOR tasks. SVM CONJUNCTION logistic regression. logistic regression CONJUNCTION SVM. logistic regression CONJUNCTION linear SGD. linear SGD CONJUNCTION logistic regression. k - NN CONJUNCTION SVM. SVM CONJUNCTION k - NN. hyper - parameter configuration COMPARE configuration. configuration COMPARE hyper - parameter configuration. fixed algorithm USED-FOR hyper - parameter configuration. hyper - parameter configuration USED-FOR learning. dataset EVALUATE-FOR configuration. OpenML benchmarking suite USED-FOR dataset. SVM HYPONYM-OF fixed algorithm. DSS CONJUNCTION DATASET2VEC architectures. DATASET2VEC architectures CONJUNCTION DSS. tasks EVALUATE-FOR DIDA. tasks EVALUATE-FOR models. DIDA COMPARE models. models COMPARE DIDA. DIDA COMPARE DATASET2VEC architectures. DATASET2VEC architectures COMPARE DIDA. tasks EVALUATE-FOR DATASET2VEC architectures. DIDA COMPARE DSS. DSS COMPARE DIDA. DATASET2VEC architectures CONJUNCTION models. models CONJUNCTION DATASET2VEC architectures. hand - crafted meta - features USED-FOR models. OtherScientificTerm are permutation of the samples, and permutation of the features. Method are neural architectures, and universal approximation. Generic are architecture, and task. ","This paper proposes a meta-learning approach to improve the robustness of deep learning models on classification and regression tasks. The proposed method, called DIDA, is based on the meta-features, which are generated by permutation of the samples in the input distribution. The authors show that the proposed method is more robust than DSS and DATASET2VEC on classification tasks and logistic regression tasks on OpenML benchmarking suite. ","This paper proposes a meta-feature-based approach to improve the robustness of deep learning models to Lipschitz-bounded transformations of the input distribution. The proposed method, called DIDA, is based on the idea of meta-features, which is a set of features that can be added to the training data in order to improve robustness. The authors show that DIDA can be applied to a wide range of tasks, including SVM, logistic regression, linear SGD, and k-NN. They also show that the proposed method can improve the performance of DSS and DATASET2VEC architectures."
261,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,data clustering CONJUNCTION visualization. visualization CONJUNCTION data clustering. dimensionality reduction CONJUNCTION data clustering. data clustering CONJUNCTION dimensionality reduction. data representation and analysis CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION data representation and analysis. manifold learning CONJUNCTION data representation and analysis. data representation and analysis CONJUNCTION manifold learning. Graph learning USED-FOR data mining and machine learning tasks. visualization HYPONYM-OF data mining and machine learning tasks. manifold learning HYPONYM-OF data mining and machine learning tasks. data clustering HYPONYM-OF data mining and machine learning tasks. data representation and analysis HYPONYM-OF data mining and machine learning tasks. dimensionality reduction HYPONYM-OF data mining and machine learning tasks. approach USED-FOR ultra - sparse undirected graphs. graphLaplacian - like matrix PART-OF graphical Lasso. graphLaplacian - like matrix FEATURE-OF precision matrix. high - dimensional input data USED-FOR ultra - sparse undirected graphs. GRASPEL USED-FOR graphs. spectrally - critical edges PART-OF graph. nearly - linear time spectral methods USED-FOR ultrasparse yet spectrally - robust graphs. spectral clustering ( SC ) CONJUNCTION dimensionality reduction. dimensionality reduction CONJUNCTION spectral clustering ( SC ). graph learning approaches COMPARE GRASPEL. GRASPEL COMPARE graph learning approaches. manifold learning CONJUNCTION spectral clustering ( SC ). spectral clustering ( SC ) CONJUNCTION manifold learning. computing efficiency CONJUNCTION solution quality. solution quality CONJUNCTION computing efficiency. GRASPEL USED-FOR data mining and machine learning applications. solution quality EVALUATE-FOR data mining and machine learning applications. computing efficiency EVALUATE-FOR data mining and machine learning applications. solution quality EVALUATE-FOR GRASPEL. computing efficiency EVALUATE-FOR GRASPEL. dimensionality reduction HYPONYM-OF data mining and machine learning applications. manifold learning HYPONYM-OF data mining and machine learning applications. spectral clustering ( SC ) HYPONYM-OF data mining and machine learning applications. Method is spectral graph densification approach,"This paper proposes a new graph learning method for sparse undirected graphs. The proposed method is based on a graph Laplacian-like matrix, which is used to compute the precision matrix of the graph. The precision matrix is then used to train a graph Lasso. The method is shown to be computationally efficient in terms of time and memory.   ","This paper proposes a new graph learning method, GRASPEL, for ultra-sparse undirected graphs. The main idea is to use a graphLaplacian-like matrix as the precision matrix of the Lasso. The precision matrix is computed by computing the graph Laplacians of each edge of the graph. The paper also proposes a spectral graph densification method, which can be applied to the high-dimensional input data. Experiments show that the proposed method outperforms the state-of-the-art in terms of computing efficiency and solution quality."
270,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"images CONJUNCTION text descriptions. text descriptions CONJUNCTION images. deep reinforcement learning USED-FOR goal - conditioned policy. explicit embedding space USED-FOR nonparametric distance. abstract - level policy CONJUNCTION goal - conditioned policy. goal - conditioned policy CONJUNCTION abstract - level policy. unsupervised learning approach USED-FOR goal - conditioned policy. unsupervised learning approach USED-FOR abstract - level policy. intrinsic motivation ( GPIM ) FEATURE-OF goal - conditioned policy. goal - conditioned policy HYPONYM-OF unsupervised learning approach. discriminator USED-FOR abstract - level policy. latent variable USED-FOR abstract - level policy. discriminator USED-FOR intrinsic reward function. intrinsic reward function USED-FOR goal - conditioned policy. intrinsic reward function USED-FOR trajectory. discriminator USED-FOR goal - conditioned policy. discriminator USED-FOR trajectory. abstract - level policy USED-FOR trajectory. robotic tasks EVALUATE-FOR GPIM method. GPIM method COMPARE prior techniques. prior techniques COMPARE GPIM method. robotic tasks EVALUATE-FOR prior techniques. OtherScientificTerm are perceptually - specific goals, and hand - crafted rewards. Generic is policy. ","This paper proposes an unsupervised reinforcement learning method for goal-conditioned reinforcement learning. The method is based on the idea of intrinsic motivation (GPIM), which aims to learn an abstract-level policy and a goal-conditional policy in a shared embedding space. The main idea is to use a discriminator to predict the trajectory of the abstract policy, which is then used to train the goal conditioned policy. The proposed method is evaluated on a variety of robotic tasks and achieves state-of-the-art performance.   ","This paper proposes a method for unsupervised reinforcement learning a goal-conditioned policy (i.e., an abstract-level policy and a goal conditioned policy) with intrinsic motivation (GPIM). The method is based on the notion of intrinsic motivation, which is defined as a nonparametric distance between an abstract policy and the goal-conditional policy. The authors propose to use a discriminator to estimate the intrinsic reward function of the abstract policy, and then use the discriminator as a reward function for the goal conditioned and goal conditioned policies. The proposed method is evaluated on a variety of robotic tasks, and it is shown to outperform the state-of-the-art."
279,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"policy switches HYPONYM-OF low switching cost. low switching cost USED-FOR deep reinforcement learning problems. robotics CONJUNCTION dialogue agents. dialogue agents CONJUNCTION robotics. recommendation systems CONJUNCTION education. education CONJUNCTION recommendation systems. education CONJUNCTION robotics. robotics CONJUNCTION education. medical domains CONJUNCTION recommendation systems. recommendation systems CONJUNCTION medical domains. recommendation systems CONJUNCTION robotics. robotics CONJUNCTION recommendation systems. dialogue agents HYPONYM-OF applications. medical domains HYPONYM-OF applications. robotics HYPONYM-OF applications. education HYPONYM-OF applications. recommendation systems HYPONYM-OF applications. Q - network CONJUNCTION learning Q - network. learning Q - network CONJUNCTION Q - network. deep Q - networks USED-FOR policy switching criteria. feature distance USED-FOR adaptive approach. medical treatment environment CONJUNCTION Atari games. Atari games CONJUNCTION medical treatment environment. switching cost EVALUATE-FOR feature - switching criterion. sample efficiency EVALUATE-FOR feature - switching criterion. OtherScientificTerm are low - switching - cost constraint, and representation learning perspective. ","This paper proposes an adaptive feature-switching criterion for policy switching in reinforcement learning problems. The proposed method is based on a Q-network and a learning Q-networks. The authors show that the feature switching criterion can be used to improve the sample efficiency of policy learning. The method is evaluated on a variety of settings, including medical treatment environments and Atari games. ","This paper proposes an adaptive policy switching strategy to reduce the switching cost of deep reinforcement learning (DRL) problems. The main idea is to use a Q-network to learn a feature-switching criterion, which is based on the feature distance between two features. The authors show that the feature switching criterion can be used to improve the sample efficiency of DRL. They also show that their method can be applied to a variety of applications, such as medical treatment, robotics, and Atari games."
288,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,First - order stochastic methods USED-FOR large - scale non - convex optimization problems. First - order stochastic methods USED-FOR big - data applications. deep neural networks HYPONYM-OF big - data applications. homotopy methods CONJUNCTION SGD. SGD CONJUNCTION homotopy methods. diffusion CONJUNCTION mollifying networks. mollifying networks CONJUNCTION diffusion. Gaussian continuation USED-FOR optimization. diffusion HYPONYM-OF heuristics. mollifying networks HYPONYM-OF heuristics. optimization HYPONYM-OF heuristics. homotopy methods USED-FOR first - order stochastic algorithm. SGD USED-FOR first - order stochastic algorithm. scheme USED-FOR H - SGD. scheme USED-FOR homotopy parameter. fast and inexpensive iterations FEATURE-OF H - SGD. global linear rate of convergence EVALUATE-FOR H - SGD. H - SGD COMPARE SGD. SGD COMPARE H - SGD. Metric is slow global convergence rate. OtherScientificTerm is neighborhood of a minimizer. Generic is algorithm. ,"This paper studies the convergence of first-order stochastic methods for non-convex optimization problems with homotopy methods. The authors show that the global linear rate of convergence of the proposed H-SGD algorithm is faster than that of SGD. The main contribution of the paper is to introduce a scheme to compute the homotopic parameter of the SGD algorithm, which allows for fast and inexpensive iterations.",This paper proposes a new first-order stochastic method for solving non-convex optimization problems. The main idea is to use a homotopy-based SGD-like method to solve the problem. The authors prove that the global linear rate of convergence of H-SGD is faster than SGD. They also provide a theoretical analysis of the convergence rate of the method.
297,SP:195d090d9df0bda33103edcbbaf300e43f4562be,Bayesian meta - learning problem USED-FOR shape completion. encoder USED-FOR posterior distribution. encoder USED-FOR latent representation. posterior distribution FEATURE-OF latent representation. sparse cloud USED-FOR latent representation. encoder USED-FOR learning of object shapes. sparse point clouds USED-FOR learning of object shapes. meta - learning algorithm USED-FOR shape completion of newly - encountered objects. object - specific properties CONJUNCTION object - agnostic properties. object - agnostic properties CONJUNCTION object - specific properties. sparse observations USED-FOR shape completion of newly - encountered objects. ICL - NUIM benchmarks EVALUATE-FOR method. Method is shape representations. ,"This paper proposes a Bayesian meta-learning method for shape completion in the presence of sparse observations. The proposed method is based on the idea of learning a latent representation of the object shape using a sparse point cloud, which is then used to train an encoder to predict the posterior distribution of the observations. This latent distribution is used to learn the shape representations of newly encountered objects. The method is evaluated on the ICL-NUIM and ICLR datasets. ","This paper proposes a meta-learning method for shape completion in the context of Bayesian meta learning. The paper proposes to use a sparse point cloud to learn the latent representations of object shapes, and then use the learned representations to perform shape completion of newly encountered objects. The proposed method is evaluated on the ICL-NUIM benchmark, and it is shown to perform better than the state-of-the-art."
306,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. adversarial examples COMPARE natural examples. natural examples COMPARE adversarial examples. activation magnitudes FEATURE-OF adversarial examples. channels COMPARE natural examples. natural examples COMPARE channels. channel - wise activation perspective FEATURE-OF adversarial examples. defense adversarial training USED-FOR activation magnitudes. CAS USED-FOR model. model USED-FOR adversarial activation. robustness EVALUATE-FOR defense methods. OtherScientificTerm are uniform activation, redundant activation, and adversarial perturbations. Method is intermediate layer activation of DNNs. ","This paper studies the activation magnitudes of adversarial examples in deep neural networks (DNNs) in terms of channel-wise adversarial perturbations. The authors first show that the activation of DNNs with uniform activation is redundant, and then propose a defense adversarial training (CAS) method to improve the robustness of the model against adversarial attacks. The main contribution of the paper is a theoretical analysis of the effect of the activation magnitude on the adversarial robustness, and the authors show that it is a function of the number of channels in the network, and that the higher the channel, the more robust the model is. ","This paper studies the problem of defense against adversarial examples in terms of the activation magnitudes of DNNs. The authors show that the activation magnitude of the adversarial example depends on the channel-wise activation of the DNN. They show that if the activation of a DNN is uniform across channels, then it is more robust to adversarial perturbations than if it is redundant across channels. They propose a new defense method, called CAS, which is based on the idea that adversarial activation is not uniform across all channels, but rather across the intermediate layers of the network. They also show that CAS can be used to improve the robustness of the model.   "
315,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"random initialization USED-FOR gradient descent. generalization EVALUATE-FOR Neural networks. gradient descent USED-FOR Neural networks. random initialization USED-FOR Neural networks. Neural Tangent Kernel ( NTK ) USED-FOR implicit regularization effect. gradient flow / descent USED-FOR infinitely wide neural networks. implicit regularization effect FEATURE-OF gradient flow / descent. random initialization USED-FOR gradient flow / descent. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance CONJUNCTION initialization. initialization CONJUNCTION generalization performance. optimization USED-FOR finite width networks. initialization USED-FOR finite width networks. optimization CONJUNCTION overparametrization. overparametrization CONJUNCTION optimization. initialization CONJUNCTION optimization. optimization CONJUNCTION initialization. generalization performance EVALUATE-FOR overparametrization. hidden layer width CONJUNCTION scaled ) random initialization. scaled ) random initialization CONJUNCTION hidden layer width. low - dimensional manifold FEATURE-OF network parameters. min - norm solution USED-FOR linear case. O(h−1/2 ) upper - bound FEATURE-OF operator norm distance. network CONJUNCTION min - norm solution. min - norm solution CONJUNCTION network. operator norm distance FEATURE-OF network. operator norm distance EVALUATE-FOR min - norm solution. OtherScientificTerm are regularization, and imbalance of the network weights. Method are non - asymptotic analysis, overparametrized single - hidden layer linear networks, and gradient flow. Metric are squared loss, and convergence rate. Generic is manifold. ",This paper studies the implicit regularization effect of gradient flow/descent in infinite-width neural networks with random initialization. The authors show that the gradient flow of gradient descent in infinite width neural networks can be explained by the Neural Tangent Kernel (NTK) and show that this is the case for overparametrized single-hidden layer linear networks.   Theoretical results on the convergence rate of the min-norm solution of the network are provided. The convergence rate is shown to be O(1/2) in the linear case and O(2/3) for the non-linear case.,"This paper studies the implicit regularization effect of gradient flow in neural networks. In particular, the authors study the generalization performance of neural networks with overparametrized single-hidden layer linear networks. The authors provide a non-asymptotic analysis of the convergence rate of the gradient flow on a low-dimensional manifold of the parameters of the network parameters. They also provide an upper-bound on the operator norm distance between a network and a min-norm solution of the min-Norm solution of a linear network."
324,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,Deep networks COMPARE shallow ones. shallow ones COMPARE Deep networks. approximation EVALUATE-FOR shallow ones. tractable algorithms USED-FOR deep models. deep networks COMPARE shallow ones. shallow ones COMPARE deep networks. approximation USED-FOR kernels. tractable ) kernel methods USED-FOR over - parameterized regime. gradient descent USED-FOR deep networks. architecture USED-FOR kernel. eigenvalue decay FEATURE-OF integral operator. kernels COMPARE shallow ” two - layer counterpart. shallow ” two - layer counterpart COMPARE kernels. kernels USED-FOR ReLU activations. deep fully - connected networks USED-FOR kernels. approximation properties EVALUATE-FOR kernels. kernel framework USED-FOR deep architectures. differentiability properties FEATURE-OF kernel function. sphere FEATURE-OF kernels. kernel function USED-FOR eigenvalue decays. differentiability properties USED-FOR eigenvalue decays. ,This paper studies the approximation properties of kernels in deep neural networks. The authors show that the eigenvalue decay of the integral operator of a deep network with ReLU activations can be approximated by a kernel function that is differentiable on the sphere. They show that this is the case for deep fully-connected networks. They also show that kernels can be used to approximate ReLU activation functions in deep networks.  ,This paper proposes a tractable kernel method for deep networks that is tractable in the over-parameterized regime. The main contribution of the paper is to study the differentiability properties of the kernel function. The authors show that the eigenvalue decay of the integral operator can be approximated by a kernel function that is differentiable in the sphere of the sphere. They also show that this kernel function can be used to approximate the ReLU activations of deep fully-connected networks.
333,SP:3dd495394b880cf2fa055ee3fe218477625d2605,amplified value estimates CONJUNCTION suboptimal policies. suboptimal policies CONJUNCTION amplified value estimates. overestimation problem PART-OF deep value learning. underestimation bias CONJUNCTION instability. instability CONJUNCTION underestimation bias. algorithm USED-FOR overestimation. overestimation issues FEATURE-OF continuous control. algorithm USED-FOR policy improvement. deep reinforcement learning USED-FOR continuous control. deep reinforcement learning USED-FOR overestimation issues. combined value of weighted critics USED-FOR policy. weight factor USED-FOR independent critics. method USED-FOR policy improvement. algorithms USED-FOR continuous control. algorithms COMPARE algorithms. algorithms COMPARE algorithms. classical control tasks EVALUATE-FOR method. OtherScientificTerm is function approximation errors. ,This paper studies the overestimation problem in deep reinforcement learning and proposes a method to address it in continuous control problems. The authors propose to use the combined value of weighted critics to improve the performance of the policy. The proposed method is based on the idea of weighting the independent critics in the loss function. The method is evaluated on a variety of continuous control tasks and achieves state-of-the-art performance. ,"This paper proposes a method for improving the performance of deep value learning in continuous control. The authors propose a method to improve the performance in the presence of overestimation bias and instability. The method is based on a weighted sum of the combined value of weighted critics and the independent critics. The proposed method is evaluated on a variety of continuous control tasks, and it is shown that it outperforms the state of the art. "
342,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"inverse reinforcement learning ( IRL ) problem USED-FOR reward functions. expert demonstrations USED-FOR reward functions. policy USED-FOR reward functions. expert demonstrations USED-FOR policies. ill - posed inverse problem HYPONYM-OF IRL problem. IRL problem USED-FOR well - posed expectation optimization problem. solution USED-FOR SIRL problem. solution USED-FOR learning task. solutions USED-FOR IRL problem. solution USED-FOR solutions. formulation USED-FOR IRL problem. objectworld EVALUATE-FOR approach. OtherScientificTerm are probability distribution over reward functions, and probability distribution. ","This paper studies the inverse reinforcement learning in the presence of a distribution over reward functions. In this setting, the goal is to learn a policy that maximizes the expected reward over the probability distribution over the reward functions, given a set of expert demonstrations. The authors propose to use the ill-posed inverse problem (ILR) to solve the well-posed expectation optimization (SIRL) problem. They show that the IRL problem is equivalent to the SIRL problem in the sense that it can be solved by a well-known solution of the well known inverse problem.   The authors show that under certain assumptions on the distribution of reward functions and the distribution over expert demonstrations, the proposed IRL method can be used to learn policies that maximize the expected rewards over the expert demonstrations in the environment. ","This paper proposes a new inverse reinforcement learning (IRL) algorithm for the ill-posed inverse problem (ILR) problem, which is a well-posed expectation optimization problem for the well-known SIRL problem. The proposed algorithm is based on the well known SIRl problem, and the authors show that it can be used to solve the inverse problem for well-policied expectation optimization problems. The authors also show that the proposed algorithm can also be used for the problem of learning the distribution of reward functions in the environment."
351,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"Self - training algorithms USED-FOR model. model USED-FOR pseudolabels. model USED-FOR pseudolabels. neural networks USED-FOR unlabeled data. self - training USED-FOR linear models. unsupervised domain adaptation CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION unsupervised domain adaptation. self - training USED-FOR semi - supervised learning. deep networks USED-FOR semi - supervised learning. deep networks USED-FOR unsupervised domain adaptation. semi - supervised learning CONJUNCTION unsupervised domain adaptation. unsupervised domain adaptation CONJUNCTION semi - supervised learning. self - training USED-FOR unsupervised domain adaptation. deep networks USED-FOR self - training. self - training CONJUNCTION input - consistency regularization. input - consistency regularization CONJUNCTION self - training. accuracy EVALUATE-FOR minimizers of population objectives. self - training USED-FOR minimizers of population objectives. input - consistency regularization USED-FOR minimizers of population objectives. margin CONJUNCTION Lipschitzness. Lipschitzness CONJUNCTION margin. sample complexity guarantees FEATURE-OF neural nets. margin FEATURE-OF neural nets. Lipschitzness FEATURE-OF neural nets. input consistency regularization USED-FOR self - training algorithms. OtherScientificTerm are neighborhoods, ground - truth labels, and generalization bounds. Generic is assumptions. ",This paper studies the problem of self-training for semi-supervised learning and unsupervised domain adaptation in the presence of pseudolabels. The authors show that self-trained linear models with input consistency regularization and input-consistency regularization are able to achieve better generalization bounds than those with ground-truth labels. They also provide sample complexity guarantees for self-train and self-adaptation.  ,"This paper studies the generalization bounds of self-training for linear models with pseudolabels. The authors show that under certain assumptions, the sample complexity of neural networks is bounded by the margin and the Lipschitzness of the network. They also show that self-trained neural networks can be used for unsupervised domain adaptation and semi-supervised learning.  The authors also provide a generalization bound for the minimizers of population objectives."
360,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"hierarchical models USED-FOR video prediction. stochastic recurrent estimator USED-FOR long - term prediction. car driving CONJUNCTION human dancing. human dancing CONJUNCTION car driving. it USED-FOR complicated scene structures. video prediction COMPARE approaches. approaches COMPARE video prediction. Generic is method. OtherScientificTerm are semantic structures, structures, and discrete semantic structure space. Method is videoto - video translation. ","This paper proposes a hierarchical model for video prediction based on video-to-video translation. The main idea is to model the video as a set of discrete semantic structures, where each structure is represented as a discrete semantic space, and the goal is to predict the next video frame given the current frame. The model is trained using a stochastic recurrent estimator, which is used to estimate the long-term prediction error. The proposed model is evaluated on a variety of video prediction tasks, including driving, human dancing, and car driving.  ","This paper proposes a new video prediction method for video translation. The proposed method is based on a stochastic recurrent estimator, which is used to estimate the long-term prediction of a video. The method is applied to video prediction in the context of videoto-video translation, where the goal is to translate a video into a discrete semantic structure space. The authors show that the proposed method can be applied to a variety of video prediction tasks, such as car driving, human dancing, and human dancing."
369,SP:e50b1931800daa7de577efd3edca523771227b3f,"undirected graph CONJUNCTION directed graph. directed graph CONJUNCTION undirected graph. Iterated Graph Neural Network System ( IGNNS ) HYPONYM-OF Graph Neural Networks ( GNNs ). Iterated Function System ( IFS ) HYPONYM-OF fractal geometry. Iterated Function System ( IFS ) PART-OF IGNNS. adjoint probability vector USED-FOR IFS layer. affine transformations USED-FOR IGNNS. geometric properties FEATURE-OF IGNNS. dynamical system USED-FOR IGNNS. dynamical system USED-FOR geometric properties. Frobenius norm FEATURE-OF constant matrix. IGNNS USED-FOR IFS. Hausdorff distance FEATURE-OF fractal set of IFS. Cora CONJUNCTION PubMed. PubMed CONJUNCTION Cora. citeser CONJUNCTION Cora. Cora CONJUNCTION citeser. citation network datasets USED-FOR semi - supervised node classification. PubMed HYPONYM-OF citation network datasets. citeser HYPONYM-OF citation network datasets. Cora HYPONYM-OF citation network datasets. OtherScientificTerm are graph nodes, latent space, and node features. Method are high - level representation of graph nodes, and fractal representation of graph nodes. Generic is method. ","This paper proposes a method to learn a high-level representation of node features in a directed and undirected graph. The proposed method is based on the Iterated Function System (IFS), which is an extension of the iterated function system in graph neural networks (GNNs). The main contribution of this paper is to extend the IFS to the directed and directed graphs, and to use it as an adjoint probability vector in the GNN layer. The authors show that the proposed method can be used for semi-supervised node classification on the Cora and PubMed datasets.  ","This paper proposes a new graph neural network (GNN) architecture for graph node classification. The main idea is to use the iterated function system (IFS) as a high-level representation of the graph nodes. The IFS is composed of two parts: (1) an adjoint probability vector for each node, and (2) a set of affine transformations. The authors show that the IFS has Frobenius norm and Hausdorff distance between the nodes. They also show that their method can be used for semi-supervised node classification on a variety of datasets. "
378,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"modeling complex relations CONJUNCTION modeling isomorphic graphs. modeling isomorphic graphs CONJUNCTION modeling complex relations. GG - GAN USED-FOR graphs. GG - GAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GG - GAN. GG - GAN USED-FOR distribution statistics. Task is graph generation. OtherScientificTerm are similarity function, complex relations, isomorphic graphs, latent distribution, and problem - specific knowledge. Method are geometric interpretation, and Wasserstein GAN. ","This paper proposes a generalization of Wasserstein GAN (WGAN) to the problem of graph generation, where the goal is to generate graphs that are isomorphic to a set of isomorphic graphs. The proposed method is based on the geometric interpretation of WGAN, which allows the model to model complex relations and isomorphism. The authors show that the proposed method achieves state-of-the-art performance on a variety of tasks.   ","This paper proposes a Wasserstein GAN (WGAN) method for graph generation. The main idea is to use the WGAN to model complex relations and isomorphic graphs. The authors show that WGAN can be used to generate graphs that are isomorphic to a set of isomorphous graphs. They also show that the proposed method can be applied to the problem of graph generation, and that it can be combined with other GAN methods."
387,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"activation values FEATURE-OF network. Minimum Description Length principle USED-FOR problem. rules CONJUNCTION super - charge prototyping. super - charge prototyping CONJUNCTION rules. Generic are method, and they. Method are neural network, and unsupervised EXPLAINN algorithm. OtherScientificTerm are noise - robust rules, class - specific traits, and convolutional layers. Material is activation data. ","This paper studies the problem of learning a class-specific set of activation values for a neural network. The goal is to learn a set of rules that are robust to noise. The authors propose a new algorithm called EXPLAINN, which is based on the Minimum Description Length principle. The main idea is to use super-charge prototyping to learn the activation values of a class of convolutional layers. The proposed method is shown to be computationally efficient.",This paper proposes an unsupervised explanation method for the problem of learning a class-specific neural network from activation data. The main contribution of the paper is to propose a new method for learning a neural network that is robust to noise-robust rules and super-charge prototyping. The proposed method is based on the Minimum Description Length principle. The authors show that the proposed method can be used to learn a network that can be described in terms of the minimum description length of the training data. 
396,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"worst - case guarantees FEATURE-OF fixed - dataset policy optimization algorithms. algorithms USED-FOR regime. principle USED-FOR algorithms. tabular gridworld CONJUNCTION deep learning. deep learning CONJUNCTION tabular gridworld. MinAtar environments EVALUATE-FOR deep learning. Method are naı̈ve approaches, pessimism principle, and pessimistic algorithms. OtherScientificTerm are erroneous value overestimation, and policy. ",This paper studies the worst-case analysis of policy optimization algorithms in fixed-dataset policy optimization. The authors show that pessimistic algorithms can be viewed as a variant of the pessimism principle. They show that pessimism is necessary and sufficient for pessimistic algorithms to converge to the optimal policy in the worst case. They also show that the pessimistic algorithm can be seen as a special case of the optimistic algorithm.   ,"This paper studies the worst-case guarantees of policy optimization algorithms for fixed-dataset policy optimization. The main contribution of the paper is a theoretical analysis of the pessimism principle, which is a generalization of the pessimistic approach to the problem of policy optimisation. The authors show that the worst case guarantees of pessimistic algorithms can be obtained for a wide range of settings, including tabular gridworlds, deep learning, and MinAtar environments. They also show that pessimistic algorithms are more likely to be wrong than optimistic ones."
405,SP:363661edd15a06a800b51abc1541a3191311ee0e,"Neural ordinary differential equations ( Neural ODEs ) HYPONYM-OF deeplearning models. continuous depth FEATURE-OF deeplearning models. naive method CONJUNCTION adaptive checkpoint adjoint method ( ACA ). adaptive checkpoint adjoint method ( ACA ) CONJUNCTION naive method. continuous case FEATURE-OF numerical estimation of the gradient. accuracy EVALUATE-FOR gradient estimation. accuracy EVALUATE-FOR reverse - time trajectory. constant memory cost FEATURE-OF ALF Integrator ( MALI ). heavy memory burden CONJUNCTION inaccuracy. inaccuracy CONJUNCTION heavy memory burden. MALI COMPARE ResNet. ResNet COMPARE MALI. MALI COMPARE adjoint method. adjoint method COMPARE MALI. MALI USED-FOR Neural ODE. MALI COMPARE methods. methods COMPARE MALI. image recognition tasks EVALUATE-FOR MALI. tasks EVALUATE-FOR MALI. ImageNet USED-FOR Neural ODE. image recognition tasks HYPONYM-OF tasks. tasks EVALUATE-FOR MALI. MALI USED-FOR continuous generative models. image recognition tasks EVALUATE-FOR MALI. adjoint method USED-FOR time series modeling. MALI USED-FOR time series modeling. Metric is memory cost. OtherScientificTerm are integration time, and solver steps. Method are asynchronous leapfrog ( ALF ) solver, and pypi package. ","This paper proposes an ALF Integrator (ALF) solver for continuous Neural ODEs. The ALF solver is based on the asynchronous leapfrog solver, which can be used to reduce the integration time. The main contribution of this paper is to introduce a continuous ALF integration time reduction strategy. The proposed ALF integrator is evaluated on image classification and time series modeling tasks. ","This paper proposes an ALF Integrator (ALF) solver for Neural ODEs. The main contribution of the paper is a new ALF solver called MALI, which is based on the adaptive checkpoint adjoint method (ACA). The main contributions of this paper are two-fold. First, the authors propose a new solver based on pypi package, which can be used to reduce the memory cost of ALF. Second, they show that the proposed ALF can be applied to a variety of tasks, such as time series modeling and image recognition. "
414,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,methodology EVALUATE-FOR complex scene conditional generation models. model USED-FOR seen conditionings. seen object combinations FEATURE-OF unseen conditionings. unseen object combinations USED-FOR unseen conditionings. methods USED-FOR recognizable scenes. compositionality USED-FOR unseen conditionings. compositionality USED-FOR methods. seen conditionings USED-FOR methods. seen object combinations USED-FOR unseen conditionings. unseen object combinations FEATURE-OF conditionings. image quality degradation EVALUATE-FOR methods. semantically aware losses USED-FOR generation process. robustness EVALUATE-FOR unseen conditionings. robustness FEATURE-OF unseen conditionings. instance - wise spatial conditioning normalizations USED-FOR compositionality. scene - graph perceptual similarity HYPONYM-OF semantically aware losses. Generic is models. Method is pipeline components. ," image quality degradation is studied in the context of scene conditional generation models. The paper proposes to use unseen object combinations to improve the quality of the generated images. The proposed method is based on the observation that unseen objects are more likely to be seen in the original image than seen objects in the generated image. To improve the compositionality of the unseen objects, the paper proposes a spatial conditioning normalization to encourage compositionality between seen and unseen objects.  ","This paper proposes a new method to improve the robustness of scene conditional generation models. The proposed method is based on the idea of compositionality, which is the ability of the model to capture the compositionality of unseen conditionings. The authors propose a new loss called scene-graph perceptual similarity (SGP), which is a semantically aware loss for the generation process. Experiments are conducted to demonstrate the effectiveness of the proposed method.   "
423,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"expressive power CONJUNCTION learning. learning CONJUNCTION expressive power. multi - hop operators FEATURE-OF graph. multi - hop operators USED-FOR node features. GA - MLPs USED-FOR non - isomorphic graphs. WeisfeilerLehman ( WL ) test CONJUNCTION GNNs. GNNs CONJUNCTION WeisfeilerLehman ( WL ) test. operators USED-FOR GA - MLPs. GA - MLPs CONJUNCTION GNNs. GNNs CONJUNCTION GA - MLPs. expressive power EVALUATE-FOR GNNs. expressive power EVALUATE-FOR GA - MLPs. node - level functions USED-FOR them. GNNs COMPARE GA - MLPs. GA - MLPs COMPARE GNNs. GA - MLPs USED-FOR attributed walks. community detection EVALUATE-FOR GA - MLPs. GNNs USED-FOR learning. operator family USED-FOR GA - MLPs. Generic is alternative. Method is learnable node - wise functions. OtherScientificTerm are node - wise functions, and rooted graphs. Task is graph isomorphism testing. ","This paper proposes a new method for non-isomorphism testing on non-Isomorphic graphs. The proposed method is based on the use of multi-hop operators in the node features of the graph. The main idea is to use node-level functions to learn node-wise functions, which are then used to test the expressiveness of the nodes. The method is evaluated on community detection and attributed walks.   ",This paper proposes a new type of graph isomorphism testing method for non-isomorphic graphs. The proposed method is based on the Weisfeiler-lehman (WL) test. The main idea of the method is to learn a set of learnable node-level functions that can be used to test whether a graph is isomorphic or not. The authors show that the proposed method outperforms the state-of-the-art GNNs in terms of expressive power and community detection. 
432,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"model complexity EVALUATE-FOR Reinforcement Learning ( RL ) agents. real - world applications EVALUATE-FOR Reinforcement Learning ( RL ) agents. robotics HYPONYM-OF real - world applications. acting USED-FOR distributed RL settings. unaccelerated hardware USED-FOR acting. CPUs HYPONYM-OF unaccelerated hardware. model complexity EVALUATE-FOR supervised learning. distillation USED-FOR learning progress. large capacity learner model COMPARE small capacity actor model. small capacity actor model COMPARE large capacity learner model. system USED-FOR acting. transformer models COMPARE LSTMs. LSTMs COMPARE transformer models. procedure USED-FOR partially - observable environments. computational complexity EVALUATE-FOR transformer models. transformer models CONJUNCTION LSTMs. LSTMs CONJUNCTION transformer models. fast inference CONJUNCTION total training time. total training time CONJUNCTION fast inference. Actor - Learner Distillation USED-FOR transformer learner model. total training time EVALUATE-FOR LSTM actor model. fast inference EVALUATE-FOR LSTM actor model. Actor - Learner Distillation USED-FOR memory environments. OtherScientificTerm are compute, model size, intractable experiment run times, actor - latency ” constrained settings, and model capacity. ","This paper proposes an actor-learner distillation method for distributed reinforcement learning. The main idea is to train a transformer-based actor model that distills the knowledge from a large capacity learner model to a small capacity actor model. The actor model is trained in an unaccelerated manner, where the actor is trained on a small number of experiments, and the distillation is performed on the large number of observations. The method is shown to improve the performance of the actor model compared to the LSTM model.  ","This paper proposes an actor-learner distillation method to speed up the training of transformer-based RL agents. The main idea of the method is to distill the learner learner model to the actor learner, which is then used to accelerate the learning progress of the actor. The method is evaluated on a variety of environments, including partially-observable environments and unaccelerated hardware. The results show that the method can significantly reduce the total training time and inference time of the transformer learner. "
441,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,"benchmarks USED-FOR problem. Meta - Dataset HYPONYM-OF benchmarks. universal features USED-FOR few - shot classification. Meta - Dataset EVALUATE-FOR URT. model USED-FOR cross - domain generalization. Task are Few - shot classification, multi - domain few - shot image classification, and multi - domain setting. Material is diverse data sources. Method are feature representations, Universal Representation Transformer ( URT ) layer, and domain - specific representations. Generic is it. OtherScientificTerm is attention score heatmaps. ","This paper studies the few-shot image classification problem in the multi-domain setting. The authors propose a novel Universal Representation Transformer (URT) layer to learn universal features from diverse data sources. The proposed URT layer is based on the attention score heatmaps, which can be used to learn domain-specific representations. Experiments on Meta-Dataset and CIFAR-10 show the effectiveness of the proposed method. ","This paper proposes a universal representation transformer (URT) for few-shot image classification in multi-domain setting. The URT model is based on attention score heatmaps, which is used to learn universal features from diverse data sources. The model is trained on the Meta-Dataset dataset, where it is shown to perform better than the state-of-the-art in terms of cross-domain generalization.   "
450,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"non - stationary stream of unlabeled data USED-FOR salient representations. representations USED-FOR classification tasks. Self - Taught Associative Memory ( STAM ) HYPONYM-OF online clustering module. architecture USED-FOR UPL problem. online clustering module PART-OF architecture. online clustering CONJUNCTION novelty detection. novelty detection CONJUNCTION online clustering. novelty detection CONJUNCTION forgetting outliers. forgetting outliers CONJUNCTION novelty detection. novelty detection USED-FOR Layered hierarchies of STAM modules. online clustering USED-FOR Layered hierarchies of STAM modules. UPL context EVALUATE-FOR latter. Task is Unsupervised Progressive Learning ( UPL ) problem. Material is limited labeled data. Method are prototypical representations, and STAM architecture. ", is an unsupervised progressive learning (UPL) framework that learns representations from a stream of unlabeled data. The authors propose a self-taught associative memory (STAM) architecture that uses online clustering and novelty detection to learn representations. The proposed STAM architecture is shown to be effective in the UPL setting.,"This paper proposes a novel self-taught association memory (STAM) architecture for the Unsupervised Progressive Learning (UPL) problem, where the goal is to extract salient representations from a non-stationary stream of unlabeled data. The proposed STAM architecture is based on the Self-Taught Associative Memory (SAM) module, which is an online clustering module that is used to learn representations for the task of unsupervised progressive learning. The main contribution of the paper is to propose a novel STAM module that can be used for the UPL problem. The authors show that the proposed architecture outperforms the state-of-the-art in terms of performance on a variety of tasks. "
459,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"communication topology CONJUNCTION data partitioning. data partitioning CONJUNCTION communication topology. network size CONJUNCTION communication topology. communication topology CONJUNCTION network size. models COMPARE models. models COMPARE models. data partitioning HYPONYM-OF parameters. communication topology HYPONYM-OF parameters. network size HYPONYM-OF parameters. network topology CONJUNCTION learning rate. learning rate CONJUNCTION network topology. generalization gap FEATURE-OF decentralized deep learning. Method are deep learning models, on - device learning over networks, decentralized training, centralized training, communication efficient training schemes, and training schemes. OtherScientificTerm are large compute clusters, and consensus distance. ",This paper studies the generalization gap between decentralized and centralized deep learning models in the presence of large compute clusters. The authors show that decentralized training is more efficient than centralized training in terms of the number of clients and the communication costs. They show that the communication cost is proportional to the communication topology and the size of the network. They also show that there is a trade-off between communication cost and learning rate between the two. ,"This paper studies the generalization gap between decentralized and centralized deep learning models. The authors consider the problem of on-device learning over networks (ODL), where the goal is to reduce the communication distance between the compute clusters. They show that decentralized OODL models are more general than centralized ones in terms of generalization performance. They also show that the learning rate and communication topology are important factors in generalization.  "
468,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,activity recognition CONJUNCTION natural language processing. natural language processing CONJUNCTION activity recognition. sequence alignment approaches CONJUNCTION representation learning. representation learning CONJUNCTION sequence alignment approaches. Sequence metric learning USED-FOR applications. sequential multi - variate data USED-FOR applications. natural language processing HYPONYM-OF applications. activity recognition HYPONYM-OF sequential multi - variate data. natural language processing HYPONYM-OF sequential multi - variate data. sequence alignment approaches USED-FOR applications. activity recognition HYPONYM-OF applications. representation learning USED-FOR applications. synchronized trajectories CONJUNCTION distance between similar sequences. distance between similar sequences CONJUNCTION synchronized trajectories. dynamical systems USED-FOR synchronized trajectories. siamese recurrent neural network USED-FOR distance between similar sequences. sub - networks CONJUNCTION dynamical systems. dynamical systems CONJUNCTION sub - networks. dynamical systems PART-OF siamese recurrent network. sub - networks PART-OF siamese recurrent network. gate PART-OF classical Gated Recurrent Unit architecture. neural network model USED-FOR coupling. gate USED-FOR neural network model. gate USED-FOR coupling. model USED-FOR synchronization of unaligned multi - variate sequences. model USED-FOR similarity metric. similarity metric CONJUNCTION synchronization of unaligned multi - variate sequences. synchronization of unaligned multi - variate sequences CONJUNCTION similarity metric. coupling USED-FOR siamese Gated Recurrent Unit architecture. activity recognition dataset EVALUATE-FOR siamese Gated Recurrent Unit architecture. Method is dynamical system theory. ,"This paper proposes a siamese recurrent neural network to learn the distance between similar sequences. The proposed method is inspired by dynamical system theory and is able to model the synchronization of unaligned multi-variate sequences.    The proposed model is based on a Gated Recurrent Unit architecture, where a gate is used to control the coupling between the model and the dynamical systems. The model is trained using a combination of a sequence alignment loss and a similarity loss. The method is evaluated on the activity recognition dataset. ",This paper proposes a new model for sequence alignment based on dynamical system theory. The model is based on a siamese recurrent neural network that learns the distance between similar sequences. The authors propose to use a classical Gated Recurrent Unit architecture to model the coupling between two dynamical systems. The proposed model is evaluated on the activity recognition dataset and shows promising results.
477,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"codistillation PART-OF distributed training setup. models COMPARE models. models COMPARE models. codistillation USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. synchronization mechanism USED-FOR models. synchronous data - parallel methods USED-FOR models. codistillation USED-FOR models. batch sizes CONJUNCTION learning rate schedules. learning rate schedules CONJUNCTION batch sizes. it USED-FOR distributed computing environment. Method is Codistillation. OtherScientificTerm are auxiliary loss, model replicas, large batch sizes, and moderate batch sizes. Metric is accuracy. ",This paper proposes to use codistillation in distributed training to improve the performance of models trained with large batch sizes. The idea is to use a synchronization mechanism to synchronize the data-parallel training of different models in a distributed training setup. The authors show that this synchronization mechanism can be used to reduce the computational cost of large batch size and learning rate schedules. They also show that the proposed method can be applied in distributed computing environments. ,This paper proposes a new method for synchronous data-parallel training with codistillation (codistillation) to improve the performance of distributed models in distributed training. The authors show that the proposed method can improve the accuracy of models with large batch sizes and moderate batch sizes. They also show that it can be used in a distributed computing environment with learning rate schedules.
486,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"capacity CONJUNCTION complexity. complexity CONJUNCTION capacity. stochastic gradient descent ( SGD ) USED-FOR deep learning. SGD USED-FOR local minimum. SGD iterates USED-FOR heavy - tailed stationary distribution. algorithm parameters CONJUNCTION b. b CONJUNCTION algorithm parameters. independent and identically distributed Gaussian data USED-FOR linear regression problem. dimension CONJUNCTION curvature. curvature CONJUNCTION dimension. algorithm parameters CONJUNCTION dimension. dimension CONJUNCTION algorithm parameters. algorithm parameters USED-FOR tails. SGD USED-FOR deep learning. synthetic data CONJUNCTION fully connected neural networks. fully connected neural networks CONJUNCTION synthetic data. OtherScientificTerm are eigenvalues of the Hessian, batch size b, stochastic gradient noise, tail - index ’, network weights, and Hessian. Metric is tail - index. Task are generalization, and quadratic optimization. ","This paper studies the generalization properties of SGD with heavy-tailed Gaussian data in linear regression with stochastic gradient noise. In particular, the authors show that the tail-index of the Hessian is a function of the batch size b and the number of training samples. They show that SGD iterates converge to the local minimum of the heavy-tailed stationary distribution, which is a stationary distribution with heavy eigenvalues. The authors also show that this is the case even when the data is independent and identically distributed.   ","This paper studies the generalization properties of stochastic gradient descent (SGD) in the context of linear regression. The authors show that SGD can be used to learn a heavy-tailed stationary distribution, where the eigenvalues of the Hessian are eigenvectors of a stationary Hessian. They also show that the number of iterations of SGD iterates is bounded by the size of the batch size b. They show that this can be reduced to a quadratic optimization problem if the size b is large enough.   "
495,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,models USED-FOR community detection. spectral manipulations USED-FOR models. bandpass filtering USED-FOR GCN. high - frequencies USED-FOR community detection. images HYPONYM-OF Euclidean graph. spectral components USED-FOR supervised community detection task. graph structure USED-FOR cascade of filtering. low - frequency domain FEATURE-OF spectral components. low frequencies USED-FOR classifiers. Task is nodes classification. Method is GCNs. ," and community detection.  The paper proposes to use bandpass filtering to improve the performance of community detection models. The proposed method is based on the observation that the high-frequency components of the image are responsible for high-fidelity detection, while the low-frequency component is responsible for low-quality detection. The authors show that the proposed method can improve the accuracy of the community detection model.   ","This paper proposes a new method for supervised community detection (GCN) based on the Euclidean graph. The proposed method is based on bandpass filtering, where the high-frequency bandpass filter is used to detect high-frequencies and the low-frequency filter is applied to detect low frequencies. The authors show that the proposed method outperforms the state-of-the-art GCNs in terms of accuracy. "
504,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"graph structure USED-FOR Graph neural networks ( GNNs ). structure COMPARE real - world applications. real - world applications COMPARE structure. structure CONJUNCTION GNN parameters. GNN parameters CONJUNCTION structure. taskspecific supervision USED-FOR structure. taskspecific supervision USED-FOR GNN parameters. method USED-FOR supervision. method USED-FOR graph structure. self - supervision USED-FOR method. models USED-FOR task - specific graph structure. SLAPS COMPARE models. models COMPARE SLAPS. OtherScientificTerm are task - specific latent structure, graph structures, and Self - supervision. Method is GNN. ","This paper proposes a self-supervised learning method for graph neural networks (GNNs) that learns a task-specific latent structure for each node in the graph. The proposed method, called SLAPS, is based on the observation that GNNs learn the structure of the graph in the latent space as a function of the parameters of the GNN. The authors show that the learned structure can be used to improve the performance of a GNN model.    The main contribution of the paper is a method to learn a task specific graph structure that is independent of the underlying graph structure. The method is evaluated on synthetic and real-world datasets. ","This paper proposes a self-supervised graph neural network (GNN) that learns to predict task-specific graph structure. The proposed method, called SLAPS, is based on the idea of self-substitution. The authors show that SLAPS can be used to learn task specific graph structure for GNNs. They also show that the proposed method can be applied to any GNN parameters. "
513,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,Continual Learning USED-FOR catastrophic forgetting. supervised training USED-FOR they. model USED-FOR label - agnostic incremental setting. network confusion USED-FOR novelty detection method. class - imbalance USED-FOR detection method. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. CIFAR-100 CONJUNCTION CRIB. CRIB CONJUNCTION CIFAR-100. image classification benchmarks EVALUATE-FOR approach. CRIB HYPONYM-OF image classification benchmarks. MNIST HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. SVHN HYPONYM-OF image classification benchmarks. ,"-based continual learning (CL) methods have been shown to suffer from catastrophic forgetting. This paper proposes a novel novelty detection method to mitigate this issue. The novelty detection is based on the network confusion and class-imbalance. The proposed method is evaluated on CIFAR-10, MNIST, and SVHN datasets. ","This paper proposes a novel novelty detection method for continuous learning. The novelty detection is based on the idea of label-agnostic incremental learning, where the model is trained in a label agnostic incremental setting. The proposed method is evaluated on CIFAR-10, CRIB, MNIST, SVHN, and Cifar-100 datasets. "
522,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,robotics CONJUNCTION autonomous cars. autonomous cars CONJUNCTION robotics. domain expertise USED-FOR specifications. natural language constraints USED-FOR safe RL. HAZARDWORLD HYPONYM-OF multi - task benchmark. agent USED-FOR multi - task benchmark. free - form text USED-FOR constraints. agent USED-FOR tasks. modular architecture FEATURE-OF agent. policy network USED-FOR policy. constraint interpreter USED-FOR spatial and temporal representations of forbidden states. constraint interpreter USED-FOR textual constraints. representations USED-FOR policy. minimal constraint violations FEATURE-OF policy. representations USED-FOR policy network. policy network PART-OF model. constraint interpreter PART-OF model. method COMPARE approaches. approaches COMPARE method. rewards CONJUNCTION constraint violations. constraint violations CONJUNCTION rewards. HAZARDWORLD EVALUATE-FOR method. constraint violations EVALUATE-FOR method. rewards EVALUATE-FOR method. Task is safe reinforcement learning ( RL ). OtherScientificTerm is mathematical form. ,"This paper proposes to use natural language constraints for safe reinforcement learning (RL) in the presence of constraints in the form of natural language specifications. The authors propose to use language constraints as input to a policy network and use a constraint interpreter to enforce constraints on the policy. The constraint interpreter is trained to generate representations of forbidden states, which are then used by the policy network to learn a policy with minimal constraint violations. The proposed method is evaluated on the multi-task environment of HazardWorld, which is a new environment for RL.   ","This paper proposes a safe reinforcement learning (RL) framework that leverages natural language constraints for safe RL. The key idea is to use a constraint interpreter to generate a set of constraints that can be used to guide the agent in a safe manner. The constraint interpreter is composed of spatial and temporal representations of forbidden states, which are then used as representations for the policy network. The proposed method is evaluated on the HAZARD-World benchmark, where it is shown to outperform the state of the art."
531,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"few - shot semantic edge detection USED-FOR boundaries of novel categories. few - shot semantic edge detection HYPONYM-OF few - shot learning challenge. image generation CONJUNCTION medical imaging. medical imaging CONJUNCTION image generation. semantic segmentation CONJUNCTION localization. localization CONJUNCTION semantic segmentation. object reconstruction CONJUNCTION image generation. image generation CONJUNCTION object reconstruction. boundary information USED-FOR semantic segmentation. boundary information USED-FOR localization. boundary information USED-FOR object reconstruction. Few - shot semantic edge detection USED-FOR recovery of accurate boundaries. small - scale FEATURE-OF semantic segmentation module. semantic segmentation module USED-FOR CAFENet. predicted segmentation mask USED-FOR attention map. multi - split matching USED-FOR regularization method. meta - training USED-FOR metric - learning problem. highdimensional vectors USED-FOR metric - learning problem. FSE-1000 CONJUNCTION SBD-5. SBD-5 CONJUNCTION FSE-1000. them EVALUATE-FOR CAFENet. FSE-1000 HYPONYM-OF datasets. SBD-5 HYPONYM-OF datasets. CAFENet COMPARE baseline methods. baseline methods COMPARE CAFENet. fine - tuning or few - shot segmentation USED-FOR CAFENet. fine - tuning or few - shot segmentation USED-FOR baseline methods. Method are meta - learning strategy, and decoder module. OtherScientificTerm are lack of semantic information, and low - dimensional sub - vectors. ","This paper proposes a method for few-shot semantic edge detection, which aims to recover the boundaries of novel categories from images. The proposed method is based on a meta-learning strategy, where the semantic segmentation mask is predicted from a predicted segmentation map, and the attention map is computed using a multi-split matching method. The method is evaluated on FSE-1000 and SBD-5, and achieves state-of-the-art performance. ","This paper proposes a new few-shot semantic edge detection method CAFENet for semantic segmentation and localization. The proposed method is based on a meta-learning strategy, where the decoder module is trained with a predicted segmentation mask, and the attention map is trained using a multi-split matching method. The method is evaluated on FSE-1000 and SBD-5 datasets, and it outperforms the state-of-the-art on both of them."
540,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"explainability EVALUATE-FOR GNN. feature attribution USED-FOR explanation generation. causal interpretability FEATURE-OF GNNs. It USED-FOR graph feature. causal attribution FEATURE-OF graph feature. edge HYPONYM-OF graph feature. Causal Screening USED-FOR GNN model. Causal Screening USED-FOR model - agnostic tool. Causal Screening COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE Causal Screening. graph classification datasets EVALUATE-FOR Causal Screening. predictive accuracy CONJUNCTION contrastivity. contrastivity CONJUNCTION predictive accuracy. predictive accuracy HYPONYM-OF quantitative metrics. contrastivity HYPONYM-OF quantitative metrics. Method is graph neural networks ( GNNs ). OtherScientificTerm are graph features, features, cause - effect, and sanity checks. Metric is statistical interpretability. Generic is method. ","This paper proposes a method to improve the interpretability of graph neural networks (GNNs) in terms of causal interpretability, i.e., the ability to generate explanations for a given graph feature. The proposed method is based on causal attribution, which is an approach to measure the impact of a given edge on a given cause-effect relationship between two graphs. The authors propose to use a model-agnostic approach to train a GNN model that uses causal attribution as a regularization term. The method is evaluated on a variety of graph classification datasets and compared with a number of baselines. ","This paper proposes a new method to improve the interpretability of graph neural networks (GNNs). The authors propose a new metric, called causal interpretability, to measure the impact of a graph feature on the explanation generation of a GNN model. The authors show that the proposed metric can be used to measure both the predictive accuracy and contrastivity of GNNs. They also show that it can also be used as a model agnostic tool for model-agnostic GNN models. "
549,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"parameter norms HYPONYM-OF simplicity. measure EVALUATE-FOR network ’s simplicity. measure USED-FOR model. convolutional networks USED-FOR model. CIFAR-10 USED-FOR convolutional networks. flatness of minima CONJUNCTION optimization speed. optimization speed CONJUNCTION flatness of minima. models ’ margin CONJUNCTION flatness of minima. flatness of minima CONJUNCTION models ’ margin. mutual information EVALUATE-FOR measures. mutual information EVALUATE-FOR measure. measure COMPARE measures. measures COMPARE measure. flatness of minima USED-FOR measures. optimization speed USED-FOR measures. models ’ margin USED-FOR measures. measure COMPARE flatness - based measures. flatness - based measures COMPARE measure. Method is over - parameterized neural networks. Generic is they. Task are machine learning, and pruning. OtherScientificTerm are Occam ’s razor, and network ’s parameters. Metric is training loss. ","This paper proposes a new measure to evaluate the model’s simplicity in terms of the number of parameters in a network. The proposed measure is based on the flatness of minima, which is defined as the mutual information between the training loss and minima of the model. The authors show that the proposed measure can be used as a measure of model simplicity, and that it is more efficient than existing methods. The paper also shows that the new measure is better than the existing methods in the sense that it does not require the network to be over-parameterized.","This paper proposes a new measure for measuring the simplicity of a neural network. The measure is based on Occam’s razor, which is a measure of the number of parameters in a network, and is derived from the fact that the training loss of a network is the same for all the parameters. The authors show that the measure can be used to measure the flatness of minima and the optimization speed of the network. They also show that their measure is better than other measures in terms of mutual information. "
558,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,Discrete representations USED-FOR temporally - extended tasks. expert specifications USED-FOR they. deep reinforcement learning USED-FOR long - horizon tasks. exploratory video data USED-FOR temporally - abstracted discrete representations. mutual information maximization objective USED-FOR temporally - abstracted discrete representations. abstract states USED-FOR low - level model - predictive controller. DORP USED-FOR low - level model - predictive controller. DORP USED-FOR abstract states. DORP USED-FOR long - horizon tasks. it USED-FOR binary properties. key - and - door HYPONYM-OF binary properties. ,This paper proposes a novel method for learning discrete representations for long-horizon exploration in video games. The proposed method is based on a novel mutual information maximization (MIM) objective that maximizes the mutual information between the discrete representations and the high-level model-predictive controller. The authors show that the proposed method achieves state-of-the-art performance on a number of long-term exploration tasks. ,"This paper proposes a novel method for learning discrete representations for long-horizon tasks. The authors propose a novel mutual information maximization objective (DORP) objective to learn a discrete representation of a sequence of discrete states. They also propose a low-level model-predictive controller to predict the abstract states. Experiments show that DORP can be applied to a variety of tasks, including video exploration and long-term planning."
567,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"performance CONJUNCTION compression rate. compression rate CONJUNCTION performance. Mixed - precision quantization USED-FOR deep neural networks. compression rate EVALUATE-FOR deep neural networks. performance EVALUATE-FOR deep neural networks. compression rate EVALUATE-FOR Mixed - precision quantization. neural architecture search USED-FOR vast search space. manuallydesigned search space USED-FOR methods. neural architecture search USED-FOR methods. bit - level sparsity quantization ( BSQ ) USED-FOR mixed - precision quantization. bit - level sparsity quantization ( BSQ ) USED-FOR inducing bit - level sparsity. BSQ USED-FOR dynamic precision reduction. BSQ USED-FOR mixed - precision quantization scheme. BSQ USED-FOR all - zero bits. mixed - precision quantization scheme USED-FOR model. hyperparameter USED-FOR gradient - based optimization process. gradient - based optimization process USED-FOR method. BSQ COMPARE methods. methods COMPARE BSQ. accuracy CONJUNCTION bit reduction. bit reduction CONJUNCTION accuracy. model architectures EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR model architectures. bit reduction EVALUATE-FOR BSQ. accuracy EVALUATE-FOR BSQ. CIFAR-10 and ImageNet datasets EVALUATE-FOR BSQ. Generic are it, and approaches. OtherScientificTerm are quantization scheme, optimal quantization scheme, independent trainable variable, and weight elements. Method is differentiable bit - sparsity regularizer. Metric is compression. ","This paper proposes bit-level sparsity quantization (BSQ), a novel mixed-precision quantization method for deep neural networks. The proposed method is based on the idea of bit-sparsity regularization, which aims to reduce the number of all-zero bits in the weight matrix. Theoretical analysis is provided to show that the proposed method achieves the best performance on CIFAR-10 and ImageNet.  ","This paper proposes bit-level sparsity quantization (BSQ) to improve the performance of mixed-precision quantization in deep neural networks. The main idea is to use a differentiable bit-sparsity regularizer, which is differentiable in the sense that it can be applied to all-zero bits. The authors also propose a gradient-based optimization process to find the optimal quantization scheme. The proposed method is evaluated on CIFAR-10 and ImageNet datasets. "
576,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,memory consumption CONJUNCTION faster computation. faster computation CONJUNCTION memory consumption. bitwise operations FEATURE-OF quantized networks. bitwise operations USED-FOR faster computation. generalization capabilities EVALUATE-FOR they. quantized networks USED-FOR gradient based adversarial attacks. robustness FEATURE-OF quantized networks. robustness FEATURE-OF quantized models. gradient vanishing issues FEATURE-OF quantized models. temperature scaling approach USED-FOR decision boundary. forward - backward signal propagation PART-OF network. forward - backward signal propagation USED-FOR gradient vanishing. adversarially trained models CONJUNCTION floating - point networks. floating - point networks CONJUNCTION adversarially trained models. temperature scaled attacks COMPARE attacks. attacks COMPARE temperature scaled attacks. CIFAR-10/100 datasets CONJUNCTION multiple network architectures. multiple network architectures CONJUNCTION CIFAR-10/100 datasets. near - perfect success rate EVALUATE-FOR quantized networks. quantized networks EVALUATE-FOR temperature scaled attacks. adversarially trained models EVALUATE-FOR attacks. floating - point networks EVALUATE-FOR attacks. near - perfect success rate EVALUATE-FOR temperature scaled attacks. Method is Neural network quantization. ,-based adversarial attacks have been shown to be effective in the presence of quantized neural networks. This paper proposes to address the gradient vanishing issue in quantized networks by using forward-backward signal propagation in the network to mitigate gradient vanishing. The proposed method is based on a temperature scaling approach to compute the decision boundary. Experiments on CIFAR-10/100 datasets and multiple network architectures show that the proposed method achieves near-perfect success rate.,-based attacks on neural networks are known to suffer from gradient vanishing issues. This paper proposes a temperature-scaled approach to address the gradient vanishing issue. The temperature scaling approach is based on the forward-backward signal propagation of the network. The authors show that the temperature scaled attacks can achieve near-perfect success rate on CIFAR-10/100 datasets. 
585,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,prototype trajectories PART-OF interpretable recurrent neural network ( RNN ) model. ProtoryNet HYPONYM-OF interpretable recurrent neural network ( RNN ) model. prototype theory USED-FOR ProtoryNet. prototype USED-FOR ProtoryNet. RNN backbone USED-FOR temporal pattern. RNN backbone USED-FOR prototypes. temporal pattern FEATURE-OF prototypes. method COMPARE prototype - based method. prototype - based method COMPARE method. ProtoryNet COMPARE prototype - based methods. prototype - based methods COMPARE ProtoryNet. Material is text sequence. Generic is model. ,"This paper proposes a novel prototype-based RNN model, ProtoryNet, for text classification tasks. The proposed method is based on the idea of prototype theory and proposes to use prototypes as input to a recurrent neural network (RNN) model, which is trained to predict a sequence of prototypes. The authors show that the proposed method outperforms existing methods in terms of accuracy on text classification task. ","This paper proposes a prototype-based interpretable RNN model for text-to-text translation. The proposed method is based on the prototype theory of RNNs. The authors propose a novel RNN-based prototype network that can be used to generate prototypes of text sequences. The prototype network is trained using a recurrent neural network (RNN) backbone, which is trained to predict the temporal pattern of the prototypes. The paper also proposes a method to learn prototypes of the prototype network. "
594,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,Hidden Markov models ( HMMs ) USED-FOR disease progression modeling. patient covariates USED-FOR estimation. local optima FEATURE-OF HMMs. HMRNN COMPARE discrete - observation HMM. discrete - observation HMM COMPARE HMRNN. likelihood function FEATURE-OF HMRNN. hidden Markov recurrent neural networks ( HMRNNs ) HYPONYM-OF recurrent neural networks ( RNNs ). HMRNN CONJUNCTION predictive neural networks. predictive neural networks CONJUNCTION HMRNN. patient covariate information USED-FOR predictive neural networks. Baum - Welch algorithm USED-FOR HMRNN parameter estimates. HMRNN USED-FOR parameter estimation. HMRNN CONJUNCTION neural networks. neural networks CONJUNCTION HMRNN. neural networks USED-FOR parameter estimation. Alzheimer ’s disease dataset EVALUATE-FOR HMRNN. HMRNN ’s solution COMPARE HMM. HMM COMPARE HMRNN ’s solution. HMRNN ’s solution USED-FOR clinical interpretation. HMRNN ’s solution USED-FOR disease forecasting. HMM USED-FOR clinical interpretation. OtherScientificTerm is patient health state. ,This paper proposes to use hidden Markov models (HMM) for disease prediction in Alzheimer’s disease. The proposed method is based on the idea of using hidden RNNs to model the patient covariate information. The authors show that the proposed method outperforms the state-of-the-art discrete-observation HMMs in disease prediction. They also show that their method is computationally efficient.,"This paper proposes a new method for disease prediction using hidden Markov models (HMMs). The proposed method is based on the Baum-Welch algorithm, which can be used to estimate the patient covariate information. The method is evaluated on Alzheimer’s disease dataset, where it is shown to outperform the state-of-the-art."
603,SP:6355337707f1dd373813290e26e9c0a264b993f9,"phenotypes FEATURE-OF structural and functional properties of neuronal types. supervised learning approach USED-FOR gene expression data. components USED-FOR phenotypic characteristics. phenotypic feature CONJUNCTION feature combination. feature combination CONJUNCTION phenotypic feature. sparsity - based regularization algorithm USED-FOR feature combination. sparsity - based regularization algorithm USED-FOR phenotypic feature. sparsity - based regularization algorithm USED-FOR approach. dendritic and axonal phenotypes FEATURE-OF single - cell RNA - Seq dataset. Drosophila T4 / T5 neurons FEATURE-OF single - cell RNA - Seq dataset. single - cell RNA - Seq dataset EVALUATE-FOR approach. analysis EVALUATE-FOR methods. Task are neurobiology, and linear transformation of gene expressions. Material is Single - cell RNA sequencing. OtherScientificTerm are neuronal phenotypes, phenotypic factor, and genetic organization. Generic is method. Method is factorized linear discriminant analysis ( FLDA ). ","This paper proposes a novel method for learning phenotypic information from single-cell RNA-Seq data. The method is based on factorized linear discriminant analysis (FLDA), which is an extension of the FLDA framework. The main contribution of the paper is a novel regularization term that encourages the model to learn a combination of features that capture both phenotypical and gene expression information. The proposed method is evaluated on the Drosophila T4/T5 dataset, and compared with a number of baselines.",This paper proposes a factorized linear discriminant analysis (FLDA) method for learning the phenotypic characteristics of a single-cell RNA-Seq dataset of Drosophila T4/T5 neurons. The FLDA method is based on a sparsity-based regularization algorithm that learns the feature combination between the gene expression data and the phenotypes. The authors show that the FLDA can be used to learn phenotypes of dendritic and axonal phenotypes in the dataset. 
612,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"Saliency maps USED-FOR image classifier. interpretability method USED-FOR posterior distribution. posterior distribution FEATURE-OF saliency map. random variable FEATURE-OF saliency map. image USED-FOR classifier ’s predictive probability. approximate posterior USED-FOR classifier. OtherScientificTerm are likelihood function, prior distribution, positive correlation, and auxiliary information. Method is variational approximation. Generic is It. ",This paper proposes a method to estimate the posterior distribution of the saliency map of an image classifier using an interpretability method. The proposed method is based on the observation that the posterior of the classifier depends on the positive correlation between an image and a random variable. The authors show that this positive correlation is independent of the prior distribution and can be interpreted as an auxiliary information. They show that the proposed method can be used in conjunction with other interpretability methods to infer the posterior distributions of image classifiers.,"This paper studies the interpretability of saliency maps for image classifiers. In particular, the authors propose a new interpretability method for the saliency map, which is based on the notion of positive correlation. They show that positive correlation can be interpreted as a positive correlation between the classifier’s predictive probability and a random variable in the image. They also show that the posterior distribution of the image classifier can be approximated by a variational approximation of the prior distribution."
621,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"Pretrained text encoders USED-FOR natural language processing ( NLP ) tasks. BERT HYPONYM-OF Pretrained text encoders. social bias FEATURE-OF pretrained NLP models. sentence - level fairness EVALUATE-FOR pretrained encoders. neural debiasing method USED-FOR pretrained sentence encoder. fair filter ( FairFil ) network USED-FOR debiased representations. filtered embeddings CONJUNCTION bias words. bias words CONJUNCTION filtered embeddings. contrastive learning framework USED-FOR FairFil. real - world datasets EVALUATE-FOR FairFil. FairFil USED-FOR bias degree. FairFil USED-FOR pretrained text encoders. bias degree FEATURE-OF pretrained text encoders. FairFil USED-FOR downstream tasks. Task is word - level debiasing. OtherScientificTerm are pretrained encoder outputs, and rich semantic information. Method are post hoc method, and text encoders. ","This paper proposes a novel debiasing method for sentence-level fairness in pretrained text encoders. The proposed method is based on the idea that the debiased representations of a pretrained sentence encoder are biased due to the presence of bias words in the original sentence. To address this issue, the authors propose a novel fair filter (FairFil) network that filters out bias words from the original word embeddings and uses a contrastive learning framework to improve the fairness of the debased representations. Experiments show that the proposed method improves the performance of the pretrained encoder on downstream tasks. ","This paper proposes a novel neural debiasing method to improve the performance of BERT-based text encoders on sentence-level fairness tasks. The proposed method is based on a contrastive learning framework, where the debiased word embeddings are used to filter out bias words. The authors show that the proposed method outperforms the state-of-the-art methods in terms of performance on a variety of downstream tasks."
630,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"certified models USED-FOR machine learning security. certified models USED-FOR adversarial perturbations. randomized smoothing USED-FOR models. l2 perturbations FEATURE-OF models. test accuracy CONJUNCTION average certified robust radius. average certified robust radius CONJUNCTION test accuracy. sample - wise randomized smoothing USED-FOR noise levels. sample - wise randomized smoothing USED-FOR defense. robust regions USED-FOR certification. accuracy - robustness trade - off FEATURE-OF transductive setting. transductive setting EVALUATE-FOR method. accuracy - robustness trade - off EVALUATE-FOR method. Task is certifying l2 perturbations. OtherScientificTerm are Gaussian noise, and noise level. Method is pretrain - to - finetune framework. Generic is model. Material is CIFAR-10 and MNIST datasets. ","This paper proposes a method for certifying models against adversarial perturbations in the presence of Gaussian noise. The proposed method is based on randomized smoothing, where the noise level is determined by the sample-wise smoothing. The method is evaluated on CIFAR-10 and MNIST datasets.   ",This paper proposes a new method for training certified models for detecting l2 perturbations. The proposed method is based on the pretrain-to-finetune framework. The main idea is to use sample-wise randomized smoothing to improve the robustness of the certified model. The method is evaluated on CIFAR-10 and MNIST datasets. 
639,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,probabilistic method USED-FOR unsupervised recovery of corrupted data. method USED-FOR posteriors of clean values. degraded samples USED-FOR method. variational methods USED-FOR collapsed densities. reduced entropy condition approximate inference method USED-FOR rich posteriors. imputation CONJUNCTION de - noising. de - noising CONJUNCTION imputation. missing values CONJUNCTION noise. noise CONJUNCTION missing values. variational methods USED-FOR de - noising. variational methods USED-FOR imputation. model COMPARE variational methods. variational methods COMPARE model. real data sets USED-FOR variational methods. real data sets USED-FOR de - noising. data recovery task EVALUATE-FOR model. propagating uncertainty USED-FOR downstream tasks. classification accuracy EVALUATE-FOR imputation. model USED-FOR downstream tasks. OtherScientificTerm is solution space. ,This paper proposes a probabilistic method for unsupervised recovery of corrupted data from corrupted data. The method is based on a variational approach to estimate the posteriors of clean values from degraded samples. The paper proposes to use a reduced entropy condition approximate inference method to estimate posteriors for rich posteriors. The proposed method is evaluated on the data recovery task and is shown to outperform existing variational methods. ,"This paper proposes a probabilistic method for unsupervised recovery of corrupted data from corrupted data. The proposed method is based on variational methods for recovering the posteriors of clean values from corrupted samples. The main idea is to use a reduced entropy condition approximate inference method to infer the rich posteriors from the corrupted samples, and then use a variational method to recover the clean posteriors. The method is evaluated on a variety of data recovery tasks, and it is shown that the proposed method outperforms the state-of-the-art methods."
648,SP:4b7d050f57507166992034e5e264cccab3cb874f,"Self - attention mechanism USED-FOR graph neural networks ( GNNs ). Self - attention mechanism USED-FOR graph representation learning task. multi - hop context information USED-FOR attention computation. long - range interactions PART-OF GNN. MAGNA USED-FOR attention. diffusion prior USED-FOR MAGNA. MAGNA USED-FOR large - scale structural information. MAGNA USED-FOR informative attention. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. Citeseer CONJUNCTION Pubmed. Pubmed CONJUNCTION Citeseer. node classification CONJUNCTION knowledge graph completion benchmarks. knowledge graph completion benchmarks CONJUNCTION node classification. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. relative error reduction EVALUATE-FOR state - of - the - art. node classification EVALUATE-FOR MAGNA. MAGNA COMPARE MAGNA. MAGNA COMPARE MAGNA. knowledge graph completion benchmarks EVALUATE-FOR MAGNA. MAGNA COMPARE state - of - the - art. state - of - the - art COMPARE MAGNA. node classification EVALUATE-FOR MAGNA. Citeseer EVALUATE-FOR state - of - the - art. Pubmed EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR state - of - the - art. Cora EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. relative error reduction EVALUATE-FOR MAGNA. large - scale Open Graph Benchmark dataset EVALUATE-FOR MAGNA. WN18RR CONJUNCTION FB15k237. FB15k237 CONJUNCTION WN18RR. Method is attention mechanism. OtherScientificTerm are nodes, network context, attention scores, and receptive field. Generic is network. Task is knowledge graph completion. Metric is performance metrics. ","This paper proposes a new self-attention mechanism for graph representation learning. The proposed method, called MAGNA, is based on the diffusion prior. The main idea is to use multi-hop context information to improve the attention computation. The authors show that the proposed method improves the performance on Cora, Citeseer, and Pubmed datasets.","This paper proposes a new self-attention mechanism for graph neural networks (GNNs) for knowledge graph completion task. The proposed method, called MAGNA, is based on diffusion prior, which is a GNN-based attention mechanism. The main idea of the proposed method is to use multi-hop context information to improve the attention computation of GNNs. The method is evaluated on a large-scale Open Graph Benchmark dataset (WN18RR, FB15k237, and Citeseer), and compared with state-of-the-art methods."
657,SP:36310d761deb19e71c8a57de19b48f857707d48b,"test EVALUATE-FOR text model. multitask accuracy EVALUATE-FOR test. multitask accuracy EVALUATE-FOR text model. computer science CONJUNCTION law. law CONJUNCTION computer science. US history CONJUNCTION computer science. computer science CONJUNCTION US history. elementary mathematics CONJUNCTION US history. US history CONJUNCTION elementary mathematics. elementary mathematics CONJUNCTION computer science. computer science CONJUNCTION elementary mathematics. tasks PART-OF test. elementary mathematics HYPONYM-OF tasks. law HYPONYM-OF tasks. US history HYPONYM-OF tasks. computer science HYPONYM-OF tasks. world knowledge CONJUNCTION problem solving ability. problem solving ability CONJUNCTION world knowledge. world knowledge FEATURE-OF models. problem solving ability FEATURE-OF models. GPT-3 model COMPARE random chance. random chance COMPARE GPT-3 model. models COMPARE GPT-3 model. GPT-3 model COMPARE models. near random - chance accuracy EVALUATE-FOR models. tasks EVALUATE-FOR models. lopsided performance FEATURE-OF Models. morality CONJUNCTION law. law CONJUNCTION morality. nearrandom accuracy EVALUATE-FOR they. academic and professional understanding FEATURE-OF model. Metric are accuracy, and expert - level accuracy. ","This paper presents a set of experiments to evaluate the performance of text models on a range of tasks, including computer science, mathematics, history, and law. The authors show that the GPT-3 model is able to achieve near-random accuracy on most of the tasks. They also show that it is possible for the model to achieve better performance than random chance.   ","This paper proposes a new test to measure the accuracy of a text model on a variety of tasks. The main idea of the test is to test the model’s ability to perform well on a set of tasks (e.g., computer science, law, US history, physics, mathematics, etc.) that are well-known in the academic community. The test is designed to measure how well the model can perform on tasks that are not well known in the community.   The paper presents a new model, GPT-3, that is trained to perform better than the state-of-the-art GPT model on these tasks. It is shown that the model is able to achieve near-random accuracy on the tasks, and that it can achieve better performance on the task of morality. The paper also shows that the GPT3 model outperforms the state of the art in terms of accuracy on a number of other tasks."
666,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"pre - training approach USED-FOR table semantic parsing. GRAPPA HYPONYM-OF pre - training approach. synchronous context - free grammar ( SCFG ) USED-FOR synthetic question - SQL pairs. GRAPPA USED-FOR structural properties. structural properties PART-OF pre - training language model. structural properties PART-OF table semantic parsing. synthetic data USED-FOR GRAPPA. masked language modeling ( MLM ) USED-FOR pre - training process. model USED-FOR real - world data. table - and - language datasets USED-FOR masked language modeling ( MLM ). OtherScientificTerm are compositional inductive bias, and pre - trained embeddings. Method is pre - training strategy. ","This paper proposes a pre-training approach for table semantic parsing based on synchronous context-free grammar (SCFG) to generate synthetic question-SQL pairs. The proposed method, GRAPPA, pre-train a language model on the synthetic data and use masked language modeling (MLM) to improve the performance of the model on real-world data. Experiments show that the proposed method outperforms baselines on both table- and language datasets. ",This paper proposes a new pre-training approach for table semantic parsing. The proposed method is based on synchronous context-free grammar (SCFG) and uses synthetic question-SQL pairs to train a pre-trained language model. The model is trained using masked language modeling (MLM) on synthetic data and real-world data. 
675,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"random matrix analysis USED-FOR Gaussian mixture data model. random matrix analysis USED-FOR MTL LS - SVM. small - dimensional ) statistics FEATURE-OF deterministic limit. single - task LS - SVMs COMPARE MTL approach. MTL approach COMPARE single - task LS - SVMs. sufficient statistics USED-FOR method. MTL LS - SVM method COMPARE multi - task and transfer learning techniques. multi - task and transfer learning techniques COMPARE MTL LS - SVM method. Method are multi - task and transfer learning methods, MTL LS - SVM algorithm, and cross - validation procedure. OtherScientificTerm is hyperparameters. ",This paper proposes a new multi-task self-supervised learning (LSV) method based on a random matrix analysis of a Gaussian mixture data model. The authors show that the proposed method is equivalent to the state-of-the-art single-task LS-SVM. The main contribution of the paper is to provide sufficient statistics for sufficient statistics to prove the existence of a deterministic limit of the MTL LS-sVM.  ,This paper studies the problem of multi-task transfer learning (MTL) from a Gaussian mixture data model. The main contribution of the paper is to propose a new method for learning a single-task LS-SVM with sufficient statistics. The method is based on a random matrix analysis of the Gaussian data model and a cross-validation procedure. The authors show that the proposed method outperforms the state-of-the-art in terms of the number of tasks and transferability. 
684,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,group equivariant conditional neural process ( EquivCNP ) HYPONYM-OF metalearning method. permutation invariance FEATURE-OF data set. data space FEATURE-OF transformation equivariance. permutation invariance FEATURE-OF metalearning method. transformation equivariance FEATURE-OF it. rotation and scaling equivariance HYPONYM-OF group equivariance. EquivCNPs USED-FOR group symmetries. decomposition theorem USED-FOR permutation - invariant and group - equivariant maps. infinite - dimensional latent space USED-FOR group symmetries. decomposition theorem USED-FOR EquivCNPs. infinite - dimensional latent space USED-FOR EquivCNPs. Lie group convolutional layers USED-FOR architecture. EquivCNP COMPARE CNPs. CNPs COMPARE EquivCNP. translation equivariance FEATURE-OF EquivCNP. 1D regression task EVALUATE-FOR EquivCNP. 1D regression task EVALUATE-FOR CNPs. EquivCNP USED-FOR zero - shot generalization. zero - shot generalization USED-FOR image - completion task. EquivCNP USED-FOR image - completion task. Lie group equivariance PART-OF EquivCNP. Lie group equivariance USED-FOR EquivCNP. Method is conditional neural processes ( CNPs ). OtherScientificTerm is symmetry of real - world data. ,This paper proposes a new method for learning group-equivariant conditional neural processes (EquivCNPs) based on Lie group convolutional layers. The main idea is to learn a permutation-invariant and group equivariant maps in an infinite-dimensional latent space that is permutation invariant to group symmetries. Theoretical analysis is provided to show that the proposed method can be viewed as a decomposition of permutation and equivariance maps in the data space. Experiments are conducted on image classification and regression tasks.   ,This paper proposes a new metalearning method for group equivariant conditional neural process (EquivCNP) based on permutation-invariant and group-equivariant maps. The method is based on the Lie group convolutional layers (LGE) layers. The authors prove the decomposition of group symmetries in an infinite-dimensional latent space. They also show that the proposed method can achieve zero-shot generalization on a 1D regression task.
693,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"autoregressive models CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION autoregressive models. approaches USED-FOR text generation. autoregressive models USED-FOR approaches. maximum likelihood estimation USED-FOR approaches. mismatched history distributions FEATURE-OF exposure bias. expert demonstrations USED-FOR offline reinforcement learning ( RL ) problem. offline reinforcement learning ( RL ) problem USED-FOR text generation. demonstrations USED-FOR easy - to - optimize algorithm. importance weighting USED-FOR easy - to - optimize algorithm. optimization issues FEATURE-OF prior RL approaches. online data collection USED-FOR prior RL approaches. MLE CONJUNCTION policy gradient. policy gradient CONJUNCTION MLE. summarization CONJUNCTION question generation. question generation CONJUNCTION summarization. question generation CONJUNCTION machine translation. machine translation CONJUNCTION question generation. models COMPARE those. those COMPARE models. automatic and human evaluation EVALUATE-FOR models. policy gradient USED-FOR question generation. MLE USED-FOR question generation. GOLD USED-FOR those. policy gradient USED-FOR summarization. policy gradient USED-FOR machine translation. summarization EVALUATE-FOR those. summarization EVALUATE-FOR models. MLE USED-FOR models. MLE USED-FOR those. policy gradient USED-FOR those. GOLD USED-FOR models. models USED-FOR exposure bias. decoding algorithms USED-FOR models. Generic is paradigm. OtherScientificTerm are mismatched learning objective, and model - generated histories. ","This paper proposes a novel offline reinforcement learning approach for text generation from expert demonstrations. The proposed approach is based on the idea of using expert demonstrations as an easy-to-optimize algorithm for offline RL. The authors propose to use importance weighting to reduce the exposure bias caused by the mismatch between the generated history distributions and the expert demonstrations, and then use a policy gradient based on this weighting function to improve the performance of the model. Experiments are conducted on question generation, summarization, and machine translation tasks. ","This paper proposes a novel offline reinforcement learning (RL) approach for text generation from expert demonstrations. The authors propose a novel approach to tackle the problem of mismatched history distributions in offline RL. They propose an easy-to-optimize algorithm based on importance weighting, which can be applied to both online data collection and offline data collection. They show that the proposed approach can outperform the state-of-the-art in terms of performance on question generation and summarization. "
702,SP:e77eca51db362909681965092186af2e502aaedc,"intermediate activations USED-FOR back - propagation. gradient - isolated modules PART-OF network. local supervision USED-FOR network. early layers FEATURE-OF task - relevant information. E2E loss FEATURE-OF local modules. information propagation ( InfoPro ) loss USED-FOR local modules. reconstruction loss CONJUNCTION normal cross - entropy / contrastive term. normal cross - entropy / contrastive term CONJUNCTION reconstruction loss. ImageNet CONJUNCTION Cityscapes. Cityscapes CONJUNCTION ImageNet. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. SVHN CONJUNCTION STL-10. STL-10 CONJUNCTION SVHN. InfoPro COMPARE E2E training. E2E training COMPARE InfoPro. CIFAR CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR. CIFAR CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR. datasets EVALUATE-FOR InfoPro. memory footprint EVALUATE-FOR E2E training. Cityscapes HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. STL-10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. method USED-FOR training acceleration. local modules USED-FOR training acceleration. method USED-FOR local modules. Method are deep networks, and locally supervised learning. Metric is GPUs memory footprint. Generic are model, and algorithm. OtherScientificTerm are useful information, task - irrelevant information, InfoPro loss, surrogate optimization objective, and GPU memory constraint. ","This paper proposes an information propagation (InfoPro) loss for locally supervised learning. The idea is to use intermediate activations for back-propagation in the early layers of the network to remove task-irrelevant information. The proposed method is evaluated on ImageNet, Cityscapes, SVHN, and STL-10 and achieves better performance compared to E2E training.  ","This paper proposes a method to accelerate the training of local modules in deep neural networks by using information propagation (InfoPro) loss. The idea is to use intermediate activations for back-propagation in the early layers of the network, where the task-relevant information is extracted from the local modules. The paper also proposes a surrogate optimization objective for the InfoPro loss. Experiments on CIFAR-10, STL-10 and SVHN datasets show that the proposed method outperforms the state-of-the-art methods."
711,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,Graph neural networks ( GNNs ) USED-FOR framework. expressive power FEATURE-OF learning representation of nodes and graphs. expressive power FEATURE-OF GNNs. multiple aggregation functions HYPONYM-OF complex neighborhood aggregation functions. injective aggregation function HYPONYM-OF complex neighborhood aggregation functions. aggregation function USED-FOR expressive power. framework USED-FOR GNNs. framework USED-FOR expressive power. expressive power EVALUATE-FOR GNNs. diverse sampling USED-FOR diverse neighborhoods. diverse sampling USED-FOR representation of target node. GNN model USED-FOR representation of diverse neighborhoods. representation of diverse neighborhoods USED-FOR representation of target node. rooted sub - graphs HYPONYM-OF diverse neighborhoods. diversity of different neighborhoods USED-FOR expressive power. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GNNs EVALUATE-FOR framework. GAT HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. multi - class node classification task CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION multi - class node classification task. benchmark datasets CONJUNCTION multi - label node classification task. multi - label node classification task CONJUNCTION benchmark datasets. benchmark datasets EVALUATE-FOR multi - class node classification task. dataset USED-FOR multi - label node classification task. method USED-FOR GNN models. framework USED-FOR GNN models. framework USED-FOR GNNs. Method is layer - wise neighborhood aggregation. ,"This paper proposes a method to improve the expressive power of graph neural networks (GNNs) by introducing multiple neighborhood aggregation functions. The proposed method is based on the idea that diverse sampling from diverse neighborhoods can improve the representation of target nodes. The method is evaluated on a variety of node classification tasks, including multi-label node classification. ","This paper proposes a novel approach to learning expressive power for graph neural networks (GNNs) by combining multiple aggregation functions (e.g. multiple aggregation function, multiple neighborhood aggregation functions) and injective aggregation function (i.e., a complex neighborhood aggregation function). The main idea is to use diverse sampling of diverse neighborhoods to learn the representation of target node. The proposed approach is evaluated on GCN, GAT, and multi-class node classification tasks."
720,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,network transmission failures CONJUNCTION hardware errors. hardware errors CONJUNCTION network transmission failures. defenses USED-FOR corruptions. video machine learning models USED-FOR bit - level network and file corruptions. robustness EVALUATE-FOR video machine learning models. common action recognition CONJUNCTION multi - object tracking tasks. multi - object tracking tasks CONJUNCTION common action recognition. corruption levels FEATURE-OF network and file corruptions. defenses USED-FOR bit - level corruptions. corruption - agnostic and corruption - aware defenses HYPONYM-OF defenses. corruption - agnostic defenses COMPARE no - defense baseline. no - defense baseline COMPARE corruption - agnostic defenses. adversarial training HYPONYM-OF corruption - agnostic defenses. Bit - corruption Augmented Training ( BAT ) HYPONYM-OF corruptionaware baseline. model invariance USED-FOR corruptions. knowledge of bit - level corruptions FEATURE-OF corruptionaware baseline. BAT COMPARE corruption - agnostic defenses. corruption - agnostic defenses COMPARE BAT. BAT COMPARE no - defense baseline. no - defense baseline COMPARE BAT. highly - corrupted videos EVALUATE-FOR no - defense baseline. highly - corrupted videos EVALUATE-FOR BAT. Material is clean / near - clean data. ,This paper studies the problem of robustness to corruptions in video machine learning models. The authors propose a corruption-aware baseline based on bit-corruption augmented training (BAT) to improve robustness against corruptions. They show that BAT improves robustness in terms of corruption-agnostic robustness and corruptions-aware robustness. They also show that model invariance can be achieved with BAT.   ,"This paper proposes a corruption-aware training method for video machine learning models. The method is based on the notion of bit-level corruptions, which are corruptions at the level of the network and file level. The authors show that corruption-agnostic defenses are not robust to corruptions and propose a corruption aware defense based on adversarial training. The proposed method is evaluated on a variety of video tasks, including action recognition and multi-object tracking."
729,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"Representation learning models USED-FOR graphs. machine learning algorithms USED-FOR feature spaces. feature spaces FEATURE-OF nodes. skip - gram embedding approach USED-FOR implicit tensor factorization. implicit tensor factorization USED-FOR tensor representations of time - varying graphs. skip - gram embedding approach USED-FOR tensor representations of time - varying graphs. learned representations COMPARE state - of - the - art methods. state - of - the - art methods COMPARE learned representations. learned representations USED-FOR downstream tasks. state - of - the - art methods USED-FOR downstream tasks. approach USED-FOR downstream tasks. network reconstruction HYPONYM-OF downstream tasks. method USED-FOR contagion risk. method USED-FOR early risk awareness. disease spreading HYPONYM-OF dynamical processes. contact tracing data USED-FOR early risk awareness. Material is real - world networks. Generic are techniques, and approaches. ", data is collected from time-varying graphs. The authors propose a skip-gram embedding approach to learn tensor representations of time-evolving graphs. They show that the learned representations outperform the state-of-the-art methods on downstream tasks. ,"This paper proposes a new representation learning method for time-varying graphs. The authors propose a skip-gram embedding approach for implicit tensor factorization (i.e., tensor representations of time-evolving graphs) that can be used to learn representations for downstream tasks. They show that the proposed method outperforms state-of-the-art representation learning methods on a variety of downstream tasks, including disease spreading, contact tracing, and network reconstruction. "
738,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,self - supervised language modeling USED-FOR logical reasoning. self - supervised language modeling USED-FOR mathematical formulas. logical reasoning abilities EVALUATE-FOR language models. evaluation ( downstream ) tasks EVALUATE-FOR logical reasoning abilities. evaluation ( downstream ) tasks EVALUATE-FOR language models. language models USED-FOR formal mathematics. skip - tree task USED-FOR language models. models COMPARE models. models COMPARE models. skip - tree task USED-FOR models. mathematical reasoning abilities FEATURE-OF models. skipsequence tasks USED-FOR models. ,"This paper proposes to use self-supervised language modeling to learn mathematical formulas in a skip-sequence setting, where the goal is to learn a sequence of logical formulas that can be used to evaluate the reasoning ability of a language model. The authors show that the proposed method outperforms the state-of-the-art methods on a number of downstream evaluation tasks.   ",This paper presents a study of self-supervised language modeling for logical reasoning. The authors propose a new skip-sequence task to evaluate the ability of language models to perform logical reasoning in the context of formal mathematics. The main contribution of the paper is to propose a skip-tree task for evaluating logical reasoning abilities in the form of a skipsequence task. The proposed skip sequence task is designed to evaluate logical reasoning ability in terms of the number of steps in the skip sequence. The paper also presents a set of experiments to demonstrate the effectiveness of the proposed task. 
747,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"robustness EVALUATE-FOR defense model. robustness PART-OF adversarial robustness research. defense model PART-OF adversarial robustness research. Obfuscated gradients HYPONYM-OF gradient masking. Obfuscated gradients USED-FOR defense methods. Margin Decomposition ( MD ) attack USED-FOR margin loss. attackability FEATURE-OF terms. Margin Decomposition ( MD ) attack USED-FOR imbalanced gradients. two - stage process USED-FOR terms. two - stage process USED-FOR attackability. models USED-FOR imbalanced gradients. label smoothing USED-FOR models. PGD attack EVALUATE-FOR PGD robustness. PGD robustness EVALUATE-FOR MD attacks. attack USED-FOR defenses. PGD robustness EVALUATE-FOR defenses. PGD robustness EVALUATE-FOR attack. adversarial robustness EVALUATE-FOR imbalanced gradients. OtherScientificTerm are Imbalanced Gradients, and gradient. Metric is overestimated adversarial robustness. Method is defense models. ","This paper studies the problem of adversarial robustness in the presence of imbalanced gradients. The authors propose a novel margin decomposition (MD) attack to improve the robustness of the defense model. The MD attack is based on a two-stage process, where the first stage is to compute the margin loss and the second stage is a label smoothing step to improve robustness.  The authors show that the proposed MD attack improves the adversarial performance of the proposed defense model in terms of PGD robustness and robustness against adversarial perturbations.",This paper proposes a novel method to improve the robustness of defense models against imbalanced gradients. The authors propose a two-stage process to measure the attackability of a defense model. The first stage is to use a margin-decomposition (MD) attack on the imbalanced gradient masking. The second stage is a label-smoothing-based attack to improve robustness. The proposed method is evaluated on a variety of adversarial robustness metrics.
756,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,rich multi - type symbolic language USED-FOR linear algebra. proving semantic equivalence FEATURE-OF complex expressions. expressions USED-FOR system. typed trees HYPONYM-OF complex expressions. rich multi - type symbolic language USED-FOR expressions. rich multi - type symbolic language USED-FOR system. graph - to - sequence deep learning system USED-FOR axiomatic proofs of equivalence. operators PART-OF expressions. scalars PART-OF expressions. incremental graph - to - sequence networks USED-FOR complex and verifiable symbolic reasoning. robustness EVALUATE-FOR system. zero false positives EVALUATE-FOR It. average true positive coverage EVALUATE-FOR It. OtherScientificTerm is axioms of equivalence. ,This paper proposes a method for proving linear algebra equivalence in a multi-type symbolic language. The main idea is to use a graph-to-sequence deep learning system to learn a set of operators that can be used to represent the equivalence of linear algebra operators. The method is evaluated on a variety of datasets and compared to existing methods.   ,"This paper proposes a new method for proving semantic equivalence in linear algebra. The main idea is to use a graph-to-sequence deep learning system to learn a rich multi-type symbolic language that can be used to prove the equivalence of complex expressions. The system is built on top of incremental graph to sequence networks, and is trained using a set of axiomatic proofs of equivalence. The authors show that the proposed method is robust to zero false positives.   "
765,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"it USED-FOR multimodal setting. Transformers USED-FOR language domain. Transformers USED-FOR multimodal setting. language model USED-FOR visual model. multimodal Transformers USED-FOR audio - visual video representation learning. modality - specific and modality - shared parts PART-OF Transformer. low - rank approximation USED-FOR parameter sharing scheme. approach USED-FOR Transformers. approach USED-FOR model. model CONJUNCTION Transformers. Transformers CONJUNCTION model. CNN embedding space FEATURE-OF instance similarity. instance similarity USED-FOR negative sampling approach. it USED-FOR audio - visual classification tasks. Method is vision module. OtherScientificTerm are cross - modal information, and memory requirement. Material is Kinetics-700. ",This paper proposes a low-rank approximation method for multi-modal audio-visual video representation learning. The proposed method is based on a low rank approximation to the parameters of the Transformer architecture. The method is evaluated on the Kinetics-700 dataset and shows promising results.   ,This paper proposes a low-rank approximation method for multi-modal audio-visual video representation learning. The method is based on the idea that each modality-specific part of the Transformer is modality specific and modalities-shared parts of Transformer are modality shared parts of the model. The authors propose a low rank approximation scheme for the parameter sharing scheme. They also propose a negative sampling approach for the instance similarity between modalities in the CNN embedding space. They demonstrate the effectiveness of the proposed method on a variety of tasks.
774,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"few - shot learning USED-FOR online, continual setting. large scale indoor imagery USED-FOR visual experience. large scale indoor imagery USED-FOR few - shot learning dataset. spatiotemporal contextual information USED-FOR contextual prototypical memory model. Task are human and machine - learning environments, and online few - shot learning setting. Generic are setting, and models. OtherScientificTerm are spatiotemporal context, and Object classes. Method is few - shot learning approaches. ","This paper proposes a method for online few-shot learning in the online, continual setting, where the goal is to learn object classes in an online setting without access to the entire environment. The method is based on the idea of using spatiotemporal contextual information to learn a contextual prototypical memory model. The proposed method is evaluated on a set of indoor scenes from a large scale indoor dataset.   ","This paper proposes a method for few-shot learning in the continual learning setting, where the goal is to learn a set of objects from a dataset of large-scale indoor imagery. The method is based on the notion of spatiotemporal contextual information, which is used to train a prototypical memory model. The model is trained on the dataset of indoor imagery, and is trained to learn the object classes and the spatial and temporal context of the dataset. "
783,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"distribution shift CONJUNCTION gbv rowing. gbv rowing CONJUNCTION distribution shift. temporal graphs USED-FOR GNNs. GNN architectures CONJUNCTION scalable GNN techniques. scalable GNN techniques CONJUNCTION GNN architectures. vertices CONJUNCTION edges. edges CONJUNCTION vertices. accuracy EVALUATE-FOR GNN ’s receptive field. Method is graph neural networks ( GNNs ). OtherScientificTerm are full graph, and temporal window. ","This paper studies the effect of distribution shift on the receptive field of graph neural networks (GNNs) in the temporal setting. The authors show that GNNs are sensitive to distribution shift and gbv rowing. They also show that a GNN with a temporal window is more sensitive than one with a fixed temporal window. Finally, the authors propose a new GNN architecture that is able to handle distribution shift.  ","This paper proposes a new method for training graph neural networks (GNNs) on temporal graphs. The main idea is to train GNNs on a temporal graph, where each node in the graph is represented as a set of vertices, and each edge is represented by a pair of edges, and the vertices are connected by a temporal window. The authors show that the temporal window can be used to train a GNN on the full graph, and that it can be applied to any GNN architecture. They also show that it is possible to train the GNN in a scalable manner.  "
792,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"Representation learning USED-FOR deep reinforcement learning ( RL ). visualized input USED-FOR Representation learning. visualized input USED-FOR feature space. technique USED-FOR representation feature space. cross - state self - constraint(CSSC ) HYPONYM-OF technique. constraint USED-FOR general feature recognition. implicit feedback USED-FOR constraint. learning process USED-FOR general feature recognition. generalization EVALUATE-FOR constraint. generalization EVALUATE-FOR method. OpenAI ProcGen benchmark EVALUATE-FOR method. Method is RL agent. OtherScientificTerm are representation similarity, and Procgen games. ",This paper proposes a new self-constraint method for representation learning in reinforcement learning. The proposed method is based on the observation that representation similarity is a critical component of generalization in RL. The authors propose to use implicit feedback to encourage the representation similarity to be close to the true representation. The method is evaluated on the OpenAI ProcGen benchmark and shows promising results.,"This paper proposes a new representation learning method called cross-state self-constraint (CSCC) for representation learning in deep reinforcement learning (DRL). CSCC is based on the idea of implicit feedback, which is used to enforce a constraint on the representation space. The authors show that CSCC can be used to improve the generalization of DRL agents in ProcGen games. They also show that it can improve the performance of the agent in terms of generalization. "
801,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"GNNs USED-FOR real - world applications. robustness FEATURE-OF GNNs. adversarial attacks FEATURE-OF GNNs. model parameters CONJUNCTION model predictions. model predictions CONJUNCTION model parameters. restricted near - black - box setup FEATURE-OF GNNs. attacks CONJUNCTION influence maximization problem. influence maximization problem CONJUNCTION attacks. influence maximization problem FEATURE-OF graph. adversarial attack FEATURE-OF GNNs. strategies COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE strategies. GNN models COMPARE baseline adversarial attack strategies. baseline adversarial attack strategies COMPARE GNN models. GNN models EVALUATE-FOR strategies. Method are Graph neural networks ( GNNs ), and near - black - box attack strategies. Material is realistic setups. OtherScientificTerm is features. ","This paper studies adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting. The authors show that the influence maximization problem of GNNs can be viewed as a variant of the influence minimization problem, where the goal is to maximize the influence of a given node in a graph over a set of nodes in the graph.    The authors propose a new adversarial attack strategy based on a modified version of influence maximisation, which they call influence-maximization-based adversarial training (IMBAT).   They show that IMBAT can be used to improve the robustness of a GNN model to adversarial perturbations.  They evaluate the effectiveness of the proposed method on a variety of real-world datasets. ","This paper studies adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting. The authors study the influence maximization problem of GNNs, which is an important problem in real-world applications. In particular, the authors propose a new attack strategy, which they call influence-maximization-based attack, which aims to maximise the influence of a GNN on the input graph. They show that the proposed attack strategy outperforms baseline GNN attack strategies in terms of the impact on the model parameters and model predictions. "
810,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"directed acyclic graphs ( DAGs ) USED-FOR learning causal structures. ( weighted ) adjacency matrix FEATURE-OF DAG causal model. low rank assumption FEATURE-OF ( weighted ) adjacency matrix. low rank assumption USED-FOR DAG causal model. methods USED-FOR causal structure learning. interpretable graphical conditions CONJUNCTION low rank assumption. low rank assumption CONJUNCTION interpretable graphical conditions. assumption USED-FOR methods. maximum rank FEATURE-OF hubs. low rank FEATURE-OF scale - free networks. rank FEATURE-OF DAG. they COMPARE algorithms. algorithms COMPARE they. OtherScientificTerm are causal structures, graphs, and low rank condition. Task is high dimensional settings. Method is low rank adaptations. ","This paper studies the problem of learning causal structures in directed acyclic graphs (DAGs) with a low rank assumption on the (weighted) adjacency matrix of the DAG causal model. The authors propose two methods for learning DAGs with low rank: (1) using a scale-free network with a weighted matrix of DAG hubs, and (2) applying a low-rank adaptation to the weighted matrix. The proposed methods are shown to outperform the state-of-the-art methods in terms of accuracy and computational complexity. ",This paper studies the problem of learning directed acyclic graphs (DAGs) with a low rank assumption. The authors propose a new algorithm for learning DAGs with a weighted adjacency matrix (weighted by a low-rank matrix) and show that the proposed method outperforms the state-of-the-art in terms of the number of nodes in the graph. They also show that scale-free networks with low rank can be used to learn high-rank graphs.
819,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"neural architecture search ( NAS ) CONJUNCTION hyper - parameter optimization ( HPO ). hyper - parameter optimization ( HPO ) CONJUNCTION neural architecture search ( NAS ). automated data augmentation ( DA ) CONJUNCTION neural architecture search ( NAS ). neural architecture search ( NAS ) CONJUNCTION automated data augmentation ( DA ). components PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) PART-OF automated machine learning ( AutoML ) pipeline. automated data augmentation ( DA ) HYPONYM-OF components. hyper - parameter optimization ( HPO ) HYPONYM-OF components. neural architecture search ( NAS ) HYPONYM-OF components. components USED-FOR joint optimization. it USED-FOR NAS. end - to - end solution USED-FOR ready - to - use model. hyper - parameters CONJUNCTION data augmentation policies. data augmentation policies CONJUNCTION hyper - parameters. co - optimization USED-FOR neural architectures. co - optimization USED-FOR method. data augmentation policies USED-FOR method. DiffAutoML COMPARE end - to - end AutoML algorithms. end - to - end AutoML algorithms COMPARE DiffAutoML. DiffAutoML COMPARE multi - stage AutoML algorithms. multi - stage AutoML algorithms COMPARE DiffAutoML. ImageNet EVALUATE-FOR end - to - end AutoML algorithms. computational efficiency EVALUATE-FOR multi - stage AutoML algorithms. ImageNet EVALUATE-FOR DiffAutoML. NAS CONJUNCTION HPO. HPO CONJUNCTION NAS. automated DA CONJUNCTION NAS. NAS CONJUNCTION automated DA. en - to - end manner FEATURE-OF HPO. Generic is component. OtherScientificTerm is search dimension. Task is search and retraining stages. Method are differentiable joint optimization solution, model retraining, and retraining. ","This paper proposes a joint optimization solution for neural architecture search and hyper-parameter optimization (HPO) and automated data augmentation (DA) in automated machine learning (AutoML) pipeline. The authors propose an end-to-end solution for training a ready to use model for both NAS and HPO. The proposed method, called DiffAutoML, is based on co-optimization for neural architectures and hyper parameters. The main contribution of the paper is the joint optimization of hyper parameters and DA policies in the search and retraining stages.   ","This paper proposes a method for joint optimization between neural architecture search (NAS) and hyper-parameter optimization (HPO) in an end-to-end manner. The authors propose a joint optimization solution for NAS and HPO that is differentiable between the search and retraining stages. They show that the joint optimization can be done in an en- to- end manner, and that it can be combined with the automated data augmentation (DA) component. The proposed method is evaluated on ImageNet, and it outperforms the state-of-the-art in terms of performance."
828,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,Prior Networks HYPONYM-OF models. interpretable measures of uncertainty EVALUATE-FOR models. tasks EVALUATE-FOR ensemble approaches. calibration CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION calibration. They USED-FOR ensemble of models. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. uncertainty estimates PART-OF model. Ensemble Distribution Distillation ( EnD ) USED-FOR ensemble of models. Prior Networks USED-FOR classification tasks. Prior Networks CONJUNCTION EnD. EnD CONJUNCTION Prior Networks. EnD USED-FOR regression tasks. Prior Networks USED-FOR regression tasks. synthetic data CONJUNCTION UCI datasets. UCI datasets CONJUNCTION synthetic data. UCI datasets CONJUNCTION monocular depth estimation tasks. monocular depth estimation tasks CONJUNCTION UCI datasets. monocular depth estimation tasks EVALUATE-FOR Regression Prior Networks. UCI datasets EVALUATE-FOR Regression Prior Networks. synthetic data EVALUATE-FOR Regression Prior Networks. They COMPARE ensemble approaches. ensemble approaches COMPARE They. OtherScientificTerm is Normal - Wishart distribution. ,"This paper proposes a method to improve the calibration of ensemble models by distilling uncertainty estimates from the ensemble of models into a single model. The proposed method is based on the prior distribution distillation (EnD) method, which distills the uncertainty estimates of the ensemble models into the same distribution. The authors show that EnD improves the calibration and uncertainty estimates on a variety of classification and regression tasks.   ","This paper proposes Ensemble Distribution Distillation (EnD), a method for distilling the ensemble of models into a single model that can be used for classification and regression tasks. EnD distills the ensemble into a set of models that are trained on the same dataset and can be combined with a prior network. The authors show that EnD can be applied to a wide range of tasks, including classification, regression, and monocular depth estimation tasks. They also show that the EnD method is able to achieve state-of-the-art performance on synthetic data and UCI datasets. "
837,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,catastrophic forgetting FEATURE-OF Neural networks. problem USED-FOR large - scale supervised classification. catastrophic forgetting USED-FOR few - shot classification problems. few - shot tasks USED-FOR Few - shot metalearning algorithms. Bayesian online meta - learning framework USED-FOR sequential few - shot tasks problems. Bayesian online meta - learning framework USED-FOR catastrophic forgetting. catastrophic forgetting CONJUNCTION sequential few - shot tasks problems. sequential few - shot tasks problems CONJUNCTION catastrophic forgetting. Laplace approximation CONJUNCTION variational inference. variational inference CONJUNCTION Laplace approximation. MAML PART-OF Bayesian online learning algorithm. Laplace approximation USED-FOR Bayesian online learning algorithm. variational inference USED-FOR Bayesian online learning algorithm. MAML PART-OF framework. framework USED-FOR few - shot classification. sequentially arriving datasets USED-FOR few - shot classification. framework USED-FOR catastrophic forgetting. framework USED-FOR online meta - learning. online meta - learning USED-FOR few - shot classification settings. Material is sequential datasets. Generic is algorithm. Method is meta - learned model. Task is sequentially arriving few - shot tasks. ,This paper proposes a Bayesian online meta-learning framework for few-shot classification problems with catastrophic forgetting. The proposed method is based on a meta-learned model with MAML and uses a variational inference approach to learn the meta-model. The authors show that the proposed method outperforms the state-of-the-art methods in terms of catastrophic forgetting performance on a variety of tasks.   ,This paper proposes a Bayesian online meta-learning framework for few-shot metalearning problems with catastrophic forgetting. The proposed method is based on the Laplace approximation and variational inference. The authors show that the proposed method outperforms the state-of-the-art methods in terms of catastrophic forgetting on a variety of tasks. The main contribution of the paper is the use of the MAML-based meta-training framework.
846,SP:89d2765946e70455105a608d998c3b900969cb8d,"expressive power EVALUATE-FOR higher - order GNNs. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. RNP - GNNs COMPARE higher - order k - GNN. higher - order k - GNN COMPARE RNP - GNNs. higher - order k - GNN CONJUNCTION Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks CONJUNCTION higher - order k - GNN. RNP - GNNs COMPARE Local Relational Pooling ( LRP ) networks. Local Relational Pooling ( LRP ) networks COMPARE RNP - GNNs. computational complexity EVALUATE-FOR higher - order k - GNN. computational complexity EVALUATE-FOR Local Relational Pooling ( LRP ) networks. computational complexity EVALUATE-FOR RNP - GNNs. Task is learning with graphs. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. OtherScientificTerm is local neighborhoods. ",This paper proposes a new low-rank graph neural network (RNP-GNN) architecture for graph learning. The main idea is to use a local neighborhood pooling technique to learn the subgraphs in the original graph. The authors show that the proposed method is computationally efficient and achieves better expressive power compared to the existing high-rank GNNs. ,"This paper studies the expressive power of higher-order graph neural networks (GNNs) in the context of learning with graphs. In particular, the authors consider the problem of learning a high-order k-GNN, where k is the number of subgraphs in the graph, and k is a subgraph of the original graph. The authors propose a new method to learn a k-GAN, which they call Recurrent Local Relational Pooling (RNP-GAN). The main idea is to use a local pooling technique of local neighborhoods, where each subgraph is represented as a set of nodes, and each node is represented by a local subgraph. They show that the proposed method can learn a higher order GNN with a lower computational cost. They also show that their method is more expressive than previous works.  "
855,SP:c43f5deb340555d78599a3496318514a826b1aae,"competitive environments CONJUNCTION games. games CONJUNCTION competitive environments. GANs HYPONYM-OF games. irregular behaviors FEATURE-OF systems. Multiplicative Weights Update ( MWU ) HYPONYM-OF learning algorithms. canonical game decomposition USED-FOR zero - sum and coordination components. volume - expansion argument USED-FOR characterizations. canonical game decomposition USED-FOR volume - expansion argument. components USED-FOR volume - changing behaviors. matrix domination CONJUNCTION linear program. linear program CONJUNCTION matrix domination. general games CONJUNCTION graphical games. graphical games CONJUNCTION general games. MWU CONJUNCTION OMWU. OMWU CONJUNCTION MWU. MWU USED-FOR potential games. OMWU USED-FOR potential games. local equivalence of volume change USED-FOR multi - player games. Method is Machine Learning. Material is two - person zero - sum games. OtherScientificTerm are Lyapunov chaos, cumulative payoff space, persistent chaos, and zero - sum games. Task are normal - form game settings, and bimatrix games. ",This paper studies two-player zero-sum games in the presence of Lyapunov chaos. The authors show that the dynamics of the game can be characterized by the volume-expansion argument of the canonical game decomposition. They show that this argument can be used to characterize volume-changing behaviors in games with irregular behaviors. They then propose a novel learning algorithm called Multiplicative Weights Update (MWU) that uses the volume expansion argument to learn the game dynamics. Theoretical results are provided to show that MWU can learn the dynamics in two-person zero-Sum games.,"This paper studies two-player zero-sum games in the context of the Lyapunov chaos setting. The main contribution of the paper is to provide a canonical game decomposition of the zero- sum and coordination components of the game, which can be used for characterizing the volume-expansion argument. The authors show that this decomposition can be applied to multi-player games and bimatrix games. They also provide a local equivalence of volume change argument for multi-players games, which is used to identify potential games. "
864,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"adaptive algorithms USED-FOR deep learning. AMSGrad CONJUNCTION Radam. Radam CONJUNCTION AMSGrad. Radam HYPONYM-OF adaptive algorithms. AMSGrad HYPONYM-OF adaptive algorithms. convergence rate EVALUATE-FOR adaptive algorithms. marginal regret bound minimization HYPONYM-OF proximal function of adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. adaptive algorithms COMPARE adaptive algorithms. marginal optimality FEATURE-OF adaptive algorithms. deep learning EVALUATE-FOR adaptive algorithms. Generic are modifications, and algorithm. ","This paper studies the convergence of adaptive algorithms in deep learning. In particular, the authors consider two adaptive algorithms, AMSGrad and Radam, and show that they converge to a minimax minimax regret minimization. The main contribution of the paper is a theoretical analysis of the convergence rate of the adaptive algorithms.  ","This paper studies the convergence of adaptive algorithms for deep learning. The authors study the convergence rate of AMSGrad and Radam, two popular adaptive algorithms. They show that adaptive algorithms with marginal regret bound minimization converge faster than adaptive algorithms without marginal regret minimization. They also provide a theoretical analysis of the marginal optimality of the adaptive algorithms, and show that the marginal minimization is a proximal function of the function of adaptive algorithm. "
873,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"risk management USED-FOR real - world decision - making problems. mean - variance criterion HYPONYM-OF risk management approaches. quadratic utility function USED-FOR risk management. reward - constrained variance minimization CONJUNCTION regularization. regularization CONJUNCTION reward - constrained variance minimization. EQUM COMPARE mean - variance RL methods. mean - variance RL methods COMPARE EQUM. double sampling USED-FOR mean - variance RL methods. RL and financial data EVALUATE-FOR EQUM. Method are expected quadratic utility maximization ( EQUM ), and mean - variance control. Task is agent utility maximization. ","This paper studies the expected quadratic utility maximization (EQU) problem in risk management, where the goal is to maximize the expected utility function of the agent. The authors propose a new algorithm, called EQUM, which is based on the idea of reward-constrained variance minimization (UCM) and regularization. The main contribution of the paper is to show that the proposed algorithm is equivalent to the state-of-the-art mean-variance control (MVCC) algorithm in terms of reward and variance.    The main contributions of this paper are as follows:  1. A new algorithm called EQU is proposed for risk management.  2. The algorithm is shown to be equivalent to MVCC and regularizes the reward minimization objective.  3. Empirical results are shown on simulated and real-world data. ",This paper proposes a new risk management framework for risk management based on the expected quadratic utility maximization (EQUM) criterion. The main contribution of the paper is to propose a new algorithm for risk-aware risk management. The proposed algorithm is based on a simple yet effective algorithm that can be applied to a wide range of risk management problems. The authors show that the proposed algorithm can be used to improve the performance of risk minimization in a variety of settings.  
882,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"neural networks USED-FOR auxiliary tasks. auxiliary tasks PART-OF coherent loss. network USED-FOR coherent objective function. network USED-FOR nonlinear interactions. network USED-FOR auxiliary task. it COMPARE methods. methods COMPARE it. tasks EVALUATE-FOR AuxiLearn. image segmentation HYPONYM-OF tasks. Task are multi - task learning setting, and designing useful auxiliary tasks. Generic is framework. OtherScientificTerm are implicit differentiation, useful auxiliaries, and low data regime. ","This paper proposes AuxiLearn, a method for learning auxiliary tasks in multi-task learning. The auxiliary tasks are designed to be independent of the main task, and are learned by a neural network. The main idea is to learn a coherent loss function, which is then used to learn auxiliary tasks. The proposed method is evaluated on image segmentation and image classification tasks. ","This paper proposes AuxiLearn, a framework for multi-task learning with auxiliary tasks. The auxiliary tasks are defined as a set of tasks that are part of a coherent loss, and the auxiliary tasks can be used to improve the performance of the overall objective function. The proposed method is based on the idea of implicit differentiation, which is an important problem in multi-tasks learning. The main idea is to use a neural network to learn a coherent objective function for each auxiliary task. This objective function is then used to design auxiliary tasks that can be useful in the low-data regime.  "
891,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,out - of - training - distribution sentences PART-OF Neural Machine Translation. Bayesian Deep Learning equivalent of Transformer models USED-FOR out - of - training - distribution sentences. measure USED-FOR long sequences of discrete random variables. approaches USED-FOR long sentences. measure USED-FOR Transformer model. dropout approximate inference USED-FOR Transformer model. WMT13 CONJUNCTION Europarl. Europarl CONJUNCTION WMT13. dropout uncertainty USED-FOR measure. model COMPARE German. German COMPARE model. measure USED-FOR Dutch source sentences. measure USED-FOR German - English translation. WMT13 USED-FOR German - English translation. Europarl USED-FOR German - English translation. ,"This paper proposes a Bayesian deep learning approach for out-of-training sentences in NMT. The proposed approach is based on a measure of the uncertainty in the output of the Transformer model, which is used to estimate the dropout uncertainty of the model. The authors show that the proposed approach outperforms the existing methods in terms of accuracy and training time on the German-English translation task.",This paper proposes a new measure for out-of-training-distribution sentences. The measure is based on the dropout approximate inference of the Transformer model. The authors show that the proposed measure can be used to improve the performance of Transformer models for long sequences of discrete random variables. The proposed measure is applied to the German-English translation of WMT13.
900,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"SNIP CONJUNCTION GraSP. GraSP CONJUNCTION SNIP. GraSP CONJUNCTION SynFlow. SynFlow CONJUNCTION GraSP. SynFlow CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION SynFlow. GraSP CONJUNCTION magnitude pruning. magnitude pruning CONJUNCTION GraSP. methods COMPARE random pruning. random pruning COMPARE methods. accuracy EVALUATE-FOR magnitude pruning. they COMPARE magnitude pruning. magnitude pruning COMPARE they. methods USED-FOR per - weight pruning decisions. Task is pruning neural networks. Method are neural networks, and pruning heuristics. ","This paper studies the effect of per-weight pruning on the accuracy of pruning neural networks. The authors propose three pruning heuristics: SNIP, GraSP, and SynFlow. They show that SNIP and GraSP perform better than magnitude pruning in terms of accuracy. They also show that per-weights pruning is more effective than random pruning.   ","This paper proposes a new method to perform per-weight pruning of neural networks. The method is based on SNIP, GraSP, and SynFlow. The main idea of the method is to prune the weights of a neural network by pruning the weights at each layer of the network. The authors show that this method is more accurate than random pruning. They also show that magnitude pruning is more effective than SNIP and GraSP."
909,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,semi - honest server CONJUNCTION Byzantine malicious clients. Byzantine malicious clients CONJUNCTION semi - honest server. FED - LEARNING HYPONYM-OF federated learning protocol. robust mean estimator USED-FOR FED - LEARNING. FED - LEARNING HYPONYM-OF FL protocol. FED - LEARNING USED-FOR dimension - free estimation error. robust mean estimator USED-FOR FL protocol. FL protocol USED-FOR dimension - free estimation error. FilterL2 HYPONYM-OF robust mean estimator. secure aggregation USED-FOR FED - LEARNING. FilterL2 CONJUNCTION secure aggregation. secure aggregation CONJUNCTION FilterL2. optimal or close - to - optimal performance EVALUATE-FOR FED - LEARNING. OtherScientificTerm is shards. Method is robust FL protocols. ,This paper proposes a federated learning (FL) protocol that is robust to Byzantine malicious clients. The main idea is to use a robust mean estimator to estimate the dimension-free estimation error in the FL protocol. The authors show that this estimator can be used to compute the dimension free estimation error for FL protocols.   The authors also propose a secure aggregation scheme to improve the performance of the proposed method. ,"This paper proposes a robust mean estimator for federated learning (FL) protocols. The main idea is to estimate the mean of the average of the number of shards in the network. The authors show that this estimator is robust to Byzantine malicious clients, semi-honest servers, and semi-hybrid clients. They also provide a secure aggregation scheme for FED-LEARNING.   "
918,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"aircraft CONJUNCTION robot morphology. robot morphology CONJUNCTION aircraft. design input USED-FOR unknown objective function. robot morphology HYPONYM-OF domains. aircraft HYPONYM-OF domains. method USED-FOR function. offline MBO methods USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR highdimensional problems. high - capacity deep neural network function approximators USED-FOR offline MBO methods. evaluation tasks EVALUATE-FOR field. Design - Bench HYPONYM-OF offline MBO tasks. Design - Bench HYPONYM-OF benchmark suite. benchmark suite EVALUATE-FOR offline MBO tasks. unified evaluation protocol USED-FOR benchmark suite. unified evaluation protocol USED-FOR offline MBO tasks. biology CONJUNCTION material science. material science CONJUNCTION biology. material science CONJUNCTION robotics. robotics CONJUNCTION material science. benchmark EVALUATE-FOR offline MBO methods. Generic are problems, and benchmarks. OtherScientificTerm are feedback, and objective function. Task is data - driven offline MBO setting. ",This paper proposes a new offline MBO setting where the objective function is unknown and the design input is unknown. The authors propose to use a deep neural network function approximator to solve the problem. The proposed method is evaluated on a suite of design-benchmarking tasks in three domains.  ,This paper proposes a new benchmark suite of offline multi-objective problem solving (MBO) tasks. The main idea is to use a high-capacity deep neural network function approximator to solve high-dimensional offline MBO problems. The paper also proposes a unified evaluation protocol to evaluate the performance of the proposed benchmark suite. 
927,SP:073958946c266bf760d1ad66bd39bc28a24c8521,"self - supervised generative models USED-FOR ELBO. self - supervised generative models USED-FOR multimodal models. generalized ELBO formulation USED-FOR multimodal data. methods PART-OF objective. method COMPARE state - of - the - art models. state - of - the - art models COMPARE method. selfsupervised, generative learning tasks EVALUATE-FOR state - of - the - art models. selfsupervised, generative learning tasks EVALUATE-FOR method. OtherScientificTerm are real - world phenomena, posterior approximation functions, semantic coherence, and joint data distribution. Generic is them. Task is machine learning research. ",This paper proposes a self-supervised generative model for multi-modal data. The main idea is to use a generalized ELBO formulation to model the joint distribution of the data. This formulation is based on the notion of semantic coherence. The proposed method is evaluated on a variety of synthetic and real-world datasets.   ,This paper proposes a generalized ELBO formulation for self-supervised generative models for multimodal data. The main idea is to use a generative model to model the joint data distribution of a set of multi-modal datasets. The authors show that the proposed method outperforms state-of-the-art models on a variety of tasks.
936,SP:98004554447b82b3d2eb9724ec551250eec7a595,"Bayesian Optimization ( BO ) USED-FOR optimizing expensive black - box functions. priors USED-FOR PrBO. probabilistic model USED-FOR pseudo - posterior. BO USED-FOR pseudo - posterior. priors CONJUNCTION BO. BO CONJUNCTION priors. priors CONJUNCTION probabilistic model. probabilistic model CONJUNCTION priors. BO CONJUNCTION probabilistic model. probabilistic model CONJUNCTION BO. PrBO USED-FOR pseudo - posterior. probabilistic model USED-FOR PrBO. BO USED-FOR PrBO. priors USED-FOR PrBO. PrBO COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PrBO. PrBO COMPARE random search. random search COMPARE PrBO. state - of - the - art methods COMPARE random search. random search COMPARE state - of - the - art methods. real - world hardware design application EVALUATE-FOR PrBO. it USED-FOR misleading priors. OtherScientificTerm are function evaluations, machine learning hyperparameters, and user priors. Method are Prior - guided Bayesian Optimization ( PrBO ), and optimization process. ","This paper proposes Prior-Guided Bayesian Optimization (PrBO), a Bayesian optimization method that uses a probabilistic model to model the pseudo-posteriority of a black-box function. The proposed method is based on prior-guided Bayesian optimisation (PBO), which is an extension of prior-guided Bayes Optimization. The main contribution of the paper is that the proposed method can be used in conjunction with Bayes-based methods to improve the efficiency of Bayes optimisation. The method is evaluated on a variety of benchmarks and compared with Bayesian methods.","This paper proposes Prior-guided Bayesian Optimization (PrBO), a method for optimizing expensive black-box functions. PrBO uses a probabilistic model to model the pseudo-posterior of the function evaluations, which is then used to guide the optimization process. The authors show that PrBO outperforms the state-of-the-art methods in terms of accuracy and efficiency. "
945,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"Deep generative models USED-FOR real - world data. execution time FEATURE-OF computational cost. binary weights USED-FOR neural networks. binary neural networks USED-FOR generative models. binary neural networks USED-FOR generative models. computational cost EVALUATE-FOR models. techniques USED-FOR deep generative models. ResNet VAE and Flow++ models HYPONYM-OF deep generative models. Generic is they. Metric is complexity. Method are binary weight normalization, binarized generative models, binary models, and regular models. ",This paper proposes to use binary weight normalization to reduce the computational cost of deep generative models. The main idea is to normalize the weights of the generative model so that the output of the model can be represented as a weighted sum of binary weights. The authors show that using binary weights improves the efficiency of the models in terms of the number of parameters and training time. They also show that the proposed method can be used in combination with existing methods to improve the performance of the trained models.,This paper studies the computational complexity of deep generative models with binary weight normalization. The authors propose a new metric to measure the computational cost of a generative model with binary weights. They show that the complexity of a binarized model can be reduced by reducing the number of binary weights in the model. They also provide a theoretical analysis of the effect of the binary weights normalization on the cost of the model and show that it can be used to reduce the complexity.
954,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"frameworks USED-FOR DL. adversarial training USED-FOR DL. approach USED-FOR DL. adversarial training USED-FOR approach. robustness EVALUATE-FOR DL. adversarial training USED-FOR robustness. norm - bounded perturbations FEATURE-OF DL. robustness EVALUATE-FOR approach. unbounded shifts in the data distribution FEATURE-OF DL. natural, out - of - distribution shifts FEATURE-OF robustness. perturbation - based adversarial robustness CONJUNCTION model - based robust deep learning. model - based robust deep learning CONJUNCTION perturbation - based adversarial robustness. paradigm USED-FOR models of natural variation. model - based robust training algorithms USED-FOR DL. robustness EVALUATE-FOR DL. robustness EVALUATE-FOR model - based robust training algorithms. adversarial training CONJUNCTION domain adaptation techniques. domain adaptation techniques CONJUNCTION adversarial training. ERM CONJUNCTION adversarial training. adversarial training CONJUNCTION ERM. classifiers COMPARE classifiers. classifiers COMPARE classifiers. algorithms COMPARE classifiers. classifiers COMPARE algorithms. domain adaptation techniques USED-FOR classifiers. ERM USED-FOR classifiers. algorithms USED-FOR classifiers. ERM USED-FOR classifiers. domain adaptation techniques USED-FOR classifiers. adversarial training USED-FOR classifiers. adversarial training USED-FOR classifiers. algorithms COMPARE baseline methods. baseline methods COMPARE algorithms. top-1 accuracy EVALUATE-FOR baseline methods. top-1 accuracy EVALUATE-FOR algorithms. Method is deep learning ( DL ). OtherScientificTerm are natural variation, data distribution, and natural conditions. Material are images, ImageNet, ImageNet - c, and natural, out - ofdistribution data. Generic are models, and methods. ",This paper studies the robustness of deep learning models in the presence of out-of-distribution data. The authors show that adversarial training with perturbation-based adversarial robustness and model-based robust deep learning can improve robustness. They also show that ERM can be used to train robust models of natural variation. They show that the proposed methods outperform baseline methods on ImageNet-c.  ,"This paper proposes a new framework for model-based robust deep learning based on natural variation in the data distribution. The main contribution of the paper is that it introduces a new paradigm for model robustness, which is based on the notion of natural variation, i.e., data distribution shifts that are non-uniform across data sets. The authors show that the proposed framework can be used to improve the robustness of deep learning models in the presence of out-of-distribution data. They also show that it can be combined with adversarial training and domain adaptation techniques to improve robustness."
963,SP:011dab90d225550e77235cbec1615e583ae3297e,polynomial complexity FEATURE-OF exact convex optimization formulations. ReLU activations USED-FOR Convolutional Neural Networks ( CNNs ). convex analytic framework USED-FOR convex optimization problems. convex optimization problems USED-FOR twoand three - layer CNN architectures. semi - infinite duality USED-FOR convex analytic framework. ` 2 norm regularized convex program USED-FOR two - layer CNNs. ` 1 regularized convex program USED-FOR sparsity. spectral domain FEATURE-OF sparsity. ` 1 regularized convex program USED-FOR multi - layer circular CNN training problems. ReLU layer USED-FOR multi - layer circular CNN training problems. ReLU layers USED-FOR three - layer CNNs. approach USED-FOR pooling methods. convex regularizers USED-FOR implicit architectural bias. OtherScientificTerm is data dimension. , in this paper is a convex optimization problem for two-layer and three-layer convolutional neural networks with ReLU activations. The main contribution is a regularized convex program for two layer CNNs with a `2 norm regularization and a spectral domain regularization. The authors show that this regularization can be used to improve the performance of pooling methods.   ,"This paper proposes a new convex analytic framework for two-layer and three-layer convolutional neural networks (CNNs) with ReLU activations. The main idea is to use the semi-infinite duality of a regularized convex program `2 norm norm regularized program (i.e., `1 norm) as a regularization term for the two layer CNNs. The authors show that this regularization can be used to improve the performance of pooling methods for CNNs with two-and-three layer architectures. They also show that the proposed method can be applied to multi-layer circular CNN training problems."
972,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"rich sensory modalities USED-FOR Robotic manipulation tasks. Human - robot interaction USED-FOR teaching robots. probabilistic generative model USED-FOR optimisation problem. high - capacity neural network USED-FOR model. latent variables USED-FOR model. latent variables CONJUNCTION high - level notions. high - level notions CONJUNCTION latent variables. table - top robot manipulation tasks EVALUATE-FOR approach. PR2 robot USED-FOR table - top robot manipulation tasks. visual information CONJUNCTION arm joint positions. arm joint positions CONJUNCTION visual information. arm joint positions CONJUNCTION arm joint efforts. arm joint efforts CONJUNCTION arm joint positions. robot USED-FOR visual information. robot USED-FOR arm joint positions. arm joint efforts FEATURE-OF robot. OtherScientificTerm are soft sponge, restricted vocabulary, and sponge. Material is rich data streams. Generic are alignment, and tasks. ","This paper proposes to use a soft-sponging model to learn to manipulate a robotic arm. The method is based on a generative model, which is trained using a neural network with an encoder-decoder architecture. The model is trained to predict the distribution of arm joint positions and joint actions, which are then used as input to the neural network to train the model. The proposed method is evaluated on a set of table-top robot manipulation tasks.  ","This paper proposes a probabilistic generative model for the task of human-robot interaction. The model is based on a neural network with a high-capacity neural network. The proposed model is trained using a soft-spongy model, which is trained on a dataset of human and robot data. It is shown that the model is able to learn a good alignment between the human and the robot, and that it can be used to improve the performance of the robot on a table-top robot manipulation task."
981,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,low - resource scenarios FEATURE-OF overfitting. tasks EVALUATE-FOR large - scale pretrained language models. general - purpose feature extractors USED-FOR models. Variational Information Bottleneck ( VIB ) USED-FOR irrelevant features. Variational Information Bottleneck ( VIB ) USED-FOR fine - tuning. method USED-FOR overfitting. fine - tuning USED-FOR low - resource target tasks. Variational Information Bottleneck ( VIB ) USED-FOR low - resource target tasks. VIB model USED-FOR sentence representations. natural language inference datasets USED-FOR sentence representations. generalization EVALUATE-FOR VIB model. low - resource datasets EVALUATE-FOR method. method USED-FOR transfer learning. low - resource scenarios FEATURE-OF transfer learning. low - resource scenarios EVALUATE-FOR method. generalization EVALUATE-FOR it. Generic is they. OtherScientificTerm is features. Material is out - of - domain datasets. ,-based language models suffer from overfitting in low-resource scenarios. This paper proposes a method called Variational Information Bottleneck (VIB) to remove irrelevant features from the model during fine-tuning. The proposed method is based on the idea of using a general-purpose feature extractor to extract the relevant information from the sentence representations. Experiments show that the proposed method can improve the performance of the model on the target tasks. ,"This paper proposes a method for fine-tuning language models for low-resource tasks. The main idea is to use the Variational Information Bottleneck (VIB) model to extract irrelevant features from the sentence representations. The VIB model is trained on a set of out-of-domain datasets, where it is shown to be effective for transfer learning. The method is evaluated on a variety of tasks, and it is found to improve the generalization performance of the model."
990,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,Natural images HYPONYM-OF projections of 3D objects. 2D image plane FEATURE-OF projections of 3D objects. they USED-FOR 3D object structures. 2D generative models USED-FOR natural image manifold. GANs HYPONYM-OF 2D generative models. knowledge USED-FOR 3D shapes of objects. 2D GAN USED-FOR 3D geometric cues. RGB images USED-FOR 2D GAN. pre - trained GAN USED-FOR 3D shape. rich 3D knowledge PART-OF pre - trained GAN. unsupervised manner USED-FOR 3D shape. iterative strategy USED-FOR diverse viewpoint and lighting variations. diverse viewpoint and lighting variations FEATURE-OF GAN image manifold. iterative strategy USED-FOR framework. 2D keypoint CONJUNCTION 3D annotations. 3D annotations CONJUNCTION 2D keypoint. cars CONJUNCTION buildings. buildings CONJUNCTION cars. it USED-FOR 3D shapes. human faces CONJUNCTION cars. cars CONJUNCTION human faces. precision FEATURE-OF 3D shapes. precision EVALUATE-FOR it. relighting CONJUNCTION object rotation. object rotation CONJUNCTION relighting. 3D shapes USED-FOR image editing. object rotation HYPONYM-OF image editing. relighting HYPONYM-OF image editing. 3D shape reconstruction CONJUNCTION face rotation. face rotation CONJUNCTION 3D shape reconstruction. approach COMPARE methods. methods COMPARE approach. methods USED-FOR face rotation. approach USED-FOR face rotation. methods USED-FOR 3D shape reconstruction. approach USED-FOR 3D shape reconstruction. OtherScientificTerm is object shapes. ,"This paper proposes a method for 3D shape reconstruction from RGB images. The method is based on a pre-trained 2D GAN that is trained to predict 3D geometric cues from the RGB images in an unsupervised manner. The key idea is to use a 2D keypoint in the RGB image to predict the 3D shapes of the objects in the image, and then use the learned 3D knowledge from the GAN to reconstruct the objects from the 2D images.   The proposed method is evaluated on a variety of 3D object reconstruction tasks, including object relighting, object rotation, and image editing. The results show that the proposed method achieves state-of-the-art performance on these tasks. ","This paper proposes a method for 3D shape reconstruction using a pre-trained 2D GAN model trained on a natural image manifold. The key idea is to use a 2D keypoint and 3D annotations to learn 3D shapes of 3D objects. The method is based on an iterative strategy to learn diverse viewpoint and lighting variations of the GAN image manifold, which can be used to generate diverse 3D object shapes. Experiments show that the method is able to reconstruct 3D faces, cars, buildings, and cars with high precision."
999,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"recognition methods USED-FOR imbalanced classification. tail accuracies CONJUNCTION head accuracies. head accuracies CONJUNCTION tail accuracies. class re - balancing / re - weighting USED-FOR recognition methods. model bias CONJUNCTION variance analysis. variance analysis CONJUNCTION model bias. RoutIng Diverse Experts ( RIDE ) HYPONYM-OF long - tailed classifier. It USED-FOR model variance. It USED-FOR model bias. computational cost EVALUATE-FOR dynamic expert routing module. distribution - aware diversity loss USED-FOR model bias. CIFAR100 - LT CONJUNCTION ImageNet - LT. ImageNet - LT CONJUNCTION CIFAR100 - LT. ImageNet - LT CONJUNCTION iNaturalist 2018 benchmarks. iNaturalist 2018 benchmarks CONJUNCTION ImageNet - LT. RIDE COMPARE state - of - the - art. state - of - the - art COMPARE RIDE. ImageNet - LT EVALUATE-FOR RIDE. iNaturalist 2018 benchmarks EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR RIDE. CIFAR100 - LT EVALUATE-FOR state - of - the - art. backbone networks CONJUNCTION long - tailed algorithms. long - tailed algorithms CONJUNCTION backbone networks. universal framework USED-FOR backbone networks. universal framework USED-FOR long - tailed algorithms. long - tailed algorithms CONJUNCTION training mechanisms. training mechanisms CONJUNCTION long - tailed algorithms. It HYPONYM-OF universal framework. It USED-FOR backbone networks. Material are Natural data, and tail data. OtherScientificTerm are dynamic view of the training data, hard negatives, and tail. Method is long - tail classifiers. Metric is head - tail model bias gap. ","This paper proposes a novel long-tailed classifier, named RIDE, to address the imbalanced classification problem. The proposed method is based on a dynamic expert routing module, which is able to re-weight the training data according to a dynamic view of training data. The authors also propose a distribution-aware diversity loss to mitigate the model bias. The experimental results show that the proposed method outperforms state-of-the-art long-tail classifiers on CIFAR-100 and ImageNet-LT. ",This paper proposes a novel method for long-tail classifiers to reduce the head-tail model bias gap. The proposed method is based on a dynamic expert routing module that learns the diversity of the training data. The authors also propose a diversity-aware diversity loss to mitigate the model bias. Experimental results on CIFAR-100-LT and ImageNet-LT show the effectiveness of the proposed method. 
1008,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"pruning criteria USED-FOR redundant filters. Channel pruning USED-FOR compressing convolutional neural networks ( CNNs ). pruning criteria USED-FOR filters ’ Importance Score. layer - wise pruning CONJUNCTION global pruning. global pruning CONJUNCTION layer - wise pruning. layer - wise pruning USED-FOR pruning criteria. global pruning USED-FOR pruning criteria. Gaussian - alike distribution FEATURE-OF convolutional filters. Material is convolutional neural networks ( CNNs ). Generic is criteria. OtherScientificTerm are pruned structures, and Convolutional Weight Distribution Assumption. Method is ` 1 and ` 2 pruning. ",This paper studies the importance score of filters in CNNs. The authors show that the Importance Score (IS) is a measure of the importance of a filter to the final output of the network. The IS is a function of the number of pruned layers and the size of the pruned filters.    The authors propose two pruning criteria: layer-wise pruning and global pruning. The first pruning criterion is based on the weight distribution of the weights of the filters. The second pruning objective is to prune the filters that are close to each other in terms of the IS score. The importance score is computed by computing the difference between the importance scores of all the filters with the same weight distribution.  The importance scores are then used to select the filters to be pruned. ,"This paper studies the problem of channel pruning in convolutional neural networks (CNNs). The authors propose two criteria for pruning the weight distribution of the weights of the filters in a CNN. The first criteria is based on the importance score, which is a measure of the importance of each layer of the network. The second criteria is the global pruning criteria, which takes into account layer-wise pruning. The authors show that both criteria can be used to reduce the size of the CNN. "
1017,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"code completion CONJUNCTION code summarization. code summarization CONJUNCTION code completion. code search CONJUNCTION code completion. code completion CONJUNCTION code search. Pre - trained models USED-FOR programming language. Pre - trained models USED-FOR code - related tasks. code search HYPONYM-OF code - related tasks. code summarization HYPONYM-OF code - related tasks. code completion HYPONYM-OF code - related tasks. code snippet USED-FOR pre - trained models. pre - trained model USED-FOR programming language. GraphCodeBERT HYPONYM-OF pre - trained model. inherent structure of code USED-FOR pre - trained model. data flow USED-FOR pre - training stage. data flow USED-FOR semantic - level structure of code. abstract syntax tree ( AST ) HYPONYM-OF syntactic - level structure of code. Transformer USED-FOR GraphCodeBERT. graph - guided masked attention function USED-FOR code structure. graph - guided masked attention function USED-FOR model. code translation CONJUNCTION code refinement. code refinement CONJUNCTION code translation. clone detection CONJUNCTION code translation. code translation CONJUNCTION clone detection. code search CONJUNCTION clone detection. clone detection CONJUNCTION code search. tasks EVALUATE-FOR model. code refinement HYPONYM-OF tasks. code search HYPONYM-OF tasks. clone detection HYPONYM-OF tasks. code translation HYPONYM-OF tasks. pre - training tasks USED-FOR GraphCodeBERT. code structure CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION code structure. code structure USED-FOR GraphCodeBERT. structure - level attentions COMPARE token - level attentions. token - level attentions COMPARE structure - level attentions. model COMPARE token - level attentions. token - level attentions COMPARE model. structure - level attentions USED-FOR model. OtherScientificTerm are code semantics, semantic - level structure, hierarchy of AST, and code structure edges. Task are code understanding process, masked language modeling, and structure - aware pre - training tasks. Generic is downstream tasks. ","This paper proposes a method for pre-training a transformer-based model on code-related tasks such as code search, code summarization, code refinement, code translation, and clone detection. The proposed method is based on the idea that code is represented as a set of nodes in an abstract syntax tree (AST) and the model is trained to predict the semantic-level structure of each node in the AST. The authors propose a graph-guided masked attention function to model the structure of the code, which is then used for downstream tasks. Experiments show that the proposed method outperforms baselines on code search and code refinement tasks.","This paper proposes a pre-trained model for code-related tasks. The model is based on a Transformer-based model that learns the structure of the code in a graph-guided manner. The proposed method is evaluated on three tasks: code search, code translation, code refinement, and clone detection. The experimental results show that the proposed method outperforms the state-of-the-art on all three tasks. "
1026,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"drug discovery CONJUNCTION material design. material design CONJUNCTION drug discovery. drug discovery HYPONYM-OF scientific fields. material design HYPONYM-OF scientific fields. distribution FEATURE-OF labeled result data. it USED-FOR regression model. skewed data USED-FOR it. approach USED-FOR regression models. accuracy EVALUATE-FOR regression models. accuracy EVALUATE-FOR approach. skewed dataset USED-FOR regression models. forcing algorithm USED-FOR regression. domain knowledge USED-FOR true distribution. neural networks USED-FOR regression model. pLogP CONJUNCTION Diamond. Diamond CONJUNCTION pLogP. pLogP HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. Diamond HYPONYM-OF real - world datasets. datasets EVALUATE-FOR approach. approach COMPARE regression models. regression models COMPARE approach. root mean squared error EVALUATE-FOR regression. root mean squared error EVALUATE-FOR regression models. datasets EVALUATE-FOR regression models. adjustment of the distribution USED-FOR regression models. regression EVALUATE-FOR approach. root mean squared error EVALUATE-FOR approach. Generic is method. OtherScientificTerm are regression outputs, and estimated ‘ true ’ distribution. Material is unlabeled data. Method is adversarial network. ","This paper proposes a method to improve the accuracy of regression models on unlabeled data. The method is based on the observation that the true distribution of labeled result data is not always the same as the distribution of the labeled data, and the authors propose to use the skewed data as a forcing algorithm to train a regression model. The authors show that the proposed method can improve the regression accuracy on pLogP and Diamond datasets. ",This paper proposes an approach to improve the performance of regression models on unlabeled data. The main idea is to estimate the true distribution of the data and use it as a forcing algorithm to train a regression model. The authors show that the proposed method outperforms the state-of-the-art regression models in terms of accuracy on pLogP and Diamond.
1035,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"Compositional generalization HYPONYM-OF algebraic capacity. known components USED-FOR algebraic capacity. human intelligence USED-FOR out - of - distribution generalization. It PART-OF human intelligence. It USED-FOR out - of - distribution generalization. model USED-FOR representations. representations USED-FOR inference. regularized hidden representations USED-FOR auxiliary reconstruction network. approach COMPARE baselines. baselines COMPARE approach. accuracy EVALUATE-FOR approach. compositional representations USED-FOR it. compositional generalization CONJUNCTION artificial intelligence research. artificial intelligence research CONJUNCTION compositional generalization. Method are neural networks, and extraction network. OtherScientificTerm are extraction ability, divergence of distributions, and compositionality. Task is transferability of compositionality. ","This paper proposes a method to improve compositional generalization, i.e., the ability of neural networks to learn representations that can be used for out-of-distribution (OOD) generalization. The method is based on the observation that compositionality is an important property of human intelligence, and that it can be leveraged to improve generalization to unknown distributions. The authors propose to use an auxiliary reconstruction network to learn compositional representations, which are then used for OOD generalization in the training process. The proposed method is evaluated on a variety of datasets, and compared to a number of baselines.","This paper proposes a new approach to improve compositional generalization in the context of neural networks. In particular, the authors propose an auxiliary reconstruction network that can be used to learn compositional representations for the task of out-of-distribution generalization. The proposed method is based on the notion of compositionality, which is the ability of a neural network to learn representations that are compositional across different distributions. The authors show that their approach can achieve better generalization performance than the state of the art. "
1044,SP:ffab573a977c819e86601de74690c29a39c264cd,"Poisoning attacks USED-FOR Reinforcement Learning ( RL ) systems. RL algorithm USED-FOR Poisoning attacks. poisoning methods USED-FOR supervised learning. supervised learning USED-FOR RL. poisoning methods USED-FOR RL. generic poisoning framework USED-FOR online RL. heterogeneous poisoning models USED-FOR RL. heterogeneous poisoning models USED-FOR generic poisoning framework. poisoning method USED-FOR policy - based RL agents. strategic poisoning algorithm USED-FOR on - policy deep RL agents. stability radius FEATURE-OF RL. stability radius HYPONYM-OF metric. stability radius USED-FOR VA2C - P. metric USED-FOR VA2C - P. Task are learning, and poisoning RL. OtherScientificTerm are Markov Decision Process ( MDP ), MDP, and limited attacking budget. Method are RL algorithms, and poisoning algorithm. Material is deep RL agents. ","This paper studies the problem of online poisoning attacks in reinforcement learning. The authors propose a new poisoning method for policy-based RL agents, which is based on the idea of heterogeneous poisoning models. They show that the proposed poisoning method can be used for on-policy deep RL agents with a limited attacking budget. They also show that this poisoning method is more efficient than previous poisoning methods. ","This paper proposes a new poisoning method for online reinforcement learning. The authors propose a generic poisoning method that can be applied to any policy-based RL agent. The proposed method is based on the VA2C-P metric, which measures the stability radius of an RL agent in a Markov Decision Process (MDP). The authors also propose a strategic poisoning algorithm for on-policy deep RL agents. "
1053,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,Checkpointing USED-FOR deep learning models. restricted memory budgets FEATURE-OF deep learning models. static computation graphs USED-FOR checkpointing techniques. greedy online algorithm USED-FOR checkpointing. Dynamic Tensor Rematerialization ( DTR ) HYPONYM-OF greedy online algorithm. Dynamic Tensor Rematerialization ( DTR ) USED-FOR online algorithm. DTR USED-FOR anN -layer linear feedforward network. O(N ) tensor operations USED-FOR DTR. Ω ( √ N ) memory budget FEATURE-OF anN -layer linear feedforward network. DTR COMPARE optimal static checkpointing. optimal static checkpointing COMPARE DTR. tensor allocations CONJUNCTION operator calls. operator calls CONJUNCTION tensor allocations. DTR prototype PART-OF PyTorch. lightweight metadata PART-OF tensors. OtherScientificTerm is eviction policy. Method is dynamic models. ,"This paper proposes a dynamic tensor reconstruction method for checkpointing in deep neural networks. Theoretical analysis shows that the proposed method is efficient in terms of the number of tensor operations and the memory budget. In particular, the authors show that for an N-layer linear feedforward network with $n$ layers, the proposed DTR method can recover $O(\sqrt{N})$ tensors in $O(N^2)$ time, which is faster than the state-of-the-art static checkpointing. ","This paper proposes a dynamic checkpointing algorithm for dynamic models with restricted memory budgets. The authors propose Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing with dynamic tensors. They show that DTR can be used to reduce the number of tensor operations in a linear feedforward network by O(N) tensors, which is equivalent to O(1/2) tensor allocations and O(2/3) operator calls. Theoretical analysis is provided to show that the proposed DTR is competitive with optimal static checkpointing. Experimental results are provided to demonstrate the effectiveness of DTR."
1062,SP:20efc610911443724b56f57f857060d0e0302243,"manually annotated evaluation sets USED-FOR task. method USED-FOR hallucination detection. synthetic data USED-FOR pretrained language models. pretrained language models USED-FOR method. machine translation CONJUNCTION abstract text summarization. abstract text summarization CONJUNCTION machine translation. machine translation EVALUATE-FOR approach. abstract text summarization EVALUATE-FOR approach. average F1 EVALUATE-FOR benchmark datasets. average F1 EVALUATE-FOR approach. token - level hallucination labels USED-FOR fine - grained loss. fine - grained loss PART-OF low - resource machine translation. Method is Neural sequence models. Generic are they, model, and baseline methods. OtherScientificTerm is automatically inserted hallucinations. Material is annotated data. "," is an interesting problem in machine translation. The authors propose a novel method to detect hallucination in the input text. The method is based on the observation that the output of a pretrained language model can be corrupted by the presence of hallucination tokens. To detect the hallucination, the authors propose to use token-level hallucination labels and use a fine-grained loss. The proposed method is evaluated on two benchmark datasets and achieves state-of-the-art results.","This paper proposes a novel method for hallucination detection in low-resource machine translation tasks. The authors propose a novel approach to automatically insert hallucination labels into the training data of a pre-trained language model. The proposed method is based on the idea of fine-grained hallucination loss, which can be applied to a variety of tasks, such as machine translation and abstract text summarization. The method is evaluated on three benchmark datasets, and it is shown to outperform the baseline methods."
1071,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"Conditional Generative Adversarial Networks ( cGAN ) USED-FOR images. idea USED-FOR architecture. NAS USED-FOR architecture. NAS USED-FOR idea. reduction of training data USED-FOR class generator. latter USED-FOR class - specific information. regular and class - modulated convolutions PART-OF search space. weight - sharing pipeline CONJUNCTION mixed - architecture optimization. mixed - architecture optimization CONJUNCTION weight - sharing pipeline. weight - sharing pipeline USED-FOR search algorithm. Markov decision process PART-OF search algorithm. Markov decision process USED-FOR sampling policy. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. FID scores FEATURE-OF image generation quality. OtherScientificTerm are class - level distributions, and generating architecture. Metric is search cost. Method are moving average, and cGAN models. ","This paper proposes a method to improve conditional generative adversarial networks (cGANs) by reducing the training data for each class in the training set. The method is based on a mixture of regular convolutions and class-modulated convolutions, where the former is used to capture the class-specific information, and the latter is used for class specific information. The proposed method is evaluated on CIFAR-10 and Cifar-100 and achieves state-of-the-art results. ",This paper proposes a new conditional generative adversarial network (cGAN) architecture for image generation. The main idea is to use regular and class-modulated convolutions in the search space of the cGAN to reduce the training data. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets.   
1080,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,estimation of causal effects USED-FOR Decision - making. observational data USED-FOR estimation of causal effects. regularization framework USED-FOR unconfoundedness. orthogonality constraint USED-FOR unconfoundedness. asymptotically normal estimator USED-FOR average causal effect. estimators COMPARE asymptotic variance. asymptotic variance COMPARE estimators. regularization framework USED-FOR deep orthogonal networks. deep orthogonal networks USED-FOR unconfounded treatments ( DONUT ). DONUT COMPARE state - of - the - art. state - of - the - art COMPARE DONUT. benchmark datasets EVALUATE-FOR DONUT. benchmark datasets USED-FOR causal inference. benchmark datasets EVALUATE-FOR state - of - the - art. OtherScientificTerm is treatment assignment. ,"This paper proposes a novel method for estimating causal effects in observational data. The method is based on orthogonality constraint, which is a regularization term to ensure the unconfoundedness of the treatment assignment. The authors show that the asymptotic variance of the average causal effect can be estimated using an orthogonal estimator. The proposed method is evaluated on two benchmark datasets and compared with state-of-the-art methods. ","This paper proposes an orthogonality-based estimator for unconfounded causal effect estimation. The estimator is based on an asymptotically normal estimator, where the variance of the average causal effect is estimated by a deep orthogonal network. The authors show that their estimator outperforms the state-of-the-art estimator in terms of variance. They also show that the estimator can be used to estimate the average of the causal effect. "
1089,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"style transfer CONJUNCTION multitask learning. multitask learning CONJUNCTION style transfer. multitask learning HYPONYM-OF deep learning techniques. style transfer HYPONYM-OF deep learning techniques. affine transformations of features USED-FOR deep learning techniques. affine parameters USED-FOR features. parameters PART-OF BatchNorm. randomly chosen parameters PART-OF network. affine parameters USED-FOR deep learning. shifting and rescaling random features USED-FOR neural networks. Method is affine transform. OtherScientificTerm are random initializations, and random features. Metric is accuracy. ",This paper studies the effect of affine transformations of features on style transfer in deep learning. The authors show that the affine transformation of features can be used to improve the accuracy of deep learning models. They show that affine parameters can be added to BatchNorm to improve style transfer. They also show that random initializations of random features can improve the performance. ,"This paper studies the problem of style transfer in the context of deep learning. The authors propose a new method called BatchNorm, which is based on the idea of affine transformations of features. The key idea is to use a set of randomly chosen parameters of a neural network to transform the features of the network. They show that this method can be used for style transfer and multitask learning. They also show that it can be applied to a wide range of tasks."
1098,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"model USED-FOR test - time adaptation. model USED-FOR confidence. method USED-FOR normalization statistics. method USED-FOR channel - wise affine transformations. Tent USED-FOR source - free domain adaptation. corrupted ImageNet CONJUNCTION CIFAR-10/100. CIFAR-10/100 CONJUNCTION corrupted ImageNet. Tent USED-FOR semantic segmentation. GTA CONJUNCTION Cityscapes. Cityscapes CONJUNCTION GTA. Tent USED-FOR image classification. Tent USED-FOR digit recognition. source - free domain adaptation USED-FOR digit recognition. generalization error EVALUATE-FOR image classification. Tent USED-FOR Tent. CIFAR-10/100 USED-FOR image classification. corrupted ImageNet USED-FOR image classification. VisDA - C benchmark EVALUATE-FOR Tent. generalization error EVALUATE-FOR Tent. Metric is entropy. Material are SVHN, and MNIST / MNIST - M / USPS. Method is test - time optimization. ","This paper proposes a method for source-free domain adaptation based on channel-wise affine transformations. The proposed method, Tent, is based on the idea of normalization statistics, which is used to estimate the confidence of the model for test-time adaptation. Tent is evaluated on corrupted ImageNet and CIFAR-10/100 and achieves state-of-the-art performance on the VisDA-C benchmark. ","This paper proposes a new method for source-free domain adaptation for image classification. The proposed method is based on the idea of channel-wise affine transformations, where the channel is represented by a set of affine transforms. The authors propose a new normalization metric for the normalization statistics of the channel-wide affine transformation, which they call Tent. Tent is trained on a large dataset of corrupted ImageNet, CIFAR-10/100, and MNIST/MNIST-M/USPS datasets. Tent achieves good performance on the VisDA-C benchmark. Tent can also be used for semantic segmentation and digit recognition."
1107,SP:ed544ee661580592063aa17aee8924cc99919130,Uncertainty quantification USED-FOR machine learning systems. recurrent timesteps FEATURE-OF stochastic discrete state transitions. stochastic discrete state transitions USED-FOR recurrent neural networks ( RNNs ). uncertainty quantification COMPARE method. method COMPARE uncertainty quantification. method USED-FOR deterministic and probabilistic automata. well - calibrated models USED-FOR real - world classification tasks. method USED-FOR well - calibrated models. explorationexploitation trade - off FEATURE-OF reinforcement learning. method USED-FOR out - of - distribution detection. method USED-FOR explorationexploitation trade - off. Generic is model. OtherScientificTerm is recurrent state transition distribution. ,This paper proposes a method for estimating uncertainty quantification in RNNs with stochastic discrete state transitions. The method is based on the observation that the uncertainty of the model depends on the number of timesteps in the recurrent state transition distribution. The authors show that the proposed method can be used to estimate the uncertainty in deterministic and probabilistic automata. The proposed method is shown to outperform the state-of-the-art methods on classification tasks.,This paper proposes a new method for uncertainty quantification of stochastic discrete state transitions in recurrent neural networks (RNNs). The key idea is to use a deterministic and probabilistic automata to estimate the uncertainty of the state transition distribution of the RNN. The authors show that their method can be applied to a variety of well-calibrated RNNs. They show that the proposed method outperforms the state-of-the-art uncertainty quantization method for deterministic RNN models. They also show that it can be used to improve the exploration-exploitation trade-off in reinforcement learning.
1116,SP:a38c523196f68a90b5db45671f9dbd87981a024c,"Protecting data privacy PART-OF deep learning ( DL ). stochastic differential equation principled residual perturbation USED-FOR privacy - preserving DL. Gaussian noise USED-FOR residual mapping of ResNets. residual perturbation USED-FOR differential privacy ( DP ). generalization gap FEATURE-OF DL. residual perturbation USED-FOR generalization gap. residual perturbation COMPARE DP stochastic gradient descent ( DPSGD ). DP stochastic gradient descent ( DPSGD ) COMPARE residual perturbation. DP stochastic gradient descent ( DPSGD ) USED-FOR membership privacy protection. residual perturbation USED-FOR DL models ’ utility. residual perturbation USED-FOR membership privacy protection. ResNet8 USED-FOR IDC dataset classification. residual perturbation USED-FOR perfect membership privacy. residual perturbation COMPARE DPSGD. DPSGD COMPARE residual perturbation. accuracy EVALUATE-FOR DPSGD. accuracy EVALUATE-FOR residual perturbation. Task is data privacy. OtherScientificTerm are utility degradation, and ResNets. ",This paper proposes a principled residual perturbation method for privacy-preserving deep learning models. The proposed method is based on the observation that the utility of deep neural networks (DNNs) is affected by Gaussian noise in the residual mapping of ResNets. The authors show that the generalization gap between DNNs trained with and without the proposed method can be reduced to zero by introducing residual noise into the training process. They also show that this method is more efficient than DP stochastic gradient descent.,"This paper proposes a principled residual perturbation for privacy-preserving deep learning (DL) models. In particular, the authors propose to use Gaussian noise in the residual mapping of ResNets to reduce the generalization gap between ResNet and ResNet8. They show that the proposed method outperforms DPSGD in terms of utility degradation. They also show that DPSGD can be used for membership privacy protection. "
1125,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"limited computational resources USED-FOR inference. natural language processing EVALUATE-FOR transformers. computational efficiency USED-FOR inference. model USED-FOR inference scenario. inefficiency CONJUNCTION redundancy. redundancy CONJUNCTION inefficiency. PoWER - BERT USED-FOR inefficiency. PoWER - BERT USED-FOR redundancy. it USED-FOR inference scenarios. extension USED-FOR large - scale transformer. Length - Adaptive Transformer HYPONYM-OF large - scale transformer. LengthDrop HYPONYM-OF structural variant of dropout. LengthDrop USED-FOR transformer. multi - objective evolutionary search USED-FOR length configuration. accuracy EVALUATE-FOR length configuration. PoWER - BERT USED-FOR token - level classification. sequence - level classification USED-FOR token - level classification. PoWER - BERT USED-FOR sequence - level classification. span - based question - answering HYPONYM-OF token - level classification. SQuAD 1.1 CONJUNCTION MNLI - m. MNLI - m CONJUNCTION SQuAD 1.1. MNLI - m CONJUNCTION SST-2. SST-2 CONJUNCTION MNLI - m. accuracyefficiency trade - off EVALUATE-FOR setups. accuracyefficiency trade - off EVALUATE-FOR approach. SST-2 HYPONYM-OF setups. SQuAD 1.1 HYPONYM-OF setups. MNLI - m HYPONYM-OF setups. Generic are they, and approaches. Metric is computational complexity. OtherScientificTerm are computational budget, Drop - and - Restore, and word - vectors. ","This paper proposes a novel extension to the Drop-and-Restore (DR) framework for large-scale transformers. The proposed extension is called Length-Adaptive Transformer (length-ADT), which is a structural variant of dropout. The authors propose a multi-objective evolutionary search to find the optimal length configuration for each word in the input sequence. The experiments show that the proposed extension can improve the accuracy and reduce the computational cost.   ",This paper proposes a structural variant of Drop-and-Restore (DR) for large-scale transformers. The proposed method is based on a multi-objective evolutionary search for the length configuration of the word vectors. The authors show that the proposed method outperforms the state-of-the-art SST-2 and MNLI-m in terms of accuracy and computational efficiency. 
1134,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"1 - WL test USED-FOR distinguishing graph structures. expressiveness EVALUATE-FOR graph neural networks ( GNNs ). neighborhood aggregation GNNs COMPARE 1 - WL test. 1 - WL test COMPARE neighborhood aggregation GNNs. neighborhood aggregation GNNs USED-FOR distinguishing graph structures. aggregators PART-OF GNNs. aggregators USED-FOR expressiveness. aggregation coefficient matrix USED-FOR aggregators. aggregation coefficient matrix USED-FOR injective aggregators. aggregators CONJUNCTION injective aggregators. injective aggregators CONJUNCTION aggregators. aggregation coefficient matrix USED-FOR aggregation. It USED-FOR rank of hidden features. nonlinear units USED-FOR aggregation - based GNNs. ExpandingConv CONJUNCTION CombConv. CombConv CONJUNCTION ExpandingConv. CombConv HYPONYM-OF GNN layers. ExpandingConv HYPONYM-OF GNN layers. models USED-FOR large and densely connected graphs. OtherScientificTerm are graph structures, weak distinguishing strength, and low - rank transformations. Method is WL test. Generic is it. ",This paper proposes neighborhood aggregation GNNs to improve the 1-WL test for distinguishing graph structures. The aggregation coefficient matrix is defined as the rank of hidden features and is used to measure the expressiveness of the hidden features. The authors show that the aggregation coefficients are independent of the number of aggregators and injective aggregators in the GNN layers. The proposed method is shown to outperform existing methods in the WL test. ,This paper proposes a new 1-WL test to measure the expressiveness of graph neural networks (GNNs) based on aggregation-based GNNs. The main contribution of the paper is to introduce a new aggregation coefficient matrix that can be used to estimate the rank of hidden features in GNN layers. The authors show that the aggregation coefficient matrices are more expressive than the original WL test. The paper also provides a theoretical analysis of the effect of the aggregation coefficients on the expressivity of the GNN.
1143,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"robustness EVALUATE-FOR generative models. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. generalization EVALUATE-FOR generative models. method USED-FOR quantifying disentanglement. conditional submanifolds PART-OF representation. topological similarity FEATURE-OF conditional submanifolds. generative model USED-FOR method. unsupervised and supervised variants PART-OF method. method COMPARE models. models COMPARE method. Task are Learning disentangled representations, and measuring disentanglement. ","This paper proposes a method for learning disentangled representations from conditional submanifolds in a generative model. The method is based on topological similarity between the conditional sub-manifold embeddings, which is used to measure the disentanglement between the two representations. The proposed method is evaluated on both unsupervised and supervised settings.   ","This paper proposes a method for quantifying disentanglement between two representations. The key idea is to measure the topological similarity between the two representations, which is a measure of disentangledness. The method is based on the notion of conditional submanifolds, which can be defined as a set of sub-manifold embeddings of the original representation. The authors show that their method can be applied to both unsupervised and supervised settings, and show that it can improve generalization and robustness. "
1152,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"unauthorized exploitation of personal data USED-FOR commercial models. sample - wise and class - wise forms FEATURE-OF error - minimizing noise. personal data USED-FOR deep learning models. Method is deep learning. Task are unauthorized data exploitation, and face recognition. OtherScientificTerm are Error - minimizing noise, and noise. Generic is model. Metric is normal data utility. ","This paper studies the problem of unauthorized data exploitation in face recognition models. The authors show that the error-minimizing noise can be viewed as a form of normal data utility. They show that this noise is independent of the class and sample size, and that it can be seen as a function of the sample size and the class. They then show that if the class is large enough, then it is possible to estimate the data utility of the model. ",This paper studies the problem of unauthorized data exploitation in the context of face recognition. The authors study the problem in terms of sample-wise and class-wise forms of error-minimizing noise. They show that the data utility of a model trained on the data is affected by the sample size and the class of the data. They also provide a theoretical analysis of the effect of the noise on the utility of the model.
1161,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,chess CONJUNCTION Go. Go CONJUNCTION chess. Go CONJUNCTION shogi. shogi CONJUNCTION Go. MuZero USED-FOR game - playing agents. MuZero COMPARE AlphaZero. AlphaZero COMPARE MuZero. game - playing agents COMPARE AlphaZero. AlphaZero COMPARE game - playing agents. model of environmental dynamics USED-FOR MuZero. deterministic environments USED-FOR MuZero. MuZero USED-FOR Nondeterministic MuZero ( NDMZ ). Nondeterministic Monte Carlo Tree Search CONJUNCTION extensive - form games. extensive - form games CONJUNCTION Nondeterministic Monte Carlo Tree Search. MuZero network architecture CONJUNCTION tree search. tree search CONJUNCTION MuZero network architecture. chance player PART-OF MuZero network architecture. chance player PART-OF tree search. Nondeterministic Monte Carlo Tree Search USED-FOR NDMZ. extensive - form games USED-FOR NDMZ. NDMZ USED-FOR chance. chance player PART-OF NDMZ. NDMZ USED-FOR model. model USED-FOR game. Method is MuZero algorithm. Material is Atari suite. OtherScientificTerm is environmental dynamics. ,"This paper proposes a novel algorithm for playing games in deterministic environments. The main idea is to use MuZero to learn a model of the environment dynamics, which is then used to train a game-playing agent in a deterministic environment. The model is trained using tree search and extensive-form games. Experiments on Atari games show that the proposed method outperforms the state-of-the-art AlphaZero. ","This paper proposes a model of the environment in which MuZero can be used to train a game-playing agent in a deterministic environment. The model is based on the Nondeterministic Monte Carlo Tree Search (NDMZ) algorithm, which can be applied to extensive-form games (e.g., Chess, Go, Shogi, Go-Shogi, etc). The authors show that MuZero is able to outperform AlphaZero in a variety of Atari games. They also show that the NDMZ model can also be used in a tree search setting."
1170,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"Hierarchical approaches USED-FOR reinforcement learning. Hierarchical approaches USED-FOR learning. Hierarchical approaches USED-FOR data efficiency. Hindsight Off - policy Options ( HO2 ) HYPONYM-OF off - policy option learning algorithm. temporal and action abstraction USED-FOR option framework. flat policies COMPARE mixture policies. mixture policies COMPARE flat policies. mixture policies COMPARE option policies. option policies COMPARE mixture policies. flat policies COMPARE on - policy option methods. on - policy option methods COMPARE flat policies. off - policy training CONJUNCTION backpropagation. backpropagation CONJUNCTION off - policy training. policy components USED-FOR backpropagation. dynamic programming inference procedure USED-FOR off - policy training. dynamic programming inference procedure USED-FOR backpropagation. HO2 COMPARE option learning methods. option learning methods COMPARE HO2. raw pixel inputs USED-FOR simulated robot manipulation tasks. intuitive extension USED-FOR temporal abstraction. OtherScientificTerm are abstractions, data - generating behavior policy, trust - region constraints, and pre - trained options. Method are policy optimization, off - policy optimization, and action and temporal abstraction. Task is off - policy option learning. ",This paper proposes a new method for off-policy option learning in reinforcement learning. The proposed method is based on a combination of Hindsight Off-Policy Options (HOTO) and Hindsight Backpropagation (HBP). Theoretical results show that the proposed method achieves better performance than the state-of-the-art option learning methods in simulated robot manipulation tasks.   ,This paper proposes a new off-policy option learning algorithm based on Hindsight Off-Policy Options (HO2) framework. The main idea is to use temporal and action abstraction in the option framework to improve the data efficiency of the method. The proposed method is evaluated on simulated robot manipulation tasks with raw pixel inputs. 
1179,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,Reinforcement learning ( RL ) algorithms USED-FOR maximizing the expected cumulative return. drug discovery HYPONYM-OF applications. objective function USED-FOR expected maximum reward. functional form USED-FOR Bellman equation. Bellman operators USED-FOR functional form. formulation USED-FOR synthesizable molecule generation. real - world drug discovery pipeline FEATURE-OF synthesizable molecule generation. Generic is framework. Method is RL agent. OtherScientificTerm is expected cumulative return. ,"This paper studies the problem of maximizing the expected cumulative return (ECR) in reinforcement learning. In particular, the authors propose to use the Bellman equation to represent the expected maximum reward as a functional form of the expected Bellman operator. The authors show that this functional form can be expressed in terms of a Bellman function, which is then used to define an RL algorithm that can be used to maximize the expected ECR.  The authors then show that their method can be applied to synthesizable molecule generation problems. ","This paper studies the problem of maximizing the expected cumulative return (ECR) of an RL agent in the context of drug discovery. The authors propose a new formulation of the Bellman equation, where the expected maximum reward is defined as the sum of the expected total reward and the cumulative return. They show that this formulation can be applied to synthesizable molecule generation in a real-world drug discovery pipeline. They also provide a theoretical analysis of their formulation."
1188,SP:bd4b1781448def4327214c78f07538d285119ef9,"neural networks USED-FOR fixed output dimension. neural network architectures USED-FOR output features. neural networks USED-FOR features. Contextual HyperNetwork ( CHN ) HYPONYM-OF auxiliary model. base model USED-FOR feature. CHN COMPARE re - training and fine - tuning approaches. re - training and fine - tuning approaches COMPARE CHN. neural network USED-FOR CHN. CHN USED-FOR partial variational autoencoder ( P - VAE ). partial variational autoencoder ( P - VAE ) HYPONYM-OF deep generative model. deep generative model USED-FOR missing features. missing features PART-OF sparsely - observed data. CHN USED-FOR CHNs. e - learning CONJUNCTION healthcare tasks. healthcare tasks CONJUNCTION e - learning. system COMPARE imputation and meta - learning baselines. imputation and meta - learning baselines COMPARE system. recommender systems CONJUNCTION e - learning. e - learning CONJUNCTION recommender systems. imputation and meta - learning baselines USED-FOR recommender systems. few - shot learning USED-FOR features. system USED-FOR features. few - shot learning EVALUATE-FOR imputation and meta - learning baselines. healthcare tasks EVALUATE-FOR system. recommender systems EVALUATE-FOR system. few - shot learning EVALUATE-FOR system. Method is deep learning. Task are online learning settings, and recommender system. ","This paper proposes Contextual HyperNets (CHNets), an auxiliary model for sparsely-observed data with missing features. The proposed method is based on a partial variational autoencoder (P-VAE), which is a generative model for missing features in sparsely observed data. The authors show that the proposed method outperforms the baselines on few-shot learning tasks.   ","This paper proposes Contextual HyperNetwork (CHN), an auxiliary model for the contextual hypernetworks (CHNs). The authors propose to use a partial variational autoencoder (P-VAE) model to generate missing features in sparsely-observed data. The authors show that the proposed CHN can be used to improve few-shot learning in the context of recommender systems. The proposed method is evaluated on a variety of healthcare tasks. "
1197,SP:8e4677cc6071a33397347679308165c10dca2aae,data inefficiency CONJUNCTION catastrophic forgetting. catastrophic forgetting CONJUNCTION data inefficiency. Bayesian paradigm USED-FOR deep learning. poor calibration CONJUNCTION data inefficiency. data inefficiency CONJUNCTION poor calibration. Bayesian inference USED-FOR high - dimensional parameter spaces. high - dimensional parameter spaces FEATURE-OF deep neural networks. deep neural networks USED-FOR Bayesian inference. restrictive approximations USED-FOR Bayesian inference. model parameters USED-FOR inference. expressive posterior approximations USED-FOR full model. Bayesian deep learning method USED-FOR full covariance Gaussian posterior approximation. Bayesian deep learning method USED-FOR point estimate. subnetwork USED-FOR full covariance Gaussian posterior approximation. subnetwork selection procedure USED-FOR posterior uncertainty. approach COMPARE point - estimated networks. point - estimated networks COMPARE approach. approach COMPARE methods. methods COMPARE approach. full network USED-FOR expressive posterior approximations. expressive posterior approximations USED-FOR methods. OtherScientificTerm is point estimates. ,This paper proposes a Bayesian deep learning method for Bayesian inference in deep neural networks. The proposed method is based on a subnetwork selection procedure to select the posterior uncertainty for each layer in the network. The method is evaluated on a variety of Bayesian image classification tasks.   ,"This paper proposes a new Bayesian deep learning method for expressive posterior approximations for deep neural networks. The proposed method is based on a subnetwork selection procedure, where each subnetwork is trained to approximate the full covariance Gaussian posterior approximation of the full model. The authors show that the proposed method outperforms the state-of-the-art in terms of data inefficiency, catastrophic forgetting, and calibration."
1206,SP:be361952fe9de545f68b8a060f790d54c6755998,generalization CONJUNCTION applicability. applicability CONJUNCTION generalization. generalization EVALUATE-FOR embedding techniques. state representations USED-FOR Model - free reinforcement learning approaches. approach USED-FOR jointly learning embeddings. model USED-FOR embeddings. generic architecture USED-FOR policy. these USED-FOR policy. these USED-FOR generic architecture. embedded representations USED-FOR generalization. approach USED-FOR embedded representations. it COMPARE models. models COMPARE it. approach COMPARE it. it COMPARE approach. gaming EVALUATE-FOR it. recommender systems EVALUATE-FOR it. approach COMPARE models. models COMPARE approach. state / action spaces FEATURE-OF discrete / continuous domains. discrete / continuous domains EVALUATE-FOR models. discrete / continuous domains EVALUATE-FOR it. recommender systems EVALUATE-FOR approach. gaming EVALUATE-FOR approach. Method is reinforcement learning. Generic is approaches. Material is discrete and continuous domains. OtherScientificTerm is embedding spaces. ,"This paper proposes a model-free reinforcement learning approach that jointly learns embeddings for discrete and continuous state and action spaces. The proposed approach is based on the idea that the embedding space can be jointly learned by a model and a policy. The model is trained by jointly learning embedding representations for the state and the action space, and the policy is trained to maximize the mutual information between the two. Experiments on recommender systems show that the proposed approach achieves state-of-the-art performance.   ",This paper proposes a model-free reinforcement learning approach to jointly learn embeddings for discrete and continuous state-action spaces. The authors propose a generic architecture for embedding the state representations of a policy in a discrete/continuous state space. They show that their approach can achieve better generalization performance than the state-of-the-art in terms of generalization and applicability. They also show that the proposed approach can be applied to a variety of settings. 
1215,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"viewmaker networks HYPONYM-OF generative models. Viewmakers HYPONYM-OF stochastic bounded adversaries. they USED-FOR views. cropping CONJUNCTION color jitter. color jitter CONJUNCTION cropping. transfer accuracy EVALUATE-FOR welltuned SimCLR augmentations. color jitter HYPONYM-OF transformations. cropping HYPONYM-OF transformations. speech recordings CONJUNCTION wearable sensor data. wearable sensor data CONJUNCTION speech recordings. wearable sensor data EVALUATE-FOR baseline augmentations. speech recordings EVALUATE-FOR baseline augmentations. Viewmaker views CONJUNCTION handcrafted views. handcrafted views CONJUNCTION Viewmaker views. transfer performance EVALUATE-FOR they. robustness EVALUATE-FOR they. viewmakers USED-FOR representation learning algorithms. Viewmaker networks USED-FOR unsupervised learning. Viewmaker networks USED-FOR complex and diverse input - dependent views. complex and diverse input - dependent views USED-FOR unsupervised learning. Task is unsupervised representation learning. Generic is models. Method is unsupervised representation learning methods. OtherScientificTerm are ` p - bounded perturbation, common image corruptions, and domain expertise. Material is CIFAR-10. ","This paper proposes a method for unsupervised representation learning in the presence of image corruptions. The proposed method is based on viewmaker networks, a generative model that is trained to generate a set of views for each input image. The viewmaker network is trained with an adversarial perturbation. The authors show that the proposed method outperforms the state-of-the-art methods in terms of transfer performance on CIFAR-10 and SimCLR.","This paper proposes a viewmaker network for unsupervised representation learning. The viewmaker is a generative model that learns a set of views for each input image. The authors show that the viewmaker can be used to learn representations that are robust to common image corruptions. They also show that it can be applied to a variety of image transformations, such as cropping, color jitter, and handcrafted views.  "
1224,SP:ef7735be9423ad53059505c170e75201ca134573,"autonomous driving CONJUNCTION air traffic management. air traffic management CONJUNCTION autonomous driving. deep learning models USED-FOR high - assurance systems. air traffic management CONJUNCTION medical diagnosis. medical diagnosis CONJUNCTION air traffic management. medical diagnosis HYPONYM-OF high - assurance systems. autonomous driving HYPONYM-OF high - assurance systems. air traffic management HYPONYM-OF high - assurance systems. statistical, geometric, or topological signatures USED-FOR techniques. detection approaches USED-FOR outliers. KMNIST CONJUNCTION F - MNIST. F - MNIST CONJUNCTION KMNIST. CIFAR10 ( for SVHN ) CONJUNCTION KMNIST. KMNIST CONJUNCTION CIFAR10 ( for SVHN ). SVHN CONJUNCTION MNIST. MNIST CONJUNCTION SVHN. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. Imagenet CONJUNCTION LSUN. LSUN CONJUNCTION Imagenet. WideResNet CONJUNCTION DenseNet. DenseNet CONJUNCTION WideResNet. DenseNet CONJUNCTION LeNet5. LeNet5 CONJUNCTION DenseNet. in - distribution data CONJUNCTION Imagenet. Imagenet CONJUNCTION in - distribution data. ResNet34 CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNet34. F - MNIST USED-FOR OOD data. F - MNIST HYPONYM-OF DNN architectures. SVHN USED-FOR in - distribution data. MNIST CONJUNCTION Imagenet. Imagenet CONJUNCTION MNIST. MNIST USED-FOR in - distribution data. SVHN CONJUNCTION Imagenet. Imagenet CONJUNCTION SVHN. ResNet34 HYPONYM-OF DNN architectures. LeNet5 HYPONYM-OF DNN architectures. DenseNet HYPONYM-OF DNN architectures. WideResNet HYPONYM-OF DNN architectures. Method are Deep neural networks ( DNNs ), and integrated","This paper proposes a method to detect outliers in image data from deep neural networks (DNNs) using geometric or topological signatures. The proposed method is based on the observation that outliers can be detected using statistical, geometric, and topological signature. The method is evaluated on CIFAR-10, SVHN, Imagenet, and F-MNIST datasets. The results show that the method is effective in detecting outliers. ","This paper proposes a method to detect outliers in data from deep neural networks (DNNs) using MNIST, F-MNIST, Imagenet, and LSUN. The main idea is to use the in-distribution data to identify outliers, and the OOD data to find outliers. The method is based on a combination of statistical, geometric, and topological signatures. The authors show that the proposed method can detect outlier data from DNNs, and that it can also detect out of distribution data. They also show that their method is able to detect out-of-distributions data from in- distribution data as well. "
1233,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"hierarchical VAE COMPARE PixelCNN. PixelCNN COMPARE hierarchical VAE. log - likelihood EVALUATE-FOR PixelCNN. natural image benchmarks EVALUATE-FOR PixelCNN. VAEs USED-FOR autoregressive models. VAEs USED-FOR models. autoregressive models COMPARE VAEs. VAEs COMPARE autoregressive models. loglikelihood EVALUATE-FOR autoregressive models. loglikelihood EVALUATE-FOR VAEs. ImageNet CONJUNCTION FFHQ. FFHQ CONJUNCTION ImageNet. stochastic depth FEATURE-OF VAE. PixelCNN COMPARE VAEs. VAEs COMPARE PixelCNN. likelihoods EVALUATE-FOR VAEs. VAE USED-FOR hierarchical visual representations. FFHQ-256 USED-FOR VAE. VAEs USED-FOR global features. VAEs USED-FOR local details. multiscale generative procedure COMPARE PixelCNN. PixelCNN COMPARE multiscale generative procedure. log - likelihood EVALUATE-FOR PixelCNN. log - likelihood EVALUATE-FOR multiscale generative procedure. Generic is they. OtherScientificTerm are insufficient depth, and Low resolution High resolution. Material is high - resolution images. Method is generative process. ","This paper proposes a VAE-based generative model for image generation. The proposed method is based on a hierarchical VAE, where each pixel is represented by a local embedding of the image and a global embedding is used to represent the global features. The method is evaluated on ImageNet and FFHQ, where the proposed method achieves state-of-the-art results.   ","This paper proposes a new hierarchical VAE-based generative model called PixelCNN. The proposed method is based on the idea that VAEs can be used to improve the performance of autoregressive models. The main contribution of the paper is that the proposed method can be applied to both high-resolution and low-resolution images. The method is evaluated on ImageNet and FFHQ datasets, and it is shown that it outperforms the state-of-the-art in terms of likelihood."
1242,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"methods USED-FOR unsupervised visual representations. uninformative examples PART-OF this. randomly sampled negative examples USED-FOR NCE. semi - hard negatives USED-FOR contrastive representations. bias CONJUNCTION variance. variance CONJUNCTION bias. estimators COMPARE NCE. NCE COMPARE estimators. variance EVALUATE-FOR NCE. CMC CONJUNCTION MoCo. MoCo CONJUNCTION CMC. IR CONJUNCTION CMC. CMC CONJUNCTION IR. image benchmarks EVALUATE-FOR linear evaluation. models USED-FOR approach. IR HYPONYM-OF models. linear evaluation EVALUATE-FOR approach. MoCo HYPONYM-OF models. accuracy EVALUATE-FOR approach. image benchmarks EVALUATE-FOR approach. CMC HYPONYM-OF models. instance segmentation CONJUNCTION key - point detection. key - point detection CONJUNCTION instance segmentation. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. features USED-FOR image distributions. object detection CONJUNCTION key - point detection. key - point detection CONJUNCTION object detection. features USED-FOR downstream tasks. Meta - Dataset collection FEATURE-OF image distributions. key - point detection HYPONYM-OF downstream tasks. object detection HYPONYM-OF downstream tasks. instance segmentation HYPONYM-OF downstream tasks. Method are contrastive learning, metric learning, and mutual information estimators. OtherScientificTerm are noise - contrastive estimation ( NCE ) bound, mutual information, and lower - bounds of mutual information. ","This paper proposes to use semi-hard negatives as negative examples for contrastive learning. The idea is that the negative examples can be used to improve the estimation of the mutual information between positive and negative samples. The proposed method is evaluated on image classification, key-point detection, and instance segmentation tasks. ","This paper proposes a new contrastive learning method for unsupervised visual representation learning. The proposed method is based on the idea of semi-hard negatives, where the negative samples are randomly sampled from a set of negative examples. The authors show that the proposed method outperforms the state-of-the-art noise-contrastive estimation (NCE) method in terms of variance, bias, and variance variance. They also provide a lower-bound of the mutual information between the two estimators, which they call mutual information estimators."
1251,SP:613a0e2d8cbe703f37c182553801be7537333f64,"gradient sharing mechanism USED-FOR machine learning systems. gradient sharing mechanism USED-FOR Private training data. federated learning ( FL ) HYPONYM-OF machine learning systems. data leakage attack USED-FOR batch data. shared aggregated gradients USED-FOR batch data. catastrophic data leakage PART-OF federated learning ( CAFE ). data leakage attacks COMPARE CAFE. CAFE COMPARE data leakage attacks. CAFE USED-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR large - batch data leakage attack. data recovery quality EVALUATE-FOR CAFE. CAFE USED-FOR private data. vertical and horizontal FL settings EVALUATE-FOR CAFE. shared aggregated gradients USED-FOR private data. vertical case HYPONYM-OF FL. data leakage risks FEATURE-OF learning settings. OtherScientificTerm are batch size, data leakage, and training gradients. Generic is method. ",This paper proposes a new method to mitigate the catastrophic data leakage in federated learning. The proposed method is based on aggregating the gradients of the training data from different clients and aggregating them into a shared aggregated gradients for the batch data. The method is evaluated on both vertical and horizontal FL settings and shows that it is able to recover the private data in both cases. ,This paper proposes a new method to mitigate the catastrophic data leakage in federated learning (CAFE) attacks. The proposed method is based on the idea of aggregating aggregated gradients for batch data. The authors show that the proposed method can recover the data from a large batch data leakage attack. They also show that CAFE can recover data in vertical and horizontal FL settings. 
1260,SP:ce229295081ff04b26f33829f2c3396b90897b5d,physics CONJUNCTION vision. vision CONJUNCTION physics. vision CONJUNCTION robotics. robotics CONJUNCTION vision. Unsupervised learning of interactions USED-FOR physics. physics CONJUNCTION robotics. robotics CONJUNCTION physics. Unsupervised learning of interactions USED-FOR vision. Unsupervised learning of interactions USED-FOR robotics. multi - agent trajectories USED-FOR Unsupervised learning of interactions. neural relational inference USED-FOR static relations. deep generative model USED-FOR dynamic relations. simulated physics system USED-FOR dynamic relation scenarios. periodic and additive dynamics HYPONYM-OF dynamic relation scenarios. training scheme CONJUNCTION model architecture. model architecture CONJUNCTION training scheme. dynamic relational inference accuracy EVALUATE-FOR model architecture. model USED-FOR coordination and competition patterns. real - world multi - agent basketball trajectories USED-FOR model. real - world multi - agent basketball trajectories USED-FOR coordination and competition patterns. Task is dynamic relational inference. OtherScientificTerm is interactions. ,This paper proposes a method for unsupervised learning of interactions between agents in a multi-agent physics system. The proposed method is based on a generative model that learns dynamic relations between the agents. The model is trained using a combination of supervised learning and a dynamic relational inference approach. The method is evaluated on a simulated physics system and a real-world basketball environment.  ,This paper proposes a generative model for dynamic relational inference. The model is trained on a simulated physics system and a real-world multi-agent basketball environment. The proposed model is able to learn dynamic relations between agents in a dynamic way. The authors show that the model can learn the dynamics of a dynamic relation in periodic and additive dynamics. They also show that their model can also learn the dynamic relations in a static setting.
1269,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"Collaborative filtering USED-FOR predicting potential user - item ratings. latent factors PART-OF user - item rating matrix. transductive setting USED-FOR user - specific latent factors. inductive collaborative filtering framework USED-FOR hidden relational graph. rating matrix USED-FOR hidden relational graph. model USED-FOR inductively computing user - specific representations. expressiveness EVALUATE-FOR feature - driven inductive models. model COMPARE feature - driven inductive models. feature - driven inductive models COMPARE model. feature USED-FOR inductively computing user - specific representations. model COMPARE transductive models. transductive models COMPARE model. model USED-FOR inductive learning. cold - start users EVALUATE-FOR them. matrix completion benchmarks EVALUATE-FOR inductive learning. matrix completion benchmarks EVALUATE-FOR model. OtherScientificTerm are user - item ratings, dense weighted graphs, historical rating patterns, relational graphs, and latent space. Method are base matrix factorization model, and relation inference model. ",This paper proposes an inductive collaborative filtering framework for predicting potential user-item ratings. The proposed method is based on the idea that latent factors in the rating matrix can be decomposed into user-specific latent factors and a relation inference model. The model is evaluated on a number of matrix completion benchmarks and achieves state-of-the-art performance.,"This paper proposes an inductive collaborative filtering framework for predicting potential user-item ratings from a hidden relational graph. The proposed method is based on the transductive setting, where the latent factors of the user- item rating matrix are extracted from a user-specific latent space. The model is trained using a base matrix factorization model, and a relation inference model is used to infer the relation between the latent space and the latent matrix. Experiments show that the proposed method outperforms the state-of-the-art in terms of expressiveness on matrix completion benchmarks."
1278,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"autoencoder - based disentangled representation learning methods USED-FOR disentanglement. disentangled representation learning CONJUNCTION reconstruction quality. reconstruction quality CONJUNCTION disentangled representation learning. detail information FEATURE-OF image data. correlated latent variables USED-FOR detail information. deep generative model USED-FOR missing correlated latent variables. deep generative model USED-FOR low - quality reconstruction. β - TCVAE HYPONYM-OF disentangled representation learning method. disentangled representation learning method USED-FOR disentangled factors. normalizing flows CONJUNCTION mixtures of Gaussians. mixtures of Gaussians CONJUNCTION normalizing flows. likelihood - based models CONJUNCTION implicit models. implicit models CONJUNCTION likelihood - based models. implicit models CONJUNCTION tractable models. tractable models CONJUNCTION implicit models. variational autoencoders CONJUNCTION implicit models. implicit models CONJUNCTION variational autoencoders. generative adversarial networks HYPONYM-OF implicit models. variational autoencoders HYPONYM-OF likelihood - based models. tractable models HYPONYM-OF model classes. likelihood - based models HYPONYM-OF model classes. mixtures of Gaussians HYPONYM-OF tractable models. implicit models HYPONYM-OF model classes. normalizing flows HYPONYM-OF tractable models. multi - stage model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE multi - stage model. reconstruction quality EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR state - of - the - art methods. disentanglement EVALUATE-FOR multi - stage model. reconstruction quality EVALUATE-FOR multi - stage model. OtherScientificTerm are statistical independence of the latent factors, and D - separation. Generic are approach, and model. Method is multi - stage modelling approach. ","This paper proposes a new method for learning disentangled representation learning from image data. The proposed method is based on a mixture of variational autoencoders (VAEs) and implicit models (GANs). The main idea is to use a latent variable model to model the disentanglement between the latent factors, and then use a deep generative model to reconstruct the latent variables from the reconstructed data.    The main contribution of the paper is to propose a novel method for disentangling latent factors from the reconstruction data, which is called D-separation. The authors show that the proposed method achieves state-of-the-art performance in terms of reconstruction quality on image datasets. ","This paper proposes a new disentangled representation learning method for image disentanglement. The key idea is to use a deep generative model to generate a low-quality reconstruction of the image, which is then used to disentangle the latent factors of the data. The authors show that their method can achieve state-of-the-art performance in terms of reconstruction quality and disentangling the factors. "
1287,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ",This paper studies the problem of learning representations of data in reinforcement learning. The authors propose to use mutual information maximization (MI) to learn representations of high-dimensional observations. The main idea is to maximize the mutual information between the state representation and the optimal policy. The proposed method is evaluated on a variety of simulated environments and compared to a number of baselines.,"This paper studies the problem of learning representations of data in reinforcement learning (RL). The authors propose a new objective called mutual information maximization (MI) maximization, which is based on the notion of ""mutual information"" (MI). MI maximizes the mutual information between the two parties in a MDP (i.e., the MDP is represented as a set of high-dimensional observations). They show that this objective can be used to learn representations of high dimensional observations, which are then used to improve the performance of a policy. The authors also show that MI can be applied to a variety of MDPs (e.g., a toy environment, a real-world environment, and a simulated environment).  "
1296,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"finite dimensional representation FEATURE-OF semi - infinite dual. finite - dimensional convex copositive program USED-FOR non - convex neural network training problem. global optima CONJUNCTION copositive programs. copositive programs CONJUNCTION global optima. neural networks CONJUNCTION copositive programs. copositive programs CONJUNCTION neural networks. neural networks USED-FOR global optima. neural networks USED-FOR copositive programs. semi - nonnegative matrix factorization USED-FOR neural networks. semi - nonnegative matrix factorization USED-FOR copositive programs. algorithms USED-FOR global minimum. algorithms USED-FOR vector output neural network training problem. global minimum FEATURE-OF vector output neural network training problem. computational complexity EVALUATE-FOR filter size. computational complexity EVALUATE-FOR convolutional architectures. global optimum FEATURE-OF neural network training problem. soft - thresholded SVD USED-FOR neural network training problem. OtherScientificTerm is convex semi - infinite dual. Method are copositive relaxation, and Stochastic Gradient Descent. ",This paper studies the problem of learning a non-convex convex program from a finite-dimensional representation of a convex semi-infinite dual of a neural network. The authors show that the global optima of the neural network training problem can be approximated by a copositive relaxation of the convex dual. They show that this relaxation can be achieved by using a semi-nonnegative matrix factorization of the input matrix. They also show that a soft-thresholded SVD can be used to approximate the global optimum of the training problem.,"This paper studies the problem of learning a convex semi-infinite dual of a finite-dimensional convex copositive program from a non-convex neural network training problem. In particular, the authors show that the global minimum of a neural network trained on a finite dimensional convex program is a function of the number of parameters of the neural network. The authors also provide a soft-thresholded SVD algorithm for solving the problem. "
1305,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"vision USED-FOR learning disentangled, object - centric scene representations. unsupervised object segmentation USED-FOR LORL. MONet and Slot Attention HYPONYM-OF unsupervised object segmentation. algorithms USED-FOR object - centric representation. properties CONJUNCTION spatial relationships. spatial relationships CONJUNCTION properties. object categories CONJUNCTION properties. properties CONJUNCTION object categories. representations USED-FOR concepts. object categories HYPONYM-OF concepts. spatial relationships HYPONYM-OF concepts. properties HYPONYM-OF concepts. object - centric concepts USED-FOR object - centric representations. language USED-FOR object - centric concepts. LORL CONJUNCTION unsupervised segmentation algorithms. unsupervised segmentation algorithms CONJUNCTION LORL. LORL USED-FOR object segmentation. LORL USED-FOR MONet and Slot Attention. MONet and Slot Attention USED-FOR object segmentation. language USED-FOR LORL. concepts USED-FOR tasks. LORL CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION LORL. concepts CONJUNCTION segmentation algorithms. segmentation algorithms CONJUNCTION concepts. segmentation algorithms USED-FOR tasks. LORL USED-FOR concepts. MONet HYPONYM-OF segmentation algorithms. referring expression comprehension HYPONYM-OF tasks. OtherScientificTerm is language input. ","This paper proposes a method for learning disentangled, object-centric scene representations from images. The method is based on the idea of using language to encode object-related concepts into the representation, which are then used for object-based object segmentation. The proposed method, called LORL, is able to learn object-specific concepts from images, and then uses these concepts to improve the performance of unsupervised segmentation algorithms.  ","This paper proposes a language-based framework for learning disentangled, object-centric scene representations. The key idea is to learn a set of concepts that can be used to represent objects in the scene, and then use these concepts to learn unsupervised object segmentation algorithms. The proposed framework is tested on a variety of tasks, and it is shown that it is able to learn disentanglement and segmentation. "
1314,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"fact triplets PART-OF KG. fact triplets USED-FOR embedding methods. logic rules USED-FOR rich background information. rules USED-FOR reasoning. EM - RBR USED-FOR multi - relation reasoning link prediction. relational background knowledge USED-FOR multi - relation reasoning link prediction. rules FEATURE-OF relational background knowledge. relational background knowledge USED-FOR EM - RBR. FB15k CONJUNCTION WN18. WN18 CONJUNCTION FB15k. EM - RBR COMPARE models. models COMPARE EM - RBR. dataset EVALUATE-FOR model. WN18 EVALUATE-FOR models. FB15k EVALUATE-FOR EM - RBR. FB15k EVALUATE-FOR models. Task are Knowledge graph completion, and representation of the knowledge graph. OtherScientificTerm are knowledge graph, algebraic space, and relational patterns. Method are embedding models, and embedding. Generic is framework. Metric is prediction accuracy. Material is FB15k - R. ","This paper proposes a new embedding method for knowledge graph completion based on fact triplets. The proposed method is based on the fact triplet embedding model, where the embeddings are constructed using a set of logic rules. The method is evaluated on FB15k and WN18, where it achieves state-of-the-art performance. ","This paper proposes a new embedding model for multi-relational reasoning link prediction. The proposed model is based on the fact-triplet embedding method, which embeds the fact triplets in the knowledge graph. The key idea is to use the relational background knowledge of the embeddings in the KG as well as the logic rules to extract the rich background information. The authors show that the proposed model outperforms the state-of-the-art on FB15k-R and WN18 datasets."
1323,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"video game HYPONYM-OF structured, dynamic environment. procedural knowledge USED-FOR Black - box models. monolithic hidden state USED-FOR Black - box models. modularity FEATURE-OF knowledge. architecture USED-FOR declarative and procedural knowledge. schemata USED-FOR state updates. active modules CONJUNCTION passive external knowledge sources. passive external knowledge sources CONJUNCTION active modules. passive external knowledge sources USED-FOR state updates. object files HYPONYM-OF active modules. schemata HYPONYM-OF passive external knowledge sources. active modules PART-OF architecture. attention USED-FOR object files. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. input - output interface FEATURE-OF drop - in replacement. drop - in replacement PART-OF architecture. LSTM HYPONYM-OF normal recurrent networks. GRU HYPONYM-OF normal recurrent networks. OtherScientificTerm are declarative knowledge, systematicity, and object tokens. Generic is they. Metric is generalization. Material is intuitive physics benchmark. ","This paper proposes a method for learning procedural knowledge in video games. The method is based on a combination of two existing methods: (1) object files, which are external knowledge sources, and (2) schemata, which is a set of object tokens that are stored in the state space. The authors show that the proposed method is able to learn both declarative and procedural knowledge, and that it can generalize better than existing methods.    The main contribution of this paper is to propose a method to learn procedural knowledge from object files. The proposed method consists of two modules: (i) an active module that generates object tokens and (ii) a passive module that stores object tokens. The active module is responsible for generating the object tokens, while the passive module is used to store the state updates. ","This paper proposes a new method for learning declarative and procedural knowledge in a video game environment. The main idea is to use a combination of active modules and passive external knowledge sources to generate schemata that can be used to update the state of the game. The proposed method is evaluated on a variety of datasets, and compared to a number of baselines.  "
1332,SP:42a3c0453ab136537b5944a577d63412f3c22560,"Neural module networks ( NMN ) USED-FOR image - grounded tasks. synthetic images USED-FOR Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) HYPONYM-OF image - grounded tasks. NMN USED-FOR video - grounded language tasks. complexity EVALUATE-FOR visual tasks. tasks COMPARE visual tasks. visual tasks COMPARE tasks. visual temporal variance FEATURE-OF visual tasks. complexity EVALUATE-FOR tasks. NMN approaches USED-FOR image - grounded tasks. information retrieval process USED-FOR video - grounded language tasks. VilNMN USED-FOR action - based inputs. VilNMN USED-FOR language components. video QA CONJUNCTION video - grounded dialogues. video - grounded dialogues CONJUNCTION video QA. video - grounded language tasks EVALUATE-FOR VilNMN. video - grounded dialogues HYPONYM-OF video - grounded language tasks. video QA HYPONYM-OF video - grounded language tasks. Method are neural modules, and neural module networks. OtherScientificTerm is visual cues. ","This paper proposes VilNMN, a neural module network architecture for video-grounded language tasks. The architecture consists of two modules: one that takes an image as input and processes it into an action-based input, and another that processes the visual input into a language component. The proposed architecture is evaluated on VQA and dialog tasks, where it achieves state-of-the-art performance.  ","This paper proposes VilNMN, a neural module network (NMN) for video-grounded language tasks. The proposed method is based on the idea that the visual components of a language task can be represented as a sequence of visual cues. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks, including visual question answering, video QA, and video-based dialogues."
1341,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"empirical game analysis CONJUNCTION deep reinforcement learning ( Deep RL ). deep reinforcement learning ( Deep RL ) CONJUNCTION empirical game analysis. Deep RL USED-FOR best response. mixture of opponent policies USED-FOR best response. PSRO USED-FOR Deep RL training. algorithms USED-FOR PSRO. PSRO USED-FOR policies. policies PART-OF empirical game. Mixed - Opponents USED-FOR pure - strategy opponent. strategy ’s action - value estimates COMPARE policies. policies COMPARE strategy ’s action - value estimates. strategy ’s action - value estimates USED-FOR pure - strategy opponent. algorithms USED-FOR PSRO. algorithms USED-FOR game. Method are Policy - Space Response Oracles ( PSRO ), and Mixed - Oracles. Task is learning policies in multiagent systems. Generic are algorithm, first, second, and policy. OtherScientificTerm is unobserved distribution of opponents. ",This paper studies the problem of learning policies in multi-agent reinforcement learning in the presence of an unknown distribution of opponents. The authors propose a method called Policy-Space Response Oracles (PSRO) to estimate the best response from a mixture of opponent policies. The main idea is to learn a policy-space response oracle that estimates the action-value of the best opponent policy from the unobserved distribution of policies.  The authors show that the PSRO can be used to learn policies that are close to the true best response in the empirical game. They also show that PSRO is computationally efficient.  ,"This paper proposes a method for learning a policy-space response (PSRO) for multi-agent reinforcement learning (MRL). The PSRO is based on the notion of ""mixed-opponent"" (i.e., a mixture of opponent policies) that is used to estimate the best response for each agent in an empirical game. The authors show that the PSRO can be used to learn the best policy for a MRL game, and that it can also be used for deep RL training. They also provide an algorithm for learning the best PSRO.  "
1350,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,attention mechanism CONJUNCTION duration predictor. duration predictor CONJUNCTION attention mechanism. Tacotron 2 text - to - speech model USED-FOR Non - Attentive Tacotron. unaligned duration ratio CONJUNCTION word deletion rate. word deletion rate CONJUNCTION unaligned duration ratio. metrics USED-FOR large - scale robustness evaluation. pre - trained speech recognition model USED-FOR metrics. pre - trained speech recognition model USED-FOR large - scale robustness evaluation. Gaussian upsampling USED-FOR Non - Attentive Tacotron. Non - Attentive Tacotron COMPARE Tacotron 2. Tacotron 2 COMPARE Non - Attentive Tacotron. 5 - scale mean opinion score USED-FOR naturalness. 5 - scale mean opinion score EVALUATE-FOR Non - Attentive Tacotron. fine - grained variational auto - encoder USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR duration predictor. semi - supervised or unsupervised manner USED-FOR method. fine - grained variational auto - encoder USED-FOR method. Metric is robustness. Method is supervised training. ,otron 2 text-to-speech model is trained with attention mechanism and duration predictor. This paper proposes a novel non-attentionive Tacotron model with Gaussian upsampling to improve robustness evaluation. The proposed method is evaluated on 5-scale mean opinion score for naturalness.,"-to-speech model (Tacotron 2) is used to train a non-attentive version of Tacotron. The authors propose a new metric for evaluating the robustness of the Tacoton model, which is based on the unaligned duration ratio, word deletion rate, and duration predictor. They also propose a Gaussian upsampling method for the duration predictor, and a fine-grained variational auto-encoder for the attention mechanism. The proposed method is evaluated on a variety of datasets, and it is shown to be more robust than the original model. "
1359,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"layout estimation USED-FOR planning and navigation. planning and navigation USED-FOR robotics applications. layout estimation USED-FOR robotics applications. self - driving HYPONYM-OF robotics applications. supervised end - to - end framework USED-FOR estimation of bird ’s eye view layout. deep learning networks USED-FOR disparity estimation. deep learning networks USED-FOR network. internal bird ’s eye view feature representation USED-FOR layout estimation. stereo images USED-FOR features. features USED-FOR disparity feature volume. stereo images USED-FOR disparity feature volume. scene structure FEATURE-OF coarse - grained information. rich bird ’s eye view representation USED-FOR spatial reasoning. IPM features USED-FOR rich bird ’s eye view representation. IPM features CONJUNCTION projected feature volume. projected feature volume CONJUNCTION IPM features. projected feature volume USED-FOR rich bird ’s eye view representation. representation USED-FOR BEV semantic map. IPM features USED-FOR supervisory signal. supervisory signal USED-FOR stereo features. IPM features USED-FOR stereo features. datasets EVALUATE-FOR approach. synthetically generated dataset EVALUATE-FOR approach. synthetically generated dataset HYPONYM-OF datasets. datasets EVALUATE-FOR baseline techniques. Method are explicit depth estimation, and inverse perspective mapping ( IPM ). Generic is it. OtherScientificTerm are bird ’s eye view coordinates, bird ’s eye view, and fine - grained texture information. ","This paper proposes a method for bird’s eye view layout estimation. The method is based on inverse perspective mapping (IPM), which is a method to estimate the disparity between the bird's eye view coordinates and the ground truth coordinates in a scene. The proposed method is evaluated on two synthetic and two real-world datasets. ",This paper proposes an end-to-end framework for bird’s eye view layout estimation. The proposed method is based on the inverse perspective mapping (IPM) framework. The authors propose a deep learning-based method to estimate the disparity feature volume between the bird's eye view and stereo features. The method is evaluated on a synthetic and real-world dataset. 
1368,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,over - parameterization CONJUNCTION over - smoothing. over - smoothing CONJUNCTION over - parameterization. vanishing gradients CONJUNCTION over - parameterization. over - parameterization CONJUNCTION vanishing gradients. Relational Graph Neural Networks ( GNN ) COMPARE GNNs. GNNs COMPARE Relational Graph Neural Networks ( GNN ). methods USED-FOR GNNs. normalization techniques CONJUNCTION skip connection. skip connection CONJUNCTION normalization techniques. normalization techniques PART-OF methods. learning long - range patterns FEATURE-OF multi - relational graphs. GNNs USED-FOR learning long - range patterns. relation - aware GNN architecture USED-FOR long - range modeling between nodes. vector - based approach USED-FOR relation - aware GNN architecture. gated skip connections USED-FOR relation - aware GNN architecture. Graph Attention Network USED-FOR relation - aware GNN architecture. method COMPARE architectures. architectures COMPARE method. method COMPARE GNN variants. GNN variants COMPARE method. GNN variants COMPARE architectures. architectures COMPARE GNN variants. GNN variants USED-FOR deeper configurations. Method is deeper networks. Material is synthetic and real data. ,This paper proposes a novel GNN architecture for learning long-range patterns in multi-relational graphs. The authors propose a vector-based approach for relation-aware GNNs and a gated skip connection to improve the performance. The experimental results on synthetic and real-world datasets show the effectiveness of the proposed architecture. ,"This paper proposes a new GNN architecture for learning long-range patterns in multi-relational graphs. The main idea is to use a vector-based approach to model the relationships between nodes in the graph. The proposed architecture is based on the Graph Attention Network (GAT), which is an extension of GNNs with skip connections. The authors show that the proposed architecture outperforms other GNN variants on synthetic and real-world datasets."
1377,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"Symbolic techniques USED-FOR neural network properties. Satisfiability Modulo Theory ( SMT ) solvers USED-FOR Symbolic techniques. gradient - based methods CONJUNCTION symbolic techniques. symbolic techniques CONJUNCTION gradient - based methods. technique USED-FOR gradient - based methods. technique USED-FOR minimal regions. technique USED-FOR large networks. Integrated Gradients USED-FOR gradient information. gradient information USED-FOR approach. SMT constraints USED-FOR minimal input mask discovery problem. approach USED-FOR mask regions. approach USED-FOR minimal masks. ImageNet CONJUNCTION Beer Reviews. Beer Reviews CONJUNCTION ImageNet. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. saliency scores EVALUATE-FOR gradient - based methods. approach COMPARE gradient - based methods. gradient - based methods COMPARE approach. saliency scores EVALUATE-FOR approach. MNIST EVALUATE-FOR technique. Task are model explanation, and neural network ’s prediction. OtherScientificTerm are threshold, mask, and saliency map. ","This paper proposes a new method for learning saliency maps for neural networks. The proposed method is based on the Satisfiability Modulo Theory (SMT) solver. The main idea is to use SMT constraints to solve the minimal input mask discovery problem. The method is evaluated on ImageNet, Beer Reviews and MNIST.","This paper proposes a new method for solving the SMT solvers problem for symbolic neural network properties. The method is based on the Satisfiability Modulo Theory (SMT) solvers. The main idea is to use the input mask discovery problem to find the minimal input mask regions for each layer of the network. The proposed method is evaluated on MNIST, ImageNet, and Beer Reviews datasets. "
1386,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,3D pose estimation HYPONYM-OF computer vision. deep learning approaches USED-FOR 3D pose estimation. deep neural networks CONJUNCTION 3D generative representations of objects. 3D generative representations of objects CONJUNCTION deep neural networks. deep neural networks PART-OF unified neural architecture. 3D generative representations of objects PART-OF unified neural architecture. generative vision models USED-FOR partial occlusion. NeMo HYPONYM-OF unified neural architecture. generative model of neural feature activations PART-OF dense 3D mesh. generative model of neural feature activations USED-FOR NeMo. differentiable rendering USED-FOR 3D object pose. NeMo CONJUNCTION feature representation of the target image. feature representation of the target image CONJUNCTION NeMo. reconstruction error EVALUATE-FOR NeMo. feature representations PART-OF mesh. local optima FEATURE-OF reconstruction loss. feature extractor USED-FOR feature representations. contrastive learning USED-FOR feature representations. contrastive learning USED-FOR feature extractor. occluded - PASCAL3D+ CONJUNCTION ObjectNet3D. ObjectNet3D CONJUNCTION occluded - PASCAL3D+. PASCAL3D+ CONJUNCTION occluded - PASCAL3D+. occluded - PASCAL3D+ CONJUNCTION PASCAL3D+. NeMo COMPARE deep networks. deep networks COMPARE NeMo. NeMo USED-FOR partial occlusion. regular data EVALUATE-FOR NeMo. mesh representation USED-FOR object geometry. 3D geometry USED-FOR 3D pose estimation. cuboid USED-FOR mesh representation. mesh representation USED-FOR NeMo. cuboid USED-FOR object geometry. Generic is code. ,"This paper proposes a novel method for 3D pose estimation from images. The proposed method is based on a generative model of neural feature activations in a dense 3D mesh, which is then used to estimate the 3D object pose using a differentiable rendering. The method is evaluated on three image datasets and achieves state-of-the-art results.",This paper proposes a unified 3D generative model for 3D pose estimation. The main idea is to combine the features of the target image and the feature representation of the 3D mesh. The feature extractor is trained by contrastive learning to extract the feature representations from the target images. The reconstruction error of NeMo is based on a local optima of the reconstruction loss. Experiments show that NeMo outperforms the state-of-the-art on a variety of datasets. 
1395,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"large scale retrieval - based applications USED-FOR Feature Compatible Learning ( FCL ). approaches USED-FOR feature compatible learning. old training data CONJUNCTION classifiers. classifiers CONJUNCTION old training data. old training data USED-FOR approaches. classifiers USED-FOR approaches. approach USED-FOR feature compatible learning. features USED-FOR approach. unified framework USED-FOR FCL. model USED-FOR pseudo classifier. random walk algorithm USED-FOR it. model USED-FOR embedding features. ImageNet ILSVRC 2012 and Places365 data EVALUATE-FOR approach. Method are embedding model, and Non - Inherent Feature Compatible Learning. Generic is old model. ","-based retrieval-based applications require feature compatible learning (FCL) for large-scale retrieval. In this paper, the authors propose a unified framework for FCL. The proposed method is based on a pseudo classifier, which is trained using an embedding model and a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 show the effectiveness of the proposed method. ","This paper proposes a novel approach to feature compatible learning (FCL) for large-scale retrieval-based applications. The main idea is to use a pseudo-classifier trained on the old training data to learn the features of the new data. The pseudo classifier is trained using a random walk algorithm, where the data is retrieved from an embedding model, and the embedding features are used to train the pseudo classifiers. The proposed approach is evaluated on ImageNet ILSVRC 2012 and Places365 data."
1404,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"generalization error EVALUATE-FOR deep neural networks ( DNNs ). validation accuracy EVALUATE-FOR model selection. gradient norm USED-FOR model selection criterion. generalization error CONJUNCTION gradient norm measures. gradient norm measures CONJUNCTION generalization error. efficiency CONJUNCTION effectiveness. effectiveness CONJUNCTION efficiency. approximated gradient norm USED-FOR models. approximated gradient norm USED-FOR hyper - parameter search objectives. generalization error EVALUATE-FOR models. gradient norm CONJUNCTION generalization error. generalization error CONJUNCTION gradient norm. BOHB HYPONYM-OF bandit - based or population - based algorithms. gradient norm USED-FOR models. gradient norm USED-FOR generalization. generalization EVALUATE-FOR models. gradient norm COMPARE algorithms. algorithms COMPARE gradient norm. models COMPARE algorithms. algorithms COMPARE models. architectures USED-FOR models. Method are neural network architectures, hyper - parameter optimization, and DNNs. Metric are gradient complexity, computation cost, and computation overhead. OtherScientificTerm are loss gradient, and gradient norm objectives. ",This paper studies the relationship between the generalization error and the gradient norm in hyper-parameter optimization. The authors propose to use the approximation of gradient norm as a model selection criterion and show that it can be used to improve the efficiency and effectiveness of model selection.  The authors show that the proposed method outperforms existing methods in terms of generalization performance.   ,"This paper studies the generalization performance of hyper-parameter optimization (HPO) in the context of model selection. The authors propose a new metric, the gradient norm, to measure the efficiency of HPO. They show that HPO can be used as a generalization metric for model selection and generalization error. They also show that it can be combined with other generalization metrics, such as the loss gradient, to improve generalization efficiency. "
1413,SP:13359456defb953dd2d19e1f879100ce392d6be6,"Wikipedia HYPONYM-OF Encyclopedias. entity linking CONJUNCTION open - domain question answering. open - domain question answering CONJUNCTION entity linking. open - domain question answering HYPONYM-OF knowledge - intensive tasks. entity linking HYPONYM-OF knowledge - intensive tasks. weight vectors USED-FOR entity representations. entity meta information USED-FOR entity representations. memory footprint USED-FOR dense representations. vector dot product USED-FOR entity affinity. GENRE HYPONYM-OF system. context CONJUNCTION entity name. entity name CONJUNCTION context. vocabulary size COMPARE entity count. entity count COMPARE vocabulary size. datasets USED-FOR entity disambiguation. datasets EVALUATE-FOR approach. Generic is approaches. Method are classifiers, autoregressive formulation, and encoder - decoder architecture. OtherScientificTerm are missing fine - grained interactions, softmax loss, and entities. Material is negative data. ","This paper proposes a method for entity disambiguation on Wikipedia and open-domain question answering tasks. The proposed method is based on the idea of entity affinity, which is a weighted combination of the entity meta information and the context information. The authors show that the proposed method outperforms existing methods on both datasets.   ","This paper proposes a method for entity disambiguation in Wikipedia-like encyclopedias (e.g. Wikipedia, Wikipedia encyclopedia). The key idea is to use a vector dot product to represent the entity affinity between two entities. This is achieved by using a weight vector that is composed of the entity meta information and a decoder that encodes the entity representations. The encoder-decoder architecture is based on an autoregressive formulation. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy on several datasets."
1422,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"infinite time horizon FEATURE-OF unknown congestion functions. fe CONJUNCTION congestion function. congestion function CONJUNCTION fe. observation USED-FOR routing decisions. algorithm USED-FOR ce. algorithm USED-FOR routing decisions. observation USED-FOR algorithm. total cost CONJUNCTION minimum cost. minimum cost CONJUNCTION total cost. cumulative regret FEATURE-OF algorithm. space complexity CONJUNCTION time complexity. time complexity CONJUNCTION space complexity. time complexity EVALUATE-FOR algorithm. space complexity EVALUATE-FOR algorithm. Task is routing users. OtherScientificTerm are unknown distribution, routing requests, and regret. Material is New York City road networks. ","This paper studies the problem of routing users in a road network with unknown congestion functions over an infinite time horizon. In this setting, the goal is to estimate the total cost and the minimum cost of each user. The authors propose an algorithm that can estimate the users' total cost in a time-efficient manner. The algorithm is shown to have a regret of $O(1/\sqrt{C})$, where $C$ is the number of users in the network. The regret is also shown to be bounded by a constant factor.    The main contribution of this paper is to study the problem in the setting where the users are unknown and the congestion function is unknown. ","This paper studies the problem of routing users on a road network with unknown congestion functions over an infinite time horizon. The authors propose an algorithm that can be used to estimate the total cost and the minimum cost of each user. They show that their algorithm can achieve a time complexity of $O(\sqrt{C})$ in terms of time complexity, space complexity, and cumulative regret. They also provide a theoretical analysis of the regret of their algorithm."
1431,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"BERT HYPONYM-OF Masked Language Models ( MLMs ). uniform masking USED-FOR MLM. PMI - Masking HYPONYM-OF masking strategy. Pointwise Mutual Information ( PMI ) USED-FOR masking strategy. entity / phrase masking CONJUNCTION random - span masking. random - span masking CONJUNCTION entity / phrase masking. whole - word masking CONJUNCTION entity / phrase masking. entity / phrase masking CONJUNCTION whole - word masking. PMIMasking COMPARE prior more heuristic approaches. prior more heuristic approaches COMPARE PMIMasking. entity / phrase masking HYPONYM-OF prior more heuristic approaches. whole - word masking HYPONYM-OF prior more heuristic approaches. random - span masking HYPONYM-OF random uniform token masking. entity / phrase masking HYPONYM-OF random uniform token masking. whole - word masking HYPONYM-OF random uniform token masking. PMI - Masking COMPARE prior masking approaches. prior masking approaches COMPARE PMI - Masking. OtherScientificTerm are shallow local signals, token n - gram, and collocation. Task is pretraining inefficiency. ","This paper proposes a new masking strategy called PMI-Masking for Masked Language Models (MLMs). The main idea is to use Pointwise Mutual Information (PMI) to learn a mask for each token in the language model. PMI is based on the observation that masking the entire language model with uniform masking can lead to poor performance. To address this issue, the authors propose to use PMI to learn the mask for the entity/phrase masking and random-span masking. The experiments show that the proposed method outperforms the existing masking strategies.","This paper proposes a new masking strategy called PMI-Masking, which is based on Pointwise Mutual Information (PMI) masking. The main idea is to use PMI masking to improve the efficiency of masking in a BERT-based MLM. The authors show that the proposed method is more efficient than other masking strategies such as random-span masking, whole-word masking and entity/phrases masking (e.g., entity/phrase masking). The authors also show that PMI Masking can be used in conjunction with other heuristics such as whole-words masking or random uniform token masking as well."
1440,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"Amortised inference USED-FOR sequential latent - variable models ( LVMs ). Bayesian filter HYPONYM-OF mixture of smoothing posteriors. ELBO objective USED-FOR partially - conditioned amortised posteriors. partially - conditioned amortised posteriors USED-FOR products of smoothing posteriors. traffic flow CONJUNCTION handwritten digits. handwritten digits CONJUNCTION traffic flow. handwritten digits CONJUNCTION aerial vehicle dynamics. aerial vehicle dynamics CONJUNCTION handwritten digits. aerial vehicle dynamics HYPONYM-OF scenarios. traffic flow HYPONYM-OF scenarios. handwritten digits HYPONYM-OF scenarios. generative modelling CONJUNCTION multi - step prediction. multi - step prediction CONJUNCTION generative modelling. fully - conditioned approximate posteriors USED-FOR generative modelling. OtherScientificTerm are evidence lower bound ( ELBO ), variational posteriors, posteriors, and approximate posteriors. Generic is setting. Method is generative model. ","This paper proposes a method for amortized inference in sequential latent variable models (LVMs). The main idea is to use a mixture of smoothing posteriors (e.g., a Bayesian filter) to approximate the product of the products of the variational posteriors. The authors show that the proposed method is able to achieve a lower bound on the ELBO objective for the partially-conditioned amortised posteriors, and a fully-conditional approximation of the product. The proposed method can be used for generative models and multi-step prediction.","This paper studies the amortised inference problem in sequential latent variable models (LVMs). The main contribution of the paper is to propose an ELBO objective for partially-conditioned amortized posteriors, which is a mixture of smoothing posteriors with a Bayesian filter. The authors show that the product of the smoothed posteriors is a product of products of the products of two products of smoothed postersiors. They show that this product can be used as evidence lower bound (ELBO) for the fully-conditional and partially-convex posteriors."
1449,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"statistical properties FEATURE-OF distributed kernel ridge regression. distributed kernel ridge regression CONJUNCTION random features ( DKRR - RF ). random features ( DKRR - RF ) CONJUNCTION distributed kernel ridge regression. statistical properties FEATURE-OF random features ( DKRR - RF ). divide - and - conquer technique CONJUNCTION random features. random features CONJUNCTION divide - and - conquer technique. O(|D| ) memory CONJUNCTION O(|D| ) time. O(|D| ) time CONJUNCTION O(|D| ) memory. statistical accuracy EVALUATE-FOR KRR. random features USED-FOR KRR. divide - and - conquer technique USED-FOR KRR. O(|D| ) time USED-FOR KRR. O(|D| ) memory USED-FOR KRR. communication strategy USED-FOR DKRR - RF. OtherScientificTerm are optimal generalization bounds, generalization bounds, and average information. Generic is theoretical bounds. ","This paper studies distributed kernel ridge regression with distributed random features (DKRR-RF), which is a variant of DKRR with random features. Theoretical results on the generalization bounds for DKRR are provided, which are shown to be optimal in terms of the number of samples and the average information. In particular, the authors show that the optimal generalization bound is upper bounded by $O(D^2/\sqrt{D|)$, where $D$ is the dimension of the data distribution. The authors also show that a divide-and-conquer technique can be used to divide the data into sub-datasets and compute the random features using the divide and conquer algorithm.   ","This paper studies the generalization properties of distributed kernel ridge regression (KRR) with random features (DKRR-RF). Theoretical bounds are provided for generalization bounds for KRR with DKRR and random features. Theoretically, the authors show that KRR has O(D|) memory, O(|D| ) time, and O(d|) average information. They also provide a generalization bound for DKRR with divide-and-conquer (D&C) technique.   "
1458,SP:129872706a12d89f0886c2ad0fd4083d0632343c,"search step USED-FOR architectures. validation performance EVALUATE-FOR architectures. accuracy EVALUATE-FOR weight - sharing architectures. RandomNAS USED-FOR architectures. global search space(GS ) FEATURE-OF architectures. top - performing architectures PART-OF GS. proxy search space ( PS ) USED-FOR RandomNAS. EPS HYPONYM-OF Proxy Search Space. EPS HYPONYM-OF RandomNAS - based approach. EPS COMPARE state - of - the - art. state - of - the - art COMPARE EPS. NASBench-201 EVALUATE-FOR EPS. image classification CONJUNCTION natural language processing. natural language processing CONJUNCTION image classification. DARTS - like search spaces USED-FOR tasks. EPS USED-FOR tasks. search time EVALUATE-FOR EPS. natural language processing HYPONYM-OF tasks. image classification HYPONYM-OF tasks. Method is NAS approach. Metric are achievable accuracy, and RandomNAS ’s search efficiency. OtherScientificTerm are NAS search space, and ground - truth ranking. ","This paper proposes a proxy search space (PS) for weight-sharing architectures to improve the accuracy of weight sharing architectures. The proposed PS is based on the RandomNAS algorithm, which uses the top-performing architectures in the global search space to find the ground-truth ranking in the PS. The paper also proposes a Proxy Search Space (PS), which is a search space that is more efficient than the original NAS search space. Experiments show that the proposed PS outperforms the state-of-the-art on the NASBench-201 benchmark.","This paper proposes a proxy search space (PS) for weight-sharing NAS architectures. The proposed PS is based on RandomNAS, which is a search space for weight sharing NAS architectures, where the top-performing architectures are ranked using the ground-truth ranking. The paper also proposes a new search space, called Proxy Search Space (PS), which can be used to find the best-performing NAS architectures in the global search space. The authors show that the proposed PS outperforms the state-of-the-art in terms of accuracy on NAS benchmark NASBench-201."
1467,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"it CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION it. imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation learning. imitation learning CONJUNCTION meta reinforcement learning. meta reinforcement learning CONJUNCTION imitation learning. imitation learning USED-FOR method. Probabilistic Embeddings USED-FOR method. PERIL USED-FOR exploration policies. Dual inference strategies USED-FOR PERIL. imitation learning COMPARE approach. approach COMPARE imitation learning. uncertainties FEATURE-OF it. it EVALUATE-FOR approach. meta - RL USED-FOR PERIL. sparse rewards FEATURE-OF meta - RL benchmarks. Method are Imitation learning, and meta reinforcement learning ( meta - RL ). Task is exploration. Material is interaction data. Metric is adaptation rates. ","This paper proposes a meta-RL method for reinforcement learning in the presence of sparse rewards. The proposed method is based on Probabilistic Embedding (PERIL), which is an extension of PEARL. PERIL is a probabilistic inference method that learns to model the uncertainty in the reward distribution. The authors show that PERIL can be used to improve the performance of reinforcement learning on a variety of tasks.   ",This paper proposes a method for meta-RL that uses Probabilistic Embeddings (PERIL) to improve the performance of imitation learning in the meta-learning setting. PERIL is based on the idea that imitation learning should be able to adapt to the environment and the environment should adapt to imitation learning. The authors show that PERIL outperforms imitation learning and reinforcement learning in terms of performance on a variety of meta-reinforcement learning benchmarks. 
1476,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"learning method USED-FOR image classification. overparameterized convolutional neural networks USED-FOR image classification. overparameterized convolutional neural networks HYPONYM-OF learning method. overparameterized convolutional neural networks CONJUNCTION gradient based optimization. gradient based optimization CONJUNCTION overparameterized convolutional neural networks. 3 - layer overparameterized convolutional network CONJUNCTION stochastic gradient descent ( SGD ). stochastic gradient descent ( SGD ) CONJUNCTION 3 - layer overparameterized convolutional network. orthogonal patches PART-OF images. 3 - layer overparameterized convolutional network USED-FOR images. pattern detectors CONJUNCTION detected patterns. detected patterns CONJUNCTION pattern detectors. SGD USED-FOR setting. pattern statistics USED-FOR dot - product. learning algorithm USED-FOR PSI. learning algorithm USED-FOR setting. sample complexity EVALUATE-FOR learning algorithm. overparameterized CNNs USED-FOR MNIST. non - orthogonal patches FEATURE-OF MNIST. non - orthogonal patches USED-FOR overparameterized CNNs. Task is image classification task. OtherScientificTerm are Pattern Statistics Inductive Bias ( PSI ), filter dimension, and VC dimension lower bound. Generic is it. ",This paper studies the problem of learning over-parameterized convolutional neural networks for image classification. The authors propose to use a 3-layer overparametrized convolution network with a dot-product of the pattern statistics to learn the dot product of the detected patterns and the predicted patterns. The proposed method is shown to be computationally efficient in terms of sample complexity.   ,This paper proposes a new learning method for overparameterized convolutional neural networks (CNNs) with orthogonal patches. The main idea is to use a 3-layer overparametrized CNN with a dot-product-based learning algorithm to learn the pattern statistics of the dot product. The method is motivated by the PSI bias of the image classification task. The authors provide a lower bound on the sample complexity of the learning algorithm. They also provide an upper bound for the number of patches in the image.
1485,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,approach USED-FOR representation learning. Contrastive learning USED-FOR representation learning. contrastive learning USED-FOR representation of documents. topic modeling assumptions FEATURE-OF document classification. topic posterior information FEATURE-OF representation of documents. procedure USED-FOR semi - supervised setup. linear classifiers USED-FOR document classification tasks. representations USED-FOR linear classifiers. OtherScientificTerm is embeddings of data. Method is linear models. ,This paper studies the problem of document representation learning in semi-supervised learning. The authors propose to use contrastive learning to learn the representation of documents in the presence of a topic model. The proposed method is based on the idea that the topic model can be viewed as an embedding of the embedding space of the data. The main contribution of the paper is to show that the proposed method outperforms the state-of-the-art representation learning methods in terms of performance on document classification tasks.  ,This paper proposes a novel approach to contrastive learning for document classification. The authors propose a method to learn the topic posterior information of the representation of documents. They show that this information can be used to improve the performance of linear classifiers on document classification tasks. They also show that their method can be applied to semi-supervised settings. 
1494,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"vision CONJUNCTION language. language CONJUNCTION vision. Multimodal learning USED-FOR generative models. it USED-FOR generalisable representations. related ” multimodal data USED-FOR models. contrastive framework USED-FOR generative model learning. contrastive framework USED-FOR model. method USED-FOR multimodal learning. framework USED-FOR generative model. Task is learning generalisable representations. Method is multimodal variational autoencoder ( VAE ) models. Material is unlabeled, unpaired multimodal data. "," generative models have been shown to be able to learn generalizable representations on unlabeled, unpaired multimodal data. This paper proposes a contrastive framework for generative model learning on unlabelled multimodel data. The proposed method is based on a VAE model with a multi-modal variational autoencoder (VAE) architecture.  ","This paper proposes a contrastive generative model learning framework for multi-modal generative models. The main idea is to use contrastive contrastive learning to learn representations from unlabeled, unpaired multimodal data. The proposed method is based on a VAE-based VAE model. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization performance. "
1503,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"Variational autoencoders ( VAEs ) HYPONYM-OF likelihood - based generative models. base prior distribution CONJUNCTION reweighting factor. reweighting factor CONJUNCTION base prior distribution. reweighting factor USED-FOR energy - based prior. base prior distribution USED-FOR energy - based prior. it USED-FOR hierarchical VAEs. latent variable groups FEATURE-OF hierarchical VAEs. noise contrastive estimation USED-FOR reweighting factor. CIFAR-10 CONJUNCTION CelebA 64. CelebA 64 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CelebA 64 CONJUNCTION CelebA HQ 256 datasets. CelebA HQ 256 datasets CONJUNCTION CelebA 64. generative EVALUATE-FOR VAEs. noise contrastive priors USED-FOR VAEs. noise contrastive priors USED-FOR generative. MNIST EVALUATE-FOR VAEs. Generic is they. OtherScientificTerm are prior, tempering, prior hole problem, prior distribution, aggregate approximate posterior, latent space, and aggregate posterior. ","This paper proposes to use a reweighting factor to improve the prior distribution of variational autoencoders (VAEs) in order to mitigate the prior hole problem. Specifically, the authors propose to use an energy-based prior to estimate the prior of the latent variable groups in the latent space of the VAE. The authors show that this reweighted prior can be used to improve generative performance of VAEs in the presence of prior holes.  ","This paper proposes a new approach to generative generative models for latent variable groups in VAEs. The authors propose a reweighting factor for the base prior distribution of the latent variable group, which is based on the energy-based prior distribution. They also propose a noise contrastive estimation for the reweighted base prior. They show that their approach can improve the generative performance of VAEs on CIFAR-10, MNIST, CelebA 64 and CelebA HQ 256 datasets."
1512,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"convex regularizers USED-FOR learner ’s policy. degenerate solutions HYPONYM-OF constant rewards. convex regularizers USED-FOR Regularized IRL. constant rewards USED-FOR expert ’s behavior. practical methods USED-FOR them. them USED-FOR regularized IRL. practical methods USED-FOR regularized IRL. tractable solutions CONJUNCTION practical methods. practical methods CONJUNCTION tractable solutions. maximum - entropy IRL framework USED-FOR methods. Shannon - entropy regularizers USED-FOR them. theoretical backing USED-FOR IRL method. IRL method USED-FOR discrete and continuous controls. Method is Inverse Reinforcement Learning ( IRL ). OtherScientificTerm are expert behavior, and reward functions. Generic are solutions, and tasks. ","This paper studies the problem of inverse reinforcement learning (IRL), where the goal is to learn a policy that maximizes a reward function that minimizes the entropy of the reward function. The main contribution of this paper is a theoretical analysis of the convex regularization term that is used to regularize the reward functions in IRL. Theoretical results show that this regularization is equivalent to a maximum entropy IRL method, and that it is equivalent in practice to a Shannon-entropy regularization method.  ","This paper studies the problem of Inverse Reinforcement Learning (IRL), where the learner is trying to learn a policy that maximizes the entropy of the expert’s reward function, but the expert is not able to maximize the reward function. The main contribution of the paper is a theoretical analysis of the regularization of IRL, which is based on the Shannon-entropy regularization (Shannon-Entropy) framework. Theoretical results are provided for a variety of settings, including discrete control, continuous control, and continuous control. "
1521,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"hard binary gates USED-FOR LS or shared paths. CLSR USED-FOR LS or shared paths. hard binary gates USED-FOR token representations. hard binary gates USED-FOR CLSR. translation signals CONJUNCTION budget constraints. budget constraints CONJUNCTION translation signals. LS capacity PART-OF MNMT. translation signals USED-FOR MNMT. CLSR COMPARE many - to - one translation. many - to - one translation COMPARE CLSR. one - to - many translation COMPARE many - to - one translation. many - to - one translation COMPARE one - to - many translation. LS computation USED-FOR top and/or bottom encoder / decoder layers. unbalanced training data USED-FOR many - to - one translation. LS modeling USED-FOR MNMT. OPUS-100 and WMT datasets EVALUATE-FOR Transformer. CLSR USED-FOR one - to - many translation. shared capacity CONJUNCTION LS capacity. LS capacity CONJUNCTION shared capacity. LS capacity USED-FOR multilingual translation. Task is multilingual neural machine translation ( MNMT ). Method are conditional language - specific routing ( CLSR ), and multilingual Transformers. Generic is gates. ",This paper proposes conditional language-specific routing (CLSLR) for multilingual neural machine translation (MNMT). CLSR is based on hard binary gates that can be used to encode the token representations of multiple languages into a single token representation. The authors show that CLSR achieves better performance than one-to-many translation on the OPUS-100 and WMT datasets. CLSR can also be used for unbalanced training data. ,"This paper proposes a new method for multilingual neural machine translation (MNMT) based on conditional language-specific routing (CLS). CLSR is based on hard binary gates that can be used to learn a set of token representations for each language. The authors show that CLSR can be applied to many-to-many translation, and that it can also be used for one- to-one translation. They also show that LS computation can be performed by the top and/or bottom encoder/decoder layers of a Transformer, and show that it is possible to use LS computation for both top and bottom encoders/decoders.  The authors also show how CLSR and LS can be combined to improve the performance of the Transformer.  "
1530,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"Wasserstein distributional normalization ( WDN ) algorithm USED-FOR noisy labels. Wasserstein distributional normalization ( WDN ) algorithm USED-FOR accurate classification. noisy labels USED-FOR accurate classification. geometric constraints FEATURE-OF uncertain samples. Wasserstein ball USED-FOR them. WDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE WDN. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR state - of - the - art methods. Clothing1 M and CIFAR-10/100 datasets EVALUATE-FOR WDN. WDN COMPARE classification methods. classification methods COMPARE WDN. accuracy EVALUATE-FOR it. accuracy EVALUATE-FOR WDN. OtherScientificTerm are small loss criteria, geometric relationship, and diverse noisy labels. Generic is relation. ",This paper proposes a new Wasserstein distributional normalization (WDN) algorithm for classification with noisy labels. The main idea is to use the geometric relationship between the noisy labels and the true labels as a regularization term. The WDN algorithm is based on the fact that the true and noisy labels have a geometric relationship. The authors show that the WDN can be used to improve the classification accuracy on Clothing1M and CIFAR-10/100 datasets. ,"This paper proposes a new Wasserstein distributional normalization (WDN) algorithm for noisy labels. The main idea of WDN is to use the Wasserstein ball to normalize the labels of uncertain samples. The WDN algorithm is motivated by the geometric constraints of the uncertain samples, which are defined as the geometric relationship between the noisy labels and the true labels.  The authors show that under certain conditions, WDN can achieve better accuracy than state-of-the-art methods on Clothing1M and CIFAR-10/100 datasets."
1539,SP:e0029422e28c250dfb8c62c29a15b375030069e8,predictive accuracy EVALUATE-FOR Convolutional image classifiers. uncertainty quantification techniques USED-FOR probability estimates. uncertainty quantification techniques USED-FOR network. network USED-FOR probability estimates. Platt scaling HYPONYM-OF uncertainty quantification techniques. algorithm USED-FOR predictive set. algorithm USED-FOR classifier. user - specified probability FEATURE-OF predictive set. algorithm COMPARE Platt scaling. Platt scaling COMPARE algorithm. formal finite - sample coverage guarantee FEATURE-OF model. formal finite - sample coverage guarantee FEATURE-OF algorithm. conformal prediction algorithm USED-FOR method. scheme COMPARE approaches. approaches COMPARE scheme. scheme COMPARE stand - alone Platt scaling baseline. stand - alone Platt scaling baseline COMPARE scheme. Imagenet - V2 EVALUATE-FOR scheme. Imagenet CONJUNCTION Imagenet - V2. Imagenet - V2 CONJUNCTION Imagenet. Imagenet EVALUATE-FOR scheme. ResNet-152 CONJUNCTION classifiers. classifiers CONJUNCTION ResNet-152. classifiers EVALUATE-FOR scheme. Imagenet - V2 EVALUATE-FOR approaches. Imagenet - V2 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION Imagenet - V2. ResNet-152 EVALUATE-FOR scheme. Imagenet - V2 CONJUNCTION classifiers. classifiers CONJUNCTION Imagenet - V2. coverage EVALUATE-FOR approaches. coverage EVALUATE-FOR scheme. OtherScientificTerm is formal guarantees. ,"This paper proposes a new method to improve the predictive accuracy of convolutional image classifiers. The proposed method is based on the Platt scaling method, which is an extension of Platt-based methods. The main idea is to use a conformal prediction algorithm to estimate the predictive set of the classifier, which can then be used to compute the user-specified probability of the predicted class. The authors show that the proposed method achieves better coverage than the existing methods in terms of accuracy and coverage.","This paper proposes a new method for quantifying uncertainty quantification in convolutional neural networks. The proposed method is based on the Platt scaling method, which is a well-known and well-studied technique for quantification. The main contribution of this paper is to introduce a conformal prediction algorithm to improve the coverage of the predictive set. The method is evaluated on Imagenet-V2, ResNet-152, and ResNets-152."
1548,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"algorithm USED-FOR Wasserstein-2 barycenters. input convex neural networks CONJUNCTION cycle - consistency regularization. cycle - consistency regularization CONJUNCTION input convex neural networks. entropic or quadratic regularization USED-FOR approaches. minimax optimization USED-FOR approach. low - dimensional qualitative scenarios CONJUNCTION high - dimensional quantitative experiments. high - dimensional quantitative experiments CONJUNCTION low - dimensional qualitative scenarios. high - dimensional quantitative experiments EVALUATE-FOR approach. low - dimensional qualitative scenarios EVALUATE-FOR approach. OtherScientificTerm are Wasserstein barycenters, and error bounds. Method is optimal transport. ",This paper studies the problem of finding Wasserstein-2 barycenters for input convex neural networks with cycle-consistency regularization and entropic or quadratic regularization. The authors propose a minimax optimization algorithm for this problem and provide error bounds for the proposed method. They show that their method is optimal in the sense that the error bound is upper bounded by the optimal transport. They also provide experimental results on a variety of experiments.,This paper studies the problem of Wasserstein-2 barycenters in the context of input convex neural networks and cycle-consistency regularization. The main contribution of this paper is to provide a minimax optimization algorithm for the problem. The authors show that the optimal transport can be obtained by minimizing the minimax convergence rate of the minimization. They also provide an upper bound on the error of their algorithm.  
1557,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"multiple manifold problem HYPONYM-OF binary classification task. deep fully - connected neural network USED-FOR multiple manifold problem. randomly - initialized gradient descent USED-FOR manifolds. randomlyinitialized network CONJUNCTION gradients. gradients CONJUNCTION randomlyinitialized network. nonasymptotic framework USED-FOR generalization of networks. neural tangent kernel PART-OF deep fullyconnected ReLU networks. NTK regime FEATURE-OF generalization of networks. structured data USED-FOR generalization of networks. martingale concentration USED-FOR statistical dependencies. approach USED-FOR network architectures. Task are machine vision, and practically - motivated model problem. OtherScientificTerm are manifold configuration, network depth L, geometric and statistical properties of the data, network width n, i.i.d. samples, depth, fitting resource, class manifolds, width, and statistical resource. Method are nonasymptotic analysis of training overparameterized neural networks, and random network. ","This paper studies the multiple manifold problem, which is a binary classification task with a deep fully connected neural network. The main contribution is a non-asymptotic analysis of training overparameterized neural networks in the NTK regime. Theoretical results are given for the network depth L, the width n, and the number of samples in the training set. ","This paper proposes a nonasymptotic analysis of training overparameterized neural networks for the multi-manifold classification problem. The main contribution of the paper is to study the generalization of neural networks in the NTK regime, where the number of samples per class and the width of the network depends on the class of the data. The authors show that under certain conditions, the training of a fully-connected ReLU network with a randomly-initialized network and gradients can lead to better generalization performance than training with a random network with gradients. They also provide a theoretical analysis of the martingale concentration of the statistical dependencies between the data and the network depth."
1566,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"supervised learning methods USED-FOR subroutines. subroutines USED-FOR reinforcement learning algorithm. supervised learning methods USED-FOR reinforcement learning algorithm. weighted target actions USED-FOR policy. supervised learning steps PART-OF approach. one HYPONYM-OF supervised learning steps. supervised learning methods USED-FOR method. experience replay USED-FOR off - policy data. it COMPARE RL algorithms. RL algorithms COMPARE it. AWR COMPARE RL algorithms. RL algorithms COMPARE AWR. OpenAI Gym benchmark tasks EVALUATE-FOR AWR. AWR USED-FOR policies. AWR COMPARE off - policy algorithms. off - policy algorithms COMPARE AWR. environmental interactions FEATURE-OF static datasets. off - policy algorithms USED-FOR policies. complex simulated characters FEATURE-OF continuous control tasks. continuous control tasks EVALUATE-FOR algorithm. Method is advantage - weighted regression ( AWR ). OtherScientificTerm are value function, and continuous and discrete actions. ","This paper proposes to use advantage-weighted regression (AWR) for reinforcement learning. The main idea is to learn a policy that maximizes the advantage of the current state-action-value function over a set of subroutines that are sampled from an off-policy dataset. The proposed method is evaluated on a variety of continuous control tasks, where it is shown to outperform baselines in terms of performance on both continuous and discrete actions. ","This paper proposes a method for advantage-weighted regression (AWR) for off-policy reinforcement learning. The main idea of AWR is to use experience replay to learn a policy that maximizes the advantage of the target actions, and then use the experience replay data to train the policy. The method is evaluated on a variety of continuous control tasks, and it is shown to outperform the state-of-the-art in terms of performance."
1575,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,Heterogeneous assignment of bitwidths FEATURE-OF layers. parametrized sinusoidal regularizer USED-FOR WaveQ. WaveQ USED-FOR quantized weights. training USED-FOR stochastic gradient descent. sinusoidal regularizer USED-FOR stochastic gradient descent. quantized weights CONJUNCTION heterogeneous bitwidths. heterogeneous bitwidths CONJUNCTION quantized weights. WaveQ HYPONYM-OF gradient - based mechanism. gradient - based mechanism USED-FOR quantized weights. gradient - based mechanism USED-FOR heterogeneous bitwidths. ResNet-20 CONJUNCTION SVHN. SVHN CONJUNCTION ResNet-20. ResNet-18 CONJUNCTION ResNet-20. ResNet-20 CONJUNCTION ResNet-18. MobileNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION MobileNet. heterogeneous bitwidth assignment USED-FOR quantization. CIFAR10 CONJUNCTION MobileNet. MobileNet CONJUNCTION CIFAR10. AlexNet CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION AlexNet. SVHN CONJUNCTION VGG-11. VGG-11 CONJUNCTION SVHN. compute efficiency CONJUNCTION accuracy. accuracy CONJUNCTION compute efficiency. WaveQ USED-FOR heterogeneous bitwidth assignment. heterogeneous bitwidth assignment USED-FOR deep networks. WaveQ USED-FOR deep networks. accuracy EVALUATE-FOR WaveQ. compute efficiency EVALUATE-FOR WaveQ. VGG-11 HYPONYM-OF deep networks. AlexNet HYPONYM-OF deep networks. SVHN HYPONYM-OF deep networks. MobileNet HYPONYM-OF deep networks. ResNet-20 HYPONYM-OF deep networks. CIFAR10 HYPONYM-OF deep networks. ResNet-18 HYPONYM-OF deep networks. predetermined bitwidths USED-FOR WaveQ. DoReFa CONJUNCTION WRPN. WRPN CONJUNCTION DoReFa. accuracy EVALUATE-FOR quantized training algorithms. quantized training algorithms EVALUATE-FOR,"This paper proposes WaveQ, a novel quantized training method for deep neural networks with heterogeneous bitwidths. The main idea is to use a parametrized sinusoidal regularizer to regularize the quantized weights during training. The proposed method is based on gradient-based quantization. The method is evaluated on CIFAR-10, MobileNet, and SVHN datasets. ","This paper proposes WaveQ, a parametrized sinusoidal regularizer for quantized weights for training deep neural networks with heterogeneous bitwidths. The proposed method is based on a gradient-based mechanism, which can be applied to both quantized and non-quantized weights. The method is evaluated on CIFAR-10, SVHN, ResNet-20, and AlexNet datasets."
1584,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"data augmentation methods USED-FOR translation. monolingual data USED-FOR data augmentation methods. data augmentation methods USED-FOR neural machine translation ( NMT ). in - domain monolingual data USED-FOR it. backtranslation HYPONYM-OF data augmentation methods. data augmentation method USED-FOR neural machine translation. small and large scale datasets EVALUATE-FOR method. method COMPARE baseline models. baseline models COMPARE method. small and large scale datasets EVALUATE-FOR baseline models. Method is neural machine translation models. OtherScientificTerm are aligned word pairs, and bilingual embeddings. ","This paper proposes a data augmentation method to improve the performance of neural machine translation (NMT) models on monolingual data. The proposed method is based on backtranslation, which is a data-augmented version of backpropagation. The main idea is to use bilingual embeddings to augment the data with aligned word pairs from the source and target languages. The method is evaluated on both small and large scale datasets.   ",This paper proposes a new data augmentation method for neural machine translation (NMT) models. The main idea is to augment the monolingual data with bilingual embeddings to improve the performance of the NMT model. The proposed method is evaluated on both small and large scale datasets. The experimental results show that the proposed method outperforms the baseline models.
1593,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"Federated learning USED-FOR distributed data privacy. data quantity CONJUNCTION data quality. data quality CONJUNCTION data quantity. Shapley Value PART-OF game theory. Shapley Value COMPARE method. method COMPARE Shapley Value. maintaining real - time EVALUATE-FOR method. data quality EVALUATE-FOR method. data quantity EVALUATE-FOR method. Method are contribution measurement mechanism, real - time contribution measurement method, and pseudo - distributed training. Generic is mechanism. OtherScientificTerm is contribution rate. Material is Penn Treebank dataset. ","This paper studies the contribution measurement mechanism in federated learning. The authors propose a real-time contribution measurement method, which is based on the Shapley Value in game theory. They show that the contribution rate is a function of the number of epochs and the quality of the data. The contribution rate can be estimated in real time, and the authors show that it can be computed in a way that preserves the privacy of the contributions. ","This paper proposes a new contribution measurement method for federated learning. The contribution measurement mechanism is based on the Shapley Value (Shapley Value) in game theory, which is a well-studied metric for measuring the contribution rate of a group of players in a game. The authors show that the contribution of each player can be measured in a real-time manner, and that it can be computed in a way that preserves the privacy of the players. They also show that their method can be used in a pseudo-distributed setting, where each player contributes to the training of the other players."
1602,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"graph structure FEATURE-OF fully - observable case. nearlylinear time algorithm USED-FOR problem. dimension - independent error guarantee FEATURE-OF nearlylinear time algorithm. error guarantees EVALUATE-FOR robust algorithms. robust learning of Bayesian networks CONJUNCTION robust mean estimation. robust mean estimation CONJUNCTION robust learning of Bayesian networks. Task is learning Bayesian networks. Method are Bayesian networks, Bayesian network, and robust mean estimation algorithm. Generic is algorithm. ","This paper studies the problem of learning Bayesian networks in a fully observable setting. In this setting, the goal is to learn a Bayesian network that is robust to perturbations in the graph structure. The main contribution of this paper is to provide a nearly-linear time algorithm for this problem, which is based on a robust mean estimation algorithm. The authors show that the error of the proposed algorithm is dimension-independent, and that it can be used to train a robust Bayesian neural network.  ","This paper studies the problem of learning Bayesian networks with robust mean estimation. In particular, the authors consider the case where the graph structure of a Bayesian network is fully observable. They show that a nearly-linear time algorithm with dimension-independent error guarantees can be used to learn a robust mean estimator in a fully-observable setting. They also provide a theoretical analysis of the robustness of the proposed algorithm."
1611,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"raw high - dimensional observations USED-FOR autonomous agents. images HYPONYM-OF raw high - dimensional observations. shaped reward functions USED-FOR model - based reinforcement learning ( RL ). short - horizon reasoning USED-FOR shaped reward functions. trajectory optimization USED-FOR long - horizon reasoning. it USED-FOR image - based setting. probabilistic latent variable models USED-FOR algorithm. probabilistic latent variable models USED-FOR it. approach USED-FOR longer - horizon visual planning. latent collocation method ( LatCo ) USED-FOR approach. latent collocation method ( LatCo ) USED-FOR longer - horizon visual planning. approach COMPARE prior model - based approaches. prior model - based approaches COMPARE approach. sparse rewards CONJUNCTION long - term goals. long - term goals CONJUNCTION sparse rewards. sparse rewards FEATURE-OF visual control tasks. long - term goals FEATURE-OF visual control tasks. visual control tasks EVALUATE-FOR prior model - based approaches. visual control tasks EVALUATE-FOR approach. Method are temporally extended reasoning, myopic, short - sighted planning, and collocation - based planning. OtherScientificTerm is latent variables. ","This paper proposes a method for long-horizon visual planning in image-based reinforcement learning. The method is based on a latent collocation method (LatCo), which uses a probabilistic latent variable model to model long-term trajectories. The proposed method is shown to achieve state-of-the-art performance on several visual control tasks. ",This paper proposes a method for long-horizon visual planning based on a probabilistic latent variable model (LatCo) for visual control tasks. LatCo is a model-based reinforcement learning (RL) approach that learns a long-term trajectory optimization algorithm for the task of visual control. The authors show that the proposed method outperforms the state-of-the-art on a variety of tasks.   
1620,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"Bayesian neural networks COMPARE neural networks. neural networks COMPARE Bayesian neural networks. tempered ” or “ cold ” posterior USED-FOR uncertainty. BNNs USED-FOR image classification. CIFAR-10 HYPONYM-OF image benchmark datasets. generative model USED-FOR curation. generative model USED-FOR Bayesian account of cold posteriors. likelihood COMPARE tempered likelihoods. tempered likelihoods COMPARE likelihood. generative model USED-FOR likelihood. Method is Bayesian inference / decision theory. OtherScientificTerm are posterior, and prior. ","This paper proposes a Bayesian approach to estimate the likelihood of an image using a generative model. The approach is based on the notion of a ""cold"" posterior, which is defined as the likelihood that is more sensitive to uncertainty than a ""tempered"" posterior. The authors show that the cold posterior can be expressed as a combination of the prior and the prior of the likelihood, and that it can be used as a way to estimate uncertainty in the posterior distribution. They show that this approach can be applied to Bayesian neural networks (BNNs) and show that it improves the performance of BNNs on image classification tasks.","This paper proposes a generative model for the curation of the posterior of a Bayesian neural network (BNN) model. The model is based on the notion of a ""cold"" posterior, which is defined as the posterior that is more sensitive to uncertainty than a ""tempered"" posterior. The authors show that the cold posterior can be more sensitive than the tempered posterior in terms of the likelihood of the prior. They show that this is due to the fact that the prior of the BNN can be a mixture of the tempered and cold posterior. They also show that a ""hot"" posterior is a more sensitive version of the ""cold posterior"".  "
1629,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,non - autoregressive neural machine translation COMPARE autoregressive machine translation. autoregressive machine translation COMPARE non - autoregressive neural machine translation. GPUs USED-FOR autoregressive machine translation. latter COMPARE former. former COMPARE latter. non - autoregressive models COMPARE autoregressive baselines. autoregressive baselines COMPARE non - autoregressive models. translation quality - speed tradeoffs EVALUATE-FOR autoregressive baselines. translation quality - speed tradeoffs EVALUATE-FOR non - autoregressive models. accuracy EVALUATE-FOR autoregressive baselines. encoders USED-FOR autoregressive models. single - layer autoregressive decoder COMPARE non - autoregressive models. non - autoregressive models COMPARE single - layer autoregressive decoder. inference speed EVALUATE-FOR single - layer autoregressive decoder. inference speed EVALUATE-FOR non - autoregressive models. suboptimal layer allocation CONJUNCTION insufficient speed measurement. insufficient speed measurement CONJUNCTION suboptimal layer allocation. autoregressive baselines COMPARE non - autoregressive methods. non - autoregressive methods COMPARE autoregressive baselines. speed disadvantage EVALUATE-FOR autoregressive baselines. speed disadvantage EVALUATE-FOR non - autoregressive methods. OtherScientificTerm is knowledge distillation. Task is machine translation. ,This paper proposes to use autoregressive neural machine translation (ATL) models to improve the performance of machine translation models. The main contribution of the paper is that the authors propose to use a single layer ATL decoder to reduce the computational cost of the encoder and decoder architecture. The authors show that the proposed ATL models achieve comparable performance to the state-of-the-art in terms of translation quality-speed tradeoffs.  ,"This paper studies the trade-off between quality-speed tradeoffs between autoregressive and non-autoregressive neural machine translation. The authors show that the quality trade-offs between the two are not the same, but the speed tradeoffs are. They also show that a single-layer autore progressive decoder can be faster than a single layer non-Autoregressive decoder.   The authors also provide a theoretical analysis of the tradeoff between accuracy and speed. "
1638,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"test error EVALUATE-FOR deep neural network ( DNN ). bell - shaped variance USED-FOR model - wise double descent. test error EVALUATE-FOR DNN. test error HYPONYM-OF epoch - wise double descent. bias - variance analysis USED-FOR epoch - wise double descent. variance USED-FOR zero - one loss. metric USED-FOR diversity of model updates. stochastic gradients of random training batches USED-FOR diversity of model updates. It USED-FOR generalization ability. It USED-FOR DNN. generalization ability EVALUATE-FOR DNN. It USED-FOR early stopping. zero - one loss USED-FOR DNN. validation set USED-FOR early stopping. Method are statistical learning theory, and bias - variance decomposition. OtherScientificTerm are double descent, model complexity, U - shaped curve, OV, and unknown ) test error. Generic is descent. Metric is optimization variance ( OV ). ","This paper studies the double descent of the test error of deep neural networks (DNNs) in the presence of epoch-wise double descent. The authors propose a bias-variance decomposition of the model-wise test error as a function of model complexity, and show that it is a bell-shaped curve with unknown variance. They also show that the variance of the loss function is bounded by the model complexity.    The authors also propose to use stochastic gradients of random training batches to measure the diversity of model updates. ","This paper proposes a bias-variance analysis of the test error of epoch-wise double descent for deep neural networks (DNNs). The main contribution of the paper is to decompose the error into two parts: (1) a bell-shaped variance of the model-wise test error, and (2) a zero-one loss. The authors show that the bell-shape variance can be used to estimate the diversity of model updates in the training process, which can be useful for early stopping of DNNs. "
1647,SP:8d8b738c676938952e62a6b2aea42e79518ece06,"meta - learning techniques PART-OF few - shot learning. It USED-FOR meta - initialization of model parameters. labeled training data USED-FOR It. labeled training data USED-FOR meta - initialization of model parameters. few - shot learning USED-FOR MAML. MAML USED-FOR adversarial robustness. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. robustness USED-FOR meta - model. adversarial robustness FEATURE-OF MAML. robustness USED-FOR task - specific fine - tuning stage. training protocol USED-FOR latter. robust regularization USED-FOR MAML. fast adversarial attack generation CONJUNCTION computationally - light fine - tuning. computationally - light fine - tuning CONJUNCTION fast adversarial attack generation. unlabeled data augmentation CONJUNCTION fast adversarial attack generation. fast adversarial attack generation CONJUNCTION unlabeled data augmentation. auxiliary contrastive learning task USED-FOR MAML. auxiliary contrastive learning task USED-FOR adversarial robustness. adversarial robustness FEATURE-OF MAML. methods USED-FOR robust few - shot learning. OtherScientificTerm are robustness - promoting regularization, and meta - update stage. Metric is robustness adaptation. ",This paper studies the adversarial robustness in few-shot learning. The authors propose to use adversarial regularization to improve the robustness of the meta-model during the fine-tuning stage of the training protocol. The proposed method is based on adversarial attack generation and adversarial perturbation. The experiments show the effectiveness of the proposed method.   ,"This paper studies the robustness of few-shot learning with meta-learning (MAML) in the context of adversarial robustness. In particular, the authors propose a robustness-promoting regularization for MAML, which is based on a contrastive learning task. They show that this regularization can improve the generalization and robustness adaptation of the meta-model during the fine-tuning stage. They also show that it can be used to improve robustness during the task-specific fine- tuning stage. "
1656,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"optimization algorithms USED-FOR optimizer. Learning - to - learn USED-FOR optimizers. optimization algorithms USED-FOR Learning - to - learn. metagradient descent USED-FOR meta - objective. trajectory USED-FOR metagradient descent. metagradient descent USED-FOR approach. step size USED-FOR quadratic loss. backpropagation USED-FOR meta - gradient. neural networks USED-FOR learned optimizers. OtherScientificTerm are metagradient explosion / vanishing problems, metagradient, and numerical issues. Method is learning - to - learn approach. Task is gradient explosion / vanishing problems. ","This paper studies the problem of learning to learn in the metagradient explosion/vanishing problems, where the goal is to learn a meta-optimizer that minimizes the meta-gradient of the gradient of the original optimizer. The authors propose to use meta-propagation to learn meta-gradients of the optimizer, which are then used to compute meta-adversarial gradients for the original optimization problem. They show that meta-learning is computationally efficient and can be used to solve gradient explosion and vanishing problems.   ","This paper proposes a meta-learning approach for the metagradient explosion/vanishing problem. The main idea of the paper is to use the meta-gradients as meta-objectives to learn a trajectory for the optimization of the optimizer. The meta-adversarial objective is defined as a quadratic loss with a step size of $O(\sqrt{1/2}$ where $O(1)$ is the number of steps and $Omega(1/\sqrt(1,2)$ the size of the meta gradient. The authors show that this meta-gradient can be used to learn an optimal meta-optimizer for the problem. They also show that it is possible to learn meta-adaptive optimizers for this problem.  "
1665,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"methods USED-FOR large and complex graph problems. neural networks USED-FOR methods. dual views USED-FOR representations. neighborhood aggregation capability FEATURE-OF GVCLN. loss functions CONJUNCTION supervised loss. supervised loss CONJUNCTION loss functions. loss functions USED-FOR view - consistent representations. view - consistent loss USED-FOR consistent representation. view - consistent loss USED-FOR views. supervised loss CONJUNCTION view - consistent loss. view - consistent loss CONJUNCTION supervised loss. view - consistent loss CONJUNCTION pseudo - label loss. pseudo - label loss CONJUNCTION view - consistent loss. known labeled set USED-FOR supervised loss. common high - confidence predictions USED-FOR pseudo - label loss. GVCLN USED-FOR view - consistent representations. loss functions USED-FOR view - consistent representations. loss functions USED-FOR GVCLN. Citeseer CONJUNCTION PubMed. PubMed CONJUNCTION Citeseer. Cora CONJUNCTION Citeseer. Citeseer CONJUNCTION Cora. node classification tasks EVALUATE-FOR GVCLN. Task are acquisition of ground - truth labels, semisupervised learning, and classification tasks. OtherScientificTerm are viewing angles, observation objects, observation representations, and node features. ","This paper proposes a new method for learning representations for node classification tasks. The proposed method is based on the dual view-consistency loss, which aims to learn a representation that is consistent across multiple views of the same node. The method is evaluated on several graph classification tasks and achieves state-of-the-art performance. ","This paper proposes a new graph classification method called GVCLN. The main idea is to combine the view-consistency loss with the supervised loss and pseudo-label loss. The proposed method is evaluated on Citeseer, Cora, and PubMed datasets. "
1674,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"physics bias USED-FOR neural networks. neural networks USED-FOR dynamics of systems. coordinates USED-FOR conserved quantities. cyclic coordinates HYPONYM-OF coordinates. Hamiltonian dynamics USED-FOR classical systems. canonical transformations USED-FOR coordinates. Hamiltonian dynamics USED-FOR loss functions. loss functions USED-FOR coordinates. network USED-FOR conserved quantities. network COMPARE networks. networks COMPARE network. network USED-FOR dynamics of the system. Hamiltonian USED-FOR networks. classical physics systems EVALUATE-FOR method. synthetic and experimental data EVALUATE-FOR method. symmetry orbits PART-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF phase space. lower dimensional sub - spaces of phase space HYPONYM-OF symmetry orbits. analytic formulae USED-FOR networks. conserved quantities USED-FOR networks. ( angular ) momentum HYPONYM-OF conserved quantities. OtherScientificTerm are dynamics, and symmetries. Task is description of physical systems. ","This paper proposes a method for learning the dynamics of systems using neural networks. The method is based on the observation that the classical Hamiltonian dynamics of classical systems can be expressed as a function of the loss functions of the system. The authors show that the loss function of a neural network can be decomposed into two terms: (1) the angular momentum, and (2) the cyclic momentum, which is a conserved quantity. The proposed method is shown to be able to learn the dynamics in terms of the conserved quantities.   ","This paper proposes a method to learn the dynamics of a system in terms of conserved quantities. The main idea is to use the Hamiltonian dynamics of the system as a surrogate for the dynamics in the system, and then use a neural network to model the dynamics. The proposed method is based on the idea that the dynamics can be represented as a sequence of symmetries in the phase space, and that the network can be used to learn a Hamiltonian for the system. The method is evaluated on synthetic and experimental data, and the results show that the proposed method outperforms the state-of-the-art."
1683,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"models USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) USED-FOR graph representation learning tasks. Graph neural networks ( GNNs ) HYPONYM-OF models. gradient boosted decision trees ( GBDT ) COMPARE machine learning methods. machine learning methods COMPARE gradient boosted decision trees ( GBDT ). heterogeneous tabular data EVALUATE-FOR machine learning methods. approach USED-FOR graphs with tabular node features. GNN models USED-FOR networks. homogeneous sparse features FEATURE-OF networks. GBDT CONJUNCTION GNN. GNN CONJUNCTION GBDT. architecture USED-FOR GBDT. architecture USED-FOR GNN. GBDT model USED-FOR heterogeneous features. GNN USED-FOR graph structure. endto - end optimization USED-FOR model. Material is heterogeneous setting. OtherScientificTerm are trees, and graphs with tabular features. Method is GBDT and GNN models. ","This paper proposes a novel GNN-based model for graph representation learning on heterogeneous tabular data. The proposed model is based on gradient-boosted decision trees (GBDT), which is an extension of gradient-based GNNs. The main contribution of the paper is to show that GBDT and GNN can be used together to learn graphs with tabular node features. The experiments show that the proposed model outperforms the state-of-the-art GNN and GBDT models in terms of accuracy and computational cost.",This paper proposes a novel GNN-based approach for heterogeneous tabular graph representation learning. The main idea is to use GNNs with sparse sparse features and GBDTs with heterogeneous sparse features to learn graphs with tabular features. The authors show that the proposed approach outperforms the state-of-the-art GBDT and GNN models on heterogeneous data. They also show that GBDT can be used to learn heterogeneous features in heterogeneous graphs.
1692,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"Meta - learning USED-FOR fast adaptation. train - validation split USED-FOR meta - learning. train - validation split COMPARE non - splitting method. non - splitting method COMPARE train - validation split. training EVALUATE-FOR non - splitting method. per - task data USED-FOR non - splitting method. train - validation split USED-FOR linear centroid meta - learning problem. splitting method COMPARE non - splitting method. non - splitting method COMPARE splitting method. regularization parameter CONJUNCTION split ratio. split ratio CONJUNCTION regularization parameter. split ratio USED-FOR methods. regularization parameter USED-FOR methods. asymptotic excess risk EVALUATE-FOR non - splitting method. non - splitting method COMPARE splitting method. splitting method COMPARE non - splitting method. Generic are predictor, and model. Method are linear models, splitting and non - splitting methods, and data splitting. OtherScientificTerm is data distribution. ","This paper studies the linear centroid meta-learning problem with data splitting in the training process. The authors show that data splitting is necessary to improve the performance of the model, and propose two methods to do so. The first method is based on splitting the training data into training and validation sets. The second method is a non-splitting method based on a regularization term that encourages the split ratio to be close to zero. ","This paper studies the problem of linear centroid meta-learning, where the data distribution is split into two parts: (1) per task data and (2) per-task data. The authors propose two methods for splitting the data, one for per task and one for each task. They show that the splitting method outperforms the non-splitting method in terms of the asymptotic excess risk. They also provide a theoretical analysis of the split ratio of the two methods. "
1701,SP:bb566eda95867f83a80664b2f685ad373147c87b,"noisy training data USED-FOR hard confident examples. physics USED-FOR momentum. non - simple patterns PART-OF hard confident examples. Me - Momentum USED-FOR hard confident examples. classification EVALUATE-FOR Me - Momentum. OtherScientificTerm are decision boundary, hard examples, and simple patterns. Method are classifiers, deep learning paradigm, deep neural networks, and classifier. Task are Extracting confident examples, and extracting hard confident examples. Material are noisy labels, and inaccurately labeled examples. Generic is approach. ","This paper studies the problem of extracting hard confident examples from noisy labels in deep neural networks. The authors propose Me-Momentum, a method to extract hard examples from the noisy training data. The method is based on the observation that the decision boundary of a deep neural network is defined by the momentum of the classifier, which is a function of the number of noisy examples and the distance between the decision boundaries.  The authors show that Me-momentum can be used to identify hard examples in the training data that are not easy to classify due to non-simple patterns in the data.    The main contribution of the paper is a theoretical analysis of the dynamics of momentum in deep learning models. ","This paper proposes Me-Momentum, a method for extracting hard-confident examples from noisy training data. Me-momentum is motivated by the fact that the decision boundary of a classifier can be defined as the distance between the hard examples and simple patterns in the data. The authors propose to use momentum as a measure of confidence in the classifier, and show that it can be used to extract hard examples from the training data that are not easy to classify due to the noisy labels. They also provide a theoretical analysis of the physics of momentum. "
1710,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,certified defenses USED-FOR data poisoning attacks. certified defenses USED-FOR majority vote mechanism. k nearest neighbors ( kNN ) CONJUNCTION radius nearest neighbors ( rNN ). radius nearest neighbors ( rNN ) CONJUNCTION k nearest neighbors ( kNN ). intrinsic majority vote mechanisms FEATURE-OF Nearest neighbor algorithms. radius nearest neighbors ( rNN ) HYPONYM-OF Nearest neighbor algorithms. k nearest neighbors ( kNN ) HYPONYM-OF Nearest neighbor algorithms. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic majority vote mechanisms USED-FOR certified robustness guarantees. intrinsic majority vote mechanisms USED-FOR rNN. intrinsic majority vote mechanisms USED-FOR kNN. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. kNN CONJUNCTION rNN. rNN CONJUNCTION kNN. intrinsic certified robustness guarantees EVALUATE-FOR rNN. intrinsic certified robustness guarantees EVALUATE-FOR kNN. intrinsic certified robustness guarantees COMPARE certified defenses. certified defenses COMPARE intrinsic certified robustness guarantees. Task is Data poisoning attacks. Method is machine learning model. OtherScientificTerm is voter. ,"This paper studies the robustness of nearest neighbor algorithms against data poisoning attacks. The authors propose two methods: k nearest neighbors (kNN) and radius nearest neighbor (rNN). The authors show that kNN and rNN can be certified robust against poisoning attacks in the presence of a majority vote mechanism. They show that the certified robustness guarantees of kNN are better than that of rNN, and they also show that rNN is more robust than kNN.","This paper studies the robustness of k-nearest neighbor (kNN) and r-neighbor (rNN) algorithms against data poisoning attacks. The authors propose a new certified robustness guarantee for kNN and rNN, which is based on the notion of intrinsic majority vote mechanism (i.e., the number of voters that can be poisoned). They show that kNN can be certified more robust than rNN under certain conditions. They also show that rNN can also be certified robust under some conditions."
1719,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"training time CONJUNCTION model. model CONJUNCTION training time. model EVALUATE-FOR it. training time EVALUATE-FOR it. stochastic gradient decent ( SGD ) method USED-FOR deep learning models. batch size selection problem USED-FOR graph neural network ( GNN ). SGD method USED-FOR graph neural network ( GNN ). variance of gradients CONJUNCTION compute time. compute time CONJUNCTION variance of gradients. compute time FEATURE-OF mini - batch. compute time FEATURE-OF metric. variance of gradients PART-OF metric. formula USED-FOR optimal batch size. estimator USED-FOR gradients. randomness USED-FOR estimator. Ogbnarxiv CONJUNCTION Reddit. Reddit CONJUNCTION Ogbnarxiv. Ogbn - products CONJUNCTION Ogbnarxiv. Ogbnarxiv CONJUNCTION Ogbn - products. FastGCN CONJUNCTION GraphSAINT. GraphSAINT CONJUNCTION FastGCN. Reddit CONJUNCTION Pubmed. Pubmed CONJUNCTION Reddit. ClusterGCN CONJUNCTION FastGCN. FastGCN CONJUNCTION ClusterGCN. GraphSAINT HYPONYM-OF datasets. FastGCN HYPONYM-OF datasets. Pubmed HYPONYM-OF datasets. Ogbn - products HYPONYM-OF datasets. Reddit HYPONYM-OF datasets. Ogbnarxiv HYPONYM-OF datasets. deep learning models COMPARE GNNs. GNNs COMPARE deep learning models. large batch sizes USED-FOR GNNs. OtherScientificTerm are Batch size, batch - size, and batch size. Method are decent model, and GNN. ","This paper studies the batch size selection problem in graph neural networks (GNNs). The authors propose to use the variance of gradients and compute time as a metric to evaluate batch size. They show that this metric can be used to find the optimal batch size for GNNs. They also provide a formula for finding the optimal number of mini-batches for a given batch size, and show that it can be done efficiently. ",This paper proposes a new metric to measure the variance of gradients in the training of a graph neural network (GNN) model. The variance of the gradients is defined as the number of mini-batch sizes of a GNN trained with a decent model. This metric is based on the SGD-based SGD method. The authors show that the optimal batch-size selection problem for GNNs can be formulated as a batch size selection problem. They provide a formula for finding the batch size that maximizes the variance and compute time of the mini-batch. They also provide an estimator of the variance for the best batch size for the GNN model.
1728,SP:30d97322709cd292a49f936c767099f11b0e2913,"neural network classifiers USED-FOR real - world applications. confidence scores USED-FOR detecting misclassification errors. framework USED-FOR detecting misclassification errors. framework USED-FOR confidence scores. Gaussian Processes USED-FOR calibrated confidence scores. confidence estimation methods COMPARE approach. approach COMPARE confidence estimation methods. UCI datasets EVALUATE-FOR confidence estimation methods. method USED-FOR neural network classifiers. deep learning architecture USED-FOR vision task. OtherScientificTerm are lowconfidence predictions, and classifier ’s inherent confidence indicators. Metric is confidence metrics. Method is RED. Material is out - of - distribution and adversarial samples. ","This paper proposes to use confidence scores to detect misclassification errors in neural network classifiers. The proposed method is based on a Gaussian Processes (GP) model, which is used to estimate confidence scores for a classifier’s inherent confidence indicators. The method is evaluated on UCI and CIFAR-10 datasets. ","This paper proposes a new confidence-based method for detecting misclassification errors in neural network classifiers. The method is based on Gaussian Processes (GPs), which is a well-studied framework for estimating confidence scores for neural networks. The main contribution of the paper is the use of GPs to calibrate the confidence scores of a classifier, which can be used to detect misclassifications. The proposed method is evaluated on UCI datasets, and it is shown to be competitive with other confidence estimation methods. "
1737,SP:131b3da98f56d3af273171f496b217b90754a0a7,information retrieval PART-OF natural language processing systems. open domain question answering HYPONYM-OF natural language processing systems. methods COMPARE continuous representations. continuous representations COMPARE methods. neural networks USED-FOR continuous representations. hand - crafted features USED-FOR methods. supervised data USED-FOR retriever model. supervised data USED-FOR methods. retriever models USED-FOR downstream tasks. technique USED-FOR retriever models. technique USED-FOR downstream tasks. approach USED-FOR synthetic labels. attention scores USED-FOR task. synthetic labels USED-FOR retriever. reader model USED-FOR task. attention scores USED-FOR reader model. approach USED-FOR task. reader model USED-FOR approach. attention scores USED-FOR approach. retrieved documents USED-FOR approach. retrieved documents USED-FOR task. question answering EVALUATE-FOR method. Task is knowledge distillation. ,This paper proposes a method for knowledge distillation in open-domain question answering. The method is based on the idea of using synthetic labels to train a retriever model and a reader model. The retriever is trained using the synthetic labels and the reader model is trained on the retrieved documents. The proposed method is evaluated on the task of open domain question answering and achieves state-of-the-art performance. ,This paper proposes a method for open-domain knowledge distillation in natural language processing systems. The method is based on the idea of using synthetic labels to train a retriever model and a reader model. The retriever is trained on the synthetic labels and the reader model on the retrieved documents. The attention scores of the retriever and reader model are learned by combining the attention scores from the synthetic label and the attention score from the retrieved document. Experiments show that the proposed method outperforms state-of-the-art methods on a variety of downstream tasks. 
1746,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"exploration USED-FOR reinforcement learned ( RL ) controllers. software engineering CONJUNCTION controller synthesis. controller synthesis CONJUNCTION software engineering. constraints FEATURE-OF constrained Markov decision process. controller synthesis USED-FOR safety methods. software engineering USED-FOR safety methods. formal languages USED-FOR them. finite automata USED-FOR constraint violations. finite automata USED-FOR constraints. Constraint states USED-FOR dense cost function. Constraint states USED-FOR MDP state. methods USED-FOR RL algorithms. constraints USED-FOR RL algorithms. Safety Gym HYPONYM-OF constraints. Atari environments HYPONYM-OF constraints. OtherScientificTerm are safety conditions, safety critical situations, and joint MDP / constraint dynamics. Method are safe controller, and learning. ","This paper studies the problem of safety exploration in reinforcement learning with constrained MDPs. The authors propose a method for learning a safe controller that is able to handle constraints on the MDP dynamics. The proposed method is based on the idea of learning a dense cost function that can be used as a constraint on the cost function of an MDP. The cost function is defined as the sum of the cost of a set of MDP states that satisfy the constraints. The paper shows that this cost function can be expressed in terms of a finite set of finite automata, which can then be used to learn a controller that can handle the constraints in a safe manner. ","This paper studies the problem of constrained Markov Decision Process (MDP) control in reinforcement learning (RL) where the controller is constrained by a set of MDP constraints. The authors propose a new language for modeling the MDP dynamics, which they call Safety Gym, which is composed of two parts: (1) a finite automaton that can be used to represent the dynamics of the controller, and (2) a dense cost function that is used to model the dynamics between the controller and MDP. The paper shows that Safety Gym can be applied to a variety of environments, including Atari environments, and that it can be combined with existing controller synthesis methods to improve the safety of RL."
1755,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"binary classification algorithm USED-FOR models. decision tree learning HYPONYM-OF binary classification algorithm. first - class transparency FEATURE-OF models. decision tree model USED-FOR comprehensibility of classifications. Cascading Decision Trees HYPONYM-OF decision tree model. decision path CONJUNCTION explanation path. explanation path CONJUNCTION decision path. monolithic decision tree COMPARE decision subtrees. decision subtrees COMPARE monolithic decision tree. subtree USED-FOR features. subtrees USED-FOR positive classification. model COMPARE decision tree model. decision tree model COMPARE model. datasets CONJUNCTION real - world applications. real - world applications CONJUNCTION datasets. datasets EVALUATE-FOR model. real - world applications EVALUATE-FOR model. positive classifications EVALUATE-FOR model. explanation depth EVALUATE-FOR model. real - world applications EVALUATE-FOR algorithm. datasets EVALUATE-FOR algorithm. Method are decision trees, cascading decision subtrees, and cascading decision trees. OtherScientificTerm is decision paths. ","This paper proposes a novel decision tree model, called Cascading Decision Trees (CDFT), to improve the comprehensibility of classifications in binary classification. CDFT consists of a decision tree with a decision path and an explanation path, where the explanation path is composed of a set of decision subtrees. The explanation path consists of the decision tree features, and the decision subtree features are used for positive classification. The main contribution of the paper is to show that the explanation depth of the model can be improved by a factor of at least 1.5. The proposed model achieves state-of-the-art performance on several benchmark datasets.","This paper proposes a new model for decision tree learning, called Cascading Decision Trees (CDFT). CDFT is based on the idea of cascading decision subtrees, which is an extension of monolithic decision trees, where each decision tree is composed of a decision tree, a decision path, and an explanation path. The main idea is that the decision tree model should be able to capture the comprehensibility of classifications. The authors show that the cascaded decision tree can be used to improve the first-class transparency of decision trees. They also show that their model can be applied to a variety of real-world applications."
1764,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"random, static sparsity pattern FEATURE-OF weight tensors. random, static sparsity pattern USED-FOR models. training accuarcy EVALUATE-FOR model. Gaussian Process kernels USED-FOR models. sparse finite - width model kernel CONJUNCTION infinite - width kernel. infinite - width kernel CONJUNCTION sparse finite - width model kernel. Method is neural networks. OtherScientificTerm are network width, and model width. ","This paper studies the sparsity of weight tensors in deep neural networks. The authors show that the weight tensor sparsity depends on the width of the network. They show that if the network width is finite, then the weight sparsity increases linearly with the width, and if the width is infinite, the weights sparsity decreases linearly as the width increases. They also show that a Gaussian process kernel with finite width is equivalent to an infinite-width model kernel.  ","This paper studies the problem of training neural networks with Gaussian process kernels. The authors show that Gaussian Process kernels are sparse finite-width model kernels and infinite-width kernel. They also show that the weight tensor sparsity pattern of weight tensors can be modeled as a random, static sparsity. They show that this can be used to improve the training accuarcy of a model."
1773,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,world knowledge CONJUNCTION entities. entities CONJUNCTION world knowledge. Knowledge graphs ( KGs ) USED-FOR world knowledge. entities PART-OF Knowledge graphs ( KGs ). they COMPARE pre - trained language models. pre - trained language models COMPARE they. KG USED-FOR language modeling. joint pre - training framework USED-FOR knowledge graph. knowledge graph CONJUNCTION language. language CONJUNCTION knowledge graph. joint pre - training framework USED-FOR language. JAKET USED-FOR knowledge graph. JAKET USED-FOR language. JAKET HYPONYM-OF joint pre - training framework. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module CONJUNCTION language module. language module CONJUNCTION knowledge module. knowledge module USED-FOR embeddings. language module USED-FOR context - aware initial embeddings. knowledge - aware NLP tasks EVALUATE-FOR framework. knowledge in language understanding USED-FOR framework. OtherScientificTerm is graph. Method is pre - trained model. ,"This paper proposes a knowledge-based pre-training framework for knowledge-aware NLP. The proposed method is based on a knowledge graph (KG) and a language model (JAKET). The KG is used as a pre-trained language model, and the language model is used to model the knowledge of the KG. The authors show that the proposed method outperforms the baselines on a variety of knowledge-driven NLP tasks.","This paper proposes a joint pre-training framework for knowledge-aware NLP. The authors propose a joint knowledge graph (KG) and language model (JAKET) framework to combine knowledge graph and language. The knowledge graph consists of a knowledge module, a language module, and an embedding module. The language module is used to generate context-aware initial embeddings, and the knowledge module uses the language module to generate the embedding modules. Experiments are conducted on a variety of knowledge-based NLP tasks. "
1782,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"hand - designed loss functions USED-FOR specific orders. domain - specific insight USED-FOR approaches. unsupervised learner USED-FOR autoregressive orders. neural network USED-FOR variational inference. learner HYPONYM-OF neural network. autoregressive ordering USED-FOR variational inference. algorithm USED-FOR end - to - end optimization. policy gradients USED-FOR algorithm. algorithm USED-FOR autoregressive orders. algorithm COMPARE fixed orders. fixed orders COMPARE algorithm. sequence modeling tasks EVALUATE-FOR algorithm. autoregressive orders COMPARE fixed orders. fixed orders COMPARE autoregressive orders. sequence modeling tasks EVALUATE-FOR solution. Task is language modeling. OtherScientificTerm are predefined ordering, insertion operations, domain - specific prior, latent variable, and variational lower bound. Metric is time complexity. ","This paper proposes an autoregressive ordering for variational inference in language modeling. In this setting, the goal is to learn a latent variable that can be used as a prior over a set of input sequences. The authors propose to use an unsupervised learner to learn this latent variable, and then use a variational lower bound on the gradient of the learned latent variable to optimize the policy gradient. The proposed method is shown to be efficient in terms of time complexity.   ","This paper proposes an autoregressive learning algorithm for variational inference in the context of language modeling. The main idea is to use an unsupervised learner to learn a latent variable of the domain-specific prior of the learner, and then use a neural network to infer the latent variable from the learned latent variable. The authors provide a lower bound on the time complexity of the algorithm, and show that it can be used for end-to-end optimization. They also provide a theoretical analysis of the lower bound."
1791,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - related applications. large graphs USED-FOR GCNs. evolving parameters USED-FOR optimization. doubly variance reduction schema USED-FOR sampling method. O(1 / T ) convergence rate EVALUATE-FOR it. schema USED-FOR sampling methods. them USED-FOR large real - world graphs. OtherScientificTerm are computational and memory issues, nodes, memory budget, and induced variance. Method are sampling - based methods, variance of sampling methods, forward propagation, and backward propagation. Generic is works. Metric is theoretical convergence guarantees. ",This paper proposes a sampling-based sampling method for graph convolutional neural networks (GCNs) with evolving parameters. The main idea is to use a doubly variance reduction scheme to reduce the variance of the forward propagation and backward propagation. The proposed sampling method is shown to have a convergence rate of O(1/T) under the assumption that the number of nodes in the graph grows linearly with the size of the graph. ,This paper proposes a new sampling-based sampling method for graph convolutional neural networks (GCNs). The main contribution of the paper is to propose a new variance reduction scheme for the sampling of GCNs. The main idea is to use a doubly variance reduction (doubled variance) scheme to reduce the variance of sampling methods. The authors show that the proposed method converges to a convergence rate of O(1/T) with O(T) variance reduction rate. They also provide theoretical convergence guarantees for the proposed sampling method.
1800,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"deep neural network methods USED-FOR image manipulation tasks. conditional adversarial generator USED-FOR complex image manipulations. edges CONJUNCTION segmentation. segmentation CONJUNCTION edges. primitive input representation USED-FOR generator. Task are Image manipulation, and single image training. Generic are task, network, method, and it. Method are deep methods, and augmentation method. ","This paper proposes a method for image manipulation tasks that uses a conditional adversarial generator to learn a primitive representation of the input image, which is then used as a primitive input representation for image segmentation. The proposed method is evaluated on a variety of image manipulation benchmarks and compared with a number of state-of-the-art methods. The results show that the proposed method outperforms existing methods on most of the benchmarks. ","This paper proposes a new method for image manipulation. The proposed method is based on a conditional adversarial generator (CAG), which can be used to perform complex image manipulations. The authors show that the proposed method can be applied to a variety of image manipulation tasks, including edge manipulation, segmentation, and segmentation. They also show that their method is able to learn a primitive input representation for each task, which is then used to augment the original input representation."
1809,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"malware detection CONJUNCTION cloud computing. cloud computing CONJUNCTION malware detection. biomedical analysis CONJUNCTION malware detection. malware detection CONJUNCTION biomedical analysis. Detecting the Maximum Common Subgraph ( MCS ) USED-FOR biomedical analysis. heuristics in search USED-FOR MCS solvers. Graph Neural Network based model USED-FOR MCS detection. GLSEARCH HYPONYM-OF Graph Neural Network based model. branch and bound algorithm USED-FOR backbone search algorithm. model USED-FOR subgraphs. branch and bound algorithm USED-FOR subgraphs. branch and bound algorithm USED-FOR model. search process USED-FOR supervision. imitation learning stage USED-FOR agent. search process USED-FOR DQN. search CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION search. framework USED-FOR reinforcement learning. framework USED-FOR search. synthetic and real - world large graph pairs EVALUATE-FOR model. MCS solvers CONJUNCTION neural graph matching network models. neural graph matching network models CONJUNCTION MCS solvers. model COMPARE neural graph matching network models. neural graph matching network models COMPARE model. model COMPARE MCS solvers. MCS solvers COMPARE model. synthetic and real - world large graph pairs EVALUATE-FOR neural graph matching network models. synthetic and real - world large graph pairs EVALUATE-FOR MCS solvers. OtherScientificTerm are Maximum Common Subgraph ( MCS ), large graph pairs, limited search budget, and node selection decision. Task are drug design, extraction of common substructures, and MCS computation. Method is node selection heuristics. ","This paper proposes a method for finding the Maximum Common Subgraph (MCS) in large graphs. The method is based on a graph neural network (GNN) architecture, which is trained using reinforcement learning. The main idea is to use a DQN-based backbone search algorithm to find the most common subgraphs among the large graph pairs. The proposed method is evaluated on synthetic and real-world graph pairs and achieves state-of-the-art performance.","This paper proposes a novel method for detecting the Maximum Common Subgraph (MCS) from large graph pairs. The method is based on GLSEARCH, a graph neural network (GNN) based model for finding the subgraphs of a large graph. The main idea of the method is to use a DQN-based search process to find the most common subgraph. The search process is supervised by an imitation learning stage, where the agent learns the search process in a reinforcement learning stage. Experiments on synthetic and real-world graph pairs show that the proposed method outperforms the state-of-the-art methods."
1818,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"PC2WF USED-FOR wireframe model. network USED-FOR wireframe. vertices CONJUNCTION edges. edges CONJUNCTION vertices. model USED-FOR architecture. it USED-FOR candidate vertices. features USED-FOR it. candidate edges USED-FOR wireframe. ground truth wireframes FEATURE-OF synthetic dataset. real - world dataset EVALUATE-FOR model. synthetic dataset EVALUATE-FOR model. model COMPARE baselines. baselines COMPARE model. model USED-FOR wireframe abstractions. OtherScientificTerm are 3D point cloud, line segments, feature vectors, and corner vertices. Task is Recovering the wireframe. Generic is It. ","This paper proposes a method for recovering the ground truth wireframe from a 3D point cloud. The method is based on PC2WF, a neural network architecture that learns to predict the vertices and edges of a point cloud from a set of feature vectors. The model is trained on a synthetic dataset and on a real-world dataset, where it achieves state-of-the-art results.","This paper proposes a new model for recovering the wireframe from a 3D point cloud. The model is based on the PC2WF architecture, which is a network-based model that learns a set of vertices, edges, and line segments from a point cloud, which are then used to reconstruct a wireframe. The proposed model is evaluated on a synthetic dataset and a real-world dataset, where it is shown to perform better than other baselines. "
1827,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"assumptions FEATURE-OF distribution of noise. assumptions USED-FOR stochastic optimization. uniform bound USED-FOR moments of the gradient noise. optimal convergence rates FEATURE-OF stochastic optimization. optimization algorithms USED-FOR neural network training. noise level FEATURE-OF stochastic gradients. convergence rates FEATURE-OF stochastic gradient methods. adaptive step size methods COMPARE SGD. SGD COMPARE adaptive step size methods. Method are neural networks, online estimator of the noise level, and RMSProp. OtherScientificTerm are noise, nonstationary behavior of noise, stochastic oracle, noise variation, step - size, noise variability, noise statistics, and theoretical guarantees. ","This paper studies the convergence of stochastic gradient methods in the presence of non-stationary noise in the training of neural networks. In particular, the authors show that the moments of the gradient noise with respect to a uniform bound can be estimated online, and the convergence rate is shown to be O(1/\epsilon^2) under certain assumptions on the distribution of the noise. The authors also provide an online estimator for the noise level, and show that this estimator is equivalent to RMSProp. Theoretical results are then used to show that adaptive step-size methods can converge to the optimal convergence rate of SGD with the proposed estimator.","This paper studies the convergence rate of stochastic gradient descent (SGD) under the assumption of non-stationary behavior of noise. The authors provide a uniform bound on the moments of the gradient noise of SGD. They show that SGD converges to the optimal convergence rate with respect to the noise level. They also provide theoretical guarantees on the step-size, noise variability, and noise statistics of noise statistics."
1836,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,Prior word alignment USED-FOR translation. statistical machine translation ( SMT ) models USED-FOR word alignment. method USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR neural machine translation ( NMT ). prior word alignment information USED-FOR method. dictionaries USED-FOR approaches. decoding speed EVALUATE-FOR methods. model PART-OF neural MT model. learning model USED-FOR target information. target information USED-FOR MT input. method COMPARE baseline model. baseline model COMPARE method. English - Korean EVALUATE-FOR baseline model. English - Korean EVALUATE-FOR method. Generic is prior. Task is decoding process. Method is enhancement learning model. OtherScientificTerm is prior alignment information. ,"This paper proposes a method to improve the performance of neural machine translation (NMT) models by incorporating prior word alignment information. The proposed method is based on the observation that prior alignment information can improve the decoding speed of NMT models. To this end, the authors propose an enhancement learning model to learn the target information from the MT input, which is then used to improve NMT performance. The method is evaluated on the English-Korean translation task and achieves state-of-the-art performance.",This paper proposes a method to improve the performance of neural machine translation (NMT) models by using prior word alignment information. The method is based on the idea that the prior alignment information can be used as an enhancement learning model in the training of the NMT model. The proposed method is evaluated on English-Korean and English-Chinese datasets. 
1845,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,MDP Playground HYPONYM-OF Reinforcement Learning ( RL ) algorithms. MDP Playground HYPONYM-OF benchmark. benchmark EVALUATE-FOR Reinforcement Learning ( RL ) algorithms. dimensions of hardness FEATURE-OF MDP Playground. stochasticity CONJUNCTION image representations. image representations CONJUNCTION stochasticity. image representations CONJUNCTION irrelevant features. irrelevant features CONJUNCTION image representations. sparsity of rewards CONJUNCTION stochasticity. stochasticity CONJUNCTION sparsity of rewards. time unit CONJUNCTION action range. action range CONJUNCTION time unit. irrelevant features CONJUNCTION time unit. time unit CONJUNCTION irrelevant features. delayed rewards CONJUNCTION rewardable sequences. rewardable sequences CONJUNCTION delayed rewards. rewardable sequences CONJUNCTION sparsity of rewards. sparsity of rewards CONJUNCTION rewardable sequences. action range HYPONYM-OF hardness dimensions. time unit HYPONYM-OF hardness dimensions. sparsity of rewards HYPONYM-OF hardness dimensions. stochasticity HYPONYM-OF hardness dimensions. irrelevant features HYPONYM-OF hardness dimensions. delayed rewards HYPONYM-OF hardness dimensions. image representations HYPONYM-OF hardness dimensions. rewardable sequences HYPONYM-OF hardness dimensions. benchmarks EVALUATE-FOR RL algorithms. benchmarks EVALUATE-FOR RL algorithms. fine - grained control FEATURE-OF environments ’ hardness. MDP Playground USED-FOR adaptive and intelligent RL algorithms. MDP Playground EVALUATE-FOR algorithms. OtherScientificTerm is hardness. Material is OpenAI Gym. Generic is dimensions. ,"This paper proposes a new benchmark for reinforcement learning called MDP Playground, which aims to measure the hardness of the environments in the MDP. The main contribution of the paper is a theoretical analysis of the impact of the hardness on the performance of different RL algorithms on the benchmark. The authors show that the hardness is related to the number of rewards, the amount of delayed rewards, and the sparsity of rewards. They also show that different hardness dimensions such as the time unit, action range, and irrelevant features can be used to quantify the importance of each of these dimensions.  ","This paper proposes a new benchmark, MDP Playground, to measure the hardness of the environments in the MDP. The main contribution of the paper is a theoretical analysis of the different dimensions of hardness of MDP, which are defined as the time unit, action range, delayed rewards, and irrelevant features of the environment. The authors show that these dimensions can be used to evaluate the performance of a variety of RL algorithms on the benchmark.  "
1854,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"they USED-FOR overconfident predictions. approaches USED-FOR classification models. Isotonic Regression USED-FOR regression calibration. Isotonic Regression USED-FOR regression calibration. formulation USED-FOR quantile regularizer. quantile regularizer USED-FOR probabilistic regression model. approaches USED-FOR regression models. Dropout VI CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION Dropout VI. approach USED-FOR regression models. calibration USED-FOR regression models. architectures USED-FOR uncertainty estimates. approach USED-FOR calibration. architectures USED-FOR regression models. Deep Ensembles HYPONYM-OF uncertainty estimates. Dropout VI HYPONYM-OF uncertainty estimates. Method are Deep learning models, quantile calibration, and entropy estimation. Generic are it, model, and method. ",This paper proposes a quantile calibration method for regression models. The proposed method is based on isotonic regression. Theoretical analysis is provided to show that the quantile regularizer is necessary and sufficient for calibration. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. ,"This paper proposes a new quantile calibration method for probabilistic regression models. The key idea is to use a quantile regularizer to calibrate the uncertainty of the model. The proposed method is based on the Isotonic Regression (IR) framework. The authors show that the proposed method can be applied to a wide range of deep learning models, including Dropout VI, Deep Ensembles, and Deep Ensemble."
1863,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"6 - DoF localisation CONJUNCTION 3D dense reconstruction in spatial environments. 3D dense reconstruction in spatial environments CONJUNCTION 6 - DoF localisation. deep state - space model USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR 3D dense reconstruction in spatial environments. approximate Bayesian inference USED-FOR 6 - DoF localisation. multiple - view geometry CONJUNCTION rigid - body dynamics. rigid - body dynamics CONJUNCTION multiple - view geometry. rigid - body dynamics USED-FOR learning and domain knowledge. multiple - view geometry USED-FOR learning and domain knowledge. learning and domain knowledge USED-FOR approach. neural networks CONJUNCTION differentiable raycaster. differentiable raycaster CONJUNCTION neural networks. variational inference CONJUNCTION neural networks. neural networks CONJUNCTION variational inference. realistic unmanned aerial vehicle flight data EVALUATE-FOR approach. model USED-FOR generative prediction and planning. OtherScientificTerm is spatial environments. Method are visual SLAM solutions, and visual - inertial odometry systems. ",This paper proposes a method for 3D dense reconstruction in spatial environments using a deep state-space model and a differentiable raycaster. The method is based on an approximate Bayesian inference for approximate DoF localisation. The proposed method uses multiple-view geometry and rigid-body dynamics for learning and domain knowledge. Experiments show that the proposed method achieves state-of-the-art performance on realistic aerial vehicle flight data. ,This paper proposes a deep state-space model for 3D dense reconstruction in spatial environments and 6-DoF localisation. The model is based on a deep neural network and a differentiable raycaster. The authors show that the model can be used for generative prediction and planning and can be applied to real-world environments. The proposed method is evaluated on a variety of datasets. 
1872,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"textual descriptions USED-FOR generalization of control policies. symbol grounding CONJUNCTION control policy. control policy CONJUNCTION symbol grounding. environment rewards USED-FOR supervision. environment rewards USED-FOR EMMA. free - form natural language FEATURE-OF text manuals. framework EVALUATE-FOR model. crowd - sourcing USED-FOR free - form natural language. crowd - sourcing USED-FOR text manuals. zeroshot generalization EVALUATE-FOR EMMA. noisy descriptions FEATURE-OF grounding. EMMA USED-FOR grounding. OtherScientificTerm are prior knowledge, concrete supervision, and dynamics. Generic is policies. Method is multi - modal entity - conditioned attention module. ","This paper proposes EMMA, a model that learns to generalize from text descriptions. The model is based on a multi-modal entity-conditioned attention module, which is trained to focus on entities in the text. Empirical results show that EMMA can generalize well to unseen environments.   ","This paper proposes EMMA, a novel framework for generalization of control policies based on natural language descriptions. Empirical results show that EMMA is able to generalize well to a wide range of environments, and that it can generalize to zeroshot generalization in the presence of noisy descriptions. The model is based on a multi-modal entity-conditioned attention module, which is trained by crowd-sourcing a set of free-form natural language manuals from the public domain. Experiments are conducted on a variety of environments to demonstrate the effectiveness of EMMA."
1881,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"Policy gradient algorithms USED-FOR decision making and control tasks. high sample complexity CONJUNCTION instability issues. instability issues CONJUNCTION high sample complexity. instability issues EVALUATE-FOR methods. high sample complexity EVALUATE-FOR methods. approach USED-FOR critic. actor - critic framework FEATURE-OF critic. mean value COMPARE absolute value. absolute value COMPARE mean value. continuous control tasks CONJUNCTION algorithms. algorithms CONJUNCTION continuous control tasks. sparse rewards USED-FOR method. Method are actor - critic algorithms, actor - critic, and gradient estimator. OtherScientificTerm is value function. ",This paper studies policy gradient algorithms for continuous control tasks with sparse rewards. The authors propose an actor-critic framework where the policy gradient estimator is used to estimate the mean value of the value function. They show that the proposed method achieves better performance than the state-of-the-art algorithms in terms of sample complexity and instability.  ,"This paper proposes a new actor-critic framework for policy gradient algorithms for decision making and control tasks with high sample complexity and instability issues. The main contribution of the paper is to propose a new method for estimating the mean value of a policy gradient estimator. The method is based on the actor critic framework, where the value function is defined as the sum of the mean and the mean of the critic’s value function. The authors show that the mean-value estimator can be used to estimate the gradient of the policy gradient. They also show that their method can be applied to continuous control tasks.  "
1890,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"network USED-FOR overparameterization. overparameterized regime FEATURE-OF depth. locality of the relevant feature FEATURE-OF classification rule. initialization CONJUNCTION infinitesimal learning rate. infinitesimal learning rate CONJUNCTION initialization. finite networks COMPARE neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) COMPARE finite networks. infinitely wide network USED-FOR neural tangent kernel ( NTK ). initialization USED-FOR infinitely wide network. infinitesimal learning rate FEATURE-OF infinitely wide network. depth dependence FEATURE-OF generalization performance. feature learning COMPARE lazy learning. lazy learning COMPARE feature learning. NTK USED-FOR depth dependence. generalization performance EVALUATE-FOR NTK. Task are generalization, and machinelearning tasks. OtherScientificTerm are local and global labels, classification rules, local labels, and global labels. ",This paper studies the generalization properties of deep neural networks in the overparameterized regime. The authors show that the depth dependence between local and global labels can be explained by the locality of the relevant feature in the classification rule. They then propose a neural tangent kernel (NTK) which is an extension of the NTK to infinite-wide networks. The NTK is shown to improve generalization performance on a variety of image classification tasks.,"This paper studies the generalization performance of neural tangent kernels (NTK) in the overparameterized regime, where the depth of the network depends on the locality of the relevant feature of the classification rule. The authors show that NTK can generalize better than a finite network with infinitesimal learning rate. They also show that the depth dependence of NTK is related to the number of layers in the network, and show that deep NTK generalizes better than lazy learning."
1899,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"sample complexity EVALUATE-FOR representation. representation learning USED-FOR few - shot learning. i.i.d. task assumption USED-FOR Ω ( 1 T ) barrier. high - dimensional linear regression CONJUNCTION neural networks. neural networks CONJUNCTION high - dimensional linear regression. representation learning USED-FOR high - dimensional linear regression. representation learning USED-FOR neural networks. representation learning USED-FOR representation learning. Material is n2 ( n1 ) data. Generic is common representation. Task is sample size reduction. Metric is risk bound. OtherScientificTerm are linear representation class, and representation function class. ","This paper studies the sample complexity of representation learning for linear regression in the presence of a common linear representation class. The authors show that for any i.i.d. task assumption on the number of samples, they can obtain a sample complexity lower than $\tilde{O}(\sqrt{1 T})$ for any linear representation function class. They show that this is the case for high-dimensional linear regression with neural networks. They also show that representation learning can be used for few-shot learning.","This paper studies the problem of few-shot representation learning in the context of high-dimensional linear regression. The authors show that the sample complexity of the representation learning problem is bounded by the i.i.d. task assumption under which the task assumption is that the number of samples in the training set is small. They show that under this assumption, there is an upper bound on the sample size of the problem. The upper bound is based on the assumption that there is a common linear representation class, and that the representation function class can be represented as a function of the data distribution. They also show that this common representation can be used to reduce sample complexity.  "
1908,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,lowestlevel features FEATURE-OF network robustness. robustness EVALUATE-FOR networks. semantic features USED-FOR networks. black - box approach USED-FOR features. network USED-FOR features. features USED-FOR provably robust neighborhoods. provably robust neighborhoods CONJUNCTION adversarial examples. adversarial examples CONJUNCTION provably robust neighborhoods. robust features CONJUNCTION adversarial examples. adversarial examples CONJUNCTION robust features. weak features USED-FOR adversarial examples. robust features USED-FOR provably robust neighborhoods. PCA features EVALUATE-FOR approach. provably robust neighborhoods COMPARE neighborhoods. neighborhoods COMPARE provably robust neighborhoods. adversarial examples COMPARE state - of - the - art. state - of - the - art COMPARE adversarial examples. L2 distortion EVALUATE-FOR state - of - the - art. L2 distortion EVALUATE-FOR adversarial examples. attack USED-FOR ensemble adversarial training. Method is neural networks. Task is neural networks ’ robustness. OtherScientificTerm is perturbations. ,"This paper proposes a method to improve the robustness of deep neural networks against adversarial perturbations. The main idea is to use a black-box approach to learn a set of features that are provably robust to adversarial examples, and then use these features to train a network with adversarial training. The proposed method is evaluated on image classification tasks, and compared with the state-of-the-art.  ","This paper proposes a black-box approach to improve the robustness of neural networks. The key idea is to use a black box approach to learn features that are provably robust to adversarial examples and adversarial features. The black box method is based on the idea that the features of a network are learned by a network that is trained to be robust to perturbations. The main contribution of the paper is that it proposes a method to learn robust features that can be used to improve robustness. The method is evaluated on a variety of datasets, and it is shown that the proposed method outperforms the state of the art."
1917,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"transfer learning CONJUNCTION multi - task learning. multi - task learning CONJUNCTION transfer learning. Meta - learning CONJUNCTION transfer learning. transfer learning CONJUNCTION Meta - learning. uniform similarity USED-FOR approaches. method USED-FOR clusters of related tasks. sample complexity EVALUATE-FOR these. expectation - maximization algorithm USED-FOR method. policies USED-FOR agent. expectation step EVALUATE-FOR policies. method COMPARE multi - task learning algorithms. multi - task learning algorithms COMPARE method. complex bipedal walker tasks CONJUNCTION Atari games. Atari games CONJUNCTION complex bipedal walker tasks. discrete and continuous control tasks CONJUNCTION complex bipedal walker tasks. complex bipedal walker tasks CONJUNCTION discrete and continuous control tasks. complex bipedal walker tasks EVALUATE-FOR approach. discrete and continuous control tasks EVALUATE-FOR approach. sample complexity EVALUATE-FOR approaches. Method are reinforcement learning agents, and maximization step. Task is training. OtherScientificTerm is policy. ","This paper proposes a method for multi-task learning in reinforcement learning, where the goal is to learn a set of tasks that are similar to each other. The main idea is to use an expectation-maximization algorithm to find a policy that maximizes the sum of the expected rewards of all the tasks in the set. The method is evaluated on a variety of tasks from bipedal walker tasks to Atari games.  ","This paper proposes a method for multi-task learning with uniform similarity between tasks. The method is based on the expectation-maximization algorithm, where the goal is to maximize the performance of the agent on a set of tasks that are similar to each other. The main idea of the method is to learn a policy for each task that maximizes the performance on the set of related tasks, and then use this policy to train a new policy for the next task. The authors show that their method outperforms the state-of-the-art in terms of sample complexity on a wide range of tasks."
1926,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"self - supervised framework USED-FOR generalizable representations. generalizable representations USED-FOR non - stationary time series. local smoothness USED-FOR neighborhoods in time with stationary properties. local smoothness USED-FOR approach. debiased contrastive objective USED-FOR framework. framework USED-FOR time series representations. method COMPARE unsupervised representation learning approaches. unsupervised representation learning approaches COMPARE method. clustering and classification tasks EVALUATE-FOR multiple datasets. clustering and classification tasks EVALUATE-FOR method. Material are Time series, time series data, and labeling data. Method is Temporal Neighborhood Coding ( TNC ). OtherScientificTerm are encoding space, neighborhood, and distribution of non - neighboring signals. Task is medical field. ","This paper proposes Temporal Neighborhood Neighborhood Coding (TNC), a self-supervised representation learning method for non-stationary time-series data. The proposed method is based on the observation that non-neighborhoods in time with stationary properties can be represented by local smoothness in time. To achieve this, the authors propose to use a debiased contrastive objective to learn the representation of the time series. Experiments on clustering and classification tasks demonstrate the effectiveness of the proposed method on multiple datasets.","This paper proposes a self-supervised representation learning framework for non-stationary time series. The authors propose Temporal Neighborhood Coding (TNC) to learn representations for time series with stationary properties. TNC is based on a debiased contrastive objective, which aims to improve the generalizability of time series representations. The proposed method is evaluated on a variety of tasks, including clustering and classification tasks. "
1935,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"Conditional computation CONJUNCTION modular networks. modular networks CONJUNCTION Conditional computation. Conditional computation USED-FOR multitask learning and other problems. modular networks USED-FOR multitask learning and other problems. fully - differentiable approach USED-FOR modular networks. modules USED-FOR knowledge transfer. knowledge transfer USED-FOR tasks. soft weight sharing USED-FOR tasks. transfer learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION transfer learning. method USED-FOR self - organization of modules. transfer learning HYPONYM-OF tasks. multi - task learning CONJUNCTION transfer learning. transfer learning CONJUNCTION multi - task learning. domain adaptation HYPONYM-OF tasks. self - organization of modules USED-FOR multi - task learning. tasks EVALUATE-FOR method. it USED-FOR unsupervised multi - source domain adaptation. architectures USED-FOR image classification tasks. accuracy EVALUATE-FOR architectures. approach USED-FOR architectures. it USED-FOR adaptation. computation order USED-FOR modules. accuracy EVALUATE-FOR approach. order of pretrained modules USED-FOR adaptation. IMAGENET HYPONYM-OF image classification tasks. Task is problem solving. OtherScientificTerm are order of computation, and parameter increase. ","This paper proposes a self-organization method for multi-source domain adaptation, where the goal is to learn a set of modules that can be used for different tasks in the same domain. The proposed method is based on the idea of soft weight sharing, where each module is responsible for a specific task. The method is evaluated on image classification tasks and transfer learning tasks, where it achieves state-of-the-art performance. ",This paper proposes a new approach for multi-task learning in modular networks. The main idea is to use soft weight sharing between modules in order to improve transfer learning and domain adaptation in multi-source domain adaptation. The method is based on a fully differentiable approach that can be applied to any modular network architecture. The proposed method is evaluated on IMAGENET and CIFAR-10 datasets. 
1944,SP:cae669c631e11fe703bf6cb511404866b19f474a,"local optima FEATURE-OF objective function. hyperparameter USED-FOR data variance. hyperparameter USED-FOR local optima. variance parameter USED-FOR VAE. variance parameter USED-FOR smoothness. gradient FEATURE-OF smoothness. It USED-FOR regularization. variance parameter USED-FOR It. variance parameter USED-FOR regularization. Fréchet inception distance ( FID ) EVALUATE-FOR Generation models. MNIST and CelebA datasets USED-FOR Fréchet inception distance ( FID ) of images. objectives USED-FOR Generation models. Method are Variational autoencoders ( VAEs ), and AR - ELBO. OtherScientificTerm are posterior collapse, latent space, oversmoothness, and linear approximated objective function. Generic are parameter, and model. ","This paper proposes a new objective function for variational autoencoders (VAEs) based on the Fréchet inception distance (FID) of images. The main contribution of the paper is to introduce a variance parameter to the ELBO objective, which is used as a regularization term. The variance parameter is defined as the difference between the gradient of the objective and the true gradient. The authors show that the variance parameter can be used to improve the performance of the model in terms of FID.  ","This paper studies the problem of estimating the Fréchet inception distance (FID) of an autoencoder from a set of images. The authors show that the FID of a VAE trained on MNIST and CelebA can be approximated by a linear approximated objective function. They also show that this objective can be used to estimate the variance of the data, which is used to improve the performance of the model.   "
1953,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,non i.i.d. variational autoencoders USED-FOR global dependencies. non i.i.d. variational autoencoders USED-FOR deep generative model. semi - supervised alternatives USED-FOR global modeling. mixture model CONJUNCTION global Gaussian latent variable. global Gaussian latent variable CONJUNCTION mixture model. global modeling USED-FOR deep generative models. semi - supervised alternatives USED-FOR deep generative models. semi - supervised alternatives COMPARE approach. approach COMPARE semi - supervised alternatives. local or data - dependent space FEATURE-OF mixture model. mixture model PART-OF approach. global Gaussian latent variable PART-OF approach. induced latent global space USED-FOR interpretable disentangled representations. user - defined regularization FEATURE-OF evidence lower bound. domain alignment USED-FOR model. shared attributes CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION shared attributes. face images CONJUNCTION defined sequences of digits images. defined sequences of digits images CONJUNCTION face images. shared attributes FEATURE-OF face images. face images HYPONYM-OF non - trivial underlying structures. Method is beta - VAE. OtherScientificTerm is global space. ,"This paper proposes a method for semi-supervised generative models that uses a mixture model and a global Gaussian latent variable to model the global dependencies in the data. The proposed method is based on the idea that the mixture model can be viewed as a mixture of a Gaussian mixture model (GMM) and a beta-VAE. The authors show that the GMM can be used to learn a disentangled representation of the data in a global space, which can then be used as a regularizer to improve the performance of the generative model.   ","This paper proposes a new approach to semi-supervised generative models for non-i.i.d. (i.e., non-parametric) models. The proposed method is based on a mixture model and a global Gaussian latent variable. The mixture model is a mixture of a Gaussian mixture model, and the global latent variable is a latent space induced by a user-defined regularization. The global latent space is then used to generate interpretable disentangled representations. The authors show that the proposed method outperforms the state-of-the-art in terms of disentanglement performance."
1962,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"images CONJUNCTION videos. videos CONJUNCTION images. images HYPONYM-OF visual data. videos HYPONYM-OF visual data. visual data USED-FOR representation learning approaches. representations COMPARE visualonly representations. visualonly representations COMPARE representations. human interaction and attention cues USED-FOR approach. body part movements CONJUNCTION gaze. gaze CONJUNCTION body part movements. gaze FEATURE-OF human interactions. human interactions FEATURE-OF dataset. body part movements FEATURE-OF human interactions. gaze FEATURE-OF dataset. body part movements FEATURE-OF dataset. dynamics prediction ( physics ) CONJUNCTION walkable surface estimation ( affordance ). walkable surface estimation ( affordance ) CONJUNCTION dynamics prediction ( physics ). scene classification ( semantic ) CONJUNCTION action recognition ( temporal ). action recognition ( temporal ) CONJUNCTION scene classification ( semantic ). action recognition ( temporal ) CONJUNCTION depth estimation ( geometric ). depth estimation ( geometric ) CONJUNCTION action recognition ( temporal ). depth estimation ( geometric ) CONJUNCTION dynamics prediction ( physics ). dynamics prediction ( physics ) CONJUNCTION depth estimation ( geometric ). muscly - supervised ” representation USED-FOR interaction and attention cues. muscly - supervised ” representation USED-FOR target tasks. muscly - supervised ” representation COMPARE MoCo. MoCo COMPARE muscly - supervised ” representation. walkable surface estimation ( affordance ) HYPONYM-OF target tasks. scene classification ( semantic ) HYPONYM-OF target tasks. dynamics prediction ( physics ) HYPONYM-OF target tasks. depth estimation ( geometric ) HYPONYM-OF target tasks. action recognition ( temporal ) HYPONYM-OF target tasks. human ’s interactions USED-FOR representation learning. cues USED-FOR visual embedding. representation COMPARE self - supervised vision - only techniques. self - supervised vision - only techniques COMPARE representation. Task are representations of visual data, and computer vision. OtherScientificTerm is first person observations. ","This paper proposes to use human interaction and attention cues to improve the performance of self-supervised vision-only representation learning. The proposed method is based on the observation of human body movements and gaze in a video dataset, which is composed of body part movements, gaze, and body parts. The method is trained using a combination of two methods: (1) a ""musclely supervised"" representation learning method, which learns to use the human interaction as an attention mechanism, and (2) an ""action-aware"" representation, which uses the human gaze as a visual embedding.   The proposed model is evaluated on three tasks: scene classification (semantic), action recognition (temporal), and depth estimation (geometric), where it achieves state-of-the-art performance. ","This paper proposes a new representation learning approach for visual data, which is based on human interaction and attention cues. The authors propose a new dataset of human interactions and human gaze and body part movements, which they use to train their representation learning algorithm. They show that the proposed approach can learn better representations than the state-of-the-art self-supervised vision-only representation learning method MoCo. They also show that MoCo can learn a better representation than MoCo on a variety of tasks, including scene classification, action recognition, and depth estimation. "
1971,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"pretrained model COMPARE model. model COMPARE pretrained model. generalization EVALUATE-FOR model. generalization EVALUATE-FOR pretrained model. learning rate USED-FOR pretraining. neural network training CONJUNCTION pretraining. pretraining CONJUNCTION neural network training. OtherScientificTerm are Negative pretraining, negative pretraining effect, learning process, learning task - level, discretization of data distribution, model - level, negative pretraining effects, and negative pretraining. Method is neural networks. Generic is interventions. ",This paper studies the effect of negative pretraining on the generalization ability of neural networks trained with different learning rates. The authors show that the learning rate of a pretrained model can affect the performance of the trained model in terms of generalization performance. They also show that a large learning rate can have a negative effect on generalization. They show that this effect is independent of the task-level learning rate and can be explained by discretization of the data distribution. ,This paper studies the effect of negative pretraining on the generalization ability of neural networks. The authors show that the learning rate of a neural network trained at a certain learning rate can affect the generalizability of a model trained at the same learning rate. They also show that this generalization effect can be controlled by discretizing the data distribution. They show that discretization of data distribution can be used to reduce the generalisation ability of a trained model. 
1980,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"adversarially perturbed inputs FEATURE-OF classifier ’s robustness. bi - level optimization algorithm USED-FOR adversarially trained classifiers. bi - level optimization algorithm USED-FOR safe spots. ImageNet datasets EVALUATE-FOR adversarially trained classifiers. they USED-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR smoothed classifiers. empirical and certified robustness EVALUATE-FOR they. safe spot inducing model training scheme CONJUNCTION safe spot generation method. safe spot generation method CONJUNCTION safe spot inducing model training scheme. out - of - distribution detection algorithm USED-FOR near - distribution outliers. safe spot generation method USED-FOR out - of - distribution detection algorithm. Task is adversarial defense. Method are classifier, and classifiers. Material is natural images. OtherScientificTerm is adversarial attacks. ","This paper proposes a method to improve the robustness of adversarially trained classifiers against adversarial perturbations. The proposed method is based on a bi-level optimization algorithm to find the safe spots in the training set, which is then used to train a classifier with a safe spot inducing model. Experiments on ImageNet and CIFAR-10 show that the proposed method outperforms the state-of-the-art robustness methods.",This paper proposes a bi-level optimization algorithm to improve the robustness of adversarially trained classifiers against adversarial attacks. The authors propose a safe spot generation method and an out-of-distribution detection algorithm to detect near-distinct outliers. The proposed method is evaluated on ImageNet and CIFAR-10 datasets. 
1989,SP:1350ab543b6a5cf579827835fb27011751cc047f,"regularities CONJUNCTION order. order CONJUNCTION regularities. regularities FEATURE-OF temporal dimension. order FEATURE-OF temporal dimension. grid based convolutions USED-FOR video processing. point spatio - temporal ( PST ) convolution USED-FOR informative representations of point cloud sequences. PST convolution USED-FOR point cloud sequences. temporal convolution USED-FOR dynamics of the spatial regions. spatial convolution USED-FOR local structure. time dimension FEATURE-OF dynamics of the spatial regions. deep network USED-FOR features of point cloud sequences. PST convolution PART-OF deep network. PST convolution USED-FOR features of point cloud sequences. hierarchical manner USED-FOR point cloud sequences. PSTNet HYPONYM-OF deep network. PSTNet USED-FOR point cloud sequences. 3D action recognition CONJUNCTION 4D semantic segmentation datasets. 4D semantic segmentation datasets CONJUNCTION 3D action recognition. 4D semantic segmentation datasets EVALUATE-FOR PSTNet. Material is Point cloud sequences. OtherScientificTerm are spatial dimension, and 3D space. ",This paper proposes a point spatio-temporal convolution (PST) method for learning representations of point cloud sequences in 3D space. The proposed method is based on the observation that the dynamics of the spatial regions in a point cloud sequence are dependent on the time dimension. The authors propose to use a spatial convolution to capture the dynamics in the 3D spatial regions and a temporal convolution for the local structure of the regions. Experiments on 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of the proposed method.,"This paper proposes a novel point spatio-temporal convolutional (PSTP) convolution for point cloud sequences. The proposed method is based on the idea of point-spatially-temporally (SPT) convolutions, which can be used to capture the dynamics of the spatial regions of a point cloud sequence in a hierarchical manner. The main contribution of this work is the use of PST convolution to represent the spatial structure of the point cloud. The method is evaluated on 3D action recognition and 4D semantic segmentation datasets. "
1998,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"source TTS model USED-FOR personal voice. Custom voice HYPONYM-OF text to speech ( TTS ) service. text to speech ( TTS ) service PART-OF commercial speech platforms. Custom voice USED-FOR TTS adaptation. adaptive TTS system USED-FOR customization of new voices. AdaSpeech HYPONYM-OF adaptive TTS system. utterance and phoneme level FEATURE-OF acoustic information. acoustic encoder USED-FOR utterance - level vector. acoustic predictor USED-FOR phonemelevel vectors. one USED-FOR phoneme - level vectors. acoustic encoder CONJUNCTION one. one CONJUNCTION acoustic encoder. acoustic encoder USED-FOR phoneme - level vectors. utterance - level vector USED-FOR inference. adaptation parameters CONJUNCTION voice quality. voice quality CONJUNCTION adaptation parameters. speaker embedding USED-FOR adaptation. mel - spectrogram decoder PART-OF AdaSpeech. part CONJUNCTION speaker embedding. speaker embedding CONJUNCTION part. conditional layer normalization USED-FOR mel - spectrogram decoder. conditional layer normalization PART-OF AdaSpeech. acoustic conditions PART-OF LibriTTS. acoustic conditions FEATURE-OF VCTK and LJSpeech datasets. LibriTTS datasets USED-FOR source TTS model. VCTK and LJSpeech datasets USED-FOR it. adaptation data USED-FOR it. AdaSpeech COMPARE baseline methods. baseline methods COMPARE AdaSpeech. adaptation quality EVALUATE-FOR baseline methods. AdaSpeech USED-FOR custom voice. adaptation quality EVALUATE-FOR AdaSpeech. Method is adaptation model. Material are source speech data, and audio samples. OtherScientificTerm is memory usage. ","This paper proposes AdaSpeech, a text-to-speech (TTS) system that adapts a source TTS model to a new voice using utterance and phoneme level information. The method is based on a mel-spectrogram decoder, a speaker embedding, and a conditional layer normalization. Experiments on VCTK and LJSpeech datasets show the effectiveness of the proposed method.   ","This paper proposes AdaSpeech, an adaptive text-to-speech (TTS) system that adapts a source TTS model to a custom voice for TTS adaptation. The adaptive TTS system is based on a mel-spectrogram decoder and a speaker embedding, which is used to predict the utterance and phoneme level of the speaker. The acoustic encoder encodes utterance-level vectors and the phoneme-level vector is used for inference. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of TTS performance. "
2007,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,sparse networks COMPARE dense neural architectures. dense neural architectures COMPARE sparse networks. regularizers USED-FOR dense networks. activation functions CONJUNCTION regularizers. regularizers CONJUNCTION activation functions. optimizers CONJUNCTION activation functions. activation functions CONJUNCTION optimizers. activation functions USED-FOR dense networks. regularizers USED-FOR sparse networks. gradient flow USED-FOR sparse networks. training regime USED-FOR gradient flow. tailoring optimization USED-FOR sparse networks. OtherScientificTerm is initialization. Task is training sparse networks. ,"This paper studies the effect of regularization on the training of sparse neural networks. The authors show that the gradient flow of a sparse network trained with regularization is different from that of a dense network trained without regularization. In particular, they show that gradient flow is differentiable in the training regime, and that regularization can be used to improve the performance of sparse networks.   ","This paper studies the problem of training sparse neural networks in the training regime. The authors show that sparse networks can be trained in the same training regime as dense neural networks, but with different activation functions and regularizers. They also show that the gradient flow of sparse networks is different from dense networks in terms of the number of regularizers and optimizers. Finally, they show that training sparse networks in this training regime can lead to better performance than dense networks."
2016,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Graph Convolutional Neural Networks ( GCN ) HYPONYM-OF message passing algorithms. Label Propagation ( LPA ) CONJUNCTION Graph Convolutional Neural Networks ( GCN ). Graph Convolutional Neural Networks ( GCN ) CONJUNCTION Label Propagation ( LPA ). Label Propagation ( LPA ) HYPONYM-OF message passing algorithms. graphs USED-FOR message passing algorithms. GCN USED-FOR node feature information. LPA USED-FOR node label information. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. GCN CONJUNCTION LPA. LPA USED-FOR node classification. GCN CONJUNCTION LPA. LPA CONJUNCTION GCN. end - to - end model USED-FOR node classification. GCN USED-FOR node classification. GCN PART-OF end - to - end model. LPA PART-OF end - to - end model. LPA USED-FOR GCN. LPA USED-FOR regularization. model COMPARE feature - based attention models. feature - based attention models COMPARE model. attention weights USED-FOR model. node labels USED-FOR attention weights. real - world graphs EVALUATE-FOR model. model COMPARE GCN - based methods. GCN - based methods COMPARE model. real - world graphs EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR GCN - based methods. node classification accuracy EVALUATE-FOR model. OtherScientificTerm are edges of the graph, feature / label, feature / label influence, and edge weights. Task are feature / label smoothing, and classification. Method is unified model. ","This paper proposes to combine Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) to improve node classification on graphs. In particular, the authors propose to use LPA to propagate the label information between nodes and GCN to improve the classification performance. The proposed method is evaluated on several real-world graph classification tasks and achieves state-of-the-art performance.","This paper proposes a unified graph convolutional neural network (GCN) and label propagating (LPA) model for node classification. The authors propose a unified model that combines GCN and LPA to improve the performance of node classification on real-world graph classification tasks. The proposed model is trained with GCN, LPA, and label propagation. The model is evaluated on a variety of datasets, and it is shown to outperform GCN-based methods."
2025,SP:c5883e3a59e6575eff044251b38175a6ed024034,"VC - dimension CONJUNCTION Rademacher complexity ( R - Complexity ). Rademacher complexity ( R - Complexity ) CONJUNCTION VC - dimension. complexity EVALUATE-FOR classifier ’s function space. complexity FEATURE-OF generalization gap. generalization gap EVALUATE-FOR classifier ’s function space. classifier CONJUNCTION generator function spaces. generator function spaces CONJUNCTION classifier. R - Complexity EVALUATE-FOR generator function spaces. R - Complexity EVALUATE-FOR classifier. generalization performance EVALUATE-FOR generator space. invariances CONJUNCTION local smoothness. local smoothness CONJUNCTION invariances. generator space USED-FOR constraints. invariances HYPONYM-OF constraints. local smoothness HYPONYM-OF constraints. classifier CONJUNCTION generator. generator CONJUNCTION classifier. invariance co - complexity term CONJUNCTION dissociation co - complexity term. dissociation co - complexity term CONJUNCTION invariance co - complexity term. classifier USED-FOR generator. invariant transformations FEATURE-OF generator. invariance co - complexity term PART-OF It. invariant transformations FEATURE-OF classifier. dissociation co - complexity term PART-OF It. invariance co - complexity FEATURE-OF classifier. CNN architecture CONJUNCTION transformation - equivariant extensions. transformation - equivariant extensions CONJUNCTION CNN architecture. Co - complexity USED-FOR classifiers. Metric are generalization error bounds, generalization error, co - complexity, dissociation co - complexity, and training error. OtherScientificTerm are ground truth label generating function ( LGF ), LGF, ground truth labels, and function space. Generic is it. ","This paper studies the generalization gap between classifiers and generators in terms of the Rademacher complexity (R-Complexity) of the classifier's function space. The authors show that the R-complexity of the generator function space is a function of the VC-dimension and the complexity of the function space, and that the classifiers' generalization error bounds are upper bounded by R-computation. They show that R-Computation can be used as a generalization bound for classifiers trained with CNNs.   ","This paper studies the generalization gap between classifier and generator function spaces in terms of the Rademacher complexity (R-complexity) of the classifier's function space. The main contribution of the paper is a theoretical analysis of generalization error bounds for classifier-generator function spaces. The authors show that the R- complexity of the generator function space is a function of the invariances, local smoothness, and dissociation co-complexity. They also provide a generalization bound for CNNs with transformation-equivariant extensions."
2034,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"neural network quantization USED-FOR task. end - toend retraining USED-FOR neural network quantization. Post - training Quantization ( PTQ ) HYPONYM-OF neural network quantization. PTQ USED-FOR quantized models. PTQ COMPARE Quantization - Aware Training ( QAT ). Quantization - Aware Training ( QAT ) COMPARE PTQ. Quantization - Aware Training ( QAT ) USED-FOR quantized models. bitwidth FEATURE-OF PTQ. BRECQ HYPONYM-OF PTQ framework. neural networks USED-FOR BRECQ. crosslayer dependency CONJUNCTION generalization error. generalization error CONJUNCTION crosslayer dependency. crosslayer dependency EVALUATE-FOR BRECQ. generalization error EVALUATE-FOR BRECQ. mixed precision technique PART-OF framework. handcrafted and searched neural architectures USED-FOR image classification and object detection tasks. 4 - bit ResNet CONJUNCTION MobileNetV2. MobileNetV2 CONJUNCTION 4 - bit ResNet. PTQ COMPARE QAT. QAT COMPARE PTQ. MobileNetV2 COMPARE QAT. QAT COMPARE MobileNetV2. MobileNetV2 EVALUATE-FOR PTQ. 4 - bit ResNet EVALUATE-FOR PTQ. OtherScientificTerm are INT2, quantization, and inter - layer and intra - layer sensitivity. Metric is second - order error. ",This paper proposes a post-training quantization (PTQ) method to improve the performance of quantized neural networks. The main idea is to use a mixed precision technique to reduce the bitwidth of the quantized network. The proposed method BRECQ is evaluated on image classification and object detection tasks.   ,This paper proposes a novel post-training quantization (PTQ) framework BRECQ for neural network quantization. The main idea is to use a mixed precision technique to improve the generalization performance of the model. Experiments are conducted on image classification and object detection tasks and show that the proposed method outperforms QAT.
2043,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"large labeled datasets USED-FOR deep learning deployment. distributional shift CONJUNCTION labeling cost. labeling cost CONJUNCTION distributional shift. medicine HYPONYM-OF real - world applications. dataset properties USED-FOR calibration. dataset properties COMPARE architecture. architecture COMPARE dataset properties. common strategies USED-FOR class imbalance. dataset properties USED-FOR calibration. dataset curation USED-FOR calibration. Method are Neural networks, and neural networks. Metric are downstream prediction accuracy, accuracy, and calibration error. OtherScientificTerm are model uncertainty, label quality, label noise, small dataset sizes, and network expressivity. Generic is complementary approach. Task is dataset imbalance. ",This paper studies the effect of class imbalance on the calibration error of deep neural networks on downstream prediction accuracy. The authors show that class imbalance is a major cause of calibration errors in deep learning models. They show that the class imbalance can be controlled by the architecture of the network and the dataset size. They also show that dataset curation can help reduce the calibration errors.,This paper studies the problem of class imbalance in training neural networks. The authors propose a new approach to calibrate the accuracy of neural networks on large labeled datasets. The main idea of the paper is to use the dataset curation strategy to reduce the calibration error of the network. They show that this strategy can be applied to a wide range of datasets. They also show that it can be used to improve the accuracy on the downstream prediction accuracy.
2052,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"information exchange CONJUNCTION cooperation. cooperation CONJUNCTION information exchange. Effective communication USED-FOR information exchange. Effective communication USED-FOR cooperation. symbolic channels USED-FOR emergent communication. 3D environment FEATURE-OF joints. non - uniform distribution of intents CONJUNCTION commonknowledge energy cost. commonknowledge energy cost CONJUNCTION non - uniform distribution of intents. agents USED-FOR protocols. OtherScientificTerm are discrete cheap - talk channels, and latent feature. Method is emergent protocols. Generic is modality. Material is training curricula. ","This paper studies the problem of learning effective communication in a 3D environment. The authors propose to use symbolic communication as a way to train agents to communicate with each other in order to improve communication efficiency. They show that agents can learn to communicate via symbolic communication, where the communication is done via a set of discrete cheap-talk channels. They also show that this communication can be used to improve the communication efficiency of other agents. ","This paper studies the problem of communication between two agents in a 3D environment. The authors propose a new communication protocol for agents to communicate with each other in a non-uniform manner. The communication protocol is based on the notion of ""cheap-talk channels"", where each agent has access to a set of symbolic channels that can be used to communicate across agents. The agents are trained to communicate in a way that minimizes the cost of communication across agents in the environment.   "
2061,SP:5ba686e2eef369fa49b10ba3f41f102740836859,Generating high quality uncertainty estimates USED-FOR sequential regression. deep recurrent networks HYPONYM-OF sequential regression. real world non - stationary signals CONJUNCTION drift. drift CONJUNCTION real world non - stationary signals. method USED-FOR symmetric and asymmetric uncertainty estimates. method COMPARE baselines. baselines COMPARE method. drift and non drift scenarios EVALUATE-FOR baselines. sequential regression USED-FOR real - world applications. modeling toolbox USED-FOR sequential uncertainty quantification. Generic is approaches. OtherScientificTerm is stationarity. ,This paper proposes a method for estimating uncertainty estimates for sequential regression with deep recurrent networks. The method is based on the observation that the uncertainty estimates can be computed in both symmetric and asymmetric settings. The authors show that the proposed method is able to achieve better uncertainty estimates than the state-of-the-art methods in both the drift and non-divergence scenarios.  ,"This paper proposes a method for estimating high-quality uncertainty estimates for sequential regression. The main idea is to use deep recurrent networks (DNNs) to model the uncertainty quantification problem in sequential regression, which is a well-studied problem in the literature. The authors propose a method to estimate the uncertainty for symmetric and asymmetric uncertainty estimates. They show that their method can be used to estimate high quality uncertainty estimates in a variety of scenarios, including drift, non-stationarity, and real-world non-stagnant signals. "
2070,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,metric measure spaces USED-FOR machine learning problems. metric space HYPONYM-OF Comparing metric measure spaces. probability distribution FEATURE-OF metric space. Gromov - Wasserstein ( GW ) distance HYPONYM-OF metric measure spaces. probability distribution FEATURE-OF metric measure spaces. distance CONJUNCTION upper - bounding relaxation. upper - bounding relaxation CONJUNCTION distance. upper - bounding relaxation HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. distance HYPONYM-OF Unbalanced Gromov - Wasserstein formulations. They USED-FOR metric spaces. isometries FEATURE-OF positive measures. positive measures FEATURE-OF metric spaces. formulation USED-FOR positive and definite divergence. relaxation of the mass conservation constraint USED-FOR positive and definite divergence. quadratically - homogeneous divergence USED-FOR relaxation of the mass conservation constraint. quadratically - homogeneous divergence USED-FOR positive and definite divergence. entropic regularization approach USED-FOR large scale optimal transport problems. entropic regularization approach USED-FOR divergence. parallelizable and GPU - friendly iterative scheme USED-FOR non - convex optimization problem. distance FEATURE-OF mm - spaces. distance USED-FOR formulation. isometries FEATURE-OF distance. isometries FEATURE-OF mm - spaces. conic lifting USED-FOR distance. synthetic examples CONJUNCTION domain adaptation data. domain adaptation data CONJUNCTION synthetic examples. unbalanced divergence USED-FOR ML. domain adaptation data CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION domain adaptation data. synthetic examples CONJUNCTION Positive - Unlabeled learning task. Positive - Unlabeled learning task CONJUNCTION synthetic examples. Task is quadratic assignment problem. OtherScientificTerm is GW distance. ,This paper studies the unbalanced Gromov-Wasserstein (UGW) distance between positive and definite measures on metric measure spaces. The main contribution of the paper is to show that the UGW distance can be viewed as an upper-bounding relaxation of the distance between the positive measures and the definite measures. The authors also show that UGW can be used to solve the quadratic assignment problem in the positive-unlabeled learning task. ,"This paper studies the problem of unbalanced Gromov-wasserstein (GW) distance, which is a well-known problem in machine learning. The authors propose a novel formulation of GW distance, where the divergence between positive and definite measures is defined by a quadratically-homogeneous divergence. They show that this formulation can be used to solve the quadratic assignment problem, and that it is parallelizable and GPU-friendly. They also show that it can be applied to large scale optimal transport problems. "
2079,SP:47dcefd5515e772f29e03219c01713e2403643ce,"computational cost CONJUNCTION memory consumption. memory consumption CONJUNCTION computational cost. Network pruning USED-FOR memory consumption. compression ratios EVALUATE-FOR saliencybased pruning. sparse parameters FEATURE-OF well - trainable networks. pruning method USED-FOR pruned networks. all - alive pruning ( AAP ) HYPONYM-OF pruning method. trainable weights USED-FOR pruned networks. AAP USED-FOR saliency - based pruning methods. AAP USED-FOR model architectures. saliency - based pruning methods CONJUNCTION model architectures. model architectures CONJUNCTION saliency - based pruning methods. one - shot pruning CONJUNCTION dynamic pruning. dynamic pruning CONJUNCTION one - shot pruning. iterative pruning CONJUNCTION one - shot pruning. one - shot pruning CONJUNCTION iterative pruning. pruning methods USED-FOR AAP. dynamic pruning USED-FOR AAP. accuracy EVALUATE-FOR AAP. benchmark datasets EVALUATE-FOR AAP. dynamic pruning HYPONYM-OF pruning methods. iterative pruning HYPONYM-OF pruning methods. one - shot pruning HYPONYM-OF pruning methods. Material is low - resource devices. Metric is accuracy loss. Method is network pruning. OtherScientificTerm are model capacity, and dead connections. ",This paper proposes an all-alive pruning (AAP) method for network pruning. The main idea is to prune the weights of sparsely-trained networks with sparse parameters to reduce the number of dead connections in the network. The method is based on the observation that sparse networks are more efficient than dense networks in terms of compression ratio. The authors show that the pruning method is effective in reducing the computational cost and memory consumption of pruned networks.,"This paper proposes a new pruning method called all-alive pruning (AAP) to reduce the computational cost and memory consumption of saliency-based pruning methods. The main idea of AAP is to prune sparse parameters of well-trainable networks with sparse parameters, and then prune the weights of the pruned network. The method is evaluated on a variety of datasets, and it is shown to be competitive with existing pruning strategies."
2088,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"Generative Adversarial Net ( GAN ) USED-FOR latent space. approaches USED-FOR task. Generative Adversarial Net ( GAN ) USED-FOR latent - space transformations. latent space CONJUNCTION latent - space transformations. latent - space transformations CONJUNCTION latent space. Generative Adversarial Net ( GAN ) USED-FOR approaches. Generative Adversarial Net ( GAN ) USED-FOR task. global image identity CONJUNCTION diminished photo - realism. diminished photo - realism CONJUNCTION global image identity. attribute edits CONJUNCTION global image identity. global image identity CONJUNCTION attribute edits. content loss CONJUNCTION adversarial loss. adversarial loss CONJUNCTION content loss. maintenance of image identity CONJUNCTION photo - realism. photo - realism CONJUNCTION maintenance of image identity. attribute regression USED-FOR transformation functions. adversarial loss USED-FOR maintenance of image identity. content loss USED-FOR maintenance of image identity. quantitative evaluation strategies EVALUATE-FOR controllable editing. image identity CONJUNCTION realism. realism CONJUNCTION image identity. model USED-FOR singleand multipleattribute editing. model USED-FOR targeted image manipulation. natural and synthetic images EVALUATE-FOR model. Task are Controllable semantic image editing, and qualitative evaluation. OtherScientificTerm are image attributes, and multiple attribute transformations. ",This paper proposes a method for controllable semantic image editing. The method is based on a GAN-based model that is trained to predict latent transformations in the latent space. The proposed method is evaluated on both synthetic and natural images. The results show that the proposed method achieves state-of-the-art results in terms of global image identity and image realism. ,"This paper proposes a new model for controllable semantic image editing. The model is based on a GAN-based approach, where the model is trained to predict the attributes of a given image. The key idea is to use the latent space of the image as a latent space, and the model can be used to model the transformation functions of the input image. This is done by modeling the transformation function as an attribute regression problem, and then using a content loss and an adversarial loss. The proposed model is evaluated on both natural and synthetic images, and it is shown that the proposed model outperforms the state-of-the-art."
2097,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"mode dropping CONJUNCTION unstable training. unstable training CONJUNCTION mode dropping. Generative Adversarial Networks ( GAN ) USED-FOR synthesizing sequences of discrete elements. unstable training HYPONYM-OF synthesizing sequences of discrete elements. mode dropping HYPONYM-OF synthesizing sequences of discrete elements. binary classifier PART-OF discriminator. Feature Statistics Alignment ( FSA ) paradigm USED-FOR fine - grained signals. latent high - dimensional representation space FEATURE-OF fine - grained signals. FSA USED-FOR mean statistics. finite - dimensional feature space FEATURE-OF real data. approach USED-FOR discrete sequence generation. synthetic and real benchmark datasets EVALUATE-FOR approach. quantitative evaluation EVALUATE-FOR approach. Gumbel - Softmax based GAN framework USED-FOR sequence generation. feature alignment regularization USED-FOR Gumbel - Softmax based GAN framework. OtherScientificTerm are learning signals, and binary classification feedback. Task is adversarial training. ","This paper proposes a novel feature alignment regularization method for generating discrete sequences of discrete elements in GANs. The proposed method is based on the Feature Statistics Alignment (FSA) paradigm, which aims to align the mean statistics of the learned signals in the latent space of the discriminator with the true mean statistics in the feature space. The main contribution of this paper is to introduce a regularization term to the Gumbel-Softmax-based GAN framework to improve the performance of discrete sequence generation. Experiments are conducted on synthetic and real-world datasets.",This paper proposes a novel feature alignment-based GAN framework for generating discrete sequences of discrete elements. The main idea is to use the Feature Statistics Alignment (FSA) paradigm for generating fine-grained signals in the latent space of the Gumbel-Softmax GAN. The proposed method is evaluated on synthetic and real-world datasets.
2106,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"progressive rewards FEATURE-OF reinforcement learning tasks. tasks HYPONYM-OF reinforcement learning tasks. Spectral DQN USED-FOR reward. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE benchmarks. benchmarks COMPARE it. it COMPARE it. it COMPARE it. Spectral DQN USED-FOR reward progressivity. OtherScientificTerm are rewards, training loss, and extreme reward progressivity. Method are value - based deep reinforcement learning agents, and value - based methods. Generic are agent, and approach. Material is Atari games. ","This paper studies the problem of reward progressivity in reinforcement learning. The authors propose a new reward function called Spectral DQN, which is based on the idea that the reward function should be independent of the training loss. Theoretical analysis is provided to show that this reward function is independent of training loss, and that it can be used to improve the performance of reinforcement learning agents. Experimental results show that the proposed method outperforms baselines on a variety of Atari games.","This paper proposes a method to improve the performance of value-based reinforcement learning agents in Atari games. The main idea is to use Spectral DQN, which is an extension of Spectral Q-Q-QN to the reinforcement learning setting. The authors show that the proposed method outperforms the state-of-the-art in terms of reward progressivity. They also show that their method can be applied to a wide range of Atari games, and show that it can outperform the state of the art in some cases."
2115,SP:bff215c695b302ce31311f2dd105dace06307cfc,representations USED-FOR task. usable information FEATURE-OF representation. deep network USED-FOR representation. minimal sufficient representations USED-FOR task. learning - rate CONJUNCTION small batch size. small batch size CONJUNCTION learning - rate. learning - rate FEATURE-OF Stochastic Gradient Descent. Stochastic Gradient Descent USED-FOR implicit regularization. neuroscience literature USED-FOR perceptual decision - making tasks. Generic is it. Method is minimal sufficient representation. OtherScientificTerm is learning dynamics. Task is image classification tasks. ,"This paper studies the problem of learning minimal sufficient representations for image classification tasks. The authors show that the learning rate and the batch size of the training set are important factors in the learning dynamics of the representation learning process. They show that learning rate increases with the learning-rate and the small batch size, and that this is due to implicit regularization. They further show that this regularization can be achieved by using stochastic gradient descent.  ","This paper studies the problem of learning sufficient representations for perceptual decision-making tasks. In particular, the authors consider the task of image classification, where the goal is to learn a sufficient representation for a given task. The authors propose a new learning-rate-based learning strategy, called Stochastic Gradient Descent (SGD), which is motivated by the idea of implicit regularization. They show that SGD can be used to learn representations that are sufficient for the task at hand. They also show that the learning rate of SGD is sensitive to the small batch size and the learning dynamics of the task. "
2124,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"robust adversarial learning CONJUNCTION inverse reinforcement learning. inverse reinforcement learning CONJUNCTION robust adversarial learning. Min - max optimization USED-FOR machine learning problems. nonconvexstrongly - concave min - max optimization HYPONYM-OF machine learning problems. robust adversarial learning HYPONYM-OF machine learning problems. inverse reinforcement learning HYPONYM-OF machine learning problems. variance reduction algorithm SREDA USED-FOR problem. accuracy level FEATURE-OF optimal complexity dependence. initialization accuracy CONJUNCTION -dependent stepsize. -dependent stepsize CONJUNCTION initialization accuracy. -dependent stepsize USED-FOR per - iteration progress. convergence guarantee EVALUATE-FOR SREDA. initialization accuracy USED-FOR convergence guarantee. analytical framework USED-FOR SREDA. SREDA - Boost COMPARE SREDA. SREDA COMPARE SREDA - Boost. SREDA - Boost USED-FOR zeroth - order variance reduction algorithm. ZO - SREDA - Boost COMPARE complexity dependence on. complexity dependence on COMPARE ZO - SREDA - Boost. ZO - SREDA - Boost HYPONYM-OF zeroth - order variance reduction algorithm. variance reduction technique USED-FOR zeroth - order algorithm. zeroth - order algorithm USED-FOR min - max optimization problems. variance reduction technique USED-FOR min - max optimization problems. OtherScientificTerm are restrictive initialization requirement, and gradients. ","This paper studies the variance reduction problem in nonconvex-strongly-concave min-max optimization. The authors propose a variance reduction algorithm called SREDA, which is based on the initialization accuracy and the-dependent stepsize. They show that the convergence rate of the proposed algorithm is O(1/\sqrt{T}^2) with a certain complexity dependence on the number of iterations and the step size. They also show that a zeroth-order algorithm called ZO-SREDA-Boost can be used to reduce the variance of the original algorithm. ","This paper proposes a new variance reduction algorithm SREDA for nonconvex-strongly-concave min-max optimization problems. The main contribution of the paper is a theoretical analysis of the complexity dependence of the variance reduction problem. The authors show that the optimal complexity dependence depends on the initialization accuracy, the stepsize, and the per-iteration progress of the algorithm. They also provide a convergence analysis for the algorithm, showing that it converges to an optimal complexity of $O(\sqrt{O}(x,y)$, where $O(x)$ is the number of iterations, and $y$ is a function of $x$. "
2133,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"COCO EVALUATE-FOR state - of - the - art. it USED-FOR models. Example based object detection USED-FOR Detection of Novel Objects. Task are one - shot object detection, few - shot learning, and data annotation. OtherScientificTerm are generalization gap, Object categories, and object categories. Metric is generalization. Method are few - shot detection models, and metric learning approaches. ","This paper studies the problem of few-shot object detection in the presence of novel objects. The authors propose a novel object detection method, called COCO, which is based on the idea that novel objects can be represented as a set of examples. The proposed method is evaluated on the standard few shot object detection task, where it is shown to outperform the state-of-the-art in terms of accuracy and generalization.","This paper proposes a new metric learning approach for few-shot object detection. The main idea of the paper is to measure the generalization gap between two classes of objects in the dataset. To do so, the authors propose to use the example-based COCO dataset, which is a collection of examples of novel objects. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization.  "
2149,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"3D representations USED-FOR modeling clean mesh surfaces. occupancy fields CONJUNCTION signed distance functions ( SDF ). signed distance functions ( SDF ) CONJUNCTION occupancy fields. Implicit neural shape functions USED-FOR 3D representations. signed distance functions ( SDF ) HYPONYM-OF Implicit neural shape functions. occupancy fields HYPONYM-OF Implicit neural shape functions. representations USED-FOR single - view object reconstruction. Existing approaches USED-FOR single - view object reconstruction. representations USED-FOR Existing approaches. supervision signals USED-FOR Existing approaches. spatial gradient FEATURE-OF implicit field. supervision USED-FOR single - view reconstruction. feature map USED-FOR spatial gradient. real - world scenes USED-FOR single view implicit surface reconstructions. scanned dataset USED-FOR single view implicit surface reconstructions. ShapeNet CONJUNCTION ScannetV2. ScannetV2 CONJUNCTION ShapeNet. ShapeNet HYPONYM-OF datasets. ScannetV2 HYPONYM-OF datasets. model USED-FOR 3D implicit surface reconstruction. RGB image USED-FOR model. Task are real - world scenarios, and training on large - scale scenes. Material are ideal watertight geometric training data, large - scale scenes, internet, Internet, and pix3d dataset. OtherScientificTerm are training signal, spatial gradients, feature maps, and dense 3D supervision. Generic is this. Method are Pix3D, and DGS module. ",This paper proposes a method for 3D implicit surface reconstruction from a single view image. The method is based on the use of occupancy fields and signed distance functions (SDFs) to model the 3D surface. The proposed method is evaluated on ShapeNet and ScannetV2 and achieves state-of-the-art results on single-view object reconstruction.,"This paper proposes a new method for 3D implicit surface reconstruction from a single view image of a scene. The proposed method is based on the use of occupancy fields and signed distance functions (SDFs) to model 3D representations. The authors propose a new DGS module, which is trained on Pix3D and ScannetV2 datasets. They show that the proposed method can achieve state-of-the-art performance on the Pix3d dataset. "
2165,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"deep learning algorithms CONJUNCTION distributed training. distributed training CONJUNCTION deep learning algorithms. distributed training CONJUNCTION hardware design. hardware design CONJUNCTION distributed training. hardware design USED-FOR large models. GPT-3 CONJUNCTION Switch Transformer. Switch Transformer CONJUNCTION GPT-3. distributed training USED-FOR large models. Switch Transformer HYPONYM-OF extreme - scale models. GPT-3 HYPONYM-OF extreme - scale models. limited resources USED-FOR extreme - scale model training. memory footprint USED-FOR extreme - scale model training. Pseudo - to - Real USED-FOR high - memoryfootprint - required large models. training strategy USED-FOR high - memoryfootprint - required large models. Pseudo - to - Real HYPONYM-OF training strategy. Pseudo - to - Real CONJUNCTION large models. large models CONJUNCTION Pseudo - to - Real. architecture of sequential layers USED-FOR large models. GPUs USED-FOR state - of - the - art. Granular CPU offloading USED-FOR CPU memory. technique USED-FOR CPU memory. Granular CPU offloading HYPONYM-OF technique. Metric are model convergence, and carbon footprint. Method is large model. OtherScientificTerm is GPU utilities. Task is greener AI. ","This paper proposes a method to reduce the memory footprint of large deep learning models. The main idea is to use pseudo-to-real (P2R) training strategy, which is based on the idea of Pseudo-To-Real (PTR). The authors show that PTR can be used to train large models with limited resources on a limited number of GPUs. The authors also show that the proposed method can reduce the CPU memory footprint and reduce the computational cost.","This paper proposes a new training strategy Pseudo-to-real (P2R) for large models with high memory footprint. The main idea of the paper is to reduce the memory footprint of large models by reducing the number of layers in the training process. The authors show that this strategy can be applied to a variety of architectures, including GPT-3 and Switch Transformer. They also show that it can be combined with a number of other techniques to reduce memory footprint, such as granular CPU offloading.   "
2181,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"Energy - based models ( EBMs ) HYPONYM-OF generative models. maximum likelihood estimation USED-FOR generative models. Gibbs distribution USED-FOR energy. Fenchel duality USED-FOR variational principles. variational principles USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR maximum likelihood EBMs. shallow overparametrized neural network energies USED-FOR variational principles. dual formulation USED-FOR training algorithm. particles CONJUNCTION neurons. neurons CONJUNCTION particles. particles PART-OF sample space. parameter space FEATURE-OF neurons. dual formulation USED-FOR active regime. maximum likelihood CONJUNCTION score matching training. score matching training CONJUNCTION maximum likelihood. intermediate parameter setups USED-FOR dual algorithm. Generic are approach, and algorithm. ",This paper proposes a new method for training energy-based generative models (EBMs) based on the Fenchel duality. The main idea is to use a shallow overparametrized neural network to model the energy of the particles and neurons in the model. The authors show that the proposed method is computationally efficient and can be used to train EBMs with score matching training.   ,"This paper proposes a new method for training energy-based generative models (EBMs) with maximum likelihood estimation. The main idea is to use Fenchel duality in the training of EBMs, where the energy is represented as a Gibbs distribution over a set of particles and neurons. The authors propose a dual formulation of the training algorithm, which is based on a shallow overparametrized neural network energies. They show that the proposed method can be used to train EBMs with score matching training and maximum likelihood."
2197,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"differentially private ERM USED-FOR convex functions. lower bounds FEATURE-OF convex functions. lower bounds FEATURE-OF differentially private ERM. logarithmic terms USED-FOR lower bounds. biased mean property USED-FOR fingerprinting codes. ` 2 loss function COMPARE linear functions. linear functions COMPARE ` 2 loss function. ` 2 loss function USED-FOR pure - DP. Method are approximate - DP, and DP - ERM. Material is constrained case. OtherScientificTerm are unconstrained case, auxiliary dimension, ` 2 loss, and one - way marginals. Generic is it. ","This paper studies the differentially private ERM for convex functions with logarithmic terms. The main contribution is a lower bound on the lower bound of the DP-ERM for the convex function. The lower bound is based on the fact that the loss function of the original DP has a `2 loss function, which is the same as the one in the unconstrained case. The authors show that this loss function is equivalent to the one-way marginals of the true DP.   ","This paper studies differentially private ERM for convex functions with logarithmic terms. The authors prove lower bounds for the convex case, where the lower bounds are lower than the lower bound for the unconstrained case. The lower bound is based on the biased mean property, which is a result of the fact that the log-likelihood of the lower-bounded lower bounds is lower than that of the original lower bounds. The upper bounds are based on a differentiable lower bound on the auxiliary dimension of the function.   "
2213,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"gradient flow USED-FOR machine learning applications. Wasserstein metric FEATURE-OF space of probability densities. approach USED-FOR Wasserstein gradient flow. finite difference USED-FOR approach. finite difference USED-FOR Wasserstein gradient flow. scalable proximal gradient type algorithm USED-FOR Wasserstein gradient flow. variational formulation of the objective function USED-FOR JKO proximal map. variational formulation of the objective function USED-FOR method. primal - dual optimization USED-FOR JKO proximal map. heat equation CONJUNCTION porous medium equation. porous medium equation CONJUNCTION heat equation. framework USED-FOR Wasserstein gradient flows. porous medium equation HYPONYM-OF Wasserstein gradient flows. heat equation HYPONYM-OF Wasserstein gradient flows. OtherScientificTerm are grid, and inner and outer loops. Task is primal - dual problem. Generic is algorithm. ","This paper proposes a proximal gradient type algorithm for Wasserstein gradient flow in the space of probability densities. The main idea is to use a variational formulation of the objective function to approximate the JKO proximal map of the primal-dual problem, which is then used to solve the primal dual problem. The authors show that the proposed method is computationally efficient and scalable.   ","This paper studies the problem of Wasserstein gradient flow in the space of probability densities. The main contribution of the paper is a new proximal gradient type algorithm that can be used to solve the primal-dual problem. The authors provide a variational formulation of the objective function of the JKO proximal map, which is then used to formulate the method. The method is shown to be scalable and efficient. "
2229,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,ML algorithm CONJUNCTION hyper - parameter configuration. hyper - parameter configuration CONJUNCTION ML algorithm. ML algorithm USED-FOR AutoML problem. approach USED-FOR meta - features. Optimal Transport procedure USED-FOR meta - features. MetaBu meta - features USED-FOR topology. hyper - parameter configurations USED-FOR AutoML. MetaBu meta - features USED-FOR AutoML systems. AutoSkLearn CONJUNCTION Probabilistic Matrix Factorization. Probabilistic Matrix Factorization CONJUNCTION AutoSkLearn. OpenML CC-18 benchmark EVALUATE-FOR AutoML systems. OpenML CC-18 benchmark EVALUATE-FOR MetaBu meta - features. AutoSkLearn HYPONYM-OF AutoML systems. Probabilistic Matrix Factorization HYPONYM-OF AutoML systems. topology USED-FOR intrinsic dimensionality. intrinsic dimensionality FEATURE-OF OpenML benchmark. MetaBu meta - features USED-FOR intrinsic dimensionality. MetaBu meta - features USED-FOR topology. Method is MetaBu. OtherScientificTerm is manually designed meta - features. ,"This paper proposes a meta-feature-based approach to learn the meta-features for AutoML problems. The proposed method is based on meta-transport, which is an Optimal Transport procedure. The meta-trajectories are generated by a set of hyper-parameter configurations. The authors show that the proposed method achieves state-of-the-art performance on the OpenML CC-18 benchmark.","This paper proposes a meta-feature-based approach to learn meta-features for AutoML problems. The authors propose an Optimal Transport procedure (MT) to learn Meta-features, which can be used to learn hyper-parameter configurations for auto-learning. They show that the Meta-Feature-based method can improve the performance of AutoML systems on the OpenML CC-18 benchmark.  "
2245,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"Federated learning ( FL ) USED-FOR distributed learning framework. robustness FEATURE-OF models. Split - Mix FL strategy USED-FOR heterogeneous participants. base sub - networks USED-FOR customization. communication CONJUNCTION storage. storage CONJUNCTION communication. storage CONJUNCTION inference. inference CONJUNCTION storage. split - mix strategy USED-FOR customization. method COMPARE heterogeneous - architecture FL methods. heterogeneous - architecture FL methods COMPARE method. in - situ customization EVALUATE-FOR heterogeneous - architecture FL methods. in - situ customization EVALUATE-FOR method. Task is FL scenarios. OtherScientificTerm are hardware and inference dynamics, and inference requirements. Method are FL approaches, and FL. ",This paper proposes a split-mix FL strategy for heterogeneous participants in a federated learning setting. The proposed method splits the network into a set of sub-networks and allows each sub-network to be customized in a different way. The method is evaluated on a variety of distributed learning tasks and achieves state-of-the-art performance.,"This paper proposes a new method for heterogeneous federated learning (FL) with heterogeneous participants. The authors propose a split-mix FL strategy that allows heterogeneous sub-networks to be customized in a federated setting. The main idea is that the base sub-network can be split into two parts, one for each heterogeneous participant, and the other for the heterogenous participants. They show that this strategy can be applied to a variety of heterogeneous heterogeneous-architecture FL methods. They also show that their method can be used for in-situ customization. "
2261,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"extragradient - type algorithm USED-FOR nonconvex - nonconcave minimax problems. local solution USED-FOR general minimax problems. first order methods USED-FOR variational inequalities. problem class USED-FOR non - trivial structures. algorithms USED-FOR limit cycles. algorithm USED-FOR constrained and regularized problems. adaptive stepsize USED-FOR stepsizes. adaptive stepsize PART-OF algorithm. limit cycles FEATURE-OF operator. it USED-FOR training of generative adversarial networks. variant USED-FOR training of generative adversarial networks. stochastic oracles USED-FOR variant. monotone setting FEATURE-OF it. OtherScientificTerm are weak Minty variational inequality ( MVI ), and weak MVI. Generic is scheme. Method are generative adversarial networks, and stochastic algorithm. ",This paper studies the nonconvex-nonconcave minimax problems with non-trivial structures. The authors propose an extension of the MVI-based method to the non-concavity minimax problem. The main contribution of this paper is to show that the weak MVI can be expressed as a local solution of a general minimax.   The authors show that this is the case for both constrained and regularized problems. They also provide an adaptive stepsize for the stepsizes in the limit cycles of the proposed algorithm. ,"This paper studies the problem of nonconvex-nonconcave minimax minimax problems. The main contribution of the paper is a novel extension of the well-known Gaussian variational inequality (MVI) algorithm for the problem class of non-trivial minimax problem. The authors show that the weak MVI can be approximated by a stochastic gradient-based algorithm, which can be used to solve general minimax generalization problems. They also provide a variant of the algorithm for training generative adversarial networks (GANs), where they show that it can be applied to the monotone setting."
2277,SP:af22742091277b726f67e7155b412dd35f29e804,"neural contextual bandits HYPONYM-OF contextual bandits. learning algorithm USED-FOR raw feature vector. upper confidence bound ( UCB ) approach USED-FOR last linear layer ( shallow exploration ). upper confidence bound ( UCB ) approach USED-FOR learning algorithm. finitetime regret EVALUATE-FOR algorithm. neural contextual bandit algorithms COMPARE approach. approach COMPARE neural contextual bandit algorithms. deep neural network USED-FOR it. OtherScientificTerm are reward generating function, and learning time horizon. ","This paper studies the problem of contextual bandits, where the goal is to learn a feature vector from a set of samples from the reward generating function. The authors propose a learning algorithm that uses a deep neural network to learn the feature vector and then uses an upper confidence bound (UCB) approach to estimate the last linear layer (shallow exploration) of the last layer of the neural network. They show that their algorithm achieves a regret of $O(1/\sqrt{T})$ with a time horizon of $T$, where $T$ is the number of samples in the set.  ","This paper proposes a novel learning algorithm for contextual bandits. The main idea is to use a deep neural network to learn the last linear layer (shallow exploration) of the feature vector, and then use a UCB-based upper confidence bound (UCB) approach to estimate the regret of the final linear layer. The UCB approach is based on the fact that the regret is bounded by a linear combination of the reward generating function and the learning time horizon. The paper shows that the UCB bound can be used to estimate regret for the last layer of the deep learning algorithm. The authors also provide a theoretical analysis of the regret."
2293,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"Training action space selection USED-FOR reinforcement learning ( RL ). Shapley - inspired methodology USED-FOR action space categorization. Monte Carlo simulation USED-FOR unnecessary explorations. Monte Carlo simulation PART-OF methodology. cloud infrastructure resource tuning case study EVALUATE-FOR methodology. It USED-FOR search space. it USED-FOR RL model design. data - driven methodology USED-FOR reinforcement learning algorithms. OtherScientificTerm are complex state - action relationships, and exponential - time shapley computations. ","This paper proposes a Shapley-inspired methodology for action space categorization in reinforcement learning. The proposed method is based on Shapley's method for state-action categorization, which is an extension of Shapley’s method to the action space. The main contribution of this paper is to use Shapley methods to reduce the number of unnecessary explorations in the search space.   The main contributions of the paper are as follows:  1. The authors propose a new Shapley method for action selection in RL.  2. They show that the proposed method can be used to improve the performance of existing RL algorithms. ",This paper proposes a Shapley-inspired methodology for action space categorization in reinforcement learning. The authors propose an exponential-time shapley computations approach to the problem of training action space selection in RL. The approach is based on Monte Carlo simulation to reduce the number of unnecessary explorations in the search space. Experiments are conducted on a cloud infrastructure resource tuning case study to demonstrate the effectiveness of the approach.
2309,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"approach USED-FOR probably approximately correct ( PAC ) prediction sets. importance weights USED-FOR algorithm. confidence intervals FEATURE-OF importance weights. DomainNet CONJUNCTION ImageNet. ImageNet CONJUNCTION DomainNet. approach USED-FOR covariate shifts. DomainNet USED-FOR approach. ImageNet USED-FOR approach. PAC constraint FEATURE-OF approaches. PAC constraint FEATURE-OF algorithm. Method are machine learning, predictive model, and uncertainty quantification algorithms. OtherScientificTerm are data distribution, and covariate shift. Generic is shifts. Metric is average normalized size. ",This paper proposes a method to estimate the uncertainty quantification error for PAC prediction sets with covariate shifts in the data distribution. The method is based on using importance weights to estimate uncertainty quantified confidence intervals. The authors show that the importance weights are independent of the covariate shift and can be used to estimate confidence intervals of the prediction sets. The paper also shows that the average normalized size of the predicted sets can be estimated using the importance weight.,This paper proposes a new approach to estimate the probably approximately correct (PAC) prediction sets. The key idea is to use importance weights as confidence intervals for the confidence intervals of the importance weights. The importance weights are computed by using the average normalized size of the data distribution. The authors show that their approach is able to estimate PAC sets with high confidence intervals. 
2325,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"generalization error EVALUATE-FOR iterative SSL algorithms. information - theoretic principles USED-FOR generalization error. binary Gaussian mixture model HYPONYM-OF model. benchmark datasets EVALUATE-FOR model. MNIST and CIFAR datasets HYPONYM-OF benchmark datasets. OtherScientificTerm are model parameters, class conditional variances, and pseudo - labelling iterations. ",This paper studies the generalization error of SSL algorithms for binary Gaussian mixture models. The authors show that the error depends on the number of pseudo-labelling iterations and the class conditional variances of the model parameters. They show that this error is upper bounded by an information-theoretic principle. They then propose a new algorithm that uses pseudo-labeling iterations to reduce the variances. The proposed algorithm is shown to outperform existing SSL algorithms on the MNIST and CIFAR datasets.,This paper studies the generalization error of iterative SSL algorithms for binary Gaussian mixture models. The authors consider the problem of class conditional variances and pseudo-labelling iterations of SSL algorithms. They provide a theoretical analysis of the generalisation error of SSL algorithm for the binary Gaussians model. They show that the error is bounded by the number of pseudo-labeling iterations. They also provide an empirical analysis of their results.   
2341,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"model - free reinforcement learning algorithms USED-FOR policy. intentional action sequences USED-FOR high value regions. intentional action sequences USED-FOR plans. it USED-FOR plans. multi - step plans USED-FOR temporally coordinated exploration. GPM USED-FOR temporally coordinated exploration. multi - step plans USED-FOR GPM. GPM USED-FOR it. crude initial plan generator USED-FOR GPM. benchmark environments EVALUATE-FOR baseline methods. OtherScientificTerm are inefficient exploration, single step nature, single step level, consistent movement, and multi - step plan. Method are Generative Planning method ( GPM ), generative planning, and actionrepeat strategy. ","This paper proposes a generative planning method for model-free reinforcement learning (MRL) that learns to generate multi-step plans for exploration. The proposed method is based on the idea that the goal of MRL is to find high-value regions in the environment and then explore these regions using a set of intentional action sequences. The main contribution of this paper is that it proposes to use a simple initial plan generator to generate the initial plan for exploration, which is then used as the initial action. The method is evaluated on a number of benchmark environments and achieves state-of-the-art performance. ","This paper proposes a generative planning method (GPM) for model-free reinforcement learning (MRL) where the goal is to find high-value regions in the environment. The main idea of GPM is to generate multi-step plans that can be used to coordinate the exploration of high value regions. The authors propose a simple initial plan generator for GPM, and show that GPM can be applied to a variety of environments. They also show that the proposed method is able to achieve state-of-the-art performance on several benchmark environments. "
2357,SP:ce6a93847209a0926ed0be5190378a3f61db1935,"deep linear and nonlinear matrix factorizations USED-FOR machine learning. deep learning CONJUNCTION tensor decomposition. tensor decomposition CONJUNCTION deep learning. matrices CONJUNCTION tensors. tensors CONJUNCTION matrices. factorization methods USED-FOR matrix and tensor completion problems. methods COMPARE matrix and tensor factorization methods. matrix and tensor factorization methods COMPARE methods. generalization error bounds EVALUATE-FOR matrix and tensor factorization methods. generalization error bounds EVALUATE-FOR methods. synthetic data and real datasets EVALUATE-FOR methods. recovery accuracy EVALUATE-FOR baselines. methods COMPARE baselines. baselines COMPARE methods. synthetic data and real datasets EVALUATE-FOR baselines. recovery accuracy EVALUATE-FOR methods. Method are deep nonlinear matrix factorization methods, and multi - mode deep matrix and tensor factorizations. ",This paper studies the problem of matrix and tensor decomposition in deep neural networks. The authors propose to use multi-mode deep matrix/tensor factorization methods to solve matrix decomposition problems. The main contribution of the paper is to provide a generalization error bound for matrix and tensor decomposition methods. The proposed method is shown to outperform the state-of-the-art methods on both synthetic and real-world datasets.,"This paper studies the problem of matrix and tensor factorization for deep linear and nonlinear matrix factorization in machine learning. The authors propose a new generalization error bounds for matrix-and-tensor factorization methods. The generalization bounds are based on the assumption that the matrix and the tensor can be represented as a set of matrices and tensors, and that the factorization problem can be solved in a multi-mode manner. The main contributions of the paper are:  1. A generalization bound for the generalization of matrix- and tens-factorization methods is derived. 2. A theoretical analysis of the error bounds is provided. 3. Experimental results on synthetic data and real datasets demonstrate the effectiveness of the proposed methods. "
2373,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"interpretation technique USED-FOR structured output models. features USED-FOR structured model. function USED-FOR interpreter. energy - based training process USED-FOR interpreter function. structural information PART-OF model. simulated and real data sets EVALUATE-FOR method. OtherScientificTerm are output variables, computational path of output variables, feature, output variable, and input space. Method are structured models, and structured output model. ",This paper proposes a novel interpretation technique for structured output models. The main idea is to use the energy-based training process to learn an interpretable function for each input variable in a structured model. The proposed method is evaluated on simulated and real-world data sets. ,"This paper proposes an interpretative approach for structured output models. The main idea is to use energy-based training to train an interpreter for structured models, which can be used to learn the structure of a structured model. The authors show that the interpreter can be trained using an energy-free training process, and that it can learn the structural information of the model. They also show that their method can be applied to both simulated and real data sets."
2389,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"approaches USED-FOR distributional DRL. reward function USED-FOR agent behavior. variance reduction measures USED-FOR on - policy learning. asymptotically consistent estimate of the policy gradient USED-FOR CDF - based objectives. sampling USED-FOR asymptotically consistent estimate of the policy gradient. sampling USED-FOR CDF - based objectives. algorithm USED-FOR agents. risk profiles FEATURE-OF penalty - based formulations. accumulation of positive rewards CONJUNCTION frequency of incurred penalties. frequency of incurred penalties CONJUNCTION accumulation of positive rewards. OpenAI Safety Gym environments FEATURE-OF penalty - based formulations. penalty - based formulations USED-FOR agents. risk profiles FEATURE-OF agents. risk profile COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) COMPARE risk profile. risk profile COMPARE PPO. PPO COMPARE risk profile. Proximal Policy Optimization ( PPO ) COMPARE PPO. PPO COMPARE Proximal Policy Optimization ( PPO ). Proximal Policy Optimization ( PPO ) CONJUNCTION positive reward. positive reward CONJUNCTION Proximal Policy Optimization ( PPO ). positive reward COMPARE PPO. PPO COMPARE positive reward. Lagrangians USED-FOR cost levels. positive reward EVALUATE-FOR risk profile. Lagrangians USED-FOR PPO. Method is deep reinforcement learning ( DRL ) agents. Generic are policy, approach, and technique. Task is human decision - making. OtherScientificTerm are distributional context, projected distribution of returns, distribution of full - episode outcomes, cumulative distribution function ( CDF ), relative quality, continuous and discrete action spaces, and policy gradient. ",This paper proposes a method to improve the performance of on-policy learning by using the cumulative distribution function (CDF) to estimate the policy gradient. The proposed method is based on sampling from the distribution of full episode outcomes and using the CDF as the objective function. The authors show that the proposed method outperforms the state-of-the-art methods in the OpenAI Safety Gym environments. ,"This paper proposes a new method for on-policy learning in the context of distributional reinforcement learning (DRL). The main idea is to use the cumulative distribution function (CDF) to estimate the policy gradient of the agent, and then use a sampling strategy to learn the CDF-based objectives. The authors show that the proposed method outperforms Proximal Policy Optimization (PPO) in terms of the risk profile of the agents. They also show that PPO outperforms PPO when the reward function is positive.  "
2405,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"Interactive Neural Process ( INP ) HYPONYM-OF Bayesian active learning framework. Bayesian active learning framework USED-FOR deep learning surrogate model. Interactive Neural Process ( INP ) USED-FOR simulation. deep sequence model CONJUNCTION active learning. active learning CONJUNCTION deep sequence model. neural process CONJUNCTION deep sequence model. deep sequence model CONJUNCTION neural process. deep sequence model USED-FOR framework. neural process USED-FOR framework. active learning USED-FOR framework. spatiotemporal neural process model USED-FOR simulator dynamics. model USED-FOR latent process. latent process USED-FOR intrinsic uncertainty. latent information gain USED-FOR acquisition function. Bayesian active learning algorithms USED-FOR simulator. approach COMPARE random sampling. random sampling COMPARE approach. sample complexity EVALUATE-FOR random sampling. high dimension FEATURE-OF random sampling. sample complexity EVALUATE-FOR approach. framework USED-FOR rapid simulation and scenario exploration. framework USED-FOR complex infectious disease simulator. Task is Stochastic simulations. OtherScientificTerm are fine - grained resolution, and theoretical analysis. ",This paper proposes a Bayesian active learning framework for simulating infectious disease. The proposed method is based on a deep learning surrogate model and a spatiotemporal neural process model to model the simulator dynamics. The authors show that the proposed method achieves better sample complexity compared to random sampling in the high-dimensional setting.  ,This paper proposes a Bayesian active learning framework for simulating a complex infectious disease simulator. The main idea is to use a neural process model to model the dynamics of the simulator dynamics. The model is modeled as a spatiotemporal neural process with a deep sequence model and an active learning surrogate model. The authors show that the proposed method can achieve better sample complexity than random sampling with high dimension. They also show that their method can be applied to a more complex simulator.
2421,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"Differentially Private ( DP ) learning USED-FOR large deep learning models of text. hyperparameters USED-FOR DP optimization. hyperparameters CONJUNCTION fine - tuning objectives. fine - tuning objectives CONJUNCTION hyperparameters. pretraining procedure USED-FOR fine - tuning objectives. private NLP models COMPARE private training approaches. private training approaches COMPARE private NLP models. private training approaches CONJUNCTION nonprivate baselines. nonprivate baselines CONJUNCTION private training approaches. private NLP models COMPARE nonprivate baselines. nonprivate baselines COMPARE private NLP models. moderately - sized corpora USED-FOR DP optimization. DP optimization USED-FOR pretrained models. moderately - sized corpora USED-FOR pretrained models. linear layer PART-OF model. per - example gradients USED-FOR linear layer. memory saving technique USED-FOR clipping. clipping PART-OF DP - SGD. large Transformers USED-FOR DP - SGD. memory saving technique USED-FOR DP - SGD. privately training Transformers COMPARE non - private training. non - private training COMPARE privately training Transformers. technique USED-FOR privately training Transformers. memory cost EVALUATE-FOR non - private training. memory cost EVALUATE-FOR privately training Transformers. modest run - time overhead EVALUATE-FOR non - private training. DP optimization USED-FOR high - dimensional models. pretrained models USED-FOR private learning. Task is NLP tasks. Metric is computational overhead. Method is large pretrained models. OtherScientificTerm are noise, and dimension - dependent performance degradation. ",This paper proposes a method for privately training large deep learning models of text. The main idea is to train a linear layer of the model with per-example gradients and then use DP-SGD to clip the gradients of the linear layer during training. The method is evaluated on a variety of NLP tasks and compared with non-differentially private methods.   ,"This paper proposes a new method for training large models of text for differentially private (DP) learning. The method is based on the idea of clipping, which is a memory-saving technique to reduce the computational overhead of training large Transformers. The authors show that this method can be applied to a variety of datasets, and that it can be used to improve the performance of DP-SGD. "
2437,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. length CONJUNCTION size. size CONJUNCTION length. size CONJUNCTION strength. strength CONJUNCTION size. strength HYPONYM-OF joint attributes. length HYPONYM-OF joint attributes. size HYPONYM-OF joint attributes. design procedure PART-OF decision - making process. agent PART-OF decision - making process. design procedure USED-FOR agent. skeletal structure CONJUNCTION joint attributes. joint attributes CONJUNCTION skeletal structure. transform actions USED-FOR joint attributes. transform actions USED-FOR skeletal structure. control actions USED-FOR design. message passing USED-FOR joint - specific actions. policy gradient methods USED-FOR approach. approach USED-FOR joint optimization of agent design and control. joint optimization of agent design and control CONJUNCTION experience sharing. experience sharing CONJUNCTION joint optimization of agent design and control. experience sharing USED-FOR approach. approach COMPARE prior methods. prior methods COMPARE approach. Transform2Act COMPARE prior methods. prior methods COMPARE Transform2Act. Transform2Act HYPONYM-OF approach. convergence speed EVALUATE-FOR approach. convergence speed EVALUATE-FOR prior methods. giraffes CONJUNCTION squids. squids CONJUNCTION giraffes. squids CONJUNCTION spiders. spiders CONJUNCTION squids. OtherScientificTerm are agent ’s functionality, and design space. Generic is function. Method are optimal controller, conditional policy, and graph - based policy. Metric is sample efficiency. ","This paper proposes a method for joint optimization of agent design and control in reinforcement learning. The proposed method, called Transform2Act, is based on a combination of transform actions and message-passing. The main idea is to use transform actions to learn joint attributes such as strength, size, and joint attributes, which are then used to train a conditional policy. The authors show that the proposed method achieves better sample efficiency and convergence speed compared to previous methods.","This paper proposes a new method for joint optimization of agent design and control. The method is based on a graph-based policy gradient method. The key idea is to learn a set of joint attributes (skeletal structure, strength, size, and joint attributes) that can be used to optimize the agent's design. The proposed method is evaluated on synthetic data and real-world data.  "
2453,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"view synthesis CONJUNCTION 3D object representation and rendering. 3D object representation and rendering CONJUNCTION view synthesis. Implicit neural representations USED-FOR tasks. multi - layer perceptrons ( MLPs ) USED-FOR Implicit neural representations. 3D object representation and rendering HYPONYM-OF tasks. view synthesis HYPONYM-OF tasks. MLP USED-FOR image, video, or 3D object. MLP USED-FOR training. coordinate - based MLPs USED-FOR implicit neural representations. inference CONJUNCTION training. training CONJUNCTION inference. CoordX USED-FOR initial layers. coordinate - based MLPs USED-FOR inference. coordinate - based MLPs USED-FOR training. layers USED-FOR intermediate features. accuracy EVALUATE-FOR baseline MLP. training CONJUNCTION inference. inference CONJUNCTION training. architecture USED-FOR implicit neural representation tasks. speedup EVALUATE-FOR baseline model. Generic are representations, approach, and them. Method is split MLP architecture. OtherScientificTerm is memory overheads. ","This paper proposes a split MLP architecture for training and inference of implicit neural representations for image, video, and 3D rendering tasks. The authors propose to use coordinate-based MLPs to train the initial layers and split the intermediate features in the intermediate layers to reduce memory overheads. Experiments show that the proposed method outperforms the state-of-the-art baselines in terms of accuracy and inference time. ","This paper proposes a split MLP architecture for the implicit neural representation task. The main idea is to use a coordinate-based MLP for the initial layers, and then use a multi-layer perceptron for the intermediate layers. The authors show that the proposed architecture can achieve a significant speedup over the baseline model in terms of accuracy and inference speedup. "
2469,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"method USED-FOR object - centric representations of visual scenes. appearance CONJUNCTION 3D pose. 3D pose CONJUNCTION appearance. structured representation USED-FOR shape. shape CONJUNCTION appearance. appearance CONJUNCTION shape. structured representation USED-FOR appearance. localized neural radiance field USED-FOR 2D views of the scene. object representation USED-FOR localized neural radiance field. differentiable rendering process USED-FOR localized neural radiance field. differentiable rendering process USED-FOR 2D views of the scene. reconstruction loss USED-FOR model. inferred scenes USED-FOR representations. 3D object representations USED-FOR visual reasoning task. CATER dataset USED-FOR 3D object representations. Method are INFERNO, and neural 3D rendering. OtherScientificTerm are annotations, rendered scenes, and supervision. ",This paper proposes a method for learning object-centric representations of visual scenes. The proposed method is based on a differentiable rendering process that generates 2D views of the scene and uses a localized neural radiance field to reconstruct the 2D images. The method is evaluated on the CATER dataset and achieves state-of-the-art performance on the visual reasoning task.,"This paper proposes a method for learning object-centric representations of visual scenes. The proposed method is based on a differentiable rendering process, where the object representation is represented as a localized neural radiance field, and the 2D views of the scene are rendered as a reconstruction loss. The method is evaluated on the CATER dataset, where it is shown to perform well on the visual reasoning task. "
2485,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"feature attribution framework USED-FOR GNN ’s prediction. features USED-FOR GNN ’s prediction. edges HYPONYM-OF features. subgraph USED-FOR model prediction. distribution shift USED-FOR out - ofdistribution problem. surrogate variable USED-FOR subgraphs. generative model USED-FOR unbiased estimation of subgraph importance. explanation fidelity EVALUATE-FOR DSE. Method are graph neural networks ( GNNs ), GNN, in - depth causal analysis, Deconfounded Subgraph Evaluation ( DSE ), and front - door adjustment. OtherScientificTerm are influential subgraph, subgraph importance, OOD effect, explanatory subgraph, and data distribution. Task is evaluation. ",This paper proposes a method to evaluate the importance of subgraphs in GNNs in the context of OOD effects. The main idea is to use a surrogate variable to measure the influence of a subgraph on the prediction of a GNN. The surrogate variable is then used to estimate the OOD effect of a given subgraph in terms of its influence on the GNN prediction. This surrogate variable can then be used as a proxy for the true importance of the subgraph to the prediction. The paper shows that the proposed method can be used to evaluate subgraph importance in the presence of distribution shift.,"This paper proposes Deconfounded Subgraph Evaluation (DSE), a feature attribution framework for graph neural networks (GNNs) that can be applied to the out-of-distribution (OOD) problem. The key idea is to use a surrogate variable (e.g., a subgraph) to measure the importance of a given subgraph, and then use the surrogate variable to estimate the OOD effect of the subgraphs. The authors show that DSE can be used for unbiased estimation of subgraph importance, and that it can improve explanation fidelity."
2501,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"pretrained models COMPARE active learners. active learners COMPARE pretrained models. spurious correlations CONJUNCTION latent minority groups. latent minority groups CONJUNCTION spurious correlations. latent minority groups CONJUNCTION domain shifts. domain shifts CONJUNCTION latent minority groups. pretrained models COMPARE random sampling. random sampling COMPARE pretrained models. spurious correlations FEATURE-OF image and text datasets. data USED-FOR pretrained models. uncertainty sampling USED-FOR data. accuracy EVALUATE-FOR pretrained models. uncertainty sampling USED-FOR pretrained models. minority classes CONJUNCTION informative examples. informative examples CONJUNCTION minority classes. spurious feature CONJUNCTION class label. class label CONJUNCTION spurious feature. active learning COMPARE unpretrained models. unpretrained models COMPARE active learning. Active learning USED-FOR task ambiguity. Pretraining USED-FOR models. Pretraining USED-FOR task ambiguity. Pretraining USED-FOR active learners. active learners USED-FOR task ambiguity. disambiguating examples USED-FOR active learners. Method are machine learning systems, and pretraining process. Material is few - shot settings. OtherScientificTerm is shape. ",This paper studies the effect of uncertainty sampling in few-shot image and text classification tasks. The authors show that pretrained models trained with uncertainty sampling perform better than random sampling. They also show that active learning can improve the performance of a pretrained model in the presence of task ambiguity.  ,"This paper studies the problem of active learning in the few-shot setting, where the task is to disambiguate a set of samples from a dataset. The authors show that active learning can outperform the state-of-the-art in terms of accuracy when the sample size is small. They also show that the active learner is able to learn better than the unpretrained learner when the dataset size is limited. The paper also shows that active learners can learn better when the data is more diverse."
2517,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,GRAPHIX HYPONYM-OF pre - trained graph edit model. automatically detecting and fixing bugs CONJUNCTION code quality issues. code quality issues CONJUNCTION automatically detecting and fixing bugs. pre - trained graph edit model USED-FOR automatically detecting and fixing bugs. code quality issues FEATURE-OF Java programs. pre - trained graph edit model USED-FOR code quality issues. sequence - tosequence models COMPARE GRAPHIX. GRAPHIX COMPARE sequence - tosequence models. abstract syntax structure of code USED-FOR GRAPHIX. multi - head graph encoder USED-FOR GRAPHIX. model USED-FOR graph edit actions. graph edit actions USED-FOR automated program repair. model USED-FOR automated program repair. autoregressive tree decoder PART-OF model. pre - training strategy USED-FOR GRAPHIX. pre - training strategy USED-FOR model. implicit knowledge of program structures USED-FOR model. deleted sub - tree reconstruction HYPONYM-OF pre - training strategy. unlabeled source code USED-FOR implicit knowledge of program structures. bug fixing task USED-FOR downstream learning. pre - training objective CONJUNCTION bug fixing task. bug fixing task CONJUNCTION pre - training objective. pre - training objective USED-FOR downstream learning. abstract and concrete code USED-FOR GRAPHIX. Wild Java benchmark EVALUATE-FOR GRAPHIX. CodeBERT CONJUNCTION BART. BART CONJUNCTION CodeBERT. GRAPHIX COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE GRAPHIX. GRAPHIX COMPARE baselines. baselines COMPARE GRAPHIX. baselines COMPARE pre - trained Transformer models. pre - trained Transformer models COMPARE baselines. GRAPHIX COMPARE BART. BART COMPARE GRAPHIX. GRAPHIX COMPARE CodeBERT. CodeBERT COMPARE GRAPHIX. BART HYPONYM-OF baselines. CodeBERT HYPONYM-OF baselines. GRAPHIX USED-FOR structural and semantic code patterns. Material is abstract and concrete source code. ,This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The proposed model is based on the abstract syntax structure of code and uses a multi-head graph encoder to model the graph edit actions for automated program repair. The model is trained using deleted sub-tree reconstruction and unlabeled source code to improve the implicit knowledge of program structures. The paper shows that the proposed model achieves state-of-the-art performance on the Wild Java benchmark.,"This paper proposes a pre-trained graph edit model, GRAPHIX, for automatically detecting and fixing code quality issues in Java programs. The model is based on a multi-head graph encoder, which encodes the abstract syntax structure of code, and a tree decoder that reconstructs the sub-trees of the original code. The proposed model is evaluated on the Wild Java benchmark, where it is shown to perform better than the state-of-the-art on both abstract and concrete code. "
2533,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,Federated Adversarial Training ( FAT ) USED-FOR data privacy and governance issues. adversarial attack FEATURE-OF model robustness. inner - maximization optimization of Adversarial Training USED-FOR data heterogeneity. lower bound USED-FOR Federated Learning. convergence FEATURE-OF FAT. convergence EVALUATE-FOR α - weighted mechanism. adversarial learning methods CONJUNCTION federated optimization methods. federated optimization methods CONJUNCTION adversarial learning methods. α - WFAT COMPARE FAT. FAT COMPARE α - WFAT. benchmark datasets EVALUATE-FOR α - WFAT. benchmark datasets EVALUATE-FOR FAT. adversarial learning methods USED-FOR FAT. adversarial learning methods USED-FOR α - WFAT. Method is inner - maximization of Adversarial Training. ,"This paper proposes a federated adversarial training (FAT) framework to improve the robustness of models against adversarial attacks in federated learning. The proposed method is based on the idea of inner-maximization of Adversarial Training (OMT), which aims to mitigate the issue of data heterogeneity in the federated setting. Theoretical analysis is provided to show the convergence of the proposed method. Empirical results are provided to support the theoretical results. ","This paper studies the problem of federated adversarial training (FAT) in the context of data privacy and governance. The authors propose a new lower bound on the convergence of FAT under data heterogeneity. The lower bound is based on the inner-maximization optimization of Adversarial training, which is motivated by the fact that data heterogeneity is a problem in the federated learning setting.  The authors show that the lower bound can be derived for the case where the data is heterogeneous. They also provide a convergence analysis for the inner maximization of FAT."
2549,SP:ff3c787512035e2af20778d53586752852196be9,"LML USED-FOR supervised learning. models USED-FOR semi - supervised continual learning exceptions. Mako HYPONYM-OF wrapper tool. wrapper tool PART-OF supervised LML frameworks. data programming USED-FOR wrapper tool. data programming USED-FOR Mako. Mako USED-FOR continual semi - supervised learning. labeled data USED-FOR continual semi - supervised learning. tool COMPARE fully labeled data. fully labeled data COMPARE tool. per - task accuracy CONJUNCTION resistance. resistance CONJUNCTION per - task accuracy. resistance EVALUATE-FOR catastrophic forgetting. resistance EVALUATE-FOR tool. per - task accuracy EVALUATE-FOR tool. CIFAR-10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. accuracy CONJUNCTION catastrophic forgetting prevention. catastrophic forgetting prevention CONJUNCTION accuracy. Mako USED-FOR unlabeled data. Mako USED-FOR LML tools. LML tools USED-FOR supervised learning. fully labeled data USED-FOR supervised learning. accuracy EVALUATE-FOR supervised learning. image classification data sets USED-FOR LML task sequences. CIFAR100 HYPONYM-OF image classification data sets. CIFAR-10 HYPONYM-OF image classification data sets. MNIST HYPONYM-OF image classification data sets. ORDisCo CONJUNCTION DistillMatch. DistillMatch CONJUNCTION ORDisCo. CNNL CONJUNCTION ORDisCo. ORDisCo CONJUNCTION CNNL. Mako COMPARE them. them COMPARE Mako. baseline semi - supervised LML tools COMPARE Mako. Mako COMPARE baseline semi - supervised LML tools. DistillMatch HYPONYM-OF baseline semi - supervised LML tools. CNNL HYPONYM-OF baseline semi - supervised LML tools. accuracy EVALUATE-FOR Mako. ORDisCo HYPONYM-OF baseline semi - supervised LML tools. Task are Lifelong machine learning ( LML ), and human learning process. Method is LML methods. OtherScientificTerm is knowledge base overhead. ",This paper proposes a new method for continual semi-supervised learning (CSL) in the context of continual machine learning (LML). The proposed method is based on data programming and is able to handle unlabeled data in the continual learning framework. The method is evaluated on CIFAR-10/100 and MNIST datasets and achieves state-of-the-art performance.  ,"This paper proposes Mako, a wrapper tool for continual continual learning (LML) framework for semi-supervised continual learning. Mako is a data programming-based wrapper for supervised LML framework, where the data programming is done using data programming and data programming. The paper proposes a new method for continual LML, which is called Mako. The main idea is to use data programming as a wrapper for continual supervised learning framework, and then use the data program to learn the task sequences of the task. The method is evaluated on CIFAR-10, CIFar-100, MNIST, and MNIST-CIFAR10 datasets, and compared with a number of baselines."
2565,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"adversarial examples USED-FOR Evading adversarial example detection defenses. attack techniques USED-FOR adversarial examples. Selective Projected Gradient Descent CONJUNCTION Orthogonal Projected Gradient Descent. Orthogonal Projected Gradient Descent CONJUNCTION Selective Projected Gradient Descent. Selective Projected Gradient Descent CONJUNCTION attack techniques. attack techniques CONJUNCTION Selective Projected Gradient Descent. technique USED-FOR detection defenses. accuracy EVALUATE-FOR technique. Generic are model, and attacks. OtherScientificTerm is gradients. Method is gradient - based attacks. Metric is detection rate. ","This paper studies the problem of detecting adversarial examples in the presence of gradient-based adversarial attacks. The authors propose to use gradient descent to generate adversarial samples in order to improve the detection accuracy. The proposed method is based on the idea that the gradients of the adversarial example can be computed in an orthogonal fashion, which allows the gradient to be computed at different locations in the input space. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed method. ",This paper studies the problem of detecting adversarial examples in the presence of gradient-based attacks. The authors propose a novel method to detect the gradients of the adversarial example. They show that the detection rate of the proposed method can be as low as 0.5% and as high as 1.5%. They also provide a theoretical analysis of the effectiveness of their method.
2581,SP:5eef907024017849303477eed92f317438c87a69,data valuation CONJUNCTION model valuation. model valuation CONJUNCTION data valuation. feature interpretation CONJUNCTION data valuation. data valuation CONJUNCTION feature interpretation. model valuation USED-FOR ensembles. Valuation problems PART-OF machine learning applications. model valuation HYPONYM-OF Valuation problems. data valuation HYPONYM-OF Valuation problems. feature interpretation HYPONYM-OF Valuation problems. Shapley value CONJUNCTION Banzhaf value. Banzhaf value CONJUNCTION Shapley value. game - theoretic criteria USED-FOR problems. Banzhaf value HYPONYM-OF game - theoretic criteria. Shapley value HYPONYM-OF game - theoretic criteria. energy - based treatment USED-FOR cooperative games. maximum entropy principle USED-FOR energy - based treatment. one - step fixed point iteration USED-FOR ELBO objective. mean - field variational inference USED-FOR classical game - theoretic valuation criteria. mean - field variational inference USED-FOR energy - based model. one - step fixed point iteration USED-FOR classical game - theoretic valuation criteria. uniform initializations USED-FOR variational valuations. game - theoretic axioms FEATURE-OF variational valuations. decoupling error CONJUNCTION valuation. valuation CONJUNCTION decoupling error. valuation FEATURE-OF synthetic and real - world valuation problems. valuation EVALUATE-FOR Variational Index. decoupling error EVALUATE-FOR Variational Index. synthetic and real - world valuation problems EVALUATE-FOR Variational Index. Generic is criteria. Method is fixed point iteration. ,"This paper studies the problem of model valuation for ensembles, where the goal is to estimate the value of each component of a set of data points. The authors propose to use the Banzhaf and Shapley values as game-theoretic criteria for solving this problem. The main contribution of the paper is to derive a mean-field variational inference algorithm for this problem, which is based on the maximum entropy principle. Theoretical analysis is provided to show the convergence of the proposed algorithm to the true value of the true data point. Experiments are conducted on both synthetic and real-world data sets.","This paper studies the problem of variational valuations in cooperative games. The authors propose a mean-field variational inference approach for the problem, which is based on the maximum entropy principle. The main contribution of the paper is a one-step fixed point iteration for the ELBO objective. The paper also proposes a uniform initialization method for the variational valuation problem. Experiments on synthetic and real-world data show that the proposed approach outperforms the baselines."
2597,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"out - of - sample prediction error FEATURE-OF Epistemic uncertainty. approach USED-FOR epistemic uncertainty. intrinsic unpredictability HYPONYM-OF estimate of aleatoric uncertainty. active learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION active learning. estimator USED-FOR interactive learning environments. estimator USED-FOR epistemic uncertainty. active learning USED-FOR interactive learning environments. reinforcement learning USED-FOR interactive learning environments. sequential model optimization CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION sequential model optimization. methods USED-FOR uncertainty estimation. methods USED-FOR tasks. uncertainty estimation USED-FOR tasks. sequential model optimization HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. uncertainty estimates USED-FOR estimating uncertainty. DEUP USED-FOR probabilistic classification of images. uncertainty estimates USED-FOR probabilistic classification of images. synergistic drug combinations FEATURE-OF estimating uncertainty. DEUP USED-FOR uncertainty estimates. OtherScientificTerm are model variance, and generalization error. Method is Direct Epistemic Uncertainty Prediction ( DEUP ). ","This paper proposes a method for estimating epistemic uncertainty, i.e., uncertainty in the model variance that is independent of the true model variance. The method is based on the idea of intrinsic uncertainty, which is an estimate of aleatoric uncertainty that captures the uncertainty that arises from the uncertainty of the model.    The main contribution of this paper is to propose a method to estimate the intrinsic uncertainty in model variance, which can be used for estimating uncertainty in uncertainty estimation. The proposed method is called Direct Epistemic Uncertainty Prediction (DEUP) and it is shown that DEUP is able to estimate uncertainty in both model variance and generalization error. Experiments are conducted on image classification and reinforcement learning tasks, where DEUP outperforms existing methods.","This paper proposes a new method for estimating the uncertainty of the epistemic uncertainty of a model. The method is based on the notion of aleatoric uncertainty, i.e., the uncertainty that is caused by the model variance, which is defined as the difference between the model's prediction error and the uncertainty in the data. The authors propose a method called Direct Epistemic Uncertainty Prediction (DEUP) to estimate this uncertainty. They show that DEUP can be used to estimate uncertainty in a variety of settings, such as active learning, reinforcement learning, and sequential model optimization. They also show that the method can be applied to the problem of estimating uncertainty in an interactive learning environment. "
2613,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,Product quantization ( PQ ) USED-FOR approximate nearest neighbor ( ANN ) search systems. Product quantization ( PQ ) CONJUNCTION space rotation. space rotation CONJUNCTION Product quantization ( PQ ). disk storage USED-FOR embeddings. rotation learning methods USED-FOR quantization distortion. quantization distortion FEATURE-OF fixed embeddings. block Givens coordinate descent algorithms USED-FOR rotation matrix. geometric intuitions USED-FOR block Givens coordinate descent algorithms. Lie group theory USED-FOR geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF geometric intuitions. special orthogonal group SO(n ) HYPONYM-OF Lie group theory. SVD method COMPARE Givens algorithms. Givens algorithms COMPARE SVD method. runtime EVALUATE-FOR GPUs. GPUs USED-FOR Givens algorithms. runtime EVALUATE-FOR Givens algorithms. vanilla product quantization USED-FOR end - to - end training scenario. vanilla product quantization USED-FOR They. end - to - end training scenario EVALUATE-FOR They. Task is inner product computation. OtherScientificTerm is convex objectives. ,"This paper studies the problem of approximate nearest neighbor (ANN) search with product quantization (PQ) and space rotation. The authors propose to use block Givens coordinate descent (GCD) to compute the rotation matrix of the embedding matrix, which is then used to learn the quantization distortion of the fixed embeddings. The proposed method is shown to be computationally efficient in terms of computation time and memory. ",This paper proposes a novel block Givens coordinate descent algorithm for approximate nearest neighbor (ANN) search with product quantization (PQ) and space rotation. The main contribution of the paper is to introduce a special orthogonal group SO(n) which is a special case of the Lie group theory of orthogonality. The authors show that this group can be used to improve the performance of the Given coordinate descent (Givens) algorithm. They also show that the SVD method can outperform the vanilla product quantisation method in the end-to-end training scenario. 
2629,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"sensory information USED-FOR conceptual abstractions. Structure Mapping Theory USED-FOR human analogical reasoning. ( higher - order ) relations USED-FOR analogical mappings. two - stage neural framework USED-FOR visual analogies. Raven ’s Progressive Matrices HYPONYM-OF abstract visual reasoning test of fluid intelligence. Neural Structure Mapping ( NSM ) HYPONYM-OF two - stage neural framework. Raven ’s Progressive Matrices USED-FOR visual analogies. multi - task visual relationship encoder CONJUNCTION neural module net - based analogy inference engine. neural module net - based analogy inference engine CONJUNCTION multi - task visual relationship encoder. neural module net - based analogy inference engine USED-FOR framework. raw visual input USED-FOR multi - task visual relationship encoder. multi - task visual relationship encoder USED-FOR framework. structure USED-FOR analogical reasoning. NSM approach USED-FOR relational structure. Generic is them. Task are human intelligence, and Abstract reasoning. OtherScientificTerm are analogies, and novel domains. Material is known domains. Method is machine learning ( ML ) models. ",This paper proposes a two-stage neural framework for learning visual analogies. The first stage is a multi-task visual relationship encoder and a neural module net-based analogy inference engine. The second stage is an extension of Raven’s Progressive Matrices (RPM) to learn higher-order relations between (higher-order) relations. The experiments show that the proposed method outperforms baselines on the abstract visual reasoning test. ,"This paper proposes a two-stage neural framework for visual analogical reasoning. The main idea is to use Raven’s Progressive Matrices (RPM) to map visual analogies to higher-order relations (e.g., relationships between objects) and then use a neural module net-based analogy inference engine to infer the relationship between objects. The proposed method is tested on a variety of visual tasks, and it is shown that the proposed method outperforms the state-of-the-art methods. "
2645,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,contrastive self - supervised method USED-FOR nucleotide genome representation learning. Self - GenomeNet HYPONYM-OF contrastive self - supervised method. Self - GenomeNet HYPONYM-OF self - supervised framework. method USED-FOR latent space. reverse - complement of genomic sequences USED-FOR method. reverse - complement of genomic sequences USED-FOR latent space. framework USED-FOR semantic representations. features USED-FOR context network. context network USED-FOR framework. encoder network USED-FOR features. context network USED-FOR semantic representations. unsupervised contrastive loss USED-FOR network. method COMPARE deep learning methods. deep learning methods COMPARE method. self - supervised and semi - supervised settings USED-FOR deep learning methods. self - supervised and semi - supervised settings USED-FOR method. representations USED-FOR datasets. Material is nucleotide genomic data. OtherScientificTerm is domain - specific characteristics. ,-based self-supervised method for nucleotide genome representation learning is proposed. The proposed method is based on the reverse-computation of genomic sequences and uses a context network to learn the semantic representations. The encoder network is trained with an unsupervised contrastive loss and the context network is used to extract the features from the sequence. The experimental results show that the proposed method achieves state-of-the-art performance on the proposed datasets. ,This paper proposes a contrastive self-supervised method for nucleotide genome representation learning. The proposed method is based on the Self-GenomeNet framework. The authors propose to use the reverse-complement of genomic sequences to learn the latent space of the genome representation. They also propose an unsupervised contrastive loss to improve the performance of the network. 
2661,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"low - level vision theory USED-FOR steerable filters. steerable feed - forward learning - based approach USED-FOR point clouds. spherical decision surfaces PART-OF steerable feed - forward learning - based approach. 3D steerability constraint USED-FOR hypersphere neurons. conformal embedding of Euclidean space USED-FOR 3D steerability constraint. conformal embedding of Euclidean space USED-FOR hypersphere neurons. synthetic point set CONJUNCTION real - world 3D skeleton data. real - world 3D skeleton data CONJUNCTION synthetic point set. spherical filter banks USED-FOR invariant class predictions. online optimization USED-FOR invariant class predictions. invariant class predictions USED-FOR known point sets. online optimization USED-FOR spherical filter banks. unknown orientations FEATURE-OF invariant class predictions. unknown orientations FEATURE-OF known point sets. Method is steerable convolutional neural networks. OtherScientificTerm are 3D geometry, rotational equivariance, and model parameters. Task is learning representations of point sets. ",This paper proposes a steerable convolutional neural network for learning representations of 3D point clouds with spherical decision surfaces. The proposed method is based on a conformal embedding of Euclidean space with a 3D steerability constraint. Theoretical results show that the proposed method can learn representations of point sets that are invariant to rotational equivariance and can be used to learn representations with unknown orientations.,This paper proposes a steerable feed-forward learning-based approach to steerable convolutional neural networks for learning representations of point clouds. The proposed method is based on a conformal embedding of Euclidean space and a 3D steerability constraint. Theoretical results on synthetic and real-world 3D skeleton data demonstrate the effectiveness of the proposed method. 
2677,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,Pre - trained language models ( PLMs ) USED-FOR continual learning. continual learning USED-FOR natural language problems. continual learning methods CONJUNCTION PLMs. PLMs CONJUNCTION continual learning methods. PLMs CONJUNCTION CL approaches. CL approaches CONJUNCTION PLMs. benchmarks EVALUATE-FOR continual learning. benchmarks EVALUATE-FOR CL approaches. PLMs CONJUNCTION CL methods. CL methods CONJUNCTION PLMs. representativeness probing analyses USED-FOR PLMs ’ performance characteristics. representativeness probing analyses USED-FOR layer - wise and task - wise manner. layer - wise and task - wise manner FEATURE-OF PLMs ’ performance characteristics. Task is Continual learning ( CL ). Generic is model. OtherScientificTerm is forgetting. Method is continual learning techniques. ,This paper proposes a new continual learning framework for continual learning in natural language tasks. The proposed method is based on a pre-trained language model (PLM) that is trained with continual learning. The authors propose to use representativeness probing analyses to evaluate the performance of PLMs on continual learning tasks. Experiments show that the proposed method outperforms the state-of-the-art continual learning methods on various continual learning benchmarks.,This paper studies the problem of continual learning (CL) in the context of pre-trained language models (PLMs) and continual learning methods (CL methods). The authors propose a representativeness probing analyses of PLMs’ performance characteristics in layer-wise and task-wise manner. They show that PLMs perform better than CL methods on a variety of benchmarks. They also provide a theoretical analysis of CL methods and PLMs. 
2693,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"model poisoning attacks COMPARE centralized learning. centralized learning COMPARE model poisoning attacks. Federated learning COMPARE centralized learning. centralized learning COMPARE Federated learning. Federated learning USED-FOR model poisoning attacks. multi - party, distributed learning HYPONYM-OF Federated learning. model updates USED-FOR global model. Bulyan CONJUNCTION FABA. FABA CONJUNCTION Bulyan. FABA CONJUNCTION FoolsGold. FoolsGold CONJUNCTION FABA. Krum CONJUNCTION Bulyan. Bulyan CONJUNCTION Krum. FoolsGold HYPONYM-OF Byzantine - resilient federated learning algorithms. Krum HYPONYM-OF Byzantine - resilient federated learning algorithms. FABA HYPONYM-OF Byzantine - resilient federated learning algorithms. Bulyan HYPONYM-OF Byzantine - resilient federated learning algorithms. defense USED-FOR directed deviation attack. TESSERACT HYPONYM-OF defense. TESSERACT USED-FOR directed deviation attack. learning algorithms CONJUNCTION models. models CONJUNCTION learning algorithms. reputation scores USED-FOR TESSERACT. TESSERACT USED-FOR attack. robustness EVALUATE-FOR TESSERACT. Method are untargeted model poisoning attack, and federated learning. OtherScientificTerm are gradient updates, and gradient flips. Metric is test error rate. Task is model poisoning attack. ",This paper proposes a Byzantine-resilient federated learning algorithm called TESSERACT to defend against untargeted model poisoning attacks. The proposed method is based on the idea that the gradients of the global model can be corrupted by the gradient of the local model. The authors show that the test error rate of the proposed method can be used as a metric to evaluate the robustness of the model.   ,"This paper proposes a Byzantine-resilient federated learning algorithm TESSERACT to defend against model poisoning attacks. TESSerACT is a defense against the directed deviation attack, which is an untargeted model poisoning attack. The defense is based on a reputation score that is used to measure the test error rate of the defense. The authors show that the defense is robust against the attack. "
2709,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"regularization CONJUNCTION model selection. model selection CONJUNCTION regularization. debiasing USED-FOR regularization. debiasing USED-FOR model selection. debiasing USED-FOR √ n - consistent and asymptotically normal estimation. double robustness CONJUNCTION Neyman orthogonality. Neyman orthogonality CONJUNCTION double robustness. functional - specific theoretical derivation USED-FOR influence function. correction term PART-OF plug - in estimator of the functional. Neural Nets CONJUNCTION Random Forests. Random Forests CONJUNCTION Neural Nets. Neural Nets USED-FOR Riesz representation of the linear functional. Random Forests USED-FOR Riesz representation of the linear functional. Neural Nets USED-FOR automatic debiasing procedure. Random Forests USED-FOR automatic debiasing procedure. value query oracle access USED-FOR linear functional. value query oracle access USED-FOR method. representation layers USED-FOR functions. stochastic gradient descent minimization USED-FOR Riesz representer and regression loss. stochastic gradient descent minimization USED-FOR multi - tasking Neural Net debiasing method. Random Forest method USED-FOR locally linear representation of the Riesz function. methodology USED-FOR arbitrary functionals. neural net based estimator USED-FOR average treatment effect functional. it COMPARE neural net based estimator. neural net based estimator COMPARE it. method USED-FOR estimating average marginal effects. gasoline demand FEATURE-OF semi - synthetic data of gasoline price changes. semi - synthetic data of gasoline price changes EVALUATE-FOR method. continuous treatments USED-FOR estimating average marginal effects. Task are causal and policy effects of interest, and Debiasing. ","This paper proposes a debiasing method for estimating the average marginal effects of treatments in continuous-action settings. The proposed method is based on the Riesz representation of the linear functional of the treatment effect, which is then approximated by a random forest method. The authors show that the proposed method achieves double robustness and asymptotically normal estimation. The method is shown to be computationally efficient and can be used to estimate the average treatment effect functional.","This paper proposes a method for estimating the average marginal effects of continuous treatments. The method is based on the Riesz representation of the linear functional, which is derived from a functional-specific theoretical derivation of the influence function. The main contribution of the paper is to propose a neural net-based estimator of the average treatment effect functional. The authors show that the proposed method is asymptotically normal and double robust to double robustness and Neyman orthogonality. They also show that their method can be used to estimate average marginal effect of continuous treatment."
2725,SP:96e1da163020441f9724985ae15674233e0cfe0d,"finite - time convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION finite - time convergence. sample complexity EVALUATE-FOR algorithm. single - agent actorcritic algorithms USED-FOR reinforcement learning. sample complexity bound EVALUATE-FOR single - agent actorcritic algorithms. Method is actor - critic algorithm. OtherScientificTerm are average reward, global average reward, and communication network. Generic is problem. Task is MARL setting. ","This paper studies the actor-critic algorithm for reinforcement learning in the multi-agent MARL setting. In this setting, there are two agents and a communication network, and the goal is to maximize the average reward of each agent. The main contribution of this paper is to provide a sample complexity bound of $O(1/\sqrt{T})$ for a single-agent actor critic algorithm. The sample complexity of this algorithm is shown to be bounded by a factor of $\Omega(T)$, where $T$ is the number of agents in the network.  ","This paper studies the problem of actor-critic reinforcement learning in a multi-agent MARL setting. In this setting, the agent is given a global average reward and a communication network, and the goal is to improve the performance of the agent with respect to the global average. The authors prove a sample complexity bound for a single-agent actor critic algorithm. They also prove a finite-time convergence bound for the sample complexity of the algorithm."
2741,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"contrastive learning USED-FOR large - scale self - supervised learning. approach USED-FOR large - scale self - supervised learning. contrastive learning HYPONYM-OF approach. conditional independence assumption USED-FOR guarantee. theory COMPARE contrastive methods. contrastive methods COMPARE theory. synthetic and real - world datasets EVALUATE-FOR contrastive methods. synthetic and real - world datasets EVALUATE-FOR theory. contrastive learning USED-FOR class - separated representations. Generic is it. OtherScientificTerm are positive samples, and intra - class samples. Task is surrogate task. ","This paper studies the problem of self-supervised learning with contrastive learning in the presence of a surrogate task. The main contribution of the paper is to prove that the contrastive loss is independent of the surrogate task, which is a generalization of the conditional independence assumption. The authors show that this is the case under certain assumptions, including that the surrogate tasks are independent of each other and that there are no intra-class samples. They then show that under certain conditions, contrastive methods can learn class-separated representations that are independent from each other. They also show that contrastive losses can be used to improve the performance of surrogate tasks.  ","This paper studies the problem of contrastive learning for large-scale self-supervised learning. The main contribution of the paper is to provide a theoretical guarantee of the conditional independence of class-separated representations. The authors show that under certain conditions, the class separation can be guaranteed. They show that this guarantee holds for both positive samples and intra-class samples. They also provide an empirical analysis on synthetic and real-world datasets. "
2757,SP:b491314336c503b276e34e410cf461cb81294890,"Speech restoration USED-FOR distortions in speech signals. speech denoising CONJUNCTION speech declipping. speech declipping CONJUNCTION speech denoising. Prior methods USED-FOR single - task speech restoration ( SSR ). speech declipping HYPONYM-OF single - task speech restoration ( SSR ). speech denoising HYPONYM-OF single - task speech restoration ( SSR ). SSR systems USED-FOR speech restoration tasks. SSR systems USED-FOR speech restoration problem. speech super - resolution HYPONYM-OF speech restoration tasks. generative framework USED-FOR GSR task. VoiceFixer1 HYPONYM-OF generative framework. VoiceFixer1 USED-FOR GSR task. analysis stage CONJUNCTION synthesis stage. synthesis stage CONJUNCTION analysis stage. VoiceFixer USED-FOR speech analysis. synthesis stage USED-FOR speech analysis. synthesis stage PART-OF VoiceFixer. analysis stage PART-OF VoiceFixer. ResUNet USED-FOR analysis stage. neural vocoder USED-FOR synthesis stage. ResUNet CONJUNCTION neural vocoder. neural vocoder CONJUNCTION ResUNet. neural vocoder USED-FOR analysis stage. additive noise CONJUNCTION room reverberation. room reverberation CONJUNCTION additive noise. room reverberation CONJUNCTION low - resolution, and clipping distortions. low - resolution, and clipping distortions CONJUNCTION room reverberation. additive noise USED-FOR VoiceFixer. room reverberation FEATURE-OF VoiceFixer. low - resolution, and clipping distortions EVALUATE-FOR VoiceFixer. baseline GSR model COMPARE speech denoising SSR model. speech denoising SSR model COMPARE baseline GSR model. mean opinion score ( MOS ) EVALUATE-FOR speech denoising SSR model. mean opinion score ( MOS ) EVALUATE-FOR baseline GSR model. VoiceFixer COMPARE GSR baseline model. GSR baseline model COMPARE VoiceFixer. MOS score EVALUATE-FOR GSR baseline model. MOS score EVALUATE-FOR VoiceFixer. old movies CONJUNCTION historical speeches. historical speeches CONJUNCTION old movies. VoiceFixer USED-FOR historical speeches.","This paper proposes a generative framework for speech super-resolution (GSR) task. The proposed method consists of two stages: analysis stage and synthesis stage. The analysis stage is used for speech analysis, and the synthesis stage uses a neural vocoder to generate speech samples. The synthesis stage is then used to train a neural network for speech restoration. Experiments show that the proposed method outperforms the state-of-the-art speech denoising and speech declipping methods. ",This paper proposes a new generative framework for single-task speech restoration (SSR) called VoiceFixer1. The proposed method is based on the GSR framework. The main idea is to combine the synthesis stage of GSR with the analysis stage of SpeechFixer. The synthesis stage consists of a neural vocoder and ResUNet. The analysis stage is composed of two stages: 1) synthesis stage and 2) analysis stage. The authors show that the proposed method outperforms the state-of-the-art GSR model in terms of mean opinion score (MOS) for speech denoising.
2773,SP:c80a7392ec6147395a664734601fb389a1eb4470,"Residual Tensor Networks ( MVSRTN ) USED-FOR multivariate time series. Residual Tensor Networks ( MVSRTN ) USED-FOR Variable Space. tensor network USED-FOR variable space. low - rank approximation USED-FOR variable space. low - rank approximation USED-FOR tensor network. translation invariance FEATURE-OF network. tensor components USED-FOR translation invariance. tensor components USED-FOR network. it USED-FOR space - approximated tensor network. seriesvariable encoder USED-FOR variable space. skip - connection layer USED-FOR dissemination of information. scale HYPONYM-OF dissemination of information. multivariate time series forecasting benchmark datasets EVALUATE-FOR method. Material is Multivariate time series. OtherScientificTerm are latent space, time window, and long - term sequences. Generic is framework. Method is N - order residual connection approach. ","This paper proposes a novel residual tensor network (MVSRTN) for multivariate time series forecasting. The proposed method is based on a low-rank approximation to the variable space, which is then used to train a series variable encoder and a skip-connection layer. Theoretical analysis is provided to show that the proposed method achieves better performance than the state-of-the-art in terms of time-series forecasting accuracy.",This paper proposes a new method for forecasting multivariate time series using residual tensor networks (MVSRTN). MVSRTN is a low-rank approximation of a tensor network that is invariant to time-varying time series. The key idea is to use a series variable encoder and a skip-connection layer to speed up the dissemination of information in the variable space. The proposed method is evaluated on a variety of time series forecasting datasets. 
2789,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"Neighbor sampling USED-FOR Graph Neural Networks ( GNNs ). large graphs USED-FOR Graph Neural Networks ( GNNs ). Stochastic Compositional Optimization ( SCO ) problems USED-FOR samplingbased GNN training. SCO algorithms USED-FOR samplingbased GNN training. SCO algorithms USED-FOR GNNs. moving averages USED-FOR aggregated features. moving averages USED-FOR they. large graphs USED-FOR GNNs. GPU memory limit CONJUNCTION CPU memory limit. CPU memory limit CONJUNCTION GPU memory limit. GPU memory limit FEATURE-OF moving averages. SCO algorithms USED-FOR GNN training. sparse moving averages USED-FOR SCO algorithms. moving averages USED-FOR algorithm. fixed size buffer USED-FOR algorithm. convergence rate EVALUATE-FOR SCO algorithm. algorithm USED-FOR SCO algorithm. convergence rate EVALUATE-FOR algorithm. Adam SGD USED-FOR GNN training. algorithm COMPARE Adam SGD. Adam SGD COMPARE algorithm. algorithm USED-FOR GNN training. memory overhead EVALUATE-FOR algorithm. OtherScientificTerm are graph, graph size, and buffer size. ",This paper studies the problem of sampling-based GNN training in the presence of large graphs. The authors propose to use sparse moving averages as the aggregated features for aggregation in GNNs. The main contribution of this paper is to propose a new SCO algorithm for GNN sampling based on moving averages. Theoretical analysis is provided to show that the proposed SCO algorithms converge to a fixed size buffer with a convergence rate of $O(1/\sqrt{n})$ with a memory overhead of $\Omega(n^2)$ in the case of a fixed graph size. The proposed algorithm is shown to be faster than Adam SGD in terms of convergence rate and memory overhead.,This paper proposes a new Stochastic Compositional Optimization (SCO) algorithm for sampling-based GNN training. The main contribution of the paper is to propose a new SCO algorithm that can be used for sampling based GNNs. The proposed algorithm is based on the Adam SGD algorithm. The authors show that the proposed SCO algorithms can converge to a fixed size buffer with a convergence rate of 1.5x faster than the current state-of-the-art. They also provide a theoretical analysis of the convergence rate and show that their algorithm is more efficient than the existing SGD algorithms.
2805,SP:72e0cac289dce803582053614ec9ee93e783c838,"random hashes USED-FOR Jaccard ( resemblance ) similarity. Minwise hashing ( MinHash ) USED-FOR random hashes. massive binary ( 0/1 ) data USED-FOR Jaccard ( resemblance ) similarity. large - scale learning models CONJUNCTION approximate near neighbor search. approximate near neighbor search CONJUNCTION large - scale learning models. massive data USED-FOR approximate near neighbor search. independent random permutations USED-FOR MinHash. that COMPARE classical MinHash. classical MinHash COMPARE that. Jaccard estimation variance EVALUATE-FOR classical MinHash. circulant manner FEATURE-OF independent random permutations. permutations USED-FOR it. estimation accuracy EVALUATE-FOR it. Method are Circulant MinHash ( C - MinHash ), and C - MinHash variant. Generic is method. ","This paper proposes Circulant MinHash (C-MinHash), a variant of MinHash that uses independent random permutations to improve the Jaccard (similarity) estimation variance of random hash functions. The authors show that C-minHash is more efficient than the classical MinHash in terms of the variance of the similarity between the two hash functions, and that it can be used in conjunction with approximate near-neighbor search. The experiments show that the proposed method is competitive with the state-of-the-art.","This paper proposes Circulant MinHash (C-MinHash), a variant of MinHash that uses independent random permutations to improve the Jaccard (similarity) similarity between two random hashes. The authors show that C-minHash is more accurate than classical MinHash in terms of the variance of the Jacard estimation variance. They also show that their method can be used in conjunction with a large-scale learning model for approximate near neighbor search. "
2821,SP:d254b38331b6b6f30de398bae09380cd5c951698,Adversarial training ( AT ) USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR single lpthreat models. lp - threat models USED-FOR adversarial robustness. training scheme USED-FOR adversarial robustness. training scheme USED-FOR union of lp - threat models. adversarial training USED-FOR lp - threat model. E - AT scheme USED-FOR lp - robust model. multiple - norm robustness EVALUATE-FOR state - of - the - art. multiple norm robustness FEATURE-OF ImageNet models. CIFAR-10 EVALUATE-FOR multiple - norm robustness. CIFAR-10 EVALUATE-FOR state - of - the - art. adversarial robustness FEATURE-OF threat models. CIFAR-10 EVALUATE-FOR SOTA l1 - robustness. Task is safety - critical systems. OtherScientificTerm is lp - balls. Metric is multiple norm adversarial robustness. ,"This paper proposes a new adversarial training scheme to improve the adversarial robustness of single lp-threat models. The main idea is to train a union of lp threat models, where each model is trained with adversarial perturbations from a set of multiple adversarial examples. The proposed method is evaluated on ImageNet and CIFAR-10.   ","This paper proposes a new adversarial training scheme to improve the adversarial robustness of a single lp-threat model against multiple norm adversarial attacks. The proposed method is based on the E-AT scheme, which is a union of two lp threat models. The main contribution of the paper is to introduce a new metric for measuring the multiple norm robustness. The metric is defined as the number of lp balls in the training set, and it can be used to measure the robustness to multiple norm attacks. Empirical results on CIFAR-10 and ImageNet show that the proposed method achieves state-of-the-art performance."
2837,SP:4c2928f6772664d63c02c29f913b476e1c932983,MTL models COMPARE single - task counterpart. single - task counterpart COMPARE MTL models. private encoders CONJUNCTION gates. gates CONJUNCTION private encoders. gates CONJUNCTION private decoders. private decoders CONJUNCTION gates. public encoder CONJUNCTION private encoders. private encoders CONJUNCTION public encoder. private encoder CONJUNCTION gate. gate CONJUNCTION private encoder. gate CONJUNCTION private decoder. private decoder CONJUNCTION gate. public encoder CONJUNCTION private encoder. private encoder CONJUNCTION public encoder. storage cost FEATURE-OF inference stage. SMTL USED-FOR gate. SMTL USED-FOR gates. benchmark datasets EVALUATE-FOR methods. Method is Multi - Task Learning ( MTL ). Generic is problem. OtherScientificTerm is negative sharing. Task is safe multi - task learning. ,This paper studies the problem of safe multi-task learning in the presence of negative sharing. The authors propose a new approach called Safe Multi-Task Learning (SMTL) to address the negative sharing problem. The main contribution of the paper is the introduction of a new private encoder for each task and a gate for each private decoder. Theoretical analysis is provided to show that the storage cost of the gate can be reduced by using SMTL. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed method. ,"This paper proposes a new multi-task learning (MTL) framework for the problem of negative sharing. The key idea is to use a single-task model for each task, and then use the same model for all the tasks. The main contribution of the paper is to propose a new method for the task-sharing problem, which is based on the SMTL framework. The proposed method is evaluated on a variety of benchmark datasets, and it is shown that the proposed method outperforms the state-of-the-art in terms of performance."
2853,SP:c4cee0d44198559c417750ec4729d26b41061929,"energy - based sequence models USED-FOR partition functions. expressive parametric families USED-FOR energy - based sequence models. model selection CONJUNCTION learning model parameters. learning model parameters CONJUNCTION model selection. partition functions FEATURE-OF sequence model families. asymptotic guarantees FEATURE-OF statistical procedures. OtherScientificTerm are model parameters, partition function, rational number, reduced expressiveness, and computability concerns. Generic is they. Task is sequence modeling. Method is model parametrizations. ","This paper studies the problem of learning partition functions for energy-based sequence models. In particular, the authors propose a family of partition functions that can be expressed in terms of an expressive parametric family. The authors show that the partition function can be represented as a function of the number of model parameters and the rational number of the model parameters. They then show that this family can be used as a way to estimate the partition functions of the sequence model families.","This paper studies the problem of learning expressive parametric families for energy-based sequence models (e.g., energy-driven sequence models). The authors consider a family of sequence models where the partition function is a rational number and the model parameters are parametrizable. The authors provide asymptotic bounds on the number of partition functions that can be learned in this family. They show that the partition functions can be chosen in such a way that they are computationally tractable. They also provide a theoretical analysis of the choice of model parameters for the family."
2869,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"Wasserstein distance USED-FOR large - scale machine learning problems. random projection USED-FOR sliced Wasserstein distance. computational efficiency EVALUATE-FOR sliced Wasserstein distance. augmented sliced Wasserstein distances ( ASWDs ) HYPONYM-OF distance metrics. neural networks USED-FOR higher - dimensional hypersurfaces. they USED-FOR complex structures of the data distribution. gradient ascent USED-FOR hypersurfaces. ASWD COMPARE Wasserstein variants. Wasserstein variants COMPARE ASWD. Wasserstein variants USED-FOR synthetic and real - world problems. synthetic and real - world problems EVALUATE-FOR ASWD. Metric is computational cost. OtherScientificTerm are projections, ( random ) linear projections, and nonlinear projections. Method is injective neural network architecture. ","This paper proposes a new method for computing sliced Wasserstein distance (SWD) based on neural networks. The main idea is to use a neural network to map the data to a set of higher dimensional hypersurfaces, which can then be used to compute the sliced SWD. The proposed method is based on the fact that the data is represented as a mixture of random projections. The authors show that the proposed method can be used in conjunction with existing methods for computing SWD, and that it is computationally efficient.  ","This paper proposes a new method for computing sliced Wasserstein distances (ASWDs) for large-scale machine learning problems. The proposed method is based on the idea of augmented sliced wasserstein distance, which is a generalization of sliced WASSERstein distance. The main contribution of the paper is to propose a new algorithm for computing ASWDs. The authors show that the proposed ASWD is computationally efficient and can be used to compute higher-dimensional hypersurfaces. They also show that ASWD can be applied to synthetic and real-world problems."
2885,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,coordinated exploration and behaviour PART-OF multi - agent systems. framework USED-FOR multi - agent reinforcement learners ( MARL ). reinforcement learning ( RL ) CONJUNCTION switching controls. switching controls CONJUNCTION reinforcement learning ( RL ). switching controls USED-FOR LIGS. LIGS USED-FOR learning process. intrinsic rewards USED-FOR learning process. LIGS USED-FOR intrinsic rewards. reinforcement learning ( RL ) USED-FOR LIGS. LIGS USED-FOR systems of RL agents. sparse rewards USED-FOR systems of RL agents. it USED-FOR joint policies. multi - agent RL algorithms USED-FOR LIGS. Foraging CONJUNCTION StarCraft. StarCraft CONJUNCTION Foraging. LIGS framework USED-FOR Foraging. LIGS framework USED-FOR StarCraft. Method is reinforcement learners ( RL ). ,"This paper proposes a framework for multi-agent reinforcement learning (MARL) in which agents learn to coordinate their exploration and behaviour in order to improve the performance of RL agents. The proposed method, called LIGS, is a combination of reinforcement learning and switching controls. The main idea is to learn a set of intrinsic rewards for each agent, which are then used to guide the agent's exploration and behavior. The authors show that the proposed method outperforms existing MARL methods in a variety of experiments. ","This paper proposes a new framework for multi-agent reinforcement learning (MARL) called LIGS, which aims to improve the performance of MARL agents in the multi-player RL setting. The authors propose a new reward function for MARL, which is based on the notion of switching controls. They show that the proposed method can be applied to a variety of RL problems, including Foraging, StarCraft, and Foraging-like environments. They also show that it can be used to learn a policy that can be combined with other RL algorithms. "
2901,SP:9eadc19f7f712c488cf50d091f372092f6352930,multi - hop QA systems COMPARE DOCHOPPER. DOCHOPPER COMPARE multi - hop QA systems. document information CONJUNCTION q. q CONJUNCTION document information. QA tasks EVALUATE-FOR DOCHOPPER. datasets EVALUATE-FOR DOCHOPPER. inference time EVALUATE-FOR DOCHOPPER. Generic is model. Method is compact neural representation of q. ,"This paper proposes a method to learn document embeddings for multi-hop QA tasks. The proposed method is based on the idea of learning a compact neural representation of the document embedding, which is then used to train a model to predict the QA QA task. The method is evaluated on a variety of QA datasets and achieves state-of-the-art performance.","This paper proposes a new model for multi-hop QA, called DOCHOPPER, which is based on a compact neural representation of document information. The model is trained on a set of datasets, and the authors show that it can achieve state-of-the-art performance on a variety of QA tasks. The main contribution of the paper is the use of a compact representation of the document information, which allows the model to be trained in an efficient manner. The proposed model is evaluated on a range of tasks, and it is shown that it is able to achieve state of the art performance on most of them."
2917,SP:4e79b326bbda5d1509e88869dde9886764366d41,"modalities USED-FOR voice search request. voice casting USED-FOR audiovisual productions. it USED-FOR modalities. it USED-FOR voice recommendation system. it USED-FOR voice search request. characteristic extraction USED-FOR voice casting. taxonomy USED-FOR comedian voices. taxonomy USED-FOR annotation protocol. Label Refining HYPONYM-OF semi - supervised learning method. vocal characteristics HYPONYM-OF refined labels. clustering algorithm USED-FOR refined representation extractor. refined labels USED-FOR refined representation extractor. representation extractor USED-FOR method. clustering algorithm USED-FOR refined labels. Label Refining USED-FOR method. subsidiary corpus USED-FOR voice characteristics. OtherScientificTerm are characteristic, and priori knowledge. Material is MassEffect 3 video game. ","This paper proposes a semi-supervised learning method for voice casting in video games. The proposed method is based on the idea of ""label refining"", which is a method to refine the labels of the voice attributes. The method is evaluated on the MassEffect 3 video game.   ","This paper proposes a semi-supervised learning method for voice casting. The proposed method is based on the idea of label refining, where the labels are refined based on prior knowledge of the characteristics of the target modality. The method is applied to voice casting in the MassEffect 3 video game. The authors show that the proposed method outperforms the state-of-the-art voice casting method in terms of quality and accuracy."
2933,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"federated and split learning HYPONYM-OF Distributed collaborative learning approaches. Vision Transformer ( ViT ) USED-FOR common representation. Vision Transformer ( ViT ) USED-FOR computer vision applications. global attention USED-FOR common representation. distributed learning framework USED-FOR image processing tasks. ViT USED-FOR distributed learning framework. task - agnostic Vision Transformer CONJUNCTION task - specific head / tail. task - specific head / tail CONJUNCTION task - agnostic Vision Transformer. task - specific heads and tails CONJUNCTION task - agnostic Transformer body. task - agnostic Transformer body CONJUNCTION task - specific heads and tails. features PART-OF representation. global attention USED-FOR Transformer body. task - agnostic learning USED-FOR Transformer. task - specific learning USED-FOR heads. alternating training strategy USED-FOR task - specific learning. method USED-FOR task - specific network. multi - task learning EVALUATE-FOR method. Method is neural networks. Generic are they, applications, and translation. OtherScientificTerm is customer - specific head and tail. Material is medical image data. ","This paper proposes a method for distributed multi-task learning using vision transformers (ViTs) for image processing tasks. The main idea is to train a Transformer with task-agnostic heads and tails, and then use an alternating training strategy for task-specific learning. Experiments show that the proposed method achieves state-of-the-art performance on a variety of image classification tasks.",This paper proposes a new distributed learning framework for multi-task learning. The main idea is to use a Transformer-based model to learn a task-agnostic head and tail representation for each task. The task-specific heads and tails are learned using an alternating training strategy. The proposed method is evaluated on a variety of medical image datasets. 
2949,SP:249a72ef4e9cf02221243428174bb749068af6b2,"misspecified reward functions USED-FOR RL agents. misspecified rewards FEATURE-OF RL environments. action space resolution CONJUNCTION observation space noise. observation space noise CONJUNCTION action space resolution. model capacity CONJUNCTION action space resolution. action space resolution CONJUNCTION model capacity. observation space noise CONJUNCTION training time. training time CONJUNCTION observation space noise. agent capabilities USED-FOR reward hacking. training time HYPONYM-OF agent capabilities. model capacity HYPONYM-OF agent capabilities. observation space noise HYPONYM-OF agent capabilities. action space resolution HYPONYM-OF agent capabilities. proxy reward CONJUNCTION true reward. true reward CONJUNCTION proxy reward. capability thresholds HYPONYM-OF phase transitions. anomaly detection task USED-FOR aberrant policies. Task are Reward hacking, and ML systems. OtherScientificTerm is reward misspecifications. Method is baseline detectors. ",This paper studies the problem of reward hacking in reinforcement learning. The authors propose a method for detecting reward misspecification in RL agents. The method is based on the observation-based anomaly detection task and uses a combination of a proxy reward and a true reward. The paper shows that the proposed method can detect reward misclassification in a variety of settings.   ,"This paper studies the problem of reward hacking in reinforcement learning (RL). The authors propose a new anomaly detection task to detect anomalous behavior in the reward-hacking setting. They show that the agent capabilities can be affected by a number of factors, including the training time, model capacity, action space resolution, and observation space noise. They also propose a method to detect anomalies in this setting. "
2965,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,Kullback – Leibler ( KL ) divergence CONJUNCTION arbitary differeitiable f divergence. arbitary differeitiable f divergence CONJUNCTION Kullback – Leibler ( KL ) divergence. f -TVO USED-FOR Thermodynamic Variational Objective ( TVO ). f -TVO USED-FOR dual function of model evidence f∗(p(x ) ). log model evidence PART-OF TVO. dual function of model evidence f∗(p(x ) ) COMPARE log model evidence. log model evidence COMPARE dual function of model evidence f∗(p(x ) ). deformed χ - geometry perspective USED-FOR f -TVO. variational posterior distribution CONJUNCTION true posterior distribution. true posterior distribution CONJUNCTION variational posterior distribution. χ - exponential family exponential USED-FOR f -TVO. χ - path USED-FOR f -TVO. reparameterization trick CONJUNCTION Monte Carlo approximation. Monte Carlo approximation CONJUNCTION reparameterization trick. reparameterization trick PART-OF f -TVO. Monte Carlo approximation PART-OF f -TVO. VAE CONJUNCTION Bayesian neural network. Bayesian neural network CONJUNCTION VAE. f -TVO COMPARE cooresponding baseline f -divergence variational inference. cooresponding baseline f -divergence variational inference COMPARE f -TVO. Bayesian neural network EVALUATE-FOR f -TVO. VAE EVALUATE-FOR f -TVO. Bayesian neural network EVALUATE-FOR cooresponding baseline f -divergence variational inference. OtherScientificTerm is deformed geodesic. ,"This paper studies the dual function of model evidence f(p(x)$ in the variational objective f-TVO. The dual function is defined as the dual of f-divergence, which is a Kullback-Leibler (KL) divergence. The main contribution of the paper is to show that the dual can be expressed in terms of a deformed χ-geometry perspective.   The main contributions of this paper are as follows:  1. The authors show that f(P(x)) is a dual of the true posterior p(x), which can be represented by a deformation of the geodesic.  2. They show that this dual function can be approximated using a reparameterization trick and a Monte Carlo approximation.  3. They prove the convergence of the dual to the true true posterior.  4. They demonstrate the convergence in Bayesian inference.","This paper studies the dual function of model evidence in the Thermodynamic Variational Objective (TVO) framework. The main contribution of the paper is to derive a new dual function f-TVO for the Kullback-Leibler (KLK) divergence and the arbitary differeitiable f divergence. The dual function is defined as a function of the true posterior distribution f(x) and the true model evidence f(p(x), where f is the dual of the model evidence and p is the log of the log model evidence. This dual function can be used to derive the true and true posterior distributions for the fTVO objective.  The main contributions of the work are as follows:  1. The authors derive a deformed χ-geometry perspective for fTVo, which allows them to define the dual for f TVO. 2. They show that fTVOs can be approximated by a family of exponential family exponential f-divergence variational approximations. 3. They also provide a Monte Carlo approximation of fTVos. 4. They provide a Bayesian neural network experiment on VAE and Bayesian Neural Network."
2981,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"critics ’ initialization USED-FOR ensemble - based actor - critic exploration. approximated UCB CONJUNCTION weighted Bellman backup. weighted Bellman backup CONJUNCTION approximated UCB. strategy COMPARE approximated UCB. approximated UCB COMPARE strategy. weighted Bellman backup COMPARE clipped double Q - Learning. clipped double Q - Learning COMPARE weighted Bellman backup. additive action noise USED-FOR exploration. weighted Bellman backup USED-FOR strategy. actors ’ initialization USED-FOR training. posterior sampling USED-FOR strategy. methods USED-FOR policies. Method are deep reinforcement learning ( RL ), RL toolbox, and ED2. Generic are task, and tools. Material is continuous control setting. OtherScientificTerm is evaluation runs. Task is continuous control tasks. ","This paper proposes an ensemble-based actor-critic exploration method for continuous control tasks. The proposed method is based on an ensemble of critic policies, where the critic policies are trained using a weighted Bellman update. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of continuous control benchmarks.  ",This paper proposes an ensemble-based actor-critic exploration method for continuous control tasks. The authors propose a weighted Bellman backup method for the ensemble exploration problem. They show that the proposed method outperforms the state-of-the-art in terms of UCB and Q-learning performance. They also show that their method is more robust to additive action noise. 
2997,SP:21819b54433fa274657d9fe418f66407eee83eeb,"natural language processing CONJUNCTION face recognition. face recognition CONJUNCTION natural language processing. lending CONJUNCTION college admission. college admission CONJUNCTION lending. Supervised learning models USED-FOR domains. college admission CONJUNCTION natural language processing. natural language processing CONJUNCTION college admission. lending HYPONYM-OF domains. face recognition HYPONYM-OF domains. college admission HYPONYM-OF domains. natural language processing HYPONYM-OF domains. fairness notions USED-FOR fairness issues. fair predictor USED-FOR constrained optimization problem. Equalized Loss ( EL ) HYPONYM-OF fairness notion. prediction error / loss USED-FOR fairness notion. algorithms USED-FOR global optimum. algorithms USED-FOR non - convex problem. global optimum USED-FOR non - convex problem. convex programming tools USED-FOR algorithms. ELminimizer algorithm USED-FOR EL fair predictor. non - convex optimization problem USED-FOR EL fair predictor. convex constrained optimizations USED-FOR non - convex optimization problem. algorithm USED-FOR sub - optimal EL fair predictor. algorithm COMPARE ELminimizer. ELminimizer COMPARE algorithm. unconstrained convex programming tools USED-FOR algorithm. unconstrained convex programming tools USED-FOR sub - optimal EL fair predictor. real - world data EVALUATE-FOR algorithms. Generic are models, it, and constraint. OtherScientificTerm are protected social groups, and loss function. Method is learning process. ","This paper studies the problem of fair learning in the presence of protected social groups in supervised learning models. In particular, the authors consider the fair learning problem, where the goal is to learn a fair predictor that maximizes the prediction error and minimizes the fair loss. The authors propose to use the Equalized Loss (EL) as the fair predictor, which is a general notion of fairness. They show that the EL fair predictor is a convex optimization problem with a non-convex constrained optimization problem. They provide two algorithms for solving this problem:   1. The first algorithm is based on the ELminimizer algorithm.  2. The second algorithm uses convex constrained optimizations to solve the problem.   ","This paper studies the problem of finding a fair fair predictor for a constrained optimization problem with Equalized Loss (EL) in a non-convex optimization problem. The authors propose a new algorithm, called ELminimizer, to find a sub-optimal EL fair predictor. The proposed algorithm is based on convex constrained optimization, where the objective is to find the global optimum of the fair predictor in a constrained setting.  The authors show that the proposed algorithm outperforms the state-of-the-art in terms of accuracy and fairness. "
3013,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"neural networks USED-FOR cognitive capacity. meaningful learning USED-FOR systematic generalization. compositional skills FEATURE-OF models. semantic connections USED-FOR models. semantic connections USED-FOR compositional skills. semantic links USED-FOR models. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. CNNs CONJUNCTION Transformers. Transformers CONJUNCTION CNNs. SCAN CONJUNCTION real - world datasets. real - world datasets CONJUNCTION SCAN. real - world datasets USED-FOR semantic parsing. semantic linking USED-FOR sequenceto - sequence models. RNNs HYPONYM-OF sequenceto - sequence models. CNNs HYPONYM-OF sequenceto - sequence models. Transformers PART-OF sequenceto - sequence models. prior knowledge CONJUNCTION semantic linking. semantic linking CONJUNCTION prior knowledge. prior knowledge USED-FOR systematic generalization. semantic linking USED-FOR systematic generalization. inductive learning COMPARE deductive learning. deductive learning COMPARE inductive learning. neural networks USED-FOR systematic generalization. learning schemes USED-FOR neural networks. learning schemes USED-FOR systematic generalization. Material is SCAN dataset. Method are meaningful learning principle, and data augmentation techniques. OtherScientificTerm is inductive or deductive manner. Generic is them. ","This paper investigates the role of meaningful learning in systematic generalization in the context of semantic parsing. The authors propose to use semantic linking to improve the generalization ability of models trained on semantic parsing tasks. They show that meaningful learning can be used to improve generalization performance in a variety of ways, including using data augmentation techniques. They also show that using semantic linking can improve the performance in terms of generalization.","This paper studies the effect of semantic linking on the generalization ability of deep neural networks. The authors show that semantic linking can be used to improve the generalizability of neural networks in the context of meaningful learning. They also show that it can be applied to a variety of models, including RNNs, CNNs, Transformers, and SCAN datasets. They show that the semantic linking helps to improve generalization in terms of data augmentation. "
3029,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"method USED-FOR 3D shape representation. multi - scale wavelet decomposition USED-FOR method. up / down - sampling USED-FOR hierarchies. sub - bands components PART-OF 3D shapes. lifting scheme USED-FOR high or low sub - bands components. Transformers USED-FOR AWT - Net. shape features USED-FOR them. 3D shape classification and segmentation benchmarks EVALUATE-FOR AWT - Net. OtherScientificTerm are decomposition tree, approximation or detail wavelet coefficients, features, and wavelet coefficients. Method are multi - resolution wavelet analysis, and holistic representations. ","This paper proposes a novel 3D shape representation learning method based on a multi-scale wavelet decomposition of 3D shapes. The proposed method, AWT-Net, is based on the idea of up/down-sampling to learn hierarchies of sub-band components of 3d shapes. A lifting scheme is used to select high or low sub-bands components and shape features for each of them. The authors show that the proposed method achieves state-of-the-art results on three benchmark datasets. ","This paper proposes a new wavelet-based 3D shape representation for 3D classification and segmentation tasks. The proposed AWT-Net is based on a multi-scale wavelet decomposition of 3D shapes into sub-band components, which are represented by a lifting scheme. The shape features are extracted from the wavelet coefficients of the sub-bands, and the shape features of the high- and low-band component are also extracted. The authors show that the proposed method is able to achieve state-of-the-art performance on 3D segmentation and shape classification benchmarks."
3045,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"pretrained language model USED-FOR natural language generation tasks. prefixtuning CONJUNCTION adapters. adapters CONJUNCTION prefixtuning. Lightweight finetuning COMPARE full finetuning. full finetuning COMPARE Lightweight finetuning. adapters HYPONYM-OF Lightweight finetuning. prefixtuning HYPONYM-OF Lightweight finetuning. lightweight finetuning COMPARE full finetuning in - distribution ( ID ). full finetuning in - distribution ( ID ) COMPARE lightweight finetuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR methods. ID CONJUNCTION OOD. OOD CONJUNCTION ID. full and lightweight finetuning USED-FOR ID. full and lightweight finetuning USED-FOR OOD. cocktail finetuning USED-FOR full finetuning. model CONJUNCTION cocktail finetuning. cocktail finetuning CONJUNCTION model. distillation USED-FOR lightweight model. lightweight model USED-FOR full finetuning. distillation USED-FOR full finetuning. distillation USED-FOR OOD behavior. distillation USED-FOR model. distillation USED-FOR ID data. OOD behavior FEATURE-OF model. Method are pretrained model, and lightweight and full finetuning models. Task is multiclass logistic regression setting. ",This paper studies the effect of finetuning in-based pre-trained language models on out-of-distribution (OOD) generation. The authors show that finetuned language models are more sensitive to OOD data than full finetunned models. They also show that lightweight pre-precision methods (prefixtuning and adapters) can improve OOD generation performance.  ,This paper proposes a new way to perform full and lightweight finetuning in-distribution (ID) in a multiclass logistic regression setting. The main idea is to distill the ID data into a lightweight model and then use the lightweight model to train a full finetuned model for OOD data. Theoretical analysis is provided to show that the distillation of the lightweight models can improve the performance of the full models. Experiments are conducted on synthetic data and real-world data.
3061,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"health CONJUNCTION governance. governance CONJUNCTION health. pointillistically labeled data USED-FOR data - hungry ML algorithms. Data programming USED-FOR probabilistic training labels. domain experts USED-FOR labelling functions. approach USED-FOR iterative and interactive improvement of weakly supervised models. WARM HYPONYM-OF Active Refinement of Weakly Supervised Models. active learning USED-FOR approach. active learning USED-FOR weakly supervised models. probabilistic accuracy EVALUATE-FOR label model. expert labelling functions PART-OF weak supervision model. probabilistic labels USED-FOR downstream classifiers. real - world medical classification datasets EVALUATE-FOR WARM. WARM USED-FOR probabilistic labels. accuracy EVALUATE-FOR probabilistic labels. accuracy EVALUATE-FOR WARM. domain shift CONJUNCTION artificial noise. artificial noise CONJUNCTION domain shift. population characteristics CONJUNCTION noisy initial labelling functions. noisy initial labelling functions CONJUNCTION population characteristics. noisy initial labelling functions FEATURE-OF WARM. population characteristics FEATURE-OF WARM. WARM USED-FOR weakly supervised systems. Method are Supervised machine learning ( ML ), and ML methods. Task are clinical research, and data collection. Generic is framework. OtherScientificTerm are weak supervision, and Gradient updates. ","This paper proposes a method to improve the performance of weakly supervised models by using data programming to generate probabilistic training labels for domain experts. The proposed method, called Active Refinement of Weakly Supervised Models (WARM), is an active learning approach to improve weakly-supervised models in the presence of domain shift, artificial noise, and domain shift. Experiments on real-world medical classification datasets demonstrate the effectiveness of the proposed method. ","This paper proposes Active Refinement of Weakly Supervised Models (WARM), a method for improving the performance of weakly supervised models. The main idea is to use data programming to improve the probabilistic training labels of a weakly-supervised model. The authors propose to use the data-driven approach of active learning to improve a model’s performance on a set of pointillistically-labeled data. The proposed method is evaluated on a variety of real-world medical classification datasets. "
3077,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"group annotated training data USED-FOR classification model. empirical risk minimization ( ERM ) objective USED-FOR models. it COMPARE ERM. ERM COMPARE it. Group - DRO COMPARE ERM. ERM COMPARE Group - DRO. ERM USED-FOR minority groups. Group - DRO USED-FOR minority groups. algorithm USED-FOR learning of features. algorithm COMPARE baselines. baselines COMPARE algorithm. ERM CONJUNCTION Group - DRO. Group - DRO CONJUNCTION ERM. minority groups FEATURE-OF benchmarks. Group - DRO HYPONYM-OF baselines. benchmarks EVALUATE-FOR Group - DRO. ERM HYPONYM-OF baselines. benchmarks EVALUATE-FOR baselines. benchmarks EVALUATE-FOR algorithm. algorithm USED-FOR smooth nonconvex functions. descent method USED-FOR algorithm. OtherScientificTerm are distribution shift, and learning of shared / common features. Task is domain generalization. Metric is regularized loss. ","This paper studies the problem of generalization in the presence of a distribution shift in the training data. The authors propose a new objective called Group-DRO, which is based on the empirical risk minimization (ERM) objective. Theoretical analysis shows that the proposed objective is equivalent to the original ERM objective in terms of the regularized loss, and that it can be used to improve the generalization performance of the model. Empirical results show that this objective can improve the performance of minority groups.   ","This paper proposes a new algorithm for the empirical risk minimization (ERM) objective for group annotated training data. The main idea of the paper is to learn a smooth nonconvex function for the training data, which can be used to improve the generalization performance of the ERM objective. The authors show that the proposed algorithm outperforms ERM and Group-DRO on a variety of benchmark datasets. "
3093,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"influential features USED-FOR prediction. bivariate methods USED-FOR feature interactions. bivariate methods USED-FOR black - box models. univariate explanation USED-FOR higher - order. feature interactions PART-OF black - box models. univariate explanation USED-FOR explainability. directionality USED-FOR influential features. directional explanations USED-FOR feature interactions. Shapley value explanations USED-FOR bivariate method. IMDB CONJUNCTION Census. Census CONJUNCTION IMDB. CIFAR10 CONJUNCTION IMDB. IMDB CONJUNCTION CIFAR10. method COMPARE state - of - the - art. state - of - the - art COMPARE method. Drug, and gene data EVALUATE-FOR method. Drug, and gene data EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR state - of - the - art. IMDB EVALUATE-FOR method. Census EVALUATE-FOR state - of - the - art. Census EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. CIFAR10 EVALUATE-FOR state - of - the - art. Method are machine learning algorithms, and explanation methods. Generic are they, and graph. OtherScientificTerm are directed graph, and features. ","This paper proposes to use Shapley value explanations (Shapley-VMs) to improve the interpretability of black-box models. The main idea is to use a directed graph to model the interaction between the features in the model. The proposed method is based on Shapley-VAEs, which is an extension of Shapley Value Exploitation (VAE) to the directed graph setting.    The main contribution of the paper is to propose a method to improve explainability of the feature interactions in the black box model by using Shapley VMs. The method is evaluated on three datasets: IMDB, Census, and CIFAR-10. ","This paper proposes a bivariate bivariate explanation method for black-box models. The main idea is to use Shapley value explanations to explain the interactions between features in a directed graph. The proposed method is evaluated on IMDB, Census, and CIFAR-10 datasets. "
3109,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"observed behaviour USED-FOR models of human decision - making. clinical care HYPONYM-OF real - world policies. framework USED-FOR interpretable policy learning. probabilistic tree policies USED-FOR physician actions. Policy Extraction USED-FOR interpretable policy learning. decision Trees ( POETREE ) USED-FOR Policy Extraction. medical history USED-FOR probabilistic tree policies. representation of patient history USED-FOR decision tree policies. complexity USED-FOR modelling task. Fullydifferentiable tree architectures USED-FOR modelling task. recurrence USED-FOR representation of patient history. patient information USED-FOR decision tree policies. policy learning method COMPARE stateof - the - art. stateof - the - art COMPARE policy learning method. policy learning method USED-FOR decision support systems. it USED-FOR decision support systems. real and synthetic medical datasets EVALUATE-FOR stateof - the - art. real and synthetic medical datasets EVALUATE-FOR policy learning method. Method is policy learning approaches. Generic is they. Task are decision - making process, and optimization. ",This paper proposes a method for learning a probabilistic model of medical history using decision trees (POETREE). The main idea is to learn a representation of the history of a patient and then use this representation to train a model of the decision process. The model is trained using a fully-differentiable tree architecture. The proposed method achieves state-of-the-art performance on both real and synthetic medical datasets. ,"This paper proposes a probabilistic tree-based approach to interpretable policy learning for medical decision-making. The proposed method is based on decision trees (POETREE), which is a fully-differentiable tree architecture with a recurrence-based representation of patient history. The authors show that the proposed method outperforms state-of-the-art methods on both real and synthetic medical datasets. "
3125,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"Data augmentation ( DA ) USED-FOR deep learning models. search USED-FOR automated DA methods. image level FEATURE-OF search. joint optimal augmentation policies USED-FOR patches. Patch AutoAugment HYPONYM-OF fine - grained automated DA approach. Stanford Cars CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION Stanford Cars. FGVC - Aircraft CONJUNCTION Pascal VOC 2007. Pascal VOC 2007 CONJUNCTION FGVC - Aircraft. CUB-200 - 2011 CONJUNCTION Stanford Cars. Stanford Cars CONJUNCTION CUB-200 - 2011. image classification CONJUNCTION fine - grained image recognition. fine - grained image recognition CONJUNCTION image classification. ImageNet CONJUNCTION CUB-200 - 2011. CUB-200 - 2011 CONJUNCTION ImageNet. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. fine - grained image recognition CONJUNCTION object detection. object detection CONJUNCTION fine - grained image recognition. CUB-200 - 2011 CONJUNCTION FGVC - Aircraft. FGVC - Aircraft CONJUNCTION CUB-200 - 2011. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-100 HYPONYM-OF object detection. CIFAR-10 HYPONYM-OF object detection. object detection EVALUATE-FOR method. fine - grained image recognition EVALUATE-FOR method. image classification EVALUATE-FOR method. method COMPARE DA methods. DA methods COMPARE method. computational resources EVALUATE-FOR method. computational resources EVALUATE-FOR DA methods. OtherScientificTerm are DA policies, grid of patches, augmentation policy, semantics, and team reward. Task is exploration of diversity in local regions. Generic are it, and agents. ","This paper proposes Patch AutoAugment, a data augmentation method for image classification and object detection. The proposed method is based on joint optimal augmentation policies, where each augmentation policy selects a set of patches to be added to the training set, and a team of augmentation agents is trained to select the best patches from the set. The method is evaluated on Stanford Cars, FGVC-Aircraft, Pascal VOC, CUB-200-2011 and ImageNet. ","This paper proposes a fine-grained automated data augmentation (DA) method, Patch AutoAugment, to improve the diversity of the training data. The proposed method is based on a joint optimal augmentation policy, where each agent is given a set of patches, and the goal is to find the best patch for each of the patches. The agents are trained on a grid of patches in a joint way, where the agent is trained to select the best patches for each patch in the grid. The agent is then trained on the patch, and a reward function is used to reward the agent that finds the best of the patch. The method is evaluated on CUB-200-2011, Stanford Cars, FGVC-Aircraft, Pascal VOC 2007, and CIFAR-10."
3141,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"adversarial vulnerability FEATURE-OF deep neural networks. deep neural networks PART-OF machine learning. machine learning USED-FOR adversarial vulnerability. causality USED-FOR distribution change. causal reasoning USED-FOR distribution change. adversarial attacks FEATURE-OF distribution change. causal formulations USED-FOR intuition of adversarial attacks. causal formulations USED-FOR robust DNNs. causal graph USED-FOR generation process of adversarial examples. adversarial distribution USED-FOR intuition of adversarial attacks. models USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR origin of adversarial vulnerability. spurious correlations USED-FOR adversarial distribution alignment method. causality USED-FOR adversarial vulnerability. OtherScientificTerm are causal perspective, natural and adversarial distribution, and natural and adversarial distributions. Method is causal understanding. Generic is method. ","This paper studies the problem of adversarial vulnerability in deep neural networks. The authors propose to use causal reasoning to understand the distribution change caused by adversarial attacks. They show that adversarial examples are generated from a causal graph, which can be used to identify the source of distribution change. They then propose a causal distribution alignment method to align the natural and adversarial distributions. ","This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs) in the context of causal reasoning. The authors propose a causal formulation of the distribution change of the adversarial examples generated by DNNs, which can be used to identify the source of the vulnerability. They show that the source distribution change is caused by spurious correlations between the natural and adversarial distributions. They also propose an adversarial distribution alignment method that can identify the origin of the spurious correlations.   "
3157,SP:9f09449a47464efb5458d0732df7664865558e6f,"Continual learning USED-FOR catastrophic forgetting of deep neural networks. network layer FEATURE-OF convolutional filters. filter atoms USED-FOR convolutional filters. filter atom swapping USED-FOR continual learning. filter subspace FEATURE-OF convolutional layer. models USED-FOR forgetting. scheme USED-FOR continual learning. atom swapping framework USED-FOR model ensemble. optimization schemes CONJUNCTION convolutional network structures. convolutional network structures CONJUNCTION optimization schemes. method USED-FOR optimization schemes. method USED-FOR convolutional network structures. benchmark datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. accuracy CONJUNCTION scalability. scalability CONJUNCTION accuracy. benchmark datasets EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR state - of - the - art methods. scalability EVALUATE-FOR method. accuracy EVALUATE-FOR method. Method is deep neural networks. OtherScientificTerm are low - rank filter subspace, subspace coefficients, and continual learning settings. ","This paper proposes a method for continual learning of convolutional filters in deep neural networks. The proposed method is based on atom swapping, which swaps the filter atoms in the filter subspace of each layer of the network. The method is evaluated on several benchmark datasets and achieves state-of-the-art accuracy and scalability. ","This paper proposes a method for continual learning of convolutional filters with atom swapping to prevent catastrophic forgetting in continual learning. The method is based on atom swapping, which is an atom swapping framework that swaps the atoms in the filter subspace of each layer of the network. The atom swapping is done by swapping the subspace coefficients of the filter atoms. The authors show that atom swapping can be used to improve the accuracy and scalability of the model ensemble. The proposed method is evaluated on a variety of datasets."
3173,SP:b806dd540708b39c10d3c165ea7d394a02376805,"Stein variational gradient descent ( SVGD ) HYPONYM-OF deterministic inference algorithm. variance collapse FEATURE-OF SVGD. SVGD update COMPARE gradient descent. gradient descent COMPARE SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR SVGD update. maximum mean discrepancy ( MMD ) objective EVALUATE-FOR gradient descent. proportional asymptotic limit FEATURE-OF variance collapse. SVGD USED-FOR variance collapse. SVGD CONJUNCTION MMD - descent. MMD - descent CONJUNCTION SVGD. equilibrium variance USED-FOR SVGD. equilibrium variance USED-FOR MMD - descent. equilibrium variance USED-FOR learning high - dimensional isotropic Gaussians. Task are variance collapse phenomenon, and variance estimation. Method are deterministic updates, and high - dimensional isotropic Gaussians. OtherScientificTerm is near - orthogonality condition. ",This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD) in the presence of a near-orthogonality condition. The authors show that SVGD update is equivalent to gradient descent in terms of the variance of the MMD objective. They also show that the proportional asymptotic limit of variance collapse can be derived for SVGD and MMD-descent.   ,This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVG) in the context of the maximum mean discrepancy (MMD) objective. The authors show that SVGD and MMD-descent converge to the proportional asymptotic limit of variance collapse under a near-orthogonality condition. They also show that the equilibrium variance of SVGD can be used to learn high-dimensional isotropic Gaussians. 
3189,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"Noisy labels ( NL ) CONJUNCTION adversarial examples. adversarial examples CONJUNCTION Noisy labels ( NL ). measure USED-FOR intrinsic geometric property. AT COMPARE NL. NL COMPARE AT. sample selection USED-FOR NL. PGD steps USED-FOR sample selection. AT COMPARE training. training COMPARE AT. NL COMPARE training. training COMPARE NL. AT HYPONYM-OF NL correction. AT USED-FOR NL. smoothing effects FEATURE-OF AT. AT USED-FOR general - purpose robust learning criterion. NL USED-FOR AT. natural accuracy EVALUATE-FOR AT. Generic are models, and they. OtherScientificTerm are projected gradient descent ( PGD ) steps, adversarial example, class boundary, noisy - class boundary, and NL corrections. Metric is robustness. Material is natural data. ","This paper studies the effect of noisy labels (NL) and adversarial examples on the robustness of deep neural networks. The authors propose a new measure, called AT, that measures the distance between the noisy-class boundary and the adversarial example. Theoretical analysis shows that AT can be used as a general-purpose robust learning criterion. Experiments on synthetic and real-world data show that AT improves robustness compared to adversarial training. ","This paper proposes a general-purpose robust learning criterion for adversarial examples and noisy labels (NL). The authors propose a new measure of robustness, called AT, which measures the intrinsic geometric property of the robustness of an adversarial example. They show that AT can be used to improve the performance of adversarial training. The authors also provide a theoretical analysis of AT. "
3205,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"language processing CONJUNCTION protein folding. protein folding CONJUNCTION language processing. classification CONJUNCTION language processing. language processing CONJUNCTION classification. Neural network models USED-FOR tasks. classification HYPONYM-OF tasks. language processing HYPONYM-OF tasks. protein folding HYPONYM-OF tasks. adversarial inputs FEATURE-OF reliability. small input perturbations HYPONYM-OF adversarial inputs. neural networks USED-FOR critical systems. expected robustness EVALUATE-FOR neural network model. statistical method EVALUATE-FOR neural network model. statistical method USED-FOR expected robustness. random input perturbation USED-FOR misclassification. robustness EVALUATE-FOR models. neural network certification USED-FOR safety - critical applications. risk and robustness assessments USED-FOR risk mitigation. risk mitigation USED-FOR neural network certification. categorial basis USED-FOR risk mitigation. categorial basis USED-FOR risk and robustness assessments. Generic are model, method, and approach. OtherScientificTerm is Adversarial inputs. Method are Robustness Measurement and Assessment ( RoMA ), RoMA, verification methods, and classification network. Metric are model ’s robustness, robustness levels, and categorial robustness. ","This paper proposes a method to evaluate the robustness of neural networks in the presence of adversarial perturbations. The main idea is to measure the expected robustness, i.e., the expected risk of misclassification, to a given model. The method is based on a statistical analysis of the probability that a model is robust against a given perturbation. The authors show that the probability of a model misclassifying a given input is proportional to the number of categorical adversarial inputs. They then propose to use this measure of robustness as a risk metric for risk mitigation, and show that it can be used in conjunction with existing methods for risk assessment.","This paper proposes a method to measure the robustness of a neural network model against adversarial inputs. The method is based on a statistical analysis of the expected robustness (e.g., the expected risk of misclassification) of a model. The authors propose a new metric, called Robustness Measurement and Assessment (RoMA), which measures the expected level of robustness for a model against a set of small input perturbations. The proposed method is evaluated on a variety of tasks, including classification, language processing, and protein folding. "
3221,SP:6ba17dd4b31a39478abd995df894447675f2f974,"chunking USED-FOR cognitive science. HCM USED-FOR representations. non - i.i.d sequential data USED-FOR HCM. non - i.i.d sequential data USED-FOR representations. learning guarantees USED-FOR HCM. approaches USED-FOR representation learning. cognitive science CONJUNCTION theories of chunking. theories of chunking CONJUNCTION cognitive science. theories of chunking USED-FOR approaches. OtherScientificTerm are proximity, minimal atomic sequential units, sequential dependence, and partial representational structure. Method are hierarchical chunking model ( HCM ), and hierarchy of chunk representation. ",This paper proposes a hierarchical chunking model (HCM) for learning representations from non-i.i.d sequential data. The authors show that HCM is able to learn representations with minimal atomic sequential units. They show that the learned representations are independent of the number of atoms in the chunk representation. They also provide a learning guarantee for HCM.   ,"This paper proposes a hierarchical chunking model (HCM) for learning representations from non-i.i.d sequential data. The main idea of the paper is to learn a hierarchy of chunk representations, which are represented as a set of atomic sequential units (i.e., subatomic units) with respect to each other. The authors show that this hierarchy can be used to learn representations that are independent of the number of atoms in the subatomic unit. They also show that the representation learned by the hierarchical HCM can be learned by learning a partial representational structure that is independent of atomic units."
3237,SP:625e3908502fd5be949bb915116ed7569ba84298,"gradient flow FEATURE-OF neural reparametrization. graph convolutional network ( GCN ) USED-FOR neural network architecture. GCN USED-FOR aggregation function. gradients of the loss function USED-FOR aggregation function. network synchronization CONJUNCTION persistent homology optimization. persistent homology optimization CONJUNCTION network synchronization. method USED-FOR optimization problems. persistent homology optimization HYPONYM-OF optimization problems. network synchronization HYPONYM-OF optimization problems. OtherScientificTerm are optimization variables, maximum speed up, and Hessian. Method is neural network. Task is optimization. ","This paper studies the gradient flow of neural reparametrization in graph convolutional networks (GCNs). The authors show that the gradients of the loss function can be used to compute the aggregation function of the graph convolutions of a GCN, which is then used to estimate the gradient of the Hessian of the network. The authors then show that this aggregation function is a good approximation of the true loss function, and show that it can be computed in terms of the maximum speed up in gradient flow.  The authors also show that their method can be applied to network synchronization and persistent homology optimization problems.","This paper studies the gradient flow of a graph convolutional network (GCN), which is an extension of the graph neural network (GNN) architecture. In particular, the authors show that the gradients of the loss function of the GCN can be used to estimate the Hessian of the aggregation function of a GNN, which is then used to compute the maximum speed up of the network. The authors also provide a theoretical analysis of the convergence rate of their method. "
3253,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,Deep convolutional neural networks ( DCNNs ) USED-FOR image data. DCNNs USED-FOR supervised learning of image data. pre - labeled images USED-FOR real - world problems. SVMnet USED-FOR non - parametric image classification. SVMnet HYPONYM-OF method. method COMPARE DCNNs. DCNNs COMPARE method. accuracy EVALUATE-FOR DCNNs. SVMs COMPARE neural networks. neural networks COMPARE SVMs. accuracy EVALUATE-FOR method. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. DCNN architectures COMPARE SVMnet. SVMnet COMPARE DCNN architectures. ResNet-50 COMPARE SVMnet. SVMnet COMPARE ResNet-50. accuracy EVALUATE-FOR SVMnet. ResNet-50 HYPONYM-OF DCNN architectures. Material is labeled “ ground truth ” images. OtherScientificTerm is real - world cases. ,"This paper proposes a method for non-parametric image classification based on deep convolutional neural networks (DCNNs). The proposed method, called SVMnet, is based on the observation that SVMs can be used to improve the performance of DCNNs on image classification tasks. The method is evaluated on a variety of image classification datasets, and compared with state-of-the-art DCNN architectures. ","This paper proposes a novel method for non-parametric image classification. The proposed method, called SVMnet, is based on the well-known SVM network architecture. The authors show that the proposed method is able to achieve state-of-the-art performance in terms of accuracy on a variety of image classification tasks. The main contribution of the paper is that the method can be applied to a wide range of tasks, including image classification, image segmentation, and object detection tasks."
3269,SP:a18f4697f350a864866dac871f581b8fc67e8088,"large graphs USED-FOR GNNs. distributed algorithm USED-FOR GNN training. centralized storage CONJUNCTION model learning. model learning CONJUNCTION centralized storage. excessive communication costs CONJUNCTION large memory overheads. large memory overheads CONJUNCTION excessive communication costs. excessive communication costs FEATURE-OF distributed GNN training methods. Learn Locally, Correct Globally ( LLCG ) HYPONYM-OF distributed GNN training technique. local machine USED-FOR GNN. local machine PART-OF LLCG. local data USED-FOR GNN. Global Server Corrections USED-FOR locally learned models. Global Server Corrections USED-FOR server. distributed methods USED-FOR GNNs. periodic model averaging USED-FOR distributed methods. global corrections USED-FOR fast convergence rate. global corrections USED-FOR residual error. real - world datasets EVALUATE-FOR LLCG. efficiency EVALUATE-FOR LLCG. Method are Graph Neural Networks ( GNNs ), and locally trained model. OtherScientificTerm are graph, privacy concern, dependency between nodes, node dependency, and irreducible residual error. Metric are scalability, and communication and memory overhead. ","This paper proposes Learn Locally, Correct Globally (LLC), a distributed GNN training method for large graphs. The main idea is to train a GNN on each node in a distributed manner, where each node is represented as a set of nodes in a graph. Each node in the graph is represented by a local GNN model, which is trained locally on the local data. Then, each server computes a global correction of the local model, and sends it to the other servers, where it is used to update the global model. Theoretical analysis shows that the global corrections are irreducible, which means that they can be used to compute the residual error of the locally trained model. Experiments show that the proposed method outperforms the baselines in terms of accuracy and communication costs.","This paper proposes Learn Locally, Correct Globally (LLCG), a new distributed GNN training method for graph neural networks (GNNs). The main idea of LLCG is to train a GNN on a distributed graph, where each node in the graph is represented by a local machine, and the local machine is responsible for the training of the GNN. The local GNN is trained on the local data, while the global GNNs are trained on global data, where the local model is trained with global server corrections. The authors show that the global model averaging is faster than the local one in terms of the convergence rate. They also show that their method is more efficient than other distributed methods.   "
3285,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"model USED-FOR Anytime inference. image classification PART-OF anytime visual recognition. unified and end - toend model approach USED-FOR anytime pixel - level recognition. depth and spatial resolution FEATURE-OF features. redesigned exit architecture CONJUNCTION spatial adaptivity. spatial adaptivity CONJUNCTION redesigned exit architecture. full model USED-FOR anytime inference. spatial adaptivity USED-FOR anytime inference. spatial adaptivity USED-FOR full model. redesigned exit architecture USED-FOR full model. accuracy EVALUATE-FOR full model. semantic segmentation EVALUATE-FOR approach. approach USED-FOR anytime inference. Cityscapes semantic segmentation CONJUNCTION MPII human pose estimation. MPII human pose estimation CONJUNCTION Cityscapes semantic segmentation. Cityscapes semantic segmentation EVALUATE-FOR approach. MPII human pose estimation EVALUATE-FOR approach. total FLOPs EVALUATE-FOR models. total FLOPs EVALUATE-FOR approach. accuracy - computation curve EVALUATE-FOR method. deep equilibrium networks CONJUNCTION feature - based stochastic sampling approach. feature - based stochastic sampling approach CONJUNCTION deep equilibrium networks. method COMPARE them. them COMPARE method. accuracy - computation curve EVALUATE-FOR them. method COMPARE deep equilibrium networks. deep equilibrium networks COMPARE method. method COMPARE feature - based stochastic sampling approach. feature - based stochastic sampling approach COMPARE method. OtherScientificTerm are exits, and prior predictions. Metric is total computation. Method is spatially adaptive approach. Task is human pose estimation. ",This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. The proposed method is based on a novel exit architecture and spatial adaptivity. The main contribution of this paper is to design an exit architecture that is spatially adaptive to the depth and spatial resolution of features.  The proposed approach is evaluated on semantic segmentation and human pose estimation tasks.  ,This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. The main idea is to design an exit architecture that is spatially adaptive to the depth and spatial resolution of features. The proposed method is evaluated on Cityscapes semantic segmentation and MPII human pose estimation tasks. 
3301,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,Neural Processes USED-FOR stochastic processes. neural networks USED-FOR stochastic processes. Modeling functional uncertainty PART-OF learning stochastic processes. bootstrap method USED-FOR functional uncertainty. Gaussian assumption FEATURE-OF latent variable. bootstrap method USED-FOR Bootstrapping Neural Processes ( B(A)NP ). B(A)NP USED-FOR bootstrapping. ANP CONJUNCTION BANP. BANP CONJUNCTION ANP. NeuBANP USED-FOR bootstrap distribution of random functions. encoder CONJUNCTION loss function. loss function CONJUNCTION encoder. Bayesian optimization CONJUNCTION contextual multi - armed bandit. contextual multi - armed bandit CONJUNCTION Bayesian optimization. Bayesian optimization EVALUATE-FOR models. sequential decision - making tasks EVALUATE-FOR NP methods. NeuBANP COMPARE NP methods. NP methods COMPARE NeuBANP. functional uncertainty modeling EVALUATE-FOR NeuBANP. functional uncertainty modeling EVALUATE-FOR method. sequential decision - making tasks EVALUATE-FOR NeuBANP. Generic is approach. Method is Neural Bootstrapping Attentive Neural Processes ( NeuBANP ). ,"This paper proposes a new method for learning stochastic processes with functional uncertainty. The proposed method is based on the bootstrapping neural process (B(A)NP), which is an extension of Bootstrapping Neural Processes (BANP) that uses the bootstrap distribution of random functions. The authors show that the proposed method outperforms existing methods on Bayesian optimization and contextual multi-armed bandit tasks. ","This paper proposes a neural bootstrapping neural process (B(A)NP) bootstrap method for learning stochastic processes with functional uncertainty. The method is based on a Gaussian assumption on the latent variable of the model. The authors show that the bootstrap distribution of random functions can be used to model the functional uncertainty in the model, which is then used to bootstrap the neural network. The proposed method is evaluated on a variety of tasks, including Bayesian optimization, contextual multi-armed bandit, and sequential decision making tasks."
3317,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory genome modeling USED-FOR regulatory downstream tasks. regulatory genome modeling PART-OF genome biology research. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. deep learning methods USED-FOR genome sequences. approach USED-FOR pre - training genome data. genome data USED-FOR multi - modal and self - supervised manner. multi - modal and self - supervised manner USED-FOR pre - training genome data. robustness EVALUATE-FOR model. pre - training tasks USED-FOR model. pre - training tasks USED-FOR robustness. genome sequences USED-FOR ATAC - seq dataset. ATAC - seq dataset EVALUATE-FOR model. transaction factor binding sites prediction CONJUNCTION disease risk estimation. disease risk estimation CONJUNCTION transaction factor binding sites prediction. disease risk estimation CONJUNCTION splicing sites prediction. splicing sites prediction CONJUNCTION disease risk estimation. promoter classification CONJUNCTION transaction factor binding sites prediction. transaction factor binding sites prediction CONJUNCTION promoter classification. regulatory downstream tasks EVALUATE-FOR GeneBERT. splicing sites prediction HYPONYM-OF regulatory downstream tasks. promoter classification HYPONYM-OF regulatory downstream tasks. transaction factor binding sites prediction HYPONYM-OF regulatory downstream tasks. disease risk estimation HYPONYM-OF regulatory downstream tasks. OtherScientificTerm is regulatory elements. Generic is them. Task is biological applications. Material are 1d sequence of genome data, and large - scale regulatory genomics data. ","This paper proposes GeneBERT, a method for pre-training on large-scale regulatory genomics data. The method is based on a self-supervised learning approach, where a pre-trained model is trained on a set of sequences from the ATAC-seq dataset. The proposed method is evaluated on three downstream tasks: (1) promoter classification, (2) transaction factor binding sites prediction, and (3) disease risk estimation. The results show that the proposed method achieves state-of-the-art performance. ","This paper proposes GeneBERT, a method for pre-training a pre-trained model for regulatory genome modeling (regulatory downstream tasks) on the ATAC-seq dataset. The proposed method is based on a multi-modal and self-supervised manner, where the model is trained on a set of pre-trainable datasets. The model is evaluated on a variety of downstream tasks (promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction) and compared with state-of-the-art baselines. "
3333,SP:841b12443d0274e34b78940f220b17d36798899b,"method USED-FOR detecting OOD samples. IGEOOD USED-FOR detecting OOD samples. IGEOOD HYPONYM-OF method. IGEOOD USED-FOR pre - trained neural network. geodesic ( FisherRao ) distance USED-FOR discriminator. confidence scores CONJUNCTION features. features CONJUNCTION confidence scores. features PART-OF deep neural network. confidence scores USED-FOR discriminator. logits outputs USED-FOR confidence scores. features USED-FOR discriminator. deep neural network USED-FOR discriminator. IGEOOD COMPARE state - of - the - art methods. state - of - the - art methods COMPARE IGEOOD. Method are machine learning ( ML ) systems, and ML model. OtherScientificTerm are OOD samples, and data distributions. Material is OOD data. ",This paper proposes a method for detecting out-of-distribution (OOD) samples in machine learning systems. The proposed method is based on using a discriminator trained on a pre-trained neural network with a geodesic (FisherRao) distance. The discriminator is trained by maximizing the distance between the confidence scores and features of the input data distribution. The method is evaluated on synthetic and real-world datasets. ,"This paper proposes a new method for detecting out-of-distribution (OOD) samples in machine learning models. The proposed method, IGEOOD, uses a geodesic distance (FisherRao) distance between the discriminator and the data distribution. The discriminator is trained using a pre-trained neural network, and is trained with confidence scores and features. The authors show that the proposed method outperforms the state of the art methods in terms of OOD detection."
3349,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"translations CONJUNCTION rotations. rotations CONJUNCTION translations. group FEATURE-OF identity - preserving transformations. identity - preserving transformations USED-FOR representations of objects. translations HYPONYM-OF group. translations HYPONYM-OF identity - preserving transformations. group equivariance USED-FOR representation. Cover ’s Function Counting Theorem USED-FOR linearly separable and group - invariant binary dichotomies. linearly separable and group - invariant binary dichotomies USED-FOR equivariant representations of objects. element - wise nonlinearities CONJUNCTION global and local pooling. global and local pooling CONJUNCTION element - wise nonlinearities. convolutions CONJUNCTION element - wise nonlinearities. element - wise nonlinearities CONJUNCTION convolutions. relation USED-FOR operations. global and local pooling HYPONYM-OF operations. convolutions HYPONYM-OF operations. element - wise nonlinearities HYPONYM-OF operations. OtherScientificTerm are Equivariance, separable dichotomies, group action, and local pooling. Generic is theory. ",This paper studies the equivariance of representations of objects in a group under identity-preserving transformations. The authors show that linearly separable and group-invariant binary dichotomies can be used to represent equivariant representations. They also show that global pooling and nonlinearity are non-equivariant operations.   ,"This paper studies the equivariance of group-invariant representations of identity-preserving transformations, i.e., translations, rotations, and convolutions. The authors prove the Cover’s Function Counting Theorem for linearly separable and group- invariant binary dichotomies. They show that group equivariant representations can be represented as linearly separated and group invariant representations. They also provide a relation between group action, global and local pooling, and element-wise nonlinearities. "
3365,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"models USED-FOR machine learning objectives. concept drift CONJUNCTION mitigating biases. mitigating biases CONJUNCTION concept drift. robustness HYPONYM-OF machine learning objectives. mitigating biases HYPONYM-OF machine learning objectives. concept drift HYPONYM-OF machine learning objectives. counterfactual explanations CONJUNCTION concept activation vectors. concept activation vectors CONJUNCTION counterfactual explanations. it USED-FOR models. pretrained models USED-FOR approach. prior ideas USED-FOR CCE. counterfactual explanations HYPONYM-OF prior ideas. concept activation vectors HYPONYM-OF prior ideas. CCE USED-FOR spurious correlation. spurious correlations USED-FOR models. CCE USED-FOR models. data USED-FOR models. medical applications EVALUATE-FOR CCE. Generic are model, and systematic approach. Method are conceptual counterfactual explanations ( CCE ), and classifier. OtherScientificTerm are human - understandable concepts, faint stripes, and model mistakes. ","This paper proposes a method for learning counterfactual explanations (CCEs) for concept drift and mitigating biases in machine learning models. The idea is to use the concept drift objective as a regularization term to train a classifier that is robust to concept drift, mitigating biases, and spurious correlations. The authors show that CCE can be used to train models that are robust to concepts drift and mitigate mitigating biases. They also show that the proposed method is able to recover spurious correlations in the training data. ",This paper proposes a systematic approach to counterfactual explanations (CCE) for the problem of concept drift and mitigating biases in machine learning. The authors propose to use the concept drift objective as well as the mitigating bias objective to train a classifier that is robust to spurious correlations between concepts. They show that CCE can be used to improve the robustness of the classifier against spurious correlation. They also show that the proposed approach can be applied to medical data. 
3381,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"3D point cloud applications EVALUATE-FOR Kernel Point Convolution ( KPConv ). KPConv network USED-FOR mobile scenarios. KPConv USED-FOR neighbor - kernel correlation. Euclidean distance USED-FOR neighbor - kernel correlation. module USED-FOR KPConv. efficiency EVALUATE-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) USED-FOR KPConv. Mobile Attention Kernel Point Convolution ( MAKPConv ) HYPONYM-OF module. efficiency EVALUATE-FOR module. depthwise kernel USED-FOR resource consumption. depthwise kernel USED-FOR MAKPConv. Inverted Residual Bottleneck ( IRB ) USED-FOR design space. MAKPConv USED-FOR 3D networks. Wide & Deep Predictor USED-FOR dense and sparse neural architecture representations. carrying feature engineering USED-FOR neural architecture representations. Wide & Deep Predictor USED-FOR error in performance prediction. searchable features USED-FOR carrying feature engineering. predictor USED-FOR design space. 3D point cloud classification and segmentation benchmarks EVALUATE-FOR NAS - crafted MAKPConv network. NAScrafted model SPVNAS COMPARE NAS - crafted MAKPConv network. NAS - crafted MAKPConv network COMPARE NAScrafted model SPVNAS. Multiply - Accumulates EVALUATE-FOR NAS - crafted MAKPConv network. mIOU EVALUATE-FOR NAS - crafted MAKPConv network. OtherScientificTerm are kernel relationship, and weak representation power. Method is Neighbor - Kernel attention. Metric is representation power. ","This paper proposes a novel kernel point convolution (KPKConv) method for 3D point cloud classification and segmentation. The proposed method is based on neighbor-kernel attention (NKA), which is an efficient way to compute the kernel distance between two points in a point cloud. The authors also propose a novel depthwise kernel to reduce the computational cost of the proposed method. The experimental results show that the proposed methods outperform the state-of-the-art methods.","This paper proposes a new module for Kernel Point Convolution (KPConv) for 3D point cloud applications. The proposed module is based on the idea of neighbor-kernels, which is an extension of the neighbor-kernel attention (NK attention) module. The main idea is to use the Inverted Residual Bottleneck (IRB) to improve the efficiency of KPConv. The paper also proposes a wide-and-deep predictor (WDP) to predict the prediction error in performance prediction for dense and sparse neural architecture representations. The experimental results show that the proposed module outperforms the state-of-the-art NAS-crafted model SPVNAS."
3397,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. robustness overestimation CONJUNCTION robustness - accuracy trade - off. robustness - accuracy trade - off CONJUNCTION robustness overestimation. problems PART-OF adversarial training. robustness - accuracy trade - off HYPONYM-OF problems. robust overfitting HYPONYM-OF problems. robustness overestimation HYPONYM-OF problems. lowquality samples PART-OF dataset. strategy USED-FOR data quality. learning behaviors USED-FOR strategy. learning behaviors USED-FOR data quality. data quality CONJUNCTION problems. problems CONJUNCTION data quality. problems FEATURE-OF adversarial training. data quality CONJUNCTION adversarial training. adversarial training CONJUNCTION data quality. robust overfitting CONJUNCTION robustness overestimation. robustness overestimation CONJUNCTION robust overfitting. Material is low - quality data. Metric is adversarial robustness. ,This paper studies the trade-off between robustness overestimation and robustness underestimation in adversarial training. The authors show that robust overfitting and robust overestimation are two types of problems that arise when training with low-quality data. They show that the robustness-accuracy tradeoff is a tradeoff between the quality of the data and the accuracy of the trained model. They then propose a strategy to mitigate this tradeoff and show that this strategy can be used in conjunction with other learning behaviors to improve data quality. ,"This paper studies the trade-off between robust overfitting and robust overestimation in adversarial training. In particular, the authors consider the problem of adversarial overfitting, which is defined as a tradeoff between the robustness of the training data and the accuracy of the test data. They show that under some conditions, the tradeoff can be reduced to a trade-offs between robustness overestimation and robustness-accuracy tradeoff. They then propose a new strategy for improving the robust accuracy of training data, which they call the ""data quality"" strategy.  "
3413,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"neural network USED-FOR multivariate functions of bounded second mixed derivatives. Korobov functions HYPONYM-OF multivariate functions of bounded second mixed derivatives. upper bounds USED-FOR shallow and deep neural networks. quantities USED-FOR shallow and deep neural networks. upper bounds USED-FOR quantities. bounds FEATURE-OF activation functions. ReLU HYPONYM-OF activation functions. continuous function approximator USED-FOR Korobov functions. neural networks HYPONYM-OF near - optimal function approximators. OtherScientificTerm are training parameters, and curse of dimensionality. ","This paper studies the problem of approximating multivariate functions of bounded second mixed derivatives with bounded second derivatives. The authors consider the case where the training parameters of a neural network are bounded second and bounded second. They show that for a certain class of function approximators, there exists a near-optimal approximator that is close to the optimal function approximated by neural networks. They also show that this is the case for ReLU activation functions.   ",This paper studies the problem of learning a function approximator for Korobov functions of bounded second mixed derivatives. The main contribution of the paper is to provide upper bounds on the quantities of the training parameters of shallow and deep neural networks. The upper bounds are based on the following assumptions:   1. The training parameters are bounded second-order mixed derivatives; 2. There is a continuous function approximating the function. 3. The function is bounded in terms of the dimensionality of the data. 4. The parameters of the neural network are bounded by the dimension of the input data.  The paper also provides upper bounds for the ReLU activation function. 
3429,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"Populations USED-FOR language. agent population size FEATURE-OF speaker - listener Lewis Game. agent population size USED-FOR emergent language properties. training speed CONJUNCTION network capacity. network capacity CONJUNCTION training speed. speaker - listener asymmetry USED-FOR language structure. network capacity HYPONYM-OF diversity factors. training speed HYPONYM-OF diversity factors. relative difference of factors USED-FOR emergent language properties. Material are sociolinguistic literature, and structured languages. Method is neural agents. OtherScientificTerm are agent community, population heterogeneity, confounding factors, training speed heterogeneities, and simulated communities. ","This paper studies the effect of training speed and network capacity on the speaker-listener Lewis game. The authors show that training speed, network capacity, and diversity factors are important factors in the evolution of language. They also show that the relative difference of factors can influence the emergent language properties of the population. ","This paper studies the effect of diversity in the speaker-listener Lewis Game in terms of training speed and network capacity on the emergent language properties. The authors show that training speed, network capacity, and the number of agents in a community can affect the language properties of the speaker and listener. They also show that diversity in training speed is a factor in the diversity of language properties, and that the diversity can be controlled by the training speed of the agent population.   "
3445,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"Graph Neural Networks ( GNNs ) USED-FOR node classification task. node features CONJUNCTION graph topology. graph topology CONJUNCTION node features. node features USED-FOR Graph Neural Networks ( GNNs ). heterophilic graphs EVALUATE-FOR models. homophily FEATURE-OF GNNs. polynomial graph filters USED-FOR models. models USED-FOR polynomials. spectrum FEATURE-OF adaptive polynomial filters. model USED-FOR filter. classification accuracy EVALUATE-FOR model. node classification task EVALUATE-FOR polynomials. model COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE model. model COMPARE models. models COMPARE model. models COMPARE polynomial filter - based approaches. polynomial filter - based approaches COMPARE models. OtherScientificTerm are connected nodes, overdetermined system of equations, and eigencomponents. Method are polynomial graph filter models, eigendecomposition of the graph, and latent polynomial filters. Material is anonymized code. ","This paper proposes to use polynomial graph filters to improve the performance of GNNs on heterophilic graphs. The proposed method is based on the observation that the eigendecomposition of the graph can be expressed as a system of equations, which can be represented as a set of polynomials. The authors propose to model the eigenfunctions of the system as an over-determined system, which is then represented by a linear combination of the polynemic filters. The paper shows that the proposed method outperforms existing GNN-based methods on the node classification task. ","This paper proposes a method for learning polynomial graph filter models for heterophilic graphs. The authors propose a model that learns a spectrum of adaptive polynomials for each node in the graph. The spectrum consists of a set of eigencomponents of the eigendecomposition of the graph, and the model learns the eigenvalue of each eigen component. The proposed method is evaluated on a variety of graph classification tasks, and it is shown that the proposed method outperforms the state of the art."
3461,SP:903545b1b340ec5c13070e0f25f550c444de4124,biomedical structure prediction CONJUNCTION social relationship analysis. social relationship analysis CONJUNCTION biomedical structure prediction. graphs FEATURE-OF Shortest Distance Queries ( SDQs ). Shortest Distance Queries ( SDQs ) HYPONYM-OF network analysis. social relationship analysis HYPONYM-OF network analysis. biomedical structure prediction HYPONYM-OF network analysis. Approximate algorithms of SDQs USED-FOR complex graph applications. reduced complexity FEATURE-OF Approximate algorithms of SDQs. approaches USED-FOR embedding - based distance prediction. accuracy EVALUATE-FOR embedding - based distance prediction. efficiency EVALUATE-FOR embedding - based distance prediction. truncated random walk CONJUNCTION Pointwise Mutual Information ( PMI)-based optimization. Pointwise Mutual Information ( PMI)-based optimization CONJUNCTION truncated random walk. predictor USED-FOR global extraction of nodes ’ mutual shortest distance. Pointwise Mutual Information ( PMI)-based optimization USED-FOR Embedding - based distance prediction. truncated random walk USED-FOR Embedding - based distance prediction. Random walk HYPONYM-OF unstrained node sequence. limited distance exploration FEATURE-OF Random walk. graph domain FEATURE-OF intrinsic metric. distance range FEATURE-OF Betweenness Centrality(BC)-based random walk. intrinsic metric USED-FOR distance range. intrinsic metric EVALUATE-FOR Betweenness Centrality(BC)-based random walk. steering optimization objective USED-FOR global distance matrix. strategy USED-FOR global distance matrix. maximum likelihood optimization COMPARE PMI - based optimization. PMI - based optimization COMPARE maximum likelihood optimization. strategy USED-FOR distance relation. Distance Resampling ( DR ) COMPARE PMI - based optimization. PMI - based optimization COMPARE Distance Resampling ( DR ). maximum likelihood optimization USED-FOR Distance Resampling ( DR ). walk paths USED-FOR Distance Resampling ( DR ). steering optimization objective USED-FOR strategy. steering optimization objective USED-FOR distance relation. method COMPARE methods. methods COMPARE method. realworld graph datasets USED-FOR SDQ problems. SDQ problems EVALUATE-FOR method. SDQ problems EVALUATE-FOR methods. realworld graph datasets EVALUATE-FOR method. realworld graph datasets,This paper proposes a novel algorithm for embedding-based distance prediction in shortest distance queries (SDQs) on graphs. The proposed algorithm is based on truncated random walk and Pointwise Mutual Information (PMI)-based optimization. The main idea is to use a predictor to predict the mutual shortest distance between two nodes in the graph. The distance between the predictor and the nodes is estimated using a random walk. The prediction is then used to estimate the distance between a pair of nodes and the predictor is used to compute the global distance matrix between the two nodes.   The proposed method is evaluated on two real-world graph datasets and compared with existing methods.,This paper proposes a new algorithm for embedding-based distance prediction for shortest distance query (SDQ) problems. The proposed algorithm is based on Pointwise Mutual Information (PMI) and truncated random walk. The main idea is to use a global predictor to extract the mutual shortest distance between nodes’ mutual shortest distances. The paper also proposes a steering optimization objective to optimize the global distance matrix. Empirical results show that the proposed method outperforms the state-of-the-art in terms of accuracy and efficiency.
3477,SP:13db440061fed785f05bb41d0767225403ecf7a1,"fact - checking CONJUNCTION open dialogue. open dialogue CONJUNCTION fact - checking. question answering CONJUNCTION fact - checking. fact - checking CONJUNCTION question answering. web corpus USED-FOR knowledge - dependent downstream tasks. Large Language Models ( LMs ) USED-FOR world knowledge. open dialogue HYPONYM-OF knowledge - dependent downstream tasks. question answering HYPONYM-OF knowledge - dependent downstream tasks. fact - checking HYPONYM-OF knowledge - dependent downstream tasks. world knowledge PART-OF LMs. Continual Knowledge Learning ( CKL ) HYPONYM-OF continual learning ( CL ) problem. retention of time - invariant world knowledge CONJUNCTION update of outdated knowledge. update of outdated knowledge CONJUNCTION retention of time - invariant world knowledge. update of outdated knowledge CONJUNCTION acquisition of new knowledge. acquisition of new knowledge CONJUNCTION update of outdated knowledge. metric USED-FOR retention of time - invariant world knowledge. metric USED-FOR acquisition of new knowledge. metric USED-FOR update of outdated knowledge. benchmark USED-FOR retention of time - invariant world knowledge. benchmark CONJUNCTION metric. metric CONJUNCTION benchmark. CKL USED-FOR ever - changing LMs1. OtherScientificTerm are real - world scenarios, catastrophic forgetting, invariant knowledge, and knowledge forgetting. Task is maintenance of ever - changing LMs. Generic are baselines, and CL setups. Method is parameter expansion. ","This paper proposes a continual knowledge learning (CKL) framework to address the catastrophic forgetting problem in large language models (LMs), where the goal is to maintain invariant knowledge in knowledge-dependent downstream tasks such as fact checking, open dialogue, and question answering. The proposed method is based on continual learning (CL) with parameter expansion, where the model is trained to update the parameters of the LMs in a continual manner. The authors show that the proposed method outperforms existing methods in terms of time-invariant knowledge retention and update of outdated knowledge, as well as acquisition of new knowledge. They also propose a benchmark to evaluate the performance of the method. ","This paper proposes a new continual learning (CL) problem, called Continual Knowledge Learning (CKL), where the goal is to maintain the invariant knowledge of a large language model (LMs) over time. The authors propose a new metric to measure the retention of time-invariant world knowledge and the acquisition of new knowledge. They show that CKL can be applied to a variety of downstream tasks, such as fact checking, open dialogue, and question answering. They also provide a new benchmark for CKL, which is based on the knowledge-dependent downstream tasks. "
3493,SP:639fd88482330389019fb5be7446a909b99a8609,"approach USED-FOR supervised learning task. Decision trees USED-FOR supervised learning task. Decision trees USED-FOR applications. medical imaging CONJUNCTION computer vision. computer vision CONJUNCTION medical imaging. computer vision HYPONYM-OF applications. medical imaging HYPONYM-OF applications. feature CONJUNCTION threshold. threshold CONJUNCTION feature. exhaustive search algorithm USED-FOR criterion minimization problem. stochastic approach USED-FOR criterion minimization. algorithm COMPARE exhaustive search. exhaustive search COMPARE algorithm. algorithm COMPARE decision tree learning methods. decision tree learning methods COMPARE algorithm. algorithm COMPARE baseline non - stochastic approach. baseline non - stochastic approach COMPARE algorithm. baseline non - stochastic approach HYPONYM-OF decision tree learning methods. algorithm COMPARE baseline algorithm. baseline algorithm COMPARE algorithm. accuracy CONJUNCTION computational cost. computational cost CONJUNCTION accuracy. computational cost EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. computational cost EVALUATE-FOR baseline algorithm. accuracy EVALUATE-FOR baseline algorithm. algorithm USED-FOR Haar tree. MNIST dataset FEATURE-OF Haar tree. tree COMPARE axis - aligned tree. axis - aligned tree COMPARE tree. MNIST COMPARE axis - aligned tree. axis - aligned tree COMPARE MNIST. MNIST EVALUATE-FOR tree. test accuracy EVALUATE-FOR tree. OtherScientificTerm are leaf nodes, stopping criterion, node, features, and criterion. Method is oblique trees. Metric is training times. ",This paper proposes a novel algorithm for learning decision trees with stochasticity. The main idea is to use an exhaustive search algorithm to minimize the criterion minimization problem. Theoretical analysis is provided to show the convergence of the proposed algorithm. Empirical results on MNIST and CIFAR-10/100 datasets show that the proposed method outperforms existing methods in terms of accuracy and computational cost. ,This paper proposes a new algorithm for the criterion minimization problem in decision tree learning. The main idea of the paper is to use a stochastic approach to minimise criterion minimisation problem. The authors propose an exhaustive search algorithm to find the best criterion minimizer. They show that their algorithm outperforms the baseline non-stochastic approach in terms of accuracy and computational cost. 
3509,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,Learning rate schedulers USED-FOR deep neural networks. SGD USED-FOR problems. optimizing quadratic objectives HYPONYM-OF problems. eigenvalue distribution FEATURE-OF Hessian matrix. Eigencurve HYPONYM-OF learning rate schedules. SGD USED-FOR quadratic objectives. Eigencurve USED-FOR SGD. learning rate schedules USED-FOR SGD. minimax optimal convergence rates EVALUATE-FOR learning rate schedules. Eigencurve COMPARE step decay. step decay COMPARE Eigencurve. step decay USED-FOR image classification tasks. image classification tasks EVALUATE-FOR Eigencurve. CIFAR-10 USED-FOR image classification tasks. learning rate schedulers USED-FOR eigencurve. theory USED-FOR learning rate schedulers. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. schedulers USED-FOR optimal shape. cosine decay USED-FOR optimal shape. schedulers COMPARE cosine decay. cosine decay COMPARE schedulers. ,This paper studies learning rate schedulers for SGD for quadratic objectives. The authors show that the eigenvalue distribution of the Hessian matrix can be approximated by the learning rate schedule of SGD. They show that learning rate schedules with eigencurve learning rates have minimax optimal convergence rates. They also show that eigenschedulers are better than cosine decay in terms of convergence rate. ,"This paper studies the problem of learning rate schedulers for optimizing quadratic objectives. In particular, the authors study the eigenvalue distribution of the Hessian matrix of SGD. They show that learning rate schedules with eigencurve can converge to the minimax optimal convergence rate. They also show that the cosine decay of the learning rate schedule can be used to find the optimal shape. "
3525,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"Offline reinforcement learning USED-FOR control policies. total variation distance FEATURE-OF model. imaginary rollout horizon HYPONYM-OF hyperparameters. Bayesian Optimization USED-FOR hyperparameters. Material is online data collection. Method are offline model - based reinforcement learning, dynamics model, probabilistic model, and uncertainty heuristics. OtherScientificTerm are model uncertainty, pessimistic MDP, MDP, pessimistic return, and estimated model uncertainty. Generic are Existing methods, heuristics, and protocols. ","This paper studies offline model-based reinforcement learning with uncertainty estimation. The authors propose to use Bayesian Optimization to estimate the total variation distance between the model and the MDP. They show that Bayesian optimisation can be used to estimate model uncertainty, which is then used to improve the performance of offline RL algorithms. They also show that the uncertainty heuristic can be applied to the pessimistic MDP, where the pessimistic return is estimated from the estimated model uncertainty. ","This paper proposes a new method for offline model-based reinforcement learning. The main idea is to use a probabilistic model to estimate the total variation distance between the model and the environment, and then use Bayesian Optimization to optimize the hyperparameters of the model. The authors show that their method outperforms the state-of-the-art heuristics in terms of the pessimistic MDP and pessimistic return."
3541,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"methods USED-FOR sampling. critic networks USED-FOR policy learning. TD - error HYPONYM-OF features. TD - error USED-FOR sampled experiences. sampled experiences USED-FOR Q - values. auxiliary features USED-FOR TD - error. auxiliary features USED-FOR sampling. learnable features USED-FOR experience replay method. curriculum learning USED-FOR predicting Q - values. MaPER USED-FOR curriculum learning. MaPER COMPARE vanilla PER. vanilla PER COMPARE MaPER. computational overhead COMPARE vanilla PER. vanilla PER COMPARE computational overhead. critic network USED-FOR curriculum learning. critic network USED-FOR predicting Q - values. tasks EVALUATE-FOR MaPER. offpolicy MfRL CONJUNCTION MbRL. MbRL CONJUNCTION offpolicy MfRL. MaPER USED-FOR MbRL. off - policy MfRL algorithms USED-FOR policy optimization procedure. MaPER USED-FOR offpolicy MfRL. off - policy MfRL algorithms PART-OF MbRL. Method are Experience replay, and model - based RL ( MbRL ). ","This paper proposes an experience replay method for model-based reinforcement learning. The main idea is to learn a set of learnable features from experience replay, and then use them to sample from the set of Q-values from the learned features. The proposed method is based on the idea of using TD-error as a regularization term in experience replay. The authors show that the proposed method outperforms baselines on a variety of tasks. ","This paper proposes a new experience replay method for curriculum learning in model-based RL (MfRL). The main idea is to use a critic network to predict Q-values from a set of sampled experiences, and then use the sampled experiences as auxiliary features to learn the TD-error of the critic network. The proposed method, MaPER, is evaluated on a variety of off-policy MfRL tasks, and it is shown to outperform the state-of-the-art."
3557,SP:0db83e057c21ac10fe91624876498d8456797492,"fast learning CONJUNCTION training safety. training safety CONJUNCTION fast learning. human knowledge PART-OF reinforcement learning. Human intervention USED-FOR human knowledge. trial - and - error exploration CONJUNCTION human ’s partial demonstration. human ’s partial demonstration CONJUNCTION trial - and - error exploration. human ’s partial demonstration USED-FOR HACO. agent USED-FOR proxy values. HACO USED-FOR agent. HACO USED-FOR proxy state - action values. environmental reward USED-FOR HACO. safe driving benchmark EVALUATE-FOR sample efficiency. sample efficiency EVALUATE-FOR HACO. safe driving benchmark EVALUATE-FOR HACO. reinforcement learning CONJUNCTION imitation learning baselines. imitation learning baselines CONJUNCTION reinforcement learning. Method are learning agent, and Human - AI Copilot Optimization ( HACO ). OtherScientificTerm are human expert, trivial behaviors, human interventions, and human intervention budget. Generic is It. ","This paper proposes to use human intervention to improve the sample efficiency of reinforcement learning agents in the presence of human intervention. The idea is to use proxy state-action values from a human expert as a proxy for the state value of the agent, and then use the proxy state value as a reward to encourage the agent to learn from the proxy values. The proposed method is evaluated on a simulated driving task, where it is shown to outperform baselines in terms of sample efficiency.  ","This paper proposes Human-AI Copilot Optimization (HACO), a method for improving the sample efficiency of reinforcement learning with human intervention. HACO is motivated by the observation that human intervention can be used as a proxy for human knowledge in reinforcement learning. The authors propose to use the human expert as the proxy for the proxy state-action values, and then use the proxy values to improve the agent’s sample efficiency. The proposed method is evaluated on a safe driving benchmark, where it outperforms the state-of-the-art. "
3573,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"long - horizon unsegmented demonstrations USED-FOR subskills. hierarchical structure USED-FOR Transferring and reorganizing modular sub - skills. Dual Meta Imitation Learning ( DMIL ) HYPONYM-OF hierarchical meta imitation learning method. highlevel network USED-FOR sub - skill adaptation. likelihood of state - action pairs USED-FOR supervision. likelihood of state - action pairs USED-FOR sub - skill. high - level network adaptation USED-FOR DMIL. highlevel network USED-FOR DMIL. likelihood of state - action pairs USED-FOR DMIL. DMIL CONJUNCTION Expectation - Maximization algorithm. Expectation - Maximization algorithm CONJUNCTION DMIL. iterative training process USED-FOR DMIL. Kitchen environment EVALUATE-FOR few - shot imitation learning. Method are Hierarchical Imitation learning ( HIL ), and model - agnostic meta - learning. OtherScientificTerm is high - level network. ","This paper proposes Dual Meta Imitation Learning (DMIL), a meta-learning method for few-shot imitation learning in the presence of long-horizon unsegmented demonstrations. The main idea is to use a high-level network to learn a set of modular sub-skills, which are then used to train a low-level model for each sub-skill. This is done by maximizing the likelihood of state-action pairs in the low-dimensional space, which is used as a priori for supervision. The authors show that the proposed method outperforms baselines on a kitchen environment. ","This paper proposes a hierarchical meta-imitation learning method, Dual Meta Imitation Learning (DMIL), for few-shot imitation learning. The main idea of DMIL is to learn a high-level network that adapts to each sub-skills in a hierarchical fashion, and then use the high level network to learn the likelihood of state-action pairs between sub-kills. The high level networks are trained using an Expectation-Maximization (EM) algorithm. Experiments show that the proposed method outperforms the state-of-the-art in a kitchen environment."
3589,SP:fb0efa670729796471a7a562b231172103bb8749,Graph neural networks ( GNNs ) HYPONYM-OF deep learning models. deep learning models USED-FOR graph data. node features USED-FOR they. graph without node feature USED-FOR networks. number of degrees HYPONYM-OF graph - based node features. embeddings HYPONYM-OF input node representation. approach USED-FOR node embeddings. node embeddings CONJUNCTION GNNs. GNNs CONJUNCTION node embeddings. graphics processing unit ( GPU ) memory FEATURE-OF GNNs. embedding compression methods USED-FOR natural language processing ( NLP ) models. bit vector COMPARE float - point vector. float - point vector COMPARE bit vector. bit vector USED-FOR node. parameters USED-FOR compression method. GNNs USED-FOR parameters. node embedding compression method COMPARE alternatives. alternatives COMPARE node embedding compression method. Generic is network. Material is industrial scale graph data. ,This paper proposes a node embedding compression method for graph neural networks (GNNs) that uses node embeddings and GNNs to compress the input node representation. The proposed method is based on the observation that GNN can compress the embedding of node features in a graph without node features. The authors propose to use a bit vector to encode the node features instead of a float-point vector. They show that the bit vector is more efficient than the float vector in terms of computation time and memory usage. They also show that their method can be applied to graph neural network architectures and achieves better compression performance compared to the baselines. ,"This paper proposes a new node embedding compression method for graph neural networks (GNNs). The key idea is to use a bit vector to compress the embeddings of GNNs, instead of using a float-point vector. The authors show that the bit vector is more efficient than the float-points vector in terms of memory consumption. They also show that their method can be applied to a variety of graph-based NLP models, such as GANs."
3605,SP:15c243829ed3b2505ed1e122bd499089f8a862da,domain adaptation USED-FOR learning invariant representations. domain - adversarial training USED-FOR learning invariant representations. asymptotic convergence guarantees FEATURE-OF optimizer. gradient descent USED-FOR domain - adversarial training. optimal solutions PART-OF domain - adversarial training. local Nash equilibria USED-FOR optimal solutions. gradient descent CONJUNCTION high - order ODE solvers. high - order ODE solvers CONJUNCTION gradient descent. Runge – Kutta HYPONYM-OF high - order ODE solvers. optimizers COMPARE optimizers. optimizers COMPARE optimizers. drop - in replacement COMPARE optimizers. optimizers COMPARE drop - in replacement. optimizers USED-FOR drop - in replacement. learning rates FEATURE-OF optimizers. optimizers PART-OF domain - adversarial framework. Generic is approach. Method is domain - adversarial methods. ,This paper studies the problem of learning invariant representations in the domain adaptation setting. The authors propose to use gradient descent and high-order ODE solvers to solve the optimization problem in domain adversarial training. The main idea is to use local Nash equilibria to find the optimal solutions of the problem and then apply gradient descent to solve it.   The authors show that the proposed method converges to the optimal solution in a time-varying manner. They also provide asymptotic convergence guarantees for their method. ,"This paper studies the problem of learning invariant representations in domain-adversarial training. In particular, the authors consider the setting where the goal is to learn a representation that is invariant to adversarial attacks. The authors propose a new method for learning the representation, which they call Runge-Kutta, which is a high-order ODE solver. They show that it is possible to learn the representation by gradient descent and a drop-in replacement method. They also show that the proposed method can be used in conjunction with high- order ODEs.   "
3621,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"regularization methods USED-FOR machine learning models. loss function USED-FOR Flooding. individual Flood HYPONYM-OF regularizer. iFlood USED-FOR models. instance - level constraints FEATURE-OF training loss. instance - level constraints USED-FOR iFlood. it USED-FOR applications. it USED-FOR models. iFlood COMPARE regularizers. regularizers COMPARE iFlood. image classification and language understanding tasks EVALUATE-FOR models. iFlood USED-FOR models. Generic is one. OtherScientificTerm are under - fitted instances, inductive biases, and instance - level. Metric is generalization ability. ","This paper proposes a new regularization method called Individual Flooding (iFlood), which aims to improve the generalization performance of deep neural networks. The method is based on the idea of individual Flooding, which is an instance-level regularization that is applied to each instance in the training set. The authors show that iFlood can be used as a regularizer on image classification and language understanding tasks.   ","This paper proposes a new regularization method called iFlood, which aims to improve the generalization ability of machine learning models. The method is based on the idea of individual Flooding, where each individual model is trained with an instance-level regularizer. The idea is that the individual models are trained with the same training loss, but with a different training loss for each instance. The authors show that this method can improve the performance of models on image classification and language understanding tasks. "
3637,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"Reinforcement learning USED-FOR complex tasks. Reinforcement learning USED-FOR policies. policies USED-FOR complex tasks. methods USED-FOR long - horizon tasks. Hierarchical reinforcement learning USED-FOR lowlevel skills. Hierarchical reinforcement learning USED-FOR action abstractions. action abstractions USED-FOR lowlevel skills. lower - level policies USED-FOR state abstraction. approach USED-FOR representation. value functions USED-FOR lower - level skill. value functions USED-FOR approach. value functions USED-FOR representation. value functions USED-FOR representation. approach COMPARE model - free and model - based methods. model - free and model - based methods COMPARE approach. maze - solving and robotic manipulation tasks EVALUATE-FOR approach. zero - shot generalization EVALUATE-FOR model - free and model - based methods. zero - shot generalization EVALUATE-FOR approach. OtherScientificTerm are horizon, lower - level skills, Hierarchies, and space states. Method is Value Function Spaces. ","This paper proposes a Hierarchical Reinforcement Learning (HRL) method for long-horizon robotic manipulation tasks. The main idea is to learn a hierarchy of low-level skills, where the high-level policy is responsible for abstracting the action abstractions and the lower-level policies are responsible for the state abstraction. The proposed method is based on the idea of value function spaces, which is used to model the representation of the low level skills. The method is evaluated on a maze-solving and robotic manipulation task.   ",This paper proposes a Hierarchical Reinforcement Learning (HR) approach for solving long-horizon tasks with low-level skills. The key idea is to use Hierarchies to learn a representation of the lower-level skill that can be used as a representation for the high-level state abstraction. This representation is then used to train a lower level policy that is able to learn the low level state abstraction and the high level state abstractions. The proposed method is evaluated on a maze-solving and robotic manipulation task. 
3653,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"one - shot probabilistic decoders USED-FOR vector - shaped prior. generative adversarial networks ( GAN ) CONJUNCTION normalizing flows. normalizing flows CONJUNCTION generative adversarial networks ( GAN ). variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). functions PART-OF variational autoencoders ( VAE ). functions USED-FOR drug discovery. Transformer layers CONJUNCTION graph neural networks. graph neural networks CONJUNCTION Transformer layers. them CONJUNCTION prior vector. prior vector CONJUNCTION them. Transformer layers USED-FOR prior vector. Transformer layers USED-FOR them. graph neural networks USED-FOR them. architecture USED-FOR exchangeable distributions. VAEs CONJUNCTION GANs. GANs CONJUNCTION VAEs. exchangeability USED-FOR VAEs. exchangeability USED-FOR GANs. Top - n HYPONYM-OF deterministic, non - exchangeable set creation mechanism. VAE CONJUNCTION GAN. GAN CONJUNCTION VAE. Top - n USED-FOR VAE. i.i.d. generation USED-FOR VAE. i.i.d. generation USED-FOR GAN. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. Top - n USED-FOR complex dependencies. Top - n COMPARE i.i.d. generation. i.i.d. generation COMPARE Top - n. SetMNIST reconstruction EVALUATE-FOR Top - n. SetMNIST reconstruction EVALUATE-FOR i.i.d. generation. algorithm USED-FOR molecule generation methods. algorithm USED-FOR one - shot generation. Task is Set and graph generation. OtherScientificTerm are normal distribution, and equivariance. Material are synthetic molecule - like dataset, and QM9 dataset. ","This paper proposes Top-n, a novel one-shot generative model based on variational autoencoders (VAE) and generative adversarial networks (GANs). The idea is to learn a vector-shaped prior over a set of samples, which is then used to train a GAN to generate sets of samples from the set. Theoretical analysis shows that the distribution over the set is equivariant to the normal distribution, and the authors show that this is the case for both GANs and VAEs. The authors then propose a deterministic, non-exchangeable set creation mechanism, which they call Top-N, and show that it is equivalent to i.i.d. generation for VAE and GAN, and that it can be used to improve the performance of GAN and VAE.  ","This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for one-shot generative generative models (GANs, VAEs, and normalizing flows) that can be used to generate a set from a set of samples. The authors propose to use a Transformer layer to generate the set, and a graph neural network to predict the prior of the set. They show that Top-N is able to generate sets that are equivariant with respect to the original set.  The authors also show that their method can be applied to GANs and VAEs."
3669,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,Deep Ritz Method ( DRM ) CONJUNCTION Physics - Informed Neural Networks ( PINNs ). Physics - Informed Neural Networks ( PINNs ) CONJUNCTION Deep Ritz Method ( DRM ). deep learning techniques USED-FOR elliptic partial differential equations ( PDEs ). Deep Ritz Method ( DRM ) USED-FOR deep learning techniques. random samples USED-FOR deep learning techniques. Physics - Informed Neural Networks ( PINNs ) USED-FOR deep learning techniques. hypercube FEATURE-OF Schrödinger equation. Schrödinger equation HYPONYM-OF prototype elliptic PDE. upper bounds USED-FOR problem. upper and lower bounds USED-FOR methods. upper bounds USED-FOR upper and lower bounds. rate generalization bound USED-FOR upper bounds. rate generalization bound USED-FOR problem. PINN CONJUNCTION DRM. DRM CONJUNCTION PINN. DRM USED-FOR minimax optimal bounds. PINN USED-FOR minimax optimal bounds. Sobolev spaces FEATURE-OF minimax optimal bounds. dimension dependent power law USED-FOR deep PDE solvers. power law USED-FOR deep model accuracy. OtherScientificTerm is zero Dirichlet boundary condition. Task is quantummechanical systems. Method is Deep Ritz Method. Generic is it. ,"This paper studies the problem of solving elliptic partial differential equations (PDEs) in the hypercube setting. The authors propose to use the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs) to solve the problem. The main contribution of the paper is to provide a rate generalization bound for the problem, which is based on the generalization error bound of the DeepRitz Method. The paper also provides a power law for deep PDE solvers.  ",This paper studies the problem of solving the Schrödinger equation in the context of quantum mechanics. The main contribution of the paper is to provide upper and lower bounds for the generalization of the problem. The upper bounds are based on the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). The lower bound is based on a lower bound on the rate generalization bound. The authors show that the upper bounds can be used to derive a power law for deep PDE solvers. 
3685,SP:80614db60d27a48c3c1b1882844e298666b798d4,Machine learning ( ML ) robustness CONJUNCTION generalization. generalization CONJUNCTION Machine learning ( ML ) robustness. data distribution shift FEATURE-OF they. adversarial and natural settings FEATURE-OF data distribution shift. other USED-FOR one. norm of the last layer CONJUNCTION Jacobian norm. Jacobian norm CONJUNCTION norm of the last layer. Jacobian norm CONJUNCTION data augmentations ( DA ). data augmentations ( DA ) CONJUNCTION Jacobian norm. function class regularization process USED-FOR domain generalization. adversarial training USED-FOR robustness. function class regularization USED-FOR robustness. DA USED-FOR generalization. DA USED-FOR regularization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. Generic is theoretical framework. OtherScientificTerm is sufficient conditions. ,This paper studies the relationship between adversarial robustness and generalization in the presence of data distribution shift. The authors show that adversarial training and data augmentation (DA) are sufficient conditions for robustness to improve generalization. They also show that DA is a regularization mechanism that improves robustness.  ,"This paper proposes a theoretical framework to study the relationship between robustness and generalization in the context of data distribution shift. The main contribution of the paper is a theoretical analysis of the generalization properties of data augmentation (DA) and the Jacobian norm of the last layer. Theoretically, the authors show that DA can be used to improve the robustness of adversarial training and generalize well in the domain generalization setting. They also show that data augmentations can improve robustness in the case of data shift."
3701,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"Meta - learning USED-FOR quick learning of few - shot tasks. meta - training tasks USED-FOR meta - knowledge. Wellgeneralized meta - knowledge USED-FOR fast adaptation. methods PART-OF framework. deconfounder approaches USED-FOR methods. Dropout USED-FOR meta - knowledge. deconfounder algorithms USED-FOR memorization. causal perspective USED-FOR memorization. causal perspective USED-FOR deconfounder algorithms. benchmark datasets USED-FOR memorization. benchmark datasets EVALUATE-FOR deconfounder algorithms. Task is task - specific adaptation. Method are regularizer - based and augmentation - based methods, meta - learning, and front - door adjustment. OtherScientificTerm are causality, and universal label space. ","This paper studies the problem of meta-learning in few-shot learning, where the goal is to learn well-generalized meta-knowledge for fast adaptation to new tasks. To this end, the authors propose two methods: (1) regularization-based and augmentation-based methods, and (2) front-door adjustment. The regularizer-based method is based on the fact that the label space of the new task is a universal label space, and hence it is possible to learn a good generalization model that can adapt to a new task in a fast manner. The experiments show that the proposed methods outperform the baselines on a variety of benchmark datasets.","This paper proposes a new meta-learning framework for few-shot learning, where the goal is to learn a well-generalized meta-knowledge that can be used for fast adaptation to new tasks. The key idea is to use a dropout-based approach to learn the meta-kernels for each task, and then use a deconfounder-based method for adapting to the new task. The authors provide a causal perspective on the effect of the dropout on the memorization of new tasks, and show that it can lead to faster adaptation. They also provide a theoretical analysis of the effects of dropout, and provide empirical evidence for the effectiveness of the proposed method."
3717,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"ad hoc teamwork HYPONYM-OF problem. full observability CONJUNCTION fixed and predefined teammates ’ types. fixed and predefined teammates ’ types CONJUNCTION full observability. fixed and predefined teammates ’ types HYPONYM-OF assumptions. full observability HYPONYM-OF assumptions. reinforcement learning framework USED-FOR autonomous agent. ODITS HYPONYM-OF reinforcement learning framework. information - based regularizer USED-FOR proxy representations of the learned variables. local observations USED-FOR information - based regularizer. local observations USED-FOR proxy representations of the learned variables. ODITS COMPARE baselines. baselines COMPARE ODITS. ad hoc teamwork tasks EVALUATE-FOR ODITS. ad hoc teamwork tasks EVALUATE-FOR baselines. Task is Autonomous agents. OtherScientificTerm are teammates, and partial observability. ","This paper proposes a reinforcement learning framework for ad hoc teamwork, where the goal is to train an autonomous agent that can work with a diverse set of teammates. The proposed method is based on an information-based regularizer that learns a proxy representation of the learned variables and uses local observations to improve the performance of the agent. Experiments show that the proposed method outperforms the baselines on a variety of tasks. ","This paper proposes a reinforcement learning framework for the problem of ad hoc teamwork. The authors propose an information-based regularizer for learning the proxy representations of the learned variables. The proposed method is evaluated on a variety of tasks, and it is shown that the proposed method outperforms existing baselines."
3733,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"missing values PART-OF high - dimensional data. approach USED-FOR down - stream analysis. imputation CONJUNCTION model estimation. model estimation CONJUNCTION imputation. model estimation USED-FOR down - stream analysis. imputation PART-OF approach. model estimation PART-OF approach. algorithm USED-FOR imputation. normalizing flow ( NF ) model USED-FOR data space. latent space FEATURE-OF imputation. normalizing flow ( NF ) model USED-FOR algorithm. Expectation - Maximization ( EM ) algorithm USED-FOR imputation. EMFlow COMPARE methods. methods COMPARE EMFlow. predictive accuracy CONJUNCTION speed of algorithmic convergence. speed of algorithmic convergence CONJUNCTION predictive accuracy. high - dimensional multivariate and image datasets EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR EMFlow. predictive accuracy EVALUATE-FOR EMFlow. speed of algorithmic convergence EVALUATE-FOR methods. predictive accuracy EVALUATE-FOR methods. Method are data mining and machine learning methods, EMFlow algorithm, and NF alternatively. ","This paper proposes EMFlow, a method for imputation of missing values in high-dimensional data. The main idea is to use a normalizing flow (NF) model to model the data space and then use Expectation-Maximization (EM) algorithm to estimate the missing values. Theoretical results show that EMFlow achieves the state-of-the-art predictive accuracy on both multivariate and image datasets.","This paper proposes EMFlow, a new method for down-stream analysis of missing values in high-dimensional data. EMFlow is based on a normalizing flow (NF) model, which is used to model the data space and impute the missing values. The authors propose a new Expectation-Maximization (EM) algorithm for imputation and model estimation, and show that EMFlow achieves better predictive accuracy and speed of algorithmic convergence compared to other methods. Empirical results are presented on a variety of datasets, showing the effectiveness of EMFlow."
3749,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,rectified linear units ( ReLUs ) USED-FOR DNNs. gates USED-FOR neural path kernel ( NPK ). rotational invariance CONJUNCTION ensemble structure. ensemble structure CONJUNCTION rotational invariance. global pooling CONJUNCTION skip connection. skip connection CONJUNCTION global pooling. convolution USED-FOR NPK. ensemble structure USED-FOR NPK. convolution USED-FOR ensemble structure. convolution USED-FOR rotational invariance. skip connection FEATURE-OF convolution. global pooling FEATURE-OF convolution. gates USED-FOR weights. external masks USED-FOR weights. weights PART-OF network. gates USED-FOR external masks. ReLUs FEATURE-OF DNNs. deep linear network USED-FOR pre - activations. DNNs USED-FOR ‘ black box’-ness. disentanglement CONJUNCTION interpretable re - arrangement of the computations. interpretable re - arrangement of the computations CONJUNCTION disentanglement. interpretable re - arrangement of the computations PART-OF DNN. ReLUs USED-FOR interpretable re - arrangement of the computations. ReLUs USED-FOR DNN. path space FEATURE-OF weights network. DLGN USED-FOR computations. primal ’ linearity CONJUNCTION dual ’ linearity. dual ’ linearity CONJUNCTION primal ’ linearity. path space FEATURE-OF dual ’ linearity. ‘ mathematically ’ interpretable linearities PART-OF DLGN. dual ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. primal ’ linearity HYPONYM-OF ‘ mathematically ’ interpretable linearities. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DGN CONJUNCTION DLGN. DLGN CONJUNCTION DGN. DNN CONJUNCTION DGN. DGN CONJUNCTION DNN. DLGN COMPARE DNNs. DNNs COMPARE DLGN. ‘ disentangled and interpretable ’ computations PART-OF DLGN. entang,"This paper proposes a deep linear network (DLGN) based on ReLUs. The main contribution of the paper is to introduce a new convolutional layer to the neural path kernel (NPK) architecture, which can be viewed as a combination of convolution and a skip connection. The authors show that the proposed DLGN is able to achieve state-of-the-art performance on CIFAR-10/100 and ImageNet.   ","This paper proposes a deep linear network (DLGN) based on ReLUs (rectified linear units) that can be used to perform disentanglement and interpretable computations. The main idea is to use a neural path kernel (NPK) that is invariant to rotational invariance, global pooling, skip connection, and ensemble structure. In particular, the authors show that the weights of the network can be disentangled by using external masks (e.g., a skip connection) and the weights can be re-arranged in a way that allows for interpretable computation. The authors also show that DLGN can be applied to a wide range of DNNs (DNNs, DGNs, and DGNGNNs) and achieve better performance on CIFAR-10, Cifar-100, and ImageNet. "
3765,SP:5676944f4983676b5ad843fdb190bf029ad647bb,Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. Vision Transformer ( ViT ) USED-FOR computer vision tasks. PVT HYPONYM-OF Vision Transformer ( ViT ). Swin HYPONYM-OF Vision Transformer ( ViT ). Layer Normalization ( LN ) USED-FOR models. Transformers USED-FOR inductive bias. LN USED-FOR positional context. positional context HYPONYM-OF inductive bias. Dynamic Token Normalization ( DTN ) HYPONYM-OF normalizer. it USED-FOR normalization methods. unified formulation USED-FOR it. global contextual information CONJUNCTION local positional context. local positional context CONJUNCTION global contextual information. Transformers USED-FOR local positional context. Transformers USED-FOR global contextual information. DTN USED-FOR Transformers. DTN USED-FOR intra - token and inter - token manners. PVT CONJUNCTION LeViT. LeViT CONJUNCTION PVT. BigBird CONJUNCTION Reformer. Reformer CONJUNCTION BigBird. Swin CONJUNCTION PVT. PVT CONJUNCTION Swin. LeViT CONJUNCTION T2T - ViT. T2T - ViT CONJUNCTION LeViT. T2T - ViT CONJUNCTION BigBird. BigBird CONJUNCTION T2T - ViT. ViT CONJUNCTION Swin. Swin CONJUNCTION ViT. DTN USED-FOR vision transformers. ViT HYPONYM-OF vision transformers. LeViT HYPONYM-OF vision transformers. Swin HYPONYM-OF vision transformers. T2T - ViT HYPONYM-OF vision transformers. PVT HYPONYM-OF vision transformers. Reformer HYPONYM-OF vision transformers. BigBird HYPONYM-OF vision transformers. transformer COMPARE baseline model. baseline model COMPARE transformer. DTN USED-FOR transformer. computational overhead EVALUATE-FOR baseline model. DTN COMPARE LN. LN COMPARE DTN. accuracy EVALUATE-FOR Long ListOps. Long - Range Arena FEATURE-OF,"This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The proposed method is based on a unified formulation that can be applied to different normalization methods. Theoretical analysis shows that the proposed method can be used to improve the performance of vision transformer models. Experiments show that DTN can reduce the computational cost of vision transforms.  ","This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The authors propose a unified formulation of DTN that can be applied to a variety of normalization methods, including layer normalization (LN) and Transformer (ViT, Swin, PVT, LeViT) and Reformer (T2T-ViT). The authors show that DTN can be used for both intra-token and inter-token manners, and that it can reduce the computational overhead of Transformer. Experiments on the Long-range Arena show that the proposed method can significantly improve the performance of ViT and Swin compared to LN."
3781,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,deep learning models USED-FOR expressive functions. SGD USED-FOR deep learning models. theoretical models USED-FOR spectral bias. methodologies USED-FOR spectral bias. spectral bias FEATURE-OF image classification networks. interventions USED-FOR generalization. spectral bias FEATURE-OF networks. regularization USED-FOR learning of high frequencies. models COMPARE ones. ones COMPARE models. models USED-FOR high frequencies. function frequency CONJUNCTION image frequency. image frequency CONJUNCTION function frequency. low frequencies PART-OF natural images. low frequencies USED-FOR spectral bias. natural images USED-FOR spectral bias. neural networks USED-FOR image classification. OtherScientificTerm is Spectral bias. Method is deep models. ,This paper studies the effect of spectral bias in deep neural networks on the performance of image classification models. The authors show that the spectral bias can be explained as a function of the function frequency and the image frequency. They show that models trained with SGD are more sensitive to high frequencies than models trained without SGD. They also show that regularization can help alleviate the high-frequency bias. ,"This paper studies the effect of spectral bias on the generalization performance of deep neural networks for image classification. The authors propose a theoretical analysis of the spectral bias of neural networks, and show that it can be explained as a function of the function frequency and the image frequency. They show that high-frequency networks learn high frequencies, while low-frequency ones learn low frequencies. They also show that regularization can be used to improve the performance of high-frequencies networks. "
3797,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"Exploration PART-OF reinforcement learning ( RL ). monolithic behaviour policy USED-FOR methods. nonmonolithic exploration USED-FOR RL. algorithmic components USED-FOR switching mechanism. two - mode exploration CONJUNCTION switching. switching CONJUNCTION two - mode exploration. sub - episodic time - scales FEATURE-OF switching. sub - episodic time - scales FEATURE-OF two - mode exploration. switching USED-FOR Atari. two - mode exploration USED-FOR Atari. OtherScientificTerm are exploratory behaviours, switching triggers, and hyper - parameter - tuning burden. ","This paper studies the problem of exploration in reinforcement learning. The authors propose a two-mode exploration strategy, where the exploration policy is a monolithic policy and the exploration is performed by a set of sub-episodic time-scales. They show that this exploration strategy can be combined with a switching mechanism, which allows the agent to switch between different exploration modes. They also show that the switching mechanism can be used to reduce the hyper-parameter-tuning burden.  ","This paper studies the switching mechanism of two-mode exploration and switching in reinforcement learning. The switching mechanism consists of two components: (1) a switching mechanism that can be used to switch from one mode to another, and (2) a two- mode exploration mechanism that is able to switch between two modes at different time-scales. The authors show that this switching mechanism can be applied to Atari environments, and that it can be combined with two mode exploration. They also show that it is possible to use two-manual exploration in Atari environments.   "
3813,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,initialization scheme USED-FOR k - median problem. metric space FEATURE-OF k - median problem. metric embedding tree structure USED-FOR initialization scheme. discrete space HYPONYM-OF metric space. search algorithm USED-FOR initial centers. search algorithm USED-FOR local search algorithm. differential privacy ( DP ) USED-FOR private initial centers. HST initialization HYPONYM-OF method. k - median++ USED-FOR non - DP setting. initialization method USED-FOR non - DP setting. initialization method USED-FOR initial centers. HST initialization USED-FOR initial centers. k - median++ HYPONYM-OF initialization method. DP local search CONJUNCTION private HST initialization. private HST initialization CONJUNCTION DP local search. Method is clustering algorithms. Task is construction of metric embedding tree structure. OtherScientificTerm is privacy constraint. Generic is methods. ,This paper studies the k-median clustering problem in the metric embedding tree structure. The authors propose a new initialization scheme based on the HST initialization scheme for the k median problem. They show that this initialization scheme can be used to compute the initial centers of the clustering algorithm. They also show that the initialization scheme is private under the DP constraint.   ,"This paper proposes a new initialization scheme for the k-median clustering problem in the setting of differential privacy (DP) where the initial centers of the metric embedding tree are private. The key idea is to use the HST initialization scheme to construct a metric embeddings tree structure in the metric space, which is then used to search for initial centers in a local search algorithm. The authors show that their method can be applied to the non-differential privacy setting, where the privacy constraint is imposed on the initial center. "
3829,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"planning USED-FOR agent. representation USED-FOR visual perception tasks. agent USED-FOR complex dynamics of the real - world. agent USED-FOR representation. complicated dynamics CONJUNCTION broader domain. broader domain CONJUNCTION complicated dynamics. broader domain FEATURE-OF real - life datasets. complicated dynamics FEATURE-OF real - life datasets. narrow benchmarks EVALUATE-FOR video prediction models. underfitting USED-FOR low quality predictions. FitVid HYPONYM-OF architecture. image augmentation techniques USED-FOR it. FitVid COMPARE models. models COMPARE FitVid. video prediction benchmarks EVALUATE-FOR models. video prediction benchmarks EVALUATE-FOR FitVid. metrics EVALUATE-FOR models. metrics EVALUATE-FOR FitVid. Generic are task, they, and state - of - theart models. Method is video models. OtherScientificTerm is overfitting. ","This paper proposes a method to improve the performance of video prediction models. The proposed method is based on the observation that video models are prone to overfitting and underfitting. To address this issue, the authors propose a new architecture called FitVid, which learns to model the complex dynamics of the real-world. The method is evaluated on several video prediction benchmarks and achieves state-of-the-art performance. ","This paper proposes a new architecture for video prediction, called FitVid, which aims to improve the performance of video prediction models on a wide range of video datasets. The main idea is to use an agent that learns to predict the dynamics of the real-world dynamics of a video dataset. The agent is trained using a combination of image augmentation techniques. The proposed method is evaluated on three video prediction benchmarks and compared with state-of-the-art models. "
3845,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,generalization EVALUATE-FOR machine learning algorithm. neural network HYPONYM-OF machine learning algorithm. model USED-FOR test loss. model USED-FOR stochastic gradient descent ( SGD ). data structure USED-FOR test loss dynamics. arbitrary covariance structure FEATURE-OF features. Gaussian features CONJUNCTION arbitrary features. arbitrary features CONJUNCTION Gaussian features. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. Gaussian model USED-FOR test loss. Gaussian model USED-FOR nonlinear random - feature models. nonlinear random - feature models CONJUNCTION deep neural networks. deep neural networks CONJUNCTION nonlinear random - feature models. theory USED-FOR Gaussian features. Gaussian model USED-FOR deep neural networks. real datasets EVALUATE-FOR deep neural networks. SGD USED-FOR deep neural networks. MNIST HYPONYM-OF real datasets. CIFAR-10 HYPONYM-OF real datasets. fixed compute budget FEATURE-OF optimal batch size. feature correlation structure USED-FOR optimal batch size. small batch sizes USED-FOR SGD. theory USED-FOR stochastic gradient descent. framework USED-FOR training and test error. real data EVALUATE-FOR framework. fixed subsampled training set USED-FOR stochastic gradient descent. OtherScientificTerm is structure of the data distribution. ,"This paper studies the generalization properties of stochastic gradient descent (SGD) in the presence of a test loss that depends on the data structure of the data distribution. The authors show that the test loss depends on a Gaussian distribution over the training data distribution, and they show that this distribution is independent of the covariance structure. They then show that SGD with a small batch size can generalize better than with a fixed subsampled training set.   ","This paper studies the generalization properties of stochastic gradient descent (SGD) under the data structure of the data distribution. The main contribution of the paper is a theoretical analysis of the test loss dynamics of SGD under a Gaussian feature structure. Theoretical analysis shows that SGD can generalize well under a fixed subsampled training set and a fixed compute budget, and that the optimal batch size depends on the feature correlation structure. Experimental results on MNIST and CIFAR-10 demonstrate the effectiveness of the proposed method."
3861,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"Stochastic gradient descent ( SGD ) USED-FOR nonlinear, nonconvex problem. learning rate CONJUNCTION model. model CONJUNCTION learning rate. AMSGrad USED-FOR local maxima. sharp minima USED-FOR SGD. Method are deep neural networks, and minimal neural network - like construction. Task is optimization problems. ","This paper studies SGD in a nonlinear, nonconvex optimization problem. The authors show that the learning rate of SGD converges to the minima of the min-max problem at a rate that scales linearly with the number of training samples and the size of the model. They show that this is the case for any learning rate that is close to the optimal learning rate for a given learning rate and a given model.   The main contribution of this paper is to show that SGD with a learning rate close to a certain learning rate can converge to a local maxima of a min-min problem with a known learning rate.  The authors also show that if the learning rates are close to some fixed learning rate, then SGD will converge to local minima. ","This paper studies the problem of stochastic gradient descent (SGD) in a nonlinear, nonconvex problem. The main contribution of the paper is a new construction of a minimal neural network-like construction, AMSGrad, that can be used to define sharp minima in SGD. The authors show that the sharp maxima of SGD can be defined as the number of local maxima in the learning rate of the model. They also show that sharp minimization can be achieved by using a simple construction of the minima.   "
3877,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"gradientbased hyperparameter optimization ( HO ) methods USED-FOR hyperparameters. Implicit Function Theorem ( IFT ) based methods USED-FOR online optimization. high - dimensional hyperparameters CONJUNCTION horizon length. horizon length CONJUNCTION high - dimensional hyperparameters. short horizon bias FEATURE-OF short horizon approximations. knowledge distillation USED-FOR second - order term. Jacobian - vector product ( JVP ) USED-FOR HO step. hyperparameter dimension CONJUNCTION horizon length. horizon length CONJUNCTION hyperparameter dimension. method USED-FOR online optimization. hyperparameter dimension USED-FOR method. meta - learning methods CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION meta - learning methods. meta - learning methods EVALUATE-FOR method. benchmark datasets EVALUATE-FOR method. Method are gradient - based meta - learning methods, inner - optimization, Unrolled differentiation methods, and HO method. ","This paper proposes a method for gradient-based hyperparameter optimization for meta-learning with hyperparameters. The method is based on the Jacobian-vector product (JVP) method, which is an extension of unrolled differentiation (UD) methods. The authors show that the JVP method can be used as a second-order term for knowledge distillation in the inner optimization step. The proposed method is evaluated on a variety of benchmark datasets.   ",This paper proposes a method for gradient-based hyperparameter optimization for meta-learning. The main idea is to use the Jacobian-vector product (JVP) as a second-order term to distill the knowledge distillation of the hyperparameters. The authors show that the JVP can be used to improve the performance of the online optimization method. The proposed method is evaluated on a variety of benchmark datasets. 
3893,SP:a64b26faef315c3ece590322291bab198932c604,"tasks USED-FOR meta - learning. meta - learning USED-FOR learning of new tasks. globally shared metalearner USED-FOR tasks. globally shared metalearner USED-FOR meta - learning. customization CONJUNCTION generalization. generalization CONJUNCTION customization. task clustering USED-FOR task - aware modulation. methods USED-FOR task representation. baselearner model USED-FOR task - specific optimization process. features USED-FOR task representation. features CONJUNCTION learning path. learning path CONJUNCTION features. features USED-FOR task representation. learning path USED-FOR task representation. geometric quantities USED-FOR learning path. path representation USED-FOR downstream clustering and modulation. meta path learner USED-FOR path representation. shortcut tunnel USED-FOR feature cluster assignments. shortcut tunnel USED-FOR path. path CONJUNCTION feature cluster assignments. feature cluster assignments CONJUNCTION path. few - shot image classification CONJUNCTION cold - start recommendation. cold - start recommendation CONJUNCTION few - shot image classification. CTML COMPARE baselines. baselines COMPARE CTML. real - world application domains EVALUATE-FOR CTML. real - world application domains EVALUATE-FOR baselines. cold - start recommendation EVALUATE-FOR CTML. cold - start recommendation HYPONYM-OF real - world application domains. few - shot image classification HYPONYM-OF real - world application domains. OtherScientificTerm is task heterogeneity. Method are global meta - learner, rehearsed task learning, and rehearsed learning. ","This paper proposes a method for meta-learning in the presence of heterogeneous tasks. The proposed method is based on a global meta-learner that learns a set of task-specific representations for each new task, which are then used for downstream clustering and modulation. The method is evaluated on few-shot image classification and cold-start recommendation tasks.   ","This paper proposes a new meta-learning framework for task-specific meta-training. The core idea is to use a global meta-learner to learn the task representation for each task, and a baselearner model to perform the task specific optimization process. The proposed method is based on the notion of task-aware modulation, which is used to modulate the task representations. The method is evaluated on a variety of tasks, including few-shot image classification, cold-start recommendation, and real-world application domains."
3909,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"in - distribution ( ID ) data EVALUATE-FOR deep neural networks. methods USED-FOR near OOD samples. labeled data USED-FOR near OOD samples. ensemble - based procedure USED-FOR semi - supervised novelty detection ( SSND ). ensemble - based procedure USED-FOR detection. unlabeled ID and OOD samples USED-FOR ensemble - based procedure. regularization USED-FOR OOD data. regularization USED-FOR It. approach COMPARE SSND methods. SSND methods COMPARE approach. image data sets CONJUNCTION medical image data sets. medical image data sets CONJUNCTION image data sets. medical image data sets EVALUATE-FOR SSND methods. image data sets EVALUATE-FOR SSND methods. medical image data sets EVALUATE-FOR approach. image data sets EVALUATE-FOR approach. Task is expert evaluation. Method is OOD detection algorithms. Material are near OOD data, and ID data. Metric is computational cost. ",This paper proposes an ensemble-based semi-supervised novelty detection (SSND) method to detect OOD samples from unlabeled ID and OOD data. The proposed method uses an ensemble of OOD and ID samples to train the network. The method is evaluated on image data sets and medical data sets.  ,This paper proposes an ensemble-based semi-supervised novelty detection (SSND) method to detect near OOD samples from unlabeled ID and OOD data. The method is based on a regularization term that is applied to the OOD and ID samples. The authors show that the proposed method outperforms the state-of-the-art SSND methods in terms of accuracy and computational cost.
3925,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"multi - agent trajectory prediction USED-FOR safe control of robotic systems. representation USED-FOR planning. encoder - decoder architectures USED-FOR scene - consistent multi - agent trajectories. Latent Variable Sequential Set Transformers HYPONYM-OF encoder - decoder architectures. Latent Variable Sequential Set Transformers USED-FOR scene - consistent multi - agent trajectories. AutoBots ” HYPONYM-OF architectures. temporal and social dimensions FEATURE-OF equivariant processing. model USED-FOR single - agent prediction case. Argoverse vehicle prediction challenge EVALUATE-FOR model. global nuScenes vehicle motion prediction leaderboard EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR model. model USED-FOR socially - consistent predictions. multi - agent setting EVALUATE-FOR model. synthetic partition of TrajNet++ dataset EVALUATE-FOR socially - consistent predictions. synthetic partition of TrajNet++ dataset EVALUATE-FOR multi - agent setting. desktop GPU ( 1080 Ti ) USED-FOR models. Method are encoder, and decoder. OtherScientificTerm is sequential structure. Material is Omniglot data. Generic is method. ",This paper proposes a method for multi-agent trajectory prediction from Omniglot data. The main idea is to use a latent variable sequential set transformer (LVST) architecture to model the temporal and social dimensions of the trajectories. The proposed method is evaluated on the Argoverse vehicle prediction challenge and achieves state-of-the-art performance. ,This paper proposes a new model for multi-agent trajectory prediction. The proposed model is based on the Latent Variable Sequential Set Transformers (LVS) architecture. The model is trained on the Omniglot dataset and is applied to the Argoverse vehicle prediction challenge. It is shown that the proposed model outperforms the state-of-the-art in terms of performance on the global nuScenes vehicle prediction leaderboard.
3941,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"methods USED-FOR image classification models. baseline explanation technique COMPARE concept - based and counterfactual explanations. concept - based and counterfactual explanations COMPARE baseline explanation technique. baseline COMPARE concept - based explanations. concept - based explanations COMPARE baseline. Counterfactual explanations COMPARE baseline. baseline COMPARE Counterfactual explanations. invertible neural network USED-FOR Counterfactual explanations. technical evaluations CONJUNCTION proxy tasks. proxy tasks CONJUNCTION technical evaluations. Generic are they, and model. Method is synthetic dataset generator. ",This paper studies the problem of providing counterfactual explanations for image classification models. The authors propose to use an invertible neural network to generate counterfactually explained images from a synthetic dataset. They show that the proposed method outperforms baseline explanations and concept-based explanations in terms of accuracy.  ,This paper proposes a new method for providing counterfactual explanations for image classification models. The proposed method is based on an invertible neural network that is trained on a synthetic dataset generated from a dataset generator. The authors show that the proposed method outperforms the baseline explanation technique and concept-based explanations. They also show that their method can be applied to a variety of tasks.
3957,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,deep networks USED-FOR backdoor data poisoning attacks. model USED-FOR inference. iterative training procedure USED-FOR poisoned data. boosting framework USED-FOR clean data. boosting framework USED-FOR poisoned data. bootstrapped measure of generalization USED-FOR algorithm. method USED-FOR dirty label backdoor attack. approach COMPARE defenses. defenses COMPARE approach. OtherScientificTerm is malicious data. Method is ensemble of weak learners. ,"This paper proposes a new training procedure for backdoor data poisoning attacks. The main idea is to train an ensemble of weak learners on poisoned data, and then use the poisoned data to improve the generalization ability of the model. The proposed training procedure is based on a boosting framework, where the weak learners are trained using a bootstrapped measure of generalization. The experimental results show that the proposed method outperforms the state-of-the-art defenses.",This paper proposes a novel method to tackle backdoor data poisoning attacks on deep networks. The authors propose a boosting framework to improve the generalization of the poisoned data. They use a bootstrapped measure of generalization to measure generalization. They show that their method outperforms the state-of-the-art defenses against backdoor backdoor attacks.
3973,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"MLTC USED-FOR modeling label correlations. single - label text classification methods COMPARE MLTC. MLTC COMPARE single - label text classification methods. document representation learning USED-FOR single - label text classification methods. label - correlation simplification CONJUNCTION sequencing label sets. sequencing label sets CONJUNCTION label - correlation simplification. sequencing label sets CONJUNCTION label - correlation overload. label - correlation overload CONJUNCTION sequencing label sets. It USED-FOR inductive bias. sequencing label sets HYPONYM-OF inductive bias. label - correlation simplification HYPONYM-OF inductive bias. latent label representations USED-FOR label correlations. latent labels USED-FOR contextual encodings. benchmark datasets EVALUATE-FOR It. label - correlation utilization CONJUNCTION document representation. document representation CONJUNCTION label - correlation utilization. token embeddings COMPARE latent labels. latent labels COMPARE token embeddings. embeddings FEATURE-OF latent labels. task information FEATURE-OF they. Task is Multi - label text classification ( MLTC ). OtherScientificTerm are complex label dependencies, and text tokens. Generic are method, and BERT. Method are latent - label encodings, latent and distributed correlation modeling, and latent label embeddings. ","This paper proposes a method for multi-label text classification. The method is based on the idea of latent label embeddings, which are used to model label correlations between documents. The proposed method is evaluated on a variety of benchmark datasets and compared with state-of-the-art text classification methods. ","This paper proposes a method for multi-label text classification (MLTC) that leverages both latent and distributed correlation modeling for document representation learning. The proposed method is based on BERT, and the authors show that the proposed method outperforms the state-of-the-art in terms of label-correlation utilization and document representation. The authors also show that their method is more efficient than BERT on a variety of datasets."
3989,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"images CONJUNCTION audio. audio CONJUNCTION images. deep convolutional networks USED-FOR tasks. deep convolutional networks USED-FOR highdimensional data. highdimensional data USED-FOR tasks. images HYPONYM-OF highdimensional data. audio HYPONYM-OF highdimensional data. convolution and pooling layers FEATURE-OF hierarchical kernels. convolutional kernel networks USED-FOR hierarchical kernels. norm USED-FOR spatial similarities. pooling layers USED-FOR norm. additive models of interaction terms PART-OF RKHS. pooling layers USED-FOR spatial similarities. pooling CONJUNCTION patches. patches CONJUNCTION pooling. sample complexity guarantees EVALUATE-FOR patches. Generic are they, and terms. Method is kernel methods. Material is vision datasets. OtherScientificTerm are functional space, inductive bias, generalization bounds, and regularities. ","This paper studies the inductive bias of convolutional kernels in high-dimensional data. In particular, the authors show that the convolution and pooling layers of RKHSs are regularized by the norm of the interaction terms in the functional space. The authors also provide sample complexity bounds for the patches and the pooling. Theoretical analysis is provided to show the generalization properties of the proposed method.",This paper studies the generalization properties of hierarchical convolution and pooling layers in deep convolutional kernel networks (RKHS). The authors provide a generalization bound for RKHS with respect to the norm of the pooling layer. They also provide a sample complexity bound for pooling and patching layers. The authors also provide theoretical guarantees for the sample complexity of patches and patches with pooling.
4005,SP:7bee8d65c68765cbfe38767743fec27981879d34,"Neural Tangent Kernel ( NTK ) PART-OF deep learning. NTK USED-FOR training and generalization of NN architectures. infinite width limit FEATURE-OF NTK. NTK USED-FOR NNs. architecture search CONJUNCTION meta - learning. meta - learning CONJUNCTION architecture search. NTK USED-FOR finite widths. compute and memory requirements FEATURE-OF NTK computation. NTK computation PART-OF finite width networks. compute and memory requirements FEATURE-OF finite width NTK. neural networks USED-FOR algorithms. algorithms USED-FOR finite width NTK. compute and memory requirements EVALUATE-FOR algorithms. attention CONJUNCTION recurrence. recurrence CONJUNCTION attention. convolutions CONJUNCTION attention. attention CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR differentiable computation. algorithms USED-FOR differentiable computation. convolutions CONJUNCTION recurrence. recurrence CONJUNCTION convolutions. general - purpose JAX function transformations USED-FOR algorithms. recurrence HYPONYM-OF algorithms. convolutions HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF algorithms. recurrence HYPONYM-OF general - purpose JAX function transformations. attention HYPONYM-OF general - purpose JAX function transformations. recurrence HYPONYM-OF differentiable computation. convolutions HYPONYM-OF differentiable computation. attention HYPONYM-OF differentiable computation. OtherScientificTerm are neural network ( NN ) Jacobians, and hyper - parameters. Method is NN architectures. ",This paper proposes a method to compute the Neural Tangent Kernel (NTK) of a neural network in the infinite width limit. The main idea is to use convolutions and recurrence to compute NTK in a differentiable way. Theoretical results show that the proposed method can compute the NTK of a network in a finite width. The method is shown to be computationally efficient.,"This paper studies the problem of computing the neural tangent kernel (NTK) of a neural network (NN) with a finite width. The authors show that NTK can be computed in a finite-width neural network, and propose a number of differentiable algorithms for computing NTK with finite width neural networks. The main contribution of the paper is that it provides a theoretical analysis of the computational complexity of NTK for finite width NNs. The paper also provides an empirical evaluation of the proposed algorithms."
4021,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"policy USED-FOR expected return. problem setting USED-FOR real - world scenarios. safety constraints FEATURE-OF policy. policy USED-FOR offline RL setting. estimation error FEATURE-OF offpolicy evaluation. offline constrained RL algorithm USED-FOR policy. stationary distribution FEATURE-OF policy. stationary distribution corrections FEATURE-OF optimal policy. algorithm USED-FOR stationary distribution corrections. algorithm USED-FOR cost - conservative policy. returns FEATURE-OF optimal policy. constraint satisfaction CONJUNCTION return - maximization. return - maximization CONJUNCTION constraint satisfaction. COptiDICE COMPARE baseline algorithms. baseline algorithms COMPARE COptiDICE. COptiDICE USED-FOR policies. return - maximization FEATURE-OF policies. constraint satisfaction FEATURE-OF policies. constraint satisfaction EVALUATE-FOR COptiDICE. return - maximization EVALUATE-FOR COptiDICE. Task is offline constrained reinforcement learning ( RL ) problem. OtherScientificTerm are cost constraints, and cost upper bound. Material is pre - collected dataset. ","This paper proposes a new offline constrained reinforcement learning algorithm, called COptiDICE, which is a cost-conservative policy learning algorithm for offline constrained RL with cost constraints. The main idea is to use a stationary distribution correction to update the policy in the offline RL setting. The authors show that the estimation error of off-policy evaluation is bounded by the cost upper bound of the stationary distribution corrections.   The authors also show that their algorithm is able to find the optimal policy with high returns in terms of the expected returns.","This paper proposes a new offline constrained reinforcement learning algorithm, called COptiDICE, for offline constrained RL. The main idea is to learn a cost-conservative policy that maximizes the return-maximization of the optimal policy under the constraints of the offline setting. The paper provides a cost upper bound on the cost of the policy, which is based on the estimation error of the off-policy evaluation error. The method is evaluated on a pre- collected dataset and compared with a number of baselines. "
4037,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,"dataparallel CONJUNCTION model - parallel training algorithms. model - parallel training algorithms CONJUNCTION dataparallel. parallelization strategies USED-FOR GRU. model - parallel training algorithms HYPONYM-OF parallelization strategies. dataparallel HYPONYM-OF parallelization strategies. training time EVALUATE-FOR approaches. parallel training scheme USED-FOR GRU. parallel - in - time HYPONYM-OF parallel training scheme. multigrid reduction in time ( MGRIT ) solver USED-FOR parallel training scheme. hierarchical correction of the hidden state USED-FOR end - to - end communication. parallel training scheme COMPARE serial approach. serial approach COMPARE parallel training scheme. HMDB51 dataset EVALUATE-FOR parallel training scheme. speedup EVALUATE-FOR serial approach. speedup EVALUATE-FOR parallel training scheme. parallelization strategy COMPARE parallel GRU algorithm. parallel GRU algorithm COMPARE parallelization strategy. sequence length FEATURE-OF parallelization strategy. Task is Parallelizing Gated Recurrent Unit ( GRU ) networks. Method are MGRIT, and gradient descent. OtherScientificTerm is processors. Material is image sequence. ","This paper proposes a parallelization strategy for training Gated recurrent units (GRUs) in parallel. The main idea is to use a multigrid reduction in time (MGRIT) solver to solve the problem of parallelizing the hidden state correction of a GRU. The proposed parallel training scheme is based on a hierarchical correction of the hidden states, which allows end-to-end communication between parallelized and parallelized GRUs. Experiments are conducted on the HMDB51 dataset and show that the proposed method achieves comparable or better performance compared to the existing parallel training algorithms. ","This paper proposes a new parallelizing Gated Recurrent Unit (GRU) training scheme for model-parallel training. The main idea is to use a multigrid reduction in time (MGRIT) solver to solve the problem of parallelizing the hidden state of a GRU network. This solver is based on the MGRIT solver, which can be used to learn a hierarchical correction of the hidden states. The proposed parallel training scheme is evaluated on the HMDB51 dataset and shows significant speedup over the existing parallel training methods."
4053,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"Functional magnetic resonance imaging ( fMRI ) HYPONYM-OF noisy measurement of brain activity. measurement resolution USED-FOR spatiotemporal averaging. PCA CONJUNCTION shared response modeling ( SRM ). shared response modeling ( SRM ) CONJUNCTION PCA. linear methods USED-FOR they. shared response modeling ( SRM ) HYPONYM-OF linear methods. PCA HYPONYM-OF linear methods. neural network USED-FOR common embedding. common space USED-FOR extensible manifold. classification accuracy EVALUATE-FOR stimulus features. framework USED-FOR applications. OtherScientificTerm are environmental differences, intrinsic dimension, brain activity, intrinsic structure, and noise. Generic is approaches. Material is raw fMRI signals. Task is cross - subject translation of fMRI signals. ",This paper proposes a method for cross-subject translation of fMRI signals. The proposed method is based on the observation that the intrinsic dimension of the fMRI signal depends on the environmental differences between subjects. The authors propose to use a shared embedding of the input fMRI data and a neural network to learn a common embedding for the input data. The embedding is then used to train a classifier that can be used for classification. The method is evaluated on a variety of datasets and compared to a number of baselines. ,This paper proposes a new method for cross-subject translation of fMRI signals. The proposed method is based on a neural network that learns a common embedding of the input fMRI signal. The embedding is then used to learn an extensible manifold that can be used to represent the stimulus features. The authors show that the proposed method can be applied to a wide range of applications. 
4069,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"biological phenomena CONJUNCTION self - driving cars. self - driving cars CONJUNCTION biological phenomena. Detecting out - of - distribution examples USED-FOR safety - critical machine learning applications. detecting novel biological phenomena HYPONYM-OF safety - critical machine learning applications. self - driving cars HYPONYM-OF safety - critical machine learning applications. benchmarks USED-FOR large - scale settings. ImageNet-21 K USED-FOR PASCAL VOC and COCO multilabel anomaly detectors. benchmark USED-FOR anomaly segmentation. road anomalies FEATURE-OF segmentation benchmark. segmentation benchmark USED-FOR benchmark. detector COMPARE prior methods. prior methods COMPARE detector. maximum logit USED-FOR detector. OtherScientificTerm are small - scale settings, and real - world settings. Task is out - of - distribution detection. Material is high - resolution images. Method is ImageNet multiclass anomaly detectors. ","This paper proposes ImageNet-21K, a new benchmark for anomaly detection in high-resolution images. The benchmark is based on the PASCAL VOC and COCO multilabel anomaly detectors. The proposed benchmark is designed for anomaly segmentation. The authors show that the proposed benchmark outperforms existing benchmarks in terms of out-of-distribution detection performance. ","This paper proposes ImageNet-21K, a multi-class anomaly detector for detecting out-of-distribution (OOD) anomalies in high-resolution images. The proposed detector is based on ImageNet multiclass anomaly detectors, and is trained on the PASCAL VOC and COCO multilabel anomaly detectors. It is shown that the proposed detector outperforms the state of the art in terms of detection accuracy and segmentation accuracy. The detector is also shown to be able to detect road anomalies. "
4085,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"parametric models USED-FOR intransitive tournaments. d dimensional node representations USED-FOR parametric models. d dimensional representations USED-FOR class of tournaments. theory USED-FOR parametric tournament representations. d dimensional representations USED-FOR class of tournaments. forbidden configurations FEATURE-OF tournament classes. tournaments PART-OF forbidden flip class. rank 2 tournaments CONJUNCTION locally - transitive tournaments. locally - transitive tournaments CONJUNCTION rank 2 tournaments. tournament class USED-FOR minimum feedback arc set problem. Quicksort procedure USED-FOR minimum feedback arc set problem. coned - doubly regular tournament FEATURE-OF flip class. forbidden configuration FEATURE-OF flip class. minimum dimension USED-FOR tournaments. upper bound USED-FOR smallest representation dimension. flip class FEATURE-OF tournament. flip class FEATURE-OF feedback arc set. OtherScientificTerm are Real world tournaments, union of flip classes, rank d tournament class, and sign - rank of matrices. ",This paper studies the representation of tournaments in parametric models. The authors show that there exists a class of tournaments with rank d tournaments and forbidden configurations. They show that the minimum feedback arc set problem can be solved by a Quicksort procedure. They also provide an upper bound on the smallest representation dimension for tournaments in this class.,This paper studies the problem of parametric tournament representation in the presence of forbidden configurations of tournaments. The authors provide a theoretical upper bound on the minimum dimension of the representation of tournaments in the worst-case setting. They show that the smallest representation dimension of a tournament is the smallest dimension of its flip class. They also provide a lower bound for the minimum number of tournaments of a given tournament class.
4101,SP:d39765dcc8950d4fc1d43e4c167208736578882e,context dataset USED-FOR Neural processes ( NPs ). identifier USED-FOR task. context representation USED-FOR identifier. dataset USED-FOR context representation. NPs USED-FOR identifier. context representation USED-FOR NPs. dataset USED-FOR NPs. network architectures CONJUNCTION aggregation functions. aggregation functions CONJUNCTION network architectures. NPs USED-FOR context embedding approaches. prediction accuracy EVALUATE-FOR NPs. permutation invariant FEATURE-OF aggregation functions. stochastic attention mechanism USED-FOR context information. stochastic attention mechanism USED-FOR NPs. NPs USED-FOR context information. method USED-FOR context embedding. method USED-FOR NPs. NPs USED-FOR context embedding. information theory USED-FOR method. features USED-FOR NPs. method USED-FOR context embedding. noisy data sets CONJUNCTION restricted task distributions. restricted task distributions CONJUNCTION noisy data sets. noisy data sets USED-FOR method. context embeddings USED-FOR NPs. predator - prey model CONJUNCTION image completion. image completion CONJUNCTION predator - prey model. 1D regression CONJUNCTION predator - prey model. predator - prey model CONJUNCTION 1D regression. approach COMPARE NPs. NPs COMPARE approach. 1D regression USED-FOR NPs. predator - prey model USED-FOR NPs. predator - prey model USED-FOR approach. image completion USED-FOR approach. 1D regression EVALUATE-FOR approach. MovieLens-10k dataset HYPONYM-OF real - world problem. real - world problem EVALUATE-FOR method. MovieLens-10k dataset EVALUATE-FOR method. ,This paper proposes a method to learn context embeddings for neural processes (NPs) using a context dataset. The proposed method is based on the information theory of context embedding. The authors propose to use an attention mechanism to extract the context information from the context representation and use this information to improve the performance of NPs. The method is evaluated on the MovieLens-10k dataset and compared with other context-based methods.,"This paper proposes a new context embedding method for Neural Processes (NPs) that is permutation-invariant. The key idea of the method is to use the information theory of information theory to learn the context embeddings of NPs. The method is evaluated on the MovieLens-10k dataset, where it outperforms the state-of-the-art in terms of prediction accuracy. "
4117,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"Transformer language models USED-FOR NLP tasks. prototype networks PART-OF model architecture. architecture COMPARE language models. language models COMPARE architecture. user interactions USED-FOR it. Metric is interpretability. Generic are black - box models, and network. OtherScientificTerm is human capabilities. Method is data - driven approaches. ","This paper studies the interpretability of Transformer language models in NLP tasks. The authors propose to use prototype networks in the model architecture, where the prototype network is a pre-trained language model that is trained with a set of user-interactions. They show that the proposed architecture is more interpretable than the baseline Transformer models. They also show that this architecture is able to learn from user interactions.   ",This paper studies the interpretability of Transformer language models in the context of NLP tasks. The authors propose a new model architecture that is designed to be interpretable. They show that the model is interpretable in terms of the user interaction with the model. They also show that their model is more interpretable than the state-of-the-art.
4133,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"Catastrophic forgetting PART-OF continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR continual learning. Trust Region Gradient Projection ( TRGP ) USED-FOR forward knowledge transfer. continual learning USED-FOR forward knowledge transfer. scaled weight projection USED-FOR frozen weights. frozen weights PART-OF trust region. layer - wise scaling matrix USED-FOR scaled weight projection. layer - wise scaling matrix USED-FOR frozen weights. TRGP USED-FOR knowledge transfer. scaling matrices CONJUNCTION model. model CONJUNCTION scaling matrices. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Generic are methods, and task. OtherScientificTerm are optimization space, task correlation, layer - wise and single - shot manner, and subspaces of old tasks. Method is norm of gradient projection. ","This paper proposes Trust Region Gradient Projection (TRGP), a method for continual learning in continual learning. The main idea is to use a layer-wise scaling matrix to estimate the scaled weight projection of frozen weights in the trust region. The scaling matrix is computed based on the layer-level scaling matrices and the frozen weights. The proposed method is evaluated on a variety of continual learning tasks and achieves state-of-the-art performance. ","This paper proposes Trust Region Gradient Projection (TRGP) for continual learning. TRGP is a trust-based continual learning method that aims to prevent catastrophic forgetting in the continual learning setting. The key idea is to use a scaled weight projection of the frozen weights in the trust region as a layer-wise scaling matrix, which is used to estimate the scaling matrix of the weights in a trust region. The authors show that TRGP can be used to transfer knowledge across different tasks in a single-shot manner, and that it can also be used in a multi-task manner. "
4149,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,Optimization CONJUNCTION generalization. generalization CONJUNCTION Optimization. generalization PART-OF machine learning. Optimization PART-OF machine learning. framework USED-FOR generalization. framework USED-FOR optimization. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. gradient flow algorithm USED-FOR length of optimization trajectory. length of optimization trajectory USED-FOR generalization error. initialization USED-FOR gradient flow. estimate USED-FOR length - based generalization bound. short optimization paths USED-FOR generalization. it USED-FOR generalization estimates. kernel regression CONJUNCTION overparameterized two - layer ReLU neural networks. overparameterized two - layer ReLU neural networks CONJUNCTION kernel regression. underdetermined lp linear regression CONJUNCTION kernel regression. kernel regression CONJUNCTION underdetermined lp linear regression. it USED-FOR machine learning models. generalization estimates USED-FOR machine learning models. overparameterized two - layer ReLU neural networks HYPONYM-OF machine learning models. underdetermined lp linear regression HYPONYM-OF machine learning models. kernel regression HYPONYM-OF machine learning models. Generic is approach. OtherScientificTerm is explicit length estimate. ,"This paper proposes a generalization bound based on the length of the optimization trajectory. The authors use a gradient flow algorithm to estimate the generalization error for a given set of optimization trajectories, which is then used to compute a bound on the generalisation error. They show that the bound depends on the number of optimization paths in the trajectory, and that short optimization paths are responsible for generalization. They also show that this bound can be used to derive a new generalization estimate. ","This paper proposes a new generalization bound for optimization-based generalization of machine learning models. The generalization bounds are based on the length of the optimization trajectory, which is defined as the number of steps in the gradient flow. The authors show that this bound can be used to estimate the generalization error of a given optimization trajectory. They also show that the bound does not depend on the optimization path length, but rather on the initialization of the gradient.  "
4165,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"Adversarial examples USED-FOR deep learning systems. high - frequency noise FEATURE-OF adversarial examples. CIFAR-10 CONJUNCTION ImageNet - derived datasets. ImageNet - derived datasets CONJUNCTION CIFAR-10. ImageNet - derived datasets USED-FOR models. CIFAR-10 USED-FOR models. frequency constraints USED-FOR robust models. Task is attacks. Generic are examples, and framework. Method is frequency - based understanding of adversarial examples. OtherScientificTerm is frequency - based explanation. ","This paper proposes a frequency-based understanding of adversarial examples. The authors show that high-frequency examples are more vulnerable to adversarial attacks than low-frequency ones. They then propose a framework to analyze the frequency of such examples, and show that frequency constraints can be used to improve the robustness of robust models.  ","This paper proposes a framework for understanding the frequency-based understanding of adversarial examples. The authors show that high-frequency examples are more likely to be adversarial than low-frequency ones, and propose a framework that can be used to understand the frequency of the examples. This framework is based on the fact that the high-fidelity examples can be represented as a function of frequency, and that the frequency constraints can be applied to robust models. The framework is tested on CIFAR-10 and ImageNet-derived datasets."
4181,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"relational inductive bias ( homophily assumption ) USED-FOR graph structures. graph structures USED-FOR Graph Neural Networks ( GNNs ). GNNs COMPARE NNs. NNs COMPARE GNNs. GNNs COMPARE graph - agnostic NNs. graph - agnostic NNs COMPARE GNNs. NNs USED-FOR real - world tasks. real - world tasks EVALUATE-FOR GNNs. aggregation operation USED-FOR GNNs. similarity matrix USED-FOR GNNs. graph structure CONJUNCTION features. features CONJUNCTION graph structure. features USED-FOR GNNs. similarity matrix USED-FOR metrics. metrics COMPARE homophily metrics. homophily metrics COMPARE metrics. synthetic graphs EVALUATE-FOR homophily metrics. diversification operation USED-FOR harmful heterophily. diversification CONJUNCTION identity channels. identity channels CONJUNCTION diversification. aggregation CONJUNCTION diversification. diversification CONJUNCTION aggregation. Adaptive Channel Mixing ( ACM ) framework USED-FOR aggregation. identity channels USED-FOR harmful heterophily. Adaptive Channel Mixing ( ACM ) framework USED-FOR diversification. Adaptive Channel Mixing ( ACM ) framework USED-FOR harmful heterophily. identity channels USED-FOR GNN layer. Adaptive Channel Mixing ( ACM ) framework USED-FOR identity channels. diversification USED-FOR GNN layer. GNN layer USED-FOR harmful heterophily. realworld node classification tasks EVALUATE-FOR ACM - augmented baselines. They COMPARE GNNs. GNNs COMPARE They. tasks EVALUATE-FOR They. tasks EVALUATE-FOR GNNs. OtherScientificTerm are Heterophily, and heterophily. Method is filterbanks. ","This paper studies the problem of heterophily in graph neural networks (GNNs) and proposes a new aggregation operation called Adaptive Channel Mixing (ACM) to reduce the harmful heterophilies in GNNs. Specifically, the proposed method is based on the aggregation operation and diversification. The proposed ACM method is evaluated on node classification tasks and shows improved performance compared to existing GNN-based methods.","This paper studies the problem of heterophily in graph neural networks (GNNs). In particular, the authors consider the problem that GNNs are heterophilic, and propose two metrics to measure the heterophilicity of a GNN. The first metric is based on the similarity matrix, which measures the similarity between features and graph structure. The second metric is the diversification metric, which considers the aggregation of features and features of the graph. The authors propose a new aggregation operation, called Adaptive Channel Mixing (ACM), to mitigate the harmful heterophiliy of GNN layers. They show that ACM-augmented baselines outperform GNN-based baselines on several real-world tasks."
4197,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,generalizability FEATURE-OF deep RL approach. RL training techniques USED-FOR deep learning architecture. deep learning architecture PART-OF proposition. equivariance USED-FOR training. local search heuristics USED-FOR value landscape. local search heuristics CONJUNCTION RL training. RL training CONJUNCTION local search heuristics. RL training USED-FOR value landscape. proposition COMPARE deep RL methods. deep RL methods COMPARE proposition. random and realistic TSP problems EVALUATE-FOR deep RL methods. random and realistic TSP problems EVALUATE-FOR proposition. Method is Deep reinforcement learning ( RL ). Material is larger - sized instances. Generic is approach. Task is ablation study. ,"This paper studies the generalizability of deep reinforcement learning (DRL) in the presence of large-sized instances. The authors propose to use a deep learning architecture to learn the value landscape of a TSP problem, which is then used to train a deep RL agent. The main contribution of the paper is to show that the proposed method is equivariant to the size of the instances and the number of training samples. The paper also proposes to use local search heuristics to improve the generalization performance of DRL. ","This paper studies the generalizability of deep reinforcement learning (DRL) in the setting of large-sized instances. The authors propose a new approach to generalize DRL in the context of TSP problems. The main idea of the paper is to train a deep RL model on the value landscape of a set of small-sized TSP instances, and then use local search heuristics and RL training to improve the generalization ability of DRL. The proposed approach is evaluated on a variety of real-world TSP tasks, and it is shown to outperform the state-of-the-art in terms of generalization."
4213,SP:8aa471b92e2671d471107c087164378f45fb204f,"Federated learning ( FL ) HYPONYM-OF privacy - preserving collaborative learning paradigm. framework USED-FOR non - IID issue. local generative adversarial network ( GAN ) USED-FOR synthetic data. parameter server ( PS ) USED-FOR global shared synthetic dataset. confident threshold USED-FOR pseudo labeling. pseudo labeling USED-FOR PS. local private dataset CONJUNCTION labeled synthetic dataset. labeled synthetic dataset CONJUNCTION local private dataset. artificial noise USED-FOR local model gradients. local GANs USED-FOR privacy. differential privacy USED-FOR local GANs. artificial noise USED-FOR local GANs. framework COMPARE baseline methods. baseline methods COMPARE framework. supervised and semi - supervised settings FEATURE-OF benchmark datasets. supervised and semi - supervised settings EVALUATE-FOR framework. supervised and semi - supervised settings EVALUATE-FOR baseline methods. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR baseline methods. Generic is it. OtherScientificTerm are IID ( independent and identically distributed ) data, and data distributions. Material are differentially private synthetic data, and global dataset. Task is global aggregation. Method is local models. ",This paper proposes a new federated learning framework that addresses the non-IID issue in federating learning. The proposed method is based on a local generative adversarial network (GAN) that generates synthetic data from a global shared synthetic dataset and then uses pseudo-labels from a local parameter server (PS) to construct a pseudo-labeled synthetic dataset. The local GANs are trained with artificial noise to improve the privacy of the local model gradients. The experimental results show that the proposed method achieves state-of-the-art performance in both supervised and semi-supervised settings.,"This paper proposes a new framework for federated learning with differentially private synthetic data. The proposed method is based on a local generative adversarial network (GAN) trained on a global shared synthetic dataset. The local GAN is trained on the synthetic data, and the global dataset is shared with the parameter server (PS). The authors propose to use pseudo-labels to improve the privacy of the local data. They show that the proposed method outperforms the state-of-the-art in both supervised and semi-supervised settings."
4229,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"classifier USED-FOR classifier. Gaussian noise USED-FOR classifier. robustness EVALUATE-FOR classifier. accuracy CONJUNCTION ( adversarial ) robustness. ( adversarial ) robustness CONJUNCTION accuracy. training method USED-FOR smoothed classifiers. sample - wise control of robustness USED-FOR training method. robustness CONJUNCTION prediction confidence. prediction confidence CONJUNCTION robustness. robustness EVALUATE-FOR smoothed classifiers. prediction confidence FEATURE-OF smoothed classifiers. certified robustness EVALUATE-FOR training methods. method COMPARE training methods. training methods COMPARE method. certified robustness EVALUATE-FOR method. OtherScientificTerm are ` 2 - adversarial perturbations, noise, adversarial robustness, training objective, and worst - case ( adversarial ) objective. Method is randomized smoothing. Generic is control. ","This paper studies the problem of adversarial robustness, i.e., the ability of a classifier to detect adversarial perturbations. The authors propose a method to improve the robustness of smoothed classifiers by controlling the sample-wise control of robustness. The proposed method is based on randomized smoothing, where the smoothing is done in a random way, and the objective is to minimize the worst-case (adversarial) perturbation. Experiments on CIFAR-10 and ImageNet show that the proposed method can improve the certified robustness compared to existing methods. ","This paper proposes a method to improve the robustness of smoothed classifiers against adversarial perturbations. The main idea is to use a sample-wise control of robustness, i.e., the number of samples that the classifier is trained to be robust against the worst-case (worst-case adversarial) perturbation. The method is based on randomized smoothing, where the smoothed model is trained with Gaussian noise. The authors show that their method improves the certified robustness compared to the state-of-the-art. "
4245,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"Wikipedia dataset USED-FOR pretraining BERT. histogram of sequence lengths USED-FOR packing. linear complexity EVALUATE-FOR algorithms. packing order FEATURE-OF Wikipedia dataset. model USED-FOR dataset. OtherScientificTerm are padding tokens, padding, near optimal packing, and 2x speed - up. Method is packing algorithms. Material is packed dataset. Metric is convergence. ",This paper studies the problem of pre-training BERT on Wikipedia with padding tokens. The authors propose a packing algorithm that uses the histogram of sequence lengths as a histogram to predict the order in which tokens should be packed into the training set. They show that the packing algorithm converges linearly to the optimal packing order and achieves a 2x speed-up in terms of the number of padding tokens compared to the original packing algorithm. They also show that their packing algorithm can be used to train a BERT model on the packed Wikipedia dataset. ,"This paper studies the problem of pre-training BERT on the Wikipedia dataset. The authors show that the packing order of the dataset is not optimal, and propose a new packing algorithm with linear complexity. The algorithm is shown to converge to a near optimal packing order with a 2x speed-up. "
4261,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,adaptive tree search algorithm USED-FOR high - scoring outputs. adaptive tree search algorithm HYPONYM-OF Monte Carlo tree search. translation models USED-FOR high - scoring outputs. algorithm USED-FOR models. autoregressivity CONJUNCTION conditional independence assumptions. conditional independence assumptions CONJUNCTION autoregressivity. algorithm COMPARE beam search. beam search COMPARE algorithm. decoding bias USED-FOR autoregressive models. algorithm USED-FOR autoregressive models. adaptive tree search algorithm COMPARE beam search. beam search COMPARE adaptive tree search algorithm. reranking techniques USED-FOR models. beam search USED-FOR autoregressive models. adaptive tree search algorithm COMPARE reranking techniques. reranking techniques COMPARE adaptive tree search algorithm. model scores EVALUATE-FOR adaptive tree search algorithm. BLEU EVALUATE-FOR translation model objectives. noisy channel model CONJUNCTION objective. objective CONJUNCTION noisy channel model. autoregressive models CONJUNCTION noisy channel model. noisy channel model CONJUNCTION autoregressive models. expected automatic metric scores CONJUNCTION noisy channel model. noisy channel model CONJUNCTION expected automatic metric scores. autoregressive models USED-FOR expected automatic metric scores. decoder USED-FOR search. objective HYPONYM-OF models. autoregressive models HYPONYM-OF models. beam search bias USED-FOR models. noisy channel model HYPONYM-OF models. search USED-FOR models. beam search CONJUNCTION reranking based methods. reranking based methods CONJUNCTION beam search. OtherScientificTerm is search objective. Task is decoding. Generic is objectives. ,"This paper proposes an adaptive tree search algorithm for autoregressive models. The main idea is to use a translation model to find the high-scoring outputs of the decoder, and then use a beam search algorithm to search for models with high scoring outputs. The authors show that the proposed method outperforms beam search and reranking based methods in terms of model scores. ","This paper proposes a new adaptive tree search algorithm for autoregressive models, where the objective is to find high-scoring outputs from a translation model. The main contribution of the paper is that the search objective is based on an autoregressivity assumption and conditional independence assumptions. The authors show that their algorithm outperforms beam search and reranking based methods in terms of model scores. "
4277,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"normal distribution FEATURE-OF task. Energy Based Model ( EBM ) USED-FOR intractability of abnormal distribution. iterative optimization procedure USED-FOR Langevin Dynamics ( LD ). Langevin Dynamics ( LD ) USED-FOR EBM. iterative optimization procedure USED-FOR EBM. anomaly detector USED-FOR task. adaptive sparse coding layer USED-FOR anomaly detector. OtherScientificTerm are anomaly, normal population, plug and play feature, and sparse coding layer. Method are AI solutions, EBMs, and meta learning scheme. ",This paper proposes an energy based model (EBM) based on Langevin dynamics (LD) to address the intractability of abnormal distribution. The authors propose a meta learning scheme to improve the performance of EBMs in the presence of anomalies. The main contribution of the paper is to introduce an adaptive sparse coding layer for anomaly detection.  ,"This paper proposes a meta-learning approach for learning energy-based models (EBMs) with Langevin Dynamics (LD) and an adaptive sparse coding layer to detect anomalies in the normal population. The proposed approach is based on the idea of plug-and-play (P&P) model, which is an extension of the Langevin model (LD). The authors show that the proposed approach can be used to learn EBMs with a Langevin-based model. The authors also show that their method can be applied to the problem of detecting anomalies in EBMs.   "
4293,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,propaganda CONJUNCTION news. news CONJUNCTION propaganda. news CONJUNCTION social media. social media CONJUNCTION news. QA systems USED-FOR misinformation. misinformation FEATURE-OF QA models. large - scale dataset USED-FOR problem. CONTRAQA HYPONYM-OF large - scale dataset. contradicting contexts USED-FOR QA models. question answering CONJUNCTION misinformation detection. misinformation detection CONJUNCTION question answering. counter - measure USED-FOR misinformation - aware QA system. misinformation detection PART-OF counter - measure. question answering PART-OF counter - measure. misinformation detection PART-OF misinformation - aware QA system. Method is QA model. OtherScientificTerm is real and fake information. ,"This paper studies the problem of misinformation in QA systems. The authors propose a large-scale dataset called CONTRAQA, which contains both real and fake information from multiple sources. They use this dataset to train a QA model that can distinguish between real and false information. They also propose a counter-measure for misinformation detection and question answering.   ","This paper proposes a new dataset called CONTRAQA, a large-scale dataset of contradicting contexts for QA models. ContraQA is a dataset that contains both real and fake information. The authors propose a new counter-measure for misinformation detection and question answering, which they call ""misinformation-aware QA system"". They show that the proposed counter measure is effective in detecting misinformation, and that it can be combined with question answering to improve the performance of the system. "
4309,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"embodiment CONJUNCTION morphology. morphology CONJUNCTION embodiment. expert demonstrations USED-FOR imitation agent. embodiment USED-FOR imitation agent. morphology USED-FOR imitation agent. method USED-FOR cross - domain imitation. GromovWasserstein distance USED-FOR method. GWIL USED-FOR optimality. rigid transformation of the expert domain CONJUNCTION arbitrary transformation of the state - action space. arbitrary transformation of the state - action space CONJUNCTION rigid transformation of the expert domain. GWIL USED-FOR continuous control domains. Task is Cross - domain imitation learning. OtherScientificTerm is stationary distributions. Generic are they, and theory. Method is Gromov - Wasserstein Imitation Learning ( GWIL ). ",This paper proposes a Gromov-Wasserstein distance-based method for cross-domain imitation learning. The main idea is to use the distance between the expert demonstrations and the state-action space of the imitation agent as a proxy for the similarity between the two domains. The authors show that this distance can be used to estimate the distance of the expert and the imitation agents to the same state space. The method is shown to be optimal in continuous control domains. ,"This paper proposes a Gromov-Wasserstein distance (GWIL) method for cross-domain imitation learning. The main contribution of the paper is a theoretical analysis of GWIL, which shows that the distance between the expert domain and the state-action space of the imitation agent can be defined as a Gomov-wasserstein (GW) distance. The GW distance is defined as the difference between the GW-distance between the state action space and the expert state space. The authors show that GWIL can be applied to continuous control domains, and show that it can be used to improve the performance of imitation learning in continuous control environments."
4325,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"self - supervised learning ( SSL ) USED-FOR computer vision. labeling cost FEATURE-OF computer vision. labeling cost EVALUATE-FOR self - supervised learning ( SSL ). SSL USED-FOR invariant visual representations. contrastive loss EVALUATE-FOR representation invariant. hidden layer PART-OF projection head. hierarchical projection head USED-FOR raw representations of the backbone. hierarchical projection head USED-FOR HCCL. cross - level contrastive learning USED-FOR HCCL. generalization ability EVALUATE-FOR visual representations. generalization ability EVALUATE-FOR HCCL. HCCL USED-FOR SSL frameworks. detection CONJUNCTION segmentation. segmentation CONJUNCTION detection. classification CONJUNCTION detection. detection CONJUNCTION classification. segmentation CONJUNCTION few - shot learning tasks. few - shot learning tasks CONJUNCTION segmentation. HCCL USED-FOR detection. HCCL USED-FOR segmentation. few - shot learning tasks EVALUATE-FOR HCCL. classification EVALUATE-FOR HCCL. HCCL COMPARE methods. methods COMPARE HCCL. benchmark datasets EVALUATE-FOR HCCL. benchmark datasets EVALUATE-FOR methods. Method are SSL methods, and Hierarchical Cross Contrastive Learning(HCCL ). Generic is approach. OtherScientificTerm are latent spaces, and latent features. ","This paper proposes a hierarchical cross-contrastive learning (HCCL) method for self-supervised learning (SSL) in computer vision. The main idea is to use a hierarchical projection head to learn the raw representations of the backbone in the latent space, and then use contrastive learning to improve the generalization ability of the learned representations. The proposed method is evaluated on detection, classification, and few-shot learning tasks. ","This paper proposes Hierarchical Cross Contrastive Learning (HCCL), a new approach for self-supervised learning (SSL) that aims to improve the generalization ability of SSL. HCCL is based on hierarchical cross-contrastive learning (HCTL), which aims to learn representations that are invariant to contrastive loss. HCTL uses a hierarchical projection head to learn the raw representations of the backbone, and a hidden layer to represent the latent features of the hidden layer. The proposed method is evaluated on a variety of tasks, including few-shot learning, detection, segmentation, and classification. "
4341,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"Real economies HYPONYM-OF sequential imperfect - information game. heterogeneous, interacting strategic agents PART-OF sequential imperfect - information game. Dynamic general equilibrium models USED-FOR economic activity. Dynamic general equilibrium models USED-FOR interactions. economic activity CONJUNCTION interactions. interactions CONJUNCTION economic activity. Dynamic general equilibrium models USED-FOR systems. analytical and computational methods USED-FOR explicit equilibria. structured learning curricula CONJUNCTION GPU - only simulation and training. GPU - only simulation and training CONJUNCTION structured learning curricula. market clearing HYPONYM-OF unrealistic assumptions. GPU implementation USED-FOR training and analyzing economies. real - business - cycle models HYPONYM-OF DGE models. approach USED-FOR real - business - cycle models. RL policies CONJUNCTION economic intuitions. economic intuitions CONJUNCTION RL policies. meta - game -Nash equilibria PART-OF open RBC models. approximate best - response analyses USED-FOR meta - game -Nash equilibria. Method are joint learning, and meta - game. OtherScientificTerm are reward function, consumer ’s expendable income, -Nash equilibria, analytical tractability, and worker - consumers. Task is economic simulations. ","This paper proposes a meta-game-Nash equilibrium model for real-world economic systems with heterogeneous, interacting strategic agents in a sequential imperfect-information game. The proposed model is based on a dynamic general equilibrium model, which is able to capture the dynamics of economic activity and interactions between agents. The authors show that this model is computationally tractable using approximate best-response analyses, and that it can be used to estimate the meta-games-nash equilibria of open RBC models with open-RBC models. Theoretical tractability is demonstrated by showing that the proposed model can be trained with a GPU-only simulation and training.","This paper proposes a method for simulating real-business-cycle models (RBCs) with dynamic general equilibrium models (DGEs) that can be used to model the dynamics of a heterogeneous, interacting strategic agents in a sequential imperfect-information game. The authors propose an approach to simulate the meta-game-Nash equilibria of RBCs with dynamic DGEs. They show that their approach is tractable and can be applied to a wide range of open RBC models. They also provide a theoretical analysis of their approach. "
4357,SP:f885c992df9c685f806a653398736432ba38bd80,public API USED-FOR machine learning model. robustness CONJUNCTION model utility. model utility CONJUNCTION robustness. defenses USED-FOR model stealing. query access USED-FOR model extraction. differential privacy USED-FOR calibration. victim model USED-FOR method. Task is model extraction attacks. Generic is model. Metric is computational effort. OtherScientificTerm is proof - of - work. Method is machine learning practitioners. ,"This paper studies model stealing in the context of model extraction attacks, where the goal is to steal a machine learning model from a public API. The authors propose two defenses against model stealing. The first defense is based on differential privacy. The second defense is a combination of the two defenses. The main contribution of the paper is a proof-of-work algorithm that is able to recover the original model from the target model.","This paper proposes a new defense against model extraction attacks. The defense is based on the notion of differential privacy, where the target model is the one that is most likely to be affected by the attack, and the attacker is the model that is more likely to have access to the victim model. The authors show that the defense can be applied to any public API, and that it can be combined with other defenses against model stealing. They also show that it is possible to recover the model from the attacker. "
4373,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"Neural Ordinary Differential Equations ( ODEs ) USED-FOR generative models of images. Continuous Normalizing Flows ( CNFs ) USED-FOR generative models of images. models USED-FOR exact likelihood calculation. exact likelihood calculation CONJUNCTION invertible generation / density estimation. invertible generation / density estimation CONJUNCTION exact likelihood calculation. models USED-FOR invertible generation / density estimation. approach COMPARE prior methods. prior methods COMPARE approach. likelihood values FEATURE-OF image datasets. likelihood values EVALUATE-FOR approach. GPU USED-FOR prior methods. image datasets EVALUATE-FOR approach. training time EVALUATE-FOR prior methods. Method is MRCNF ). OtherScientificTerm are conditional distribution, fine image, coarse image, and log likelihood. ","This paper proposes a continuous normalizing flow (CNF) method for image generation and density estimation. The proposed method is based on the idea of continuous normalization flows, which is an extension of normalizing flows in neural ODEs. The main contribution of this paper is the use of continuous normals to model the conditional distribution of the fine and coarse images. The method is evaluated on a variety of image datasets and achieves state-of-the-art results. ","This paper proposes a new method for continuous normalizing flows (CNFs) for invertible generative models of images. CNFs are used to generate images from a fine image and a coarse image, where the fine image is generated from a conditional distribution, and the coarse image is collected from a coarse distribution. The method is based on the idea that the fine and coarse images are generated from the same set of samples, and that the invertibility of the coarse and fine images is the same. The authors show that the proposed method can achieve better performance than the state-of-the-art in terms of training time and computational complexity."
4389,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"Label noise PART-OF real - world datasets. robust training techniques USED-FOR DNNs. DNNs USED-FOR corrupted patterns. overfitting USED-FOR corrupted patterns. noisy supervisions USED-FOR model. and training - free solution USED-FOR detect noisy labels. neighborhood information USED-FOR methods. nearby representations USED-FOR local voting. noisy label consensuses USED-FOR local voting. one HYPONYM-OF methods. local voting USED-FOR one. ranking - based approach USED-FOR one. representations USED-FOR ranking - based approach. worst - case error bound EVALUATE-FOR ranking - based method. training - free solutions COMPARE training - based baselines. training - based baselines COMPARE training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - free solutions. synthetic and real - world label noise EVALUATE-FOR training - based baselines. Method is generalization of deep neural networks ( DNNs ). Generic is approach. OtherScientificTerm are noisy labels, and clean label. ","This paper proposes a new method for detecting noisy labels in deep neural networks. The proposed method is based on a ranking-based approach, where the labels are ranked according to their proximity to nearby representations. The authors show that the proposed method can detect noisy labels better than existing methods.   ","This paper proposes a training-free method for detecting noisy labels in deep neural networks (DNNs). The proposed method is based on a ranking-based approach, where each label is ranked by the number of nearby representations. The authors show that the proposed method can detect noisy labels more accurately than existing training-based methods. They also provide a worst-case error bound for their method. "
4405,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"robustness EVALUATE-FOR RL agents. state observations USED-FOR strongest / optimal adversarial perturbations. strongest / optimal adversarial perturbations USED-FOR reinforcement learning ( RL ) agent. Existing works USED-FOR adversarial RL. heuristics - based methods USED-FOR Existing works. attacking method USED-FOR optimal attacks. designed function CONJUNCTION RL - based learner. RL - based learner CONJUNCTION designed function. algorithm COMPARE RL - based works. RL - based works COMPARE algorithm. PA - AD COMPARE RL - based works. RL - based works COMPARE PA - AD. PA - AD HYPONYM-OF algorithm. PA - AD COMPARE attacking methods. attacking methods COMPARE PA - AD. attacking methods USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR Atari and MuJoCo environments. PA - AD USED-FOR adversarial training. empirical robustness EVALUATE-FOR PA - AD. OtherScientificTerm are optimal adversary, optimal attack, agent, large state space, policy perturbation direction, policy perturbation directions, large state spaces, and strong adversaries. Method is RL - based adversary. ","This paper proposes a new adversarial training method for reinforcement learning. The main idea is to train an RL-based learner to find the strongest/optimal adversarial perturbations in the large state space, which are then used to train a policy perturbation direction. The proposed method is based on an existing heuristics-based adversarial learning method, and is shown to outperform existing heuristic-based methods on Atari and MuJoCo.","This paper proposes a new adversarial training method for adversarial reinforcement learning (RL) agents. The main idea is to learn a policy perturbation direction that maximizes the robustness of the agent to the strongest/optimal adversarial perturbations. The authors propose a new algorithm called PA-AD, which is based on a heuristics-based attack method. The proposed method is evaluated on Atari and MuJoCo environments, where it outperforms existing RL-based works."
4421,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"cumulative reward FEATURE-OF monotonous policies. diversity CONJUNCTION novelty. novelty CONJUNCTION diversity. novelty FEATURE-OF policies. diversity FEATURE-OF policies. policy generation workflow USED-FOR diverse and well - performing policies. novelty metric USED-FOR novelty - seeking problem. behavioral novelty FEATURE-OF multi - objective optimization approaches. constrained optimization literature FEATURE-OF interior point method. interior point method USED-FOR Interior Policy Differentiation ( IPD ). Interior Policy Differentiation ( IPD ) HYPONYM-OF policy seeking algorithm. constrained optimization USED-FOR novelty - seeking problem. IPD COMPARE novelty - seeking methods. novelty - seeking methods COMPARE IPD. benchmark environments EVALUATE-FOR IPD. benchmark environments EVALUATE-FOR novelty - seeking methods. Task is problem - solving. Generic are problem, and metric. Method are reinforcement learning algorithms, and learning algorithms. OtherScientificTerm is novelty of generated policies. ","This paper studies the problem of policy generation in reinforcement learning, where the goal is to generate policies that maximize the novelty of generated policies. The authors propose to use a novel metric to measure the novelty in policy generation. The novelty is defined as the difference between the cumulative reward of a policy and a set of policies with high novelty. The proposed method is based on the idea of interior point method, which is a constrained optimization method for policy generation, and is shown to outperform other novelty-seeking methods.   ","This paper proposes a novel policy-seeking algorithm for the problem of novelty-seeking in reinforcement learning. The key idea is to use a new metric to measure the diversity of policies generated by the policy generation process. The novelty of the generated policies is measured by the cumulative reward, which is a measure of how well the policy is performing compared to the other policies. The authors propose a novel method to measure this metric, which they call Interior Policy Differentiation (IPD). They show that IPD can be used to find policies that are diverse and well-performing policies. They also provide a theoretical analysis of IPD."
4437,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"accuracy EVALUATE-FOR automatic speech recognition. quality of speech USED-FOR human perception. Reverberation FEATURE-OF audio reflecting off surfaces. audio modality USED-FOR reverberation. reverberation effects FEATURE-OF audio stream. real - world 3D scans of homes FEATURE-OF realistic acoustic renderings of speech. large - scale dataset EVALUATE-FOR task. realistic acoustic renderings of speech USED-FOR large - scale dataset. speech enhancement CONJUNCTION speech recognition. speech recognition CONJUNCTION speech enhancement. speech recognition CONJUNCTION speaker identification. speaker identification CONJUNCTION speech recognition. it COMPARE audio - only methods. audio - only methods COMPARE it. simulated and real imagery USED-FOR speech enhancement. approach USED-FOR speech enhancement. approach USED-FOR speech recognition. simulated and real imagery USED-FOR approach. OtherScientificTerm are audio - visual observations, visual environment, room geometry, speaker location, visual scene, and room acoustics. Method is end - to - end approach. ","This paper proposes a method to improve the quality of speech recognition by modeling the reverberation effects of audio reflections in a room. The proposed method is an end-to-end approach that combines audio enhancement, speech recognition, and speaker identification. The method is evaluated on a large-scale dataset of 3D acoustic renderings of speech and achieves state-of-the-art performance.",This paper proposes an end-to-end approach to improve the accuracy of speech recognition and speech enhancement. The key idea is to combine audio-visual observations and audio modality to capture the reverberation effects of the audio stream. The authors show that their method is able to achieve better accuracy than audio-only speech recognition on a large-scale dataset of 3D audio recordings of real-world 3D scenes.
4453,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"model USED-FOR extrapolation. methods USED-FOR extrapolation. position representation method USED-FOR extrapolation. Attention with Linear Biases ( ALiBi ) HYPONYM-OF position method. it USED-FOR query - key attention scores. positional embeddings USED-FOR word embeddings. perplexity EVALUATE-FOR sinusoidal position embedding model. method COMPARE sinusoidal position embedding model. sinusoidal position embedding model COMPARE method. perplexity EVALUATE-FOR method. it COMPARE position methods. position methods COMPARE it. WikiText-103 benchmark EVALUATE-FOR it. WikiText-103 benchmark EVALUATE-FOR position methods. Method are transformer model, and ALiBi. OtherScientificTerm are memory, and recency. ","This paper proposes a novel position representation method for word embeddings. The proposed method, Attention with Linear Biases (ALiBi), is based on a transformer-based attention model. The authors show that the proposed method is able to improve the query-key attention scores of word embedding models. The method is evaluated on the WikiText-103 benchmark and achieves state-of-the-art performance.","This paper proposes a new position representation method, Attention with Linear Biases (ALiBi), to improve query-key attention scores for word embeddings. ALiBi is based on a transformer model, which is used to learn a position embedding for each query key. The proposed method is evaluated on the WikiText-103 benchmark, where it outperforms other position representation methods. "
4469,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,unconstrained max - min form FEATURE-OF multi - objective dynamic regret. multi - objective dynamic regret PART-OF Multi - Objective Online Convex Optimization. regret USED-FOR zero - order multi - objective bandit setting. it COMPARE regret. regret COMPARE it. vanilla min - norm solver CONJUNCTION L1 - regularized min - norm solver. L1 - regularized min - norm solver CONJUNCTION vanilla min - norm solver. variants USED-FOR composite gradient. Online Mirror Multiple Descent algorithm USED-FOR composite gradient. variants USED-FOR Online Mirror Multiple Descent algorithm. L1 - regularized min - norm solver USED-FOR composite gradient. vanilla min - norm solver USED-FOR composite gradient. regret bounds FEATURE-OF variants. lower bound FEATURE-OF L1 - regularized variant. Task is multi - objective online learning. Method is first - order gradient - based methods. Generic is algorithm. ,"This paper studies the multi-objective online convex optimization problem with dynamic regret in the max-min form. In particular, the authors show that the regret is in the form of a zero-order bandit, where the regret can be expressed in terms of a max-max form. They show that this is the case in the online mirror multiple descent (OMMD) setting. They also show that it is possible to achieve a regret of $O(1/\epsilon^2)$ in this bandit setting.   ","This paper studies the multi-objective dynamic regret in the zero-order bandit setting. The main contribution of the paper is to provide a lower bound on the regret of a multi- objective online learning algorithm. The upper bound is based on the max-min form of the regret, and the lower bound of the L1-regularized variant of the algorithm is derived. The regret of this algorithm is shown to be lower than that of a vanilla solver. "
4485,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"intelligence USED-FOR real - world problems. Learning continually PART-OF intelligence. simple scenarios CONJUNCTION low - dimensional benchmarks. low - dimensional benchmarks CONJUNCTION simple scenarios. generative models USED-FOR replay patterns. simplified assumptions USED-FOR it. generative models USED-FOR Generative replay. simple scenarios HYPONYM-OF simplified assumptions. low - dimensional benchmarks HYPONYM-OF simplified assumptions. OtherScientificTerm are catastrophic forgetting, and learning experiences. Method are continual learning, replay approach, and generative replay approaches. Metric is classification accuracy. Generic are they, and approach. Material are ImageNet-1000, and high - dimensional data. ","This paper proposes a generative replay approach for continual learning, where the goal is to sample from a set of training examples and use them to improve the performance of the current model. The approach is based on the observation that there is a trade-off between the quality of the new examples and the performance on the old ones. The authors propose to use generative models to model this tradeoff, and show that the proposed approach outperforms baselines on ImageNet-1000.   ","This paper proposes a generative replay approach for continual learning. The main idea is to use generative models to predict the replay patterns of a dataset, and then use them to learn a replay model that can be applied to the dataset. The authors show that this approach can be used to improve the performance of the model on a variety of datasets, including ImageNet-1000. They also show that it can improve the generalization ability of the generative model. "
4501,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"modularity maximization CONJUNCTION NCut minimization. NCut minimization CONJUNCTION modularity maximization. graph partitioning ( GP ) FEATURE-OF NP - hard combinatorial optimization problems. NP - hard combinatorial optimization problems USED-FOR network systems. NCut minimization HYPONYM-OF NP - hard combinatorial optimization problems. NCut minimization HYPONYM-OF graph partitioning ( GP ). modularity maximization HYPONYM-OF NP - hard combinatorial optimization problems. modularity maximization HYPONYM-OF graph partitioning ( GP ). machine learning techniques USED-FOR Existing methods. heuristic strategies USED-FOR GP methods. inductive graph partitioning ( IGP ) framework USED-FOR NP - hard challenge. transductive GP methods COMPARE inductive graph partitioning ( IGP ) framework. inductive graph partitioning ( IGP ) framework COMPARE transductive GP methods. inductive graph partitioning ( IGP ) framework USED-FOR graphs. dual graph neural network USED-FOR IGP. historical graph snapshots USED-FOR dual graph neural network. model USED-FOR graphs. model USED-FOR online GP. quality CONJUNCTION efficiency. efficiency CONJUNCTION quality. graphs USED-FOR online GP. IGP USED-FOR online GP. IGP HYPONYM-OF framework. graphs USED-FOR online GP. benchmarks EVALUATE-FOR IGP. efficiency EVALUATE-FOR state - of - the - art baselines. IGP COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE IGP. benchmarks EVALUATE-FOR state - of - the - art baselines. efficiency EVALUATE-FOR IGP. OtherScientificTerm are NP - hardness, quality degradation, and optimization. Method is GP. Metric is complexity. Generic is system. ","This paper studies the problem of graph partitioning (GP), a combinatorial optimization problem in which the goal is to maximize the modularity of a set of graphs. The authors propose a novel inductive graph partition (IGP) framework to solve this problem. The main idea is to use a dual graph neural network to model the history of the graphs, which is then used to train an online GP model. Theoretical analysis is provided to show that the proposed method is efficient in terms of the quality of the generated graphs. Empirical results show the effectiveness of the proposed IGP method.","This paper proposes a novel inductive graph partitioning (IGP) framework for the problem of NP-hard combinatorial optimization problems. The authors propose a dual graph neural network (GPNN) model that learns to partition graphs using historical graph snapshots. They show that the proposed method outperforms existing methods in terms of quality, efficiency, and quality degradation."
4517,SP:ad28c185efd966eea1f44a6ff474900812b4705a,Multiresolution Equivariant Graph Variational Autoencoders ( MGVAE ) HYPONYM-OF hierarchical generative model. hierarchical generative model USED-FOR graphs. multiresolution and equivariant manner USED-FOR graphs. higher order message passing USED-FOR graph. MGVAE USED-FOR graph. MGVAE USED-FOR resolution level. higher order message passing USED-FOR resolution level. higher order message passing USED-FOR MGVAE. MGVAE USED-FOR hierarchical generative model. hierarchical generative model USED-FOR hierarchy of coarsened graphs. node ordering FEATURE-OF framework. general graph generation CONJUNCTION molecular generation. molecular generation CONJUNCTION general graph generation. unsupervised molecular representation learning USED-FOR molecular properties. molecular generation CONJUNCTION unsupervised molecular representation learning. unsupervised molecular representation learning CONJUNCTION molecular generation. link prediction CONJUNCTION graph - based image generation. graph - based image generation CONJUNCTION link prediction. MGVAE COMPARE generative tasks. generative tasks COMPARE MGVAE. unsupervised molecular representation learning CONJUNCTION link prediction. link prediction CONJUNCTION unsupervised molecular representation learning. citation graphs USED-FOR link prediction. unsupervised molecular representation learning HYPONYM-OF generative tasks. general graph generation HYPONYM-OF generative tasks. graph - based image generation HYPONYM-OF generative tasks. link prediction HYPONYM-OF generative tasks. molecular generation HYPONYM-OF generative tasks. Generic is it. OtherScientificTerm is hierarchy of latent distributions. ,"This paper proposes a hierarchical generative model for graph generation. The proposed model is based on the idea of multiresolution equivariant graph variational autoencoders (MGVAE), which is an extension of GVAE. The main idea is to use a message-passing network to model the coarsened graphs as a hierarchy of latent distributions.   The proposed method is evaluated on graph-based image generation and link prediction tasks. The experimental results show that the proposed method outperforms the baselines. ","This paper proposes a hierarchical generative model for graph generation and graph-based image generation. The main idea is to use a multiresolution and equivariant manner to generate graphs in a hierarchical manner. The proposed method is based on a message passing mechanism, where the message passing is done in a higher order message passing. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization and performance on a variety of graph generation tasks."
4533,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"Nonlinear ICA HYPONYM-OF machine learning. framework USED-FOR nonlinear ICA. volume - preserving transformation FEATURE-OF mixing function. artificial data CONJUNCTION synthesized images. synthesized images CONJUNCTION artificial data. synthesized images EVALUATE-FOR theory. volume - preserving flow - based models USED-FOR framework. artificial data EVALUATE-FOR theory. framework USED-FOR interpretable features. real - world images EVALUATE-FOR framework. OtherScientificTerm are independent components ( sources ), and temporal structure. Generic are sources, and methods. ","This paper studies the problem of nonlinear ICA, where the mixing function is a volume-preserving transformation. The authors propose a novel flow-based model for nonlinear mixing functions. Theoretical analysis is provided to show that the mixing functions can be decomposed into independent components (source) and a mixing function (mixture of sources). The authors show that this decomposition can be achieved by a volume preserving flow model. Experiments on synthetic and real-world data demonstrate the effectiveness of the proposed method. ","This paper proposes a new framework for nonlinear ICA (i.e., the mixing function of a mixing function is a volume-preserving transformation) that can be used to learn a volume preserving flow-based model. Theoretical results on synthetic data and real-world data demonstrate the effectiveness of the proposed method. "
4549,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"Graph convolutional networks USED-FOR representation learning of networked data. single message passing strategy USED-FOR they. sharing scheme USED-FOR filters. sharing scheme USED-FOR spectral graph convolutional operators. sharing scheme USED-FOR filters. BankGCN HYPONYM-OF graph convolution operator. sharing scheme USED-FOR spectral methods. BankGCN USED-FOR multi - channel signals. adaptive filters USED-FOR BankGCN. graphs USED-FOR multi - channel signals. filters PART-OF filter bank. filters PART-OF subspaces. frequency response FEATURE-OF filters. filter bank CONJUNCTION signal decomposition. signal decomposition CONJUNCTION filter bank. filter bank USED-FOR spectral characteristics. signal decomposition USED-FOR spectral characteristics. spectral characteristics FEATURE-OF graph data. benchmark graph datasets EVALUATE-FOR BankGCN. Method are message passing graph convolutional networks ( MPGCNs ), and MPGCNs. OtherScientificTerm are low - frequency information, graph features, and single ‘ low - pass ’ features. Task is overfitting problems. Generic is compact architecture. ","This paper proposes a new message-passing graph convolutional network (MPGCN) based on the idea of sharing low-frequency information across multiple subspaces of the graph. The idea is that the low-pass filters are shared across multiple filters in the filter bank, and the high-pass ones are shared among subspacings of the filters. The authors show that the proposed MPGCN achieves state-of-the-art performance on several benchmark graph datasets. ","This paper proposes a new graph convolutional network called BankGCN, which is based on the message-passing GNN (MPGCN). The main idea is to use a single message passing strategy to share the low-frequency information between the graph features. The authors show that the proposed method can be used to improve the performance of graph convolutions. The proposed method is evaluated on a variety of graph datasets. "
4565,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"pretrain - finetune paradigm USED-FOR deep learning. model USED-FOR downstream tasks. self - supervised pre - training COMPARE supervised pre - training. supervised pre - training COMPARE self - supervised pre - training. transferability EVALUATE-FOR self - supervised pre - training. supervised methods USED-FOR pre - training stage. supervised pretraining model USED-FOR downstream tasks. transferability EVALUATE-FOR supervised pre - training methods. It USED-FOR overfitting upstream tasks. method USED-FOR large datasets. state - of - the - art methods USED-FOR supervised and self - supervised pre - training. LOOK COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LOOK. LOOK USED-FOR supervised and self - supervised pre - training. multiple downstream tasks EVALUATE-FOR LOOK. Material is ImageNet. OtherScientificTerm are negligence of valuable intra - class semantic difference, visual contents, multi - mode distribution, and intra - class difference. Generic is methods. Task is overfit of upstream tasks. Method is supervised pre - training method. ","This paper proposes a self-supervised pre-training method to improve the transferability of downstream tasks in deep learning. The proposed method, called LOOK, is based on the notion of intra-class semantic difference, which is defined as the difference between the visual contents of a set of images from different classes. The authors show that the proposed method can improve the performance on multiple downstream tasks. They also show that their method can be used for large datasets.","This paper proposes a new pretrain-finetune paradigm for self-supervised and supervised pre-training. The main idea of the paper is to use a multi-modal distribution for the downstream tasks to improve the transferability between downstream tasks. The authors show that the proposed method, called LOOK, outperforms the state-of-the-art methods in terms of transferability across multiple downstream tasks on ImageNet. They also show that their method can be used for large datasets."
4581,SP:2b3916ba24094c286117126e11032820f8c7c50a,"wrinkling of cheeks CONJUNCTION formation of dimples. formation of dimples CONJUNCTION wrinkling of cheeks. Morphable Models ( 3DMMs ) USED-FOR fine details. PCA - based representations USED-FOR fine details. FaceDet3D HYPONYM-OF method. single image geometric facial details USED-FOR method. vertex displacement map USED-FOR facial details. method USED-FOR facial geometric details. FDS USED-FOR detailed geometry. hallucinated details PART-OF smooth proxy geometry. facial details FEATURE-OF detailed geometry. Neural Rendering USED-FOR detailed geometry. zoom - in FEATURE-OF predicted facial details. Predicted Facial Detail CONJUNCTION Render of Predicted Facial Detail. Render of Predicted Facial Detail CONJUNCTION Predicted Facial Detail. OtherScientificTerm are Facial Expressions, 3D face geometry, smile, edit expressions, and Proxy Shading. Method is Neural Renderer. Task is Facial detail hallucination and rendering. ","This paper proposes a method for predicting 3D geometric facial details from a single image. The proposed method is based on Proxy Shading, which is an extension of FaceDet3D, a recent method for generating 3D face geometry from single images. The method is evaluated on a variety of facial expression tasks, including facial wrinkles, dimples, and smiles.   ","This paper proposes a new method for 3D face rendering based on 3D Morphable Models (3DMMs). The proposed method, FaceDet3D, is based on the FDS-based FaceDet model, which maps the 3D geometric facial details of a single image to a smooth proxy geometry. The method is trained using a combination of 3D Rendering and Proxy Shading. The proposed FaceDet-3D method is evaluated on a variety of datasets, including CIFAR-10, Cifar-100, and CIFar-200. "
4597,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"neural representations USED-FOR NLP models. neural representations CONJUNCTION linguistic factors. linguistic factors CONJUNCTION neural representations. syntactic roles PART-OF factors. latent variables CONJUNCTION realizations of syntactic roles. realizations of syntactic roles CONJUNCTION latent variables. attention USED-FOR deep probabilistic generative model. Attention - Driven Variational Autoencoder ( ADVAE ) HYPONYM-OF probabilistic model. attention USED-FOR ADVAEs. evaluation protocol EVALUATE-FOR disentanglement. disentanglement FEATURE-OF realizations of syntactic roles. evaluation protocol USED-FOR realizations of syntactic roles. attention maxima CONJUNCTION latent variable perturbations. latent variable perturbations CONJUNCTION attention maxima. latent variable perturbations USED-FOR decoder. attention maxima USED-FOR encoder. latent variable perturbations USED-FOR protocol. attention maxima USED-FOR protocol. ADVAE USED-FOR syntactic roles. sequence VAEs CONJUNCTION Transformer VAEs. Transformer VAEs CONJUNCTION sequence VAEs. ADVAE COMPARE Transformer VAEs. Transformer VAEs COMPARE ADVAE. ADVAE COMPARE sequence VAEs. sequence VAEs COMPARE ADVAE. raw English text PART-OF SNLI dataset. latent variables USED-FOR realizations of syntactic roles. Generic is they. OtherScientificTerm are decomposition of predicative structures, and supervision. Method is Transformer - based machine translation models. Task are disentanglement of syntactic roles, and unsupervised controllable content generation. ",This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The authors propose to use attention to disentangle the syntactic roles in the generated text from the latent variables. They evaluate the performance of ADVAE on the SNLI dataset and show that it achieves better disentanglement than the state-of-the-art Transformer models.,This paper proposes a probabilistic generative model for unsupervised controllable content generation. The model is based on attention-driven variational autoencoder (ADVAE) with attention as a key component. The authors show that ADVAE can disentanglement the syntactic roles of the input text. They also show that the model is able to disentangle the realizations of syntactic role and the latent variables of the text.
4613,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"Discovering successful coordinated behaviors PART-OF Multi - Agent Reinforcement Learning ( MARL ). joint action space USED-FOR it. mechanism USED-FOR sufficient exploration and coordination. framework USED-FOR coordination protocols. sparse rewards CONJUNCTION partial observability. partial observability CONJUNCTION sparse rewards. StarCraft Multi - Agent Challenge HYPONYM-OF exploration scheme. exploration scheme USED-FOR complex cooperative strategies. methods COMPARE baselines. baselines COMPARE methods. Method is intrinsic motivation functions. OtherScientificTerm are agents ’ interactions, and counterfactual rollouts. Generic is approach. ","This paper proposes a novel exploration strategy for multi-agent reinforcement learning in the StarCraft Multi-Agent Challenge (MAMC) setting. The proposed method is based on the observation-exploration framework, where agents are encouraged to explore the joint action space in order to maximize the mutual information between the agents. The authors show that this exploration strategy can be used in conjunction with counterfactual rollouts to improve the performance of the agent.   ","This paper proposes a new exploration strategy for multi-agent reinforcement learning (MARL) in the StarCraft Multi-Agent Challenge (MAMC) setting. The main idea is to learn a joint action space in which agents can explore and coordinate their actions in order to maximize exploration and coordination. The exploration strategy is based on the idea of counterfactual rollouts, where agents are encouraged to explore the action space as well as observe the actions of the other agents. The proposed strategy is evaluated on the StarCraft MAMC dataset and compared with a variety of baselines."
4629,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"fine - tuning approaches COMPARE editing algorithms. editing algorithms COMPARE fine - tuning approaches. Gradient Decomposition ( MEND ) USED-FOR Model Editor Networks. Model Editor Networks USED-FOR post - hoc editing. MEND USED-FOR gradient. low - rank decomposition of the gradient USED-FOR transformation. low - rank decomposition of the gradient USED-FOR gradient. fine - tuning USED-FOR gradient. low - rank decomposition of the gradient USED-FOR MEND. MEND USED-FOR edits. edits USED-FOR pre - trained model. GPU USED-FOR MEND. MEND USED-FOR model editing. approach USED-FOR model editing. Method are large pre - trained models, large neural networks, and small auxiliary editing networks. Generic are models, and model. OtherScientificTerm are targeted edits, and pre - trained model ’s behavior. ","This paper proposes a method for post-hoc model editing in large pre-trained models. The proposed method is based on a low-rank decomposition of the gradient of the training loss, which is then used to select edits to be made to the model. The method is evaluated on a variety of image classification tasks and compared with a number of existing methods.  ","This paper proposes a method for post-hoc model editing for large pre-trained models. The method is based on a low-rank decomposition of the gradient of the model, which can be used for fine-tuning the model. The authors show that the proposed method can be applied to a variety of editing tasks, and that it can be combined with a number of small auxiliary editing networks. The proposed method is shown to be competitive with the state-of-the-art."
4645,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,learning abilities CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION learning abilities. artificial neural networks CONJUNCTION physical and structural knowledge. physical and structural knowledge CONJUNCTION artificial neural networks. learning abilities FEATURE-OF artificial neural networks. FINN USED-FOR constituents of partial differential equations ( PDEs ). numerical simulation USED-FOR physical and structural knowledge. artificial neural networks USED-FOR FINN. learning abilities PART-OF FINN. oneand two - dimensional PDEs EVALUATE-FOR FINN. modeling accuracy CONJUNCTION outof - distribution generalization ability. outof - distribution generalization ability CONJUNCTION modeling accuracy. initial and boundary conditions FEATURE-OF outof - distribution generalization ability. modeling accuracy EVALUATE-FOR FINN. outof - distribution generalization ability EVALUATE-FOR FINN. diffusion - sorption HYPONYM-OF oneand two - dimensional PDEs. FINN COMPARE physics - aware models. physics - aware models COMPARE FINN. FINN COMPARE machine learning. machine learning COMPARE FINN. machine learning CONJUNCTION physics - aware models. physics - aware models CONJUNCTION machine learning. FINN COMPARE calibrated physical model. calibrated physical model COMPARE FINN. calibrated physical model USED-FOR sparse real - world data. sparse real - world data USED-FOR FINN. generalization abilities EVALUATE-FOR FINN. diffusionsorption scenario FEATURE-OF sparse real - world data. Task is spatiotemporal advection - diffusion processes. OtherScientificTerm is unknown retardation factor. ,This paper proposes a method for learning the constituents of partial differential equations (PDEs) from numerical simulations. The method is based on a neural network architecture that is trained to predict the initial and boundary conditions of the PDEs. The authors show that the model is able to generalize well to out-of-distribution generalization in sparse data. The model is shown to be able to learn from sparse real-world data. ,This paper presents a new method for learning the constituents of partial differential equations (PDEs) from numerical simulation. The authors propose a method called Finetuned Finetuning Network (Finetuned) to learn constituents of PDEs from numerical simulations. The method is based on a neural network that learns the initial and boundary conditions of the PDE constituents. The main contributions of the paper are: (1) The authors show that the method is able to generalize well to sparse real-world data with out-of-distribution generalization ability. (2) The method can generalize to one-dimensional and two-dimensional PDE components. (3) They show that it can generalise well to a spatiotemporal advection-diffusion process. 
4661,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"unsupervised machine learning ( ML ) models CONJUNCTION representations of computer programs. representations of computer programs CONJUNCTION unsupervised machine learning ( ML ) models. representations of computer programs USED-FOR representations of computer programs. unsupervised machine learning ( ML ) models USED-FOR representations of computer programs. abstract syntax tree ( AST)-related information CONJUNCTION runtime information. runtime information CONJUNCTION abstract syntax tree ( AST)-related information. brain representations USED-FOR static and dynamic properties of code. runtime information HYPONYM-OF static and dynamic properties of code. abstract syntax tree ( AST)-related information HYPONYM-OF static and dynamic properties of code. brain representations USED-FOR representations. representations USED-FOR ML models. Material is Python code. Metric is complexity. Method are Multiple Demand system, and machine learned representations of code. OtherScientificTerm is code. ","This paper studies the problem of learning representations of programs from code in the Multiple Demand system. The authors propose a method to learn representations of code by combining abstract syntax tree (AST) information and runtime information. They show that the learned representations can capture both static and dynamic properties of code, and that they can be used to improve the performance of ML models trained on code.  ","This paper proposes a method for learning representations of Python code that can be used to improve the performance of unsupervised machine learning (ML) models. The proposed method is based on the Multiple Demand system (MDS), which is a multi-task multi-objective learning (MOL) model. The authors show that the MOL model can learn representations of code that capture both static and dynamic properties of code. They also show that it is possible to learn representations for code that are more expressive than those learned by ML models."
4677,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,phoneme decoder CONJUNCTION mel - spectrogram synthesizer. mel - spectrogram synthesizer CONJUNCTION phoneme decoder. speech encoder CONJUNCTION phoneme decoder. phoneme decoder CONJUNCTION speech encoder. mel - spectrogram synthesizer CONJUNCTION attention module. attention module CONJUNCTION mel - spectrogram synthesizer. attention module PART-OF Translatotron 2. mel - spectrogram synthesizer PART-OF Translatotron 2. phoneme decoder PART-OF Translatotron 2. speech encoder PART-OF Translatotron 2. Translatotron 2 COMPARE Translatotron. Translatotron COMPARE Translatotron 2. babbling CONJUNCTION long pause. long pause CONJUNCTION babbling. translation quality CONJUNCTION predicted speech naturalness. predicted speech naturalness CONJUNCTION translation quality. robustness EVALUATE-FOR predicted speech. long pause HYPONYM-OF over - generation. babbling HYPONYM-OF over - generation. predicted speech naturalness EVALUATE-FOR Translatotron 2. robustness EVALUATE-FOR Translatotron 2. translation quality EVALUATE-FOR Translatotron. translation quality EVALUATE-FOR Translatotron 2. model USED-FOR production deployment. Translatotron COMPARE it. it COMPARE Translatotron. method CONJUNCTION concatenation - based data augmentation. concatenation - based data augmentation CONJUNCTION method. Material is translated speech. Method is Translatotron 2 model. ,"This paper proposes a new Translatotron model that combines a phoneme decoder, a speech encoder, and a mel-spectrogram synthesizer to improve the performance of Translatron. The proposed model is evaluated on a variety of tasks, including babbling, long-paired speech, and long-silence. The results show that the proposed model achieves better performance than the original Translateron model on these tasks.","This paper presents Translatotron 2, a new model for speech translation. The model consists of a phoneme decoder, a speech encoder, and a mel-spectrogram synthesizer. The proposed model is evaluated on a variety of tasks, including long-paired speech, babbling, long-silence, and babbling with long pauses. Translatron 2 is shown to be more robust than the state-of-the-art Translatro model.  "
4693,SP:296102e60b842923c94f579f524fa1147328ee4b,"attribute - based representations USED-FOR concept learning. zeroshot learning HYPONYM-OF attribute - based learning paradigms. supervised learning COMPARE selfsupervised pre - training. selfsupervised pre - training COMPARE supervised learning. predictability of test attributes USED-FOR model. predictability of test attributes USED-FOR generalization ability. generalization ability EVALUATE-FOR model. OtherScientificTerm are Semantic concepts, mappings, and random splits of the attribute space. Task is rapid learning of attributes. Method is few - shot learning of semantic classes. ","This paper studies the problem of few-shot learning of semantic concepts in the attribute-based learning paradigms. In particular, the authors propose to use zeroshot learning, which is a variant of self-supervised pre-training, to improve the generalization ability of the model. The authors show that the predictability of test attributes can help the model to better generalize to unseen attributes.   The main contributions of the paper are:  - The authors propose a method for zeroth-shot concept learning based on attribute learning.  - They show that zerot learning can improve the performance of the proposed method. - The proposed method is shown to outperform the state-of-the-art methods in the experiments.","This paper proposes a new method for few-shot learning of semantic classes based on zeroshot learning. The key idea is to learn a set of attributes that can be used to predict the test attributes, and then use these attributes to train a self-supervised pre-trained model that can generalize well across different attributes. The method is evaluated on a variety of datasets, and it is shown that the proposed method outperforms the state-of-the-art."
4709,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,relative entropy gradient sampler ( REGS ) USED-FOR sampling from unnormalized distributions. particle method USED-FOR nonlinear transforms. REGS HYPONYM-OF particle method. gradient flow USED-FOR path of probability distributions. path of probability distributions USED-FOR reference distribution. density of evolving particles CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density of evolving particles. density ratios FEATURE-OF density of evolving particles. density ratios CONJUNCTION unnormalized target density. unnormalized target density CONJUNCTION density ratios. density ratios FEATURE-OF velocity fields. velocity fields FEATURE-OF ODE system. ODE system USED-FOR It. particle evolution USED-FOR ODE system. nonparametric approach USED-FOR logarithmic density ratio. neural networks USED-FOR nonparametric approach. REGS COMPARE sampling methods. sampling methods COMPARE REGS. multimodal 1D and 2D mixture distributions CONJUNCTION Bayesian logistic regression. Bayesian logistic regression CONJUNCTION multimodal 1D and 2D mixture distributions. real datasets EVALUATE-FOR Bayesian logistic regression. Method is Wasserstein gradient flow of relative entropy. ,"This paper proposes a particle method for sampling from unnormalized distributions with nonlinear transforms. The method is based on the relative entropy gradient sampler (REGS), which is a non-parametric approach to sampling from the reference distribution. The density of evolving particles and the density of the target density are estimated using the density ratios of the ODE system. The authors show that REGS can be used to sample from multimodal 1D and 2D mixture distributions and Bayesian logistic regression.","This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the relative entropy gradient sampler (REGS), which is a nonparametric approach to sampling from an ODE system. The authors show that REGS can be used to sample from a range of probability distributions with respect to the reference distribution. They also show that the method can be applied to Bayesian logistic regression. "
4725,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,quantum machine learning techniques USED-FOR classical image classification. quantum neural network USED-FOR inference. qubits USED-FOR encoding schemes. quantum systems USED-FOR framework. encoding mechanism USED-FOR approach. accuracy EVALUATE-FOR classical neural networks. framework COMPARE classical neural networks. classical neural networks COMPARE framework. personal laptop FEATURE-OF MNIST dataset. accuracy EVALUATE-FOR framework. quantum computers CONJUNCTION classical simulation. classical simulation CONJUNCTION quantum computers. work USED-FOR quantum machine learning and classification. classical datasets USED-FOR quantum machine learning and classification. OtherScientificTerm is quantum states. Generic is technique. ,This paper proposes a quantum neural network for image classification. The proposed method is based on the idea of using a quantum state encoding scheme to encode information from a quantum system. The authors show that this encoding scheme is computationally efficient and can be applied to classical image classification tasks. The method is evaluated on MNIST and CIFAR-10 datasets.  ,"This paper proposes a quantum neural network (QNN) for quantum image classification. The main idea is to use a quantum encoding scheme to encode the quantum states of an image, and then use a classical neural network to perform the inference. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy on the MNIST dataset. "
4741,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"data privacy FEATURE-OF face recognition. framework USED-FOR federated learning face recognition. communicating auxiliary and privacy - agnostic information USED-FOR framework. communicating auxiliary and privacy - agnostic information USED-FOR federated learning face recognition. Differentially Private Local Clustering ( DPLC ) mechanism USED-FOR sanitized clusters. local class centers USED-FOR sanitized clusters. consensus - aware recognition loss USED-FOR global consensuses. IJB - B CONJUNCTION IJB - C. IJB - C CONJUNCTION IJB - B. large - scale dataset EVALUATE-FOR method. Method are federated learning ( FL ) paradigm, FL methods, and PrivacyFace. Generic is task. Task is recognition. OtherScientificTerm are privacy leakage, privacy - utility paradox, discriminative features, and lightweight overhead. "," in federated face recognition. This paper proposes a new federated learning approach for face recognition, called PrivacyFace, which aims to improve privacy-utility trade-off in the face recognition setting. The proposed approach is based on two components: (1) Differentially Private Local Clustering (DPLC) mechanism to sanitize the local class centers and (2) Consensus-aware recognition loss to improve the global consensuses. The DPLC mechanism is shown to be effective in reducing the privacy leakage.  ", for federated learning face recognition. This paper proposes a new privacy-aware federated face recognition framework called PrivacyFace. The proposed framework is based on the Differentially Private Local Clustering (DPLC) mechanism to sanitize the local class centers. The authors also propose a consensus-aware recognition loss to ensure global consensuses. 
4757,SP:408d9e1299ee05b89855df9742b608626692b40d,Transfer - learning methods USED-FOR data - scarce target domain. data - rich source domain USED-FOR model. model USED-FOR Transfer - learning methods. strategy COMPARE method. method COMPARE strategy. fine - tuning USED-FOR model. linear probing USED-FOR intermediate layers. classification head USED-FOR target - domain. features USED-FOR classification head. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE fine - tuning. fine - tuning COMPARE Head2Toe. Visual Task Adaptation Benchmark ( VTAB ) EVALUATE-FOR Head2Toe. Head2Toe COMPARE Head2Toe. Head2Toe COMPARE Head2Toe. fine - tuning USED-FOR Head2Toe. Head2Toe USED-FOR out - of - distribution transfer. Generic is source model. OtherScientificTerm is pretrained layers. ,"This paper proposes a method for transfer learning from source domain to target domain. The main idea is to use the classification head from the source domain as a pre-trained intermediate layer for the target domain, and use linear probing for the intermediate layers. The proposed method, called Head2Toe, is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows superior performance compared to fine-tuning. ",This paper proposes a new method for transfer learning from source domain to target domain. The main idea is to use linear probing to fine-tune the intermediate layers of the source model. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows that it outperforms the state-of-the-art.
4773,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,dynamics models USED-FOR model - based planning. Dynamics models USED-FOR image - based games. fully observable states FEATURE-OF image - based games. models USED-FOR Text - Based Games ( TBGs ). noisy text observations FEATURE-OF partially observable states. planning algorithms USED-FOR decision - making problems. planning algorithms USED-FOR text domains. text domains USED-FOR decision - making problems. OOTD USED-FOR memory graph. OOTD model USED-FOR beliefs of object states. OOTD model USED-FOR dynamics. independently parameterized transition layers USED-FOR beliefs of object states. variational objectives USED-FOR stochasticity of predicted dynamics. object - supervised and self - supervised settings USED-FOR variational objectives. OOTD - based planner COMPARE model - free baselines. model - free baselines COMPARE OOTD - based planner. sample efficiency CONJUNCTION running scores. running scores CONJUNCTION sample efficiency. running scores EVALUATE-FOR OOTD - based planner. sample efficiency EVALUATE-FOR OOTD - based planner. running scores EVALUATE-FOR model - free baselines. sample efficiency EVALUATE-FOR model - free baselines. OtherScientificTerm is object - irrelevant information. ,"This paper proposes a novel method for planning in text-based games. The method is based on object-observable dynamics models (OOTD), which are used to model the dynamics of a set of partially observable states in a text game. The OOTD model is trained using a self-supervised self-similarity loss, which is used to learn a model-based model of the state representation of an object. The model is then used to predict the future state of the game, and the model is used for planning. The proposed method is shown to outperform existing methods in terms of sample efficiency and running score.","This paper proposes a novel model-based planning method for text-based games (TBGs). The model is based on object-to-object dynamics (OOTD) model, which is used to model the dynamics of the environment. The OOTD model is trained to predict the dynamics in a memory graph, and the transition layers are parameterized to capture the beliefs of object states. Experiments are conducted on both object-supervised and self-supervision settings to demonstrate the effectiveness of the proposed method."
4789,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"cost - sensitive loss FEATURE-OF label taxonomy. tractable method USED-FOR hierarchical learning problem. hierarchical cost - sensitive loss CONJUNCTION layer - wise abstaining losses. layer - wise abstaining losses CONJUNCTION hierarchical cost - sensitive loss. bijective mapping USED-FOR hierarchical cost - sensitive loss. symmetry assumptions USED-FOR bijective mapping. distributionally robust learning framework USED-FOR learningto - abstain problems. large - scale bird dataset CONJUNCTION cell classification problems. cell classification problems CONJUNCTION large - scale bird dataset. LAM COMPARE methods. methods COMPARE LAM. high accuracy regions FEATURE-OF hierarchical cost - sensitive loss. hierarchical cost - sensitive loss EVALUATE-FOR LAM. high accuracy regions EVALUATE-FOR LAM. perclass loss - adjustment heuristic USED-FOR performance profile. perclass loss - adjustment heuristic USED-FOR LAM. cost design USED-FOR user requirements. cost design USED-FOR optimizable cost functions. user requirements CONJUNCTION optimizable cost functions. optimizable cost functions CONJUNCTION user requirements. Task is cost - sensitive hierarchical classification. OtherScientificTerm are hierarchy, cost - sensitive hierarchical loss, non - convexity, and taxonomy. Metric is accuracy. ",This paper proposes a cost-sensitive hierarchical classification method with a bijective mapping between the hierarchical loss and the layer-wise abstaining losses. The proposed method is based on a distributionally robust learning framework for learning to abstain problems. The authors show that the proposed method achieves state-of-the-art performance on bird classification and cell classification tasks. ,This paper proposes a tractable method for learning to abstain from the hierarchical cost-sensitive label taxonomy problem. The proposed method is based on a bijective mapping between the hierarchical and the layer-wise abstaining losses. The authors propose a distributionally robust learning framework to learn the cost function for the learningto abstain problem. They also propose a per-class loss-adjustment heuristic to improve the performance of the proposed method. The experimental results show that their method outperforms the state-of-the-art in terms of high accuracy regions.
4805,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"spatial location CONJUNCTION semantic identity label. semantic identity label CONJUNCTION spatial location. estimating sound sources ’ temporal location CONJUNCTION spatial location. spatial location CONJUNCTION estimating sound sources ’ temporal location. multi - channel sound raw waveforms USED-FOR semantic identity label. multi - channel sound raw waveforms USED-FOR estimating sound sources ’ temporal location. single - scale filter bank USED-FOR sound waveforms. STFT CONJUNCTION LogMel. LogMel CONJUNCTION STFT. they USED-FOR hand - engineered features. limited time - frequency resolution capability FEATURE-OF sound waveforms. parameter tuning USED-FOR they. STFT HYPONYM-OF hand - engineered features. LogMel HYPONYM-OF hand - engineered features. parameter tuning USED-FOR hand - engineered features. frequency resolution FEATURE-OF synperiodic filter. time - frequency resolution trade - off EVALUATE-FOR synperiodic filter. synperiodic filter bank USED-FOR multi - scale perception. synperiodic filter bank USED-FOR downsampled waveform. synperiodic filter bank group USED-FOR dynamic multi - scale time - frequency representation. multi - scale perception USED-FOR synperiodic filter bank group. time and frequency domain advantage FEATURE-OF multi - scale perception. semantic identity label CONJUNCTION spatial location representation. spatial location representation CONJUNCTION semantic identity label. Transformer - like backbone USED-FOR semantic identity label. Transformer - like backbone USED-FOR spatial location representation. parallel soft - stitched branches PART-OF Transformer - like backbone. Transformer - like backbone PART-OF synperiodic filter bank group front - end. direction of arrival estimation task CONJUNCTION physical location estimation task. physical location estimation task CONJUNCTION direction of arrival estimation task. direction of arrival estimation task EVALUATE-FOR framework. physical location estimation task EVALUATE-FOR framework. OtherScientificTerm are complex waveform mixture, periodicity term, and temporal length. Generic are representation, and Existing methods. Method are parameterized synperiodic filter banks, and synperiodic filter banks. Material is raw waveform. ","This paper proposes a method for temporal location estimation from multi-channel sound waveforms. The proposed method is based on the idea of parameterized synperiodic filter banks, which are composed of a sequence of filter banks that are parameterized by a single-scale filter bank. The authors show that the proposed method achieves better time-frequency resolution trade-off compared to existing methods.   ","This paper proposes a new method for multi-scale time-frequency representation of multi-channel sound waveforms. The proposed method is based on a Transformer-like backbone, which is composed of two parts: (1) a soft-stitched backbone for spatial location representation, and (2) two parallel soft- stitched branches for temporal location representation. The authors show that the proposed method outperforms the state-of-the-art in terms of time-frequencies and frequency resolution."
4821,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"self - training USED-FOR model. iterative self - training USED-FOR model. implicit curriculum USED-FOR iterative self - training. method USED-FOR virtual samples. intermediate distributions USED-FOR virtual samples. iterative self - training CONJUNCTION GIFT. GIFT CONJUNCTION iterative self - training. self - training CONJUNCTION iterative self - training. iterative self - training CONJUNCTION self - training. GIFT USED-FOR model. domain adaptation methods USED-FOR GIFT. natural distribution shifts FEATURE-OF benchmarks. benchmarks EVALUATE-FOR iterative self - training. benchmarks EVALUATE-FOR self - training. benchmarks EVALUATE-FOR GIFT. Task is domain adaptation. Method are learning domain invariant representations, and iterative selftraining. Generic is assumptions. OtherScientificTerm is synthetic distribution shifts. ","This paper proposes a self-training method for domain adaptation. The proposed method is based on the idea of generating virtual samples from intermediate distributions, which are then used to train the model on the intermediate distributions. The authors show that the proposed method outperforms the state-of-the-art domain adaptation methods on several benchmarks.   ","This paper proposes a method for learning domain-invariant representations for self-training in the presence of synthetic distribution shifts. The main idea is to use an implicit curriculum to learn representations that are invariant to the synthetic distribution shift. The authors show that this method can be applied to a variety of domain adaptation methods, including GIFT, self-learning, and iterative self- training. They show that the proposed method outperforms the state-of-the-art in terms of performance on a range of benchmarks."
4837,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"representations USED-FOR multi - modal retrieval. recommendation CONJUNCTION search. search CONJUNCTION recommendation. representations USED-FOR applications. search HYPONYM-OF applications. recommendation HYPONYM-OF applications. video titles or audio transcripts USED-FOR video - text retrieval literature. method USED-FOR representations. videos USED-FOR representations. attention - based mechanism USED-FOR model. comments USED-FOR method. video - text retrieval benchmarks EVALUATE-FOR method. Generic is benchmarks. OtherScientificTerm are modalities, and user comments. ","This paper proposes a method for multi-modal video-text retrieval, where the goal is to retrieve videos from multiple modalities, such as video titles, audio transcripts, and user comments. The method is based on an attention-based mechanism, which is used to learn a representation of the modalities. The model is trained using a combination of supervised and unsupervised learning. The proposed method is evaluated on a variety of video retrieval benchmarks.   ","This paper proposes a method for multi-modal video-text retrieval. The authors propose an attention-based mechanism to learn representations for videos and audio transcripts from user comments. The proposed method is evaluated on a variety of video-Text retrieval benchmarks, and it outperforms the state-of-the-art. "
4853,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"policy USED-FOR latent - conditioned trajectories. discriminator USED-FOR distinguishability. trajectories USED-FOR latents. penalization USED-FOR exploration. objective USED-FOR epistemic uncertainty. objective USED-FOR intrinsic reward. intrinsic reward COMPARE pseudocount - based methods. pseudocount - based methods COMPARE intrinsic reward. objective COMPARE pseudocount - based methods. pseudocount - based methods COMPARE objective. DISDAIN USED-FOR skill learning. tabular grid world EVALUATE-FOR DISDAIN. DISDAIN USED-FOR pessimism. OtherScientificTerm are Unsupervised skill learning objectives, extrinsic rewards, inherent pessimism, information gain auxiliary objective, and discriminators. ","This paper proposes a method for unsupervised skill learning that uses an information gain auxiliary objective to penalize the epistemic uncertainty in the latent space. The proposed method, called DISDAIN, is based on the idea of using a discriminator to distinguish between trajectories conditioned on a set of latent variables. The discriminator is trained using an information-theoretic objective. The method is evaluated on a tabular grid world and compared with a number of baselines. ","This paper proposes a method for unsupervised skill learning in which a discriminator is trained to distinguish between latent-conditioned and unlabeled trajectories. The discriminator learns a set of latent trajectories that can be used to train a policy that maximizes the discriminator’s ability to discriminate between latents, and penalizes the policy that does not learn the discriminators’ discriminator. The proposed method, DISDAIN, is motivated by the fact that it is difficult to learn a policy for a skill learning objective that is not motivated by extrinsic rewards. To address this problem, the authors propose an information gain auxiliary objective, which penalizes exploration in a way that encourages the policy to explore the latent-conditional trajectories in a tabular grid world. The authors show that the proposed method outperforms the state-of-the-art pseudocount-based methods in terms of intrinsic reward. They also show that their method is also more pessimistic than the other methods."
4869,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,deep neural networks USED-FOR chemistry. deep neural networks USED-FOR generating molecules. spanning tree CONJUNCTION residual edges. residual edges CONJUNCTION spanning tree. spanning tree USED-FOR molecular graph generation. tree - constructive operations USED-FOR molecular graph connectivity. formulation USED-FOR sparsity of molecular graphs. intermediate graph structure USED-FOR framework. intermediate graph structure USED-FOR construction process. chemical valence rules FEATURE-OF molecular graphs. Transformer architecture USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR tree construction procedure. tree - based relative positional encodings USED-FOR Transformer architecture. Fréchet ChemNet distance CONJUNCTION fragment similarity. fragment similarity CONJUNCTION Fréchet ChemNet distance. validity CONJUNCTION Fréchet ChemNet distance. Fréchet ChemNet distance CONJUNCTION validity. QM9 CONJUNCTION ZINC250k. ZINC250k CONJUNCTION QM9. ZINC250k CONJUNCTION MOSES benchmarks. MOSES benchmarks CONJUNCTION ZINC250k. QM9 EVALUATE-FOR framework. MOSES benchmarks EVALUATE-FOR framework. metrics EVALUATE-FOR framework. fragment similarity EVALUATE-FOR framework. validity EVALUATE-FOR framework. validity HYPONYM-OF metrics. fragment similarity HYPONYM-OF metrics. Fréchet ChemNet distance HYPONYM-OF metrics. STGG USED-FOR penalized LogP value of molecules. Task is maximizing penalized LogP value of molecules. ,"This paper proposes a method for generating molecular graphs based on chemical valence rules. The proposed method is based on a transformer-based approach, where a spanning tree is constructed from a set of residual edges, and a relative positional encoder is used to encode the relative position of each node in the graph. The resulting graph is then used to generate the final molecule. The authors show that the proposed method achieves state-of-the-art performance on the QM9 and ZINC benchmarks.","This paper proposes a method for generating molecular graphs based on spanning trees. The proposed method is based on the Transformer architecture, which uses a relative positional encodings to construct a tree-based relative positional encoding of the molecular graph structure. The authors show that the proposed method can be applied to a wide range of chemical valence rules, and show that it can be used to generate molecules with high-quality properties. The method is evaluated on a variety of synthetic and real-world datasets."
4885,SP:3a19340d6af65e3f949dda839a6d233369891c46,"image generation CONJUNCTION face recognition. face recognition CONJUNCTION image generation. Polynomial neural networks ( PNNs ) USED-FOR image generation. Polynomial neural networks ( PNNs ) USED-FOR face recognition. spectral bias FEATURE-OF low - frequency functions. spectral bias FEATURE-OF neural networks. spectral analysis USED-FOR Neural Tangent Kernel ( NTK ). Neural Tangent Kernel ( NTK ) USED-FOR PNNs. spectral analysis USED-FOR PNNs. parametrization of PNNs USED-FOR learning. Π - Net family USED-FOR learning. parametrization of PNNs HYPONYM-OF Π - Net family. polynomials USED-FOR multiplicative interactions. OtherScientificTerm are high - frequency information, and theoretical bias. Task is training. ","This paper studies the spectral bias of neural networks with polynomials in terms of the Neural Tangent Kernel (NTK). The authors show that the NTK is biased towards high-frequency information, and that this is due to the multiplicative interactions between the high- and low-frequency components of the network. The authors then propose a new family of polynomial neural networks (PNNs) based on the $\mathcal{R}$-Net family, and show that this family of PNNs can be viewed as a special case of the $k$-PNN family.    The authors also provide a theoretical analysis of the training dynamics of the neural networks. ","This paper studies the spectral bias of polynomial neural networks (PNNs) in terms of the Neural Tangent Kernel (NTK). The authors show that the NTK is biased towards low-frequency functions, and propose a new family of PNNs called the Π-Net family, which is a family of polynomials with multiplicative interactions. The authors also propose a parametrization of the pNets family that can be used for learning. "
4901,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"memory CONJUNCTION computational costs. computational costs CONJUNCTION memory. hidden subnetworks PART-OF randomly initialized NNs. edge - popup algorithm USED-FOR hidden subnetworks. subnetworks PART-OF randomly initialized NNs. disguised subnetworks HYPONYM-OF subnetworks. disguised subnetworks COMPARE hidden counterparts. hidden counterparts COMPARE disguised subnetworks. unmasking process USED-FOR subnetworks. unmasking process USED-FOR sparse subnetwork mask. two - stage algorithm USED-FOR disguised subnetworks. operations USED-FOR two - stage algorithm. random initialization USED-FOR subnetwork. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION WideResNet-28. WideResNet-28 CONJUNCTION ResNet-50. PaB COMPARE counterparts. counterparts COMPARE PaB. PaB COMPARE edge - popup. edge - popup COMPARE PaB. CIFAR-10/100 datasets EVALUATE-FOR PaB. edge - popup COMPARE counterparts. counterparts COMPARE edge - popup. large - scale models EVALUATE-FOR PaB. CIFAR-10/100 datasets EVALUATE-FOR large - scale models. WideResNet-28 HYPONYM-OF large - scale models. ResNet-18 HYPONYM-OF large - scale models. ResNet-50 HYPONYM-OF large - scale models. Method are Sparse neural networks ( NNs ), pruningand - finetuning pipeline, random networks, and training algorithm. OtherScientificTerm are disguise, latent weights, and approximated gradients. ","This paper proposes a method for training sparse neural networks with hidden subnetworks. The method is based on a two-stage process: first, a masking operation is used to hide the hidden subnetwork from the original network, and second, an unmasking operation is performed to reveal the hidden network. The main contribution of the paper is that the method is computationally efficient and can be applied to large-scale models.   ","This paper proposes a method for hiding hidden subnetworks in sparse neural networks (NNs). The method is based on the edge-popup method (PaB), which is a two-stage method for masking the hidden subnetwork mask. In PaB, the masking process is performed in two stages. The first stage is to mask the sparse subnetwork masks, and the second stage is a pruning-and-finetuning (PFI) method. The authors show that PaB outperforms edge-pupil on CIFAR-10/100 and ResNet-50 datasets. "
4917,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"it USED-FOR risk - sensitive domains. robust GNN model USED-FOR adversarial attacks. Unified Graph Neural Network ( GUGNN ) framework USED-FOR graph. graph CONJUNCTION features. features CONJUNCTION graph. robust GNN model(R - GUGNN ) USED-FOR adversarial attacks. operations USED-FOR robust GNN model(R - GUGNN ). operations USED-FOR it. similarity of two adjacent nodes ’ features CONJUNCTION sparsity of real - world graphs. sparsity of real - world graphs CONJUNCTION similarity of two adjacent nodes ’ features. small eigenvalues FEATURE-OF perturbed graphs. operation USED-FOR graph. convolution operation USED-FOR features. Laplacian smoothness CONJUNCTION prior knowledge. prior knowledge CONJUNCTION Laplacian smoothness. Laplacian smoothness USED-FOR convolution operation. real - world datasets EVALUATE-FOR R - GUGNN. real - world datasets EVALUATE-FOR baselines. R - GUGNN COMPARE baselines. baselines COMPARE R - GUGNN. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are graphs, and denoising features. Generic is they. Task is cleaning perturbed graph structure. ","This paper proposes a novel method to train a robust GNN model against adversarial attacks on graphs. The proposed method is based on the idea that adversarial perturbations to a graph can be modeled as a set of denoising features, which can be used to improve the performance of the model. The method is evaluated on several real-world datasets and achieves state-of-the-art performance. ","This paper proposes a novel GNN framework for training a robust graph neural network (R-GUGNN) against adversarial attacks. The proposed method is based on the GUGNN framework, which is a GNN model that can be used to train a robust GNN for risk-sensitive domains. The main contribution of the paper is to propose a novel method to train the R-GugNN, which can be applied to a variety of tasks. The method is evaluated on several real-world datasets, and the proposed method outperforms the state-of-the-art."
4933,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,approach USED-FOR texture mapping. it USED-FOR document image unwarping. texture mapping USED-FOR 3D surface. method USED-FOR surface parameterization. 3D surface positions CONJUNCTION 2D texture - space coordinates. 2D texture - space coordinates CONJUNCTION 3D surface positions. continuous bijective mapping USED-FOR 3D surface positions. 2D texture - space coordinates FEATURE-OF continuous bijective mapping. continuous bijective mapping USED-FOR method. multi - view images CONJUNCTION rendering loss. rendering loss CONJUNCTION multi - view images. surface parameterization network PART-OF differentiable rendering pipeline. rendering loss USED-FOR surface parameterization network. multi - view images USED-FOR surface parameterization network. differentiable rendering techniques USED-FOR implicit surfaces. 3D scene reconstruction CONJUNCTION view synthesis. view synthesis CONJUNCTION 3D scene reconstruction. differentiable rendering techniques USED-FOR 3D scene reconstruction. methods USED-FOR appearance color. texture map extraction CONJUNCTION texture editing. texture editing CONJUNCTION texture map extraction. differentiable renderer USED-FOR implicit surfaces. texture extraction USED-FOR document - unwarping. approach USED-FOR high - frequency textures. high - frequency textures FEATURE-OF arbitrary document shapes. synthetic and real scenarios FEATURE-OF arbitrary document shapes. it USED-FOR document texture editing. system USED-FOR document texture editing. it USED-FOR system. Method is explicit surface parameterization. Generic is they. ,"This paper proposes a method for texture mapping 3D surface geometry from multi-view images. The method is based on a continuous bijective mapping between 3D positions and 2D texture-space coordinates. The proposed method is evaluated on synthetic and real-world datasets, and compared with existing methods for texture editing and document unwarping.","This paper proposes a method for texture mapping 3D surface positions and 2D texture-space coordinates for document image unwarping. The method is based on a continuous bijective mapping of the 3D position and texture space coordinates of a 3D scene, and a differentiable renderer is used to reconstruct the 3d scene from multi-view images. It is shown that the method can be applied to both synthetic and real-world document textures, and it can be used for document texture editing. "
4949,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"subtle regions USED-FOR fine - grained features. strided operations USED-FOR representation. Convolutional Neural Network USED-FOR representation. strided operations USED-FOR Convolutional Neural Network. downsampling algorithm USED-FOR network. scale of receptive field CONJUNCTION granularity of feature. granularity of feature CONJUNCTION scale of receptive field. trade - off mechanism USED-FOR ARP. ARP USED-FOR network. image classification CONJUNCTION image retrieval. image retrieval CONJUNCTION image classification. pooling operation COMPARE state - of - the - arts. state - of - the - arts COMPARE pooling operation. image classification EVALUATE-FOR state - of - the - arts. Task is Fine - grained recognition. OtherScientificTerm are feature resolution, fine - grained information, resolution of sub - sampled feature, and learning - based parameters. ", image classification is an important task in computer vision. The authors propose a novel method for fine-grained image recognition. The proposed method is based on the idea that sparse regions of the image can be represented by strided operations. The main contribution of the paper is a downsampling algorithm to reduce the size of the receptive field and the granularity of the feature. ,This paper proposes a new method for fine-grained image recognition. The authors propose a new pooling operation that can be applied to a convolutional neural network (CNN). The proposed method is based on a trade-off between the scale of the receptive field and the granularity of the feature. The paper also proposes a downsampling algorithm to improve the performance of the network. Experiments are conducted on image classification and image retrieval tasks.
4965,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"distribution shifts FEATURE-OF neural networks. structural information USED-FOR prediction. structural information FEATURE-OF graph. OOD problem USED-FOR node - level prediction. domain - invariant learning approach USED-FOR GNNs. invariant graph features USED-FOR prediction. invariant graph features USED-FOR GNNs. graphs USED-FOR OOD problem. graphs USED-FOR node - level prediction. Explore - to - Extrapolate Risk Minimization HYPONYM-OF domain - invariant learning approach. graph editers HYPONYM-OF multiple context explorers. cross - domain transfers CONJUNCTION dynamic graph evolution. dynamic graph evolution CONJUNCTION cross - domain transfers. artificial spurious features CONJUNCTION cross - domain transfers. cross - domain transfers CONJUNCTION artificial spurious features. real - world datasets USED-FOR distribution shifts. method USED-FOR distribution shifts. real - world datasets EVALUATE-FOR method. Material are Euclidean data, and graph - structured data. Method are invariant models, and OOD solution. OtherScientificTerm is virtual environments. Generic is model. ","This paper proposes a domain-invariant learning approach for node-level prediction in the presence of distribution shifts. The proposed approach is based on the observation that the GNNs trained on Euclidean data may not be invariant to distribution shifts in real-world data. To address this issue, the authors propose Explore-to-extrapolate Risk Minimization (ERM), a novel approach that uses multiple context explorers and graph editers to improve the performance of the model. The experimental results show that the proposed method outperforms the baselines in terms of accuracy and robustness to distribution shift.  ","This paper proposes a domain-invariant learning approach for GNNs that can handle distribution shifts in the graph structure. The main idea is to use graph-structured data to train a GNN that is invariant to distribution shifts. The authors propose Explore-to-extrapolate Risk Minimization (ETRM), which is an extension of the Explore to Extrapolate risk minimization (ERM) framework. They show that ETRM can be applied to the problem of OOD prediction in the context of graph structure, and show that it can be used to improve the performance of the GNN. They also show that the proposed method can handle cross-domain transfers, dynamic graph evolution, and artificial spurious features."
4981,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"contrastive learning USED-FOR time series data. encoder USED-FOR robust and discriminative representations. augmentations USED-FOR contrastive learning. ad - hoc manual selection EVALUATE-FOR time series augmentations. prefabricated human priors USED-FOR rule of thumb. rule of thumb USED-FOR augmented samples. augmentations of time series data USED-FOR contrastive learning tasks. meta - learning mechanism USED-FOR information - aware approach. InfoTS HYPONYM-OF information - aware approach. meta - learner CONJUNCTION encoder. encoder CONJUNCTION meta - learner. classification task EVALUATE-FOR leading baselines. accuracy EVALUATE-FOR classification task. MSE EVALUATE-FOR forecasting task. accuracy EVALUATE-FOR leading baselines. datasets EVALUATE-FOR forecasting task. forecasting task EVALUATE-FOR leading baselines. datasets EVALUATE-FOR leading baselines. classification task EVALUATE-FOR forecasting task. datasets EVALUATE-FOR classification task. Method are contrastive learning approaches, information theory, and contrastive representation learning. Material is image and language domains. OtherScientificTerm is sub - optimal solutions. ", data augmentation in contrastive learning is an important problem in time series forecasting. This paper proposes a meta-learning approach to improve the robustness and discriminability of time series augmentation. The proposed approach is based on the idea that augmentations can be used as a rule of thumb to select the best augmented samples for time series data. The method is evaluated on the classification task and forecasting task.   ," for contrastive learning. This paper proposes InfoTS, a meta-learning approach for time series data augmentation. The idea is to use a pre-trained meta-learner to learn a rule-of-thumb for augmentations of time series. The method is evaluated on a variety of tasks, including classification and forecasting."
4997,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"winning tickets PART-OF task. theoretical physics FEATURE-OF renormalization group theory. iterative magnitude pruning USED-FOR winning tickets. method USED-FOR winning tickets. iterative magnitude pruning HYPONYM-OF method. iterative magnitude pruning HYPONYM-OF renormalization group scheme. numerical and theoretical tools USED-FOR winning ticket universality. winning ticket universality FEATURE-OF large scale lottery ticket experiments. Task are Lottery Ticket Hypothesis, and machine learning. Generic is tasks. ","This paper studies the Lottery ticket hypothesis in the context of machine learning, where the goal is to find a set of ""winning tickets"" (i.e. the set of tickets that can be used to solve a given task) that are universal across tasks. The authors propose a method to find winning tickets by pruning the tickets that are not useful for the task. The method is based on renormalization group theory, and they show that it is possible to prune winning tickets using iterative magnitude pruning. They also show that this pruning method is computationally efficient.","This paper studies the Lottery ticket hypothesis in the context of renormalization group theory. In particular, the authors consider the problem of finding winning tickets in a large-scale lottery ticket experiment. The authors propose a novel method to find winning tickets by pruning the number of winning tickets from the total number of tickets in the experiment. They show that the winning tickets can be found by using an iterative magnitude pruning scheme. They also provide a theoretical analysis of the winning ticket universality. "
5013,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"Transformer - based models USED-FOR information retrieval problem. BERT USED-FOR information retrieval problem. BERT HYPONYM-OF Transformer - based models. joint embedding USED-FOR cross - attention ( CA ) models. separate embeddings USED-FOR dual - encoder ( DE ) models. cross - attention ( CA ) models HYPONYM-OF models. DE models USED-FOR scores. real - world problems EVALUATE-FOR DE models. CA COMPARE DE models. DE models COMPARE CA. real - world problems EVALUATE-FOR CA. benchmark neural re - ranking datasets EVALUATE-FOR distillation strategy. Method are CA models, and cross - attention. ",This paper proposes a novel cross-attention (CA) model to improve the performance of BERT-based Transformer-based models for information retrieval tasks. The proposed CA model is based on the idea of using separate embeddings for BERT and dual-encoder (DE) models. The authors show that the CA model outperforms DE models on a variety of tasks.  ,"This paper proposes a new approach to distillation of cross-attention (CA) models. The authors propose a distillation strategy that distills the output of a dual-encoder (DE) model into a set of separate embeddings, which are then used to compute the score of a CA model. They show that their approach outperforms the state-of-the-art in terms of performance on a variety of datasets."
5029,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"Off - policy methods USED-FOR Policy Optimization ( PO ) algorithms. IS PART-OF Monte Carlo simulation. variance minimization approach USED-FOR IS. variance minimization approach PART-OF Monte Carlo simulation. behavioral distribution USED-FOR sampling. variance minimization USED-FOR performance improvement tool. variance minimization COMPARE off - policy learning. off - policy learning COMPARE variance minimization. PO algorithm USED-FOR Policy Optimization. variance minimization USED-FOR Policy Optimization. Optimal Policy Evaluation ( POPE ) USED-FOR Policy Optimization. small batch sizes FEATURE-OF robustness. Method is Importance Sampling ( IS ). OtherScientificTerm are behavioral policy, and trust region. Material is continuous RL benchmarks. ","This paper proposes to use importance sampling (IS) as an off-policy method to improve the performance of Policy Optimization (PO) algorithms. Importance sampling is used to estimate the importance of each policy in a sample from the behavioral distribution, which is then used to select the best policy from a set of policies to sample from. The authors show that the importance sampling can be used as a variance-minimization technique to improve performance on continuous RL benchmarks. The paper also shows that the proposed method is robust to small batch sizes. ",This paper proposes a new method to improve the performance of off-policy policy optimization (PO) algorithms. The main idea is to use the Importance Sampling (IS) method to sample from the behavioral distribution and use the variance minimization approach to minimize the variance. The authors show that the proposed method is robust to small batch sizes and can improve the robustness of the algorithm. They also show that their method can be used to improve robustness in the case of small batch size.
5045,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"Graph Neural Networks ( GNNs ) USED-FOR atomic simulations. Graph Neural Networks ( GNNs ) USED-FOR catalyst discovery. GNNs USED-FOR task. triplets CONJUNCTION quadruplets of atoms. quadruplets of atoms CONJUNCTION triplets. they USED-FOR higher - order interactions. higher - order interactions PART-OF graphs. method USED-FOR GNNs. method USED-FOR graphs. GPUs USED-FOR graphs. force MAE metric EVALUATE-FOR S2EF task. AFbT metric EVALUATE-FOR IS2RS task. force MAE metric EVALUATE-FOR graph - parallelized models. OtherScientificTerm is climate change. Generic is models. Method are Graph Parallelism, and DimeNet++ and GemNet models. ","This paper proposes a graph parallelization method for graph neural networks (GNNs) for atomic simulation. The main idea is to parallelize the training of GNNs in order to improve their performance on the S2EF and IS2RS tasks. The proposed method is based on graph parallelism, where the graph is partitioned into subgraphs, and the GNN is trained to predict higher-order interactions in the subgraph. The authors show that the proposed method can be applied to DimeNet++ and GemNet models and achieves state-of-the-art performance on both S2E and IS-2RS benchmarks. ",This paper proposes a method for graph parallelism in atomic simulation. The main idea is to parallelize graph neural networks (GNNs) in a way that they can be used to discover higher-order interactions in the graph. The authors show that their method can be applied to the S2EF task and the IS2RS task. They show that the proposed method can achieve better performance than the state-of-the-art in both S2E and IS2R.
5061,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"language models USED-FOR text generation tasks. meandering CONJUNCTION incoherent content. incoherent content CONJUNCTION meandering. incoherent content HYPONYM-OF structural flaws. meandering HYPONYM-OF structural flaws. Time Control ( TC ) HYPONYM-OF language model. latent stochastic process USED-FOR language model. representation USED-FOR TC. representation USED-FOR language model. stochastic process USED-FOR document plan. text infilling CONJUNCTION discourse coherence. discourse coherence CONJUNCTION text infilling. domain - specific methods COMPARE TC. TC COMPARE domain - specific methods. domain - specific methods CONJUNCTION fine - tuning GPT2. fine - tuning GPT2 CONJUNCTION domain - specific methods. fine - tuning GPT2 COMPARE TC. TC COMPARE fine - tuning GPT2. text domains EVALUATE-FOR fine - tuning GPT2. discourse coherence EVALUATE-FOR TC. text infilling EVALUATE-FOR TC. ordering CONJUNCTION text length consistency. text length consistency CONJUNCTION ordering. long text generation settings EVALUATE-FOR TC. TC USED-FOR text structure. OtherScientificTerm are stochastic process of interest, and latent plan. ","This paper proposes Time Control (TC), a language model for text generation that uses a latent stochastic process to learn a document plan. The authors show that the proposed method is able to improve the performance of text generation models on long text generation tasks in terms of infilling, length consistency, and discourse coherence. They also show that their method can improve the quality of the generated text. ","This paper proposes Time Control (TC), a new language model for text generation. The model is based on a latent stochastic process of interest, which is used to model the document plan of interest. The authors show that the proposed model is able to improve the performance of GPT2 on a variety of text generation tasks, including text infilling, discourse coherence, and text length consistency. "
5077,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"representation of objects USED-FOR higher - level concepts. framework USED-FOR object - centric representation. single 2D images USED-FOR object - centric representation. object - centric models COMPARE model. model COMPARE object - centric models. model USED-FOR segmenting objects. network USED-FOR latent code space. geometric shape CONJUNCTION texture / color. texture / color CONJUNCTION geometric shape. supervision USED-FOR model. specificity FEATURE-OF object segmentation. predictive learning USED-FOR models. approach USED-FOR symbolic representation. OtherScientificTerm are prediction error of future sensory input, moving objects, latent causes, 3D environment, and clustering colors. Material is synthetic dataset. "," images are used to train an object-centric representation of objects. The proposed method is based on the idea that objects are represented as a set of latent codes, which can then be used to predict future sensory input. The method is evaluated on a synthetic dataset and compared to a number of baselines. ","This paper proposes an object-centric representation learning framework for object segmentation. The key idea is to learn a symbolic representation of objects in a latent code space, which is then used to predict the future sensory input of moving objects. The model is trained on a synthetic dataset, where it is shown to perform better than other object centric models. "
5093,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,"traffic management CONJUNCTION public safety. public safety CONJUNCTION traffic management. Spatio - temporal ( ST ) prediction task USED-FOR traffic management. Spatio - temporal ( ST ) prediction task USED-FOR public safety. mobility forecasting USED-FOR traffic management. mobility forecasting HYPONYM-OF Spatio - temporal ( ST ) prediction task. spatial and temporal domains FEATURE-OF features. independent variables PART-OF latent representation. semantic factors FEATURE-OF independent variables. It USED-FOR mobility forecasting models. It USED-FOR spatial and temporal features. VAE - based architecture USED-FOR disentangled representation. real spatio - temporal data USED-FOR mobility forecasting. real spatio - temporal data USED-FOR disentangled representation. deep generative model USED-FOR latent representation. temporal dynamics CONJUNCTION spatially varying component. spatially varying component CONJUNCTION temporal dynamics. deep generative model USED-FOR reconstructions. non - informative features USED-FOR method. Task is mobility forecasting problems. Generic are they, and models. Method are dynamic and static components, and Disentangled representation learning. Material is spatio - temporal datasets. ",This paper proposes a method to learn disentangled representations for spatio-temporal (ST) prediction in traffic forecasting. The method is based on a VAE-based architecture that learns to disentangle spatial and temporal features in the latent space. The proposed method is evaluated on two real-world ST datasets and compared with several baselines.,"This paper proposes a disentangled representation learning method for spatial-temporal mobility forecasting. The authors propose a VAE-based architecture for disentangling spatial and temporal features in the latent space of a mobility forecasting model. They use a deep generative model to reconstruct the features from the data, and use the reconstruction results to improve the performance of the model. The proposed method is evaluated on a variety of mobility forecasting datasets."
5109,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"deep learning framework USED-FOR probabilistic interpolation of irregularly sampled time series. temporal VAE architecture USED-FOR uncertainty. heteroscedastic output layer USED-FOR variable uncertainty. input layer CONJUNCTION temporal VAE architecture. temporal VAE architecture CONJUNCTION input layer. temporal VAE architecture CONJUNCTION heteroscedastic output layer. heteroscedastic output layer CONJUNCTION temporal VAE architecture. heteroscedastic output layer USED-FOR output interpolations. variable uncertainty FEATURE-OF output interpolations. input layer USED-FOR input observation sparsity. temporal VAE architecture PART-OF HeTVAE. input layer PART-OF HeTVAE. heteroscedastic output layer PART-OF HeTVAE. architecture COMPARE deep latent variable models. deep latent variable models COMPARE architecture. homoscedastic output layers USED-FOR deep latent variable models. Material is Irregularly sampled time series. Method are deep learning models, Heteroscedastic Temporal Variational Autoencoder ( HeTVAE ), and sparse and irregular sampling. OtherScientificTerm is input sparsity. ","This paper proposes a temporal variational autoencoder for time series with sparse and irregularly sampled observations. The proposed method is based on a heteroscedastic output layer and a temporal VAE architecture. The output layer is composed of a sparse input layer that is used to model the input sparsity, and a noisy output layer that models the uncertainty of the output interpolations. The experimental results show that the proposed method outperforms the state-of-the-art deep latent variable models on time series.","This paper proposes a heteroscedastic temporal VAE architecture for probabilistic interpolation of irregularly sampled time series. In particular, the authors propose to use the input and output layers of HeTVAE to interpolate the time series, and the output layer of the input layer is used to capture the input observation sparsity. The authors show that their method outperforms the state-of-the-art deep latent variable models in terms of accuracy and variance."
5125,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"structure HYPONYM-OF computational graph. importance CONJUNCTION coherence. coherence CONJUNCTION importance. coherence HYPONYM-OF proxies. importance HYPONYM-OF proxies. statistical methods USED-FOR proxies. proxies USED-FOR partitionings. network weights CONJUNCTION correlations of activations. correlations of activations CONJUNCTION network weights. correlations of activations USED-FOR edges. spectrally clustering USED-FOR partitionings. network weights USED-FOR edges. ones HYPONYM-OF partitionings. weights USED-FOR partitionings. weights USED-FOR ones. graph - based partitioning USED-FOR modularity. graph - based partitioning USED-FOR deep neural networks. Method is neural network. OtherScientificTerm are functionality, and neurons. Task is non - runtime analysis. ","This paper studies the partitioning of neural networks in terms of importance, coherence, and modularity. The authors propose a graph-based partitioning method based on spectrally clustering. They show that the importance and coherence can be expressed as a function of the number of neurons in the network. They also show that this method can be used as a proxy to measure the modularity of a network. ","This paper proposes a non-timely analysis of graph-based partitioning of neural networks. The authors consider the problem of partitioning neural networks into ""one"" and ""one-on-one"" edges, which they define as those with the highest importance (i.e., the number of neurons) and coherence (the number of edges with high coherence). The authors propose two statistical methods to measure the importance of the partitioned edges: (1) spectrally clustering and (2) using the correlations of activations between the weights of the network weights and the activations of the edges. They show that the proposed method is able to identify the ones and ones with high importance, and that the ones are more modular than the ones with low importance.  "
5141,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"mobile devices FEATURE-OF speech - interactive features. lottery ticket hypothesis USED-FOR sparse subnetworks. lottery ticket hypothesis USED-FOR lightweight speech recognition models. noise FEATURE-OF speech. CTC CONJUNCTION RNN - Transducer, and Transformer models. RNN - Transducer, and Transformer models CONJUNCTION CTC. winning tickets COMPARE full models. full models COMPARE winning tickets. sparsity USED-FOR noise robustness. Method are Lightweight speech recognition models, and speech models. Generic are systems, and full model. Task is open - world personalization. OtherScientificTerm are structured sparsity, backbones, full model weights, and background noises. ",This paper studies the lottery ticket hypothesis for sparse subnetworks in lightweight speech recognition models. The authors propose to use structured sparsity to improve the robustness against background noise in the training process. They show that the lottery tickets are more robust to background noise than the full model weights. They also show that sparsity is beneficial for open-world personalization.,"This paper studies the lottery ticket hypothesis for sparse subnetworks in lightweight speech recognition models. The authors show that the full model weights can be more robust to background noise than the backbones. They also show that winning tickets are more robust than full models, and that full models can be better than winning tickets.  "
5157,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"random weights USED-FOR Deep neural networks. skip connections CONJUNCTION Hadamard transforms. Hadamard transforms CONJUNCTION skip connections. skip connections USED-FOR ResNet architectures. Hadamard transforms USED-FOR ResNet architectures. batch normalization USED-FOR network training. random weights USED-FOR network initialization. image classification datasets EVALUATE-FOR ZerO. ImageNet HYPONYM-OF image classification datasets. OtherScientificTerm is stable signal propagation. Metric are variance, and reproducibility. Method are random weight initialization, and residual networks. ",This paper proposes to use random weight initialization for training deep neural networks with skip connections and Hadamard transforms. Theoretical analysis shows that random weights can be used to improve the stability of the signal propagation in deep networks. Experiments on ImageNet and CIFAR-10 and ImageNet-100 show the effectiveness of the proposed method.   ,"This paper proposes ZerO, a new method for training deep neural networks with random weights. The key idea is to use batch normalization to improve the reproducibility of the network training process. The authors show that ZerO can be used to train deep networks with skip connections and Hadamard transforms. They also show that it can also be used for training residual networks. The experiments are conducted on ImageNet and CIFAR-10."
5173,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,"minimax formulation USED-FOR backdoors. backdoors PART-OF poisoned model. minimax formulation USED-FOR poisoned model. clean data USED-FOR minimax formulation. backdoor removal PART-OF formulation. implicit hypergradient USED-FOR algorithm. robustness EVALUATE-FOR minimax. clean data USED-FOR minimax. I - BAU COMPARE state - ofart backdoor defenses. state - ofart backdoor defenses COMPARE I - BAU. state - ofart backdoor defenses USED-FOR backdoor attacks. I - BAU USED-FOR backdoor attacks. I - BAU COMPARE baseline. baseline COMPARE I - BAU. attack settings CONJUNCTION poison ratio. poison ratio CONJUNCTION attack settings. poison ratio CONJUNCTION clean data size. clean data size CONJUNCTION poison ratio. it COMPARE baseline. baseline COMPARE it. single - target attack setting EVALUATE-FOR baseline. single - target attack setting EVALUATE-FOR it. computation USED-FOR I - BAU. OtherScientificTerm is inner and outer optimization. Metric is convergence. Generic are its, and baselines. ","This paper studies the problem of backdoor attack in the presence of a poisoned model. In this setting, the authors propose a minimax formulation for backdoor attack that is robust to backdoors in the poisoned model, where the backdoor removal is performed by minimizing the implicit hypergradient of the model parameters. The authors show that the proposed method is computationally efficient and robust to backdoor attacks. The main contribution of the paper is the formulation of the method, which is based on the minimax algorithm. Theoretical analysis is provided to show the convergence of the proposed algorithm.","This paper proposes a new minimax-based backdoor defense against backdoor attacks. The proposed method is based on the minimax formulation, where the target model is a poisoned model and the target data is a set of clean data. The authors show that their method is robust to backdoor attacks in the single-target setting. They also show that the proposed method can be combined with other state-of-the-art backdoor defenses."
5189,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"random permutations COMPARE with - replacement sampling. with - replacement sampling COMPARE random permutations. permutations COMPARE random. random COMPARE permutations. smooth second derivatives FEATURE-OF 1 - dimensional strongly convex functions. random permutations USED-FOR strongly convex functions. easy - to - construct permutations USED-FOR accelerated convergence. easy - to - construct permutations COMPARE random. random COMPARE easy - to - construct permutations. easy - to - construct permutations USED-FOR quadratic, strongly - convex functions. convergence characterization USED-FOR optimal permutations. Method is permutation - based SGD. Metric is convergence gap. ","This paper studies the convergence of SGD with permutation-based SGD for strongly convex functions with smooth second derivatives. In particular, the authors show that the convergence is accelerated when the number of permutations is small. The authors also show that with-replacement sampling is equivalent to using random permutations. The main contribution of the paper is to provide a convergence analysis of permutation based SGD. ","This paper studies the convergence of a permutation-based SGD for strongly convex functions with smooth second derivatives. In particular, the authors consider the case of a quadratic function, where the second derivatives are smooth. The authors show that the convergence rate of the SGD is bounded by the number of permutations. They also provide a convergence characterization of the optimal permutations for the smooth function.   "
5205,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,transformations USED-FOR flow models. method USED-FOR transformation layers. unique transformations USED-FOR transformation layers. mixed distribution USED-FOR architecture optimization. invertibility FEATURE-OF NF architecture. discrete space USED-FOR architecture optimization. global minimum FEATURE-OF approximate upper bound. approximate upper bound USED-FOR mixture NF. block - wise alternating optimization algorithm USED-FOR architecture optimization. block - wise alternating optimization algorithm USED-FOR deep flow models. architecture optimization USED-FOR deep flow models. Metric is performance - cost trade - offs. Method is flow architecture. ,"This paper studies the problem of architecture optimization for deep flow models with transformation layers. The authors propose to use a block-wise alternating optimization algorithm to optimize the transformation layers of a flow model. The method is based on the observation that NF architecture is invertible in discrete space, and the authors show that the invertibility of NF architecture can be improved by using unique transformations in the transformation layer.  ","This paper studies the problem of architecture optimization for deep flow models. The authors propose a method to optimize a flow model with a mixture of transformation layers, where each transformation layer is composed of a set of unique transformations. They show that the invertibility of the mixed distribution of the transformation layers can be used to improve the performance-cost trade-off of the model. They also provide an approximate upper bound for the global minimum of the mixture NF.   "
5221,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,self - supervised learning ( SSL ) methods USED-FOR representations. Intrinsic Dimension ( ID ) USED-FOR representation space. Intrinsic Dimension ( ID ) USED-FOR expressiveness. KNN classifier USED-FOR Kmeans cluster labels. Kmeans cluster labels USED-FOR held - out representations. learning speed EVALUATE-FOR KNN classifier. KNN classifier USED-FOR Cluster Learnability ( CL ). learning speed EVALUATE-FOR Cluster Learnability ( CL ). contrastive losses CONJUNCTION pretext tasks. pretext tasks CONJUNCTION contrastive losses. ID CONJUNCTION CL. CL CONJUNCTION ID. model architecture CONJUNCTION human labels. human labels CONJUNCTION model architecture. data augmentation CONJUNCTION model architecture. model architecture CONJUNCTION data augmentation. ID USED-FOR downstream classification. CL USED-FOR downstream classification. CL COMPARE techniques. techniques COMPARE CL. ID COMPARE techniques. techniques COMPARE ID. contrastive losses USED-FOR techniques. pretext tasks USED-FOR techniques. DeepCluster USED-FOR representations. modification USED-FOR DeepCluster. ImageNet benchmarks EVALUATE-FOR DeepCluster. intermediate checkpoints USED-FOR SSL algorithms. framework USED-FOR SSL algorithms. framework USED-FOR intermediate checkpoints. Generic is architectures. ,"This paper proposes a self-supervised learning method called DeepCluster to improve the performance on ImageNet classification tasks. The proposed method is based on the Intrinsic Dimension (ID) and Cluster Learnability (CL) methods. The key idea is to use a KNN classifier to predict the ID and cluster labels for each image, which are then used for downstream classification. Experiments show that the proposed method achieves better performance compared to other SSL methods.   ","This paper proposes a method for self-supervised learning (SSL) based on Intrinsic Dimension (ID) and Cluster Learnability (CL) to improve the performance of SSL methods. The proposed method, DeepCluster, is based on a KNN classifier and a cluster of Kmeans cluster labels. The method is evaluated on ImageNet and CIFAR-10 datasets.  "
5237,SP:4f5c00469e4425751db5efbc355085a5e8709def,"Deep neural networks USED-FOR adversarial examples. query efficiency EVALUATE-FOR black - box attacks. segmentation priors USED-FOR black - box attacks. salient region FEATURE-OF perturbations. query efficiency CONJUNCTION success rate. success rate CONJUNCTION query efficiency. imperceptibility performance EVALUATE-FOR blackbox attacks. segmentation priors USED-FOR blackbox attacks. Saliency Attack HYPONYM-OF gradient - free black - box attack. attacks USED-FOR perturbations. approach USED-FOR perturbations. approach USED-FOR detection - based defense. Task are blackbox setting, and adversarial attacks. Metric is imperceptibility. ","This paper proposes a new black-box adversarial attack method, called saliency attack, which is a gradient-free black box attack. The proposed method is based on the observation that the salient region of adversarial perturbations is the most imperceptible part of the perturbation. The authors show that saliency attacks can be used as a detection-based defense against black box adversarial attacks.   ","This paper proposes a new black-box adversarial defense against adversarial examples. The main idea is to attack the salient region of the perturbations in the adversarial example, which is defined as the region of perturbation that is most imperceptible to the user. The authors propose a new adversarial attack that is gradient-free and can be used in conjunction with the detection-based defense. The proposed method is evaluated on a variety of datasets, and the proposed method outperforms the state of the art."
5253,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"Synthesis planning CONJUNCTION reaction outcome prediction. reaction outcome prediction CONJUNCTION Synthesis planning. Synthesis planning HYPONYM-OF computer - aided organic chemistry. reaction outcome prediction HYPONYM-OF computer - aided organic chemistry. Natural language approaches USED-FOR problem. Natural language approaches USED-FOR end - to - end formulation. Natural language approaches USED-FOR SMILES - to - SMILES translation. Transformer models USED-FOR text generation. Transformer models CONJUNCTION molecular graph encoders. molecular graph encoders CONJUNCTION Transformer models. molecular graph encoders USED-FOR Graph2SMILES model. Transformer models USED-FOR Graph2SMILES model. Graph2SMILES USED-FOR task. Transformer USED-FOR task. Graph2SMILES USED-FOR Transformer. molecule(s)-to - molecule(s ) transformations USED-FOR task. Graph2SMILES USED-FOR end - to - end architecture. global attention encoder USED-FOR long - range and intermolecular interactions. graph - aware positional embedding USED-FOR global attention encoder. top-1 accuracy EVALUATE-FOR Transformer baselines. Graph2SMILES COMPARE Transformer baselines. Transformer baselines COMPARE Graph2SMILES. reaction outcome prediction CONJUNCTION one - step retrosynthesis. one - step retrosynthesis CONJUNCTION reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Transformer baselines. USPTO_480k and USPTO_STEREO datasets USED-FOR reaction outcome prediction. reaction outcome prediction EVALUATE-FOR Graph2SMILES. USPTO_50k dataset USED-FOR one - step retrosynthesis. top-1 accuracy EVALUATE-FOR Graph2SMILES. Method are data - driven approaches, machine translation model architectures, SMILES representations, SMILES augmentation, and encoder. Task is data preprocessing. OtherScientificTerm are molecular structures, input data augmentation, and local chemical environments. ","This paper proposes a novel Transformer-based model for molecular synthesis planning and reaction outcome prediction in organic chemistry. The proposed method is based on SMILES-to-SMILES translation, which is a natural language approach for text-based synthesis planning. The main idea is to use a Transformer model to model the molecular structure, and then use a molecular graph encoder to predict the molecule(s)-to-molecule(s) transformations.  The proposed model is evaluated on the USPTO_480k and USPto_STEREO datasets and achieves state-of-the-art performance.  ","This paper proposes a novel Transformer-based model for molecular graph translation. The proposed model is based on the SMILES-to-SMILES model, which is a Transformer model with a molecular graph encoder and a global attention encoder to capture long-range and intermolecular interactions. The model is trained on two datasets, USPTO_480k and USPto_STEREO, and the proposed model achieves state-of-the-art performance on both datasets. "
5269,SP:ce3cde67564679a8d9a0539f1e12551390b91475,reinforcement learning ( RL ) methods USED-FOR task - oriented dialogues setting. task - oriented dialogues setting USED-FOR automatic disease diagnosis. reinforcement learning ( RL ) methods USED-FOR automatic disease diagnosis. RL tasks COMPARE action space. action space COMPARE RL tasks. action space FEATURE-OF disease diagnosis. approaches USED-FOR problem. hierarchical policy PART-OF dialogue policy learning. symptom checkers CONJUNCTION disease classifier. disease classifier CONJUNCTION symptom checkers. master model USED-FOR low level model. low level policy PART-OF high level policy. disease classifier PART-OF low level policy. symptom checkers PART-OF low level policy. master model PART-OF high level policy. hierarchical framework COMPARE systems. systems COMPARE hierarchical framework. accuracy CONJUNCTION symptom recall. symptom recall CONJUNCTION accuracy. symptom recall EVALUATE-FOR systems. accuracy EVALUATE-FOR systems. disease diagnosis EVALUATE-FOR systems. hierarchical framework USED-FOR disease diagnosis. symptom recall EVALUATE-FOR hierarchical framework. accuracy EVALUATE-FOR hierarchical framework. Task is offline consultation process. ,"This paper proposes a dialogue policy learning method for automatic disease diagnosis in offline dialogues. The proposed method is based on a hierarchical framework, where a high-level policy is trained with a classifier and a set of symptom checkers, and a low-level dialogue policy is learned with a disease classifier. The authors show that the proposed method achieves state-of-the-art results in terms of accuracy and symptom recall. ","This paper proposes a hierarchical framework for disease diagnosis in a task-oriented dialogues setting, where the goal is to learn a high-level policy and a low-level one for the task of disease diagnosis. The high level policy learns a high level model for the disease classifier, a low level policy for the symptom checker, and a symptom recall for the low level. The low level model is trained to predict the symptoms, and the symptom recall is trained on the high level and low level policies. The proposed method is evaluated on a variety of tasks, and it is shown that the proposed method outperforms the state-of-the-art in terms of accuracy and recall."
5285,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,Federated Learning ( FL ) USED-FOR ML training ecosystem. distributed training USED-FOR data privacy. edge devices USED-FOR distributed training. FL COMPARE centralized training. centralized training COMPARE FL. Addressing label deficiency PART-OF FL. framework USED-FOR algorithms. SSFL ) HYPONYM-OF self - supervised and personalized federated learning framework. centralized self - supervised learning methods USED-FOR FL setting. SimSiam networks COMPARE FedAvg algorithm. FedAvg algorithm COMPARE SimSiam networks. Ditto CONJUNCTION local fine - tuning. local fine - tuning CONJUNCTION Ditto. perFedAvg CONJUNCTION Ditto. Ditto CONJUNCTION perFedAvg. algorithms USED-FOR supervised personalization algorithms. algorithms USED-FOR self - supervised learning. supervised personalization algorithms USED-FOR self - supervised learning. local fine - tuning HYPONYM-OF self - supervised learning. perFedAvg HYPONYM-OF self - supervised learning. Ditto HYPONYM-OF self - supervised learning. personalization CONJUNCTION consensus. consensus CONJUNCTION personalization. Per - SSFL USED-FOR personalization. Per - SSFL HYPONYM-OF personalized federated self - supervised learning algorithm. distributed training system USED-FOR SSFL. distributed training system CONJUNCTION evaluation protocol. evaluation protocol CONJUNCTION distributed training system. evaluation protocol USED-FOR SSFL. supervised learning CONJUNCTION unsupervised learning. unsupervised learning CONJUNCTION supervised learning. unsupervised learning USED-FOR FL. evaluation accuracy EVALUATE-FOR unsupervised learning. synthetic non - I.I.D. dataset CONJUNCTION intrinsically non - I.I.D. dataset. intrinsically non - I.I.D. dataset CONJUNCTION synthetic non - I.I.D. dataset. training system USED-FOR synthetic non - I.I.D. dataset. evaluation accuracy EVALUATE-FOR supervised learning. supervised learning USED-FOR FL. CIFAR-10 USED-FOR synthetic non - I.I.D. dataset. batch size CONJUNCTION non-I.I.D.ness. non-I.I.D.ness CONJUNCTION batch,"This paper proposes a federated self-supervised and personalized federated learning (SSFL) framework to address label deficiency in distributed training. The authors propose a new algorithm, called Per-SSFL, to address the label deficiency problem in federated training. They show that the proposed algorithm achieves better performance than the state-of-the-art FedAvg and SimSiam networks in the federated setting. They also show that their algorithm is more efficient than FedAvg in terms of batch size and non-I.I.D. accuracy.",This paper proposes a new self-supervised and personalized federated learning (SSFL) framework for distributed learning in the federated setting. The authors propose a new algorithm called Per-SSFL to address the problem of label deficiency in the FL setting. They also propose an evaluation protocol to evaluate the performance of the proposed algorithm. Experiments are conducted on CIFAR-10 dataset and synthetic non-I.I.D. dataset.
5301,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"deep neural networks ( DNN ) CONJUNCTION partial differential equations ( PDEs ). partial differential equations ( PDEs ) CONJUNCTION deep neural networks ( DNN ). PDEs USED-FOR DNN architectures. PDEs USED-FOR ResNet - like DNN. adjustment operator USED-FOR DNN. adjustment operator USED-FOR ResNet - like DNN. adjustment operator USED-FOR PDEs. training method USED-FOR DNN models. PDEs theory USED-FOR training method. robustness EVALUATE-FOR training method. training method USED-FOR networks. PDEs USED-FOR networks. training method USED-FOR DNN. robustness EVALUATE-FOR DNN. generalization gap FEATURE-OF DNN. training method USED-FOR generalization gap. method USED-FOR DNN. DNN COMPARE baseline model. baseline model COMPARE DNN. method USED-FOR DNN. generalization EVALUATE-FOR DNN. OtherScientificTerm are neural network design space, overfitting, and adversarial perturbations. Method is neural network structures. ",This paper proposes a method to train deep neural networks (DNNs) using partial differential equations (PDEs) to improve robustness to adversarial perturbations. The proposed method is based on the observation that DNNs trained with PDEs are more robust to adversarially perturbed data. The authors show that the robustness of DNN is related to the generalization gap between DNN and a baseline model trained with DDEs. The method is evaluated on image classification and language modeling tasks.  ,This paper proposes a new training method for deep neural networks (DNNs) based on partial differential equations (PDEs). The main contribution of the paper is a theoretical analysis of the generalization gap between DNNs trained with and without PDEs. The main result is that the training method is robust to adversarial perturbations and overfitting. The method is tested on a variety of datasets and shows that the proposed method outperforms the baseline DNN.
5317,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,deep learning models USED-FOR emergence of language. emergent language USED-FOR task. simulated agents USED-FOR emergent language. language games FEATURE-OF emergence of language. expressivity FEATURE-OF emergent languages. expressivity FEATURE-OF emergent languages. contrastive loss COMPARE referential loss. referential loss COMPARE contrastive loss. Generic is languages. Metric is complexity. OtherScientificTerm is message type collapse. ,"This paper studies the emergence of language in a language game setting, where the goal is to learn a language that can be used to communicate with an agent that has access to the environment. The authors propose a novel contrastive loss that aims to improve the expressivity of the learned language. They show that this loss is effective in reducing the message type collapse in the language game. They also show that it is possible to learn language that is more expressive than the original language.   ","This paper studies the emergence of language in a language game. The authors propose a new contrastive loss for emergent languages, which they call contrastive contrastive language. They show that contrastive languages are more expressive than referential languages. They also show that the contrastive losses can be used to improve the expressivity of emergent language."
5333,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"exploration - exploitation dilemma PART-OF reinforcement learning. them USED-FOR reinforcement learning setting. exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. bandit setting FEATURE-OF Uncertainty - based exploration strategies. method USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR exploration. Sample Average Uncertainty ( SAU ) USED-FOR method. exploration USED-FOR bandit problems. value predictions USED-FOR it. δ - exploration HYPONYM-OF exploration strategy. SAU USED-FOR sequential Reinforcement Learning scenario. bandits USED-FOR SAU. Deep Q - learning case EVALUATE-FOR δ - exploration. OtherScientificTerm are posterior distributions, and model posterior distributions. Task is generic sequential decision tasks. Method are reward models, and Reinforcement Learning. ",This paper studies the exploration-exploitation dilemma in reinforcement learning in the bandit setting. The authors propose a new exploration strategy called Sample Average Uncertainty (SAU) based on uncertainty-based exploration strategies. The main idea is to estimate the sample average uncertainty of the posterior distribution of the reward function and use it to estimate exploration. The proposed method is shown to outperform other exploration-based methods in bandit problems. ,This paper studies the exploration-exploitation dilemma in reinforcement learning in the bandit setting. The authors propose a new exploration strategy called Sample Average Uncertainty (SAU) based on the sample average uncertainty (VAE) of the posterior distribution of the reward model. They show that SAU can be used to improve the exploration strategy in bandit problems. They also show that it can be applied in the Deep Q-learning setting. 
5349,SP:2f6e266b03939c96434834579999707d3268c5d6,"spatio - temporal complexity CONJUNCTION continuity of videos. continuity of videos CONJUNCTION spatio - temporal complexity. deep learning era USED-FOR long video generation. implicit neural representations ( INRs ) USED-FOR continuous signal. generative adversarial network USED-FOR video generation. motion discriminator USED-FOR unnatural motions. INR - based video generator USED-FOR motion dynamics. video extrapolation CONJUNCTION non - autoregressive video generation. non - autoregressive video generation CONJUNCTION video extrapolation. long video synthesis CONJUNCTION video extrapolation. video extrapolation CONJUNCTION long video synthesis. datasets EVALUATE-FOR DIGAN. long video synthesis HYPONYM-OF datasets. video extrapolation HYPONYM-OF datasets. UCF-101 EVALUATE-FOR FVD score. 128×128 resolution FEATURE-OF 128 frame videos. FVD score EVALUATE-FOR DIGAN. 128 frame videos USED-FOR DIGAN. UCF-101 EVALUATE-FOR DIGAN. OtherScientificTerm are video distribution, 3D grids of RGB values, scale of generated videos, continuous dynamics, INRs of video, and space and time coordinates. Method is parameterized neural network. Material is long frame sequences. ",This paper proposes a method for long video generation that uses implicit neural representations (INRs) to model the dynamics of the video distribution. The INRs are learned by a generative adversarial network (GAN) that is trained to generate videos with a discriminator that discriminates between natural and unnatural motions. The proposed method achieves state-of-the-art results on long video synthesis and video extrapolation datasets. ,"This paper proposes a generative adversarial network (DIGAN) for long video generation. DIGAN is based on the implicit neural representations (INRs) of the video, which are used to model the dynamics of a video. In particular, the INR is used as a discriminator to detect unnatural motions in the video. The video is then generated by a video generator, which is trained to generate a video of the same size as the INRs. The INRs of the generated video are then used to predict the video dynamics. The proposed method is evaluated on UCF-101, a dataset for video synthesis and video extrapolation.  "
5365,SP:878325384328c885ced7af0ebf31bbf79287c169,Private multi - winner voting USED-FOR revealing k - hot binary vectors. bounded differential privacy guarantee FEATURE-OF revealing k - hot binary vectors. task PART-OF machine learning literature. Binary HYPONYM-OF privacy - preserving multi - label mechanisms. Powerset voting HYPONYM-OF privacy - preserving multi - label mechanisms. composition USED-FOR Binary voting. ` 2 norm FEATURE-OF τ voting. binary vector USED-FOR Powerset voting. Powerset voting COMPARE Binary voting. Binary voting COMPARE Powerset voting. mechanisms USED-FOR privacy - preserving multi - label learning. canonical single - label technique USED-FOR mechanisms. PATE HYPONYM-OF canonical single - label technique. canonical single - label technique USED-FOR privacy - preserving multi - label learning. large real - world healthcare data CONJUNCTION multi - label benchmarks. multi - label benchmarks CONJUNCTION large real - world healthcare data. techniques COMPARE DPSGD. DPSGD COMPARE techniques. large real - world healthcare data EVALUATE-FOR DPSGD. multi - label benchmarks EVALUATE-FOR DPSGD. large real - world healthcare data EVALUATE-FOR techniques. multi - label benchmarks EVALUATE-FOR techniques. centralized setting EVALUATE-FOR techniques. mechanisms USED-FOR models. mechanisms USED-FOR multi - site ( distributed ) setting. multi - site ( distributed ) setting FEATURE-OF models. Material is healthcare. OtherScientificTerm is power set. Method is multi - label CaPC. ,"This paper studies the problem of privacy-preserving multi-label learning in the presence of multiple labels. The authors propose a new method called DPSGD, which is based on the notion of private multi-winner voting. DPSGD is a variant of Powerset voting, where each label is represented by a binary vector and the goal is to find the winner of the vote. The main contribution of the paper is to show that DPSGD has a bounded differential privacy guarantee and that it can be used in conjunction with PATE, a canonical single-label mechanism. The paper also provides theoretical analysis of DPSGD in the multi-site setting.","This paper proposes a new privacy-preserving multi-label voting mechanism, DPSGD. DPSGD is based on the notion of private multi-winner voting, where the winner is chosen based on a power set of k-hot binary vectors. The authors show that DPSGD can be combined with a canonical single-label mechanism, PATE, to achieve a bounded differential privacy guarantee. The proposed DPSGD outperforms the state-of-the-art DPSGD on a variety of healthcare datasets. "
5381,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"dataset CONJUNCTION regularization. regularization CONJUNCTION dataset. architecture CONJUNCTION optimizer. optimizer CONJUNCTION architecture. batch size CONJUNCTION dataset. dataset CONJUNCTION batch size. optimizer CONJUNCTION batch size. batch size CONJUNCTION optimizer. optimizer CONJUNCTION learning rate schedule. learning rate schedule CONJUNCTION optimizer. tuned optimizer COMPARE optimizer. optimizer COMPARE tuned optimizer. grafting USED-FOR non - adaptive learning rate correction. non - adaptive learning rate correction USED-FOR SGD. non - adaptive learning rate correction USED-FOR BERT model. Method are large neural networks, optimizer grafting, optimizer hyperparameter search, and deep learning. OtherScientificTerm are implicit step size schedule, and empirical performance. Task is optimizer comparisons. ","This paper studies the effect of the implicit step size schedule on the performance of large neural networks trained with different optimizers. The authors show that the step size of the optimizer is a function of the batch size, the architecture, and the training data size. They show that optimizers with step size schedules are more sensitive to batch size and learning rate schedule. They also show that a non-adaptive learning rate correction for SGD can be used to improve the performance.","This paper studies the problem of optimizer grafting in the context of deep learning. The authors propose a new method of learning rate correction for non-adaptive learning rate corrections in SGD. The method is based on the notion of implicit step size schedule, where the step size is the sum of the number of steps in the learning rate schedule and the size of the dataset. They show that the implicit step-size schedule is a function of the batch size and the optimizer architecture. They also provide empirical evidence that the learned rate correction can be used to improve the performance of large neural networks. "
5397,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"RL algorithms USED-FOR policy. algorithm USED-FOR online RL. algorithm USED-FOR offline demonstration data. sub - optimal behavior policy USED-FOR offline demonstration data. sparse reward settings FEATURE-OF online RL. policy improvement step CONJUNCTION policy guidance step. policy guidance step CONJUNCTION policy improvement step. offline demonstration data USED-FOR policy guidance step. LOGO USED-FOR policy. algorithm USED-FOR incomplete observation setting. censored version of the true state observation PART-OF demonstration data. sparse rewards CONJUNCTION censored state. censored state CONJUNCTION sparse rewards. algorithm COMPARE approaches. approaches COMPARE algorithm. censored state FEATURE-OF benchmark environments. sparse rewards FEATURE-OF benchmark environments. benchmark environments EVALUATE-FOR approaches. benchmark environments EVALUATE-FOR algorithm. LOGO USED-FOR obstacle avoidance. LOGO USED-FOR trajectory tracking. trajectory tracking CONJUNCTION obstacle avoidance. obstacle avoidance CONJUNCTION trajectory tracking. mobile robot USED-FOR trajectory tracking. mobile robot USED-FOR obstacle avoidance. mobile robot USED-FOR LOGO. LOGO USED-FOR approach. Task is real - world reinforcement learning ( RL ). OtherScientificTerm are sparsity of reward feedback, sparse reward function, fine grain feedback, exploration actions, feedback, guidance, sub - optimal policy, and learning episode. Material is offline data. Generic is it. ","This paper proposes a method for offline reinforcement learning in sparse reward settings. The method is based on a policy improvement step and a policy guidance step, where the policy is guided by a censored version of the true state observation from the offline demonstration data. The policy is trained using a combination of the policy improvement and policy guidance steps. The proposed method is shown to outperform the baselines in a variety of sparse reward environments.","This paper proposes a new method for online reinforcement learning with sparse rewards in sparse reward settings. The main idea is to learn a sub-optimal behavior policy from offline demonstration data, which is then used to guide the learner to the censored state of the true state observation in the offline data. The proposed method is evaluated on a variety of real-world environments, and it outperforms the state-of-the-art. "
5413,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"scalarization problem USED-FOR Pareto front. Linear Scalarization CONJUNCTION domain decomposition. domain decomposition CONJUNCTION Linear Scalarization. Multi - Task Learning ( MTL ) solvers USED-FOR Pareto solutions. Linear Scalarization PART-OF Multi - Task Learning ( MTL ) solvers. domain decomposition PART-OF Multi - Task Learning ( MTL ) solvers. Linear Scalarization USED-FOR Pareto solutions. MTL solvers USED-FOR real - world applications. non - convex functions CONJUNCTION constraints. constraints CONJUNCTION non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR non - convex functions. Hybrid Neural Pareto Front ( HNPF ) USED-FOR constraints. Hybrid Neural Pareto Front ( HNPF ) HYPONYM-OF two stage Pareto framework. Stage-1 neural network USED-FOR weak Pareto front. Fritz - John Conditions ( FJC ) USED-FOR Stage-1 neural network. FJC guided diffusive manifold USED-FOR weak Pareto front. low - cost Pareto filter USED-FOR strong Pareto subset. strong Pareto subset PART-OF weak front. low - cost Pareto filter USED-FOR weak front. Method is Fixed - point iterative strategies. OtherScientificTerm are convexity assumptions, Pareto definition, and convexity. Task is benchmarking and verification. Generic is approach. ","This paper proposes a two-stage Pareto framework for solving the Paretopology problem. The first stage is based on a stage-1 neural network, which is trained with a weak-Pareto filter, and the second stage is a stage with a strong-Paredto filter. The weak front is trained by minimizing the FJC condition on a diffusive manifold, while the strong front is learned by minimizing a low-cost filter on the weak front. The authors show that the proposed method outperforms the state-of-the-art linear scalarization and domain decomposition methods.",This paper proposes a two-stage Pareto framework for the problem of solving the PareTo problem. The main idea is to use a stage-1 neural network for the weak front and stage-2 for the strong front. The weak front is trained using the FJC guided diffusive manifold. The strong front is learned using a low-cost Pare to filter the strong Pare-to subset. The authors show that the proposed method outperforms the state-of-the-art fixed-point iterative strategies in terms of accuracy.
5429,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"library of diverse expert models COMPARE single generalist model. single generalist model COMPARE library of diverse expert models. library of diverse expert models USED-FOR task. single generalist model USED-FOR task. related task - specific teachers USED-FOR recognition tasks. related task - specific teachers USED-FOR consolidated image feature representation. student model PART-OF knowledge distillation. downstream transferability EVALUATE-FOR task - agnostic generalist model. distillation of task - specific teachers COMPARE task - agnostic generalist model. task - agnostic generalist model COMPARE distillation of task - specific teachers. teacher representations USED-FOR distillation of task - specific teachers. generalist teacher USED-FOR representations. generalist teacher PART-OF task - specific teacher(s ). unlabeled proxy dataset USED-FOR multihead, multi - task distillation method. task - specific teacher(s ) USED-FOR representations. multi - task joint training oracle EVALUATE-FOR method. Generic is library. OtherScientificTerm are teacher, and ImageNet pre - trained features. ","This paper proposes a method for knowledge distillation in multi-task image classification. The method is based on distilling the knowledge from a library of diverse expert models into a task-agnostic generalist model. The student model is trained with an unlabeled proxy dataset, and the teacher representations are used to distill the knowledge of the distillation of task-specific teachers. Experiments show that the proposed method outperforms the state-of-the-art in terms of downstream transferability.",This paper proposes a method to distill knowledge from a library of diverse expert models to a single generalist model for a single task. The idea is to use a proxy dataset of unlabeled proxy datasets to distil the teacher representations from the student model into a consolidated image feature representation. The teacher representations are then used for distillation of task-specific teachers. The student model is used for downstream transferability. The proposed method is evaluated on a variety of tasks and compared with a task-agnostic generalist and task agnostic model.
5445,SP:ab0d024d4060235df45182dab584c36db16d8e31,"Quantifying the data uncertainty USED-FOR learning tasks. valid coverage CONJUNCTION efficiency. efficiency CONJUNCTION valid coverage. valid coverage FEATURE-OF prediction sets. low length HYPONYM-OF efficiency. constrained empirical risk minimization ( ERM ) problem USED-FOR prediction set. empirical coverage FEATURE-OF prediction set. approximate valid population coverage CONJUNCTION near - optimal efficiency. near - optimal efficiency CONJUNCTION approximate valid population coverage. function class PART-OF conformalization step. meta - algorithm USED-FOR conformal prediction algorithms. near - optimal efficiency EVALUATE-FOR it. approximate valid population coverage EVALUATE-FOR it. it USED-FOR ERM problem. non - differentiable coverage constraint PART-OF it. differentiable surrogate losses CONJUNCTION Lagrangians. Lagrangians CONJUNCTION differentiable surrogate losses. gradient - based algorithm USED-FOR it. constrained ERM USED-FOR gradient - based algorithm. Lagrangians USED-FOR gradient - based algorithm. Lagrangians USED-FOR constrained ERM. differentiable surrogate losses USED-FOR gradient - based algorithm. differentiable surrogate losses USED-FOR constrained ERM. minimum - volume prediction sets CONJUNCTION label prediction sets. label prediction sets CONJUNCTION minimum - volume prediction sets. prediction intervals CONJUNCTION minimum - volume prediction sets. minimum - volume prediction sets CONJUNCTION prediction intervals. algorithm COMPARE approaches. approaches COMPARE algorithm. label prediction sets USED-FOR image classification. efficiency EVALUATE-FOR approaches. minimum - volume prediction sets USED-FOR multi - output regression. algorithm USED-FOR applications. approaches USED-FOR applications. image classification HYPONYM-OF applications. efficiency EVALUATE-FOR algorithm. prediction intervals HYPONYM-OF applications. minimum - volume prediction sets HYPONYM-OF applications. label prediction sets HYPONYM-OF applications. OtherScientificTerm is prediction interval. Method are Conformal prediction, and conformal prediction. ","This paper proposes a meta-algorithm for conformal prediction, which is a variant of conformal regression. Conformal regression is a popular method for estimating the uncertainty of a prediction set. The main contribution of this paper is to introduce a non-differentiable coverage constraint for the prediction set, which allows the algorithm to achieve near-optimal efficiency and valid coverage. The proposed method is based on the constrained empirical risk minimization (ERM) problem, where the coverage constraint is enforced by a differentiable surrogate loss and Lagrangians. ","This paper proposes a meta-algorithm for conformal prediction for the constrained empirical risk minimization (ERM) problem, where the prediction set is constrained to be close to the empirical coverage of the data set. The authors propose a gradient-based algorithm for this problem, which uses differentiable surrogate losses and Lagrangians to achieve near-optimal efficiency. The proposed method is evaluated on a variety of tasks, including multi-output regression, minimum-volume prediction sets, and label prediction sets."
5461,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"reinforcement learning based approach USED-FOR query object localization. ordinal metric learning USED-FOR exemplary set. exemplary set USED-FOR transferable reward signal. ordinal metric learning USED-FOR transferable reward signal. method COMPARE fine - tuning approaches. fine - tuning approaches COMPARE method. method USED-FOR test - time policy adaptation. annotated images USED-FOR fine - tuning approaches. corrupted MNIST CONJUNCTION CU - Birds. CU - Birds CONJUNCTION corrupted MNIST. CU - Birds CONJUNCTION COCO datasets. COCO datasets CONJUNCTION CU - Birds. COCO datasets EVALUATE-FOR approach. corrupted MNIST EVALUATE-FOR approach. OtherScientificTerm are reward signals, and transferable reward. "," is a reinforcement learning-based approach to query object localization. The proposed method is based on the observation that the reward signals are not transferable across tasks. To address this issue, the authors propose to learn an exemplary set from annotated images and use this set as a reward signal to improve the transferability of the query object. The experimental results on corrupted MNIST, CU-Birds and COCO datasets show the effectiveness of the proposed method.  ","This paper proposes a new method for fine-tuning a query object localization problem. The proposed method is based on the notion of ""transferable reward"", which is defined as a set of samples that can be used to learn a transferable reward signal. The authors show that the proposed method outperforms the state-of-the-art methods on corrupted MNIST and COCO datasets."
5477,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"Transformers USED-FOR vision tasks. feature matching CONJUNCTION stereo. stereo CONJUNCTION feature matching. object detection CONJUNCTION feature matching. feature matching CONJUNCTION object detection. them USED-FOR vision tasks. dense predictions USED-FOR vision tasks. object detection HYPONYM-OF vision tasks. feature matching HYPONYM-OF vision tasks. quadtree transformer USED-FOR attention. token pyramids USED-FOR quadtree transformer. flops reduction EVALUATE-FOR stereo matching. quadtree attention USED-FOR vision tasks. top-1 accuracy EVALUATE-FOR ImageNet classification. ScanNet USED-FOR feature matching. feature matching EVALUATE-FOR quadtree attention. flops reduction EVALUATE-FOR quadtree attention. feature matching HYPONYM-OF vision tasks. Metric are quadratic computational complexity, and computational complexity. Method is QuadTree Attention. OtherScientificTerm is attention scores. Task is COCO object detection. ","This paper proposes a transformer-based method to reduce the computational complexity of attention in vision tasks. The proposed method is based on the idea of token pyramids, where each token is represented as a set of tokens in a quadratic pyramid. The authors show that the proposed method can reduce the computation complexity by a factor of quadratically increasing the number of tokens. The method is evaluated on ImageNet classification, feature matching and object detection tasks.",This paper proposes a transformer-based attention method for COCO object detection and feature matching tasks. The main idea is to use token pyramids as a transformer for the attention task. The proposed method is evaluated on ImageNet classification and ScanNet and achieves top-1 accuracy on both tasks. 
5493,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"reusable options USED-FOR unknown task distribution. options USED-FOR transfer learning. options CONJUNCTION state transitions. state transitions CONJUNCTION options. MI USED-FOR method. scalable approximation USED-FOR MI maximization. scalable approximation USED-FOR InfoMax Termination Critic ( IMTC ) algorithm. gradient ascent USED-FOR scalable approximation. gradient ascent USED-FOR MI maximization. extrinsic rewards CONJUNCTION intrinsic rewards. intrinsic rewards CONJUNCTION extrinsic rewards. IMTC USED-FOR diversity of learned options. IMTC USED-FOR quick adaptation. IMTC USED-FOR complex domains. Method are reinforcement learning, and mutual information ( MI ) based skill learning. OtherScientificTerm is reusable building blocks. Task is learning reusable options. ","This paper proposes a method for learning reusable options for transfer learning from a task to a new task using mutual information maximization (MI) based skill learning. The proposed method is based on the InfoMax Termination Critic (IMTC) algorithm, which is a scalable approximation of MI maximization with gradient ascent. Theoretical analysis is provided to show that the proposed method can be used to efficiently learn options and state transitions. Experiments show that IMTC is able to learn reusable options with good performance on a variety of tasks. ","This paper proposes a new method for learning reusable options for transfer learning in the context of mutual information (MI) based skill learning. The method is based on the InfoMax Termination Critic (IMTC) algorithm, which is a scalable approximation of MI maximization. The authors show that IMTC maximizes the mutual information maximization (MI maximization) in a scalable manner. They also show that the method can be applied to complex domains. "
5509,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"Open - World Object Detection HYPONYM-OF object detection paradigm. openworld object detector USED-FOR unknown objects. independent modules USED-FOR unknown categories. independent modules USED-FOR incremental learning. node PART-OF semantic topology. discriminative feature representations USED-FOR constraint. semantic topology COMPARE open - world object detectors. open - world object detectors COMPARE semantic topology. semantic topology USED-FOR open - world object detection. well - trained language model USED-FOR semantic topology. OtherScientificTerm are Semantic Topology, and features. Method are open - world object detector, and detector. Metric is absolute open - set error. ",This paper proposes a novel open-world object detection method based on semantic topology. Semantic topology is defined as a set of independent modules that can be used to classify unknown objects into unknown categories. The proposed method is based on a well-trained language model and uses a discriminative feature representation to enforce a constraint on the open-set error. The experimental results show that the proposed method outperforms the state-of-the-art methods in terms of open set error. ,"This paper proposes a new open-world object detection framework for semantic topology. The key idea is to use a well-trained language model to model the semantic structure of the object detection problem. The semantic structure is defined as a set of independent modules that can be used to learn the open-set error of the detector. The proposed method is evaluated on a variety of datasets, and compared with the state-of-the-art."
5525,SP:97f618558f4add834e5930fd177f012a753247dc,Deep learning USED-FOR vision and natural language processing. computation CONJUNCTION human labeling effort. human labeling effort CONJUNCTION computation. deep learning models COMPARE ones. ones COMPARE deep learning models. dataset USED-FOR ones. Prior methods USED-FOR submodular objective functions. predicted class labels CONJUNCTION decision boundaries. decision boundaries CONJUNCTION predicted class labels. balancing constraints FEATURE-OF predicted class labels. balancing constraints FEATURE-OF decision boundaries. matroids HYPONYM-OF algebraic structure. algebraic structure USED-FOR linear independence. vector spaces FEATURE-OF linear independence. constant approximation guarantees FEATURE-OF greedy algorithm. matroids USED-FOR constraints. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. ImageNet CONJUNCTION long - tailed datasets. long - tailed datasets CONJUNCTION ImageNet. long - tailed datasets HYPONYM-OF classification datasets. CIFAR-100 - LT HYPONYM-OF long - tailed datasets. long - tailed datasets EVALUATE-FOR baselines. ImageNet HYPONYM-OF classification datasets. CIFAR10 HYPONYM-OF classification datasets. classification datasets EVALUATE-FOR baselines. CIFAR-100 HYPONYM-OF classification datasets. Generic is models. ,"This paper proposes a method to learn a submodular objective function that is independent of the predicted class labels and decision boundaries. The main idea is to use matroids as constraints to balance the predicted labels and the decision boundaries, and then use a greedy algorithm to compute the submodularity. The proposed method is evaluated on CIFAR-10, ImageNet, and long-tailed datasets. ",This paper proposes a greedy algorithm for learning submodular objective functions with linear independence. The main idea is to use a matroid-like algebraic structure to represent the submodularity of the objective function. The authors show that their algorithm can achieve constant approximation guarantees for linear independence in the space of matroids. They also show that the algorithm can be applied to long-tailed datasets.   
5541,SP:e0432ff922708c6c6e59124d27c1386605930346,"models USED-FOR semantic segmentation. adaptive inference strategy USED-FOR semantic segmentation. Instance - adaptive Batch Normalization ( IaBN ) USED-FOR normalization layers. feature statistics USED-FOR normalization layers. test - time training ( TTT ) approach USED-FOR semantic segmentation. Seg - TTT HYPONYM-OF test - time training ( TTT ) approach. self - supervised loss USED-FOR Seg - TTT. self - supervised loss USED-FOR model parameters. techniques COMPARE baseline. baseline COMPARE techniques. accuracy EVALUATE-FOR generalization methods. techniques COMPARE generalization methods. generalization methods COMPARE techniques. generalization USED-FOR semantic segmentation. accuracy EVALUATE-FOR techniques. Metric is Out - of - distribution robustness. Generic are model, and complementary techniques. ", semantic segmentation is an important task in machine learning. This paper proposes to use an instance-adaptive Batch Normalization (IaBN) to improve the generalization performance of semantic segmentations. The proposed method is based on a self-supervised loss and a test-time training (TTT) approach. Experiments show that the proposed method outperforms the baselines in terms of OOD robustness.,"This paper proposes a new approach for semantic segmentation, called Seg-TTT. Seg-TTT is a self-supervised test-time training (TTT) approach that aims to improve the generalization performance of the model. The main idea is to use an instance-adaptive Batch Normalization (IaBN) to normalize the feature statistics of the normalization layers. The authors show that the proposed approach is more robust to out-of-distribution (OOD) attacks than the baseline methods."
5557,SP:427100edad574722a6525ca917e84f817ff60d7e,contrastive loss USED-FOR mappings. mappings USED-FOR contrastive loss. Material is tabular data. Generic is method. Method is default set rule of hyperparameters selection. ,This paper studies the problem of choosing a set of hyperparameters for tabular data. The authors propose to use contrastive loss to select the set of mappings from the data. They show that the default set rule is sufficient to select mappings that are close to the true data distribution. They also show that this is the optimal hyperparameter selection strategy. ,"This paper proposes a new contrastive mapping method for tabular data. The key idea is to use contrastive loss as the default set rule of hyperparameters selection. The proposed method is based on the following assumptions: 1. The data is represented as a set of tabular mappings, 2. The mappings are mapped to the same set of data, and 3. The default set of mappings is the same for all mappings.  The authors show that the proposed method can be applied to a variety of settings, including tabular and non-tabular datasets."
5573,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,dimensional approach USED-FOR psychiatric classification. mining corresponded nosological relations USED-FOR low - dimensional embedding space. diagnostic information USED-FOR optimal embedding space. dual utilisation of diagnostic information PART-OF conditional variational auto - encoder. major depressive disorder CONJUNCTION schizophrenia. schizophrenia CONJUNCTION major depressive disorder. autism spectrum disorder CONJUNCTION major depressive disorder. major depressive disorder CONJUNCTION autism spectrum disorder. approaches USED-FOR synthetic functional connectivity features. autism spectrum disorder HYPONYM-OF nosological relation. major depressive disorder HYPONYM-OF nosological relation. empirical neuropsychiatric neuroimaging datasets EVALUATE-FOR approach. OtherScientificTerm is neuropsychiatric disorders. ,"This paper proposes a novel approach for neuropsychiatric classification based on the notion of ""neurological relations"" (Nosological Relations). The authors propose to use a low-dimensional embedding space to model the functional connectivity between the data points in a low dimensional space. The proposed approach is based on a conditional variational auto-encoder (CVAE) that is trained to extract the corresponding functional connectivity features from the data. The authors show that the proposed approach achieves state-of-the-art performance on a variety of datasets. ","This paper proposes a low-dimensional approach for neuropsychiatric classification. The authors propose a conditional variational auto-encoder (CVAE) based on the notion of ""neuropsychiatric relations"" (i.e., the relationship between a person with a neuro-psychiatric disorder and a person who does not have such a disorder). The proposed CVAE is based on a dual utilisation of diagnostic information, where the diagnostic information is used to construct a low dimensional embedding space for the data, and the data is then used to train the CVAE. The proposed method is evaluated on a variety of datasets, and it is shown that the proposed method outperforms the state-of-the-art. "
5589,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,quantum neural networks USED-FOR quantum learning tasks. trainable quantum tensor network ( QTN ) USED-FOR quantum embedding. variational quantum circuit ( VQC ) USED-FOR quantum embedding. QTN - VQC HYPONYM-OF end - to - end learning framework. parametric tensor - train network CONJUNCTION tensor product encoding. tensor product encoding CONJUNCTION parametric tensor - train network. tensor product encoding USED-FOR quantum embedding. parametric tensor - train network USED-FOR feature extraction. architecture USED-FOR QTN. parametric tensor - train network PART-OF architecture. parametric tensor - train network PART-OF QTN. tensor product encoding PART-OF QTN. tensor product encoding PART-OF architecture. QTN USED-FOR quantum embedding. QTN USED-FOR end - to - end parametric model pipeline. QTN - VQC HYPONYM-OF end - to - end parametric model pipeline. QTN USED-FOR quantum embedding. MNIST dataset EVALUATE-FOR QTN. QTN COMPARE quantum embedding approaches. quantum embedding approaches COMPARE QTN. ,This paper proposes a trainable quantum tensor network (QTN) for quantum embedding. The QTN consists of a parametric tensor-train network and a variational quantum circuit (VQC). The VQC is used to train the QTN and the tensor product encoding is used for feature extraction. The experimental results show that the proposed QTN outperforms the state-of-the-art on the MNIST dataset. ,"This paper proposes a parametric quantum tensor network (QTN-VQC) for quantum embedding. The QTN is an end-to-end learning framework for quantum learning with a variational quantum circuit (vQC). The VQC is used to train a tensor-train network and tensor product encoding for the QTN, which is used for feature extraction. Experiments on MNIST dataset show that the proposed QTN outperforms the state-of-the-art in terms of performance. "
5605,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"networks USED-FOR task. networks USED-FOR algorithms. neural networks USED-FOR high - level computational processes. algorithm USED-FOR low - dimensional manifolds. meta - model USED-FOR hidden states. meta - model USED-FOR DYNAMO. model PART-OF meta - model. pre - trained neural networks USED-FOR DYNAMO. model embedding vectors USED-FOR manifold. model embedding vector USED-FOR model. RNNs CONJUNCTION CNNs. CNNs CONJUNCTION RNNs. DYNAMO USED-FOR RNNs. DYNAMO USED-FOR CNNs. model embedding spaces USED-FOR applications. neural networks USED-FOR operable neural network. model embedding spaces USED-FOR clustering of neural networks. neural networks USED-FOR task. high - level computational processes USED-FOR clustering of neural networks. semi - supervised learning HYPONYM-OF applications. clustering of neural networks HYPONYM-OF applications. optimization USED-FOR semi - supervised learning. high - level computational processes FEATURE-OF topology of RNN dynamics. RNNs USED-FOR fixed - point analysis of meta - models. Method are deep neural networks, and neural network model. Generic is models. OtherScientificTerm are reparameterization, and model embedding space. ","This paper proposes a meta-learning algorithm called DYNDO, which is based on embedding a model into a low-dimensional manifold. The embedding space of the model is modeled as a set of embedding vectors, and the model embedding vector is used to represent the hidden states of the network. The authors show that the embeddings can be used to model the dynamics of RNNs and CNNs, and show that their method can be applied to semi-supervised learning. ","This paper proposes a new method for embedding neural networks on low-dimensional manifolds. The method, called DYNAMO, is based on a meta-model, where a model embedding vector is used to represent the hidden states of the network. The authors show that the embedding space can be used to model the dynamics of RNNs and CNNs, and that it can be applied to semi-supervised learning and clustering of neural networks.  "
5621,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"constraint - based approach COMPARE direct prediction. direct prediction COMPARE constraint - based approach. direct prediction USED-FOR simulation engines. constraint - based approach USED-FOR simulation engines. framework USED-FOR constraint - based learned simulation. trainable function approximator USED-FOR scalar constraint function. gradient descent USED-FOR constraint solver. graph neural network USED-FOR constraint function. graph neural network CONJUNCTION gradient descent. gradient descent CONJUNCTION graph neural network. gradient descent USED-FOR method. graph neural network USED-FOR method. backpropagation USED-FOR architecture. colliding irregular shapes CONJUNCTION splashing fluids. splashing fluids CONJUNCTION colliding irregular shapes. bouncing balls CONJUNCTION colliding irregular shapes. colliding irregular shapes CONJUNCTION bouncing balls. simulated ropes CONJUNCTION bouncing balls. bouncing balls CONJUNCTION simulated ropes. physical domains EVALUATE-FOR model. colliding irregular shapes HYPONYM-OF physical domains. splashing fluids HYPONYM-OF physical domains. simulated ropes HYPONYM-OF physical domains. bouncing balls HYPONYM-OF physical domains. model COMPARE simulators. simulators COMPARE model. forward learned simulators USED-FOR constraint - based framework. numerical methods USED-FOR learned models. Method are physical simulators, forward model, and forward approaches. Task is constraint satisfaction problem. Metric is simulation accuracy. OtherScientificTerm is hand - designed constraints. ","This paper proposes a method to learn a constraint-based simulation model for physical simulation. The method is based on a GNN-based constraint solver, which is trained with gradient descent to solve the constraint satisfaction problem. The proposed method is evaluated on a variety of simulated physical domains, and achieves state-of-the-art results.  ",This paper proposes a method to learn a constraint-based learned simulation model for physical simulation. The method is based on a graph neural network and gradient descent to solve the problem of constraint satisfaction. The authors show that their method is able to achieve state-of-the-art performance on a variety of physical simulation tasks. 
5637,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"exploration CONJUNCTION few - shot adaptation. few - shot adaptation CONJUNCTION exploration. policies USED-FOR exploration. diverse behaviors FEATURE-OF policies. controlling robots HYPONYM-OF real - world scenarios. iterative reproduction CONJUNCTION selection of policies. selection of policies CONJUNCTION iterative reproduction. iterative reproduction PART-OF evolutionary techniques. selection of policies USED-FOR methods. iterative reproduction USED-FOR methods. evolutionary techniques USED-FOR methods. EDO - CS HYPONYM-OF Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR Evolutionary Diversity Optimization algorithm. Clusteringbased Selection USED-FOR EDO - CS. EDO - CS COMPARE methods. methods COMPARE EDO - CS. EDO - CS USED-FOR policies. continuous control tasks EVALUATE-FOR EDO - CS. EDO - CS COMPARE EDO - CS. EDO - CS COMPARE EDO - CS. Method is Reinforcement Learning ( RL ). Generic is task. OtherScientificTerm are selection mechanisms, and clusters. Task are reproduction, and reproduction process. ","This paper proposes a new algorithm for selecting policies for reinforcement learning tasks. The proposed method is based on the idea that the goal of RL is to learn policies with diverse behaviors in order to improve exploration and few-shot adaptation. To do so, the authors propose to use a clustering-based selection strategy to select policies from a set of policies that are close to each other in terms of the number of clusters. The authors show that the proposed method outperforms the baselines on a variety of continuous control tasks.","This paper proposes a new evolutionary diversity optimisation algorithm, EDO-CS, which is based on cluster-based selection. The key idea is to learn a set of clusters of policies that can be used to select the best policies for a given task. The authors show that the proposed method outperforms the state-of-the-art in terms of diversity on a variety of continuous control tasks."
5653,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"Markovian noise CONJUNCTION general consensus - type interaction. general consensus - type interaction CONJUNCTION Markovian noise. general consensus - type interaction USED-FOR multi - agent linear stochastic approximation algorithm. Markovian noise USED-FOR multi - agent linear stochastic approximation algorithm. time - varying directed graph USED-FOR interconnection structure. doubly stochastic matrices USED-FOR interconnection. finite - time bounds USED-FOR mean - square error. interaction matrices FEATURE-OF uniformly strongly connected graph sequences. stochastic matrices USED-FOR consensus - type algorithm. OtherScientificTerm are local stochastic approximation process, interconnection matrix, ordinary differential equation, interconnection matrices, local equilibria, communication, constant and time - varying step - sizes, and convex combination. Method are consensus - based stochastic approximation algorithms, and push - type distributed stochastic approximation algorithm. Generic is algorithm. ","This paper studies the multi-agent linear stochastic approximation with Markovian noise in a time-varying directed graph. The authors show that the interconnection matrix of the directed graph is doubly-stochastic matrices, which is a convex combination of the interaction matrices of uniformly strongly connected graph sequences. The interconnection matrices are then used to compute the mean-square error of the algorithm, and the authors prove finite-time bounds on the mean square error.  ","This paper studies the problem of multi-agent linear stochastic approximation with Markovian noise and general consensus-type interaction in a time-varying directed graph. The authors prove a finite-time bounds on the mean square error of the mean-square error of their algorithm. They show that the interconnection matrices of the graph are doubly-stochastic matrices, and they show that their algorithm converges to a convex combination of the two matrices. They also show that for push-type distributed algorithms, they can converge to a local equilibria."
5669,SP:f7f96d545a907887396393aba310974f4d3f75ff,"ones HYPONYM-OF methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR methods. equivariant Graph Neural Networks ( GNNs ) USED-FOR ones. forward kinematics information FEATURE-OF structural object. it USED-FOR forward kinematics information. generalized coordinates USED-FOR forward kinematics information. generalized coordinates USED-FOR it. forward kinematics USED-FOR geometrical constraints. dynamics of constrained systems COMPARE unconstrained counterparts. unconstrained counterparts COMPARE dynamics of constrained systems. equivariant message passing USED-FOR GMN. orthogonality - equivariant functions USED-FOR equivariant message passing. molecular dynamics prediction CONJUNCTION human motion capture. human motion capture CONJUNCTION molecular dynamics prediction. sticks CONJUNCTION hinges. hinges CONJUNCTION sticks. particles CONJUNCTION sticks. sticks CONJUNCTION particles. prediction accuracy CONJUNCTION constraint satisfaction. constraint satisfaction CONJUNCTION prediction accuracy. GMN COMPARE GNNs. GNNs COMPARE GMN. constraint satisfaction CONJUNCTION data efficiency. data efficiency CONJUNCTION constraint satisfaction. real - world datasets USED-FOR molecular dynamics prediction. real - world datasets USED-FOR human motion capture. particles PART-OF simulated systems. sticks PART-OF simulated systems. simulated systems EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GMN. constraint satisfaction EVALUATE-FOR GNNs. data efficiency EVALUATE-FOR GMN. data efficiency EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GNNs. prediction accuracy EVALUATE-FOR GMN. Task are machine learning, and interacting systems. OtherScientificTerm is constrained systems. Method are Graph Mechanics Network ( GMN ), and equivariant formulation. ","This paper proposes an equivariant graph neural network (GNN) architecture for learning dynamics models in constrained systems. The proposed method is based on the idea that the dynamics of constrained systems can be represented as a set of geometrical constraints, which can be expressed as orthogonality-equivariant functions. The authors propose to use the forward kinematics information of a structural object as a way to encode geometrically-constrained information about the structure of the system.    The authors show that the proposed method, Graph Mechanics Network (GMN), is able to solve the problem of learning the dynamics in a constrained system. The main contribution of the paper is to introduce a novel equivariance-based message-passing method for the dynamics modeling. The method is shown to achieve state-of-the-art performance on simulated and real-world molecular dynamics tasks.","This paper proposes a graph neural network (GNN) based on equivariant graph mechanics (GMN) for predicting the dynamics of constrained systems. GMN is based on the orthogonality-equivariant message passing (e.g., orthogonal equivariance) which is used to pass the forward kinematics information of a structural object to a GNN. The authors show that GMN outperforms the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction, and data efficiency. "
5685,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"framework USED-FOR federated learning. partial model personalization USED-FOR federated learning. partial model personalization USED-FOR framework. full model personalization COMPARE partial model personalization. partial model personalization COMPARE full model personalization. domain knowledge USED-FOR model. domain knowledge USED-FOR partial model personalization. federated optimization algorithms USED-FOR partially personalized models. them USED-FOR deep learning models. algorithms USED-FOR minimizing smooth nonconvex functions. alternating update algorithm COMPARE simultaneous update algorithm. simultaneous update algorithm COMPARE alternating update algorithm. partial model personalization USED-FOR full model personalization. personalized parameters USED-FOR partial model personalization. personalized parameters USED-FOR full model personalization. OtherScientificTerm are on - device memory footprint, shared and personal parameters, shared parameters, and smooth nonconvex functions. Material is real - world image and text datasets. ","This paper proposes a federated learning framework called partial model personalization, which is an extension of federated optimization (FoE) in which the model parameters are shared between clients. The authors show that the proposed method can be used to improve the performance of the federated models in terms of performance on image classification and text classification tasks.   The main contribution of the paper is a theoretical analysis of the convergence of the proposed methods. The main results are:  1. The proposed method is able to achieve better performance than the state-of-the-art federated optimisation algorithms.  2. The method is shown to be able to outperform existing methods for federated training on ImageNet and TextNet. ","This paper proposes a new framework for partial model personalization for federated learning. The proposed framework is based on the idea of partial personalization, which is an extension of federated optimization to partially personalized models. In particular, the authors propose an alternating update algorithm for minimizing smooth nonconvex functions. The authors show that the proposed method outperforms the previous state-of-the-art algorithms in terms of on-device memory footprint. "
5701,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"contrastive learning HYPONYM-OF unsupervised representation learning. contrastive learning PART-OF machine learning. augmentation operations USED-FOR representations. framework USED-FOR unsupervised learning of object representations. CLTT USED-FOR viewing sequences. CLTT USED-FOR supervised learning. ThreeDWorld ( TDW ) USED-FOR near - photorealistic training environment. ThreeDWorld ( TDW ) USED-FOR data set. near - photorealistic training environment USED-FOR data set. CLTT USED-FOR linear classification. OtherScientificTerm are supervision, positive pairs, temporal structure, and representational similarity. Material are biology, and natural videos. Method are object representations, Contrastive Learning Through Time ( CLTT ), contrastive learning methods, and fully supervised setting. Generic is data sets. ","This paper proposes a method for unsupervised learning of object representations in videos. The proposed method is based on contrastive learning through time (CLTT), where the goal is to learn the representation of an object in a sequence of videos. To achieve this goal, the authors propose to use an augmented version of ThreeDWorld as a training data set, which is a near-photorealistic version of the real world. Experiments show that the proposed method outperforms the state-of-the-art methods in linear classification tasks. ","This paper proposes Contrastive Learning Through Time (CLTT), a contrastive learning framework for unsupervised learning of object representations in natural videos. The proposed method is based on the ThreeDWorld (TDW) training environment, which is a near-photorealistic training environment. The authors show that CLTT can be applied to both fully supervised and fully supervised data sets. CLTT is shown to outperform the state-of-the-art in linear classification, and it can be used to improve the performance of fully supervised learning."
5717,SP:2fb4af247b5022710b681037faca2420207a507a,deterministic transition model USED-FOR goal - directed planning. Monte Carlo Tree Search USED-FOR deterministic control problems. function approximators USED-FOR MCTS. MCTS USED-FOR continuous domains. MCTS USED-FOR AlphaZero family of algorithms. function approximators USED-FOR tree. algorithms USED-FOR control problems. sparse rewards FEATURE-OF control problems. goal - directed domains HYPONYM-OF sparse rewards. AlphaZero USED-FOR goal - directed planning tasks. Hindsight Experience Replay USED-FOR AlphaZero. application USED-FOR quantum compiling domain. application HYPONYM-OF simulated domains. quantum compiling domain HYPONYM-OF simulated domains. simulated domains EVALUATE-FOR approach. OtherScientificTerm is positive reward. ,This paper proposes a Monte Carlo Tree Search (MCTS) based algorithm for goal-directed planning in deterministic transition models. The main idea is to use a function approximator to search for the optimal solution to the MCTS problem in continuous and discrete domains. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms in goal-driven planning tasks with sparse rewards.,This paper proposes a new Monte Carlo Tree Search (MCTS) algorithm for goal-directed planning in deterministic control problems. The main idea is to use MCTS to solve a deterministic transition model with sparse rewards. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms in a variety of domains. They also show that their algorithm can be applied to a quantum-computing domain. 
5733,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"network capacity CONJUNCTION data replay. data replay CONJUNCTION network capacity. network capacity USED-FOR methods. data replay USED-FOR methods. virtual Feature Encoding Layer ( FEL ) USED-FOR network structures. iteratively updated optimizer CONJUNCTION virtual Feature Encoding Layer ( FEL ). virtual Feature Encoding Layer ( FEL ) CONJUNCTION iteratively updated optimizer. iteratively updated optimizer USED-FOR gradient. task descriptors USED-FOR virtual Feature Encoding Layer ( FEL ). iteratively updated optimizer PART-OF RGO. virtual Feature Encoding Layer ( FEL ) PART-OF RGO. task descriptors USED-FOR network structures. 20 - split - CIFAR100 CONJUNCTION 20 - split - miniImageNet. 20 - split - miniImageNet CONJUNCTION 20 - split - CIFAR100. RGO COMPARE baselines. baselines COMPARE RGO. continual classification benchmarks EVALUATE-FOR baselines. 20 - split - miniImageNet EVALUATE-FOR RGO. 20 - split - CIFAR100 EVALUATE-FOR RGO. continual classification benchmarks EVALUATE-FOR RGO. method USED-FOR continual learning capabilities. average accuracy EVALUATE-FOR Single - Task Learning ( STL ). Single - Task Learning ( STL ) COMPARE method. method COMPARE Single - Task Learning ( STL ). average accuracy EVALUATE-FOR method. continual learning capabilities USED-FOR learning models. gradient descent USED-FOR learning models. Method are Continual Learning ( CL ), neural networks, and Recursive Gradient Optimization ( RGO ). Generic is approach. ","This paper proposes Recursive Gradient Optimization (RGO), a method for continual learning in deep neural networks. The main idea is to use a virtual feature encoding layer (FEL) to encode the task-specific features and then update the optimizer using gradient descent. Experiments show that RGO outperforms single-task learning (STL) on continual learning benchmarks. ","This paper proposes Recursive Gradient Optimization (RGO), a method for continual learning in the context of continual learning (CL). The main idea of RGO is to use a virtual feature encoding layer (FEL) to encode the task-specific features of a neural network, which is then used to guide the gradient descent of the model. The authors show that RGO outperforms the state-of-the-art in continual learning on CIFAR-100 and mini-ImageNet datasets. "
5749,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"fine - tuning USED-FOR them. aligned StyleGAN models USED-FOR image - to - image translation. human faces CONJUNCTION churches. churches CONJUNCTION human faces. latent spaces FEATURE-OF child model. churches HYPONYM-OF distant data domains. human faces HYPONYM-OF distant data domains. aligned models USED-FOR tasks. image translation CONJUNCTION automatic cross - domain image morphing. automatic cross - domain image morphing CONJUNCTION image translation. child domain USED-FOR zero - shot vision tasks. fine - tuning CONJUNCTION inversion. inversion CONJUNCTION fine - tuning. fine - tuning USED-FOR approach. inversion USED-FOR approach. Method are aligned generative models, and StyleGAN. Generic are models, and they. OtherScientificTerm are architecture, and supervision. Task are transfer learning, and model alignment. ","This paper studies the problem of model alignment for image-to-image translation and zero-shot vision tasks. The authors propose to use aligned StyleGAN models, which are trained with fine-tuning in the latent space of the child model. They show that the aligned models are able to achieve state-of-the-art performance on image translation and automatic cross-domain image morphing tasks.   ","This paper proposes a method for learning aligned generative models for image-to-image translation and cross-domain image morphing tasks. The proposed method is based on the idea that aligned models can be fine-tuned in the latent space of the child model, and that the model alignment can be improved by inversion. The method is evaluated on a variety of zero-shot vision tasks, and it is shown that the proposed method outperforms the state-of-the-art methods."
5765,SP:0e13f831c211626195c118487f2fff36a6e293f6,Comparing structured objects PART-OF learning tasks. graphs HYPONYM-OF Comparing structured objects. Optimal Transport ( OT ) USED-FOR Gromov - Wasserstein ( GW ) distance. nodes connectivity relations USED-FOR GW. GW USED-FOR graphs. probability measures USED-FOR GW. conservation of mass USED-FOR OT. graph dictionary CONJUNCTION partition learning. partition learning CONJUNCTION graph dictionary. semi - relaxed Gromov - Wasserstein divergence USED-FOR it. partition learning HYPONYM-OF tasks. graph dictionary HYPONYM-OF tasks. it USED-FOR graph dictionary learning algorithm. partitioning CONJUNCTION clustering. clustering CONJUNCTION partitioning. clustering CONJUNCTION completion. completion CONJUNCTION clustering. graphs USED-FOR complex tasks. completion HYPONYM-OF complex tasks. completion HYPONYM-OF graphs. partitioning HYPONYM-OF graphs. partitioning HYPONYM-OF complex tasks. clustering HYPONYM-OF graphs. clustering HYPONYM-OF complex tasks. OtherScientificTerm is nodes. ,"This paper proposes to use the Gromov-Wasserstein distance between nodes in a graph as a metric to measure the distance between the nodes in the graph. The proposed metric is based on the Optimal Transport (OT) distance, which is a measure of the similarity between two nodes. Theoretical analysis is provided to show that the proposed metric can be used for graph dictionary learning, partitioning, clustering, and completion tasks.  ","This paper proposes a novel Gromov-Wasserstein (GW) distance between two graphs, which is based on the Optimal Transport (OT) distance. The authors show that the GW distance can be approximated by a semi-relaxed version of the GP divergence between the two graphs. They also show that it can be used for graph dictionary learning, partitioning, clustering, and completion tasks. "
5781,SP:d6d144be11230070ae9395db70b7c7743540bad4,"prediction CONJUNCTION collaboration. collaboration CONJUNCTION prediction. Models USED-FOR prediction. Models USED-FOR collaboration. ones HYPONYM-OF categories. ones HYPONYM-OF categories. imitation learning USED-FOR ones. predicting policies USED-FOR systematic suboptimality. Bayesian inference USED-FOR systematic deviations. Boltzmann policy distribution ( BPD ) USED-FOR human policies. sampling CONJUNCTION inference. inference CONJUNCTION sampling. generative and sequence models USED-FOR sampling. generative and sequence models USED-FOR inference. high - dimensional continuous space FEATURE-OF policies. BPD USED-FOR prediction of human behavior. prediction of human behavior CONJUNCTION human - AI collaboration. human - AI collaboration CONJUNCTION prediction of human behavior. BPD USED-FOR human - AI collaboration. human - AI collaboration CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION human - AI collaboration. prediction of human behavior CONJUNCTION imitation learning - based human models. imitation learning - based human models CONJUNCTION prediction of human behavior. BPD USED-FOR imitation learning - based human models. OtherScientificTerm are human behavior, reward function, Boltzmann rationality, and trajectories. Generic are former, and models. Material is human data. ",This paper proposes a Bayesian approach to model human behavior using Boltzmann rationality. The approach is based on Bayesian inference and Bayesian sampling. The authors show that the proposed approach is able to learn policies that are robust to systematic suboptimality. The main contribution of the paper is the use of Boltzman rationality to model the human behavior.   ,"This paper proposes a new method for predicting the Boltzmann policy distribution (BPD) of human policies. The BPD is based on the idea that the reward function is a function of the rationality of the agent. The authors show that the BPD can be used to predict the behavior of humans in a high-dimensional continuous space, and that it can be applied to human-AI collaboration. They also show that BPDs can be combined with imitation learning-based human models to predict human behavior."
5797,SP:401ef5fe2022e926b0321258efac1f369f186ace,"Quantization of deep neural networks ( DNN ) USED-FOR compressing and accelerating DNN models. Data - free quantization ( DFQ ) HYPONYM-OF approach. accuracy EVALUATE-FOR DFQ solutions. sub - second quantization time FEATURE-OF on - the - fly DFQ framework. inference - only devices USED-FOR networks. SQuant HYPONYM-OF on - the - fly DFQ framework. discrete domain FEATURE-OF data - free optimization objective. computation complexity EVALUATE-FOR objective solver. algorithm USED-FOR objective solver. back - propagation USED-FOR algorithm. computation complexity EVALUATE-FOR algorithm. fine - tuning CONJUNCTION synthetic datasets. synthetic datasets CONJUNCTION fine - tuning. SQuant COMPARE data - free post - training quantization. data - free post - training quantization COMPARE SQuant. SQuant USED-FOR data - free quantization process. SQuant USED-FOR sub - second level. synthetic datasets EVALUATE-FOR SQuant. accuracy EVALUATE-FOR models. sub - second level FEATURE-OF data - free quantization process. 4 - bit quantization FEATURE-OF models. accuracy EVALUATE-FOR SQuant. Method are DNN models, and network architecture. OtherScientificTerm are privacy - sensitive and confidential scenarios, DNN task loss, Hessian - based optimization objective, diagonal sub - items, and weight tensor. Material is synthetic data. Metric is computation and memory requirements. ","This paper proposes a data-free post-training quantization method for deep neural networks (DNNs) that can be applied on inference-only devices without access to the training data. The proposed method, called SQuant, is an on-the-fly quantization (DFQ) method that uses a discrete optimization objective in the form of a Hessian-based objective solver. Theoretical analysis shows that the proposed method can achieve sub-second quantization time in the training stage, and achieves state-of-the art accuracy on several benchmark datasets. ","This paper proposes a new data-free quantization (DFQ) method for training deep neural networks (DNNs). The main idea is to use a Hessian-based objective solver for quantization, which can be applied to any discrete domain. The main contribution of this paper is to propose a new quantization method that is on-the-fly and can be used for training DNNs on inference-only devices. The proposed method, SQuant, achieves sub-second quantization time, which is significantly faster than the current state-of-the art quantization methods. "
5813,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"stock market partitioning CONJUNCTION sleep stage labelling. sleep stage labelling CONJUNCTION stock market partitioning. sleep stage labelling CONJUNCTION human activity recognition. human activity recognition CONJUNCTION sleep stage labelling. Time series USED-FOR tasks. human activity recognition HYPONYM-OF tasks. stock market partitioning HYPONYM-OF tasks. sleep stage labelling HYPONYM-OF tasks. sliding window USED-FOR sub - sequences. overlapping stride FEATURE-OF sliding window. sliding window USED-FOR time series. accuracy EVALUATE-FOR segmentation. approach USED-FOR approximate breakpoints. it USED-FOR long - term dependencies. bi - pass architecture USED-FOR SegTime. Task are time series segmentation, and classification. OtherScientificTerm are precise breakpoints, sliding windows, and label changing frequency. ","This paper proposes SegTime, a method for time series segmentation and classification. The proposed method is based on a bi-pass architecture, where a sliding window is used to model the sub-sequences of the time series. The authors show that SegTime is able to identify approximate breakpoints in time series, which are then used for classification and segmentation. ","This paper proposes SegTime, a bi-pass architecture for time series segmentation and classification. SegTime uses a sliding window to segment time series into sub-sequences with overlapping stride. The sliding window is used to estimate the approximate breakpoints for each sub-sequence. The authors show that SegTime is able to identify long-term dependencies between sub-series, and can be used to segment the time series."
5829,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,Graph Neural Networks ( GNNs ) USED-FOR graph data. faithfulness problems CONJUNCTION unnatural artifacts. unnatural artifacts CONJUNCTION faithfulness problems. methods USED-FOR approximation based and perturbation based approaches. faithful explanation USED-FOR GNN predictions. subgraph level interpretation algorithm USED-FOR complex interactions. GNN characteristics USED-FOR algorithm. synthetic and real - world datasets EVALUATE-FOR DEGREE. node classification and graph classification tasks EVALUATE-FOR DEGREE. Method is GNNs. Generic is models. OtherScientificTerm is graph nodes. ,"This paper proposes a faithful explanation method for graph neural networks (GNNs) based on a subgraph level interpretation algorithm. The method is based on the observation that GNNs are susceptible to faithfulness problems in the presence of unnatural artifacts. To address this problem, the authors propose a new interpretation algorithm called DEGREE, which uses a GNN-based subgraph interpretation algorithm to find the subgraphs with the most GNN characteristics. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method.","This paper proposes a faithful explanation method for graph neural networks (GNNs). The key idea is to use a subgraph level interpretation algorithm for GNNs to understand the subgraphs of a graph. The method is based on the notion of faithfulness, which is defined as the number of times a GNN predicts a graph node to be true. The authors show that the faithfulness problem can be solved by using a simple subgraph-level interpretation algorithm. The proposed method is evaluated on both synthetic and real-world datasets."
5845,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"strided convolutions CONJUNCTION pooling layers. pooling layers CONJUNCTION strided convolutions. downsampling operators USED-FOR resolution of intermediate representations. downsampling operators PART-OF Convolutional neural networks. strided convolutions HYPONYM-OF downsampling operators. pooling layers HYPONYM-OF downsampling operators. computational complexity EVALUATE-FOR architecture. hyperparameter FEATURE-OF layers. stride FEATURE-OF hyperparameter. crossvalidation CONJUNCTION discrete optimization. discrete optimization CONJUNCTION crossvalidation. architecture search HYPONYM-OF discrete optimization. gradient descent USED-FOR search space. DiffStride HYPONYM-OF downsampling layer. learnable strides FEATURE-OF downsampling layer. layer USED-FOR resizing. layer USED-FOR cropping mask. Fourier domain FEATURE-OF cropping mask. DiffStride USED-FOR downsampling layers. audio and image classification EVALUATE-FOR solution. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. layer PART-OF ResNet-18 architecture. computational complexity EVALUATE-FOR architecture. regularization term USED-FOR architecture. computational complexity EVALUATE-FOR regularization term. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR regularization. accuracy EVALUATE-FOR regularization. ImageNet EVALUATE-FOR regularization. OtherScientificTerm are shift - invariance, integer factor of downsampling, strides, random stride configurations, and learnable variables. Metric is computational cost. Generic is them. ",This paper proposes a new downsampling method called DiffStride to reduce the computational cost of convolutional layers. The proposed method is based on the idea of learning a set of learnable strides for each step of the convolution operation. The method is evaluated on CIFAR-10 and ImageNet and achieves state-of-the-art performance. ,"This paper proposes a new downsampling layer, called DiffStride, to reduce the computational cost of the strided convolution and pooling layers of ResNet-18. The proposed method is based on the notion of shift-invariance, which is defined as the number of strides in the down-sampling process. The authors show that this is a good way to improve the efficiency and accuracy of the network. The method is evaluated on CIFAR-10, Cifar-100, and ImageNet datasets."
5861,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"image segmentation CONJUNCTION node classification. node classification CONJUNCTION image segmentation. Models USED-FOR image segmentation. Models USED-FOR node classification. node classification CONJUNCTION tasks. tasks CONJUNCTION node classification. Models USED-FOR tasks. method USED-FOR strictly local models. collective certificate USED-FOR softly local models. localized randomized smoothing approach USED-FOR certificate. collective guarantees FEATURE-OF locally smoothed model. image segmentation and node classification tasks EVALUATE-FOR locally smoothed model. prediction quality EVALUATE-FOR locally smoothed model. Metric is collective robustness certificate. OtherScientificTerm are small receptive field, and random perturbation strength. ","This paper proposes a novel method to improve the robustness of locally-smoothed models for image segmentation and node classification tasks. The proposed method is based on the idea of a ""collective robustness certificate"", which is defined as the number of perturbations in the receptive field that can be perturbed in a small number of rounds. The authors show that the proposed method can be used for both strictly local models and softly local models, and that it can be combined with a randomized smoothing approach to achieve the collective robustness. ","This paper proposes a new method to improve the robustness of locally smoothed models for image segmentation and node classification tasks. The proposed method is based on a randomized smoothing approach, where the perturbation strength of the model is determined by the number of perturbations in the receptive field. The authors show that the proposed method can improve the performance of the local smoothed model on the task of segmentation, and also on the node classification task."
5877,SP:aacc31e83886c4c997412a1e51090202075eda86,"Normalizing flows USED-FOR general - purpose density estimators. domain - specific knowledge USED-FOR real world applications. general - purpose transformations CONJUNCTION structured layers. structured layers CONJUNCTION general - purpose transformations. general - purpose transformations PART-OF embedded - model flows ( EMF ). equivalent bijective transformations USED-FOR user - specified differentiable probabilistic models. EMFs USED-FOR desirable properties. multimodality CONJUNCTION hierarchical coupling. hierarchical coupling CONJUNCTION multimodality. hierarchical coupling CONJUNCTION continuity. continuity CONJUNCTION hierarchical coupling. EMFs USED-FOR multimodality. EMFs USED-FOR hierarchical coupling. continuity HYPONYM-OF desirable properties. multimodality HYPONYM-OF desirable properties. hierarchical coupling HYPONYM-OF desirable properties. EMFs USED-FOR variational inference. structure of the prior model PART-OF variational architecture. approach COMPARE alternative methods. alternative methods COMPARE approach. common structured inference problems EVALUATE-FOR alternative methods. common structured inference problems EVALUATE-FOR approach. Method are normalizing flows, gated structured layers, and prior model. OtherScientificTerm is domain - specific inductive biases. Generic are layers, and models. ","This paper proposes an embedding-model-flow (EMF) method for density estimation in probabilistic models. The EMF can be viewed as an extension of normalizing flows, where the embedding is a bijective transformation. The authors show that EMFs can be used as a generalization of normalising flows, which can be applied to a variety of differentiable models. In particular, they show that the EMF is able to capture the multimodality and hierarchical coupling properties of the prior model, which are desirable properties for efficient inference.   ","This paper proposes a new normalizing flow (EMF) method for generalizing density estimators. The EMF can be viewed as a bijective transformation for user-specified differentiable probabilistic models. The authors show that the EMF has desirable properties such as multimodality, continuity, and hierarchical coupling. They also show that EMFs can be used for variational inference, and show that it can be applied to a variety of differentiable models."
5893,SP:825a254c0725008143b260ead840ae35f9f096d1,"entities CONJUNCTION objects. objects CONJUNCTION entities. conceptual structure FEATURE-OF rich conceptual structure. LMs USED-FOR grounded world representation. LMs USED-FOR conceptual domain. it USED-FOR concepts. grid world FEATURE-OF concepts. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. GPT-2 HYPONYM-OF generative language models. GPT-3 HYPONYM-OF generative language models. text - only models USED-FOR rich conceptual structure. Method are text - only language models ( LMs ), and grounded language models. Generic are representation, and model. OtherScientificTerm is conceptual structure of language. ","This paper proposes to use text-only language models (LMs) to model the conceptual structure of language. Specifically, the authors show that LMs are able to capture the concept of entities and objects in the text, and that they can learn to represent concepts in the conceptual domain. The authors also show that GPT-2 and GPT3 models can capture concepts in a similar way as LMs. ","This paper proposes a generative language model (GPTP-2 and GPT-3) for understanding the conceptual structure of text-only language models (LMs). The main idea of the paper is that LMs should be able to capture the rich conceptual structure in the text-based language models. The authors show that GPT2 can be used to represent concepts in the conceptual domain, and that the GPT3 can capture the concept representation in the grounded language model. The paper also provides a theoretical analysis of the relationship between the representation of concepts in LMs and the representation in GPTs. "
5909,SP:702029739062693e3f96051cbb38f20c53f2a223,"Reinforcement learning USED-FOR phenomena. biases PART-OF learning process. biases USED-FOR shaped rewards. shaped rewards USED-FOR emergent phenomena. sender - receiver navigation game USED-FOR shaped rewards. Generic are they, and rewards. OtherScientificTerm are base reward, inductive bias, semantics, and environmental variables of interest. Task is emergent language experimentation. Material is emergent language. Metric is entropy. ","This paper studies the role of shaped rewards in reinforcement learning in the context of language experimentation. The authors show that in a sender-receiver navigation game, shaped rewards can be seen as an inductive bias in the form of an entropy-based reward function. They show that the entropy of the reward function can be used as a measure of the influence of the environment on the agent's behavior. They also show that when the agent has access to a large enough set of environment variables, it is possible to learn a language model that is robust to such shaped rewards.   ","This paper studies emergent language experimentation in the setting of a sender-receiver navigation game, where the goal is to learn a language that is more expressive than the base language. The authors show that in this setting, the learned language can be interpreted as a set of ""shapes"" (i.e., a language in which the sender and receiver can communicate with each other) that are shaped by the environment. They show that these shaped rewards can be used to learn language that can be more expressive, and they show that this can be done in the context of a language experiment. They also show that such a language can also be used as a way of learning a ""shaped"" language.   "
5925,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"deep multilingual language models USED-FOR cross - lingual knowledge. neural modeling USED-FOR deep multilingual language models. unparallel texts USED-FOR cross - lingual knowledge. invariance FEATURE-OF feature representations. invariance FEATURE-OF transfer. prior shift estimation CONJUNCTION correction. correction CONJUNCTION prior shift estimation. representation alignment CONJUNCTION prior shift estimation. prior shift estimation CONJUNCTION representation alignment. unsupervised cross - lingual learning method USED-FOR representation alignment. unsupervised cross - lingual learning method USED-FOR prior shift estimation. importance - weighted domain alignment ( IWDA ) HYPONYM-OF unsupervised cross - lingual learning method. method COMPARE semi - supervised learning techniques. semi - supervised learning techniques COMPARE method. Method are cross - lingually shared representations, and multilingual representations. OtherScientificTerm are distributional shift in class priors, and prior shifts. ",This paper proposes an unsupervised cross-lingual learning method to address the distributional shift in class priors in multilingual language models. The authors propose an importance-weighted domain alignment (IWDA) method to improve representation alignment and prior shift estimation. IWDA is based on the observation that the invariance of feature representations to distributional shifts in the target domain is a key factor in the transfer of knowledge from one language to another.  ,"This paper proposes a new unsupervised cross-lingual learning method for unparallel language models. The authors propose an importance-weighted domain alignment (IWDA) method for representation alignment and prior shift estimation. The proposed IWDA method is based on the notion of invariance of feature representations, which is defined as the invariance between the feature representations of the two languages.  The authors show that the proposed method is able to learn representations that are invariant to the distributional shift in class priors, and prior shifts in the prior priors.  "
5941,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"Meta - learning USED-FOR artificial intelligence. algorithm USED-FOR problem. algorithm USED-FOR meta - learner. metric USED-FOR meta - optimisation. gradients USED-FOR meta - learning. bootstrapping mechanism USED-FOR meta - learning horizon. backpropagation USED-FOR bootstrapping mechanism. it USED-FOR multi - task meta - learning. Atari ALE benchmark EVALUATE-FOR model - free agents. it USED-FOR exploration. ε - greedy Q - learning agent USED-FOR exploration. Task is metaoptimisation problem. Method are metalearner, and bootstrapping. Metric is ( pseudo-)metric. OtherScientificTerm is update rule. ","This paper proposes a meta-learning algorithm for meta-optimization, where the goal is to find the best meta-learner that maximizes the gradient of the update rule of the meta-learned task. The authors propose a new metric to evaluate the meta learner's performance, which is based on the (pseudo-)metric of the gradients of the task gradients. They also propose a bootstrapping mechanism to reduce the meta learning horizon. The proposed algorithm is shown to outperform the baselines on the Atari ALE benchmark.   ","This paper studies the problem of meta-learning in the context of model-free reinforcement learning. The authors propose a new metric for meta-optimization, which is based on the (pseudo-)metric of the meta-training horizon. They also propose a bootstrapping mechanism to bootstrap a meta-learner, which can be used for multi-task meta-learners. The proposed method is evaluated on the Atari ALE benchmark, where it is shown to outperform the state-of-the-art."
5957,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"model - free agents USED-FOR generalization. generalization ability EVALUATE-FOR model - based agents. model - based agents COMPARE model - free counterparts. model - free counterparts COMPARE model - based agents. generalization ability EVALUATE-FOR model - based agents. generalization ability EVALUATE-FOR model - free counterparts. MuZero HYPONYM-OF model - based agent. procedural and task generalization EVALUATE-FOR its. self - supervised representation learning CONJUNCTION procedural data diversity. procedural data diversity CONJUNCTION self - supervised representation learning. planning CONJUNCTION self - supervised representation learning. self - supervised representation learning CONJUNCTION planning. generalization CONJUNCTION data efficiency. data efficiency CONJUNCTION generalization. data efficiency EVALUATE-FOR Procgen. planning HYPONYM-OF procedural generalization. Procgen EVALUATE-FOR techniques. generalization EVALUATE-FOR techniques. data efficiency EVALUATE-FOR techniques. factors USED-FOR task generalization. Meta - World USED-FOR task generalization. single - task, model - free paradigm CONJUNCTION self - supervised model - based agents. self - supervised model - based agents CONJUNCTION single - task, model - free paradigm. rich, procedural, multi - task environments FEATURE-OF self - supervised model - based agents. single - task, model - free paradigm USED-FOR generalizable agents. Method is model - based reinforcement learning. OtherScientificTerm is internal model of the world. Task is transfer. ","This paper studies the generalization ability of self-supervised model-based RL agents in multi-task, procedural and task generalization settings. The authors show that model-free RL agents are able to generalize better than model-trained agents in both the procedural and the task setting. They also show that the performance of model-driven RL agents can be improved by using a combination of two factors: (1) procedural data diversity and (2) learning the internal model of the world.    The authors also propose a meta-world to study the effect of different factors on task generalisation. ","This paper studies the generalization ability of self-supervised model-based agents in the context of multi-task generalization. The authors propose a new model-free paradigm, called MuZero, which is a combination of MuZero and a new method, Procgen, to improve the generalizability of the agent. The main contribution of the paper is that it proposes a meta-world to study the relationship between task generalization and procedural generalization in the Meta-World setting. The Meta-world is composed of three components: (1) Meta-data diversity, (2) procedural data diversity, and (3) planning.  The authors show that MuZero is able to generalize better than MuZero in both procedural and task-specific settings. They also show that Procgen can generalize well in both environments.  "
5973,SP:ba80e35d452d894181d51624183b60541c0f3704,"fixed graph USED-FOR relational inductive biases. graph neural networks HYPONYM-OF Machine learning frameworks. fixed graph USED-FOR Machine learning frameworks. network inverse ( deconvolution ) problem USED-FOR graph learning task. eigendecomposition - based spectral methods CONJUNCTION iterative optimization solutions. iterative optimization solutions CONJUNCTION eigendecomposition - based spectral methods. Graph Deconvolution Network ( GDN ) HYPONYM-OF parameterized neural network architecture. GDNs USED-FOR link prediction or edge - weight regression tasks. GDNs USED-FOR distribution of graphs. loss function USED-FOR GDNs. loss function USED-FOR link prediction or edge - weight regression tasks. layers USED-FOR graph objects. GDNs USED-FOR larger - sized graphs. graph objects COMPARE node features. node features COMPARE graph objects. GDN USED-FOR graph recovery. synthetic data USED-FOR GDN. synthetic data USED-FOR graph recovery. Human Connectome Project - Young Adult neuroimaging dataset EVALUATE-FOR model. inferring structural brain networks USED-FOR model. functional connectivity USED-FOR inferring structural brain networks. functional connectivity USED-FOR model. Material is network data. OtherScientificTerm are graphs, graph convolutional relationship, observed and latent graphs, and structural brain networks. Task is inferring graph structure. Method is proximal gradient iterations. ","This paper proposes a novel graph deconvolution network (GDN) architecture for learning the graph convolutional relationship between observed and latent graphs. The proposed architecture is based on the network inverse (Network Inverse) problem, which aims to learn the distribution of graphs from observed data. The authors propose a novel loss function for link prediction and edge-weight regression tasks, which can be used as a proxy for the true graph structure. Experiments on the Human Connectome Project-Young Adult neuroimaging dataset show that the proposed model is able to infer the functional connectivity of structural brain networks.  ",This paper proposes a graph deconvolution network (GDN) for graph learning. GDN is a parameterized neural network architecture that can be used for link prediction or edge-weight regression tasks. The proposed method is based on eigendecomposition-based spectral methods and iterative optimization solutions. Experiments are conducted on the Human Connectome Project-Young Adult neuroimaging dataset to demonstrate the effectiveness of the proposed method.
5989,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"Reward shaping ( RS ) USED-FOR reinforcement learning ( RL ). manually engineered shaping - reward functions USED-FOR RS. domain knowledge USED-FOR It. Reinforcement Learning Optimising Shaping Algorithm ( ROSA ) HYPONYM-OF automated RS framework. Markov game USED-FOR shaping - reward function. agent ( Controller ) USED-FOR optimal policy. agent ( Controller ) USED-FOR task. optimal policy USED-FOR task. shaped rewards USED-FOR agent ( Controller ). switching controls USED-FOR reward - shaping agent ( Shaper ). shaped rewards USED-FOR optimal policy. ROSA USED-FOR shapingreward function. shapingreward function USED-FOR task. RL algorithms USED-FOR ROSA. RS algorithms USED-FOR sparse reward environments. OtherScientificTerm are sparse or uninformative rewards, shaping rewards, and congenial properties. Task is autonomous learning. ",This paper proposes a reinforcement learning algorithm for reward shaping in reinforcement learning. The main idea is to learn a reward-shaping agent (Shaper) and a controller (Controllers) that jointly learn the optimal policy to maximize the shaping-reward function in a Markov game. The controller is trained by optimizing the reward shaping function and the controller is used to select the best policy for the task. The proposed method is evaluated on a variety of tasks and compared with several baselines. ,"This paper proposes a new reinforcement learning algorithm for reward shaping (reward-shaping) in the context of autonomous reinforcement learning (RL). In contrast to previous work, the authors propose an automated framework that learns the shaping-reward function in a Markov game, where the goal is to find the optimal policy for the task at hand. The agent (controller) is given a task and a set of tasks, and the agent (Shaper) is tasked with selecting the best policy for each task. The goal of the agent is to learn a policy that maximizes the optimal reward function in the environment. The authors show that their algorithm outperforms the state-of-the-art baselines on a variety of tasks. "
6005,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,Vertical Federated Learning ( VFL ) HYPONYM-OF distributed learning paradigm. backdoor attacks FEATURE-OF VFL. robustness EVALUATE-FOR VFL. horizontal federated learning COMPARE VFL. VFL COMPARE horizontal federated learning. RVFR HYPONYM-OF VFL training and inference framework. RVFR USED-FOR uncorrupted features. RVFR USED-FOR model. RVFR USED-FOR inferencephase adversarial and missing feature attacks. RVFR COMPARE baselines. baselines COMPARE RVFR. robustness EVALUATE-FOR baselines. robustness EVALUATE-FOR RVFR. Method is global model. OtherScientificTerm is features. Task is inference - phase attacks. Material is NUS - WIDE and CIFAR-10 datasets. ,"This paper studies the problem of robustness against backdoor attacks in vertical federated learning (VFL). The authors propose a new training and inference framework, called RVFR, to improve the robustness of VFL models against adversarial and missing feature attacks. The proposed method is based on the observation that the uncorrupted features are more vulnerable to adversarial attacks, and the proposed RVFR is able to recover the original training data in the inference phase. Experiments are conducted on NUS-WIDE and CIFAR-10 datasets to demonstrate the effectiveness of the proposed method.","This paper proposes a new method to improve the robustness of vertical federated learning (VFL) against adversarial and missing feature attacks. The proposed method, called RVFR, is based on the notion of uncorrupted features, which can be used to train a global model and recover uncorrupted features from a local model. The authors show that the proposed method is more robust to adversarial attacks and missing features attacks than previous methods. They also show that RVFR can recover the features that are missing from the global model. "
6021,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,question answering CONJUNCTION fact checking. fact checking CONJUNCTION question answering. Information retrieval USED-FOR knowledge intensive tasks. Information retrieval HYPONYM-OF natural language processing. natural language processing USED-FOR knowledge intensive tasks. fact checking HYPONYM-OF knowledge intensive tasks. question answering HYPONYM-OF knowledge intensive tasks. dense retrievers COMPARE sparse methods. sparse methods COMPARE dense retrievers. dense retrievers PART-OF information retrieval. neural networks USED-FOR dense retrievers. term - frequency USED-FOR sparse methods. datasets EVALUATE-FOR models. BM25 HYPONYM-OF term - frequency methods. contrastive learning USED-FOR unsupervised dense retrievers. it USED-FOR retrieval. model COMPARE BM25. BM25 COMPARE model. BEIR benchmark EVALUATE-FOR model. model COMPARE BM25. BM25 COMPARE model. MS MARCO dataset EVALUATE-FOR technique. pre - training USED-FOR technique. pre - training CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pre - training. fine - tuning EVALUATE-FOR technique. MS MARCO dataset EVALUATE-FOR pre - training. MS MARCO dataset EVALUATE-FOR fine - tuning. BEIR benchmark EVALUATE-FOR technique. Generic is they. Material is new domains. OtherScientificTerm is supervision. ,"This paper proposes a method to improve the performance of dense retrieval methods in knowledge-theoretic question answering tasks. The method is based on contrastive learning, where the objective is to maximize the mutual information between the query and the answer. The authors show that the proposed method is able to achieve better performance than the state-of-the-art sparse retrieval methods on the BEIR and MS MARCO datasets.","This paper proposes a novel method for unsupervised information retrieval. The idea is to use contrastive learning to improve the performance of dense retrievers in a sparse retrieval setting. The method is based on the notion of term-frequencies, which is used in the literature for sparse retrieval methods. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on the BEIR benchmark. "
6037,SP:ed4e2896dc882bd089f420f719da232d706097c5,fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. fine - tuning HYPONYM-OF methods. linear probing HYPONYM-OF methods. accuracy in - distribution ( ID ) EVALUATE-FOR fine - tuning. accuracy EVALUATE-FOR fine - tuning. fine - tuning COMPARE linear probing. linear probing COMPARE fine - tuning. DomainNet CONJUNCTION CIFAR. CIFAR CONJUNCTION DomainNet. Breeds - Entity30 CONJUNCTION DomainNet. DomainNet CONJUNCTION Breeds - Entity30. distribution shift datasets EVALUATE-FOR fine - tuning. Breeds - Living17 CONJUNCTION Breeds - Entity30. Breeds - Entity30 CONJUNCTION Breeds - Living17. CIFAR10.1 CONJUNCTION FMoW. FMoW CONJUNCTION CIFAR10.1. accuracy OOD EVALUATE-FOR linear probing. CIFAR CONJUNCTION CIFAR10.1. CIFAR10.1 CONJUNCTION CIFAR. accuracy ID EVALUATE-FOR linear probing. CIFAR HYPONYM-OF distribution shift datasets. FMoW HYPONYM-OF distribution shift datasets. Breeds - Living17 HYPONYM-OF distribution shift datasets. CIFAR10.1 HYPONYM-OF distribution shift datasets. DomainNet HYPONYM-OF distribution shift datasets. Breeds - Entity30 HYPONYM-OF distribution shift datasets. accuracy OOD EVALUATE-FOR fine - tuning. accuracy ID EVALUATE-FOR fine - tuning. fine - tuning USED-FOR pretrained features. fine - tuning CONJUNCTION linear probing. linear probing CONJUNCTION fine - tuning. linear probing COMPARE fine - tuning. fine - tuning COMPARE linear probing. ID and OOD accuracy EVALUATE-FOR fine - tuning. linear probing CONJUNCTION fine - tuning. fine - tuning CONJUNCTION linear probing. fine - tuning COMPARE fine - tuning. fine - tuning COMPARE fine - tuning. ID CONJUNCTION OOD. OOD CONJUNCTION ID. datasets EVALUATE-FOR fine - tuning. Method is pretrained model,"This paper studies the impact of fine-tuning and linear probing on the accuracy in-distribution (ID) and out-of-domain (OOD) performance on distribution shift datasets. The authors show that fine tuning improves ID and OOD accuracy on CIFAR-10, DomainNet, Breeds-Entities-30, and FMoW datasets. They also show that linear probing improves accuracy ID on the same datasets.  ","This paper proposes a method for fine-tuning the accuracy in-distribution (ID) of a model trained on a distribution shift dataset. The method is based on linear probing, which is a well-known method for ID and OOD accuracy. The main idea is to fine-tune the accuracy ID of the model using the training data, and then fine-fine-fine tune the accuracy OOD of the data. The authors show that this method is more accurate than linear probing. They also show that it is also more accurate in terms of OOD and ID accuracy."
6053,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"meta - learning COMPARE L2DNC. L2DNC COMPARE meta - learning. meta - learning USED-FOR L2DNC. meta - learning algorithms USED-FOR L2DNC problem. meta - learning - based methodology USED-FOR unlabeled data. meta - learning - based methodology USED-FOR it. L2DNC COMPARE labeling in causality. labeling in causality COMPARE L2DNC. unseen - class data USED-FOR seen - class data. Material are labeled data, and limited data. Method is clustering models. OtherScientificTerm is high - level semantic features. ","This paper studies the problem of unlabeled data with limited labeled data. The authors propose a meta-learning-based methodology to solve the L2DNC problem, where the goal is to learn a clustering model that can be used to classify the data into seen-class and unseen-class categories. The proposed method is based on meta-learned clustering models. The experiments show that the proposed method outperforms the baselines in terms of classification accuracy.   ",This paper proposes a meta-learning-based method for the problem of unlabeled data with limited data. The authors show that the L2DNC problem can be solved with meta-learned data. They show that their method can be used to solve the problem in the same way as the labeling in causality problem. They also show that it can be applied to the case where the data is not labeled.
6069,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"POMDPs USED-FOR model - based RL agents. online and offline experiences USED-FOR causal transition model. offline experiences USED-FOR agent. reinforcement learning CONJUNCTION causality. causality CONJUNCTION reinforcement learning. causal inference problem USED-FOR model - based reinforcement learning. offline data USED-FOR learning. methodology USED-FOR offline data. latent - based causal transition model USED-FOR interventional and observational regimes. latent - based causal transition model USED-FOR method. latent variable USED-FOR deconfounding. deconfounding USED-FOR POMDP transition model. latent variable USED-FOR POMDP transition model. generalization guarantees EVALUATE-FOR it. synthetic toy problems EVALUATE-FOR it. generalization guarantees EVALUATE-FOR method. synthetic toy problems EVALUATE-FOR method. OtherScientificTerm are online experiences, privileged information, and learning agent. Material are interventional data, and observational data. Method is causal framework of do - calculus. ","This paper proposes a causal inference method for model-based reinforcement learning with POMDPs. The proposed method is based on the causal framework of do-calculus, which is used to model the causal transition model between online and offline experiences. The main contribution of the paper is the use of a latent variable to learn the transition model from offline data. The method is evaluated on a variety of toy problems and achieves state-of-the-art performance. ","This paper proposes a causal inference method for model-based reinforcement learning with POMDPs. The main idea is to learn a causal transition model between online and offline experiences, where the online experience is a set of experiences that the agent can learn from, and the offline experience is the set of offline experiences that can be learned by the agent. The method is based on a causal framework of do-calculus, and it is shown to be able to generalize well to both interventional data and observational data. "
6085,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"textual knowledge corpus USED-FOR retriever. Wikipedia HYPONYM-OF textual knowledge corpus. retriever USED-FOR text generation systems. methods USED-FOR generator. retriever CONJUNCTION generator. generator CONJUNCTION retriever. methods USED-FOR retriever. generating informative utterances in conversations HYPONYM-OF open - ended generation tasks. retriever CONJUNCTION generator. generator CONJUNCTION retriever. informative conversations EVALUATE-FOR retriever. generator USED-FOR it. Wizard of Wikipedia dataset FEATURE-OF informative conversations. retriever USED-FOR it. posterior - guided training USED-FOR informative conversations. evidence lower bound ( ELBo ) USED-FOR it. posterior distribution Q USED-FOR guide retriever. OtherScientificTerm are top-10, and generator ’s responses. Method is end - to - end system. ","This paper presents an end-to-end text generation model for Wizard of Wikipedia dataset. The model is trained with two components: a retriever and a generator. The retriever is trained to predict the top-10 most informative utterances in the Wizard dataset, and the generator is used to generate the responses of the top 10 utterances. The generator is trained using a posterior-guided learning approach, where the retriever learns to predict responses from the posterior distribution of the generator's responses. The authors show that the proposed model outperforms the baselines in terms of the ELBO.   ","This paper proposes an end-to-end text generation system for open-ended text generation tasks. The paper presents a method for generating informative utterances in conversations from the Wizard of Wikipedia dataset. The approach is based on posterior-guided training, where the retriever is trained to generate the most informative responses from the top-10 most popular utterances, and the generator is trained on the top 10 most popular responses. Experiments on the Wizard dataset demonstrate the effectiveness of the approach. "
6101,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"machine learning USED-FOR combinatorial optimization. real - world problems USED-FOR large graphs. influence maximization HYPONYM-OF NP - hard problem. influence estimation HYPONYM-OF # P - hard counting problem. influence estimation CONJUNCTION influence maximization. influence maximization CONJUNCTION influence estimation. influence estimation HYPONYM-OF problems. influence maximization HYPONYM-OF problems. Graph Neural Network ( GNN ) USED-FOR upper bound of influence estimation. GLIE HYPONYM-OF Graph Neural Network ( GNN ). small simulated graphs USED-FOR it. GLIE USED-FOR influence estimation. real graphs USED-FOR influence estimation. it USED-FOR influence maximization. GLIE CONJUNCTION simulated influence estimation. simulated influence estimation CONJUNCTION GLIE. simulated influence estimation USED-FOR Lazy Forward optimization. GLIE USED-FOR Lazy Forward optimization. time complexity CONJUNCTION quality of influence. quality of influence CONJUNCTION time complexity. first HYPONYM-OF Q - network. GLIE ’s predictions USED-FOR Q - network. second USED-FOR provably submodular function. GLIE ’s representations USED-FOR provably submodular function. time efficiency CONJUNCTION influence spread. influence spread CONJUNCTION time efficiency. latter COMPARE SOTA benchmarks. SOTA benchmarks COMPARE latter. influence spread EVALUATE-FOR latter. time efficiency EVALUATE-FOR latter. Generic are perspective, and approaches. Task is small graph problems. OtherScientificTerm are predictions ranking, and computational overhead. Metric is accuracy. ",This paper studies the influence estimation problem in graph combinatorial optimization. The authors propose to use a graph neural network (GNN) to estimate the influence of each node in a graph. They show that the GNN can be used for influence estimation and influence maximization. They also show that their method is computationally efficient.   ,"This paper presents a novel approach to the problem of influence estimation in combinatorial optimization. The authors propose GLIE, a graph neural network (GNN) that can be used to estimate the influence of a graph. They show that GLIE outperforms the state-of-the-art in terms of time efficiency and influence spread. They also show that the GLIE can be applied to the Lazy Forward optimization problem."
6117,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"active learning strategies USED-FOR domain adaptation. localized class of functions USED-FOR labeling. Rademacher average CONJUNCTION localized discrepancy. localized discrepancy CONJUNCTION Rademacher average. localized discrepancy FEATURE-OF loss functions. generalization error bounds USED-FOR active learning strategies. regularity condition FEATURE-OF loss functions. Kmedoids algorithm USED-FOR large data set. theoretical bounds USED-FOR Kmedoids algorithm. algorithm COMPARE active learning techniques. active learning techniques COMPARE algorithm. active learning techniques USED-FOR domain adaptation. algorithm USED-FOR domain adaptation. large data sets EVALUATE-FOR algorithm. OtherScientificTerm are assumption of Lipschitz functions, and discrepancy distance. ","This paper studies the problem of active learning for domain adaptation, where the goal is to learn a localized class of functions that can be used for labeling in the target domain. The authors propose a generalization error bound for active learning strategies based on the Rademacher average and localized discrepancy. They show that under a regularity condition on the loss functions, the Kmedoids algorithm can be efficiently adapted to large data sets. They also provide theoretical results on the generalization performance of the proposed method. ",This paper studies the generalization error bounds for active learning strategies for domain adaptation. The main contribution of the paper is to provide theoretical bounds for the Kmedoids algorithm under the regularity condition of the Lipschitz loss function. The generalization bounds are based on the Rademacher average and the discrepancy distance between the two sets of loss functions. The theoretical bounds are proved for large data sets. The empirical results show that the proposed algorithm outperforms the state-of-the-art in terms of generalization performance.
6133,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,variational approximation USED-FOR Bayesian neural networks. singular statistical models USED-FOR neural networks. posterior distribution FEATURE-OF singular model. desingularization map HYPONYM-OF algebraic - geometrical transformation. generalized gamma mean - field variational family USED-FOR leading order term. leading order term FEATURE-OF model evidence. generalized gamma mean - field variational family USED-FOR model evidence. desingularization USED-FOR generalized gamma mean - field variational family. Affine coupling layers USED-FOR unknown desingularization map. source distribution USED-FOR normalizing flow. normalizing flow USED-FOR methodology. generalized gamma USED-FOR normalizing flow. Generic is approximation. Method is singular learning theory. OtherScientificTerm is mixture of standard forms. ,"This paper proposes to use a generalized gamma mean-field variational family for Bayesian neural networks with singular statistical models. The proposed method is based on the desingularization map, which is an algebraic-geometrical transformation of the posterior distribution of a singular model. The authors show that the proposed method can be used to approximate the distribution of the singular model in terms of the leading order term of the model evidence. The main contribution of the paper is the introduction of a new normalizing flow for the singular learning theory. ",This paper proposes a generalization of the generalized gamma mean-field variational family for Bayesian neural networks. The main idea is to use the desingularization map of the source distribution as the leading-order term of the model evidence. The authors show that this is an algebraic-geometrical transformation that can be used to approximate the posterior distribution of the singular model. They also show that the leading order term can be represented as a mixture of standard forms. 
6149,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"model USED-FOR domain generalization ( DG ) problem setting. known data distributions USED-FOR model. methods COMPARE empirical risk minimisation baseline. empirical risk minimisation baseline COMPARE methods. learning - theoretic generalisation bound USED-FOR DG. learning - theoretic generalisation bound USED-FOR domain generalisation. Rademacher complexity FEATURE-OF model. empirical risk - predictor complexity trade - off EVALUATE-FOR methods. regularised ERM USED-FOR domain generalisation. Task are general purpose DG, and DG problem. Material is DomainBed benchmark. ","This paper studies the problem of domain generalization (DG), where the goal is to learn a model that generalizes well across different domains. The authors propose to use a regularized ERM-based approach to learn the model, and show that this approach can improve the performance of the model in terms of generalization. They also provide a learning-theoretic generalization bound for the domain generalisation problem.  ","This paper studies the problem of domain generalization (DG) in the domain-specific setting, where the target domain is a set of known data distributions and the goal is to improve the generalization performance of a model trained on the known data distribution. The authors propose a new generalization bound for the problem, which is based on the learning-theoretic generalisation bound for domain generalisation. The generalization bounds are derived by considering the Rademacher complexity of the model, and the empirical risk-prediction complexity trade-off between the risk minimization and the risk-probability trade-offs. The empirical risk minimisation performance of the proposed method is evaluated on the DomainBed benchmark. "
6165,SP:b1f622cbc827e880f98de9e99eca498584efe011,overlays USED-FOR maximum n - times coverage problem. multi - set multi - cover problem USED-FOR Maximum n - times coverage. integer linear programming CONJUNCTION sequential greedy optimization. sequential greedy optimization CONJUNCTION integer linear programming. solutions USED-FOR n - times coverage. integer linear programming USED-FOR solutions. sequential greedy optimization USED-FOR solutions. it USED-FOR pan - strain COVID-19 vaccine design. pan - strain COVID-19 vaccine design COMPARE designs. designs COMPARE pan - strain COVID-19 vaccine design. maximum n - times coverage USED-FOR peptide vaccine design. predicted population coverage EVALUATE-FOR pan - strain COVID-19 vaccine design. predicted population coverage EVALUATE-FOR designs. Task is min - cost n - times coverage problem. OtherScientificTerm is HLA molecules. ,"This paper studies the maximum n-times coverage problem, which is a multi-set multi-cover problem, where each set is a set of HLA molecules and the goal is to maximize the coverage of the population. The authors propose to solve the problem using integer linear programming and sequential greedy optimization. The proposed method is shown to be able to achieve the optimal coverage in the pan-strain COVID-19 vaccine design.   ","This paper studies the problem of maximum n-times coverage for a pan-strain COVID-19 vaccine design. The authors consider a multi-set multi-cover problem, where each cover is a set of HLA molecules, and the goal is to maximize the coverage of the population. The main contribution of the paper is to propose a solution to the problem, which can be applied to the pan-strain COVID vaccine design, which is based on a linear programming-based approach. The proposed method is shown to improve the predicted population coverage by a factor of n times. "
6181,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"supervised learning algorithms USED-FOR SNNs. power consumption CONJUNCTION computational capability. computational capability CONJUNCTION power consumption. information encoding CONJUNCTION power consumption. power consumption CONJUNCTION information encoding. information encoding EVALUATE-FOR spiking neural networks ( SNNs ). weight initialization USED-FOR SNN training. It USED-FOR gradient generation. asymptotic formula USED-FOR response curve. asymptotic formula USED-FOR spiking neurons. initialization method USED-FOR gradient vanishing. slant asymptote USED-FOR initialization method. coding schemes USED-FOR classification tasks. method COMPARE deep learning initialization methods. deep learning initialization methods COMPARE method. method COMPARE SNN initialization methods. SNN initialization methods COMPARE method. deep learning initialization methods CONJUNCTION SNN initialization methods. SNN initialization methods CONJUNCTION deep learning initialization methods. model accuracy EVALUATE-FOR deep learning initialization methods. model accuracy EVALUATE-FOR SNN initialization methods. coding schemes EVALUATE-FOR method. training speed CONJUNCTION model accuracy. model accuracy CONJUNCTION training speed. MNIST and CIFAR10 dataset EVALUATE-FOR coding schemes. MNIST and CIFAR10 dataset USED-FOR classification tasks. training speed EVALUATE-FOR method. model accuracy EVALUATE-FOR method. Method is backpropagation. OtherScientificTerm are neuron response distribution, and training hyperparameters. Generic is methods. ","This paper proposes a novel initialization method for weight initialization in spiking neural networks (SNNs). The method is based on the asymptotic formula for the response curve of spiking neurons, which can be used to estimate the gradient vanishing. The authors show that the initialization method is able to estimate gradient vanishing in the SNN response curve, which is then used to compute the weight initialization for SNN training.   The authors also show that their method can improve the training speed and accuracy of SNNs.",This paper proposes a new method for weight initialization for spiking neural networks (SNNs). The main idea is to use a slant asymptotic formula to estimate the response curve of the spiking neurons in the training process. The authors show that their method is more efficient than other SNN initialization methods in terms of training speed and accuracy. The method is evaluated on MNIST and CIFAR-10 datasets and shows that it can improve the performance of SNNs.
6197,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"Federated learning USED-FOR machine learning models. mobile devices FEATURE-OF decentralized client data. decentralized client data USED-FOR machine learning models. model USED-FOR federated learning systems. mixture of distributions USED-FOR it. block - cyclic pattern USED-FOR distribution shift. light - weight branches USED-FOR network. image classification CONJUNCTION next word prediction. next word prediction CONJUNCTION image classification. algorithm USED-FOR distribution shift. image classification EVALUATE-FOR algorithm. model EVALUATE-FOR algorithm. Stack Overflow dataset USED-FOR next word prediction. EMNIST and CIFAR datasets USED-FOR image classification. OtherScientificTerm are periodically shifting distributions, and daytime and nighttime modes. Method are Federated Expectation - Maximization algorithm, and mixture model. ","This paper proposes a federated expectation-maximization (FEM) algorithm to deal with distribution shift in federated learning. The authors propose to use a mixture of distributions to model the distribution shift, and use a block-cyclic pattern to mitigate distribution shift. The proposed method is evaluated on image classification and next word prediction tasks. ","This paper proposes a new method for federated learning in which a mixture of distributions is used to model the distribution shift in the client data. The proposed method is based on the Federated Expectation-Maximization (FEM) algorithm. The main idea of the method is to use a block-cyclic pattern to model distribution shift, and then use light-weight branches to train the network. The method is evaluated on the EMNIST, CIFAR, and Stack Overflow datasets. "
6213,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"accuracy EVALUATE-FOR winning ticket ” subnetwork. pruning USED-FOR DN ’s decision boundary. pruning DN nodes USED-FOR decision boundary. spline interpretation of DNs USED-FOR theory and visualization tools. DN spline theory CONJUNCTION lottery ticket hypothesis of DNs. lottery ticket hypothesis of DNs CONJUNCTION DN spline theory. early - bird ( EB ) phenomenon FEATURE-OF DN ’s spline mappings. pruning strategy USED-FOR DN nodes. spline partition regions FEATURE-OF DN nodes. spline - based DN pruning approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE spline - based DN pruning approach. accuracy EVALUATE-FOR state - of - the - art methods. networks EVALUATE-FOR spline - based DN pruning approach. spline - based DN pruning approach USED-FOR training FLOPs. accuracy EVALUATE-FOR spline - based DN pruning approach. Method are deep network ( DN ) training, and pruning technique. Generic is model. OtherScientificTerm is spline ’s partition. ","This paper proposes a new pruning strategy for deep neural networks (DNs) based on the spline interpretation of DNs. The authors show that pruning DN nodes is a good way to find the ""winning ticket"" subnetwork, which is the one with the highest accuracy. They also show that the pruned DN nodes can be seen as early-birds, which can be explained by the lottery ticket hypothesis. The paper also shows that the early-bird phenomenon is a result of spline mappings between DNs and the decision boundary.",This paper proposes a pruning strategy for deep neural networks (DNs) based on the spline interpretation of DNs. The authors propose to prune DN nodes based on their spline partition regions. They show that pruning DN nodes with spline-based pruning can improve accuracy and reduce training FLOPs. They also propose a lottery ticket hypothesis for DNs based on early bird phenomenon.
6229,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"data representation USED-FOR optimal predictors. inner - level USED-FOR optimal group predictors. bi - level optimization USED-FOR problem. inner optimization CONJUNCTION implicit differentiation. implicit differentiation CONJUNCTION inner optimization. implicit differentiation CONJUNCTION optimization path. optimization path CONJUNCTION implicit differentiation. implicit differentiation USED-FOR implicit path alignment algorithm. inner optimization USED-FOR implicit path alignment algorithm. sufficiency rule FEATURE-OF bi - level objective. error gap EVALUATE-FOR implicit approach. classification and regression settings EVALUATE-FOR method. Task are fair representation learning perspective, and fairness measurement. Generic is representation. Method are inner - level optimization, and fair representation learning. ",This paper studies the problem of fair representation learning from the perspective of fairness measurement. The authors propose to use a bi-level optimization approach to learn the optimal group predictors from the data representation. The main idea is to use inner optimization and implicit differentiation to find the optimal groups predictors in the representation space. The proposed method is evaluated on classification and regression tasks.   ,"This paper proposes an implicit representation learning approach for the problem of fair representation learning, where the goal is to find the optimal group predictors for a set of data samples. The authors propose a bi-level objective for the inner-level optimization problem, which is based on the sufficiency rule. The main idea is to use implicit differentiation as the inner optimization objective, and then use the implicit path alignment algorithm to align the data representations of the optimal predictors. The implicit approach is shown to outperform the state-of-the-art in both classification and regression settings. "
6245,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"value - based methods USED-FOR Offline reinforcement learning ( RL ). temporal difference ( TD ) learning USED-FOR value - based methods. supervised learning methods USED-FOR optimal policies. methods COMPARE value - based approximate dynamic programming algorithms. value - based approximate dynamic programming algorithms COMPARE methods. design decisions PART-OF methods. policy architectures HYPONYM-OF design decisions. large sequence models CONJUNCTION value - based weighting schemes. value - based weighting schemes CONJUNCTION large sequence models. RvS methods COMPARE prior methods. prior methods COMPARE RvS methods. optimal data FEATURE-OF datasets. datasets HYPONYM-OF offline RL benchmarks. offline RL benchmarks EVALUATE-FOR prior methods. offline RL benchmarks EVALUATE-FOR RvS methods. Task is offline RL problem. Generic is task. Material is suboptimal data. Method is reinforcement learning via supervised learning ( RvS ). OtherScientificTerm are conditioning variable, and model capacity. ",This paper proposes a reinforcement learning method for offline reinforcement learning in the presence of suboptimal data. The proposed method is based on reinforcement learning via supervised learning (RvS) and uses temporal difference (TD) learning to find the optimal policies. The authors show that the proposed method outperforms existing value-based approximate dynamic programming algorithms on a variety of offline RL benchmarks.,This paper proposes a new method for offline reinforcement learning via supervised learning (RvS) in the context of temporal difference (TD) learning. The main idea is to use TD as a way to learn the optimal policies from suboptimal data. The authors show that the proposed method outperforms the state-of-the-art value-based approximate dynamic programming (VAE) algorithms on a variety of offline RL benchmarks. 
6261,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,computational cognitive mechanisms USED-FOR exploration algorithms. spatial information USED-FOR structure of unobserved spaces. Hierarchical Bayesian framework USED-FOR program induction. program induction USED-FOR compositional formation of proposed maps of complex spaces. Map Induction USED-FOR cognitive process. distribution of strong spatial priors USED-FOR model. computational framework USED-FOR human exploration behavior. computational framework COMPARE non - inductive models. non - inductive models COMPARE computational framework. Map Induction USED-FOR approximate planning algorithms. Task is behavioral Map Induction Task. Method is Map Induction framework. ,"This paper proposes a Bayesian Bayesian program induction framework for learning to map unobserved spaces. The proposed method is based on a hierarchical Bayesian framework, which is able to model the compositional formation of proposed maps of complex spaces. A distribution of strong spatial priors is used to model a model of exploration behavior. The authors show that the proposed method can be used to learn approximate planning algorithms for approximate planning and exploration.","This paper proposes a Hierarchical Bayesian Bayesian program induction (HBI) framework for map induction. The proposed method is based on a hierarchical Bayesian framework for program induction. It is motivated by the notion of spatial information, which is used to model the spatial structure of unobserved spaces. The authors show that the proposed method can be applied to a variety of tasks, such as exploration, planning, and exploration in the environment."
6277,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"differentiable approach USED-FOR probabilistic factors. probabilistic factors USED-FOR inference. nonparametric belief propagation algorithm USED-FOR differentiable approach. nonparametric belief propagation algorithm USED-FOR inference. probabilistic factors PART-OF graphical model. domain - specific features FEATURE-OF probabilistic factors. domain - specific features USED-FOR nonparametric belief propagation methods. differentiable neural network USED-FOR factors. labeled data USED-FOR optimization routine. differentiable neural network USED-FOR crafted factor. optimization routine USED-FOR factors. differentiable neural networks CONJUNCTION belief propagation algorithm. belief propagation algorithm CONJUNCTION differentiable neural networks. method USED-FOR marginal posterior samples. differentiable neural networks USED-FOR method. end - to - end training USED-FOR marginal posterior samples. end - to - end training USED-FOR method. differentiable nonparametric belief propagation ( DNBP ) method COMPARE learned baselines. learned baselines COMPARE differentiable nonparametric belief propagation ( DNBP ) method. articulated pose tracking tasks EVALUATE-FOR differentiable nonparametric belief propagation ( DNBP ) method. learned factors USED-FOR tracking. Method are hand - crafted approaches, and Gradient Descent. OtherScientificTerm is Feature Extractor. ","This paper proposes a differentiable nonparametric belief propagation (DNBP) method to learn probabilistic factors in a graphical model. The proposed method is based on the idea of feature extractor, which extracts domain-specific features from labeled data and uses them to construct a set of factors that are then used for training the model.   The main contribution of this paper is the use of differentiable neural networks to learn the probabilistically crafted factors.  The authors show that the proposed method achieves state-of-the-art performance on articulated pose tracking tasks. ","This paper proposes a differentiable nonparametric belief propagation (DNBP) method for the problem of identifying the best probabilistic factors in a graphical model. The key idea is to use differentiable neural networks to learn the factors from a set of labeled data, and then use them to train a neural network to estimate the factors. The authors show that their method outperforms the state-of-the-art in terms of accuracy on articulated pose tracking tasks. "
6293,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"deep learning - based modeling of molecules USED-FOR in silico drug discovery. scaffold USED-FOR drug discovery projects. MoLeR HYPONYM-OF graph - based model. MoLeR COMPARE approaches. approaches COMPARE MoLeR. MoLeR COMPARE methods. methods COMPARE MoLeR. MoLeR COMPARE them. them COMPARE MoLeR. them COMPARE approaches. approaches COMPARE them. scaffoldbased tasks EVALUATE-FOR MoLeR. scaffoldbased tasks EVALUATE-FOR them. unconstrained molecular optimization tasks EVALUATE-FOR methods. unconstrained molecular optimization tasks EVALUATE-FOR MoLeR. Method are generative models, and generative procedure. OtherScientificTerm are fragmentby - fragment, scaffolds, and generation history. Generic is constraint. ","This paper proposes a graph-based model for generating scaffolds for in-silico drug discovery. The proposed method is based on the idea that scaffolds are generated fragment-by-fragment, where each fragment is represented as a graph, and each graph is represented by a set of vertices. The goal is to learn a graph representation of the generated scaffolds, which can then be used as a scaffold for the next generation. The method is evaluated on a variety of synthetic and unconstrained molecular optimization tasks.   ","This paper proposes a graph-based generative model for in-silico drug discovery. The proposed method is based on the idea of fragment-by-fragment generative models, where each fragment is represented as a sequence of fragments, and the goal is to generate a scaffold of molecules that can be used for the next generation of a drug. The method is evaluated on a variety of unconstrained molecular optimization tasks, where it outperforms the state-of-the-art."
6309,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"Imitation learning algorithms USED-FOR policy. deterministic experts USED-FOR imitation learning. stationary reward FEATURE-OF reinforcement learning. reduction USED-FOR continuous control tasks. Task is recovery of expert reward. OtherScientificTerm is total variation distance. Method are imitation learner, and adversarial imitation learning. ","This paper studies the problem of imitation learning in the presence of deterministic experts. In particular, the authors show that the total variation distance between the policy and the expert can be reduced to a lower bound on the total variations between the expert and the policy. They then show that this lower bound can be used to reduce the imitation learning problem to an adversarial imitation learning one, where the policy is trained to imitate the expert. Finally, they show that in this setting, the imitation learner can recover the expert reward. ",This paper studies the problem of imitation learning with deterministic experts in the context of reinforcement learning. The authors show that the total variation distance between the expert and the imitation learner can be reduced to zero if the expert is deterministic and the learner is not deterministic. They show that this reduction can be achieved for continuous control tasks. They also show that it can be used to improve the performance of the expert learner. 
6325,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,reweighting algorithms USED-FOR machine learning models. reweighting algorithms USED-FOR fairness. machine learning models USED-FOR fairness. overparameterized setting FEATURE-OF algorithms. reweighting USED-FOR overparameterized model. worst - group test performance COMPARE ERM. ERM COMPARE worst - group test performance. overparameterized model USED-FOR ERM interpolator. reweighting algorithms USED-FOR interpolator. reweighting algorithms COMPARE ERM. ERM COMPARE reweighting algorithms. interpolator COMPARE ERM. ERM COMPARE interpolator. worst - group performance EVALUATE-FOR ERM. regularization CONJUNCTION data augmentation. data augmentation CONJUNCTION regularization. regularization USED-FOR reweighting algorithms. data augmentation USED-FOR reweighting algorithms. worst - group test performance EVALUATE-FOR reweighting algorithms. OtherScientificTerm is model parameters. Method is theoretical backing. Generic is model. ,This paper studies the effect of reweighting algorithms in the over-parameterized setting. The authors show that ERM can be used to improve the worst-case performance of a model trained with overparametrized parameters. The main contribution of the paper is a theoretical analysis of the performance of ERM in this setting. They show that the reweighted model is more robust to data augmentation.,"This paper studies the problem of over-parameterized reweighting in the worst-group fairness setting. The authors propose a new algorithm ERM, which is based on the idea that the overparameterization of a model can be reduced by regularizing the model parameters. They show that ERM can be used to improve the performance of reweighted models in the over-predictive setting. They also provide theoretical backing for their results. "
6341,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"Multi - agent reinforcement learning ( MARL ) USED-FOR emergent behavior. emergent behavior PART-OF complex agent - based simulations. model USED-FOR human - irrationality. Rational Inattention ( RI ) model HYPONYM-OF model. model PART-OF human - like RL agents. RI USED-FOR cost of cognitive information processing. mutual information USED-FOR RI. mutual information USED-FOR cost of cognitive information processing. multi - timestep dynamics CONJUNCTION information channels. information channels CONJUNCTION multi - timestep dynamics. heterogeneous processing costs FEATURE-OF information channels. RI USED-FOR information asymmetry. RIRL USED-FOR equilibrium behaviors. RIRL USED-FOR AI agents. AI agents USED-FOR real human behavior. Method are RL agents, and RIRL framework. OtherScientificTerm are human behavior, rational assumptions, Principal ’s inattention, inattention, and rationality assumptions. Metric is Agent welfare. ","This paper proposes a method to model human-irrationality in multi-agent reinforcement learning (MARL) using a Principal’s inattention (PI) model. PI models the cost of cognitive information processing as a function of the mutual information between the agents' timesteps and information channels. The authors show that the PI model can capture the information asymmetry between agents' information channels, which is caused by the heterogeneous processing costs of different information channels in different timesteps.   The authors then propose to use this model to model the inattentive behavior of RL agents.  The proposed method is evaluated on a variety of simulated and real-world environments, and compared to a number of baselines. ","This paper proposes a new model to model human-irrationality in multi-agent reinforcement learning (MARL). The proposed model is based on the Principal’s inattention (PI) model, which is an extension of the Rational Inattention model (RIRL) framework. The main contribution of the paper is that the proposed model can be used to model the cost of cognitive information processing in MARL. The model is built on top of the RIRL framework, and it is shown to be able to capture human-rationality.  "
6357,SP:100c91da177504d89f1819f4fdce72ebcf848902,perturbations USED-FOR Audio adversarial attacks. lp - norm CONJUNCTION auditory masking. auditory masking CONJUNCTION lp - norm. auditory masking USED-FOR magnitude spectrogram. lp - norm CONJUNCTION waveform. waveform CONJUNCTION lp - norm. perturbations USED-FOR audio adversarial attacks. phaseoriented algorithm USED-FOR imperceptible audio adversarial examples. energy dissipation FEATURE-OF imperceptible audio adversarial examples. PhaseFool HYPONYM-OF phaseoriented algorithm. energy patterns PART-OF spectrogram. spectrogram consistency USED-FOR short - time Fourier transform ( STFT ). weighted loss function USED-FOR PhaseFool. imperceptibility EVALUATE-FOR PhaseFool. weighted loss function USED-FOR imperceptibility. PhaseFool COMPARE imperceptible counterparts. imperceptible counterparts COMPARE PhaseFool. PhaseFool USED-FOR full - sentence imperceptible audio adversarial examples. realistic simulated environmental distortions USED-FOR adversarial examples. PhaseFool USED-FOR adversarial examples. phase - oriented energy dissipation FEATURE-OF audio adversarial example. PhaseFool COMPARE perturbations. perturbations COMPARE PhaseFool. perturbations FEATURE-OF audio waveform. phase - oriented energy dissipation COMPARE perturbations. perturbations COMPARE phase - oriented energy dissipation. phase - oriented energy dissipation USED-FOR PhaseFool. Method is automatic speech recognition ( ASR ) model. OtherScientificTerm is phase spectrogram. ,This paper proposes a phase-oriented adversarial attack method for audio adversarial attacks. The proposed method is based on the observation that the magnitude spectrogram of the audio is composed of a set of phase patterns that are consistent with the short-time Fourier transform (STFT). The authors propose to use this consistency as a regularization term to ensure that the spectrogram consistency with the STFT is maintained. The authors show that the proposed method can produce imperceptible adversarial examples that are similar to the original audio perturbations.,"This paper proposes a phase-oriented algorithm for audio adversarial examples with imperceptible audio perturbations. The proposed method is based on phase-orientated energy dissipation (e.g., lp-norm, auditory masking) and a short-time Fourier transform (STFT). The authors show that the proposed method can be applied to the full-sentence adversarial example, and that it can be used in conjunction with the ASR model to improve the performance of ASR models. "
6373,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,Non - contrastive methods USED-FOR representations. Non - contrastive methods USED-FOR self - supervised learning. BYOL HYPONYM-OF Non - contrastive methods. BYOL HYPONYM-OF self - supervised learning. augmentation process USED-FOR representation. DirectPred USED-FOR predictor. DirectSet(α ) USED-FOR projection matrix. sample complexity EVALUATE-FOR downstream tasks. DirectSet(α ) USED-FOR downstream tasks. DirectSet(α ) USED-FOR linear network. sample complexity EVALUATE-FOR DirectSet(α ). weight decay USED-FOR implicit threshold. eigen - decomposition step USED-FOR DirectPred. CIFAR-100 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-100. STL-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION STL-10. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. DirectCopy COMPARE DirectPred. DirectPred COMPARE DirectCopy. CIFAR-10 EVALUATE-FOR DirectCopy. ImageNet EVALUATE-FOR DirectCopy. CIFAR-10 EVALUATE-FOR DirectPred. CIFAR-10 CONJUNCTION STL-10. STL-10 CONJUNCTION CIFAR-10. Generic is approaches. Task is augmentation. ," is a non-contrastive representation learning method for image classification. This paper proposes to use DirectSet(α) as the projection matrix for self-supervised learning. The proposed method achieves better sample complexity compared to BYOL on ImageNet, CIFAR-10, and STL-10.","This paper proposes DirectCopy, a non-contrastive representation learning method for self-supervised learning. DirectCopy is an extension of BYOL, which augments the representation of a linear network with a pre-trained predictor. In contrast, DirectCopy augments a projection matrix with a weight decay step. The proposed method is evaluated on CIFAR-10, STL-10 and ImageNet datasets."
6389,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,method USED-FOR learning long - term sequential dependencies. Long Expressive Memory ( LEM ) USED-FOR learning long - term sequential dependencies. it USED-FOR input - output maps. it USED-FOR sequential tasks. long - term dependencies FEATURE-OF sequential tasks. time - discretization USED-FOR system. system of multiscale ordinary differential equations CONJUNCTION time - discretization. time - discretization CONJUNCTION system of multiscale ordinary differential equations. system of multiscale ordinary differential equations USED-FOR LEM. rigorous bounds USED-FOR exploding and vanishing gradients problem. rigorous bounds USED-FOR LEM. LEM USED-FOR dynamical systems. accuracy EVALUATE-FOR LEM. gated recurrent units CONJUNCTION long short - term memory models. long short - term memory models CONJUNCTION gated recurrent units. recurrent neural networks CONJUNCTION gated recurrent units. gated recurrent units CONJUNCTION recurrent neural networks. LEM COMPARE recurrent neural networks. recurrent neural networks COMPARE LEM. image and time - series classification CONJUNCTION dynamical systems prediction. dynamical systems prediction CONJUNCTION image and time - series classification. LEM COMPARE long short - term memory models. long short - term memory models COMPARE LEM. dynamical systems prediction CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION dynamical systems prediction. LEM COMPARE gated recurrent units. gated recurrent units COMPARE LEM. image and time - series classification CONJUNCTION keyword spotting and language modeling. keyword spotting and language modeling CONJUNCTION image and time - series classification. Method is gradient - based recurrent sequential learning methods. ,"This paper proposes Long Expressive Memory (LEM), a method for learning long-term dependencies in sequential tasks. LEM is based on a system of multiscale ODEs and time-discretization. Theoretical analysis of exploding and vanishing gradients is provided to show the effectiveness of LEM. Experiments on image and time series classification, dynamical systems prediction, and language modeling show that LEM achieves state-of-the-art performance. ","This paper proposes a novel method for learning long-term dependencies between input-output maps for sequential tasks. The method is based on a system of multiscale ordinary differential equations and time-discretization. The authors provide a rigorous analysis of the exploding and vanishing gradients problem, and show that the proposed method is able to solve it. They also show that LEM can be used for dynamical systems prediction and language modeling. "
6405,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"symmetry USED-FOR architectures. architectures USED-FOR deep learning. rotation CONJUNCTION permutation equivariance. permutation equivariance CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. rotationand permutation - equivariant architectures USED-FOR deep learning. rotationand permutation - equivariant architectures USED-FOR small point clouds. products USED-FOR reductions. reductions PART-OF rotationand permutation - equivariant architectures. attention mechanism USED-FOR reductions. rotation invariance CONJUNCTION covariance. covariance CONJUNCTION rotation invariance. attention USED-FOR permutation equivariance. chemistry CONJUNCTION biology. biology CONJUNCTION chemistry. physics CONJUNCTION chemistry. chemistry CONJUNCTION physics. models USED-FOR architectures. Method are geometric deep learning, and geometric algebra. Task is physical sciences. OtherScientificTerm are twoor three - dimensional space, and vector. ","This paper proposes a rotation-and-permutation-equivariant deep learning model for geometric point clouds. The proposed method is based on the fact that geometric algebra can be expressed as a product of products of two or three-dimensional vectors. The authors show that this product can be reduced to a simple product of product products, which can then be used to train a rotation and permutation equivariant neural networks.  The authors also show that the proposed method can be used for small point clouds, and that it can be trained with attention.   ","This paper studies the problem of geometric deep learning with rotation and permutation-equivariant architectures. The main contribution of the paper is to study the properties of rotation-and-permutation equivariant deep learning models. The authors show that rotation- and permution-equivariant architectures are invariant to rotation invariance and covariance, respectively. They also show that the attention mechanism of the rotation-permuted-architecture can be used to reduce the number of reductions in the model.   "
6421,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,order fulfillment problem HYPONYM-OF combinatorial optimization problems. it USED-FOR online retailing. combinatorial optimization problems PART-OF supply chain management. order fulfillment problem HYPONYM-OF supply chain management. exact mathematical programming methods USED-FOR problem. machine learning method USED-FOR it. tripartite graph USED-FOR machine learning method. edge - featureembedded graph attention mechanism USED-FOR assignment policy. edge - feature - embedded graph attention USED-FOR optimization problem. edge - feature - embedded graph attention USED-FOR heterogeneous information. edge - feature - embedded graph attention USED-FOR high - dimensional edge features. model COMPARE baseline heuristic method. baseline heuristic method COMPARE model. optimality EVALUATE-FOR baseline heuristic method. optimality EVALUATE-FOR model. online inference time COMPARE mathematical programming methods. mathematical programming methods COMPARE online inference time. ,"This paper proposes a novel approach to solve the order fulfillment problem in the context of supply chain management. The proposed approach is based on a novel edge-embedded graph attention mechanism to learn the assignment policy for a tripartite graph, which is then used as a heuristic for the optimization problem. The method is shown to be computationally efficient in terms of inference time and inference complexity. ",This paper proposes a novel approach to solving the order fulfillment problem in the context of supply chain management. The authors propose an edge-feature-embedded graph attention mechanism to handle heterogeneous information in the tripartite graph. They show that the proposed approach outperforms the state-of-the-art in terms of online inference time and optimality.
6437,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"natural language processing models USED-FOR inference process. inferring Concepts PART-OF dialogue summarization task. CODC inference module CONJUNCTION knowledge attention module. knowledge attention module CONJUNCTION CODC inference module. knowledge attention module PART-OF neural summarization model. CODC inference module PART-OF framework. knowledge attention module PART-OF framework. CODC USED-FOR evaluation metric. evaluation metric EVALUATE-FOR methods. automatic evaluation metrics EVALUATE-FOR out - of - context inference. automatic evaluation metrics EVALUATE-FOR natural language generation. CODC inference CONJUNCTION automatic evaluation metrics. automatic evaluation metrics CONJUNCTION CODC inference. natural language generation EVALUATE-FOR out - of - context inference. CODC inference EVALUATE-FOR summarization model. automatic evaluation metrics EVALUATE-FOR summarization model. CIDEr HYPONYM-OF automatic evaluation metrics. model EVALUATE-FOR model. Material are human dialogues, and WordNet. OtherScientificTerm are pragmatics studies, and Dialogue Context ( CODC ). ","This paper proposes to use Dialogue Context (CODC) as a new evaluation metric for dialogue summarization tasks. CODC is used to measure the importance of concepts in the dialogue context. The authors propose to use CIDEr as an evaluation metric, which is a combination of Dialogue Context and Knowledge Attention (KA). The authors show that the proposed method outperforms existing methods in terms of out-of-context inference. ","This paper proposes a dialogue summarization model that uses Dialogue Context (CODC) as an evaluation metric to evaluate the performance of a dialog summarization task. The proposed method is based on a knowledge attention module and a CODC inference module. The method is evaluated on a variety of natural language generation tasks, and it is shown that it outperforms the state-of-the-art in terms of CIDEr."
6446,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"entity embeddings USED-FOR Artificial Intelligence. embedding models USED-FOR Horn rules. embedding strategies USED-FOR monotonic and non - monotonic attribute dependencies. OtherScientificTerm are embeddings, vectors, semantic dependencies, and attribute dependencies. Method is attribute embeddings. ","This paper studies the problem of learning entity embeddings in the presence of monotonic and non-monotonic attribute dependencies. The authors show that embedding strategies can learn monotonically monotonicity-dependent attribute dependencies, which is a property of the embedding model. They show that this property is a result of the fact that the model is able to learn a set of embedding functions that are monotone and nonmonotone. They further show that the learned embedding function can be used to learn the monotony-dependent attributes.   ",This paper studies the problem of embedding embeddings that are monotonic and non-monotonic attribute dependencies. The authors show that embedding strategies can be used to learn monotonicity-dependent embedding schemes that can be applied to a variety of attribute embedding models. They show that the embedding strategy that is most effective is the one that learns to learn a set of monotonically non-convex attributes. They also show that this strategy can be combined with other embedding methods to learn an embedding model that is monotone.  
6455,SP:794cca5205d667900ceb9a1332b6272320752ef4,transformer - based models USED-FOR natural language processing tasks. transformers USED-FOR natural language. commonsense reasoning CONJUNCTION logical reasoning. logical reasoning CONJUNCTION commonsense reasoning. mathematical reasoning CONJUNCTION commonsense reasoning. commonsense reasoning CONJUNCTION mathematical reasoning. transformers USED-FOR reasoning tasks. mathematical reasoning HYPONYM-OF reasoning tasks. logical reasoning HYPONYM-OF reasoning tasks. commonsense reasoning HYPONYM-OF reasoning tasks. ,"This paper studies the use of transformers for reasoning tasks in natural language processing. The authors show that transformers can be used to perform reasoning tasks such as logical reasoning, mathematical reasoning, commonsense reasoning, and mathematical reasoning. The experiments show that the proposed model outperforms the state-of-the-art models on these tasks. ","This paper presents a set of experiments on a variety of reasoning tasks, including mathematical reasoning, commonsense reasoning, logical reasoning, and commonsense logical reasoning. The authors show that transformers can be used to perform reasoning tasks such as logical reasoning and mathematical reasoning. They also show that they can also be used for reasoning tasks like common sense reasoning.   "
6464,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"paradigms PART-OF machine learning. complexity FEATURE-OF tasks. algorithmic procedures USED-FOR representation transformations. domaingeneral framework USED-FOR algorithmic procedures. compositional recursive learner HYPONYM-OF domaingeneral framework. compositional recursive learner USED-FOR compositional generalization. compositional approach COMPARE baselines. baselines COMPARE compositional approach. compositional approach COMPARE learner. learner COMPARE compositional approach. Generic are it, and they. Task are generalization, and compositional generalization problem. OtherScientificTerm are compositional structure of the task distribution, compositional problem graph, and shared subproblems. ","This paper studies the problem of compositional generalization, i.e., the generalization of a given task to a set of tasks that are compositional in the sense that the task distribution is composed of subproblems that can be represented as a compositional graph. The authors propose a new method to learn a representation of the compositional structure of a task distribution, which is then used to train a classifier that is able to generalize to new tasks in the same way as the original task distribution. The method is based on the notion of a ""domaingeneral learner"", which is an extension of a previous work on compositional representation learning. The main contribution of the paper is to show that the proposed method is computationally efficient and can be used to learn compositional representations of new tasks.  ",This paper proposes a new approach to generalization in the context of the compositional generalization problem. The main idea of the paper is to use a compositional recursive learner (CRL) framework to learn a set of compositional subproblems from the task distribution. The authors show that CRL can be used to improve the generalization performance of the learner. They also provide a theoretical analysis of CRL. 
6468,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"weight pruning HYPONYM-OF network model compression. activation pruning CONJUNCTION weight pruning. weight pruning CONJUNCTION activation pruning. weight pruning PART-OF Integral Pruning ( IP ) technique. activation pruning PART-OF Integral Pruning ( IP ) technique. IPnet HYPONYM-OF network. execution efficiency EVALUATE-FOR network. activation functions FEATURE-OF network models. datasets EVALUATE-FOR network models. network models EVALUATE-FOR IPnet. datasets EVALUATE-FOR IPnet. testing accuracy EVALUATE-FOR IPnet. IPnet COMPARE dense models. dense models COMPARE IPnet. computation cost EVALUATE-FOR IPnet. Method are deep neural networks ( DNNs ), and DNNs. Task is compression. OtherScientificTerm are weights, connections, sparsity, and activation and weight numbers. ","This paper proposes Integral Pruning (IP), a method to prune weights and activations in deep neural networks (DNNs) in order to reduce the computational cost of training DNNs. The method is based on the observation that activation pruning and weight pruning can be seen as two different forms of pruning: one that prunes the weights, and the other that prune the activations. The authors propose to combine the two pruning methods in a unified way, and show that the proposed method can reduce the computation cost in terms of training time and accuracy.  ","This paper proposes a new method for training deep neural networks (DNNs) with weight pruning and activation pruning. The proposed method is based on the Integral Pruning (IP) technique, which prunes the weights, connections, and sparsity of DNNs by pruning the activation and weight numbers. The authors show that the proposed method can reduce the computation cost of training a DNN by a factor of 1.5. The method is evaluated on a variety of datasets, and it is shown to be competitive with the state-of-the-art."
6472,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"discovery of relevant features USED-FOR task. Machine learning driven feature selection USED-FOR discovery. methodology USED-FOR False Discovery Rate. Generative Adversarial Networks framework USED-FOR knockoffs. stability network CONJUNCTION power network. power network CONJUNCTION stability network. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. discriminator CONJUNCTION stability network. stability network CONJUNCTION discriminator. networks CONJUNCTION generator. generator CONJUNCTION networks. generator CONJUNCTION stability network. stability network CONJUNCTION generator. discriminator CONJUNCTION power network. power network CONJUNCTION discriminator. networks PART-OF model. discriminator PART-OF model. stability network PART-OF model. generator PART-OF model. power network PART-OF model. model USED-FOR feature selection. it COMPARE model. model COMPARE it. it COMPARE knockoff generation model. knockoff generation model COMPARE it. model USED-FOR non - Gaussian settings. knockoff generation model USED-FOR Gaussian setting. non - Gaussian settings EVALUATE-FOR it. real - world dataset EVALUATE-FOR it. real - world dataset EVALUATE-FOR model. Task are Feature selection, and overfitting in prediction. OtherScientificTerm are relevant genetic factors, features, and feature distribution. Metric is expert time. Method is Knockoff framework. ","This paper proposes a novel method for feature selection in machine learning based on knockoffs. The proposed method is based on the Generative Adversarial Networks (GAN) framework, where the generator and discriminator are trained to generate a set of ""knockoffs"" that can be used to improve the performance of the model. The method is evaluated on a synthetic and real-world datasets. ","This paper proposes a new method for feature selection based on knockoff generation. The proposed method is based on the Generative Adversarial Networks framework. The key idea is to use a generative adversarial network (GAN) to generate knockoffs from a set of samples. The GAN is trained using a combination of a discriminator, a stability network, and a power network. Experiments show that the proposed method outperforms the state-of-the-art in terms of false discovery rate.   "
6476,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"Pruning techniques CONJUNCTION Winograd convolution. Winograd convolution CONJUNCTION Pruning techniques. Winograd convolution USED-FOR CNN computation. Pruning techniques USED-FOR CNN computation. Winograd transformation USED-FOR sparsity. learning rates USED-FOR Winograd - domain retraining. ReLU function PART-OF Winograd domain. pruning method USED-FOR Winograd - domain weight sparsity. spatial - Winograd pruning HYPONYM-OF pruning method. importance factor matrix USED-FOR weight importance. importance factor matrix USED-FOR weight gradients. weight importance CONJUNCTION weight gradients. weight gradients CONJUNCTION weight importance. pruning PART-OF Winograd domain. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR10. models EVALUATE-FOR method. datasets EVALUATE-FOR method. datasets EVALUATE-FOR models. CIFAR10 HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. Winograddomain sparsities EVALUATE-FOR method. Method are Deep convolutional neural networks ( CNNs ), sparse Winograd convolution, and Winograd - domain network. Generic are they, and technique. OtherScientificTerm are weight sparsity, network structure, network structures, spatial - domain weights, and spatial - domain sparsity. ","This paper proposes a method to improve the sparsity of convolutional neural networks (CNNs) by pruning the weights in the Winograd convolution (Winograd-convolution) layer. The method is based on the observation that the importance factor matrix (i.e., the importance of each layer to the overall network) depends on the weight importance matrix of the previous layer, which is then used to compute the importance matrix for the current layer's weights. The authors then show that this importance matrix can be used to estimate the gradients of the weights from previous layers, which can then be used for pruning.   The authors show that the proposed method improves the performance of CNNs on CIFAR-10 and ImageNet. ","This paper proposes a new pruning method for sparse Winograd convolution. The main idea is to use spatial-Winograd pruning to reduce the spatial-domain sparsity of the weights in the convolutional network. The method is based on the importance factor matrix (i.e., the weight importance matrix) which is used to estimate the importance of the weight gradients. The authors show that the proposed method can reduce the weight sparsity by a factor of 1.5. They also show that their method can improve the performance of CNNs on CIFAR-10 and ImageNet."
6480,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,Adversarially Learned Mixture Model ( AMM ) HYPONYM-OF generative model. generative model USED-FOR unsupervised or semi - supervised data clustering. adversarially optimized method USED-FOR conditional dependence. AMM USED-FOR conditional dependence. AMM HYPONYM-OF adversarially optimized method. AMM USED-FOR semantic separation of complex data. MNIST and SVHN datasets EVALUATE-FOR AMM. unsupervised clustering error rates EVALUATE-FOR AMM. MNIST and SVHN datasets EVALUATE-FOR AMM. AMM USED-FOR semi - supervised extension. SVHN dataset EVALUATE-FOR semi - supervised extension. classification error rate EVALUATE-FOR semi - supervised extension. Material is labeled data. ," clustering is an important problem in data clustering. This paper proposes an adversarially learned model for unsupervised or semi-supervised clustering based on a generative model. The proposed model, called Adversarially Learned Mixture Model (AMM), is based on the idea of conditional dependence. AMM is shown to improve clustering performance on MNIST and SVHN datasets. ",This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed method is based on the Adversarially Learned Mixture Model (AMM). AMM is an adversarially optimized method that learns the conditional dependence of the data. The authors show that AMM can be used to improve the performance of the clustering task on MNIST and SVHN datasets. 
6484,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"stochastic binary layers USED-FOR gradients. reparameterization USED-FOR ARM estimator. REINFORCE CONJUNCTION reparameterization. reparameterization CONJUNCTION REINFORCE. variable augmentation CONJUNCTION REINFORCE. REINFORCE CONJUNCTION variable augmentation. variable augmentation USED-FOR ARM estimator. REINFORCE USED-FOR ARM estimator. adaptive variance reduction USED-FOR Monte Carlo integration. ARM estimator USED-FOR adaptive variance reduction. REINFORCE estimator USED-FOR augmented space. antithetic sampling USED-FOR augmented space. variance - reduction mechanism USED-FOR ARM estimator. antithetic sampling USED-FOR variance - reduction mechanism. auto - encoding variational inference CONJUNCTION maximum likelihood estimation. maximum likelihood estimation CONJUNCTION auto - encoding variational inference. ARM estimator USED-FOR discrete latent variable models. ARM estimator USED-FOR maximum likelihood estimation. ARM estimator USED-FOR auto - encoding variational inference. stochastic binary layers FEATURE-OF discrete latent variable models. Metric is computational complexity. OtherScientificTerm are common random numbers, and Python code. ",This paper proposes a new estimator for estimating the gradient of a discrete latent variable model with stochastic binary layers. The proposed estimator is based on a variance-reduced estimator (ARM) that is used for Monte Carlo integration and variational inference. The variance reduction is achieved by using an adaptive variance reduction to reduce the variance of the estimator. The estimator can be used to estimate the augmented space of the augmented data. The paper also provides a theoretical analysis of the variance reduction. ,"This paper proposes a new estimator for estimating the variance of a stochastic binary layer of a discrete latent variable model. The estimator is based on the REINFORCE estimator, which is an extension of the ARM estimator. The variance-reduction mechanism of the estimator consists of two steps. First, the variance reduction is used to reduce the variance in the augmented space. Second, it is used for the adaptive variance reduction in Monte Carlo integration. The authors show that their estimator can be used for auto-encoding variational inference. "
6488,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,Modularity PART-OF deep learning libraries. probabilistic distributions CONJUNCTION probabilistic model. probabilistic model CONJUNCTION probabilistic distributions. modularity FEATURE-OF probabilistic programming language. Bayesian nonparametric models HYPONYM-OF probabilistic model. probabilistic modules HYPONYM-OF re - usable building blocks. re - usable building blocks PART-OF modular probabilistic programming language. probabilistic distributions FEATURE-OF random variables. random variables PART-OF probabilistic module. probabilistic distributions PART-OF probabilistic module. inference methods PART-OF probabilistic module. pre - specified inference methods USED-FOR probabilistic modules. pre - specified inference methods USED-FOR variational inference. probabilistic modules PART-OF MXFusion. Gaussian process models EVALUATE-FOR probabilistic modules. real data EVALUATE-FOR probabilistic modules. Method is probabilistic programming. ,"This paper proposes a probabilistic programming language for Bayesian non-parametric models. The proposed method is based on the idea of modular probabilism, which is an extension of probabilistics to the Bayesian setting. The main idea is to define a set of ""probabilistic modules"" that can be used to model the distribution of random variables in the model. These modules can then be used in conjunction with any probabilistically trained model to perform inference. The authors show that the proposed method can be combined with a variety of existing methods to perform variational inference. ","This paper proposes a new probabilistic programming language, called MXFusion, that can be used to build probabilistically-modularized models. The core idea is to use a probabilist model as a pre-trained model, and then use a module for each of the parameters of the model. The module is then used to train the model by using a set of pre-specified inference methods. Experiments show that the proposed model is able to outperform the state-of-the-art in terms of performance on a variety of datasets."
6492,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"heuristically designed pruning schedules CONJUNCTION hyperparameters. hyperparameters CONJUNCTION heuristically designed pruning schedules. heuristically designed pruning schedules USED-FOR iterative optimization procedure. pruning PART-OF methods. iterative optimization procedure USED-FOR pruning. approach USED-FOR network. structurally important connections PART-OF network. connection sensitivity USED-FOR structurally important connections. network USED-FOR task. structurally important connections USED-FOR task. connection sensitivity USED-FOR saliency criterion. pretraining CONJUNCTION complex pruning schedule. complex pruning schedule CONJUNCTION pretraining. it USED-FOR architecture variations. pruning USED-FOR sparse network. method COMPARE reference network. reference network COMPARE method. accuracy EVALUATE-FOR reference network. method USED-FOR architectures. method USED-FOR sparse networks. sparse networks COMPARE reference network. reference network COMPARE sparse networks. Tiny - ImageNet classification tasks EVALUATE-FOR method. convolutional, residual and recurrent networks HYPONYM-OF architectures. MNIST EVALUATE-FOR method. Tiny - ImageNet classification tasks EVALUATE-FOR reference network. MNIST EVALUATE-FOR reference network. accuracy EVALUATE-FOR method. methods COMPARE approach. approach COMPARE methods. Task are Pruning large neural networks, and training. Metric is reduced space and time complexity. ","This paper proposes a method for pruning large neural networks. The main idea is to use connection sensitivity as a saliency criterion to select the most important connections in the network to prune. Theoretical analysis is provided to show that the proposed method is able to reduce the training time and space complexity. Experiments on MNIST, CIFAR-10, and Tiny-ImageNet show the effectiveness of the method. ","This paper proposes a new method for pruning large neural networks. The main idea is to use the saliency criterion to select the most important connections in the network to prune. The proposed method is evaluated on a variety of datasets, including MNIST, Tiny-ImageNet, and CIFAR-10. It is shown that the proposed method outperforms the state-of-the-art in terms of accuracy and training time."
6496,SP:986b9781534ffec84619872cd269ad48d235f869,"inference algorithm USED-FOR decoding neural sequence models. Beam search HYPONYM-OF inference algorithm. Beam search USED-FOR decoding neural sequence models. greedy search COMPARE beam search. beam search COMPARE greedy search. beam search USED-FOR non - greedy local decisions. beam widths USED-FOR beam search. beam search algorithm USED-FOR sequence synthesis tasks. evaluation score FEATURE-OF sequences. highly non - greedy decisions USED-FOR beam search. constrained beam search USED-FOR beam search degradation. methods USED-FOR search. OtherScientificTerm are beam width, early and highly non - greedy decisions, and ( conditional ) probability. Metric is evaluation scores. Material is copies. ","This paper proposes a beam search algorithm for decoding neural sequence models. The main contribution of the paper is to show that beam search can be used as an alternative to greedy search in sequence synthesis tasks. The proposed method is based on the observation that the evaluation score of a sequence is a function of the number of early and highly non-greedy decisions made during beam search. The authors show that the performance of beam search is affected by the beam width of the search space, and propose to use constrained beam search to reduce the beam search degradation.   ",This paper proposes a new beam search algorithm for neural sequence synthesis tasks. The main contribution of the paper is a theoretical analysis of the impact of beam search on the performance of neural sequence models. The authors show that beam search can be more efficient than greedy search in the early and highly non-greedy decisions. They also show that the beam search is more efficient when the beam width is smaller than the number of copies of the input sequence. 
6500,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"method USED-FOR sample efficiency. demonstrations USED-FOR method. curriculum USED-FOR task. Backplay COMPARE competitive methods. competitive methods COMPARE Backplay. Backplay USED-FOR large grid worlds. sample efficiency EVALUATE-FOR competitive methods. training speed EVALUATE-FOR Backplay. sample efficiency EVALUATE-FOR Backplay. reward shaping CONJUNCTION behavioral cloning. behavioral cloning CONJUNCTION reward shaping. behavioral cloning CONJUNCTION reverse curriculum generation. reverse curriculum generation CONJUNCTION behavioral cloning. Task is Model - free reinforcement learning ( RL ). OtherScientificTerm are policy, and sparse rewards. Generic is approach. ","This paper proposes Backplay, a model-free reinforcement learning method for large-scale reinforcement learning problems with sparse rewards. The main idea is to use a curriculum of demonstrations to guide the learning process. The proposed method is evaluated on a variety of tasks and achieves state-of-the-art performance.  ","This paper proposes a new method for model-free reinforcement learning (MRL) where the goal is to learn a curriculum of demonstrations that can be used to improve the sample efficiency of the learner. The proposed method, called Backplay, is based on the idea of reward cloning and reverse curriculum generation. The authors show that the proposed method outperforms the state-of-the-art in terms of sample efficiency, training speed, and reward shaping."
6504,SP:426c98718b2dbad640380ec4ccb2b656958389bc,"Model compression USED-FOR large neural networks. model size CONJUNCTION accuracy. accuracy CONJUNCTION model size. hand - crafted heuristics USED-FOR compression techniques. AlexNet CONJUNCTION VGG16. VGG16 CONJUNCTION AlexNet. hyper - parameters USED-FOR method. expert knowledge USED-FOR method. OtherScientificTerm are compression ratio, compression ratios, Hessian, and pruning criterion. Method are Multi - Layer Pruning method ( MLPrune ), and Kroneckerfactored Approximate Curvature method. Generic is state - of - theart. Material is ImageNet. ",": This paper proposes a new method for model pruning based on the Kronecker-factored approximation of the Hessian of the loss function. Theoretical analysis is provided to show that the proposed method is equivalent to the multi-layer pruning (MLPrune) method. Experiments are conducted on ImageNet, AlexNet, and VGG16. ",This paper proposes a new method for model compression. The main idea is to use the Kroneckerfactored Approximate Curvature (KAC) method to estimate the Hessian of the model compression ratio. The authors show that the KAC method is more efficient than the standard multi-layer pruning method (MLPrune) in terms of model size and accuracy. They also show that their method is better than the state-of-the-art.
6508,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"sample quality CONJUNCTION training stability. training stability CONJUNCTION sample quality. Generative Adversarial Networks ( GANs ) USED-FOR real - world applications. training stability EVALUATE-FOR GAN variants. sample quality EVALUATE-FOR GAN variants. architectural choices USED-FOR GAN learning. analytic framework USED-FOR GANs. unit-, object-, and scene - level FEATURE-OF GANs. object concepts FEATURE-OF interpretable units. segmentation - based network dissection method USED-FOR interpretable units. object concepts PART-OF images. models CONJUNCTION datasets. datasets CONJUNCTION models. internal representations CONJUNCTION models. models CONJUNCTION internal representations. artifact - causing units USED-FOR GANs. framework USED-FOR applications. open source interpretation tools USED-FOR GAN models. Generic are they, and units. Method is GAN. OtherScientificTerm is interventions. ","This paper proposes to analyze GANs at the unit-, object- and scene-level. The authors propose a segmentation-based network dissection method to identify interpretable units and object concepts in images. The proposed method is evaluated on a variety of GAN models and datasets. The experiments show the effectiveness of the proposed method.",This paper proposes a new analytic framework for analyzing GANs. The authors propose a dissection-based network dissection method to identify interpretable units in the GAN. The dissection is based on a segmentation-based neural network segmentation method. They show that the dissection can be used to identify units that are artifact-causing in GAN training. They also show that it is possible to identify the unit- and object-level representations that are interpretable in the context of a GAN model. 
6512,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"parameter ` distances COMPARE function L distances. function L distances COMPARE parameter ` distances. space FEATURE-OF networks. L distance USED-FOR optimization. L - space FEATURE-OF network. loss curvature HYPONYM-OF local approximations. Method is neural network. OtherScientificTerm are L Hilbert space, optimization trajectory, catastrophic forgetting, learning rule, and function distances. Generic are distances, and applications. Metric is L/ ` ratio. Task is multitask learning. ","This paper studies the problem of catastrophic forgetting in multi-task learning. In particular, the authors show that the loss of a neural network in L-Hilbert space is a function of the distance between the parameters of the network and the weights of the function. They show that this distance is independent of the number of parameters in the network, and that it can be used as a learning rule to prevent catastrophic forgetting. They also show that parameter ` distances are independent of function L distances and that the L/L ratio between parameter ` and function ` distances can be computed in terms of the loss curvature. ","This paper studies the problem of multi-task learning in the L-Hilbert space. The authors consider the setting where the task is to learn a neural network in a Hilbert space, and the goal is to minimize the L/L ratio between the parameters of the network and the function L. They show that the distance between parameters and function L is a function of the number of parameters in the network, and that it can be used as a measure of the complexity of the task. They also provide a theoretical analysis of the convergence of their results.  "
6516,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,measurements of dynamic entities PART-OF Biological data. DyMoN HYPONYM-OF neural network framework. probability distribution FEATURE-OF deep generative Markov model. deep generative Markov model USED-FOR neural network framework. deep generative Markov model USED-FOR DyMoN. Dynamics Modeling Network HYPONYM-OF deep neural network framework. DyMoN USED-FOR idiosyncrasies of biological data. noise CONJUNCTION sparsity. sparsity CONJUNCTION noise. data USED-FOR probability distributions. trajectories HYPONYM-OF probability distributions. dimensionality reduction methods USED-FOR trajectories. probability distributions USED-FOR DyMoN. training efficiency CONJUNCTION accuracy. accuracy CONJUNCTION training efficiency. Kalman filters CONJUNCTION hidden Markov models. hidden Markov models CONJUNCTION Kalman filters. deep models COMPARE shallow models. shallow models COMPARE deep models. hidden Markov models HYPONYM-OF shallow models. Kalman filters HYPONYM-OF shallow models. DyMoN USED-FOR biological systems. DyMoN USED-FOR features of the dynamics. OtherScientificTerm is longitudinal measurements. Material is biological data. Generic is model. ,"This paper proposes a deep generative model for biological data. The model is based on a generative Markov model, where the parameters are learned from a probability distribution over trajectories. The authors show that the model is robust to noise and sparsity in the data, and that it is able to capture the dynamics of biological data with high accuracy. The proposed model is evaluated on synthetic and real-world data.","This paper proposes a deep generative Markov model (DMoN) for biological data. The model is based on the notion of a probability distribution over a set of trajectories, where the probability distribution is defined by a generative model. The authors show that DyMoN is able to learn the probability distributions of the trajectories in biological data, and that it can be used to train a deep neural network model that is robust to noise, sparsity, and dimensionality reduction methods. They also show that the model can be trained with Kalman filters and hidden Markov models. "
6520,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"graph Laplacian USED-FOR unsupervised classification method. classification method COMPARE architecture. architecture COMPARE classification method. approximate linear map CONJUNCTION spectral clustering theory. spectral clustering theory CONJUNCTION approximate linear map. spectral clustering theory USED-FOR generative adversarial networks. approximate linear map USED-FOR generative adversarial networks. spectral clustering theory USED-FOR dimension reduced spaces. framework USED-FOR images. approximate linear connector network C USED-FOR cerebral cortex. spectral clustering HYPONYM-OF unsupervised learning. unsupervised classification method USED-FOR method. Method are human visual recognition system, and connector network. OtherScientificTerm is human brains. ","This paper proposes an unsupervised learning method for image classification based on graph Laplacian clustering. The proposed method is based on an approximate linear connector network C, which is a generalization of the graph neural networks (GNNs) proposed in [1] and [2]. The authors show that the proposed method outperforms existing methods in terms of classification accuracy on image classification tasks. The authors also show that their method can be used to learn a spectral clustering model for images.",This paper proposes a graph Laplacian-based unsupervised learning method for image classification. The proposed method is based on the spectral clustering theory of GANs. The authors show that the proposed method outperforms the state-of-the-art in terms of classification accuracy. They also show that their method can be used in conjunction with a neural network for image recognition.
6524,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,permuted set USED-FOR permutation - invariant representation. bottleneck PART-OF set models. model USED-FOR permutations and set representations. number sorting CONJUNCTION image mosaics. image mosaics CONJUNCTION number sorting. classification CONJUNCTION visual question answering. visual question answering CONJUNCTION classification. image mosaics CONJUNCTION classification. classification CONJUNCTION image mosaics. image mosaics CONJUNCTION visual question answering. visual question answering CONJUNCTION image mosaics. image mosaics USED-FOR classification. explicit or implicit supervision USED-FOR model. explicit or implicit supervision USED-FOR permutations and set representations. OtherScientificTerm is Representations of sets. Method is Permutation - Optimisation module. ,"This paper proposes a method for learning permutation-invariant representations from permuted sets. The proposed method is based on the idea that the permutation is a bottleneck in set models, and that it is possible to learn a set representation that is invariant to permutations. The method is evaluated on image classification, number sorting, and visual question answering tasks.  ","This paper proposes a new method for learning permutation-invariant representations for set models. The method is based on the idea that permutations and set representations are invariant to permutations, and that the bottleneck of set models is the number of permutations in the set. The authors propose a method to learn a set representation that is permutation invariant, and they show that this representation can be learned by using the Permutation-Optimisation module. The proposed method is evaluated on a variety of tasks, including number sorting, image mosaics, and visual question answering. "
6528,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"Projective Subspace Networks ( PSN ) HYPONYM-OF deep learning paradigm. deep learning paradigm USED-FOR non - linear embeddings. limited supervision USED-FOR non - linear embeddings. PSN approach USED-FOR end - to - end learning. Method are learning techniques, and PSN. OtherScientificTerm are dynamical environments, affine subspace, projective subspace, and higher - order information datapoints. Task is lifelong learning. Generic is modeling. ","This paper proposes Projective Subspace Networks (PSN), a new deep learning paradigm for lifelong learning. The main idea is to use a projective subspace network to learn non-linear embeddings in the affine subspace with limited supervision. The authors show that the proposed PSN can be used for end-to-end learning in dynamical environments. The proposed method is shown to outperform existing deep learning methods on a variety of tasks.","This paper proposes a new deep learning paradigm, called Projective Subspace Networks (PSN), for learning non-linear embeddings in dynamical environments. The main idea of PSN is to use a projective subspace to model the dynamical environment, where the environment is represented as a set of affine subspaces, and the subspace is represented by an affine embedding. The subspace can be represented as an embedding of the affine space. The authors show that PSN can be used to learn non-Linear embedding with limited supervision. They also show that their approach can be applied to end-to-end learning."
6532,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"meta - learning method USED-FOR fast adaptation. CAML HYPONYM-OF meta - learning method. context parameters CONJUNCTION shared parameters. shared parameters CONJUNCTION context parameters. gradient steps USED-FOR task - specific loss. gradient steps USED-FOR context parameters. approaches COMPARE method. method COMPARE approaches. method USED-FOR networks. memory writes CONJUNCTION network communication. network communication CONJUNCTION memory writes. memory writes USED-FOR training. approach COMPARE MAML. MAML COMPARE approach. approach USED-FOR task embeddings. task - specific learning rate EVALUATE-FOR approach. context parameters USED-FOR task embeddings. OtherScientificTerm are model parameters, overfitting, and partitionings of the parameter vectors. Generic are model, network, and task. Method is distributed machine learning systems. ",This paper proposes a meta-learning method for distributed distributed machine learning systems. The proposed method is based on the idea of using context parameters and shared parameters to learn a task-specific loss for each task. The authors show that the proposed method achieves better performance than the state-of-the-art MAML method in terms of learning rate. ,"This paper proposes a new meta-learning method for distributed machine learning systems. The authors propose a method to learn a task-specific loss for each task using the context parameters of the model. The proposed method is based on the CAML framework. The main idea is to learn the task embeddings of the parameters of a model and use them to adapt the model to the new task. The method is evaluated on a variety of tasks, and it is shown that the proposed method outperforms the state-of-the-art. "
6536,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"Utility providers USED-FOR data privacy. natural informationtheoretic bounds FEATURE-OF utility - privacy trade - off. explicit learning architectures USED-FOR privacypreserving representations. utility algorithms USED-FOR privacy requirements. gender CONJUNCTION emotion detection. emotion detection CONJUNCTION gender. use cases EVALUATE-FOR framework. face recognition USED-FOR mobile devices. mobile devices HYPONYM-OF application. subject - withinsubject HYPONYM-OF use cases. Task are privacy protecting challenges, and facial verification. Generic are paradigm, bound, and algorithm. Method are machine learning algorithms, privacy - preserving representations, sanitization process, space - preserving transformations, and face identity detector. OtherScientificTerm are gender - and - subject, gender attribute, emotion - and - gender, and independent variables. ",This paper studies the privacy-preserving representation learning for face verification in the face identity verification setting. The authors propose a new privacy-maximizing utility-privacy trade-off between privacy preserving representations and privacy-protective representations. The main contribution of the paper is a new natural information-theoretic bound on the utility-private trade-offs between privacy and utility in face verification.   ,This paper studies the utility-privacy trade-off between privacy-preserving representations and privacy-protecting representations. The authors provide a natural information theory bound for the trade-offs between privacy preserving representations and utility preserving representations. They show that the tradeoff can be reduced to a tradeoff between the privacy preserving representation and the utility preserving representation. They also provide an algorithm that can be applied to face identity verification and face recognition.
6540,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"training instability FEATURE-OF Generative Adversarial Networks ( GANs ). backpropagation signal USED-FOR generator. task difficulty FEATURE-OF discriminator. progressive augmentation USED-FOR generator. progressive augmentation USED-FOR GAN objective. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION CELEBA. CELEBA CONJUNCTION CIFAR10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. image generation task EVALUATE-FOR approach. Method are hyper - parameter tuning, progressive augmentation of GANs ( PAGAN ), and GAN training. OtherScientificTerm are fragile training behaviour, and input space. ","This paper proposes a progressive augmentation method for GANs to improve the performance of the discriminator. The proposed method is based on the observation that GAN discriminators are sensitive to hyper-parameter tuning. The authors propose to augment the training objective with a gradient-based gradient-augmentation method. The method is evaluated on image generation tasks from Fashion-MNIST, CIFAR-10, and CELEBA.","This paper proposes a progressive augmentation of GANs (PAGAN) to improve the performance of the discriminator. The proposed method is based on the idea of hyper-parameter tuning, where the generator and discriminator are trained with the same parameters. The authors show that PAGAN improves the performance on a variety of tasks, including CIFAR-10 and CELEBA. "
6544,SP:c210982ccdd134d4b293dbe144990398eefe1a86,"models USED-FOR neural responses. models USED-FOR primary visual cortex ( V1 ). convolutional neural networks ( CNNs ) USED-FOR V1 activity. orientation selectivity CONJUNCTION phase invariance. phase invariance CONJUNCTION orientation selectivity. orientation selectivity FEATURE-OF V1 neurons. phase invariance FEATURE-OF V1 neurons. V1 neurons USED-FOR features. framework USED-FOR common features. rotation - equivariant convolutional neural network USED-FOR framework. mouse primary visual cortex FEATURE-OF natural images. two - photon imaging USED-FOR rotation - equivariant CNN. rotation - equivariant network COMPARE regular CNN. regular CNN COMPARE rotation - equivariant network. rotation - equivariant network USED-FOR common features. feature maps USED-FOR regular CNN. Method is energy models. Task are V1 computations, and nonlinear functional organization of visual cortex. OtherScientificTerm is neural activity. ",This paper proposes a rotation-equivariant convolutional neural network (RECN) to model the activity in the primary visual cortex (V1) of mice with two-photon imaging. The proposed RECN is based on an energy-based energy model and is able to capture the common features across V1 neurons. The experiments show that the proposed method outperforms the state-of-the-art methods in terms of V1 activity.   ,This paper proposes a rotation-equivariant convolutional neural network (RECN) for the primary visual cortex (V1) of mice. The proposed RECN is based on the observation that V1 neurons are not phase invariant to the orientation of the image. The authors show that rotation equivariant CNNs can be used to learn a common feature map of the V1. They also show that the rotation-Equivariant network is able to learn the common features of V1 features. The experiments are conducted on two-photon imaging of the mouse's visual cortex.
6548,SP:f17090812ace9c83d418b17bf165649232c223e3,"large datasets USED-FOR neural networks. algorithm USED-FOR robust, communication - efficient learning. algorithm USED-FOR SIGNSGD. algorithm COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE algorithm. communication COMPARE full - precision, distributed SGD. full - precision, distributed SGD COMPARE communication. communication USED-FOR algorithm. convergence USED-FOR parameter regime. parameter regime FEATURE-OF ADAM. large and mini - batch settings FEATURE-OF SIGNSGD. majority vote USED-FOR sign gradients. SGD COMPARE majority vote. majority vote COMPARE SGD. Pytorch USED-FOR distributed training system. time EVALUATE-FOR resnet50. collective communications library ( NCCL ) COMPARE framework. framework COMPARE collective communications library ( NCCL ). Imagenet USED-FOR resnet50. parameter server PART-OF framework. time EVALUATE-FOR framework. AWS p3.2xlarge machines USED-FOR resnet50. Generic is networks. OtherScientificTerm are communicating gradients, machine counts, network faults, gradient vector, server, class of adversaries, and gradient estimate. ","This paper proposes a new distributed gradient descent algorithm for robust, communication-efficient learning on large datasets. The proposed algorithm is based on ADAM, which is a generalization of ADAM in the parameter regime. The authors show that ADAM is computationally efficient in both large and mini-batch settings. The main contribution of this paper is the use of majority vote to compute the sign gradients of the gradients computed by the server.   The main contributions of the paper are as follows:  - The authors propose a new algorithm, called SIGNSGD, which uses majority vote for computing the sign of gradients in the large batch setting. - Theoretical convergence analysis is provided for the proposed algorithm. ","This paper proposes a new method for robust, communication-efficient learning in distributed learning. The authors propose a new algorithm for robust learning in large and mini-batch settings, called SIGNSGD, which is based on ADAM. The main contribution of the paper is that the authors propose to use Pytorch, a distributed training system for training neural networks. They show that the proposed method outperforms the state-of-the-art in terms of time and accuracy. "
6552,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,approach USED-FOR realistic and high - fidelity stock market data. generative adversarial networks USED-FOR approach. conditional Wasserstein GAN USED-FOR history dependence of orders. stock market FEATURE-OF history dependence of orders. finite history dependence FEATURE-OF stochastic process. stochastic process USED-FOR order stream. actual market and synthetic data EVALUATE-FOR approach. Material is real data. ,This paper presents a method for generating high fidelity stock market data using a generative adversarial network with conditional Wasserstein GANs. The method is based on the observation that the history dependence of orders in the stock market depends on the order stream. The authors propose to model this dependence as a stochastic process with finite history dependence. The proposed method is evaluated on both real and synthetic stock data. ,This paper proposes a new approach to generate high-fidelity stock market data from synthetic and real-world data. The main idea is to use a conditional Wasserstein GAN to model the history dependence of orders in the stock market. The conditional GAN is trained using a generative adversarial network (GAN) and a stochastic process. The authors demonstrate the effectiveness of their approach on both real-data and synthetic data.
6556,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"deep reinforcement learning ( DRL ) algorithm USED-FOR arbitrary errors. deep reinforcement learning ( DRL ) algorithm USED-FOR applications. robotics HYPONYM-OF applications. RL agents USED-FOR observed rewards. reward confusion matrix USED-FOR observed rewards. RL agents USED-FOR noisy environments. approaches USED-FOR supervised learning. supervised learning USED-FOR framework. noisy data USED-FOR approaches. noisy data USED-FOR supervised learning. approaches USED-FOR framework. sample complexity EVALUATE-FOR approach. convergence CONJUNCTION sample complexity. sample complexity CONJUNCTION convergence. convergence EVALUATE-FOR approach. policies COMPARE baselines. baselines COMPARE policies. expected rewards EVALUATE-FOR policies. estimated surrogate reward USED-FOR policies. Atari games EVALUATE-FOR PPO algorithm. Method are reinforcement learning ( RL ) models, and unbiased reward estimator aided robust RL framework. OtherScientificTerm are noises, observed reward channel, perturbed rewards, and unbiased surrogate rewards. Task is noisy RL problems. Generic is solution. Material is DRL platforms. Metric is error rates. ",This paper proposes an unbiased reward estimator aided robust reinforcement learning framework for learning in noisy environments. The proposed method is based on a reward confusion matrix that estimates the observed rewards from the observed reward channel. The paper shows that the proposed method achieves better error rates than existing methods in terms of sample complexity and convergence. The experimental results show the effectiveness of the proposed approach.,"This paper proposes an unbiased reward estimator aided robust robust RL framework for noisy reinforcement learning (RL) problems. The proposed method is based on the reward confusion matrix (PPO), which is a weighted sum of the observed rewards and the expected rewards. The authors show that the proposed method converges to a state-of-the-art performance on Atari games with high sample complexity."
6560,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"network structure USED-FOR halting time. overparametrization USED-FOR weight space traversal. power - law - like relationship USED-FOR average step size. Method is gradient descent. OtherScientificTerm are computational requirements, model ’s width, gradient vectors, and traversal. Generic are larger models, and applications. ","This paper studies the problem of estimating the halting time of gradient descent in the weight space of a neural network. The authors show that the average step size of the weight traversal depends on the width of the network and the number of parameters of the weights. They show a power-law-like relationship between the step size and the model width, and show that this relationship can be explained by the overparametrization of the model parameters. They also show that if the network width is larger than a certain threshold, then the weight-space traversal can be reduced to a linear function of the width.   ","This paper studies the problem of estimating the average step size of a weight-space traversal of a neural network. The authors show that the average size of the weight space traversal depends on the width of the network and the number of weights in the network. They show that this is a power-law-like relationship with respect to the weight width. They also provide an overparametrization of the overparameterization of the traversal, and show that it can be used to estimate the average number of steps."
6564,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"learning framework USED-FOR setting. reinforcement learning USED-FOR online optimization problems. reinforcement learning USED-FOR theoretically optimal algorithms. theoretically optimal algorithms USED-FOR online optimization problems. algorithms CONJUNCTION complexity theory. complexity theory CONJUNCTION algorithms. universal and high - entropy training sets HYPONYM-OF adversarial distributions. online knapsack problem CONJUNCTION secretary problem. secretary problem CONJUNCTION online knapsack problem. AdWords problem CONJUNCTION online knapsack problem. online knapsack problem CONJUNCTION AdWords problem. online knapsack problem EVALUATE-FOR ideas. secretary problem EVALUATE-FOR ideas. AdWords problem EVALUATE-FOR ideas. optimal algorithms USED-FOR problems. online primal - dual framework USED-FOR problems. OtherScientificTerm are distributions, and worst case. Method is learner. Generic is models. ",This paper studies the problem of online optimization problems with adversarial distributions. The authors propose an online primal-dual framework to solve the problems. They show that the optimal algorithms can be learned using reinforcement learning. They also provide an algorithm for the AdWords problem and the secretary problem.,"This paper proposes an online primal-dual learning framework for online optimization problems. The main contribution of the paper is a theoretical analysis of the problem of online optimization. The authors show that the worst-case optimal algorithms can be learned from a universal and high-entropy training set. They also show that optimal algorithms for the worst case can be trained from the universal training set and the high entropy training set, and that the best-case algorithms can also be learned for the high-enumeration setting.  "
6568,SP:b99732087f5a929ab248acdcd7a943bce8671510,"inductive biases PART-OF deep reinforcement learning algorithms. domain knowledge CONJUNCTION pretuned hyperparameters. pretuned hyperparameters CONJUNCTION domain knowledge. domain knowledge HYPONYM-OF inductive biases. pretuned hyperparameters HYPONYM-OF inductive biases. fixed components COMPARE adaptive solutions. adaptive solutions COMPARE fixed components. components COMPARE adaptive components. adaptive components COMPARE components. learning EVALUATE-FOR systems. continuous control problems EVALUATE-FOR systems. adaptive components USED-FOR system. tasks EVALUATE-FOR system. deep RL algorithms USED-FOR tasks. deep reinforcement learning ( RL ) community USED-FOR tasks. Go CONJUNCTION Chess. Chess CONJUNCTION Go. board - games CONJUNCTION video - games. video - games CONJUNCTION board - games. video - games CONJUNCTION 3D navigation tasks. 3D navigation tasks CONJUNCTION video - games. Atari HYPONYM-OF video - games. Go HYPONYM-OF board - games. Chess HYPONYM-OF board - games. tuning USED-FOR these. these USED-FOR new domains. tuning USED-FOR new domains. inductive biases FEATURE-OF agents. AlphaZero COMPARE AlphaGo algorithm. AlphaGo algorithm COMPARE AlphaZero. Go - specific inductive biases CONJUNCTION human data. human data CONJUNCTION Go - specific inductive biases. Chess CONJUNCTION Shogi. Shogi CONJUNCTION Chess. AlphaZero USED-FOR Chess. AlphaZero USED-FOR Shogi. biases USED-FOR AlphaZero. Go USED-FOR AlphaZero. generality CONJUNCTION performance. performance CONJUNCTION generality. inductive biases PART-OF algorithms. domain knowledge CONJUNCTION pretuned learning parameters. pretuned learning parameters CONJUNCTION domain knowledge. domain knowledge PART-OF Inductive biases. pretuned learning parameters PART-OF Inductive biases. domain knowledge CONJUNCTION pretune parameters. pretune parameters CONJUNCTION domain knowledge. Generic are problems, and approach. OtherScientificTerm is hyper - parameters. Method are domain - specific components, RL algorithms, AlphaZero algorithm, and learning algorithm. ","This paper proposes to use inductive biases to improve the performance of deep reinforcement learning agents in continuous control problems. The authors propose to use a combination of domain knowledge and hyperparameters from the Deep RL community to improve performance. They show that the proposed method outperforms the state-of-the-art methods in Go, Chess and Shogi.   ","This paper proposes a method for learning deep reinforcement learning agents with inductive biases that can be applied to a wide range of domains. The method is based on the notion of ""domain-specific components"" that are learned by a deep RL algorithm. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization and generalization to new domains. They also show that their method can be used to learn agents that can adapt to a new domain."
6572,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,photo - realistic unknown environments FEATURE-OF navigational instruction. visual - textual co - grounding module CONJUNCTION progress monitor. progress monitor CONJUNCTION visual - textual co - grounding module. complementary components PART-OF self - monitoring agent. visual - textual co - grounding module HYPONYM-OF complementary components. progress monitor HYPONYM-OF complementary components. ablation studies USED-FOR primary components. ablation studies USED-FOR approach. benchmark EVALUATE-FOR selfmonitoring agent. method USED-FOR state. method USED-FOR art. Generic is task. OtherScientificTerm is navigation progress. Metric is success rate. ,This paper proposes a self-monitoring agent that learns to navigate in a photo-realistic environment. The proposed method consists of two components: a visual-textual co-grounding module and a progress monitor. The visual-Textual Co-Grounding module maps the state of the environment to the current state using the current visual input. The progress monitor is used to update the state-action pair and the goal is to predict the next state. The authors show that the proposed method achieves state-of-the-art performance on a benchmark task. ,This paper proposes a self-monitoring agent for navigation in a photo-realistic environment. The proposed method is based on two components: a visual-textual co-grounding module and a progress monitor. The main contribution of the paper is a new ablation study of the primary components of the proposed method. The ablation studies show that the proposed approach achieves better performance than the state-of-the-art on a benchmark task.
6576,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,input - output examples USED-FOR Neural program synthesis. encoder USED-FOR embedding. decoder USED-FOR program. syntax FEATURE-OF embedding. encoder USED-FOR encoder - decoder architecture. embedding USED-FOR decoder. encoder - decoder architecture USED-FOR neural program synthesis approaches. approaches USED-FOR tasks. approaches USED-FOR tasks. tasks EVALUATE-FOR state - of - the - art approach. tasks HYPONYM-OF tasks. accuracy EVALUATE-FOR state - of - the - art approach. Karel HYPONYM-OF tasks. Karel HYPONYM-OF tasks. FlashFill HYPONYM-OF tasks. execution - guided synthesis CONJUNCTION synthesizer ensemble. synthesizer ensemble CONJUNCTION execution - guided synthesis. techniques USED-FOR semantic information. synthesizer ensemble HYPONYM-OF techniques. execution - guided synthesis HYPONYM-OF techniques. synthesizer ensemble HYPONYM-OF semantic information. execution - guided synthesis HYPONYM-OF semantic information. techniques CONJUNCTION encoder - decoder - style neural program synthesizer. encoder - decoder - style neural program synthesizer CONJUNCTION techniques. accuracy EVALUATE-FOR techniques. Karel dataset EVALUATE-FOR techniques. ,"This paper proposes a novel architecture for neural program synthesis, where an encoder encodes input-output examples and a decoder decodes the embedding of the program into a set of embeddings that are then used to generate the output of a synthesized program. The proposed architecture is based on a two-stage process: first, the encoder is trained to predict the program embedding, and second, the decoder is used to synthesize the program.  The proposed method achieves state-of-the-art performance on two tasks: execution-guided synthesis and synthesizer ensemble.","This paper presents a novel approach to neural program synthesis (NPS) for input-output examples. The authors propose an encoder-decoder architecture for NPS, where the encoder encodes the input data and the decoder decodes the output data. The encoder is used to learn the embedding of the input and the output. The decoder is then used to synthesize the output of the program, which is then fed to a synthesizer ensemble. The proposed approach is evaluated on the Karel dataset, where it is shown to achieve state-of-the-art performance. "
6580,SP:dc7dfc1eec473800580dba309446871122be6040,"stability FEATURE-OF batch normalization ( BN ). BN USED-FOR simplified model. ordinary least squares ( OLS ) HYPONYM-OF simplified model. modeling approach USED-FOR problem. scaling law CONJUNCTION convergence. convergence CONJUNCTION scaling law. gradient descent USED-FOR OLS. arbitrary learning rates FEATURE-OF weights. convergence FEATURE-OF arbitrary learning rates. convergence CONJUNCTION acceleration effects. acceleration effects CONJUNCTION convergence. BN USED-FOR gradient descent. mathematical principles USED-FOR batch normalization. OtherScientificTerm is learning rates. Task are OLS problem, and supervised learning problems. ","This paper studies the stability of batch normalization (BN) in the context of the OLS problem. In particular, the authors show that BN can be viewed as a simplified model of the ordinary least squares (OLS) problem, where the weights of the model can be computed by gradient descent. The authors then show that gradient descent with BN converges to the solution of OLS with a certain learning rate, and that the learning rate can be arbitrarily large. Finally, they show the convergence of BN with arbitrary learning rates.   ","This paper studies the stability of batch normalization (BN) in the context of ordinary least squares (OLS) problems. The authors show that BN can be used to model the OLS problem in a simplified way. They show that the scaling law and convergence of BN are satisfied for arbitrary learning rates. They also show convergence of the learning rate at arbitrary weights. Finally, they provide a theoretical analysis of the acceleration effects of gradient descent. "
6584,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,data noising USED-FOR recurrent neural network language models. theoretical perspective USED-FOR data noising. data noising HYPONYM-OF Bayesian recurrent neural networks. variational distribution FEATURE-OF Bayesian recurrent neural networks. variational framework USED-FOR data noising. variational smoothing CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION variational smoothing. tied input and output embedding matrices CONJUNCTION element - wise variational smoothing method. element - wise variational smoothing method CONJUNCTION tied input and output embedding matrices. tied input and output embedding matrices USED-FOR variational smoothing. benchmark language modeling datasets EVALUATE-FOR data noising methods. Method is mixture of Gaussians. OtherScientificTerm is unigram distribution. Generic is method. ,"This paper proposes a variational framework for data noising in Bayesian recurrent neural networks (BNNs). The proposed method is based on variational smoothing, where the input and output embedding matrices of the BNN are tied. The authors show that the variational distribution of the unigram distribution is a mixture of Gaussians, which can be approximated by variational interpolation.  The authors also propose an element-wise interpolation method, which is a variant of variational variational inference. The experimental results show the effectiveness of the proposed method.  ","This paper proposes a variational framework for data noising in Bayesian recurrent neural network language models. The main contribution of the paper is a theoretical analysis of the variational variational distribution of the unigram distribution of Bayesian RNNs. The authors show that the data-noising problem can be formulated as a mixture of Gaussians, where the input and output embedding matrices are tied. They also propose a new element-wise variational smoothing method and a tied input-output embedding matrix. The experimental results demonstrate the effectiveness of the proposed method. "
6588,SP:f4a914d3df1a5a21a7365ba78279420f39210884,"saliency maps USED-FOR classification. classifier - dependent methods USED-FOR Extracting saliency maps. approach USED-FOR saliency maps. approach COMPARE weakly - supervised localization techniques. weakly - supervised localization techniques COMPARE approach. Method are classifier - agnostic saliency map extraction, and classifier. Material is ImageNet dataset. ","This paper proposes a classifier-agnostic saliency map extraction method for image classification. The proposed method is based on the observation that saliency maps are class-dependent and can be extracted from the input image. To this end, the authors propose to use a class-independent saliency mapping method to extract saliency information from input images. The method is evaluated on the ImageNet dataset and compared with weakly-supervised localization methods.","This paper proposes a novel method to extract saliency maps from the ImageNet dataset. The proposed method is based on the idea of classifier agnostic saliency map extraction, where the classifier does not depend on the classification accuracy of the target class. The authors show that the proposed method outperforms the state-of-the-art weakly-supervised localization methods on ImageNet."
6592,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,deep nets USED-FOR task. net USED-FOR task. net USED-FOR model. deep nets USED-FOR task. knowledge flow USED-FOR deep net model. teachers HYPONYM-OF deep nets. tasks EVALUATE-FOR they. output spaces FEATURE-OF tasks. fine - tuning COMPARE knowledge exchange ’ methods. knowledge exchange ’ methods COMPARE fine - tuning. supervised and reinforcement learning tasks EVALUATE-FOR fine - tuning. approach COMPARE fine - tuning. fine - tuning COMPARE approach. supervised and reinforcement learning tasks EVALUATE-FOR approach. ,"This paper proposes a method for fine-tuning deep neural networks in order to improve the performance of the model on downstream tasks. The method is based on the idea of knowledge flow, where a teacher network is trained to predict the output of a student network, and the student network is used to fine-tune the teacher network. The main idea is to use the knowledge from the student networks to update the teacher networks in the downstream tasks, and then use the updated model to perform the downstream task. The authors show that the proposed method is able to improve performance on both supervised and reinforcement learning tasks.","This paper proposes a knowledge exchange method for fine-tuning a deep net model. The idea is to use the knowledge flow between the teacher and the learner to fine-tune the model in the output space. The teacher and learner are trained in the same way, but the teacher is trained in a different way. The learner is trained on the same set of tasks, and the teacher can be trained on a set of different tasks. The teachers are trained on different tasks with different output spaces, and they are trained using a combination of supervised and reinforcement learning tasks."
6596,SP:a72072879f7c61270d952f06d9ce995e8150632c,"spatial and temporal dimensions FEATURE-OF higher - order inter - dependencies. higher - order inter - dependencies FEATURE-OF data streams. soft - clustering USED-FOR compact dynamical model. dynamics USED-FOR compact dynamical model. predictive accuracy EVALUATE-FOR mathematical representation. stochastic calculus PART-OF information theory inspired approach. predictive ability CONJUNCTION causal interdependence ( relatedness ) constraints. causal interdependence ( relatedness ) constraints CONJUNCTION predictive ability. data streams CONJUNCTION compact model. compact model CONJUNCTION data streams. maximization of the compression of the state variables USED-FOR model construction. convergence FEATURE-OF learning algorithm. iterative scheme USED-FOR model parameters. high - dimensional Gaussian case study EVALUATE-FOR framework. compression and prediction accuracy EVALUATE-FOR dynamical systems. algorithm USED-FOR prediction. reduced dimensions FEATURE-OF prediction. real - world dataset of multimodal sentiment intensity EVALUATE-FOR algorithm. Task are Extracting relevant information, modeling complex systems, and causal inference. Metric is accuracy. Generic is tasks. Material is high - dimensional heterogeneous data streams. ","This paper proposes a method for learning a compact dynamical model of complex systems with high-order inter-dependencies in high-dimensional heterogeneous data streams. The proposed method is based on soft-clustering the data streams into a compact model, which is then used to model the dynamics of the system. The authors show that the proposed method can be used to learn the model parameters in an iterative manner. The method is evaluated on a real-world dataset of multimodal sentiment intensity.   ","This paper proposes a method for learning a compact dynamical model that can be used to model high-dimensional heterogeneous data streams. The main idea is to use a stochastic calculus-inspired approach to model the dynamics of a dynamical system with high-order inter-dependency constraints. The proposed method is based on soft-clustering, which is a popular approach for modeling complex systems. The authors show that the proposed method converges to a state-of-the-art performance on a real-world dataset of multimodal sentiment intensity. "
6600,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"variational autoencoder USED-FOR single neural probabilistic model. stochastic variational Bayes USED-FOR model. feature imputation CONJUNCTION image inpainting problems. image inpainting problems CONJUNCTION feature imputation. synthetic data CONJUNCTION feature imputation. feature imputation CONJUNCTION synthetic data. synthetic data EVALUATE-FOR approach. OtherScientificTerm are observed features, and features. ","This paper proposes a variational autoencoder (VAE) model for a single neural probabilistic model. The proposed VAE model is based on a VAE with variational Bayes. The main idea is to use the variational inference to learn the distribution over the observed features, and then use the VAE to estimate the posterior distribution over observed features.   The proposed model is evaluated on synthetic data and image inpainting problems. The experimental results show that the proposed model outperforms the baselines.","This paper proposes a new approach to model a single neural probabilistic model with a stochastic variational autoencoder. The main idea is to use a variational variational auto-encoder (VAE) to model the features of a neural model. The VAE model is trained using a standard neural network model, and is trained with a standard VAE on synthetic data and image inpainting problems. The proposed approach is evaluated on synthetic and real-world datasets. "
6604,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"architecture design USED-FOR neural network models. tasks EVALUATE-FOR neural network models. intermediate layer activations USED-FOR back - propagation. memory footprint FEATURE-OF models. spatial resolutions FEATURE-OF layers. approximation strategy USED-FOR network. memory footprint FEATURE-OF network. training EVALUATE-FOR approximation strategy. lower - precision approximations USED-FOR activations. approximate activations USED-FOR backward pass. approximation USED-FOR gradients. CIFAR CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR. ImageNet EVALUATE-FOR approach. CIFAR EVALUATE-FOR approach. 32 - bit floating - point activations USED-FOR approach. training and validation performance EVALUATE-FOR approach. OtherScientificTerm are Limited GPU memory, memory, and forward and backward pass. Generic are architecture, and forward pass. Metric are computational expense, and memory usage. ",This paper proposes a method to reduce the computational cost of backpropagation in deep neural networks. The proposed method is based on approximating the gradients of intermediate layers with floating-point approximations. The authors show that the proposed method can reduce the computation cost of the forward and backward pass by using lower-precision approximate activations for the backward pass. The method is evaluated on CIFAR-10 and ImageNet and achieves competitive performance. ,This paper proposes a method to reduce the memory footprint of neural network models by using lower-precision approximations of intermediate layer activations for back-propagation during training. The authors propose a 32-bit floating-point approximation for the forward and backward pass of the network. They show that the proposed method can reduce the computational cost of the forward pass by a factor of two. They also show that their method can be applied to CIFAR and ImageNet datasets. 
6608,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"Face completion HYPONYM-OF task. complexity CONJUNCTION controllable attributes of filled - in fragments. controllable attributes of filled - in fragments CONJUNCTION complexity. end - to - end framework USED-FOR generative adversarial networks ( GANs ). conditional vectors USED-FOR controllable attributes. end - to - end framework USED-FOR system. mean inference time FEATURE-OF images. system USED-FOR structural and appearance variations. mean inference time FEATURE-OF feed - forward pass of computation. 1024× 1024 resolution FEATURE-OF images. feed - forward pass of computation USED-FOR system. approach COMPARE face completion methods. face completion methods COMPARE approach. OtherScientificTerm are high resolution, and frequency components. Generic are model, and network. ",This paper proposes a new method for face completion. The proposed method is based on a generative adversarial network (GAN) with conditional vectors. The conditional vectors are used to construct a set of controllable attributes that can be used to fill in the completed face images. The authors show that the proposed method achieves state-of-the-art performance in terms of mean inference time and computational complexity.,This paper proposes an end-to-end GAN framework for face completion. The main idea is to use conditional vectors to capture the controllable attributes of filled-in fragments. The conditional vectors are then used to construct a GAN-based model. The authors show that the proposed method can achieve competitive performance on the face completion task. 
6612,SP:a300122021e93d695af85e158f2b402d21525bc8,"high - precision accumulators USED-FOR systems. precision requirements FEATURE-OF partial sum accumulations. reduced accumulation precision USED-FOR deep learning training. statistical approach USED-FOR reduced accumulation precision. analysis USED-FOR benchmark networks. ImageNet ResNet 18 CONJUNCTION ImageNet AlexNet. ImageNet AlexNet CONJUNCTION ImageNet ResNet 18. single precision floating - point baseline FEATURE-OF networks. OtherScientificTerm are numerical precision, multiply - accumulate units, accumulation precision, ensemble of partial sums, accumulation, and computation hardware. Metric is quality of convergence. Generic are variance, and equations. Task is areaand power - optimal systems. ",This paper studies the problem of computing partial sum accumulators in deep neural networks. The authors show that the precision of the accumulators can be reduced to single precision. They show that this reduces the variance of the convergence of the system to power-optimal systems. They also show that it is possible to reduce the precision to a single precision and still achieve good performance. ,This paper studies the problem of reducing the precision of partial sum accumulations in the training of deep neural networks. The authors propose a statistical approach to estimate the variance of the convergence of the accumulation precision. They show that the convergence rate of the precision is bounded by the number of units in the ensemble of partial sums. They also provide a theoretical analysis of the effect of the variance on the convergence.
6616,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"gradient flow CONJUNCTION gradient descent. gradient descent CONJUNCTION gradient flow. risk convergence CONJUNCTION asymptotic weight matrix alignment. asymptotic weight matrix alignment CONJUNCTION risk convergence. gradient descent USED-FOR deep linear networks. asymptotic weight matrix alignment HYPONYM-OF implicit regularization. gradient flow USED-FOR deep linear networks. asymptotic weight matrix alignment USED-FOR gradient flow. linearly separable data USED-FOR deep linear networks. gradient flow USED-FOR strictly decreasing loss functions. normalized ith weight matrix COMPARE rank-1 approximation. rank-1 approximation COMPARE normalized ith weight matrix. decreasing step sizes FEATURE-OF gradient descent. linear function COMPARE maximum margin solution. maximum margin solution COMPARE linear function. binary cross entropy FEATURE-OF logistic loss. weight matrices USED-FOR linear function. network USED-FOR linear function. Metric is risk. OtherScientificTerm are rank-1 matrices, and alignment phenomenon. ","This paper studies the asymptotic weight matrix alignment in deep linear networks with linearly separable data. In particular, the authors show that gradient flow converges to a linear function with a rank-1 approximation of the weight matrix with increasing step sizes. The authors also show that the convergence rate of gradient descent converges linearly with decreasing step sizes in the direction of the loss function. The convergence rate is shown to be bounded by the risk of the network. ",This paper studies the asymptotic weight matrix alignment phenomenon in deep linear networks. The authors show that the gradient flow for strictly decreasing loss functions converges to a rank-1 approximation of the normalized ith weight matrix with respect to the weight matrices of the input data. They also show that this phenomenon can be used as an implicit regularization for gradient flow in the case of linear networks with linearly separable data.   
6620,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,dialogue topic CONJUNCTION speaker sentiments. speaker sentiments CONJUNCTION dialogue topic. speaker identity CONJUNCTION dialogue topic. dialogue topic CONJUNCTION speaker identity. hredGAN architecture USED-FOR utterance attributes. speaker sentiments HYPONYM-OF utterance attributes. speaker identity HYPONYM-OF utterance attributes. dialogue topic HYPONYM-OF utterance attributes. persona - based HRED generator ( PHRED ) CONJUNCTION conditional discriminator. conditional discriminator CONJUNCTION persona - based HRED generator ( PHRED ). persona - based HRED generator ( PHRED ) PART-OF phredGAN. conditional discriminator PART-OF phredGAN. persona - based HRED generator ( PHRED ) PART-OF system. conditional discriminator PART-OF system. approaches USED-FOR conditional discriminator. phredGANd HYPONYM-OF dual discriminator system. dual discriminator system CONJUNCTION adversarial discriminator. adversarial discriminator CONJUNCTION dual discriminator system. phredGANa HYPONYM-OF approaches. phredGANd HYPONYM-OF approaches. Ubuntu Dialogue Corpus ( UDC ) CONJUNCTION TV series transcripts. TV series transcripts CONJUNCTION Ubuntu Dialogue Corpus ( UDC ). phredGAN COMPARE persona SeqSeq model. persona SeqSeq model COMPARE phredGAN. Big Bang Theory and Friends FEATURE-OF TV series transcripts. conversational datasets EVALUATE-FOR phredGAN. Ubuntu Dialogue Corpus ( UDC ) HYPONYM-OF conversational datasets. TV series transcripts HYPONYM-OF conversational datasets. quantitative measures CONJUNCTION crowd - sourced human evaluation. crowd - sourced human evaluation CONJUNCTION quantitative measures. datasets CONJUNCTION ones. ones CONJUNCTION datasets. attribute modalities FEATURE-OF customer - agent interactions. Ubuntu dataset FEATURE-OF customer - agent interactions. ones EVALUATE-FOR phredGAN. attribute modalities FEATURE-OF datasets. attribute modalities FEATURE-OF ones. datasets EVALUATE-FOR phredGAN. weak attribute modalities FEATURE-OF datasets. Big Bang Theory HYPONYM-OF weak attribute modalities. Task is multi - turn dialogue scenario. Method is attribute representation. ,This paper proposes a new method for generating utterance attributes for multi-turn dialogues. The proposed method is based on a two-stage approach: (1) a speaker-symbolic generator (PHRED) is used to generate speaker sentiments and (2) a conditional discriminator is used for generating dialogue topic and speaker identity. Experiments show that the proposed method achieves state-of-the-art performance on a variety of datasets.,"This paper proposes a new HredGAN architecture for multi-turn, multi-attention dialogue. The proposed method is based on a persona-based HRED generator (PHRED) and a conditional discriminator (conditional discriminator). The proposed approach is evaluated on a variety of datasets, including Big Bang Theory, TV series transcripts, and a customer-agent interaction dataset. "
6624,SP:017b66d6262427cca551ef50006784498ffc741d,"language CONJUNCTION vision. vision CONJUNCTION language. vision CONJUNCTION action. action CONJUNCTION vision. virtual environment FEATURE-OF action. language PART-OF goal - driven collaborative task. vision PART-OF goal - driven collaborative task. action PART-OF goal - driven collaborative task. movable clip art objects PART-OF virtual world. virtual world USED-FOR game. natural language USED-FOR two - way communication. protocols CONJUNCTION metrics. metrics CONJUNCTION protocols. imitation learning CONJUNCTION goal - driven training. goal - driven training CONJUNCTION imitation learning. nearest - neighbor techniques CONJUNCTION neural network approaches. neural network approaches CONJUNCTION nearest - neighbor techniques. models USED-FOR task. nearest - neighbor techniques PART-OF models. neural network approaches PART-OF models. imitation learning USED-FOR neural network approaches. goal - driven training USED-FOR neural network approaches. game EVALUATE-FOR models. live human agents USED-FOR game. fully automated evaluation EVALUATE-FOR models. Task is Collaborative image - Drawing game. Method is CoDraw. OtherScientificTerm are clip art pieces, and crosstalk condition. Material is CoDraw dataset. Generic is testbed. ","This paper proposes a new collaborative image-drawing game called CoDraw, where the goal is to draw a set of clip art objects from a virtual world and communicate with a human agent in order to solve a goal-conditioned task. The paper introduces a new dataset for the CoDraw game, which is designed to be a testbed for multi-agent multi-task learning and multi-objective multi-goal learning.  The paper proposes to use natural language as a two-way communication protocol between the agent and the human agent, and to use vision as a metric to evaluate the performance of the agent.    The proposed method is evaluated on the CoDream dataset, and compared to a number of state-of-the-art methods. The results show that the proposed method outperforms existing methods in terms of performance on CoDraw. ","This paper proposes a new collaborative image-drawing game, called CoDraw, where the goal is to draw a set of clip art objects from a virtual world and communicate with a group of humans. The game is played in a two-way communication setting, where humans communicate with each other via natural language, and a goal-driven collaborative task is presented. The authors propose a new dataset, which they call the CoDraw dataset, to test the effectiveness of their proposed method. They show that the proposed method outperforms the state-of-the-art in terms of performance on CoDraw. They also show that their method is able to outperform the state of the art on a fully automated evaluation."
6628,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,model spaces USED-FOR machine learning. neural networks USED-FOR potential functions. potential functions USED-FOR undirected models. neural networks USED-FOR Neural random fields ( NRFs ). approach USED-FOR NRFs. gradient information USED-FOR model sampling. inclusive - NRF approach USED-FOR continuous data. inclusive - divergence minimized auxiliary generator USED-FOR approach. inclusive - divergence minimized auxiliary generator USED-FOR NRFs. images HYPONYM-OF continuous data. unsupervised / supervised image generation CONJUNCTION semi - supervised classification. semi - supervised classification CONJUNCTION unsupervised / supervised image generation. inclusive - NRFs USED-FOR unsupervised / supervised image generation. inclusive - NRFs USED-FOR semi - supervised classification. random fields USED-FOR tasks. inclusive - NRFs USED-FOR random fields. CIFAR-10 EVALUATE-FOR sample generation quality. unsupervised and supervised settings EVALUATE-FOR inclusiveNRFs. sample generation quality EVALUATE-FOR inclusiveNRFs. CIFAR-10 EVALUATE-FOR inclusiveNRFs. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. MNIST CONJUNCTION SVHN. SVHN CONJUNCTION MNIST. Semi - supervised inclusive - NRFs COMPARE generative model based semi - supervised learning methods. generative model based semi - supervised learning methods COMPARE Semi - supervised inclusive - NRFs. classification EVALUATE-FOR generative model based semi - supervised learning methods. generation EVALUATE-FOR Semi - supervised inclusive - NRFs. classification EVALUATE-FOR Semi - supervised inclusive - NRFs. ,This paper proposes a novel approach for training neural random fields (NRF) models in the presence of continuous data. The proposed approach is based on the idea of using an auxiliary generator to estimate the gradient of the model parameters. The authors show that the proposed approach can be used for both unsupervised and supervised image generation and semi-supervised classification tasks. The experimental results on CIFAR-10 and SVHN demonstrate the effectiveness of the proposed method.,"This paper proposes a new approach for training neural random fields (NRF) models in the model space. The proposed approach is based on the idea of inclusive-divergence-minimized auxiliary generator (IDAG), which is an auxiliary generator that can be used to generate samples from the training data. The authors show that the proposed approach can be applied to both unsupervised and supervised settings. They also show that it can improve the sample generation quality on CIFAR-10 and SVHN."
6632,SP:0841febf2e95da495b41e12ded491ba5e9633538,"Deep learning models USED-FOR graphs. Deep learning models USED-FOR tasks. graph neural networks USED-FOR node classification. training time attacks FEATURE-OF graph neural networks. meta - gradients USED-FOR bilevel problem. meta - gradients USED-FOR training - time attacks. small graph perturbations USED-FOR graph convolutional networks. they COMPARE baseline. baseline COMPARE they. relational information USED-FOR baseline. algorithm USED-FOR perturbations. Metric is robustness. OtherScientificTerm are discrete graph structure, and graph. Task is unsupervised embeddings. Generic is attacks. Method is classifiers. ","This paper studies the robustness of graph convolutional neural networks (GNNs) in the presence of small perturbations to the graph structure. The authors propose to use meta-gradients to attack GNNs in a bilevel setting, where each node is represented by a set of embeddings, and the goal is to perturb the embedding of each node in such a way that it is similar to the original graph. The proposed method is based on the meta-gradient method, which is an extension of meta-learning, and is shown to be robust to small graph perturbation.   ","This paper studies the problem of training-time attacks on graph neural networks (GNNs). The main idea is to use meta-gradients to improve the robustness of GNNs against small graph perturbations (e.g., node classification) at training time. The main contribution of the paper is to propose a new method to attack the GNN. The method is based on the meta-gradient method, which can be applied to the bilevel problem. The authors show that the proposed method can be used to improve robustness against small perturbation attacks. "
6636,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"Wasserstein Autoencoder ( WAE ) HYPONYM-OF generative models. Sliced - Wasserstein Autoencoders ( SWAE ) CONJUNCTION WAE - MMD ( WAE. WAE - MMD ( WAE CONJUNCTION Sliced - Wasserstein Autoencoders ( SWAE ). Cramer - Wold AutoEncoder ( CWAE ) HYPONYM-OF generative model. maximum mean discrepancy based distance function USED-FOR WAE - MMD ( WAE. CramerWold kernel HYPONYM-OF characteristic kernel. characteristic kernel USED-FOR CWAE cost function. CWAE COMPARE WAE - MMD. WAE - MMD COMPARE CWAE. OtherScientificTerm are normal prior, and distance function. Method are optimization procedure, and SWAE. ","This paper proposes a new generative model based on sliced-Wasserstein autoencoders (SWAE) and WAE-MMD (WAE-MMD). The proposed method is based on the Cramer-Wold AutoEncoder (CWAE), which is an extension of the Wasserstein Autoencoder (WAE) model. The main contribution of the paper is the development of a novel CramerWold kernel for the CWAE cost function, which is a kernel that is independent of the normal prior.   ","This paper proposes a new generative model called Cramer-wold auto-encoder (CWAE) for Wasserstein autoencoder (WAE) and Sliced-Wasserstein Autoencoders (SWAE) models. CWAE is based on the CramerWold kernel, which is the characteristic kernel of the WAE-MMD model. The authors show that CWAE can achieve better performance than SWAE and WAE with the same number of training samples. "
6640,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"It USED-FOR many - class ” problem. It USED-FOR class hierarchy. coarse - class label HYPONYM-OF class hierarchy. convolutional neural network ( CNN ) USED-FOR features. memory - augmented attention module CONJUNCTION multi - layer perceptron ( MLP ). multi - layer perceptron ( MLP ) CONJUNCTION memory - augmented attention module. multi - layer perceptron ( MLP ) USED-FOR MahiNet. memory - augmented attention module PART-OF MahiNet. convolutional neural network ( CNN ) USED-FOR MahiNet. attention module CONJUNCTION KNN classifier. KNN classifier CONJUNCTION attention module. linear classifier USED-FOR MLP. training strategies USED-FOR supervised learning. MahiNet USED-FOR supervised learning. training strategies USED-FOR MahiNet. mcfsOmniglot ” ( re - splitted Omniglot ) USED-FOR MCFS problem. benchmark datasets USED-FOR MCFS problem. mcfsOmniglot ” ( re - splitted Omniglot ) HYPONYM-OF benchmark datasets. MahiNet COMPARE models. models COMPARE MahiNet. MCFS classification tasks EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR models. supervised learning and meta - learning scenarios EVALUATE-FOR MahiNet. MCFS classification tasks EVALUATE-FOR MahiNet. Task are MCFS, MCFS learning, and few - shot ” problem. OtherScientificTerm is fine class. Material is ImageNet. ","This paper studies the problem of many-class classification in the few-shot setting. The authors propose a novel architecture for this problem, which they call MahiNet. The architecture consists of a memory-augmented attention module, a multi-layer perceptron, and a linear classifier. The proposed architecture is evaluated on a variety of image classification tasks.   ","This paper proposes a novel approach to the many-class classification problem for the few-shot classification problem. The main idea is to use a memory-augmented attention module and a multi-layer perceptron (MLP) for the task. The attention module is augmented by a KNN classifier, and the MLP is a linear classifier. The proposed approach is evaluated on several benchmark datasets and meta-learning scenarios."
6644,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,Recurrent neural networks ( RNNs ) USED-FOR natural language. neural speed reading HYPONYM-OF inference. Structural - Jump - LSTM HYPONYM-OF neural speed reading model. LSTM CONJUNCTION agents. agents CONJUNCTION LSTM. one CONJUNCTION one. one CONJUNCTION one. one HYPONYM-OF agents. one HYPONYM-OF agents. one PART-OF model. one PART-OF model. LSTM PART-OF model. agents PART-OF model. model COMPARE neural reading models. neural reading models COMPARE model. accuracy EVALUATE-FOR vanilla LSTM. Structural - Jump - LSTM COMPARE vanilla LSTM. vanilla LSTM COMPARE Structural - Jump - LSTM. model COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE model. neural reading models COMPARE Structural - Jump - LSTM. Structural - Jump - LSTM COMPARE neural reading models. accuracy EVALUATE-FOR Structural - Jump - LSTM. floating point operations ( FLOP ) reduction EVALUATE-FOR Structural - Jump - LSTM. Method is RNNs. Metric is inference time. OtherScientificTerm is end of text markers. ,"This paper proposes a novel neural speed reading model, called Structural-Jump-LSTM, which combines LSTM and agents to improve the inference speed of neural speed-reading models. The proposed model is based on the idea of using end-of-text markers as end-points in the training process. The authors show that the proposed model can reduce the number of FLOPs in the inference process by a significant amount. ","This paper proposes a novel neural speed reading model, Structural-jump-LSTM, which is based on the idea of end-of-text markers. The authors show that the model is able to reduce the number of floating point operations (FLOPs) by a factor of 1.5x compared to vanilla LSTM. They also show that their model can improve the accuracy of neural reading models by 1.7x."
6648,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"Mahalanobis distance function USED-FOR convolutional features. Mahalanobis distance function USED-FOR tight clusters. Mahalanobis distance function USED-FOR nonlinear radial basis convolutional feature transformation. MNIST CONJUNCTION ISBI ISIC skin lesion. ISBI ISIC skin lesion CONJUNCTION MNIST. ISBI ISIC skin lesion CONJUNCTION NIH ChestX - ray14. NIH ChestX - ray14 CONJUNCTION ISBI ISIC skin lesion. NIH ChestX - ray14 HYPONYM-OF image classification and segmentation data - sets. MNIST HYPONYM-OF image classification and segmentation data - sets. ISBI ISIC skin lesion HYPONYM-OF image classification and segmentation data - sets. image classification and segmentation data - sets EVALUATE-FOR method. robustness EVALUATE-FOR method. it COMPARE non - gradient masking defense strategies. non - gradient masking defense strategies COMPARE it. method USED-FOR deep convolutional neural networks. deep convolutional neural networks USED-FOR adversarial perturbations. Method is deep convolutional models. Generic is them. OtherScientificTerm are carefully crafted adversarial perturbations, clusters, small adversarial perturbations, and decision boundary. Material is clean data. ",This paper proposes to use Mahalanobis distance function to map convolutional features into tight clusters to improve robustness against small adversarial perturbations. The proposed method is based on nonlinear basis convolution with a nonlinear radial basis feature transformation. Experiments on image classification and segmentation data-sets show that the proposed method outperforms non-gradient masking defense strategies. ,"This paper proposes a novel method for robustness against small adversarial perturbations in deep convolutional neural networks. The key idea is to use the Mahalanobis distance function to map the features of the input image to a set of tight clusters, and then use a non-linear basis convolution to transform the feature transformation. The proposed method is tested on MNIST, ISBI-ISIC, and NIH ChestX-ray14 datasets, and it outperforms the state-of-the-art in terms of robustness. "
6652,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"exploratory behaviors USED-FOR local optima. exploratory behaviors USED-FOR Reinforcement learning agents. immediate dithering perturbation CONJUNCTION temporally consistent exploration. temporally consistent exploration CONJUNCTION immediate dithering perturbation. immediate dithering perturbation PART-OF behaviors. temporally consistent exploration PART-OF behaviors. sparse rewards CONJUNCTION long term information. long term information CONJUNCTION sparse rewards. stochastic policy model USED-FOR tasks. sparse rewards FEATURE-OF tasks. long term information FEATURE-OF tasks. global random variable USED-FOR conditional distribution. dropout USED-FOR reinforcement learning policies. inherent temporal consistency FEATURE-OF them. factors USED-FOR NADPEx policy. gradients ’ alignment HYPONYM-OF factors. naive exploration CONJUNCTION parameter noise. parameter noise CONJUNCTION naive exploration. NADPEx USED-FOR tasks. sparse reward USED-FOR NADPEx. naive exploration USED-FOR NADPEx. parameter noise USED-FOR NADPEx. sparse reward USED-FOR tasks. mujoco benchmark USED-FOR continuous control. mujoco benchmark EVALUATE-FOR It. Method are on - policy temporally consistent exploration strategy, and deep reinforcement learning agents. OtherScientificTerm are reward signals, and policy space. ","This paper proposes a novel exploration strategy called NADPEx, which is an on-policy exploration strategy that is temporally consistent across tasks. The exploration strategy is based on a stochastic policy model with a global random variable that is used to model the conditional distribution of the reward signal. The authors show that the proposed method is able to achieve state-of-the-art performance on the Mujoco continuous control task. ","This paper proposes a new policy called NADPEx, which is based on a stochastic policy model with a global random variable and a conditional conditional distribution. The authors show that the proposed policy can be applied to a variety of tasks with sparse rewards and long-term information. They also show that it can be used to improve the performance of the agent on the mujoco benchmark."
6656,SP:304930c105cf036ab48e9653926a5f61879dfea6,"gradient - based metric USED-FOR network. nonlinearity coefficient ( NLC ) HYPONYM-OF metric. NLC USED-FOR test error. NLC USED-FOR architecture search and design. Method are neural architectures, expert hand - tuning, and fully - connected feedforward networks. Task is architecture design. OtherScientificTerm is exploding or vanishing gradients. Generic are guideline, and it. ","This paper proposes a new metric for evaluating the quality of neural networks. The proposed metric is based on the nonlinearity coefficient (NLC), which is a gradient-based metric. The authors show that the NLC can be used to estimate the test error of a neural network. The NLC is then used as a guideline for architecture search and design.   ",This paper proposes a new nonlinearity coefficient (NLC) metric for neural networks. The NLC is based on the idea of exploding gradients and vanishing gradients. The authors show that the NLC can be used to estimate the test error of a fully connected feedforward neural network. They also show that it can also be used as a guideline for architecture search and design.
6660,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"bias amplification FEATURE-OF classifiers. inductive bias in gradient descent methods USED-FOR bias amplification. feature - wise bias amplification HYPONYM-OF bias. features FEATURE-OF model. targeted feature selection USED-FOR feature - wise bias amplification. feature selection algorithms USED-FOR bias amplification. they USED-FOR convolutional neural networks. feature selection algorithms USED-FOR linear models. bias amplification PART-OF linear models. algorithms USED-FOR reduced bias. synthetic and real data EVALUATE-FOR algorithms. Method is machine learning model. OtherScientificTerm are moderately - predictive “ weak ” features, and predictive bias. Material is insufficient training data. Metric is accuracy. ","This paper studies feature-wise bias amplification (BBIG) in deep neural networks. The authors show that bias amplification occurs when the features of the model are selected in such a way that they are biased towards features that are moderately-predictive (i.e., those that are close to the true features in the training set). They show that this bias amplification can be reduced by using targeted feature selection to select the features with the least bias. The proposed method is shown to be effective for linear models and convolutional networks.  ","This paper studies the problem of bias amplification in classifiers. The authors propose a new bias amplification method, called feature-wise bias amplification, which aims to reduce the bias of a classifier by selecting features that are moderately-predictive (i.e., weak features) in the training data. The proposed method is based on the idea of feature selection, which is an extension of the classic bias amplification. The main contribution of the paper is that the proposed method can be applied to a class of linear models, where the bias amplification can be reduced by using a feature selection algorithm. The method is evaluated on synthetic and real data."
6664,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"over - parametrization USED-FOR generalization. over - parametrization USED-FOR neural networks. neural networks USED-FOR generalization. normalized margin CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION normalized margin. global minimizer FEATURE-OF weakly - regularized cross - entropy loss. generalization error bounds FEATURE-OF deep networks. maximum normalized margin FEATURE-OF global minimizer. two - layer networks FEATURE-OF infinite - width neural network. generalization guarantees EVALUATE-FOR infinite - width neural network. neural net margin COMPARE kernel methods. kernel methods COMPARE neural net margin. kernel methods HYPONYM-OF infinite feature methods. generalization guarantees EVALUATE-FOR kernel methods. infinite - neuron viewpoint USED-FOR analyzing optimization. perturbed gradient flow USED-FOR global optimizer. perturbed gradient flow USED-FOR infinite - size networks. polynomial time FEATURE-OF global optimizer. Method are margin - based perspective, and multi - layer feedforward relu networks. Material is natural instances. ",This paper studies the generalization performance of deep neural networks from a margin-based perspective. The authors show that the maximum normalized margin is the global minimizer of the weakly-regularized cross-entropy loss. The generalization error bounds of deep networks with two-layer networks and infinite-width neural networks are shown to be bounded by the normalized margin.  The authors also show that perturbed gradient flow can be used to find the global optimizer in polynomial time for infinite-size networks.  ,"This paper studies the generalization performance of neural networks with over-parameterized generalization error bounds. The authors provide generalization bounds for two-layer neural networks and a multi-layer feedforward relu network. They show that the maximum normalized margin of the global minimizer of a weakly-regularized cross-entropy loss can be used to improve generalization of the neural networks. They also provide a generalization bound for infinite-width neural networks, showing that it is polynomial in the size of the network. "
6668,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"image captions HYPONYM-OF natural language descriptions. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. object pathway USED-FOR generator. object pathway USED-FOR discriminator. semantic layout USED-FOR approach. image background CONJUNCTION image layout. image layout CONJUNCTION image background. global pathway USED-FOR image background. global pathway USED-FOR image layout. Multi - MNIST CONJUNCTION CLEVR. CLEVR CONJUNCTION Multi - MNIST. CLEVR CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION CLEVR. Multi - MNIST CONJUNCTION MSCOCO data set. MSCOCO data set CONJUNCTION Multi - MNIST. global image characteristics CONJUNCTION image background. image background CONJUNCTION global image characteristics. object pathway USED-FOR features. global pathway USED-FOR image background. global pathway USED-FOR global image characteristics. object pathway COMPARE global pathway. global pathway COMPARE object pathway. Method is Generative Adversarial Networks ( GANs ). OtherScientificTerm are bounding boxes, and complex scenes. ","This paper proposes an object pathway for image captions. The object pathway consists of an image generator and an image discriminator. The image generator is trained to generate captions for a given image, and the discriminator is trained on the generated captions, which are then used as input to the object pathway. The proposed method is evaluated on three image datasets: Multi-MNIST, CLEVR, and MSCOCO. ","This paper proposes a novel method for generating captions for image captions. The proposed method is based on the object pathway. The object pathway is a global pathway that maps the image background into a global set of features. The global pathway is then used as a discriminator for the generator. The discriminator is trained using a GAN-based adversarial network. The method is evaluated on three datasets: Multi-MNIST, CLEVR and MSCOCO."
6672,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,models USED-FOR policy improvement. learning models USED-FOR global model methods. local model methods USED-FOR system dynamics. representations USED-FOR simple dynamics. local models USED-FOR policy learning in complex systems. real Sawyer robotic arm USED-FOR manipulation task. manipulation task HYPONYM-OF robotics tasks. robotics tasks EVALUATE-FOR approach. camera images USED-FOR manipulation task. OtherScientificTerm is local improvements. ,"This paper proposes a method to learn a local model of the dynamics of a system, which can then be used to improve the performance of policy learning in complex systems. The method is based on the idea that learning a local representation of the system dynamics can be used for policy improvement. The proposed method is evaluated on a variety of robotic manipulation tasks, where it is shown to outperform the state-of-the-art methods.  ","This paper proposes a new method for learning a local model for policy learning in complex systems. The proposed method is based on the idea that local models can be used to improve the performance of a global model by learning a representation of the dynamics of the system. The method is evaluated on a variety of robotic manipulation tasks, and it is shown that the proposed method outperforms the state of the art."
6676,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"reinforcement learning algorithms USED-FOR real experience. models USED-FOR Learning policies. POMDPs FEATURE-OF learning policies. off - policy experience USED-FOR learning policies. structural causal models USED-FOR counterfactual evaluation of arbitrary policies. It USED-FOR counterfactual evaluation of arbitrary policies. structural causal models USED-FOR It. CF - GPS COMPARE vanilla model - based RL algorithms. vanilla model - based RL algorithms COMPARE CF - GPS. logged data USED-FOR CF - GPS. off - policy algorithms COMPARE CF - GPS. CF - GPS COMPARE off - policy algorithms. model USED-FOR CF - GPS. experience data USED-FOR algorithm. Importance Sampling USED-FOR off - policy algorithms. CF - GPS USED-FOR Guided Policy Search. counterfactual methods USED-FOR reparameterization - based algorithms. Stochastic Value Gradient HYPONYM-OF reparameterization - based algorithms. Task are simulating plausible experience de novo, modelbased policy evaluation and search, and de novo synthesis of data. OtherScientificTerm are logged, real experience, counterfactual actions, and off - policy episodes. ","This paper proposes a method for counterfactual evaluation of off-policy experience in reinforcement learning. The proposed method is based on a model-based approach, which uses a causal model to model the POMDPs of arbitrary policies. The authors show that the proposed method can be used to improve the performance of existing methods for policy evaluation and search. The method is evaluated on simulated and real-world data.","This paper proposes a novel approach to counterfactual evaluation of off-policy experience in reinforcement learning. The authors propose a new approach called CF-GPS, which is based on a structural causal model that is used to model the counterfactually evaluation of arbitrary policies. The proposed approach is evaluated on a set of real-world datasets, and compared with a number of baselines.   "
6680,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"flat local minima of loss surface USED-FOR generalization. parameter space FEATURE-OF flat local minima of loss surface. parameter space FEATURE-OF loss surface. generalization EVALUATE-FOR loss surface. parameter space CONJUNCTION input space. input space CONJUNCTION parameter space. input space FEATURE-OF decision surfaces. parameter space FEATURE-OF decision surfaces. input space FEATURE-OF decision surface. adversarial robustness indicator USED-FOR neural network ’s intrinsic robustness property. method USED-FOR network ’s intrinsic adversarial robustness. Method are neural network generalization, robust training method, and adversarial training. OtherScientificTerm are adversarial settings, and adversarial attacks. Metric is adversarial robustness. ","This paper studies the effect of adversarial training on the generalization ability of neural networks. The authors show that the flat local minima of the loss surface of the network can be used as an indicator of the robustness to adversarial attacks. They show that adversarial robustness is a measure of the intrinsic robustness of a neural network to attacks, and that it is a function of the decision surfaces of the input space and the parameter space. They then propose a method to estimate the decision surface in parameter space and input space, and then use this information to compute the adversarial perturbations to the network parameters.  ",This paper studies the generalization properties of neural networks in the presence of adversarial attacks. The authors show that the flat local minima of the loss surface of a neural network can be used as an indicator of the robustness of the network to adversarial training. They also provide a method to measure the intrinsic adversarial robustness.  
6684,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"autonomous drones CONJUNCTION wearable devices. wearable devices CONJUNCTION autonomous drones. energy consumption PART-OF DNN training. weighted sparse projection CONJUNCTION input masking. input masking CONJUNCTION weighted sparse projection. end - to - end DNN training framework USED-FOR quantitative energy consumption guarantees. weighted sparse projection USED-FOR end - to - end DNN training framework. input masking USED-FOR end - to - end DNN training framework. weighted sparse projection USED-FOR quantitative energy consumption guarantees. input masking USED-FOR quantitative energy consumption guarantees. optimization problem USED-FOR DNN training. DNN training process USED-FOR constrained optimization. quantitative DNN energy estimation PART-OF DNN training process. approximate algorithm USED-FOR optimization problem. framework USED-FOR DNNs. prior energy - saving methods COMPARE framework. framework COMPARE prior energy - saving methods. Method is Deep Neural Networks ( DNNs ). OtherScientificTerm are energy budget, optimization constraint, and energy budgets. ",This paper proposes a method to reduce the energy consumption of deep neural networks (DNNs) during training. The proposed method is based on weighted sparse projection and input masking. Theoretical guarantees are provided for the proposed method. Empirical results are provided to demonstrate the effectiveness of the method. ,This paper proposes an end-to-end DNN training framework to reduce the energy consumption of deep neural networks (DNNs). The authors propose a weighted sparse projection (WSP) method to estimate the energy budget of a DNN during training. They also propose an input masking method to improve the energy efficiency of the training process. The authors show that the proposed method outperforms the state-of-the-art in terms of energy consumption. 
6688,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"Addressing uncertainty USED-FOR autonomous systems. latent model parameters FEATURE-OF posterior distribution. belief distribution USED-FOR expected long - term reward. universal policy USED-FOR Bayesian value function. policy optimization algorithms USED-FOR universal policy. universal policy USED-FOR exploration - exploitation trade - off. policy optimization algorithms USED-FOR Bayesian Policy Optimization. policy optimization algorithms USED-FOR algorithm. policy network architecture USED-FOR belief distribution. observable state FEATURE-OF belief distribution. method COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE method. method COMPARE algorithms. algorithms COMPARE method. algorithms USED-FOR model uncertainty. algorithms COMPARE Partially Observable Markov Decision Process solvers. Partially Observable Markov Decision Process solvers COMPARE algorithms. method USED-FOR model uncertainty. OtherScientificTerm are continuous latent parameter space, and belief distributions. ","This paper proposes a Bayesian policy optimization algorithm for Bayesian MDPs. The main idea is to learn a belief distribution for the expected long-term reward using a policy network architecture. This belief distribution is then used to estimate the Bayesian value function, which can be used as a universal policy for the exploration-exploitation trade-off. The proposed method is shown to outperform existing methods in terms of model uncertainty.  ",This paper proposes a new method for addressing uncertainty in the Bayesian policy optimization problem. The main idea is to learn a universal policy that maximizes the expected long-term reward of the posterior distribution of the belief distribution over the observed state of the system. This is achieved by using a policy network architecture that learns a belief distribution based on the observable state. The authors show that their method outperforms the state-of-the-art partially-observable Markov decision process solvers. 
6692,SP:3823faee83bc07a989934af5495dafd003c27921,"unified framework USED-FOR unsupervised representations of entities. optimal transport USED-FOR representations. method USED-FOR uncertainty. sentence similarity CONJUNCTION word entailment detection. word entailment detection CONJUNCTION sentence similarity. unsupervised representations USED-FOR text. tasks EVALUATE-FOR it. word entailment detection HYPONYM-OF tasks. sentence similarity HYPONYM-OF tasks. approach USED-FOR unsupervised or supervised problem. co - occurrence structure FEATURE-OF unsupervised or supervised problem. sequence data HYPONYM-OF co - occurrence structure. Wasserstein distances CONJUNCTION Wasserstein barycenters. Wasserstein barycenters CONJUNCTION Wasserstein distances. Wasserstein distances USED-FOR framework. OtherScientificTerm are histogram ( or distribution ), entities, distributions, and optimal transport map. Method is rich and powerful feature representations. ",This paper proposes a unified framework for learning unsupervised representations of entities in text using optimal transport. The proposed method is based on the Wasserstein distance between entities and their distributions. The authors show that the optimal transport map can be used to estimate the uncertainty of the representation. The method is evaluated on two tasks: sentence similarity and word entailment detection.   ,"This paper proposes a unified framework for unsupervised representation learning of entities in text. The proposed framework is based on the Wasserstein distance between entities and their distributions. The authors show that the proposed framework can be applied to a variety of tasks, such as word entailment detection, sentence similarity, and sentence similarity. They also show that it can be used to learn representations that are rich and powerful. "
6696,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,model - based reinforcement learning methods COMPARE model - free approaches. model - free approaches COMPARE model - based reinforcement learning methods. model - based reinforcement learning methods USED-FOR complex simulated environments. dynamics model CONJUNCTION planner. planner CONJUNCTION dynamics model. recursions USED-FOR long - range planning. recursions USED-FOR state estimates. model accuracy CONJUNCTION performance. performance CONJUNCTION model accuracy. task reward EVALUATE-FOR model - free approaches. Material is MuJoCo environments. OtherScientificTerm is long planning horizons. ,"This paper proposes a model-free reinforcement learning method for long-range planning in MuJoCo environments. The proposed method is based on a combination of a dynamics model and a planner. The dynamics model is used to model the dynamics of the environment, and the planner is trained to predict the state of the dynamics model. The planner is then used to estimate the state using recursion. The authors show that the proposed method outperforms the state-of-the-art model-based RL methods in the task of long-term planning.","This paper proposes a model-free reinforcement learning approach for long-range planning in MuJoCo environments. The approach is based on recursion-based planning, where a planner and a dynamics model are trained together to estimate the state of the environment. The model is trained using a combination of the dynamics model and a planner, and the planner is trained to predict the long-term planning horizon. Experiments show that the proposed approach outperforms the state-of-the-art in terms of model accuracy, performance, and reward."
6700,SP:da14205470819495a3aad69d64de4033749d4d3e,2or 3 - bit precision HYPONYM-OF ultra - low precision. precision highway USED-FOR ultralow - precision computation. precision highway USED-FOR convolutional and recurrent neural networks. precision highway USED-FOR accumulated quantization error. accumulated quantization error FEATURE-OF convolutional and recurrent neural networks. hardware accelerator EVALUATE-FOR overhead. method COMPARE quantization methods. quantization methods COMPARE method. 3 - bit weight / activation quantization CONJUNCTION 2 - bit quantization. 2 - bit quantization CONJUNCTION 3 - bit weight / activation quantization. top-1 accuracy loss EVALUATE-FOR ResNet-50. 3 - bit weight / activation quantization EVALUATE-FOR method. ResNet-50 EVALUATE-FOR 2 - bit quantization. top-1 accuracy loss EVALUATE-FOR 2 - bit quantization. accuracy loss EVALUATE-FOR method. top-1 accuracy loss EVALUATE-FOR method. method COMPARE method. method COMPARE method. LSTM USED-FOR language modeling. 2 - bit quantization FEATURE-OF LSTM. 2 - bit quantization USED-FOR method. 2 - bit quantization FEATURE-OF method. Method is Neural network quantization. ,"This paper proposes a method to reduce the computational cost of quantization in neural networks. The proposed method is based on a precision highway approach to compute the accumulated quantization error of convolutional and recurrent neural networks, which can be computed in a low-precision manner. The method is evaluated on ResNet-50 and ResNets trained with ResNet50, and achieves the state-of-the-art accuracy loss.   ","This paper proposes a new method for quantization of neural networks. The main idea is to use a precision highway to compute the quantization error of a neural network. The proposed method is based on the idea of precision highway, which is a method for computing the precision highway of a network quantization. The method is evaluated on ResNet-50 and ResNets-50, where it outperforms the state-of-the-art in terms of accuracy."
6704,SP:0355b54430b39b52df94014d78289dd6e1e81795,"method USED-FOR image restoration problems. deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION deblurring. denoising CONJUNCTION deblurring. deblurring CONJUNCTION denoising. denoising HYPONYM-OF image restoration problems. super - resolution HYPONYM-OF image restoration problems. deblurring HYPONYM-OF image restoration problems. constrained optimization problem USED-FOR problem. Generative Adversarial Network ( GAN ) USED-FOR density estimation model. OtherScientificTerm are posteriori probability of latent variables, latent variables, and degraded image. Material is MNIST dataset. ",This paper proposes a method for image restoration based on a generative adversarial network (GAN) to estimate the posteriori probability of latent variables in a deblurring and super-resolution image. The proposed method is based on the observation that the posterior probability of the latent variables depends on the number of deblurred images and the super-resolutions. The authors propose to use a GAN to model the density estimation model for the estimation of the posterior probabilities of the image denoising process.   The authors show that the proposed method can recover the original image from a corrupted version of the corrupted image. ,This paper proposes a GAN-based method for image restoration. The main idea is to use a generative adversarial network (GAN) to estimate the posterior probability of the latent variables of a degraded image. The posterior probability is estimated using a constrained optimization problem. The authors show that the proposed method outperforms the state-of-the-art methods on the MNIST dataset. 
6708,SP:2feef921a0563d52fde1c074da754f73e6cabef8,"softmax outputs CONJUNCTION feature responses. feature responses CONJUNCTION softmax outputs. large "" teacher "" network CONJUNCTION compact "" student "" network. compact "" student "" network CONJUNCTION large "" teacher "" network. full training data USED-FOR knowledge distillation methods. method USED-FOR knowledge distillation. conv - layer PART-OF student - net. layer PART-OF conv - layer. layer PART-OF conv - layer. computation cost FEATURE-OF conv - layer. teacher - net CONJUNCTION student - net constructing. student - net constructing CONJUNCTION teacher - net. OtherScientificTerm are human cognition, feature map sizes, and block - level outputs. ","This paper proposes a novel method for knowledge distillation in deep neural networks. The proposed method is based on the observation that a large teacher network and a small student network can be trained with the same amount of training data, but with different feature map sizes. The authors propose to use a convolutional layer as the intermediate layer between the teacher and student networks, and then use this intermediate layer to distill the knowledge from the teacher network to the student network. The method is evaluated on a variety of image classification tasks.   ","This paper proposes a novel knowledge distillation method for large teacher-student networks. The main idea is to use a student-net to distill the teacher-net from the full training data. The student network consists of two layers: a conv-layer and a block-level layer. The block-layer consists of a student layer and a teacher-layer, and the student layer consists of the teacher layer and the teacher network. The teacher layer is used to construct the student network, while the student-layer is used for distillation. The proposed method is evaluated on a variety of datasets, and compared with a number of baselines. "
6712,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,task relatedness USED-FOR transferability. transferred representations USED-FOR classification problems. H - score HYPONYM-OF evaluation function. evaluation function EVALUATE-FOR transferred representations. asymptotic error probability FEATURE-OF decision function. information theoretic approach USED-FOR H - score. asymptotic error probability FEATURE-OF H - score. transferred feature USED-FOR asymptotic error probability. transferred feature USED-FOR H - score. source tasks USED-FOR task transfer learning problems. transferability USED-FOR source tasks. it USED-FOR inference problems. recognition tasks USED-FOR 3D indoor - scene understanding. classification HYPONYM-OF inference problems. recognition tasks HYPONYM-OF inference problems. Task is task transfer learning. Metric is task transferability. OtherScientificTerm is representations. Generic is metric. Method is transfer learning policies. Material is synthetic and real image data. ,"This paper studies the problem of task transfer learning and proposes a new metric called H-score to measure the transferability between source and target tasks. The proposed metric is based on the notion of transferability, which is a measure of task relatedness. The authors show that the transferred representations of source tasks are more transferable than those of target tasks in terms of the asymptotic error probability of the decision function of the source task. They then propose a new evaluation function for transferability based on H-scores, which they use to evaluate the performance of transfer learning policies on source tasks. They show that transferability can be used to improve the performance on target tasks, and they show that it can improve performance on both synthetic and real image data.","This paper studies the problem of task transfer learning, where the goal is to learn representations that are transferable between source and target tasks. The authors propose a new metric, H-score, to measure the transferability of the source representations to the target representations. They show that the transferred representations are more transferable to source tasks than the target ones. They also show that transferability is related to the asymptotic error probability of the decision function, and propose an information theoretic approach to estimate the H- score. "
6716,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"Planning USED-FOR complex languages. planning phase USED-FOR global sentence structure. planning phase PART-OF neural machine translation. discrete structural representations USED-FOR syntactic information. approach USED-FOR discrete structural representations. beam search USED-FOR structural codes. discrete codes USED-FOR word generation. codes USED-FOR pure structural variations. codes USED-FOR translation. structural planning USED-FOR global sentence structure. Method is language generation models. Metric are structural diversity metric, and diversity scores. Material is sampled paraphrase translations. "," is a method for learning to predict the structure of a sentence in a language. The method is based on the idea of planning, i.e. learning a set of discrete structural representations for the syntactic information. The authors propose to use beam search to learn these discrete representations, which are then used for word generation. Experiments show that the proposed method is able to generate sentences with diverse structural structures.","This paper proposes a new structural diversity metric to measure the structural diversity of a language. The proposed metric is based on a beam search for structural codes that can be used to measure structural diversity in a language generation model. The authors show that the proposed metric can be applied to the planning phase of neural machine translation, which is a key component of the language generation process. They show that this metric is useful in the context of language generation. They also show that it can be useful to use structural diversity to improve the performance of the model."
6720,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"discriminator PART-OF generative adversarial network ( SGAN ). SGAN COMPARE integral probability metric ( IPM ) GANs. integral probability metric ( IPM ) GANs COMPARE SGAN. real data COMPARE randomly sampled fake data. randomly sampled fake data COMPARE real data. real data COMPARE fake data. fake data COMPARE real data. approaches USED-FOR GAN loss functions. Relativistic GANs ( RGANs ) CONJUNCTION Relativistic average GANs ( RaGANs ). Relativistic average GANs ( RaGANs ) CONJUNCTION Relativistic GANs ( RGANs ). Relativistic GANs ( RGANs ) HYPONYM-OF them. Relativistic average GANs ( RaGANs ) HYPONYM-OF them. IPM - based GANs HYPONYM-OF RGANs. identity function USED-FOR RGANs. identity function USED-FOR IPM - based GANs. RaGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RaGANs. RGANs COMPARE non - relativistic counterparts. non - relativistic counterparts COMPARE RGANs. images COMPARE ones. ones COMPARE images. WGAN - GP CONJUNCTION SGAN. SGAN CONJUNCTION WGAN - GP. RGANs CONJUNCTION RaGANs. RaGANs CONJUNCTION RGANs. GAN CONJUNCTION LSGAN. LSGAN CONJUNCTION GAN. RaGAN COMPARE WGAN - GP. WGAN - GP COMPARE RaGAN. WGAN - GP CONJUNCTION spectral normalization. spectral normalization CONJUNCTION WGAN - GP. discriminator update CONJUNCTION generator update. generator update CONJUNCTION discriminator update. spectral normalization USED-FOR SGAN. SGAN USED-FOR images. WGAN - GP USED-FOR images. gradient penalty USED-FOR RaGAN. SGAN USED-FOR ones. WGAN - GP USED-FOR ones. spectral normalization USED-FOR ones. Method are generator G, divergence minimization, and relativistic discriminator. OtherScientificTerm is priori knowledge. Generic is code.","This paper proposes two new GAN loss functions, called Relativistic GANs (RGANs) and RaGANs, which are based on the integral probability metric (IPM) loss functions. The main contribution of this paper is to show that the identity function of RGANs can be viewed as a function of the discriminator update and the generator update. Theoretical analysis is provided to show the convergence of the proposed loss function. Experiments on synthetic and real-world data demonstrate the effectiveness of the new loss functions compared to existing GAN methods.",This paper proposes a new GAN loss function based on the integral probability metric (IPM) GANs. The main idea is to use the identity function of the discriminator of the GAN as the loss function. The authors show that the new loss function can be used to improve the performance of the original GAN. They also show that it can be applied to the WGAN-GP and LSGAN networks.
6724,SP:8df1599919dcb3329553e75ffb19059f192542ea,"catastrophic forgetting FEATURE-OF neural network architectures. approach USED-FOR problem. approach USED-FOR model. solver HYPONYM-OF model. Task is AI and lifelong learning systems. Method are continual learning methods, and Parameter Generation. ","This paper studies the problem of catastrophic forgetting in continual learning. The authors propose a method to address this problem by learning a solver to solve the problem in a continual learning setting. The proposed method is based on the idea of Parameter Generation (PG), which is a generative model that learns a set of parameters to generate a new set of samples for each task. The goal of the method is to ensure that the model does not forget any of the previous tasks in the future. ","This paper proposes a new approach to the catastrophic forgetting problem in continual learning. The main idea is to use a solver-based approach to solve the problem of catastrophic forgetting. The solver is trained using a continuous learning model, and the model is trained to generate a set of samples that are used to solve a problem. The model is then used to generate samples that can be used in a continual learning setting. The authors show that the solver can be trained to solve this problem. "
6728,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"artificial agents USED-FOR them. rich and orderly structure USED-FOR systems. behavioral dynamics FEATURE-OF multi - agent systems. rich and orderly structure FEATURE-OF behavioral dynamics. rich and orderly structure FEATURE-OF multi - agent systems. Relational Forward Models ( RFM ) USED-FOR multi - agent learning. they USED-FOR interpretable intermediate representations. discrete entities USED-FOR models. RFM modules USED-FOR learning systems. learning systems COMPARE non - augmented baselines. non - augmented baselines COMPARE learning systems. RFM modules PART-OF agents. RFM modules COMPARE non - augmented baselines. non - augmented baselines COMPARE RFM modules. Generic is networks. OtherScientificTerm are multi - agent environments, and social interactions. Method is autonomous systems. ","This paper proposes to use Relational Forward Models (RFMs) for multi-agent reinforcement learning. The RFMs are designed to capture the interplay between agents in the environment. The authors show that RFMs can be used to learn interpretable intermediate representations of discrete entities, which are then used to guide the learning process of the agents. Experiments are conducted on simulated and real-world environments. ","This paper proposes a new approach to multi-agent learning based on Relational Forward Models (RFM). RFM is a family of models that can be used to model the interaction between agents in a multi-player environment. The authors show that RFM can be applied to a variety of learning tasks, and that it can improve the performance of existing learning systems. They also show that it is possible to use RFM to augment existing learning methods.   "
6732,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"reinforcement learning USED-FOR real world problems. expert behavior USED-FOR reward function. method USED-FOR rewards. prior USED-FOR approach. images USED-FOR rewards. OtherScientificTerm are oracle reward function, reward functions, and expressive reward functions. Method is Inverse reinforcement learning ( IRL ). Material is datasets of demonstrations. Task is IRL. Generic is tasks. ",This paper proposes a method for inverse reinforcement learning (IRL) where the goal is to learn a reward function that can be used to guide the agent to solve a given task. The method is based on the idea that the reward function can be learned by observing the agent's behavior in the presence of an oracle reward function. The reward function is learned by training a prior model that predicts the prior reward function from an image of the agent’s state and action. The prior is then used to train an RL agent that is trained to maximize the reward of the expert agent. The proposed method is evaluated on a variety of tasks and compared to a number of baselines.,"This paper proposes a method for inverse reinforcement learning (IRL) where the goal is to learn an expressive reward function that is independent of the expert reward function. The authors show that the reward function can be expressed in terms of a prior prior, and that the prior can be used to learn the expressive reward functions. They also show that their method can be applied to a variety of IRL tasks.  "
6736,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"framework USED-FOR data efficient and versatile learning. framework USED-FOR Meta - Learning approximate Probabilistic Inference. Meta - Learning approximate Probabilistic Inference USED-FOR Prediction. ML - PIP HYPONYM-OF framework. ML - PIP USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR methods. probabilistic interpretations of meta - learning USED-FOR ML - PIP. VERSA HYPONYM-OF framework. flexible and versatile amortization network USED-FOR framework. flexible and versatile amortization network USED-FOR VERSA. benchmark datasets EVALUATE-FOR method. benchmark datasets EVALUATE-FOR VERSA. few - shot ShapeNet view reconstruction task EVALUATE-FOR approach. Material is few - shot learning datasets. OtherScientificTerm are task - specific parameters, and second derivatives. Task are optimization, inference, and classification. Method is inference networks. ","This paper proposes a meta-learning framework for few-shot learning. The proposed method, called VERSA, is based on a probabilistic interpretation of meta learning, where the task-specific parameters are parameterized by an amortized neural network. The main contribution of the paper is a novel amortization network for meta learning that is able to handle multiple tasks at once. The experimental results show that the proposed method achieves state-of-the-art performance on ShapeNet and ImageNet.","This paper proposes a new meta-learning framework for few-shot learning, called VERSA, which is based on the ML-PIP framework. The main contribution of the paper is to propose a flexible amortization network that can be applied to the task-specific parameters. The proposed method is evaluated on ShapeNet and CIFAR-10 datasets."
6740,SP:44e0f63ffee15796ba6135463134084bb370627b,"deep learning architecture USED-FOR classifying structured objects. ultrafine - grained datasets USED-FOR deep learning architecture. localvisual features CONJUNCTION neighboring class information. neighboring class information CONJUNCTION localvisual features. linear - chain CRFs USED-FOR images. visual features COMPARE class - structure information. class - structure information COMPARE visual features. convolutional layers USED-FOR visual features. CRF pairwise potential matrix USED-FOR class - structure information. parametrization USED-FOR nonlinear objective function. surrogate likelihood USED-FOR local likelihood approximation. local likelihood approximation USED-FOR CRF. surrogate likelihood USED-FOR CRF. integrated batch - normalization USED-FOR CRF. integrated batch - normalization USED-FOR local likelihood approximation. CRF methods USED-FOR contextual relationships. model COMPARE CRF methods. CRF methods COMPARE model. method COMPARE linear CRF parametrization. linear CRF parametrization COMPARE method. unnormalized likelihood optimization CONJUNCTION RNN modeling. RNN modeling CONJUNCTION unnormalized likelihood optimization. linear CRF parametrization CONJUNCTION unnormalized likelihood optimization. unnormalized likelihood optimization CONJUNCTION linear CRF parametrization. linear CRF parametrization COMPARE RNN modeling. RNN modeling COMPARE linear CRF parametrization. method COMPARE RNN modeling. RNN modeling COMPARE method. OtherScientificTerm are context - based semantic similarity space, visual similarities, and contextual information. Material is images of retail - store product displays. ","This paper proposes a novel linear chain CRF method for image classification. The proposed method is based on the CRF pairwise potential matrix, which is a non-linear objective function. The authors show that the proposed method can be used in combination with unnormalized likelihood optimization and convolutional layers to improve the performance of image classification models.   ","This paper proposes a new method for learning linear-chain CRF-based class-structured object classification. The proposed method is based on an integrated batch-normalization of the CRF pairwise potential matrix, which is used as a surrogate likelihood for the local likelihood approximation. The authors show that the proposed method outperforms the state-of-the-art in terms of performance and accuracy. "
6744,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"global structure CONJUNCTION fine - scale waveform coherence. fine - scale waveform coherence CONJUNCTION global structure. audio synthesis HYPONYM-OF machine learning task. global structure FEATURE-OF human perception. fine - scale waveform coherence FEATURE-OF human perception. local structure FEATURE-OF Autoregressive models. WaveNet HYPONYM-OF Autoregressive models. global latent conditioning CONJUNCTION parallel sampling. parallel sampling CONJUNCTION global latent conditioning. global latent conditioning FEATURE-OF Generative Adversarial Networks ( GANs ). parallel sampling USED-FOR Generative Adversarial Networks ( GANs ). log magnitudes CONJUNCTION instantaneous frequencies. instantaneous frequencies CONJUNCTION log magnitudes. GANs USED-FOR high - fidelity and locally - coherent audio. log magnitudes USED-FOR GANs. spectral domain FEATURE-OF sufficient frequency resolution. sufficient frequency resolution FEATURE-OF log magnitudes. sufficient frequency resolution FEATURE-OF instantaneous frequencies. spectral domain FEATURE-OF instantaneous frequencies. sufficient frequency resolution USED-FOR GANs. GANs COMPARE WaveNet baselines. WaveNet baselines COMPARE GANs. NSynth dataset EVALUATE-FOR GANs. automated and human evaluation metrics EVALUATE-FOR WaveNet baselines. automated and human evaluation metrics EVALUATE-FOR GANs. Method is iterative sampling. OtherScientificTerm are global latent structure, and locally - coherent audio waveforms. ","This paper proposes a generative adversarial network (GAN) architecture for audio synthesis. The main idea is to use a GAN to learn a global latent representation of the audio waveform, which is then used to train an autoregressive model to generate the audio signal. The authors show that the GAN is able to generate high-fidelity and locally-coherent audio with sufficient frequency resolution in the spectral domain. Experiments on the NSynth dataset show that GANs are able to achieve state-of-the-art results on audio synthesis tasks. ",This paper proposes a new method for generating high-fidelity and locally-coherent audio waveforms using a generative adversarial network (GAN). The method is based on the idea of global latent conditioning and parallel sampling. The authors show that GANs are able to generate high-quality audio with sufficient frequency resolution in the spectral domain and high-resolution in the log magnitudes domain. They also show that their method is able to achieve good performance on the NSynth dataset.   
6748,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"training accuracy EVALUATE-FOR Neural networks. mixed integer program USED-FOR piecewise - linear neural networks. verifier COMPARE state - of - the - art. state - of - the - art COMPARE verifier. finding minimum adversarial distortions EVALUATE-FOR verifier. tight formulations USED-FOR non - linearities. tight formulations CONJUNCTION presolve algorithm. presolve algorithm CONJUNCTION tight formulations. tight formulations USED-FOR computational speedup. verifier USED-FOR networks. adversarial accuracy EVALUATE-FOR MNIST classifier. adversarial accuracy EVALUATE-FOR perturbations. perturbations FEATURE-OF MNIST classifier. adversarial example USED-FOR classifier. bounded l∞ norm FEATURE-OF perturbations. robust training procedures CONJUNCTION network architectures. network architectures CONJUNCTION robust training procedures. adversarial examples COMPARE first - order attack. first - order attack COMPARE adversarial examples. Task are Verification of networks, and verification of piecewise - linear neural networks. OtherScientificTerm are minimum adversarial distortions, ReLUs, and norm - bounded perturbations. Method is convolutional and residual networks. Material is MNIST and CIFAR-10 datasets. ","This paper studies the verification of piecewise-linear neural networks with ReLUs. The authors propose a new method to find the minimum adversarial distortions in the input data. The main idea is to use a mixed integer program to compute the adversarial perturbations for each layer of the network, and then use the perturbation to train the classifier. The paper shows that the method is computationally efficient and can find adversarial examples with bounded l-norms. ","This paper studies the problem of verifying the robustness of piecewise-linear neural networks against adversarial examples. The main contribution of the paper is a novel formulation of the adversarial perturbations for piecewise linear neural networks, which can be used to find the minimum adversarial distortions of the classifier. The authors show that the verifier can find the least adversarial distortion of a classifier with bounded l-norms. They also provide a presolve algorithm for finding the minimum perturbation. Finally, the authors provide a theoretical analysis of their method."
6752,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,rich structure FEATURE-OF real world tasks. repeated structure USED-FOR learning. default policy HYPONYM-OF component. component PART-OF KL regularized expected reward objective. it USED-FOR reusable behaviours. reusable behaviours USED-FOR policy. information bottleneck approaches CONJUNCTION variational EM algorithm. variational EM algorithm CONJUNCTION information bottleneck approaches. default policy CONJUNCTION policy. policy CONJUNCTION default policy. default policy USED-FOR tasks. Method is fixed default policy. Generic is strategy. Material is discrete and continuous action domains. ,"This paper studies the problem of learning a policy that can be used to solve a sequence of discrete and continuous action domains. In particular, the authors propose to use a ""default policy"", which is a combination of a fixed default policy and a set of reusable policies. The authors show that this combination of policies can improve the performance of the learned policy in the continuous and discrete action domains in terms of the expected reward. They also show that the learned policies can be combined with the default policy in order to improve the generalization performance. ","This paper proposes a method for learning a policy that can be used to learn a set of reusable behaviours that are reusable across different tasks. The key idea is to use a KL-regularized expected reward objective (e.g., the default policy) as a component of the reward objective. The authors show that their approach can be applied to a variety of settings, including discrete and continuous action domains. They also show that it can be combined with information bottleneck approaches and a variational EM algorithm."
6756,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,conversation history USED-FOR Conversational machine comprehension. single - turn models USED-FOR history. FLOW USED-FOR single - turn models. intermediate representations PART-OF FLOW. intermediate representations PART-OF mechanism. alternating parallel processing structure USED-FOR FLOW. alternating parallel processing structure USED-FOR intermediate representations. approaches COMPARE FLOW. FLOW COMPARE approaches. latent semantics PART-OF FLOW. CoQA CONJUNCTION QuAC. QuAC CONJUNCTION CoQA. FLOWQA HYPONYM-OF model. conversational challenges EVALUATE-FOR FLOWQA. conversational challenges EVALUATE-FOR model. tasks EVALUATE-FOR FLOW. FLOWQA COMPARE models. models COMPARE FLOWQA. sequential instruction understanding USED-FOR conversational machine comprehension. SCONE EVALUATE-FOR models. accuracy EVALUATE-FOR FLOWQA. SCONE EVALUATE-FOR FLOWQA. Metric is F1. ,"This paper proposes a new model for conversational machine comprehension, called FLOWQA, which is a multi-turn model for sequential instruction understanding. The main idea is to use an alternating parallel processing structure where intermediate representations are processed in two stages, one for each turn, and the other for the history of the previous turn. The authors show that the proposed FLOW model achieves state-of-the-art performance on SCONE and QuAC tasks.","This paper proposes a new model FLOWQA for conversational machine comprehension. The model is based on FLOW, which is a multi-turn model that learns a sequence of intermediate representations of the history of the conversation. The intermediate representations are learned in an alternating parallel fashion, where each intermediate representation is processed in a different way. The authors show that FLOW is able to achieve state-of-the-art performance on SCONE, CoQA, and QuAC."
6760,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,"Programming languages HYPONYM-OF machine learning. generative models USED-FOR generating static snapshots of code. source code HYPONYM-OF dynamic object. synthetic data USED-FOR edit patterns. synthetic data USED-FOR neural networks. generalization USED-FOR edit patterns. large - scale dataset USED-FOR models. fine - grained edits PART-OF large - scale dataset. Method are generative models of source code, and attentional and pointer network components. OtherScientificTerm are static snapshots of code, and source code files. Generic are it, and tools. Metric is scalability. ","This paper presents a method for generating static snapshots of source code. The method is based on the observation that static code is a dynamic object, and the goal is to generate static snapshots that can be used to train a generative model. The authors propose to use a combination of attentional and pointer networks to generate the static snapshots, and show that the attentional network is able to learn to predict the edit patterns of the static code. They also show that their method can generate fine-grained edits in a large-scale dataset.","This paper proposes a method for generating static snapshots of source code from a large-scale dataset. The idea is to generate a large dataset of edits from the source code, which is then used to train a generative model of the code. The model is trained using a combination of attentional and pointer network components. The authors show that their method can generate fine-grained edits from a dataset of fine-scale edits. They also show that the model is able to generalize well to a large number of edits."
6764,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"strategy USED-FOR multi - class classification. pairwise similarity HYPONYM-OF annotation. pairwise similarity USED-FOR strategy. binary classifier USED-FOR pairwise similarity prediction. method USED-FOR binary classifier. submodule USED-FOR multi - class classifier. loss function USED-FOR neural network - based models. probabilistic graphical model USED-FOR it. method COMPARE state of the art. state of the art COMPARE method. learning paradigms EVALUATE-FOR state of the art. learning paradigms EVALUATE-FOR method. multi - class labels FEATURE-OF learning multi - class classification. accuracy EVALUATE-FOR state of the art. accuracy EVALUATE-FOR method. OtherScientificTerm is class - specific labels. Method is meta classification learning. Generic are approach, and framework. ","This paper proposes a meta-learning strategy for multi-class classification. The proposed method is based on the idea of pairwise similarity prediction, where the goal is to learn a binary classifier that predicts the pairwise similarities between two classes. The authors propose to use a sub-module for each class and use a probabilistic graphical model (PGM) to model the loss function.   The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy on a variety of datasets. ","This paper proposes a new meta-learning strategy for multi-class classification. The main idea is to use pairwise similarity prediction to learn a binary classifier for each class. The proposed method is based on a probabilistic graphical model, which can be applied to any neural network-based model. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy. "
6768,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"visual scene USED-FOR interpretation of quantifier statements. inference mechanisms USED-FOR interpretation of quantifier statements. cognitive concepts USED-FOR strategies. deep learning models USED-FOR visual question answering. spatial arrangement of the scene HYPONYM-OF confounding factors. Task is psycholinguistics. Method are FiLM visual question answering model, and approximate number system. OtherScientificTerm is Weber ’s law. Generic is system. ","This paper studies the question answering task of visual question answering in the presence of a set of quantified statements. The authors propose a new model, FiLM, that is trained to predict the answer to a visual question from a given set of questions. They show that the model is able to infer the answer in terms of the number of objects in the scene. The model is trained using an approximate number system, and the authors show that it is possible to estimate the answer of the question using the number system.   The authors also show that their model is capable of predicting the answer from the set of question in a way that is consistent with the true answer.","This paper studies the problem of visual question answering in the context of psycholinguistics. The authors propose an approximate number system, which they call FiLM, for the task. They show that the number system can be used as a surrogate for the true number system in terms of Weber's law. They also provide a theoretical analysis of the effect of spatial arrangement of the scene on the performance of the model. "
6772,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"embedding models USED-FOR link prediction in relational knowledge graphs. relational facts PART-OF Knowledge graphs. Bayesian framework USED-FOR knowledge graphs. it USED-FOR gradient based optimization. divergences FEATURE-OF non - Bayesian treatment. gradient based optimization USED-FOR divergences. hyperparameters USED-FOR gradient based optimization. Models COMPARE state - of - the - art. state - of - the - art COMPARE Models. hyperparameters COMPARE state - of - the - art. state - of - the - art COMPARE hyperparameters. hyperparameters USED-FOR Models. Material is relational knowledge graphs. Task is small data problem. OtherScientificTerm is parameter uncertainty. Method are variational inference, and Bayesian approach. ","This paper proposes a Bayesian embedding model for link prediction in relational knowledge graphs. The proposed method is based on a variational inference approach, where the model is trained with a Bayes-based loss function. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and computational complexity.  ",This paper proposes a Bayesian approach to link prediction in relational knowledge graphs. The main idea is to use a non-Bayesian framework to model the divergences in the knowledge graph. The authors propose to use gradient-based optimization to solve the problem. They show that the proposed approach outperforms the state-of-the-art in terms of the number of hyperparameters and the accuracy of the model.
6776,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"dimension reduction USED-FOR visualization or prediction enhancement. online learning approaches USED-FOR supervised dimension reduction. algorithm USED-FOR supervised dimension reduction. sliced inverse regression ( SIR ) HYPONYM-OF algorithm. sliced inverse regression ( SIR ) USED-FOR algorithm. algorithm USED-FOR subspace of significant factors. intrinsic lower dimensionality FEATURE-OF subspace of significant factors. overlapping technique USED-FOR algorithm. real data applications EVALUATE-FOR algorithms. Method are Online learning, and incremental sliced inverse regression ( ISIR ). Task is online dimension reduction. Generic is it. ",This paper studies online dimension reduction with sliced inverse regression (SIR) and ISIR. SIR is an online learning algorithm that learns a subspace of significant factors in an online fashion. ISIR is a variant of SIR that uses an overlapping technique to reduce the dimensionality of the subspace to a lower dimensionality. The authors show that SIR can be used in conjunction with ISIR to improve the performance of online learning in real-world applications.,"This paper proposes a new algorithm for online dimension reduction, sliced inverse regression (SIR). The main idea is to reduce the dimensionality of a subspace of significant factors in an online learning setting. The authors show that SIR can be combined with other online learning algorithms, such as incremental sliced inverse regressors (ISIR), to achieve a lower dimensionality. They also propose an overlapping technique for the SIR algorithm, which can be applied to any online learning algorithm. The proposed algorithm is evaluated on a variety of real-world datasets."
6780,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"models USED-FOR intra - modal and cross - modal interactions. models USED-FOR unexpected missing or noisy modalities. intra - modal and cross - modal interactions USED-FOR prediction. models USED-FOR prediction. multimodal data USED-FOR joint generative - discriminative objective. model USED-FOR representations. model USED-FOR independent factors. multimodal discriminative and modality - specific generative factors HYPONYM-OF independent factors. joint multimodal features USED-FOR discriminative tasks. Multimodal discriminative factors USED-FOR discriminative tasks. joint multimodal features PART-OF Multimodal discriminative factors. sentiment prediction HYPONYM-OF discriminative tasks. model USED-FOR multimodal representations. multimodal datasets EVALUATE-FOR multimodal representations. model USED-FOR missing modalities. factorized representations USED-FOR multimodal learning. Task is Learning multimodal representations. OtherScientificTerm are multiple modalities, and Modality - specific generative factors. ","This paper proposes a method for learning multimodal representations from multiple modalities. The proposed method is based on a generative-discriminative objective, where the discriminative and modality-specific generative factors are modeled as independent factors. The authors show that the proposed method achieves better performance than baselines on a variety of datasets.  ","This paper proposes a new multimodal discriminative-discriminative objective, which is based on the joint generative-dispossibilization objective. The main idea is to learn a joint multimodality-specific generative factorization objective for each modality, which can be used for both intra- and cross-modal interactions. The authors show that this objective can be applied to a wide range of tasks, including sentiment prediction, image classification, and language modeling. The proposed objective is evaluated on a variety of datasets, and compared with a number of baselines. "
6784,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"shared conditional WaveNet core CONJUNCTION independent learned embeddings. independent learned embeddings CONJUNCTION shared conditional WaveNet core. independent learned embeddings USED-FOR multi - speaker model. shared conditional WaveNet core USED-FOR multi - speaker model. fixed weights USED-FOR neural network. stochastic gradient descent USED-FOR architecture. trained neural network encoder USED-FOR speaker embedding. sample naturalness CONJUNCTION voice similarity. voice similarity CONJUNCTION sample naturalness. approaches USED-FOR multi - speaker neural network. Method are meta - learning approach, and TTS system. Generic are network, and strategies. OtherScientificTerm is WaveNet core. Material is audio data. ","This paper proposes a meta-learning approach for multi-speaker audio classification. The proposed method is based on a shared conditional WaveNet core and independent learned embeddings. The main idea is to train a speaker encoder and a speaker embedding network with a fixed set of fixed weights, and then use the shared conditional waveNet core to learn the speaker representations. Experiments show that the proposed method outperforms baselines in terms of sample naturalness and voice similarity.","This paper proposes a meta-learning approach for multi-speaker speech synthesis, where the speaker embedding is learned using a shared conditional WaveNet core and independent learned embeddings. The key idea is to learn a shared embedding for each speaker, and then use the learned embedding to train a neural network encoder. The authors show that their approach outperforms the state-of-the-art in terms of sample naturalness and voice similarity.   "
6788,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"Robust estimation HYPONYM-OF statistics. Huber ’s -contamination model USED-FOR Robust estimation. Tukey ’s median CONJUNCTION estimators. estimators CONJUNCTION Tukey ’s median. estimators HYPONYM-OF Statistically optimal procedures. depth functions USED-FOR estimators. Tukey ’s median HYPONYM-OF Statistically optimal procedures. f -GANs CONJUNCTION depth functions. depth functions CONJUNCTION f -GANs. f -Learning USED-FOR depth functions. f -Learning USED-FOR f -GANs. depth functions USED-FOR statistically optimal robust estimators. total variation distance FEATURE-OF variational lower bounds. f -Learning USED-FOR variational lower bounds. variational lower bounds USED-FOR depth functions. GANs USED-FOR computing robust estimators. Gaussian distribution CONJUNCTION elliptical distributions. elliptical distributions CONJUNCTION Gaussian distribution. statistically optimal robust location estimators USED-FOR Gaussian distribution. statistically optimal robust location estimators USED-FOR elliptical distributions. hidden layers PART-OF GANs. discriminator networks PART-OF GANs. hidden layers PART-OF discriminator networks. OtherScientificTerm are computational intractability, and first moment. Method is f GANs. ","This paper studies the robust estimation of Tukey's median under Huber's contamination model. The authors show that f-GANs and depth functions with f-Learning can be used to compute robust estimators for Tukey’s median. They show that the depth functions can be computed using f-learning with a variational lower bound on the total variation distance between the estimators and the true estimators. They also show that for Gaussian and elliptical distributions, they can compute Gaussian robust location estimators using fGANs.",This paper studies the problem of robust estimation in GANs. The main contribution of the paper is a theoretical analysis of the generalization of Huber’s-contamination model to f-GANs.  The main result is a new lower bound on the total variation distance between the depth functions of fGANs and the robust estimators. The lower bound is based on the variational lower bounds of f-Learning.  
6792,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"symmetry CONJUNCTION repetition. repetition CONJUNCTION symmetry. higher - level, abstract regularities PART-OF scene. repetition HYPONYM-OF higher - level, abstract regularities. symmetry HYPONYM-OF higher - level, abstract regularities. vision recognition modules CONJUNCTION scene representations. scene representations CONJUNCTION vision recognition modules. symbolic program USED-FOR scene programs. model USED-FOR scene programs. hierarchical, object - based scene representation USED-FOR model. hierarchical, object - based scene representation USED-FOR scene programs. compositional structure FEATURE-OF real images. synthetic data EVALUATE-FOR model. complex visual analogy - making CONJUNCTION scene extrapolation. scene extrapolation CONJUNCTION complex visual analogy - making. scene programs USED-FOR applications. scene extrapolation HYPONYM-OF applications. complex visual analogy - making HYPONYM-OF applications. Task is Human scene perception. ","This paper proposes a method for learning symbolic programs that can be used to model the compositional structure of a scene. The main idea is to use a hierarchical object-based representation of the scene as a symbolic program, which is then used to train an encoder-decoder architecture to predict the symbolic program. The proposed method is evaluated on synthetic and real-world datasets, and compared to state-of-the-art methods.","This paper proposes a new model for learning symbolic programs for scene perception. The model is based on a hierarchical, object-based scene representation, which is composed of a symbolic program and a vision recognition module. The program consists of a set of symbolic programs that represent the compositional structure of a scene. The symbolic program is represented by a hierarchical object representation, and the model is trained to learn the symbolic program. Experiments on synthetic data and real data show that the proposed model outperforms the state-of-the-art."
6796,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"long sequence data USED-FOR Recurrent neural networks. methods USED-FOR RNN state updates. memory FEATURE-OF network. methods PART-OF architectures. timing - gated LSTM RNN model USED-FOR reducing state updates. longer memory persistence CONJUNCTION error - gradient flow. error - gradient flow CONJUNCTION longer memory persistence. time gate USED-FOR longer memory persistence. model USED-FOR long temporal dependencies. model COMPARE LSTM. LSTM COMPARE model. non - optimal initialization USED-FOR time gate parameters. temporal curriculum learning schedule USED-FOR g - LSTM. computational budget term USED-FOR network. convergence time EVALUATE-FOR LSTM. computational budget term USED-FOR training loss. long sequences EVALUATE-FOR LSTM. Task are vanishing gradient problem, and network update. OtherScientificTerm are neuron, and neuron state. ","This paper proposes a new RNN architecture for long sequence data. The proposed model is based on a timing-gated LSTM RNN model, which is able to reduce the number of state updates in the RNN while maintaining the memory persistence and error-gradient flow. The main contribution of the paper is to introduce a temporal curriculum learning schedule for the time gate parameters, which allows the model to learn long temporal dependencies. The paper also proposes a computational budget term to improve the training loss. ","This paper proposes a timing-gated LSTM RNN model for reducing the number of state updates in a recurrent neural network. The proposed model is based on a temporal curriculum learning schedule, where each neuron is trained on a sequence of time-varying time-steps. The authors show that the proposed model outperforms the state-of-the-art in terms of convergence time and accuracy. "
6800,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"ensembling USED-FOR first and second order deep feature statistics. re - training CONJUNCTION pre - processing. pre - processing CONJUNCTION re - training. pre - processing CONJUNCTION model. model CONJUNCTION pre - processing. method COMPARE state - of - the - art. state - of - the - art COMPARE method. benchmarking tasks EVALUATE-FOR state - of - the - art. benchmarking tasks EVALUATE-FOR method. TinyImageNet resize FEATURE-OF out - of - distribution dataset. true negative rate EVALUATE-FOR method. DenseNet USED-FOR method. DenseNet USED-FOR in - distribution ( CIFAR-100 ). Method are deep neural networks, and plug - and - play detection procedure. OtherScientificTerm is feature maps. ", is a method to detect out-of-distribution (OOD) samples in deep neural networks. The method is based on the observation that the true negative rate of OOD samples is higher than the true positive rate. The authors propose a plug-and-play detection procedure to detect OOD instances. The proposed method is evaluated on CIFAR-100 and TinyImageNet. ,This paper proposes a new method for identifying out-of-distribution feature maps in deep neural networks. The proposed method is based on a plug-and-play detection procedure. The authors show that the proposed method outperforms the state of the art on a variety of benchmarking tasks. They also show that their method is able to detect out of distribution feature maps.
6804,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,model recovery USED-FOR data classification. neural network USED-FOR weight vectors. Gaussian inputs USED-FOR empirical risk function. cross entropy USED-FOR empirical risk function. gradient descent USED-FOR one - hidden - layer neural networks. near - optimal sample CONJUNCTION computational complexity. computational complexity CONJUNCTION near - optimal sample. global convergence guarantee USED-FOR empirical risk minimization. computational complexity EVALUATE-FOR network input dimension. gradient descent USED-FOR cross entropy. cross entropy USED-FOR global convergence guarantee. cross entropy USED-FOR empirical risk minimization. gradient descent USED-FOR global convergence guarantee. gradient descent USED-FOR empirical risk minimization. OtherScientificTerm is sigmoid activations. Metric is sample complexity. Method is tensor method. ,This paper studies the problem of model recovery for one-hidden layer neural networks with Gaussian inputs. The authors show that the empirical risk minimization with gradient descent is guaranteed to converge to a near-optimal sample with a global convergence guarantee. They show that gradient descent converges to the near optimal sample with the same computational complexity as the nearest optimal sample. They also show that this is the case for the tensor method.,This paper studies the problem of model recovery for one-hidden-layer neural networks with Gaussian inputs. The authors prove a global convergence guarantee for the empirical risk minimization of the model recovery problem. They show that the cross entropy of the risk function can be used to estimate the global convergence of gradient descent. They also show that gradient descent can converge to near-optimal sample complexity. 
6808,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"method USED-FOR salient convolutional channels. convolutional layers USED-FOR features. small auxiliary connections PART-OF convolutional layers. small auxiliary connections USED-FOR FBS. it USED-FOR convolution. channel pruning methods COMPARE it. it COMPARE channel pruning methods. it USED-FOR full network structures. it USED-FOR CNNs. stochastic gradient descent USED-FOR FBS - augmented networks. channel pruning CONJUNCTION dynamic execution schemes. dynamic execution schemes CONJUNCTION channel pruning. FBS COMPARE channel pruning. channel pruning COMPARE FBS. FBS COMPARE dynamic execution schemes. dynamic execution schemes COMPARE FBS. ImageNet classification EVALUATE-FOR FBS. VGG-16 CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION VGG-16. VGG-16 EVALUATE-FOR FBS. ResNet-18 EVALUATE-FOR FBS. Method are deep convolutional neural networks, and feature boosting and suppression ( FBS ). Metric are computational and memory resources, and top-5 accuracy loss. OtherScientificTerm is channels. ","This paper proposes feature boosting and suppression (FBS), a method to remove salient convolutional channels from convolution layers to improve the performance of CNNs. The main idea is to use auxiliary connections in the convolution layer to boost the performance and suppress the salient features. The proposed method is evaluated on ImageNet classification tasks and compared with channel pruning and dynamic execution schemes. ","This paper proposes feature boosting and suppression (FBS), which augments convolutional layers with small auxiliary connections in order to reduce the computational and memory cost of channel pruning. The main idea of FBS is to augment the convolution layers with auxiliary connections, and then use stochastic gradient descent (SGD) to prune the salient channels. The authors show that FBS improves the performance of CNNs on VGG-16, ResNet-18, and ImageNet classification tasks."
6812,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,mixed Nash Equilibria ( NE ) perspective FEATURE-OF Generative Adversarial Networks ( GANs ). algorithmic framework USED-FOR GANs. prox methods USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR algorithmic framework. infinite - dimensional two - player game USED-FOR GANs. procedure USED-FOR prox methods. sampling routines USED-FOR prox methods. approach COMPARE methods. methods COMPARE approach. Adam CONJUNCTION RMSProp. RMSProp CONJUNCTION Adam. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. methods USED-FOR pure strategy equilibria. SGD HYPONYM-OF pure strategy equilibria. quality EVALUATE-FOR approach. OtherScientificTerm is mixed NE. ,This paper studies the mixed Nash equilibria problem in GANs from a two-player game perspective. The authors propose an algorithm that uses prox methods to find the optimal strategy in the infinite-dimensional two player game. The prox method is based on a sampling procedure that can be applied to any prox methods. The proposed algorithm is shown to be more efficient than Adam and RMSProp in terms of sample complexity.,This paper proposes a new approach to solving the mixed Nash equilibrium problem in GANs. The main idea is to use an infinite-dimensional two-player game between Adam and RMSProp to solve the problem of mixed Nash equilibria. The authors propose a new sampling procedure for prox methods and show that it can be used to improve the performance of existing prox methods. They also show that their approach can achieve better performance than the state-of-the-art. 
6816,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"deep neural networks USED-FOR method. approach USED-FOR model. pretrained network USED-FOR problems. depth - wise convolutions HYPONYM-OF low - parameter layers. approach USED-FOR sequential transfer learning. multi - task learning problems EVALUATE-FOR logits - only fine - tuning. Generic are task, and network. Method are Single Shot MultiBox Detection ( SSD ) model, 1000 - class image classification model, and SSD feature extractor. Metric is transfer - learning accuracy. OtherScientificTerm is singletask. ","This paper proposes a method for transfer learning from a single task to multiple tasks in a multi-task setting. The main idea is to use a pre-trained network to learn a feature extractor for each of the tasks, and then fine-tune the network for the new task using the extracted features. The proposed method is evaluated on the Single Shot MultiBox Detection (SSD) model and the 1000-class image classification model.   ","This paper proposes a novel approach to transfer learning for multi-task learning problems. The main idea is to use depth-wise convolutions as a low-parameter layer in the training of a deep neural network. The authors show that the proposed method can be applied to a large number of tasks, and show that it can be used to improve the transfer learning performance of a model trained on a single-shot multi-box detection (SSD) model. The proposed method is evaluated on a dataset of 1000-class image classification models."
6820,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"Normalization methods PART-OF deep learning toolbox. batch normalization ( BN ) HYPONYM-OF normalization method. method COMPARE BN. BN COMPARE method. method COMPARE normalization techniques. normalization techniques COMPARE method. BN CONJUNCTION normalization techniques. normalization techniques CONJUNCTION BN. single and multi - task datasets EVALUATE-FOR method. Generic are They, and approach. OtherScientificTerm are manually tuned learning rate schedules, and multi - modal distributions. Method is normalization. ",This paper proposes a new normalization method for batch normalization in deep learning. The proposed method is based on the observation that batch normalisation (BN) is prone to overfitting in multi-modal distributions. The authors propose to use a multi-task normalization to mitigate this issue. Theoretical analysis is provided to show that the proposed normalization is more efficient than BN. Empirical results are provided to demonstrate the effectiveness of the proposed method. ,"This paper proposes a new normalization method for batch normalization. The main idea is to use multi-modal normalization (i.e., using multiple modal distributions) to improve the performance of batch normalisation (BN). The main contribution of the paper is to provide a theoretical analysis of the effect of different learning rate schedules on the normalization process. The authors show that the learning rate schedule of BN can be biased in a way that it is not sensitive to the modal distribution of the data. They also show that BN is biased in terms of the number of tasks per batch, and propose a new method to mitigate this bias. The proposed method is evaluated on both single and multi-task datasets. "
6824,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"pruning technique USED-FOR subnetworks. test accuracy EVALUATE-FOR network. OtherScientificTerm are lottery ticket hypothesis, winning tickets, and initialization lottery. Task is training. ","This paper studies the lottery ticket hypothesis, which is that there are a large number of ""winning tickets"" in training neural networks. The authors propose a pruning technique to find the subnetworks with the most winning tickets. They show that this pruning strategy can lead to better test accuracy. They also show that the winning tickets are more likely to be found in training.","This paper studies the lottery ticket hypothesis, which claims that the number of winning tickets in the lottery is proportional to the total number of training tickets. The authors propose a pruning technique that prunes the training data into subnetworks, and show that this pruning strategy can improve the test accuracy of the network. They also provide a theoretical analysis of the effect of the pruning on test accuracy."
6828,SP:08c662296c7cf346f027e462d29184275fd6a102,"exploration FEATURE-OF reinforcement learning. attentive dynamics model ( ADM ) USED-FOR controllable elements of the observations. state representation USED-FOR exploration purposes. contingency information USED-FOR state representation. contingency information USED-FOR exploration purposes. high - level information CONJUNCTION supervisory data. supervisory data CONJUNCTION high - level information. actor - critic algorithm CONJUNCTION count - based exploration. count - based exploration CONJUNCTION actor - critic algorithm. expert demonstrations CONJUNCTION high - level information. high - level information CONJUNCTION expert demonstrations. representation USED-FOR count - based exploration. RAM states HYPONYM-OF high - level information. representation USED-FOR actor - critic algorithm. contingency - awareness USED-FOR exploration problems. exploration problems PART-OF reinforcement learning. contingency - awareness USED-FOR reinforcement learning. Method are Arcade Learning Element ( ALE ), and ADM. Material are Atari games, and MONTEZUMA ’S REVENGE. ","This paper proposes a novel method for exploration in reinforcement learning based on an attentional dynamics model (ADM). The proposed method is based on the observation-conditioned ADM, which learns a state representation of the observed observations and uses this state representation for exploration purposes. The method is evaluated on Atari games and is shown to outperform baselines on MONTEZUMA and MONtezuma’s Revenge.","This paper proposes a method for learning a state representation for exploration in Atari games. The proposed method is based on the attentive dynamics model (ADM), which is used to model the controllable elements of the observations. In particular, the ADM is used for the actor-critic algorithm and count-based exploration. The method is applied to Atari games with high-level information, expert demonstrations, and supervisory data. "
6832,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"HyperGAN HYPONYM-OF generative network. generative network USED-FOR weight parameters. weight parameters PART-OF deep neural networks. generative network USED-FOR deep neural networks. HyperGAN USED-FOR latent space. HyperGAN USED-FOR low dimensional noise. architecture COMPARE generative adversarial networks. generative adversarial networks COMPARE architecture. generated network parameter distribution CONJUNCTION unknown true parameter distribution. unknown true parameter distribution CONJUNCTION generated network parameter distribution. KL - divergence FEATURE-OF generated network parameter distribution. KL - divergence FEATURE-OF unknown true parameter distribution. HyperGAN USED-FOR classification. HyperGAN COMPARE fully supervised learning. fully supervised learning COMPARE HyperGAN. HyperGAN USED-FOR MNIST and CIFAR-10 datasets. HyperGAN COMPARE ensembles. ensembles COMPARE HyperGAN. HyperGAN USED-FOR uncertainty. uncertainty EVALUATE-FOR ensembles. HyperGAN - generated ensembles USED-FOR out of distribution data. out of distribution data CONJUNCTION adversarial examples. adversarial examples CONJUNCTION out of distribution data. HyperGAN - generated ensembles USED-FOR adversarial examples. HyperGAN USED-FOR uncertainty estimates. OtherScientificTerm are classification loss, and rich distribution of effective parameters. Material is inlier data. ","This paper proposes HyperGAN, a generative adversarial network (GAN) architecture that generates a large amount of low-dimensional noise in the latent space of a deep neural network. The proposed method is based on the idea that the true parameters of a neural network can be represented as a mixture of the generated parameters and the KL-divergence between the true and generated parameters. The authors show that the proposed method outperforms the state-of-the-art generative models on MNIST and CIFAR-10. ","This paper proposes a new generative adversarial network (GAN) architecture for training deep neural networks with low-dimensional noise. The proposed method, HyperGAN, is based on a generative model of the latent space, where the parameters of the network are generated from the input data. The authors show that HyperGAN can be used to estimate the true parameter distribution of the generated network parameters, and that it can also be used for predicting the true parameters of out-of-distribution data. HyperGAN is evaluated on MNIST and CIFAR-10 datasets, where it is shown to outperform the state of the art."
6836,SP:230b3e008e687e03a8b914084b93fc81609051c0,Variational Auto Encoder ( VAE ) HYPONYM-OF generative latent variable model. generative latent variable model USED-FOR representation learning. Variational Auto Encoder ( VAE ) USED-FOR representation learning. continuous valued latent variables USED-FOR VAEs. differentiable estimate USED-FOR ELBO. reparametrized sampling USED-FOR differentiable estimate. Stochastic Gradient Descend ( SGD ) USED-FOR it. discrete valued latent variables USED-FOR VAEs. binary or categorically valued latent representations USED-FOR VAEs. differentiable estimator USED-FOR ELBO. importance sampling USED-FOR differentiable estimator. benchmark datasets EVALUATE-FOR VAEs architectures. Bernoulli and Categorically distributed latent representations USED-FOR VAEs architectures. variational auto encoder ( VAE ) HYPONYM-OF generative model. it HYPONYM-OF generative model. It USED-FOR model. VAE USED-FOR tasks. data generation CONJUNCTION data interpolation. data interpolation CONJUNCTION data generation. density estimation CONJUNCTION data generation. data generation CONJUNCTION density estimation. data interpolation CONJUNCTION outlier and anomaly detection. outlier and anomaly detection CONJUNCTION data interpolation. VAE USED-FOR density estimation. outlier and anomaly detection CONJUNCTION clustering. clustering CONJUNCTION outlier and anomaly detection. VAE USED-FOR data generation. VAE USED-FOR outlier and anomaly detection. density estimation HYPONYM-OF tasks. data interpolation HYPONYM-OF tasks. clustering HYPONYM-OF tasks. outlier and anomaly detection HYPONYM-OF tasks. data generation HYPONYM-OF tasks. VAE HYPONYM-OF latent variable model. Generic is approach. Method is VARIATIONAL AUTO ENCODER. OtherScientificTerm is nonlinear dependent elements. ,"This paper proposes a variational auto-encoder (VAE) model for data generation and density estimation. The main idea is to use a continuous ELBO to estimate the ELBO of the data, which is then used to train the VAE. The ELBO is estimated using reparametrized sampling and a differentiable estimator. The proposed method is evaluated on a variety of tasks, including data generation, data interpolation, outlier and anomaly detection, and clustering. ","This paper proposes a new variational auto-encoder (VAE) model for data generation and data interpolation. The main idea is to use a differentiable estimator to estimate the ELBO of a VAE, which is based on a reparametrized sampling scheme. The authors show that this differentiable estimate can be used to improve the performance of VAEs on a variety of tasks, including data generation, outlier and anomaly detection, and data clustering.  "
6840,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,feed forward neural network COMPARE training approaches. training approaches COMPARE feed forward neural network. robustness EVALUATE-FOR training approaches. robustness EVALUATE-FOR feed forward neural network. mean field description of a Boltzmann machine USED-FOR pre - trained building block. MNIST dataset EVALUATE-FOR method. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial resistance EVALUATE-FOR method. OtherScientificTerm is adversarial attacks. Method is Boltzmann machine. ," and adversarial training.  The paper proposes a method to improve the robustness of a neural network against adversarial attacks. The method is based on the mean field description of a Boltzmann machine, which can be used as a pre-trained building block. The proposed method is evaluated on MNIST and CIFAR-10 datasets.",This paper proposes a method to improve the robustness of a feed-forward neural network against adversarial attacks. The main idea is to use the mean field description of a Boltzmann machine as a pre-trained building block for the adversarial attack. The method is evaluated on the MNIST dataset and shows that the proposed method is more robust than the state of the art. 
6844,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,"visible region PART-OF minimal image. deep neural networks ( DNNs ) COMPARE DNNs. DNNs COMPARE deep neural networks ( DNNs ). object location FEATURE-OF DNNs. DNN recognition ability FEATURE-OF natural images. robustness EVALUATE-FOR DNNs. natural images FEATURE-OF DNNs. Material is Minimal images. Metric are human recognition accuracy, and accuracy. OtherScientificTerm are invariance, and adversarial patterns. ",This paper studies the robustness of deep neural networks (DNNs) against adversarial attacks in minimal images. The authors show that DNNs are more robust to adversarial perturbations than natural images. They also show that adversarial training improves the performance of DNN on natural images in terms of human recognition accuracy.   ,"This paper studies the robustness of deep neural networks (DNNs) against adversarial attacks on natural images. The authors show that DNNs are invariant to adversarial patterns, and that human recognition accuracy is not invariant. They also show that adversarial training can be robust against natural images, and show that human accuracy can be more robust than DNN accuracy.  "
6848,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"multi - agent reinforcement learning ( MARL ) USED-FOR optimal collaboration. worker agents HYPONYM-OF self - interested agents. super agent USED-FOR them. super agent USED-FOR optimal coordination. manager HYPONYM-OF super agent. agent modeling CONJUNCTION policy learning. policy learning CONJUNCTION agent modeling. approach USED-FOR multi - agent management problems. environments EVALUATE-FOR approach. Resource Collection and Crafting EVALUATE-FOR approach. Resource Collection and Crafting HYPONYM-OF environments. approach USED-FOR optimal ad - hoc teaming. approach USED-FOR worker agents ’ minds. generalization FEATURE-OF optimal ad - hoc teaming. OtherScientificTerm are policy, and contracts. Generic is agents. Task is ad - hoc worker teaming. ","This paper proposes a multi-agent reinforcement learning (MARL) approach for ad-hoc worker teaming in resource collection and crafting. In this setting, agents are given a set of agents and a manager, and the goal is to maximize the mutual information between the agents in order to achieve the best performance. The proposed approach is based on the idea that agents can learn to select agents that maximize their mutual information about the environment, and then use a super-agent to coordinate with the other agents. The approach is evaluated in two environments, where it is shown to outperform the baselines.   ","This paper proposes a multi-agent reinforcement learning (MARL) approach for optimal coordination between worker agents and their super-agent manager. The approach is based on the idea that the super-super agent should be able to coordinate with the worker agents in order to achieve optimal coordination. The main idea is to model the agent’s state-action pairs as a set of agents, and the super agent is the agent that is most likely to coordinate best with the other agents. The agent is then trained to be the agent with the best agent-state pairs. The proposed approach is evaluated on two environments: resource collection and crafting, and it is shown to be effective in both environments. "
6852,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"time series USED-FOR recurrent neural network ( RNN ) solutions. asynchronous time series HYPONYM-OF series. unified RNN USED-FOR feature types. RNN framework USED-FOR sequential features. time features USED-FOR cell ’s memory state. sequential level FEATURE-OF time features. Method are RNN cells, and modeling framework. OtherScientificTerm are sparse and dense features, static ( whole sequence level ) features, and encoder output. Task is cell updates. ","This paper proposes a method to model time series using recurrent neural networks (RNNs). The proposed method is based on the observation that RNNs learn sparse and dense features in time series. The authors propose to use a unified RNN to model the feature types of time series, which can be expressed in terms of time features.   The authors show that the proposed method can be used to model sparse time series features, and that it is able to learn sparse features that are independent of the input time series (i.e. not dependent on the current time series). The authors also show that sparse features can be represented as a combination of dense and static features.","This paper proposes a unified RNN model for time series with sparse and dense features. The proposed model is based on the idea that sparse features can be represented as a sequence of time features, while dense features are represented as time features at a time-series level. The authors show that the proposed model can be used to model time series as an RNN with sparse or dense features, and that it can be combined with a unified feature encoder. They also show that it is possible to use sparse features to model the time series. "
6856,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"Method are neural model, feedforward neural network, and recurrent neural networks. OtherScientificTerm are propositional formula, and propositional atoms. Generic are network, and model. ","This paper studies the problem of learning propositional formulas in the presence of recurrent neural networks (RNNs). In particular, the authors show that the RNNs can be viewed as a set of propositional atoms, where each atom is represented by an RNN. They show that if a RNN is a feedforward RNN, then it can learn a propositional formula that can be represented as a sequence of atoms. The authors then show that this can be expressed as a linear combination of RNN and RKHS.   ","This paper studies the problem of propositional atomism in the context of recurrent neural networks (RNNs). In particular, the authors consider the case of a feedforward neural network, where the input is a set of atoms, and the output of the network is a recurrent neural network. The authors show that if the model is a RNN, then it can be used to predict the number of atoms in a propositional formula, and if it is not, then the model cannot predict any atoms. They also show that the model can predict any propositional atoms, even if it does not have access to all the atoms. "
6860,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,random mini - batches USED-FOR Training neural networks. accuracy EVALUATE-FOR network. speed of learning EVALUATE-FOR network. difficulty FEATURE-OF mini - batches. curriculum learning USED-FOR problem. CNNs USED-FOR image recognition. CIFAR-10 CONJUNCTION CIFAR-100 datasets. CIFAR-100 datasets CONJUNCTION CIFAR-10. performance EVALUATE-FOR small and competitive networks. learning speed EVALUATE-FOR small and competitive networks. competitive ” teacher ” network USED-FOR transfer learning. Imagenet database USED-FOR competitive ” teacher ” network. CIFAR-100 datasets EVALUATE-FOR small and competitive networks. CIFAR-10 EVALUATE-FOR small and competitive networks. transfer learning USED-FOR difficulty. approach COMPARE Self - Paced Learning. Self - Paced Learning COMPARE approach. Metric is difficulty measure. OtherScientificTerm is ” teacher ” network. Generic is method. ,"-based self-paced learning is a popular method for training deep neural networks (DNNs). However, the difficulty of mini-batches is not well understood. This paper proposes to use the difficulty measure as a way to measure the difficulty in learning. The difficulty measure is defined as the difference between the performance of a small network and a large network trained on the same dataset with the same difficulty measure. The authors show that this difficulty measure can be used to improve the performance on CIFAR-10/100.  ",This paper proposes a curriculum learning approach to improve the performance of small and competitive CNNs. The authors propose a method to learn the difficulty of mini-batch learning by using the Imagenet database and a competitive teacher network. The method is evaluated on CIFAR-10 and Cifar-100 datasets. The results show that the proposed method outperforms the state-of-the-art Self-Paced Learning method. 
6864,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"generalization guarantees FEATURE-OF neural networks. pre - activation values FEATURE-OF network. Method are overparameterized deep networks, stochastic gradient descent ( SGD ), and PAC - Bayesian framework. OtherScientificTerm are small random noise, weight matrices, wide training loss minimum, and wide test loss minimum. Generic are approach, framework, matrices, and prior approaches. Metric is generalization guarantee. ","This paper studies the generalization properties of overparameterized neural networks trained with stochastic gradient descent (SGD) in the presence of small random noise. The authors propose a PAC-Bayesian framework, where the weight matrices of the training and test matrices are drawn from the same distribution. They show that under certain assumptions on the training loss and the test loss, the wide training loss minimum and wide test loss minimum are necessary and sufficient for generalization to be guaranteed. They also provide a generalization bound for this framework.","This paper proposes a PAC-Bayesian framework for overparameterized deep neural networks. The main contribution of the paper is to provide a generalization guarantee for the generalization of neural networks with a wide training loss minimum and wide test loss minimum. The generalization guarantees are based on the assumption that the weight matrices of the weights of a neural network are not too large and that the training loss is not too small. The authors show that under certain assumptions on the weight matrix of the neural network, they can guarantee a wide generalization."
6868,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"Deep neural networks USED-FOR symbolic reasoning. Deep neural networks USED-FOR learning abstractions. symbolic reasoning CONJUNCTION learning abstractions. learning abstractions CONJUNCTION symbolic reasoning. discrete latent variables FEATURE-OF Deep neural networks. perplexity EVALUATE-FOR VAE. CIFAR-10 EVALUATE-FOR VAE. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR VAE. training technique USED-FOR VQ - VAE. it CONJUNCTION sequence level knowledge distillation. sequence level knowledge distillation CONJUNCTION it. nonautoregressive machine translation model COMPARE greedy autoregressive baseline Transformer. greedy autoregressive baseline Transformer COMPARE nonautoregressive machine translation model. accuracy EVALUATE-FOR greedy autoregressive baseline Transformer. it USED-FOR nonautoregressive machine translation model. sequence level knowledge distillation USED-FOR nonautoregressive machine translation model. accuracy EVALUATE-FOR nonautoregressive machine translation model. EM USED-FOR discrete autoencoder. Method are discrete latent variable models, vector quantized autoencoders ( VQ - VAE ), and Expectation Maximization ( EM ) algorithm. Task is inference. ","This paper proposes a novel training method for discrete latent variable models, called vector quantized autoencoders (VQ-VAE), which is based on the Expectation Maximization (EM) algorithm. Theoretical results show that the proposed method is able to improve the perplexity on CIFAR-10 and ImageNet. Experiments are conducted on machine translation tasks and show that it outperforms the baseline Transformer models. ",This paper proposes a new training method for vector quantized autoencoders (VQ-VAE) that is based on the Expectation Maximization (EM) algorithm. The authors show that EM can improve the performance of the VQ-VAE on CIFAR-10 and Cifar-100 datasets. They also show that it can improve performance on a non-autoregressive machine translation model.
6872,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"Multi - view learning USED-FOR self - supervision. Distributional hypothesis USED-FOR self - supervision. multi - view frameworks USED-FOR sentence representations. learning architectures USED-FOR sentence meaning. generative objective CONJUNCTION discriminative one. discriminative one CONJUNCTION generative objective. generative objective USED-FOR framework. multi - view frameworks COMPARE single - view learnt counterparts. single - view learnt counterparts COMPARE multi - view frameworks. multi - view frameworks USED-FOR representations. single - view learnt counterparts USED-FOR representations. Material is large unlabelled corpora. Generic are frameworks, and representation. Method are Recurrent Neural Network ( RNN ), and linear model. Task is downstream tasks. ",This paper proposes a multi-view self-supervised learning framework for sentence representation learning. The proposed framework is based on the Distributional Hypothesis (DH) and aims to learn sentence representations from multiple views of the same sentence. The authors propose a generative objective and a discriminative objective to learn the representations. The experiments show that the proposed framework outperforms the single-view learning methods on a variety of downstream tasks. ,"This paper proposes a new framework for multi-view learning for self-supervision. The framework is based on the distributional hypothesis, which claims that the representation learned by a single-view learner can be better than the representations learned by multiple-view learners. The authors propose a generative objective and a discriminative objective, which are used to learn the representation. The proposed framework is evaluated on a variety of downstream tasks, and compared with a single view learner."
6876,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"Distributed optimization USED-FOR large - scale machine learning problems. stragglers HYPONYM-OF slow nodes. Anytime Minibatch HYPONYM-OF online distributed optimization method. fixed communication time FEATURE-OF minibatch gradients. consensus USED-FOR minibatch gradients. dual averaging USED-FOR primal variables. approach COMPARE it. it COMPARE approach. Amazon EC2 EVALUATE-FOR approach. Method are distributed optimization techniques, and convergence analysis. OtherScientificTerm are gradients, and compute node performance. Metric is wall time. ","This paper proposes an online distributed optimization method called Anytime minibatch, which is able to handle stragglers in distributed optimization. The proposed method is based on the idea that the gradients of minibatches can be computed with a fixed communication time. The main contribution of the paper is the convergence analysis of the proposed method, which shows that the method converges to the optimal solution in a fixed amount of time.  ","This paper proposes a new online distributed optimization method called Anytime Minibatch, which aims to reduce the communication time of minibatch gradients. The proposed method is based on the dual averaging of primal variables. The authors show that the proposed method outperforms the state-of-the-art methods on Amazon EC2."
6880,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,"approach USED-FOR reinforcement learning. task - independent intrinsic reward function USED-FOR approach. peripheral pulse measurements USED-FOR task - independent intrinsic reward function. reward functions USED-FOR sparse and skewed rewards. reward functions USED-FOR reinforcement learning settings. reward functions USED-FOR sample efficiency. sparse and skewed rewards FEATURE-OF reinforcement learning settings. it USED-FOR learning. simulated driving environment EVALUATE-FOR this. OtherScientificTerm are intrinsic feedback, Physiological changes, biological preparations, and human autonomic nervous system responses. Task is learning stage. ","This paper proposes a method to learn a task-independent intrinsic reward function for reinforcement learning using peripheral pulse measurements. The method is based on the observation that reward functions in reinforcement learning can be biased in sparse and skewed rewards, which can lead to poor sample efficiency. To address this issue, the authors propose to use a reward function that is independent of the reward function in the environment. The proposed method is evaluated on a simulated driving environment and shows that the proposed method outperforms the baselines. ",This paper proposes a novel approach to learning a task-independent intrinsic reward function for reinforcement learning. The authors propose to use peripheral pulse measurements as a surrogate reward function to measure the human autonomic nervous system responses to the environment. They show that the proposed method can be used to learn a reward function that is independent of the environment and can be applied to sparse and skewed rewards. They also show that their method can improve sample efficiency in a simulated driving environment. 
6884,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"they USED-FOR predictive distributions. NPs USED-FOR conditional distributions. NPs USED-FOR observed data. attention PART-OF NPs. Method is Neural Processes ( NPs ). OtherScientificTerm are regression functions, and functions. Metric is linear complexity. Task is underfitting. Generic is this. ","This paper studies the problem of training neural processes (NPs) to predict conditional distributions in regression problems. The authors show that under certain conditions, NPs can learn to predict the conditional distributions of the observed data. The main contribution of the paper is to show that NPs are able to learn conditional distributions that are close to the true distribution in terms of the number of observations. ","This paper studies the problem of underfitting neural processes (NPs) in the context of regression. In particular, the authors consider the case where the NPs are trained to predict the conditional distributions of the observed data. They show that underfitting can happen when the attention function of NPs is linear in complexity. They also show that the underfitting problem can be reduced to a linear problem if the attention functions are trained with linear complexity. The main contribution of this paper is to study the problem in terms of the linear complexity of attention functions."
6888,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,credit assignment USED-FOR pre - adaptation behavior. sample - efficiency EVALUATE-FOR metatraining. credit assignment USED-FOR gradient - based Meta - RL. meta - learning algorithm USED-FOR estimating meta - policy gradients. meta - learning algorithm USED-FOR poor credit assignment. algorithm USED-FOR meta - learning. statistical distance FEATURE-OF pre - adaptation and adapted policies. pre - adaptation and adapted policies USED-FOR meta - policy search. approach COMPARE Meta - RL algorithms. Meta - RL algorithms COMPARE approach. approach USED-FOR pre - adaptation policy behavior. wall - clock time CONJUNCTION asymptotic performance. asymptotic performance CONJUNCTION wall - clock time. sample - efficiency CONJUNCTION wall - clock time. wall - clock time CONJUNCTION sample - efficiency. asymptotic performance EVALUATE-FOR Meta - RL algorithms. wall - clock time EVALUATE-FOR Meta - RL algorithms. asymptotic performance EVALUATE-FOR approach. wall - clock time EVALUATE-FOR approach. sample - efficiency EVALUATE-FOR Meta - RL algorithms. sample - efficiency EVALUATE-FOR approach. Task is Credit assignment. Generic is it. Method is task identification strategies. OtherScientificTerm is meta - policy gradients. ,"This paper proposes a meta-learning algorithm for estimating meta-policy gradients in the meta-RL setting. The proposed method is based on credit assignment, which is used to estimate the gradients of meta-adapted and meta-learned policies. The paper shows that credit assignment can be used to improve the sample-efficiency of the proposed method. The method is evaluated on a set of synthetic and real-world datasets.","This paper proposes a meta-learning algorithm for estimating meta-policy gradients in Meta-RL. The main idea is to use credit assignment as a metric to measure the distance between the pre-adapted and adapted policies. The paper also proposes a method to estimate the statistical distance of the preadapted policies and the adapted policies, which can be used to improve the sample-efficiency of meta-RL algorithms. Experiments show that the proposed method outperforms the state-of-the-art in terms of wall-clock time and asymptotic performance. "
6892,SP:be5f2c827605914206f5645087b94a50f59f9214,"classifier USED-FOR satisfiability. NeuroSAT HYPONYM-OF message passing neural network. message passing neural network USED-FOR SAT problems. it COMPARE SAT solvers. SAT solvers COMPARE it. NeuroSAT USED-FOR problems. SAT solvers COMPARE NeuroSAT. NeuroSAT COMPARE SAT solvers. it COMPARE NeuroSAT. NeuroSAT COMPARE it. clique detection CONJUNCTION dominating set. dominating set CONJUNCTION clique detection. graph coloring CONJUNCTION clique detection. clique detection CONJUNCTION graph coloring. it USED-FOR SAT problems. dominating set CONJUNCTION vertex cover problems. vertex cover problems CONJUNCTION dominating set. NeuroSAT USED-FOR distributions. graph coloring HYPONYM-OF SAT problems. clique detection HYPONYM-OF SAT problems. OtherScientificTerm are random SAT problems, and small random graphs. ","This paper proposes NeuroSAT, a message-passing neural network for solving SAT problems on small random graphs. The main idea is to learn a satisfiability-based classifier for satisfiability of small graphs, where satisfiability is defined as the number of nodes that satisfy a given satisfiability condition. The proposed method is evaluated on graph coloring, clique detection, and dominating set problems. ","This paper proposes NeuroSAT, a message-passing neural network for solving SAT problems. The main idea is to use a message passing neural network to solve SAT problems, where the goal is to solve a set of small random graphs. The goal is that the solver should be able to solve the problem with high satisfiability, i.e., that the problem is solvable with high satisfaction.  The main contributions of the paper are:  1. A new SAT solver is proposed for solving small random graph SAT problems with high Satisfiability. 2. The solver can be used to solve graph coloring, clique detection, dominating set, and vertex cover problems. 3. The method is evaluated on a variety of SAT solvers.  "
6896,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,policy USED-FOR autonomous driving. imitation learning USED-FOR policy. behavior cloning USED-FOR complex driving scenarios. perception system CONJUNCTION controller. controller CONJUNCTION perception system. perturbations FEATURE-OF expert ’s driving. synthesized data USED-FOR learner. perturbations FEATURE-OF synthesized data. robustness EVALUATE-FOR model. losses USED-FOR imitation loss. causal factors FEATURE-OF model. OtherScientificTerm is collisions. ,"This paper studies the problem of imitation learning in autonomous driving. The authors propose a new loss function for imitation learning, which they call the ""behavior cloning loss"". They show that this loss function is robust to perturbations in the environment, and that it can be used to train a policy that can imitate the behavior of an expert policy. They also show that the imitation loss is robust against collisions.   ","This paper studies the problem of imitation learning in the context of autonomous driving. In particular, the authors propose a new loss function for imitation learning, which they call the imitation loss. The imitation loss consists of two components: (1) an imitation loss for the learner, and (2) a robustness loss to perturbations in the environment. The first part of the loss is based on the fact that the model is trained on a synthetic dataset, and the second part is on the real data. The empirical results show that the proposed imitation loss is more robust than the original imitation loss in terms of robustness to collisions.   "
6900,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"language modeling CONJUNCTION content recommendation. content recommendation CONJUNCTION language modeling. machine learning models USED-FOR tasks. content recommendation CONJUNCTION advertising. advertising CONJUNCTION content recommendation. image classification CONJUNCTION language modeling. language modeling CONJUNCTION image classification. data USED-FOR machine learning models. data USED-FOR tasks. advertising HYPONYM-OF tasks. image classification HYPONYM-OF tasks. language modeling HYPONYM-OF tasks. content recommendation HYPONYM-OF tasks. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. dataset USED-FOR model. SVHN EVALUATE-FOR approach. CIFAR10 EVALUATE-FOR approach. Material are text, and images. Method are small proxy model, and large target model. ","This paper proposes a method for training image classification models on large sets of images. The idea is to train a proxy model on the large set of images and then train a large target model on a small subset of the images, where the proxy model is trained on a subset of images, and the target model is used as the input to the large model. The paper shows that the proposed method outperforms baselines on CIFAR-10 and SVHN.","This paper proposes a method for training a proxy model for image classification and language modeling. The proxy model is trained on a large dataset of images and text, and the target model on a small dataset of text. The paper shows that the proxy model can outperform the large target model in terms of performance on CIFAR-10 and SVHN. "
6904,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"formulation of planning USED-FOR it. probabilistic inference problem USED-FOR formulation of planning. probabilistic inference problem USED-FOR it. classical methods USED-FOR control. classical methods USED-FOR inference. Sequential Monte Carlo CONJUNCTION Bayesian smoothing. Bayesian smoothing CONJUNCTION Sequential Monte Carlo. Bayesian smoothing USED-FOR control. classical methods USED-FOR Sequential Monte Carlo. classical methods USED-FOR Bayesian smoothing. inference USED-FOR control. classical methods USED-FOR Sequential Monte Carlo Planning. classical methods USED-FOR algorithm. Sequential Monte Carlo Planning USED-FOR continuous control tasks. Sequential Monte Carlo Planning USED-FOR multimodal policies. Method is sampling methods. OtherScientificTerm are continuous domains, and fixed computational budget. ","This paper studies the problem of sequential Monte Carlo planning in continuous control problems, where the goal is to find a policy that maximizes the expected return of the current state-action pair. The authors propose to use a Bayesian smoothing approach to estimate the distribution of the state-actions, and then use this information to compute a sampling strategy for planning. They show that this sampling strategy is computationally efficient, and they show that it can be used in conjunction with classical methods such as Sequential Monte Carlo (SMC) and Bayesian Smoothing (BMS). They also show that their sampling strategy can be combined with Bayesian Bayes smoothing to find the optimal policy. ","This paper studies the problem of planning in continuous control. The authors propose a probabilistic formulation of planning for continuous control, where the planning problem is formulated as an inference problem. The main contribution of the paper is the formulation of the problem in terms of Bayesian smoothing and sequential Monte Carlo planning. They show that the classical methods for planning can be used to solve the problem. They also show that their method can be applied to a variety of continuous control tasks. "
6908,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"Neural networks USED-FOR small adversarial perturbations. Adversarial robustness COMPARE clean accuracy. clean accuracy COMPARE Adversarial robustness. adversarial training HYPONYM-OF robust training method. robustness EVALUATE-FOR adversarial trained model. semantics - preserving transformations FEATURE-OF data distribution. distribution USED-FOR adversarial trained model. clean accuracy CONJUNCTION robust accuracy. robust accuracy CONJUNCTION clean accuracy. clean accuracy EVALUATE-FOR Bayes classifier. robust accuracy EVALUATE-FOR Bayes classifier. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. semantically - identical variants USED-FOR CIFAR10. semantically - identical variants USED-FOR MNIST. robustness accuracies EVALUATE-FOR adversarially trained models. adversarial robustness EVALUATE-FOR neural networks. Generic are models, and them. Metric is clean accuracies. OtherScientificTerm is input data distribution. ","This paper studies the relationship between adversarial robustness and clean accuracy in deep neural networks. The authors show that adversarial training improves clean accuracy by a large margin. They also show that the clean accuracy of adversarial trained models can be improved by a small amount of clean accuracy. The main contribution of this paper is to show that clean accuracy is not the only measure of robustness, but also robustness to small adversarial perturbations.",This paper proposes a new metric for measuring the robustness of neural networks against adversarial perturbations. The main idea is to measure the clean accuracy of a neural network trained with adversarial training. The clean accuracy is defined as the distance between the true accuracy and the true robust accuracy of the trained model. The robust accuracy is measured as the difference between the clean and adversarial accuracy. The authors show that the robust accuracy can be better than clean accuracy for Bayes-trained models. They also show that adversarial robustness can be improved with the use of semantics-preserving transformations.
6912,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,transformation of layer weights COMPARE layer outputs. layer outputs COMPARE transformation of layer weights. normalization technique USED-FOR batch normalization. transformation of layer weights USED-FOR normalization technique. positive and negative weights USED-FOR layer output. SVHN CONJUNCTION ILSVRC 2012 ImageNet. ILSVRC 2012 ImageNet CONJUNCTION SVHN. CIFAR-10/100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-10/100. CIFAR-10/100 HYPONYM-OF benchmarks. ILSVRC 2012 ImageNet HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR method. SVHN HYPONYM-OF benchmarks. Generic is technique. ,"This paper proposes a new normalization technique for batch normalization, which is based on the transformation of layer weights. The key idea is to use a combination of positive and negative weights to normalize the layer output. Theoretical analysis and experimental results show that the proposed method outperforms the baselines on CIFAR-10/100 and SVHN. ","This paper proposes a new normalization technique for batch batch normalization. The main idea is to use the transformation of layer weights (positive and negative weights) instead of layer output. The authors show that the proposed method outperforms the state-of-the-art on SVHN, CIFAR-10/100, and ILSVRC 2012 ImageNet. "
6916,SP:8188f15c8521099305aa8664e05f102ee6cea402,Memorization USED-FOR over - parameterized neural networks. Memorization USED-FOR generalization. implicit regularization effect USED-FOR stochastic gradient descent. learning rates FEATURE-OF implicit regularization effect. learning rates FEATURE-OF stochastic gradient descent. loss statistics USED-FOR mislabeled examples. algorithm USED-FOR mislabeled examples. artificial and real - world mislabeled examples FEATURE-OF datasets. datasets EVALUATE-FOR ODD. Method is DATA DENOISING ( ODD ). OtherScientificTerm is computational overhead. ,This paper studies the problem of data denoising in over-parameterized neural networks. The authors propose a new method called Data Denoising (ODD) to detect mislabeled examples in the training data. The main idea is to use the implicit regularization effect of stochastic gradient descent (SGD) as a regularization term in SGD. The paper shows that SGD can be seen as a special case of the data-de-noising (DDD) method. Theoretical analysis is provided to show that ODD is computationally efficient. Experiments are conducted on synthetic and real-world datasets to demonstrate the effectiveness of the proposed method.,"This paper studies the problem of data denoising in over-parameterized neural networks. The authors propose a new method to identify mislabeled examples in the training data, which they call Data Denoising (DDD). The main idea is to use the implicit regularization effect of stochastic gradient descent (SGD) to estimate the learning rate of SGD with respect to the learning rates of the true loss statistics. They show that ODD can be used to reduce the computational overhead of ODD. They also show that the ODD method can be applied to both artificial and real-world datasets."
6920,SP:fbf023a772013e6eca62f92982aecf857c16a428,Pretrained language models USED-FOR downstream NLP task. analysis framework USED-FOR pretraining and downstream tasks. latent variables FEATURE-OF posterior distribution. latent variable generative model of text USED-FOR pretraining and downstream tasks. head tuning CONJUNCTION prompt tuning. prompt tuning CONJUNCTION head tuning. frozen pretrained model USED-FOR classifier. Hidden Markov Model ( HMM ) CONJUNCTION HMM. HMM CONJUNCTION Hidden Markov Model ( HMM ). latent memory component USED-FOR long - term dependencies. natural language FEATURE-OF long - term dependencies. Hidden Markov Model ( HMM ) HYPONYM-OF generative model. HMM HYPONYM-OF generative model. latent memory component USED-FOR HMM. classification heads USED-FOR downstream task. non - degeneracy conditions FEATURE-OF HMM. memory - augmented HMM COMPARE vanilla HMM. vanilla HMM COMPARE memory - augmented HMM. prompt tuning USED-FOR downstream guarantees. recovery guarantees FEATURE-OF memory - augmented HMM. non - degeneracy conditions FEATURE-OF downstream guarantees. long - term memory USED-FOR task - relevant information. HMMs USED-FOR synthetically generated data. Generic is models. Task is downstream classifier. ,This paper proposes to use a latent variable generative model of text to improve the performance of pretrained language models on downstream NLP tasks. The proposed model is based on a generative latent variable model (HMM) with a latent memory component that stores long-term dependencies in the posterior distribution of the latent variables. The authors show that the proposed model improves the performance on downstream tasks when the classification heads are frozen and the prompt tuning is performed on the downstream task. ,This paper proposes a generative model for latent variable generative models (HMM) that can be used for pretraining and downstream tasks. The model is based on the Hidden Markov Model (HMM) with a memory-augmented latent memory component that stores long-term dependencies between the latent variables of the model and the downstream task. The authors show that the HMM can recover the task-relevant information in the latent memory of the HMM. They also provide a theoretical analysis of the performance of HMM under different head tuning and prompt tuning conditions. 
6936,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,invariant features USED-FOR classifier. transferability USED-FOR domain generalization. total variation CONJUNCTION Wasserstein distance. Wasserstein distance CONJUNCTION total variation. algorithms USED-FOR domain generalization. feature embeddings USED-FOR domain generalization. transferability FEATURE-OF feature embeddings. algorithms USED-FOR feature embeddings. algorithms USED-FOR transferability. RotatedMNIST CONJUNCTION PACS. PACS CONJUNCTION RotatedMNIST. PACS CONJUNCTION Office - Home. Office - Home CONJUNCTION PACS. algorithm USED-FOR transferable features. benchmark datasets EVALUATE-FOR it. RotatedMNIST HYPONYM-OF benchmark datasets. Office - Home HYPONYM-OF benchmark datasets. PACS HYPONYM-OF benchmark datasets. algorithm COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithm. Task is Out - of - distribution generalization. Generic is model. OtherScientificTerm is transferable ” features. ,"This paper studies the problem of out-of-distribution generalization in the presence of transferable features, i.e. features that can be used to improve the generalization performance of a classifier in a new domain. The authors propose a new algorithm that uses total variation and Wasserstein distance to learn transferable embeddings of features from a given domain. They show that the total variation is a function of the distance between the feature embedding and the distribution over the new domain, and show that this distance can be expressed as a sum of total variation with respect to the distribution of the original domain and the transferable feature. They also show that transferability of the feature is related to the transferability between the new and old domains.   ","This paper studies the problem of transferability of feature embeddings for out-of-distribution (OOD) generalization. The authors propose a new algorithm for learning transferable features from a set of features that are out of distribution. The main contribution of the paper is to propose an algorithm that can learn transferable feature embedding from a subset of features. The proposed method is evaluated on RotatedMNIST, PACS and Office-Home datasets. "
6952,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"Reward USED-FOR reinforcement - learning agents. partial ordering CONJUNCTION partial ordering. partial ordering CONJUNCTION partial ordering. reward USED-FOR tasks. polynomial - time algorithms USED-FOR Markov reward function. OtherScientificTerm are acceptable behaviors, and reward function. ","This paper studies the problem of reward-based reinforcement learning in the presence of partial ordering and partial ordering. In particular, the authors show that the reward function can be computed in polynomial time in terms of a Markov reward function. The authors then show that this reward function is a function of the number of states and actions that are available to the agent, and that the agent can be trained to find states that satisfy this function.   The authors also show that in the case where the agent has access to a set of state-action pairs that satisfy the partial ordering condition, it is possible to learn a reward function that maximizes the expected return of the agent. ","This paper studies the problem of learning a Markov reward function for reinforcement learning agents. The authors show that the reward function is polynomial-time in terms of the number of acceptable behaviors, and that it can be computed polynomially. They also show that a Poisson-time algorithm can be used to solve the problem.   "
6968,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"approaches USED-FOR generalization. sequential structure FEATURE-OF RL problem. approaches USED-FOR RL problem. approaches USED-FOR sequential structure. implicit partial observability USED-FOR generalization. generalization USED-FOR RL. epistemic POMDP HYPONYM-OF induced partially observed Markov decision process. induced partially observed Markov decision process USED-FOR generalization. ensemble - based technique USED-FOR partially observed problem. algorithm COMPARE methods. methods COMPARE algorithm. generalization EVALUATE-FOR methods. epistemic POMDP USED-FOR algorithm. generalization EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR algorithm. Procgen benchmark suite EVALUATE-FOR methods. Task is Generalization. Method are reinforcement learning ( RL ) systems, supervised learning, supervised learning methods, and POMDPs. OtherScientificTerm are epistemic uncertainty, fullyobserved MDPs, failure modes of algorithms, and partial observability. ","This paper studies the problem of generalization in partially observed MDPs in reinforcement learning. The authors propose an ensemble-based algorithm for partially observed POMDPs, which is able to generalize better than existing methods. The proposed method is based on the observation of a partially observed Markov decision process (POMDP), which is induced by the epistemic uncertainty of the MDP.  The authors show that the proposed method outperforms existing methods on the Procgen benchmark. ","This paper proposes an ensemble-based algorithm for partially observed Markov Decision Processes (POMDPs) in reinforcement learning (RL). The authors show that the partially observed MDPs are more general than the fully observed ones, and that the generalization performance of the proposed algorithm is better than the state-of-the-art in terms of generalization. The authors also provide a theoretical analysis of the effect of partial observability on generalization in the context of the POMDP problem."
6984,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,Hessian matrix of value functions USED-FOR Model - agnostic meta - reinforcement learning. framework USED-FOR higherorder derivatives of value functions. off - policy evaluation USED-FOR framework. framework USED-FOR prior approaches. framework USED-FOR estimates. auto - differentiation libraries USED-FOR estimates. OtherScientificTerm is biased Hessian estimates. , is a model-agnostic meta-reinforcement learning framework for meta-learning. The authors propose to use the Hessian matrix of value functions as a surrogate for the true Hessian of the reward function. The Hessian is estimated using off-policy evaluation. The paper shows that the proposed method outperforms the baselines in terms of performance.  ,This paper proposes a meta-reinforcement learning framework for model-agnostic meta-learning with Hessian matrix of value functions (Hessian matrix) for model agnostic meta-regression. The proposed framework is based on off-policy evaluation and auto-differentiation libraries. The main contribution of the paper is to propose a framework for learning Hessian matrices of higher-order derivatives of the value functions. The authors show that the proposed framework can be applied to a wide range of prior approaches. 
7000,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"approach USED-FOR communication constraints. approach USED-FOR distributed learning problem. communication constraints FEATURE-OF distributed learning problem. central server USED-FOR distributed learning problem. algorithm COMPARE algorithms. algorithms COMPARE algorithm. algorithm USED-FOR bidirectional compression. convergence rate EVALUATE-FOR algorithms. convergence rate EVALUATE-FOR algorithm. MCM HYPONYM-OF algorithm. gradients FEATURE-OF local servers. perturbed models USED-FOR gradients. model compression CONJUNCTION memory mechanism. memory mechanism CONJUNCTION model compression. memory mechanism PART-OF MCM. model compression PART-OF MCM. worker dependent randomized - models CONJUNCTION partial participation. partial participation CONJUNCTION worker dependent randomized - models. Method is downlink compression. OtherScientificTerm are local models, global model, and perturbation. Task is convergence proofs. ","This paper studies the distributed learning problem in which the gradients of the local models are perturbed by the perturbation of the global model. The authors propose a new algorithm, called MCM, which is based on a combination of model compression and memory mechanism. Theoretical convergence analysis is provided for both bidirectional and downlink compression.  ",This paper proposes a new algorithm for bidirectional compression for distributed learning with communication constraints. The main idea is to use a perturbed global model and perturbed local models to compress the gradients of the local models. The authors show that the proposed algorithm can converge faster than the state-of-the-art in terms of convergence rate. They also show that their algorithm can be combined with a memory mechanism and model compression. 
7016,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"causal inference USED-FOR stress testing. counterfactual invariance USED-FOR outof - domain model. causal structure USED-FOR counterfactual invariance. regularization schemes USED-FOR counterfactual invariance. regularization schemes USED-FOR causal structures. causal structure FEATURE-OF domain shift guarantees. domain shift guarantees FEATURE-OF counterfactual invariance. OtherScientificTerm are spurious correlation, spurious correlations, and counterfactual examples. Method are model, and machine learning. Generic are models, and schemes. Task is text classification. ","This paper studies the problem of counterfactual invariance in out-of-domain causal inference, i.e., the ability of a model to learn counterfactually invariant causal structures in the presence of spurious correlations. The authors propose two regularization schemes to improve the robustness of the model to spurious correlations, and show that the proposed regularization scheme is robust to domain shift. They show that under certain assumptions on the causal structure, the proposed method can be used to obtain counterfactuality invariance to spurious correlation. ","This paper studies the problem of counterfactual invariance of out-of-domain models in the context of stress testing. In particular, the authors consider the case where a model is trained on a set of out of domain examples, and then the model is used to train a model that is invariant to counterfactually spurious correlations. The authors show that under certain conditions, the model can be invariant against spurious correlations, and that this invariance does not depend on the domain of the model, but rather on the causal structure of the data. They show that the invariance can be obtained by regularizing the model with a series of regularization schemes. They also provide a theoretical analysis of the properties of these regularizations, showing that they can be used to obtain domain shift guarantees."
7032,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"limited data USED-FOR GANs. data augmentations CONJUNCTION model regularization. model regularization CONJUNCTION data augmentations. generator USED-FOR real data distribution. APA USED-FOR overfitting. method COMPARE approaches. approaches COMPARE method. generator USED-FOR APA. model regularization USED-FOR approaches. data augmentations USED-FOR approaches. APA USED-FOR low - data regime. low - data regime FEATURE-OF synthesis quality. synthesis quality EVALUATE-FOR APA. It USED-FOR GANs. StyleGAN2 HYPONYM-OF GANs. Method are Generative adversarial networks ( GANs ), and Adaptive Pseudo Augmentation ( APA ). Material are high - fidelity images, and generated images. OtherScientificTerm are discriminator overfitting, and discriminator. Generic are strategy, and training strategy. ","This paper proposes Adaptive Pseudo Augmentation (APA), a method to improve the performance of GANs with limited data. APA is based on the observation that the discriminator suffers from overfitting in high-fidelity images, where the generated images are not as good as the real images. To address this issue, APA uses pseudo-augmentation to generate pseudo-images that are close to the real data distribution, which are then used to train the generator to generate high-quality images. The method is evaluated on StyleGAN2 and StyleGAN-C, where it outperforms the baselines in terms of image quality. ","This paper proposes Adaptive Pseudo-Augmentation (APA) for GANs. APA is a method to improve the quality of GAN training by augmenting the training data with pseudo-images. The authors show that APA can be used to improve GAN performance in the low-data regime. They also show that it can be applied to high-fidelity images, and that it is more effective than data augmentation."
7048,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,Causal inference CONJUNCTION discovery. discovery CONJUNCTION Causal inference. observational data USED-FOR discovery. observational data USED-FOR Causal inference. formalization USED-FOR causal inference. Rubin ’s framework USED-FOR multivariate point processes. average treatment effect ( ATE ) CONJUNCTION propensity scores. propensity scores CONJUNCTION average treatment effect ( ATE ). Rubin ’s framework USED-FOR average treatment effect ( ATE ). Rubin ’s framework USED-FOR propensity scores. multivariate recurrent event streams USED-FOR causal inference. multivariate point process USED-FOR data. joint probability distribution USED-FOR i.i.d. data. joint probability distribution COMPARE multivariate point process. multivariate point process COMPARE joint probability distribution. causal inference framework COMPARE baseline pairwise causal association scores. baseline pairwise causal association scores COMPARE causal inference framework. synthetic and real - world event datasets EVALUATE-FOR causal inference framework. Method is point process causal framework. Generic is measure. ,This paper proposes to use a multivariate recurrent event stream to perform causal inference in observational data. The proposed method is based on the notion of average treatment effect (ATE) and propensity scores. The authors show that the ATE and propensity score can be used to estimate the true treatment effect in the case of multivariate point processes. Theoretical and empirical results on synthetic and real-world event datasets demonstrate the effectiveness of the proposed method.,This paper proposes a new causal inference framework for multivariate point process causal inference. The main idea is to use the average treatment effect (ATE) and propensity scores as the two main components of the proposed framework. The ATE is defined as the sum of the treatment effect and the propensity scores. The propensity scores are defined as a weighted average of the two components. The proposed framework is evaluated on synthetic and real-world event datasets. 
7064,SP:5db39fbba518e24a22b99c8256491295048ec417,Graph neural networks ( GNNs ) USED-FOR graph representation learning. residual connections PART-OF message passing. they USED-FOR GNNs. message passing USED-FOR GNNs. abnormal node features FEATURE-OF GNNs. node features PART-OF graphs. GNNs USED-FOR abnormal features. resilience FEATURE-OF GNNs. AirGNN1 HYPONYM-OF GNN. Adaptive residual USED-FOR GNN. Task is real - world applications. OtherScientificTerm is abnormal feature scenarios. Generic is algorithm. ,"This paper proposes to use residual connections in message passing to improve the performance of GNNs in the presence of abnormal node features. Specifically, the authors propose to use the residual connections as a regularization term in the message-passing process. The authors show that the residuals can be used to improve GNN performance in a variety of settings. They also show that using residuals in GNN can improve the generalization performance.","This paper proposes a new GNN architecture, AirGNN1, which is a GNN with adaptive residuals. The main idea is to use the residuals as a regularizer in GNNs, and to use them to learn a new graph representation. The authors show that the proposed method can be used to improve the resilience of the GNN. They also show that it can be applied to a variety of real-world applications."
7080,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"Thompson sampling policy PART-OF Bayesian ‘ optimistic ’ policies. algorithm USED-FOR policies. optimistic set FEATURE-OF policies. zero - sum matrix games CONJUNCTION constrained bandits. constrained bandits CONJUNCTION zero - sum matrix games. regret analysis USED-FOR bilinear saddle - point problems. regret analysis USED-FOR optimistic policies. zero - sum matrix games HYPONYM-OF bilinear saddle - point problems. Thompson sampling USED-FOR policies. policy USED-FOR convex optimization problem. optimistic set FEATURE-OF policy. log - concavity CONJUNCTION unimodality. unimodality CONJUNCTION log - concavity. unimodality CONJUNCTION smoothness. smoothness CONJUNCTION unimodality. procedure USED-FOR posteriors. regularization CONJUNCTION constraints. constraints CONJUNCTION regularization. Task are online sequential decision problems, and stochastic multi - armed bandit case. Metric is Bayesian regret. Generic are problem, and it. OtherScientificTerm are linear regret, posterior, and exploration - exploitation tradeoff. Method is variational Bayesian optimistic sampling ’ ( VBOS ). ","This paper studies the problem of Bayesian Optimistic Sampling (BO) in online sequential decision problems, where the goal is to find a Thompson sampling policy that maximizes the optimistic set of policies in a convex optimization problem. The main contribution of the paper is to show that the regret of the optimistic policy is bounded by the log-concavity and unimodality of the posterior distribution. The regret is shown to be bounded by a linear combination of the log and the unimodal distribution of the policy distribution.  ","This paper studies the problem of online multi-armed bandit problem with Thompson sampling. The authors consider the problem in the setting of a stochastic multi-arm bandit setting, where each bandit has a set of arms, and the goal is to maximize the sum of the total number of arms. They show that the Thompson sampling policy of the bandit can be used to find a policy that maximizes the optimistic set of the ensemble of agents. They also show that this can be done in the case of a convex optimization problem, where the optimal policy is the one that maximises the optimal set of agents in the ensemble.  "
7096,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"without - replacement sampling orders COMPARE uniform - iid - sampling. uniform - iid - sampling COMPARE without - replacement sampling orders. Without - replacement sampling USED-FOR SGD without variance reduction. convergence analysis CONJUNCTION rates of variance reduction. rates of variance reduction CONJUNCTION convergence analysis. without - replacement sampling orders USED-FOR composite finite - sum minimization. rates of variance reduction EVALUATE-FOR without - replacement sampling orders. random reshuffling CONJUNCTION cyclic sampling. cyclic sampling CONJUNCTION random reshuffling. Prox - DFinito HYPONYM-OF Finito. random reshuffling USED-FOR convergence rates. rates EVALUATE-FOR full - batch gradient descent. variance - reduction USED-FOR without - replacement sampling. cyclic order USED-FOR cyclic sampling. variance reduction USED-FOR uniform - iid - sampling. Prox - DFinito USED-FOR data - heterogeneous scenario. optimal cyclic sampling USED-FOR Prox - DFinito. sample - size - independent convergence rate EVALUATE-FOR Prox - DFinito. method USED-FOR optimal cyclic ordering. Method is stochastic algorithm. OtherScientificTerm are convex and strongly convex scenarios, and optimal fixed ordering. ","This paper studies the without-replacement sampling in stochastic gradient descent (SGD) without variance reduction. In particular, the authors show that without replacement sampling orders can be used for composite finite-sum minimization in convex and strongly convex settings. They show that random reshuffling and cyclic sampling can improve the convergence rates of full-batch gradient descent.    The main contributions of this paper are:  1. The authors propose a new algorithm called Prox-DFinito, which is a variant of Finito that uses a cyclic order for sampling.  2. The convergence rate of the proposed algorithm is shown to be sample-size-independent.  3. In the data-h heterogeneous setting, they show that the proposed method achieves sample-independent convergence rates.","This paper studies the convergence rate of without-replacement sampling orders for the composite finite-sum minimization of SGD without variance reduction. The authors propose Prox-DFinito, a variant of Finito, which is based on random reshuffling, cyclic sampling, and cyclic ordering. They show convergence rates of the proposed method for the data-h heterogeneous scenario, and show that it can converge to a sample-size-independent convergence rate.   "
7112,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"algorithmic components USED-FOR reinforcement learning ( RL ) algorithms. relative entropy policy search ( REPS ) USED-FOR policy learning. simulated and real - world robotic domains EVALUATE-FOR policy learning. stochastic, gradient - based solvers USED-FOR REPS. first - order optimization methods USED-FOR REPS objective. sub - optimality FEATURE-OF policy. convergence rates FEATURE-OF sub - optimality. convergence rates FEATURE-OF policy. first - order optimization methods USED-FOR policy. technique USED-FOR parameter updates. generative access USED-FOR parameter updates. generative access USED-FOR Markov decision process. favorable convergence FEATURE-OF parameter updates. Markov decision process USED-FOR technique. generative access USED-FOR technique. OtherScientificTerm are exact gradients, near - optimality, stochastic gradients, and optimal regularized policy. ","This paper studies the relative entropy policy search (REPS) problem with stochastic gradient-based solvers in reinforcement learning. The authors show that REPS is near-optimal with respect to the optimal regularized policy, and propose a method to improve the convergence rate of REPS by using generative access to update the parameters of the policy in a Markov decision process. Theoretical analysis is provided to show the convergence of the method. Empirical results on simulated and real-world robotic domains demonstrate the effectiveness of the proposed method.","This paper studies the relative entropy policy search (REPS) problem in the context of reinforcement learning (RL) with stochastic gradient-based solvers. The main contribution of the paper is to study the convergence rate of REPS in the presence of sub-optimality, i.e., the sub-optimal performance of the policy. The authors show that REPS can converge to the optimal regularized policy with favorable convergence rates. They also show that the rate of convergence can be improved by using generative access for parameter updates.  "
7128,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,deep neural networks ( DNNs ) USED-FOR 3D point cloud processing. knowledge representations USED-FOR 3D point cloud processing. deep neural networks ( DNNs ) USED-FOR knowledge representations. translation CONJUNCTION scale. scale CONJUNCTION translation. scale CONJUNCTION local 3D structures. local 3D structures CONJUNCTION scale. rotation CONJUNCTION translation. translation CONJUNCTION rotation. representation complexity EVALUATE-FOR DNN. metrics CONJUNCTION representation complexity. representation complexity CONJUNCTION metrics. metrics USED-FOR spatial smoothness. metrics EVALUATE-FOR DNN. spatial smoothness FEATURE-OF encoding 3D structures. DNNs USED-FOR representation problems. Generic is method. Method is adversarial training. ,"This paper proposes a method to improve the performance of 3D point cloud representation learning using deep neural networks (DNNs). The proposed method is based on the observation that DNNs can learn 3D representations that are more robust to adversarial perturbations. The authors propose a new metric called ""spatial smoothness"" to measure the smoothness of the 3D representation learned by a DNN. They show that spatial smoothness is a measure of the distance between points in a 3D space and points in the input space.    The authors show that the proposed method achieves better performance than the state-of-the-art methods in terms of accuracy and representation complexity.","This paper proposes a new metric for measuring the spatial smoothness of 3D point cloud representations of deep neural networks (DNNs). The proposed metric is based on the notion of ""spatial smoothness"", which is defined as the distance between two points in the 3D space. The authors show that the proposed metric can be used to measure the distance of a DNN to a point cloud, and that it can also be used as a measure of the representation complexity of DNNs. The paper also provides a theoretical analysis of the performance of the proposed metrics."
7144,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"economics CONJUNCTION game theory. game theory CONJUNCTION economics. game theory CONJUNCTION computer science. computer science CONJUNCTION game theory. economics USED-FOR design of optimal auctions. methods USED-FOR approximating optimal auctions. deep learning USED-FOR approximating optimal auctions. deep learning USED-FOR methods. allocation fairness CONJUNCTION diversity. diversity CONJUNCTION allocation fairness. auction mechanisms USED-FOR socially desirable constraints. diversity HYPONYM-OF socially desirable constraints. allocation fairness HYPONYM-OF socially desirable constraints. neural - network - based auction mechanisms USED-FOR constraints. neural - network - based auction mechanisms USED-FOR PreferenceNet. method COMPARE neural - network based auction designs. neural - network based auction designs COMPARE method. metric EVALUATE-FOR method. metric USED-FOR auction allocations. socially desirable constraints FEATURE-OF auction allocations. human subject research USED-FOR approach. Method is strategyproof, revenuemaximizing auction designs. OtherScientificTerm are restricted settings, optimal auctions, and real human preferences. Generic are baselines, and they. Metric is maximizing revenue. ","This paper proposes a method for optimizing auctions in restricted settings. The method is based on a neural network-based auction mechanism, which is trained to maximize revenue by maximizing allocation fairness, diversity, and allocation fairness. The proposed method is shown to outperform existing methods in terms of revenue maximization.   ","This paper proposes a new method for finding optimal auctions in the context of human subject research. The main idea is to use a neural network-based auction mechanism to estimate the optimal auction allocation under a set of socially desirable constraints, i.e., allocation fairness, diversity, and allocation fairness fairness. The proposed method is tested on a variety of human subjects, and it is shown that the proposed method outperforms the state-of-the-art in terms of revenue maximization."
7160,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"user - level differential privacy USED-FOR personalization of supervised learning. user - level privacy guarantees EVALUATE-FOR approach. non - private approaches USED-FOR algorithms. nearly optimal estimation error guarantees FEATURE-OF algorithms. exponential mechanism - based algorithm USED-FOR information - theoretic upper bound. OtherScientificTerm are shared structure, and joint, user - level differential privacy. Task is linear regression problems. ","This paper studies the problem of user-level differential privacy for linear regression problems with shared structure. In particular, the authors consider the case where the data distribution is shared across users and the goal is to learn a classifier that can be used by users to improve their privacy.    The main contribution of this paper is to provide an efficient algorithm that achieves nearly optimal estimation error guarantees.  The algorithm is based on an exponential mechanism-based algorithm, which is shown to be nearly optimal in terms of estimation error. ","This paper studies the problem of user-level differential privacy for linear regression problems. The main contribution of the paper is to provide an information-theoretic upper bound on the error of a linear regression algorithm. The upper bound is based on an exponential mechanism-based algorithm, and it is proved to be nearly optimal. The paper also provides an empirical evaluation of the upper bound. "
7176,SP:3925fc528de17b8b2e93808f5440ea0503895b75,"human - adversarial examples USED-FOR them. examples EVALUATE-FOR state - of - the - art models. Material are Visual Question Answering dataset ( VQA v2 ), adversarial examples, and Adversarial VQA ( AdVQA ) benchmark. Metric is human accuracy. Method are VQA models, and VQA model. Generic is model. ","This paper introduces Adversarial VQA (AdVQA), a new benchmark for visual question answering. The paper proposes to use adversarial examples to evaluate the performance of visual question-answering models in the presence or absence of human evaluation. The proposed benchmark is based on the Visual Question Answering dataset, which is a collection of human-adversarially-annotated examples. The authors show that the proposed benchmark outperforms the state-of-the-art models in terms of accuracy on the original dataset. ","This paper proposes a new benchmark for Visual Question Answering (VQA) models, Adversarial VQA, which aims to measure the human accuracy of a model against a set of human-adversarial examples. The benchmark is based on an existing dataset, which is called Visual Question answering dataset (v2), which contains both human- and adversarial examples, and is used to evaluate the performance of the model on the dataset. The authors show that the state-of-the-art models can be trained on this dataset, and show that they can outperform the state of the art models on the human-audit dataset."
7192,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"Medial entorhinal cortex ( MEC ) USED-FOR navigational and memory related behaviors. they USED-FOR MEC functionality. they USED-FOR behavior. response profiles FEATURE-OF heterogeneous ” cells. models USED-FOR response profiles. response profiles FEATURE-OF stereotypical and heterogeneous MEC cells. task - optimized neural network models COMPARE grid cell - centric models. grid cell - centric models COMPARE task - optimized neural network models. grid cell - centric models USED-FOR MEC neuronal response profiles. task - optimized neural network models USED-FOR MEC neuronal response profiles. gated nonlinearities CONJUNCTION intermediate place cell representation. intermediate place cell representation CONJUNCTION gated nonlinearities. intermediate place cell representation HYPONYM-OF network architecture. gated nonlinearities HYPONYM-OF network architecture. heterogeneous cells COMPARE grid and border cells. grid and border cells COMPARE heterogeneous cells. heterogeneous cells USED-FOR downstream functional outcomes. path integration HYPONYM-OF downstream functional outcomes. spatial response selectivity FEATURE-OF MEC cells. reward - modulated path integration USED-FOR MEC model. non - spatial rewards FEATURE-OF MEC cells. variable - reward conditions FEATURE-OF neural recordings. OtherScientificTerm are MEC, grid, stereotypical response profiles, MEC neurons, stereotypical firing patterns, heterogeneous MEC cells, and response patterns. Method are computational approach, statistical analysis, and goal - driven modeling approach. Generic is model. Task is Neural Information Processing Systems. ","This paper proposes a method to model the response patterns of neurons in the medial entorhinal cortex (MEC) using a goal-driven modeling approach. The method is based on the observation that MEC cells are heterogeneous, with different response patterns in different regions of the MEC. The authors propose to model this heterogeneous MEC response patterns using a combination of gated nonlinearity and intermediate place cell representation. They show that the proposed method is able to capture the spatial response selectivity and non-reward-modulated path integration of MEC neurons.","This paper proposes a goal-driven neural network model for predicting the response patterns of MEC neurons. The model is based on the notion of heterogeneous MEC cells, where heterogeneous cells are composed of two types of neurons: stereotypical and heterogeneous. The authors show that the heterogeneous response patterns are more diverse than the stereotypical response patterns in the MEC. They also show that their model is able to predict the spatial response selectivity of the heterogenous cells.  "
7208,SP:57f9812fa5e7d0c66d412beb035301684d760746,"them USED-FOR physical real - world tasks. KL - regularized reinforcement learning USED-FOR deep reinforcement learning algorithms. sample efficiency EVALUATE-FOR deep reinforcement learning algorithms. KL - regularized reinforcement learning USED-FOR them. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency EVALUATE-FOR KL - regularized reinforcement learning. behavioral reference policies USED-FOR KL - regularized reinforcement learning. expert demonstrations USED-FOR behavioral reference policies. expert demonstrations USED-FOR KL - regularized reinforcement learning. sample efficiency CONJUNCTION online policy. online policy CONJUNCTION sample efficiency. KL - regularized reinforcement learning COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR KL - regularized reinforcement learning. locomotion and dexterous hand manipulation tasks EVALUATE-FOR state - of - the - art approaches. non - parametric behavioral reference policies USED-FOR pathology. OtherScientificTerm are pathological training dynamics, and behavioral policy classes. Method is online learning. ","This paper proposes a KL-regularized reinforcement learning (KL-RL) method to improve the sample efficiency of RL algorithms in the presence of pathological training dynamics. The main idea is to learn a set of behavioral reference policies from expert demonstrations, which are then used to train a policy classifier. The KL-RL method is shown to improve sample efficiency and online policy performance on a variety of physical tasks. ","This paper proposes a new method for KL-regularized reinforcement learning (KL-RRL) that uses expert demonstrations to train behavioral reference policies. The key idea is to use the expert demonstrations as a reference policy to train a KL-based RL algorithm. The expert demonstrations are generated by a set of behavioral reference policy classes, which are learned by KL-trained RL algorithms. The goal is to improve the sample efficiency of the RL algorithm by using expert demonstrations. The method is evaluated on a variety of real-world tasks, including locomotion and dexterous hand manipulation tasks."
7224,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"teacher - student framework USED-FOR kernel regression. neural tangent kernel PART-OF convolutional architectures. neural tangent kernel USED-FOR convolutional ’ kernels. filter size FEATURE-OF convolutional architectures. teacher - student framework USED-FOR problem. convolutional ’ kernels USED-FOR teacher - student framework. convolutional ’ kernels USED-FOR problem. locality USED-FOR learning curve exponent β. physics USED-FOR heuristic methods. ridge USED-FOR kernel regression. Method is Convolutional neural networks. OtherScientificTerm are ridgeless case, translational invariance, natural universality assumption, and learning curve exponents. ","This paper studies the problem of kernel regression in convolutional neural networks. The authors propose a teacher-student framework for kernel regression, where the teacher and student kernels are trained in a linear combination. They show that the learning curve exponents of the teacher kernels are invariant to the number of filters in the network. They also show that in the ridgeless case, the exponents are also translational invariant.  ","This paper proposes a teacher-student framework for kernel regression, where the teacher and student are given a teacher and a student, and the student is given a student. The teacher is a student with the student in the sense that the student has access to the teacher’s knowledge of the teacher. The student is a teacher with the knowledge of a teacher, which is the student with access to a teacher.  The teacher student is motivated by the fact that the learning curve exponents of the student and teacher are not translational invariant, and that the teacher can learn the student's learning curve exponent. The paper shows that under certain conditions, the student can learn a learning curve that is invariant to the student learning curve. The main contribution of the paper is to show that this is the case in the ridgeless case, and to prove that the learner can learn such a learner's exponents. "
7240,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"Variational Autoencoders ( VAEs ) USED-FOR representations of complex data distributions. Variational Autoencoders ( VAEs ) HYPONYM-OF probabilistic models. probabilistic models USED-FOR representations of complex data distributions. model USED-FOR latent representations. uni - modal Gaussian distribution USED-FOR latent representations. regularized autoencoders USED-FOR deterministic autoencoding framework. ex - post density estimation step USED-FOR they. latent space FEATURE-OF model. deterministic autoencoding framework USED-FOR latent space. expressive multi - modal latent distributions USED-FOR deterministic autoencoding framework. latent distribution USED-FOR encoded data. sample quality EVALUATE-FOR model. continuous and discrete domains EVALUATE-FOR model. Method are VAEs, and variational training procedure. OtherScientificTerm is VAE objective. Generic are models, and training procedure. ",This paper proposes a novel variational autoencoder (VAE) model that uses a Gaussian distribution over the latent space of the data to encode the data. The model is trained using a variational gradient descent (VGD) objective. The authors show that the proposed model achieves better sample quality than existing VAE models in both continuous and discrete settings. ,This paper proposes a new variational autoencoding framework for variational VAEs. The proposed method is based on the uni-modal Gaussian distribution of the latent space of the model. The model is trained using a variational training procedure with a regularized autoencoders. The method is evaluated on continuous and discrete data. 
7256,SP:6232d8738592c9728feddec4462e61903a17d131,adversarial examples USED-FOR Deep learning models. Autoencoder USED-FOR self - supervised ) adversarial detection. benign examples USED-FOR Autoencoder. autoencoder structure USED-FOR disentangled represen8 tations of images. disentangled represen8 tations of images USED-FOR adversarial examples. class features CONJUNCTION semantic features. semantic features CONJUNCTION class features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features CONJUNCTION paired class / semantic features. paired class / semantic features USED-FOR autoencoder. paired class / semantic features USED-FOR autoencoder. discriminator network USED-FOR autoencoder. generalization ability FEATURE-OF autoencoder. AUC CONJUNCTION FPR. FPR CONJUNCTION AUC. FPR CONJUNCTION TPR. TPR CONJUNCTION FPR. method COMPARE self - supervised detection methods. self - supervised detection methods COMPARE method. adversarial attacks CONJUNCTION victim models. victim models CONJUNCTION adversarial attacks. method COMPARE it. it COMPARE method. AUC CONJUNCTION TPR. TPR CONJUNCTION AUC. victim models USED-FOR self - supervised detection methods. adversarial attacks USED-FOR method. adversarial attacks FEATURE-OF self - supervised detection methods. AUC HYPONYM-OF measurements. measurements EVALUATE-FOR it. TPR HYPONYM-OF measurements. AUC EVALUATE-FOR it. FPR HYPONYM-OF measurements. AUC EVALUATE-FOR method. CIFAR-10 EVALUATE-FOR method. Autoencoder - based detectors COMPARE method. method COMPARE Autoencoder - based detectors. method USED-FOR adaptive adversary. Metric is reconstruction error. ,"This paper proposes a self-supervised adversarial detection method that uses an autoencoder to generate adversarial examples by disentangling represen8 tations of images from benign examples. The proposed method is evaluated on CIFAR-10 and ImageNet and achieves state-of-the-art results.   The main contribution of the paper is the use of an auto-encoder for adversarial training. The method is based on the idea that the class features and the semantic features are disentangled from each other in the image, and the class and semantic features can be used to train a discriminator network to distinguish between benign and adversarial images. ","This paper proposes an autoencoder-based method for self-supervised adversarial detection. The proposed method is based on disentangling represen8 tations of images into two parts: (1) disentangled class features and (2) paired class/semantic features. The disentanglement is achieved by using a discriminator network, which is trained to discriminate between the two parts of the image. The method is evaluated on CIFAR-10 and ImageNet datasets. "
7272,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"it USED-FOR brain activity. embedding space FEATURE-OF syntactic structure. low signal - to - noise ratio FEATURE-OF neuroimaging tools. functional Magnetic Resonance Imaging ( fMRI ) HYPONYM-OF neuroimaging tools. multi - dimensional features USED-FOR syntactic structure. features CONJUNCTION fMRI recordings. fMRI recordings CONJUNCTION features. fMRI recordings USED-FOR brain representation of syntax. fMRI recordings USED-FOR natural text. features USED-FOR brain representation of syntax. syntactic structure - based features USED-FOR brain activity. complexity metrics USED-FOR processing load. language system FEATURE-OF brain activity. OtherScientificTerm are semantics, semantic processing load, semantic representation of the stimulus words, syntactic processing load, and syntactic features. Generic is approaches. Task is syntax. ",This paper proposes a method to analyze the brain's representation of syntactic information in natural language using fMRI. The method is based on the observation that fMRI recordings of natural text contain a large amount of information about the embedding space of the stimulus words. The authors show that the embeddings of stimulus words in natural text can be used to predict the brain activity of the language system in terms of the semantic processing load.   The authors also show that this information is correlated with the number of words in the sentence. ,This paper proposes a new method to measure the syntactic structure-based features in fMRI recordings of natural text. The main contribution of the paper is that it is able to quantify the complexity of syntactic features in the embedding space of the fMRI data. The authors show that the embeddings of a natural text are highly correlated with the brain activity of the language system. They also show that syntactic feature-based brain activity is correlated with language system complexity.   
7288,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,deep generative models USED-FOR real - world applications. deep generative models USED-FOR Controllable generation. compositional ability USED-FOR concept combinations. compositional ability FEATURE-OF models. energybased models ( EBMs ) USED-FOR compositional generation. attributes USED-FOR compositional generation. them USED-FOR high - resolution image generation. EBM USED-FOR pre - trained generative model. latent space FEATURE-OF pre - trained generative model. latent space FEATURE-OF EBM. EBM USED-FOR them. EBM USED-FOR high - resolution image generation. StyleGAN HYPONYM-OF pre - trained generative model. EBM formulation USED-FOR joint distribution. it USED-FOR sampling. ordinary differential equation ( ODE ) USED-FOR sampling. pre - trained generator USED-FOR controllable generation. controllable generation USED-FOR attribute classifier. latent space USED-FOR Sampling. ODEs USED-FOR Sampling. conditional sampling CONJUNCTION sequential editing. sequential editing CONJUNCTION conditional sampling. method COMPARE state - of - the - art. state - of - the - art COMPARE method. sequential editing EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR state - of - the - art. conditional sampling EVALUATE-FOR method. sequential editing EVALUATE-FOR method. method USED-FOR attribute combinations. method USED-FOR compositional generation. energy functions CONJUNCTION logical operators. logical operators CONJUNCTION energy functions. compositionality FEATURE-OF generating photo - realistic images. OtherScientificTerm is hyperparameters. Material is photo - realistic images. ,"This paper proposes an energy-based generative model (EBM) for compositional image generation. The main idea is to use a pre-trained StyleGAN model to predict the attributes of the generated images, and then use an EBM to generate the attributes in the latent space of StyleGAN. The authors show that the proposed EBMs can be used to generate high-resolution images with controllable generation.   ",This paper proposes a method for compositional generation using energy-based models (EBMs) for controllable generation of high-resolution images. The authors propose an EBM-based method for controlling the compositional ability of a pre-trained generative model. The proposed method is based on an ODE formulation of the joint distribution of energy functions and logical operators. The method is evaluated on synthetic and real-world datasets. 
7304,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,common global parameters USED-FOR K - armed stochastic bandits. geometric structure USED-FOR collaborative algorithm. local feature vectors CONJUNCTION raw data. raw data CONJUNCTION local feature vectors. collaborative algorithm USED-FOR heterogeneity. geometric structure FEATURE-OF linear rewards. Fed - PE USED-FOR heterogeneity. Fed - PE HYPONYM-OF collaborative algorithm. near - optimal regrets FEATURE-OF disjoint and shared parameter cases. logarithmic communication costs FEATURE-OF near - optimal regrets. multi - client G - optimal design USED-FOR Fed - PE. tight minimax regret lower bound USED-FOR disjoint parameter case. collinearly - dependent policies HYPONYM-OF concept. synthetic and real - world datasets EVALUATE-FOR algorithms. Method is federated linear contextual bandits model. ,"This paper studies the problem of federated linear contextual bandits with K-armed stochastic bandits. In this setting, each client has access to a set of local feature vectors and raw data, and the goal is to find a policy that maximizes the sum of the rewards across all the clients. The authors propose a new collaborative algorithm called Fed-PE, which is a federated version of Fed-BAR with shared and disjoint global parameters. They show that the proposed algorithm achieves near-optimal regret in both the shared and shared parameter cases, with logarithmic communication costs. They also provide a tight minimax regret lower bound for the shared parameter case. ","This paper studies the problem of federated linear contextual bandits, where a group of K-armed stochastic bandits (K-armed bandits) share a common set of global parameters and a set of local parameters. The authors propose a new collaborative algorithm called Fed-PE, which is based on a G-optimal multi-client G- optimal design. They show that Fed- PE achieves near optimal regret bounds for both disjoint and shared parameter cases. They also provide a tight minimax regret lower bound for the shared parameter case. "
7320,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"model USED-FOR conservative planning. explicit uncertainty quantification USED-FOR incorporating conservatism. explicit uncertainty quantification USED-FOR model - based algorithms. deep neural networks USED-FOR Uncertainty estimation. complex models USED-FOR Uncertainty estimation. deep neural networks HYPONYM-OF complex models. uncertainty estimation USED-FOR offline model - based RL. offline dataset CONJUNCTION data. data CONJUNCTION offline dataset. COMBO USED-FOR value function. model - based offline RL algorithm USED-FOR value function. rollouts USED-FOR data. COMBO HYPONYM-OF model - based offline RL algorithm. data USED-FOR value function. offline dataset USED-FOR value function. policy improvement guarantee FEATURE-OF COMBO. COMBO COMPARE offline RL. offline RL COMPARE COMBO. offline RL USED-FOR problems. COMBO USED-FOR problems. COMBO COMPARE offline RL methods. offline RL methods COMPARE COMBO. generalization FEATURE-OF problems. offline RL benchmarks EVALUATE-FOR offline RL methods. image - based tasks HYPONYM-OF offline RL benchmarks. Method is dynamics model. Material is logged experience. Task are offline reinforcement learning ( offline RL ), and explicit uncertainty estimation. OtherScientificTerm is model rollouts. ","This paper proposes a model-based offline RL algorithm called COMBO, which is based on a combination of model-free offline RL and explicit uncertainty quantification. The main idea is to use a dynamic model to estimate the uncertainty in the offline dataset, and then use the model to train a policy on the offline data. Theoretical analysis is provided to show that the proposed method is robust to model rollouts. Empirical results show that COMBO outperforms existing offline RL methods on image-based tasks. ","This paper proposes a new model-based offline RL algorithm called COMBO. COMBO is based on the notion of explicit uncertainty quantification, which is used to improve the generalization performance of offline RL algorithms. The main idea is to use a model to estimate the value function of the data, and then use the data to train the model. The authors show that COMBO outperforms other offline RL methods in terms of generalization on image-based tasks. "
7336,SP:ca6f11ed297290e487890660d9a9a088aa106801,"stochastic gradient descent USED-FOR neural networks. deep learning training USED-FOR evolution of features. stochastic differential equations ( SDEs ) USED-FOR evolution of features. backpropagation USED-FOR features. drift term PART-OF SDE. sharp phase transition phenomenon FEATURE-OF intra - class impact. neural collapse of the features HYPONYM-OF geometric structure. local elasticity USED-FOR neural networks. synthesized dataset of geometric shapes CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION synthesized dataset of geometric shapes. Method are deep learning models, modeling strategy, and SDEs. Generic is models. OtherScientificTerm are feature spaces, and vanishing training loss. ",This paper studies the evolution of features in deep neural networks using stochastic differential equations (SDEs) in the presence of backpropagation and vanishing training loss. The authors show that SDEs in deep networks exhibit sharp phase transition phenomenon and intra-class impact in the feature space. They also show that neural collapse of the features is caused by the local elasticity of neural networks.   ,This paper studies the phenomenon of neural collapse of the features in stochastic differential equations (SDEs). The authors show that the intra-class impact of SDEs can be explained by sharp phase transition phenomenon. They also show that local elasticity of the feature space can be used to explain the local collapse of features in SDE. The authors also provide a theoretical analysis of the effect of the vanishing training loss. 
7352,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"DRL methods USED-FOR neural network policies. learning programmatic policies USED-FOR generalization. input / output state pairs CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION input / output state pairs. decision trees CONJUNCTION state machines. state machines CONJUNCTION decision trees. state machines CONJUNCTION predefined program templates. predefined program templates CONJUNCTION state machines. expert demonstrations HYPONYM-OF supervision. predefined program templates HYPONYM-OF limited policy representations. input / output state pairs HYPONYM-OF supervision. state machines HYPONYM-OF limited policy representations. limited policy representations USED-FOR works. decision trees HYPONYM-OF limited policy representations. supervision USED-FOR works. framework USED-FOR program. program embedding space USED-FOR program. framework USED-FOR task - solving programs. framework COMPARE DRL and program synthesis baselines. DRL and program synthesis baselines COMPARE framework. methods USED-FOR program embedding. Method are deep reinforcement learning ( DRL ) methods, and two - stage learning scheme. Generic is task. OtherScientificTerm are reward signals, and programs. ","This paper proposes a two-stage learning scheme for learning programmatic policies in deep reinforcement learning (DRL). The first stage learns a program embedding space for a task-solving program, which is then used to learn a policy that can solve the task. The second stage learns an embedding of the program to a set of state-action pairs, which are then used as expert demonstrations. The authors show that the learned embeddings can be used to improve the performance of existing DRL methods.  ","This paper proposes a two-stage learning scheme for learning programmatic policies in deep reinforcement learning (DRL). The first stage learns a program embedding space for a task-solving program, and the second stage learns the policy representations for the task. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization. "
7368,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"scientific machine learning USED-FOR physicsinformed neural network ( PINN ) models. machine learning methodologies USED-FOR model. soft constraints FEATURE-OF empirical loss function. soft constraints FEATURE-OF physical domain knowledge. physical domain knowledge USED-FOR approach. PINN methodologies USED-FOR models. they USED-FOR physical phenomena. soft regularization USED-FOR subtle problems. soft regularization USED-FOR PINNs. PDE - based differential operators PART-OF soft regularization. PINN ’s setup USED-FOR loss landscape. solutions USED-FOR failure modes. curriculum regularization USED-FOR PINN ’s loss term. curriculum regularization USED-FOR approach. PDE regularization USED-FOR PINN ’s loss term. approach USED-FOR problem. sequence - to - sequence learning task USED-FOR problem. methods COMPARE regular PINN training. regular PINN training COMPARE methods. Task are physical interest, and learning differential equations. OtherScientificTerm is differential equations. Method are NN architecture, and NN. ",This paper proposes a method for learning physics-informed neural networks (PINNs) with soft constraints on the empirical loss function. The proposed method is based on a PDE-based differential operator that is used to regularize the PINN’s loss landscape. The method is evaluated on a sequence-to-sequence learning task and compared with other PINN-based methods.,"This paper proposes a new method for physics-informed neural network (PINN) training. The method is based on the notion of ""curriculum regularization"", which is a soft regularization of the empirical loss function of a PINN. The idea is to use the PDE-based differential operators in the PINN’s setup as well as a sequence-to-sequence learning task. The authors show that the proposed method outperforms the state-of-the-art PINN training methods in terms of accuracy. "
7384,SP:cfd501bca783590a78305f0592f537e8f20bce27,"domaininvariant representations USED-FOR domain shift. domaininvariant representations USED-FOR unsupervised domain adaptation ( UDA ). self - training USED-FOR UDA. self - training USED-FOR unlabeled target data. Cycle Self - Training ( CST ) HYPONYM-OF self - training algorithm. CST USED-FOR target pseudo - labels. source - trained classifier USED-FOR CST. source - trained classifier USED-FOR target pseudo - labels. shared representations USED-FOR classifier. CST USED-FOR classifier. target pseudo - labels USED-FOR CST. target pseudo - labels USED-FOR classifier. Tsallis entropy USED-FOR confidence - friendly regularization. invariant feature learning CONJUNCTION vanilla self - training. vanilla self - training CONJUNCTION invariant feature learning. CST COMPARE state - of - the - arts. state - of - the - arts COMPARE CST. visual recognition and sentiment analysis benchmarks EVALUATE-FOR state - of - the - arts. visual recognition and sentiment analysis benchmarks EVALUATE-FOR CST. OtherScientificTerm are hardness or impossibility theorems, distributional shift, and pseudo - labels. Generic is forward step. ","This paper proposes Cycle Self-Training (CST), a self-training algorithm for unsupervised domain adaptation (UDA) that uses pseudo-labels generated by a source-trained classifier to train a classifier on unlabeled target data. The proposed method is based on the idea that domain-invariant representations can be used to improve the performance of UDA in the face of distributional shift. The authors show that the proposed method outperforms the state-of-the-art in both visual recognition and sentiment analysis benchmarks.","This paper proposes Cycle Self-Training (CST), a self-training method for unsupervised domain adaptation (UDA) where the source data is unlabeled and the target data is labeled with pseudo-labels. The proposed method is based on the idea of invariant feature learning (i.e., the source and target data are invariant to domain shift). The authors show that the proposed method outperforms the state-of-the-art in both visual recognition and sentiment analysis benchmarks."
7400,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"unsupervised representation learning CONJUNCTION structured network pruning. structured network pruning CONJUNCTION unsupervised representation learning. deep learning USED-FOR compact representations of features. neural network USED-FOR compact representations of features. DiscriminAtive Masking ( DAM ) HYPONYM-OF single - stage structured pruning method. graph representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION graph representation learning. recommendation system CONJUNCTION graph representation learning. graph representation learning CONJUNCTION recommendation system. dimensionality reduction CONJUNCTION recommendation system. recommendation system CONJUNCTION dimensionality reduction. structured pruning USED-FOR image classification. representation learning CONJUNCTION structured pruning. structured pruning CONJUNCTION representation learning. applications USED-FOR structured pruning. applications USED-FOR representation learning. applications EVALUATE-FOR DAM approach. graph representation learning HYPONYM-OF applications. dimensionality reduction HYPONYM-OF applications. recommendation system HYPONYM-OF applications. learning objective FEATURE-OF DAM. Generic are state - of - the - art methods, and systematic approach. Method are fine - tuning, and dam - pytorch. OtherScientificTerm is masking layer. ","This paper proposes a new method for structured pruning in representation learning. The proposed method is based on a single-stage structure pruning method, where a masking layer is used to reduce the number of parameters in the pruning process. The method is evaluated on image classification, graph representation learning, and dimensionality reduction tasks.","This paper proposes a single-stage structured pruning method, called DiscriminAtive Masking (DAM), for unsupervised representation learning and structured network pruning. The main contribution of the paper is a systematic approach to fine-tune the masking layer, which is based on the notion of discriminative masking. The method is evaluated on a variety of applications, including image classification, graph representation learning, recommendation system, and dimensionality reduction."
7416,SP:f831d25830efa88434b43e900241a5ad81119360,"compositional reasoning CONJUNCTION reuse of knowledge. reuse of knowledge CONJUNCTION compositional reasoning. they USED-FOR systematic generalization. architecture USED-FOR inference. self - attention network USED-FOR inference. functions HYPONYM-OF system of modules. architecture USED-FOR capacity extension. architecture USED-FOR computation. image classification CONJUNCTION visual abstract reasoning. visual abstract reasoning CONJUNCTION image classification. settings EVALUATE-FOR Neural Interpreters. settings EVALUATE-FOR it. image classification EVALUATE-FOR it. Raven Progressive Matrices USED-FOR visual abstract reasoning. visual abstract reasoning HYPONYM-OF settings. image classification HYPONYM-OF settings. Neural Interpreters COMPARE vision transformer. vision transformer COMPARE Neural Interpreters. former EVALUATE-FOR Neural Interpreters. Neural Interpreters COMPARE state - of - the - art. state - of - the - art COMPARE Neural Interpreters. latter EVALUATE-FOR Neural Interpreters. systematic generalization EVALUATE-FOR state - of - the - art. systematic generalization EVALUATE-FOR Neural Interpreters. Method is neural network architectures. Generic are model, and task. ","This paper proposes a new architecture for image classification and abstract reasoning tasks. The proposed architecture is based on a self-attention module and a system of modules, where the modules are composed of a set of functions that can be used for inference and capacity extension. The authors show that the proposed architecture can achieve better generalization performance compared to the state-of-the-art on image classification tasks. ","This paper proposes a new architecture, called Neural Interpreters, for the task of visual abstract reasoning. The main idea is to use a self-attention network for inference, and then use a system of modules to extend the capacity of the network to perform compositional reasoning and reuse of knowledge. The proposed architecture is evaluated on a variety of tasks, including image classification, image abstract reasoning, and image classification.  "
7432,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"fine - tuning strategies USED-FOR transfer. transfer USED-FOR challenging domains. fine - tuning strategies USED-FOR challenging domains. pre - trained policies USED-FOR exploration. Behavior Transfer ( BT ) HYPONYM-OF technique. pre - trained policies USED-FOR technique. intrinsic motivation 10 objectives USED-FOR complex behaviors. large - scale pre - training CONJUNCTION intrinsic motivation 10 objectives. intrinsic motivation 10 objectives CONJUNCTION large - scale pre - training. BT CONJUNCTION fine - tuning strategies. fine - tuning strategies CONJUNCTION BT. BT USED-FOR pre - trained 11 policies. Generic is it. Task are reinforcement learning, unsupervised pre - training phase, reinforcement learning problem, and structured exploration. Method is fine3 tuning neural network weights. OtherScientificTerm are rewards, neural network weights, pre - training, and pre - trained 15 policies. Material is supervised domains. ",This paper studies the problem of unsupervised reinforcement learning in the presence of a large amount of pre-trained policies. The authors propose a technique called Behavior Transfer (BT) to improve the transfer of knowledge from one domain to another. They show that this transfer can be achieved by fine-tuning the weights of the learned policies in order to improve their performance in the new domain. They also show that the proposed method can be used in conjunction with other methods to improve performance in new domains. ,"This paper proposes a new technique called Behavior Transfer (BT) for unsupervised pre-training in reinforcement learning. The main idea of BT is to fine-tune a pre-trained policy in a supervised setting by fine-tuning the weights of the neural network during the pre-train phase. The authors show that BT can be applied to a variety of settings, and that it can be used to improve the performance of a policy trained in the supervised setting. They also show that it is possible to transfer the learned policies to a more challenging setting. "
7448,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"machine learning approaches USED-FOR ranking. performance metrics of interest CONJUNCTION surrogate loss functions. surrogate loss functions CONJUNCTION performance metrics of interest. gradient - based methods USED-FOR surrogate loss functions. sorting operation PART-OF ranking metrics. ranking metrics EVALUATE-FOR surrogates. differentiable surrogates USED-FOR ranking. PiRank HYPONYM-OF differentiable surrogates. continuous, temperature - controlled relaxation USED-FOR sorting operator. NeuralSort USED-FOR continuous, temperature - controlled relaxation. continuous, temperature - controlled relaxation USED-FOR PiRank. continuous, temperature - controlled relaxation USED-FOR differentiable surrogates. NeuralSort USED-FOR differentiable surrogates. PiRank USED-FOR metrics. PiRank COMPARE approaches. approaches COMPARE PiRank. OtherScientificTerm are model parameters, and zero temperature. Task are real - world applications, and training. Method is divideand - conquer extension. ","This paper proposes to use differentiable surrogates for ranking, i.e., a differentiable surrogate loss function that can be used to compute ranking metrics. The authors propose to use a continuous, temperature-controlled relaxation of the sorting operator in the ranking metric. The proposed method, called PiRank, is based on NeuralSort, which is a continuous-time relaxation of a sorting operator.   The authors show that the proposed method is able to compute the ranking metrics of interest in terms of the surrogate loss functions. ","This paper proposes a differentiable differentiable surrogate for differentiable surrogates for ranking. The authors propose a continuous, temperature-controlled relaxation of the sorting operator, which is used to improve the performance of the surrogate loss functions. They show that the proposed method outperforms the state-of-the-art gradient-based methods on a variety of ranking metrics. "
7464,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"they USED-FOR near - term quantum devices. machine learning USED-FOR problem. methods USED-FOR VQE structure optimization. reinforcement learning algorithm USED-FOR ansatzes. reinforcement learning algorithm USED-FOR economic circuits. feedback - driven curriculum learning method USED-FOR learning problem. complexity EVALUATE-FOR learning problem. it USED-FOR circuit depth. feedback - driven curriculum learning method USED-FOR algorithm. chemical accuracy EVALUATE-FOR benchmark problem. Task are Variational Quantum Eigensolvers ( VQEs ), and optimization of the VQE ansatz. OtherScientificTerm are variational ansatz, near - term restrictions, VQE ansatz, low depth, and ground energy estimates. Method is learning algorithm. ","This paper proposes a reinforcement learning algorithm for learning a variational quantum entege (VQE) with near-term restrictions. The proposed method is based on a curriculum learning approach, where the goal is to learn a VQE with a low-depth circuit. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and computational complexity.","This paper studies the problem of learning Variational Quantum Eigensolvers (VQEs), which are quantum devices with near-term restrictions. The authors propose a curriculum learning algorithm for learning the structure of VQEs, which is based on a reinforcement learning algorithm. They show that the learning problem can be reduced to a low-depth learning problem, where the depth of the circuit depends on the number of parameters of the VQE. They also show that their method can be used to learn the depth in a similar way as the previous methods. "
7480,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"Transductive inference USED-FOR few - shot learning. it COMPARE inductive counterpart. inductive counterpart COMPARE it. statistics of the unlabeled query set USED-FOR few - shot task. it USED-FOR few - shot task. statistics of the unlabeled query set USED-FOR it. class - balanced tasks USED-FOR inference. few - shot benchmarks EVALUATE-FOR inference. class - balanced tasks FEATURE-OF few - shot benchmarks. arbitrary and unknown label marginals FEATURE-OF unlabeled query sets. few - shot tasks USED-FOR inference. arbitrary class distributions FEATURE-OF few - shot tasks. arbitrary class distributions USED-FOR inference. Dirichlet - distributed random variables USED-FOR marginal probabilities. arbitrary class distributions FEATURE-OF testing tasks. α - divergences USED-FOR mutual - information loss. transductive α - divergence optimization COMPARE state - of - the - art methods. state - of - the - art methods COMPARE transductive α - divergence optimization. OtherScientificTerm are artificial regularity, marginal label probability, uniform distribution, class - balance artefact, simplex, class - distribution variations, and few - shot settings. Method are transductive methods, and inductive methods. ","This paper proposes a novel method for few-shot inference in the presence of class-balance artefacts. The proposed method is based on transductive inference, where the marginal probabilities of the query set are approximated by Dirichlet-distributed random variables and the class-distribution variations are modeled as Dirichlets. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks. ","This paper studies the problem of few-shot inference in the context of transductive learning. In particular, the authors consider the setting of class-balanced tasks, where the class-distribution of the query set is different from the one of the test set. The authors propose a new method, called Transductive α-Divergence (TAD), to learn the marginal probabilities of the unlabeled query set. They show that TAD outperforms the state-of-the-art methods in terms of the mutual information loss.   "
7496,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"imbalanced memory distribution FEATURE-OF convolutional neural network ( CNN ) designs. overlapping patches CONJUNCTION computation overhead. computation overhead CONJUNCTION overlapping patches. receptive field redistribution USED-FOR receptive field. receptive field redistribution USED-FOR FLOPs. receptive field redistribution USED-FOR computation overhead. neural architecture CONJUNCTION inference scheduling. inference scheduling CONJUNCTION neural architecture. peak memory usage EVALUATE-FOR networks. neural networks USED-FOR MCUNetV2. Patch - based inference USED-FOR networks. visual wake words dataset EVALUATE-FOR accuracy. MCU EVALUATE-FOR MCUNetV2. ImageNet accuracy EVALUATE-FOR MCUNetV2. peak memory usage EVALUATE-FOR Patch - based inference. accuracy EVALUATE-FOR MCUNetV2. visual wake words dataset EVALUATE-FOR MCUNetV2. MCUNetV2 USED-FOR object detection. tiny devices USED-FOR object detection. mAP EVALUATE-FOR MCUNetV2. memory bottleneck PART-OF tinyML. image classification HYPONYM-OF vision applications. Task are Tiny deep learning, and Manually redistributing the receptive field. OtherScientificTerm are limited memory size, feature map, and peak memory. Metric is memory usage. Generic is network. Method are generic patch - by - patch inference scheduling, naive implementation, and neural architecture search. Material is Pascal VOC. ",This paper proposes a patch-by-patch inference method to reduce the memory consumption of tiny neural networks. The proposed method is based on the observation that the number of overlapping patches in the feature map increases with the size of the network. The authors propose to manually redistribute the receptive field of the convolutional layers to reduce this overhead. The method is evaluated on ImageNet and Pascal VOC datasets.  ,"This paper proposes a new method for patch-by-patch inference in tiny neural networks (MCUNetV2) to reduce the memory consumption of the network. The proposed method is based on the idea of ""rewarding the receptive field"", which is a method of redistributing the feature map of a neural network to reduce its peak memory usage. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and memory consumption.  "
7512,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"unstructured dynamic environments FEATURE-OF Bayesian automated mechanism design. optimal mechanism USED-FOR principal ’s utility. algorithm USED-FOR optimal mechanisms. linear program formulation USED-FOR algorithm. constant factor FEATURE-OF principal ’s optimal utility. unstructured environments FEATURE-OF automated dynamic mechanism design. time complexity EVALUATE-FOR algorithm. optimality CONJUNCTION computational tractability. computational tractability CONJUNCTION optimality. solution USED-FOR problem. Markov decision processes USED-FOR memoryless mechanisms. algorithms USED-FOR synthetic dynamic environments. algorithms USED-FOR algorithms. OtherScientificTerm are self - interested strategic agent, payments, individual - rationality constraints, time horizon, and strategic behavior. Task is dynamic mechanism design. ","This paper studies the problem of Bayesian automated mechanism design in an unstructured dynamic environment, where the goal is to design a mechanism that maximizes the principal’s utility. The authors propose a linear program formulation for this problem, and show that it is computationally tractable with a constant factor in terms of the principal's optimal utility. They also show that the optimal mechanism can be found in a linear time-efficient way.  ","This paper studies the problem of Bayesian automated mechanism design in an unstructured dynamic environment, where the goal is to design an optimal mechanism that maximizes the principal’s utility. The authors propose a linear program formulation for the problem, which is based on a linear-program formulation of the optimal mechanism. They show that the proposed algorithm is computationally tractable and time-efficient. They also show that their algorithm can be applied to a synthetic dynamic environment. "
7528,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,Graph Neural Networks ( GNNs ) architectures USED-FOR tasks. Neural Architecture Search ( NAS ) USED-FOR GNN architectures. GNN architectures USED-FOR tasks. Neural Architecture Search ( NAS ) COMPARE manually designed architectures. manually designed architectures COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR tasks. NAS USED-FOR GNN architectures. NAS USED-FOR GNN structures. gradient based NAS methods USED-FOR architectures. gradient based NAS USED-FOR searching suboptimal GNN architectures. graph structure learning USED-FOR search procedure. graph structure learning USED-FOR denoising process. denoising process USED-FOR search procedure. Structure Optimization ( GASSO ) USED-FOR Graph differentiable Architecture Search model. graph structure learning USED-FOR graph neural architectures. real - world graph datasets EVALUATE-FOR GASSO model. GASSO model COMPARE baselines. baselines COMPARE GASSO model. real - world graph datasets EVALUATE-FOR baselines. OtherScientificTerm is graph. Method is gradient descent. ,"This paper proposes a novel architecture search method for graph neural networks (GNNs) based on GASSO. The main idea is to use a denoising process to find suboptimal GNNs in a search procedure. The proposed method is based on the idea of structure optimization, where the goal is to find a GNN architecture that maximizes the mutual information between nodes in the graph. The method is evaluated on several graph datasets and achieves state-of-the-art performance. ",This paper proposes a new method for finding suboptimal GNN architectures for graph neural networks (GNNs). The main idea is to use a differentiable architecture search model (GASSO) to learn the structure of GNNs by denoising the graph. GASSO is trained using graph structure learning to find the best GNN architecture. The method is evaluated on a variety of real-world graph datasets. 
7544,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"Clustering HYPONYM-OF unsupervised learning problem. metric space FEATURE-OF clusters. fairness FEATURE-OF algorithm. cost FEATURE-OF clustering objective. upper bound USED-FOR clustering problem. upper bound USED-FOR constraint. constraint USED-FOR clustering problem. upper bound USED-FOR clustering objective. it USED-FOR equality of representation. group utilitarian objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group utilitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective CONJUNCTION group egalitarian objective. group leximin objective CONJUNCTION group egalitarian objective. group egalitarian objective CONJUNCTION group leximin objective. group leximin objective HYPONYM-OF fairness objectives. group egalitarian objective HYPONYM-OF fairness objectives. group utilitarian objective HYPONYM-OF fairness objectives. algorithms USED-FOR them. lower bounds USED-FOR approximation of the utilitarian and egalitarian objectives. heuristic algorithm USED-FOR leximin objective. impossibility results USED-FOR natural fairness objectives. real - world datasets EVALUATE-FOR algorithms. Method is fair clustering. OtherScientificTerm are group membership, group fairness, and utilitarian and egalitarian objectives. Generic is model. ","This paper studies the problem of fair clustering, which is an unsupervised learning problem in which the goal is to find a set of clusters in a metric space that are fair in terms of fairness. The authors propose two algorithms for this problem. The first one is based on the notion of fairness in the sense that the objective is to minimize the cost of the clustering objective in the metric space. The second one uses a heuristic algorithm to compute the leximin objective of the objective function. ","This paper studies the problem of fairness in unsupervised clustering, where the goal is to find a set of groups with equal representation in the metric space. The main contribution of the paper is a new upper bound on the fairness of the clustering objective. The upper bound is based on the fact that the cost of the fairness objective is a function of the number of groups in the group. The authors show that this upper bound can be used as a constraint on the fairness of the objective. They also provide a heuristic algorithm for the group leximin objective and show that it can be approximated by an approximation of the group egalitarian objective."
7560,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"neural - network - based graph generative models USED-FOR real - world network characteristics. high triangle density HYPONYM-OF real - world network characteristics. variational graph autoencoders CONJUNCTION CELL. CELL CONJUNCTION variational graph autoencoders. NetGAN CONJUNCTION variational graph autoencoders. variational graph autoencoders CONJUNCTION NetGAN. Erdös - Rényi and stochastic block models CONJUNCTION generative models. generative models CONJUNCTION Erdös - Rényi and stochastic block models. CELL HYPONYM-OF generative models. NetGAN HYPONYM-OF generative models. variational graph autoencoders HYPONYM-OF generative models. generative models PART-OF models. Erdös - Rényi and stochastic block models PART-OF models. edge independent models USED-FOR graphs. real - world social networks CONJUNCTION graphs. graphs CONJUNCTION real - world social networks. generative model USED-FOR graph statistics. overlap CONJUNCTION accuracy. accuracy CONJUNCTION overlap. overlap EVALUATE-FOR generative model. accuracy EVALUATE-FOR generative model. Method is edge independent random graph models. OtherScientificTerm are graph, and bounded overlap condition. Generic is model. ",This paper proposes to use edge-independent random graph models to generate graphs with high triangle density. The proposed method is based on variational graph autoencoders (VGAs) and CELLs. Theoretical analysis is provided to show that the generated graphs have bounded overlap with real-world graphs. Experiments are conducted to show the effectiveness of the proposed method. ,"This paper studies the problem of graph generative models that can be used to generate graphs with high triangle density and high accuracy. The authors propose a new model, called CELL, which is based on edge-independent random graph models. They show that CELL is able to generate high-quality graphs with a bounded overlap condition. They also provide a theoretical analysis of the effect of the model on the accuracy of the generated graphs. "
7576,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"backpropagation CONJUNCTION training. training CONJUNCTION backpropagation. ReLU′(0 ) USED-FOR neural network. ReLU′(0 ) USED-FOR backpropagation. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR10. SVHN CONJUNCTION ImageNet. ImageNet CONJUNCTION SVHN. networks CONJUNCTION datasets. datasets CONJUNCTION networks. ReLU′(0 ) USED-FOR precision levels. ImageNet HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR10 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. ResNet HYPONYM-OF networks. VGG HYPONYM-OF networks. test accuracy EVALUATE-FOR ReLU′(0 ) = 1. batch - norm CONJUNCTION ADAM. ADAM CONJUNCTION batch - norm. batch - norm HYPONYM-OF reconditioning approaches. ADAM HYPONYM-OF reconditioning approaches. OtherScientificTerm are default precision, deep learning problems, backpropagation outputs, and double precision. Method are training methods, and vanilla SGD training. Task is algorithmic differentiation of nonsmooth problems. ",This paper proposes to use ReLU′(0) as the default precision for backpropagation in deep neural networks. The authors show that this precision is sufficient to recover the original accuracy of the network. They also show that the precision can be used to recondition the network to a more efficient way of training.   ,"This paper proposes a new metric, ReLU′(0), to measure the precision of a neural network trained with backpropagation. The authors show that ReLU(0) can be used as a measure of the accuracy of neural networks trained with vanilla SGD training. They also show that it is possible to use the metric for training neural networks that are trained with batch-norm and ADAM. They show that this metric can be applied to a wide range of datasets and networks. "
7592,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"generalization CONJUNCTION transfer. transfer CONJUNCTION generalization. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. transfer CONJUNCTION computational efficiency. computational efficiency CONJUNCTION transfer. minimizing information USED-FOR supervised learning setting. method USED-FOR policies. RPC ) USED-FOR policies. method USED-FOR RPC ). information bottlenecks CONJUNCTION model - based RL. model - based RL CONJUNCTION information bottlenecks. model - based RL CONJUNCTION bits - back coding. bits - back coding CONJUNCTION model - based RL. latent - space model CONJUNCTION policy. policy CONJUNCTION latent - space model. method USED-FOR policy. method USED-FOR latent - space model. method COMPARE prior methods. prior methods COMPARE method. reward EVALUATE-FOR information bottleneck. compression EVALUATE-FOR prior methods. method COMPARE information bottleneck. information bottleneck COMPARE method. compression EVALUATE-FOR method. reward EVALUATE-FOR method. compression USED-FOR policies. method USED-FOR policies. Method are reinforcement learning ( RL ) algorithms, and RL algorithms. Task is RL setting. OtherScientificTerm are past information, and decision making. ","This paper proposes a method to reduce the amount of information in reinforcement learning (RL) algorithms by minimizing the information in the latent space model. The method is based on the idea that the information bottleneck can be decomposed into two parts: (1) the latent-space model, and (2) the policy. The idea is to use the information from the latent model to train the policy, which is then used to compress the reward function. The authors show that the proposed method outperforms previous methods in terms of transfer, robustness and generalization.","This paper proposes a new method for minimizing the information bottleneck in reinforcement learning (RL) algorithms. The key idea is to use a model-based RL algorithm with a latent-space model and a policy that is trained on the latent space model. The method is based on the idea of minimizing the RPC, which is a method for learning policies that can be used in a supervised learning setting. The authors show that the method can be applied to a variety of RL algorithms, including bit-back coding, model-driven RL, and model-free RL. They also show that their method can improve the generalization and transferability of the learned policies."
7608,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"Transformer architecture USED-FOR sequence processing. graphs HYPONYM-OF data structures. node PART-OF graph. full Laplacian spectrum USED-FOR learned positional encoding ( LPE ). learned positional encoding ( LPE ) USED-FOR Spectral Attention Network ( SAN ). LPE PART-OF node features. LPE USED-FOR fully - connected Transformer. node features PART-OF graph. model USED-FOR similar sub - structures. model USED-FOR distinguishing graphs. heat transfer CONJUNCTION electric interaction. electric interaction CONJUNCTION heat transfer. Transformer USED-FOR modeling of physical phenomenons. over - squashing HYPONYM-OF information bottleneck. information bottleneck PART-OF GNNs. electric interaction HYPONYM-OF modeling of physical phenomenons. heat transfer HYPONYM-OF modeling of physical phenomenons. datasets EVALUATE-FOR model. model COMPARE state - of - theart GNNs. state - of - theart GNNs COMPARE model. model COMPARE attention - based model. attention - based model COMPARE model. graph benchmarks EVALUATE-FOR fully - connected architecture. graph benchmarks EVALUATE-FOR attention - based model. OtherScientificTerm are Laplacian, resonance, and physical phenomenons. ","This paper proposes a transformer-based representation learning method for graph representation learning. The proposed method is based on a learned positional encoding (LPE), which is used to learn the positional encoding of each node in the graph. The LPE encodes the node features into the Laplacian of the graph, which is then used to train a transformer network. The authors show that the proposed method achieves state-of-the-art performance on a variety of graph classification tasks.","This paper proposes a transformer-based attention network for graph representation learning. The attention network is based on the learned positional encoding (LPE) of the Laplacian spectrum. The LPE encodes the position of each node in the graph as a LaplACian, which is then used to learn the node features of the graph. The proposed model is evaluated on a variety of graph datasets, and it outperforms state-of-the-art GNNs."
7624,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"Task is two - alternative elections. OtherScientificTerm are state variable, private information, and private signals. Method is Bayes Nash equilibrium. ","This paper studies the problem of two-alternate elections in the presence of private information about the state variable. In particular, the authors consider the case where the state variables are private information and private signals are available. The authors show that the Bayes Nash equilibrium of the Bayesian Nash equilibrium can be reached in the setting where the private information is available. ","This paper studies the problem of two-alternate elections. The authors consider the case where the state variable is private information, and the two alternative elections are public information. They show that the Bayes Nash equilibrium of the two elections is Bayesian Nash equilibrium, which is a Bayesian formulation of the Nash equilibrium. They also show that under certain assumptions on the state variables, the two alternatives are Bayesian.  "
7640,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"Hessian USED-FOR parameter interactions. Hessian FEATURE-OF neural network. secondorder derivatives of the loss USED-FOR Hessian. secondorder derivatives of the loss USED-FOR parameter interactions. model design CONJUNCTION optimization. optimization CONJUNCTION model design. optimization CONJUNCTION generalization. generalization CONJUNCTION optimization. model design HYPONYM-OF deep learning. optimization HYPONYM-OF deep learning. low - rank approximations CONJUNCTION heuristics. heuristics CONJUNCTION low - rank approximations. theoretical tools USED-FOR Hessian map. Hessian rank FEATURE-OF deep linear networks. rectified and hyperbolic tangent networks HYPONYM-OF models. model architecture USED-FOR rank deficiency. Generic are It, and bounds. OtherScientificTerm are network structure, and numerical Hessian rank. Method is overparameterized neural networks. ","This paper studies the Hessian of over-parameterized neural networks with rectified and hyperbolic tangent networks. The authors show that the rank of the Hessians of deep linear networks has a rank deficiency. They show that this is due to the fact that the second-order derivatives of the loss of the network are not well approximated by low-rank approximations. They also show that for rectified networks, the rank deficiency can be reduced to the same degree as that of the original Hessian. ","This paper studies the problem of overparameterized neural networks with Hessian rank deficiency. The authors show that the Hessian of a neural network can be reduced to a low-rank Hessian, which can be approximated by any low rank approximations of the network. They also provide a theoretical analysis of this problem, showing that it can be minimized by a low rank approximation of the neural network.  The authors also provide an analysis of the generalization properties of the problem. "
7656,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,metric USED-FOR LSBD. metric EVALUATE-FOR LSBD methods. metric USED-FOR LSBD. semi - supervised 6 method USED-FOR LSBD representations. LSBD - VAE HYPONYM-OF semi - supervised 6 method. metric USED-FOR LSBD - VAE. LSBD - VAE USED-FOR LSBD representations. methods USED-FOR LSBD representations. LSBD - VAE CONJUNCTION methods. methods CONJUNCTION LSBD - VAE. common VAE - based disentanglement methods USED-FOR LSBD representations. disentanglement metrics EVALUATE-FOR LSBD representations. limited supervision USED-FOR methods. limited supervision USED-FOR LSBD representations. OtherScientificTerm is Linear Symmetry - Based Disentanglement ( LSBD ). Method is linearly disentangled representations. Task is disentanglement. Metric is DLSBD. ,"This paper proposes a new disentanglement metric called DLSBD, which is based on linear symmetry-based disentangled representations. The authors show that the proposed metric can be used to evaluate the performance of existing VAE-based methods in terms of the proposed LSBD-VAE.  The authors also propose a new semi-supervised 6-layer VAE method to improve the performance. The proposed method is shown to achieve better performance than the existing methods. ","This paper proposes a new disentanglement metric for linear symmetric-symmetry-based disentangled representations (LSBD). The proposed metric is based on a VAE-based VAE, which is a semi-supervised 6-layer VAE method. The authors show that the proposed metric can be used to evaluate the performance of LSBD-VAE on a variety of datasets. They also show that their metric is better than the standard DLSBD metric. "
7672,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,Deep state - space models ( DSSMs ) USED-FOR temporal predictions. dynamics of observed sequence data USED-FOR Deep state - space models ( DSSMs ). dynamics of observed sequence data USED-FOR temporal predictions. evidence lower bound USED-FOR They. model USED-FOR dynamics. approach USED-FOR DSSMs. constrained optimisation framework USED-FOR DSSMs. constrained optimisation framework USED-FOR approach. amortised variational inference CONJUNCTION Bayesian filtering / smoothing. Bayesian filtering / smoothing CONJUNCTION amortised variational inference. extended Kalman VAE ( EKVAE ) COMPARE RNN - based DSSMs. RNN - based DSSMs COMPARE extended Kalman VAE ( EKVAE ). amortised variational inference PART-OF extended Kalman VAE ( EKVAE ). Bayesian filtering / smoothing PART-OF extended Kalman VAE ( EKVAE ). constrained optimisation framework USED-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR DSSMs. system identification and prediction accuracy EVALUATE-FOR constrained optimisation framework. EKVAE COMPARE models. models COMPARE EKVAE. EKVAE USED-FOR dynamical systems. EKVAE USED-FOR state - space representations. static and dynamic features PART-OF state - space representations. prediction accuracy EVALUATE-FOR EKVAE. prediction accuracy EVALUATE-FOR models. Task is maximising the evidence lower bound. ,This paper proposes an extension of the Kalman variational inference framework for deep state-space models (DSSMs). The proposed method is based on the idea of amortized inference and Bayesian filtering/smoothing. The main contribution of this paper is to extend the amortised Kalman VAE (EKVAE) framework with a constrained optimisation framework to improve the performance of DSSMs. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. ,This paper proposes an extension of the Kalman-VAE framework for deep state-space models (DSSMs). The authors propose a constrained optimisation framework for DSSMs with amortised variational inference and Bayesian filtering/smoothing. The authors show that the proposed method outperforms state-of-the-art RNN-based models in terms of system identification and prediction accuracy. 
7688,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"Explanation techniques USED-FOR introspecting black - box models. deep inversion approach USED-FOR counterfactual explanations. deep models USED-FOR images. training distribution USED-FOR deep models. deep inversion methods USED-FOR counterfactuals. deep inversion methods USED-FOR conditional image synthesis. DISC USED-FOR Synthesizing Counterfactuals. deep inversion USED-FOR DISC. image priors USED-FOR DISC. counterfactuals USED-FOR classifier decision boundaries. DISC USED-FOR counterfactuals. counterfactuals USED-FOR visually meaningful explanations. Task is model prediction. OtherScientificTerm are discernible changes, data manifold, manifold consistency objective, and unknown test - time corruptions. Method are deep classifier, and progressive optimization strategy. ","This paper proposes a method for generating counterfactual explanations for black-box models. The method is based on a deep inversion of the training distribution of the model, which is used to generate images that can be used to explain the classifier decision boundaries. The proposed method is applied to conditional image synthesis, where the image priors are learned from the training set. The authors show that their method is able to generate explanations that are visually meaningful. ","This paper proposes a deep inversion-based method for generating counterfactual explanations for black-box models. The method is based on the notion of ""manifold consistency"", which is defined as the consistency between the data manifold and the training distribution of the model. The main idea of the method is to use a conditional image synthesis approach to synthesize counterfactually explanations for the classifier decision boundaries. The authors show that their method can be used to generate visually meaningful explanations for model prediction."
7704,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"causal inference problem USED-FOR this. empirical objective USED-FOR algorithm. algorithm USED-FOR region of heterogeneity. algorithm COMPARE baselines. baselines COMPARE algorithm. real - world healthcare datasets EVALUATE-FOR algorithm. OtherScientificTerm are drug - related offenses, inter - decision - maker disagreement, generalization bound, and clinical knowledge. ","This paper studies the problem of causal inference in the context of drug-related offenses, where the goal is to infer whether a given drug is responsible for an individual’s criminal history. The authors propose a novel algorithm for this problem, which is based on the observation that there is a region of heterogeneity in the decision-maker disagreement in the setting where there is inter-decision disagreement. They show that the generalization bound of their algorithm is O(1/\sqrt{T}) where T is the number of participants in the dataset. They also show that their algorithm achieves a better generalization error bound compared to previous work. ","This paper studies the problem of causal inference in the context of drug-related offenses. The authors propose a new algorithm for this problem, which is motivated by the fact that the generalization bound of previous work does not take into account the inter-decision-maker disagreement. To address this issue, the authors propose an empirical objective for the causal inference problem. The empirical objective is to find the region of heterogeneity in the dataset where there is a high level of heterogeneity between the decision-maker and the drug-user groups. The proposed algorithm is evaluated on a variety of real-world healthcare datasets, and it is shown that the proposed algorithm outperforms existing baselines."
7720,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"perspective USED-FOR image synthesis. visual token generation problem USED-FOR task. paradigms COMPARE formulation. formulation COMPARE paradigms. flexible local manipulation USED-FOR image regions. formulation USED-FOR flexible local manipulation. latent tokens USED-FOR visual tokens. latent tokens USED-FOR it. constant content tokens CONJUNCTION style tokens. style tokens CONJUNCTION constant content tokens. style tokens PART-OF latent space. visual tokens USED-FOR TokenGAN. constant content tokens HYPONYM-OF visual tokens. style tokens HYPONYM-OF visual tokens. TokenGAN USED-FOR image synthesis. style tokens USED-FOR TokenGAN. Transformer USED-FOR attention mechanism. attention mechanism USED-FOR styles. attention mechanism USED-FOR TokenGAN. FFHQ CONJUNCTION LSUN CHURCH. LSUN CHURCH CONJUNCTION FFHQ. image synthesis benchmarks EVALUATE-FOR TokenGAN. LSUN CHURCH HYPONYM-OF image synthesis benchmarks. FFHQ HYPONYM-OF image synthesis benchmarks. generator USED-FOR high - fidelity images. 1024× 1024 size FEATURE-OF high - fidelity images. OtherScientificTerm are latent code, content tokens, and convolutions. Method is token - based generator. ",This paper proposes a novel token-based image synthesis method based on the visual token generation problem. The key idea is to use style tokens in the latent space of the generator to generate visual tokens. The style tokens are generated by a transformer-based attention mechanism. The proposed method achieves state-of-the-art performance on image synthesis tasks on FFHQ and LSUN.,This paper proposes a novel visual token generation method for image synthesis. The key idea is to use a transformer-based attention mechanism to generate visual tokens for each image. The attention mechanism is based on the attention mechanism of the Transformer. The proposed method is evaluated on a variety of image synthesis benchmarks. 
7736,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"min - norm interpolators CONJUNCTION max - margin classifiers. max - margin classifiers CONJUNCTION min - norm interpolators. overparameterization USED-FOR min - norm interpolators. overparameterization USED-FOR variance. variance FEATURE-OF min - norm interpolators. overparameterization USED-FOR max - margin classifiers. ridge regularization USED-FOR high dimensions. generalization EVALUATE-FOR avoiding interpolation. ridge regularization USED-FOR avoiding interpolation. linear regression CONJUNCTION classification. classification CONJUNCTION linear regression. linear regression FEATURE-OF robust risk. classification FEATURE-OF robust risk. OtherScientificTerm are noise, and interpolation. Task is robust overfitting. ","This paper studies the robust overfitting in the presence of min-norm interpolators and max-margin classifiers. In particular, the authors show that the variance of the min-Norm interpolator and the max-Margin classifier with overparametrized training is bounded by a ridge regularization term. The authors then show that this ridge regularized classifier can be used to avoid interpolation in linear regression and linear classification. ",This paper studies the problem of robust overfitting of min-norm interpolators and max-margin classifiers in the presence of noise. The authors show that the overparameterization of the min-Norm interpolator can lead to a large variance in the variance of the max-Margin classifier. They show that this variance can be reduced by using ridge regularization. They also show that ridge regularisation can be used to improve the generalization performance of the classifiers.   
7752,SP:09f080f47db81b513af26add851822c5c32bb94e,"canonical primitive USED-FOR arbitrarily ordered point cloud. sphere HYPONYM-OF canonical primitive. primitive USED-FOR unordered point clouds. unordered point clouds PART-OF canonical surface. annotation CONJUNCTION selfsupervised part segmentation network. selfsupervised part segmentation network CONJUNCTION annotation. method USED-FOR unaligned input point clouds. selfsupervised part segmentation network USED-FOR method. annotation USED-FOR method. rotation range FEATURE-OF unaligned input point clouds. 3D semantic keypoint transfer CONJUNCTION part segmentation transfer. part segmentation transfer CONJUNCTION 3D semantic keypoint transfer. model COMPARE correspondence learning methods. correspondence learning methods COMPARE model. 3D semantic keypoint transfer EVALUATE-FOR model. part segmentation transfer EVALUATE-FOR model. Method are canonical point autoencoder ( CPAE ), and autoencoder. Material is 3D shapes. OtherScientificTerm is primitive surface. Generic is models. ","This paper proposes a method for learning a canonical point autoencoder for 3D shape prediction. The method is based on the idea that a canonical primitive can be viewed as a set of points on a sphere, and the goal is to learn to predict points on the surface of the canonical primitive. The authors propose to use a self-supervised part-segmentation network to learn the part of the point cloud that is closest to the canonical surface. The proposed method is evaluated on the task of 3D semantic keypoint transfer.   ","This paper proposes a new canonical point autoencoder (CPAE) model for point cloud classification. CPAE is based on the notion of a ""canonical primitive"", which is an arbitrarily ordered point cloud that can be represented as a sphere. The authors propose to use a self-supervised part segmentation network to segment the input point clouds into a set of points. They show that the proposed method can be used to learn a canonical primitive for unordered point clouds. They also show that their method is able to transfer 3D semantic keypoint transfer between the input and the canonical primitive."
7768,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"Domain generalization ( DG ) methods USED-FOR generalizability. empirical risk minimization ( ERM ) approach COMPARE methods. methods COMPARE empirical risk minimization ( ERM ) approach. complex, non - convex loss function USED-FOR ERM. sharp minima USED-FOR sub - optimal generalizability. domain generalization gap EVALUATE-FOR flat minima. method USED-FOR flat minima. SWAD COMPARE vanilla SWA. vanilla SWA COMPARE SWAD. SWAD USED-FOR flatter minima. VLCS CONJUNCTION OfficeHome. OfficeHome CONJUNCTION VLCS. OfficeHome CONJUNCTION TerraIncognita. TerraIncognita CONJUNCTION OfficeHome. TerraIncognita CONJUNCTION DomainNet. DomainNet CONJUNCTION TerraIncognita. PACS CONJUNCTION VLCS. VLCS CONJUNCTION PACS. DG benchmarks EVALUATE-FOR SWAD. outof - domain accuracy EVALUATE-FOR SWAD. PACS HYPONYM-OF DG benchmarks. OfficeHome HYPONYM-OF DG benchmarks. DomainNet HYPONYM-OF DG benchmarks. VLCS HYPONYM-OF DG benchmarks. TerraIncognita HYPONYM-OF DG benchmarks. SWAD COMPARE generalization methods. generalization methods COMPARE SWAD. data augmentation and consistency regularization methods HYPONYM-OF generalization methods. SWAD CONJUNCTION DG method. DG method CONJUNCTION SWAD. SWAD USED-FOR DG. DG method USED-FOR DG. SWAD USED-FOR DG methods. Method are DomainBed, and Stochastic Weight Averaging Densely ( SWAD ). OtherScientificTerm is overfitting. Metric is in - domain generalizability. ",This paper proposes a novel method for domain generalization based on stochastic weight averaging. The main idea is to use a stochastically weighted average over the weights of the training set to estimate the generalization gap between the training and test domains. The method is based on the observation that the sharp minima of the empirical risk minimization (ERM) method have a sub-optimal generalizability in the test domain. The authors then propose a new method called Stochastic Weight Averaging Densely (SWAD) to improve the performance of the ERM method.   The main contribution of the paper is to show that SWAD can improve the in-domain generalization performance of ERM methods. ,"This paper proposes a new method for domain generalization (DG) based on Stochastic Weight Averaging Densely (SWAD). SWAD is based on the idea that sharp minima are sub-optimal generalizability, while flat minima can be sub-optimally generalizable. The authors show that SWAD can be used to improve the in-domain generalization of DG methods. They also show that it can improve the out-of-domain accuracy of DG."
7784,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"initialization time CONJUNCTION query time. query time CONJUNCTION initialization time. evaluation metric CONJUNCTION optimization. optimization CONJUNCTION evaluation metric. weight - sharing CONJUNCTION supervised learning. supervised learning CONJUNCTION weight - sharing. supervised learning CONJUNCTION zero - cost proxies. zero - cost proxies CONJUNCTION supervised learning. learning curve extrapolation CONJUNCTION weight - sharing. weight - sharing CONJUNCTION learning curve extrapolation. zero - cost proxies HYPONYM-OF techniques. learning curve extrapolation HYPONYM-OF techniques. supervised learning HYPONYM-OF techniques. weight - sharing HYPONYM-OF techniques. technique USED-FOR predictor - based NAS frameworks. Task are neural architecture search ( NAS ), and performance predictors. Method are neural networks, neural architectures, and performance prediction methods. Metric are correlationand rank - based performance measures, and predictive power. OtherScientificTerm is predictors. Generic is code. ","This paper studies the problem of predicting the performance of neural networks in NAS. The authors propose a method to estimate the predictive power of a predictor-based NAS framework in terms of the correlation and rank-based performance measures. The method is based on learning curve extrapolation, weight-sharing, and zero-cost proxies. The proposed method is evaluated on a variety of NAS benchmarks and shows that it outperforms the baselines. ","This paper studies the problem of predicting performance predictors for neural architecture search (NAS) in the context of correlation and rank-based performance measures. The authors propose a method for predicting the performance of a predictor-based NAS framework. The method is based on the idea of learning the correlation-and-rank performance measures, and the authors show that it can be used to estimate the predictive power of the predictors. They also show that their method can be applied to zero-cost proxies and supervised learning."
7800,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,Dirichlet posterior sampling CONJUNCTION privacy 4 guarantees. privacy 4 guarantees CONJUNCTION Dirichlet posterior sampling. exponential families FEATURE-OF differential privacy of posterior sampling. truncated concentrated differential privacy ( tCDP ) USED-FOR privacy guarantee. privacy guarantee FEATURE-OF Dirichlet posterior sampling. Dirichlet posterior sampling USED-FOR Multinomial8 Dirichlet sampling. Dirichlet posterior sampling USED-FOR private normalized histogram publishing. accuracy guarantees EVALUATE-FOR Dirichlet posterior sampling. Metric is inherent privacy. OtherScientificTerm is Dirichlet posterior 1 distribution. Method is posterior sampling. ,"This paper studies the problem of posterior sampling in the context of private normalized histogram publishing. In this setting, the goal is to ensure that the posterior distribution of the histogram is distributed in a privacy-preserving manner. The main contribution of this paper is to study the privacy properties of the Dirichlet posterior sampling.   The main contributions of the paper are as follows:  1. The authors show that the privacy of the posterior sampling can be guaranteed by the use of truncated concentrated differential privacy (tCDP).  2. They show that this privacy can be achieved by using the Diriclet posterior 1 distribution.  3. They prove that under certain assumptions on the distribution, the privacy guarantees of the tCDP can be obtained.  4. They also show that if the distribution is in the exponential family, then the privacy guarantee is guaranteed.  Finally, they show that for the Multinomial distribution, they can achieve the same privacy guarantees. ","This paper studies the problem of private normalized histogram publishing in the context of Dirichlet posterior sampling. In particular, the authors consider the case where the posterior distribution of the histogram is sampled from an exponential family of exponential families. They prove that the privacy of posterior sampling is guaranteed by a truncated concentrated differential privacy (tCDP) guarantee. They show that the tCDP can be used to guarantee the privacy guarantee of the posterior sampling in the case of the exponential family. They also provide a theoretical analysis of the privacy guarantees.  "
7816,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,clustering CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION clustering. Random walks USED-FOR machine learning algorithms. parallel algorithm USED-FOR random walks. random walks USED-FOR it. random walk USED-FOR algorithm. technique USED-FOR parallel local clustering algorithm. algorithm COMPARE approaches. approaches COMPARE algorithm. Generic is method. OtherScientificTerm is graph. ,This paper proposes a new clustering algorithm based on random walks for graph clustering. The main idea is to use random walks as a regularization term in the clustering process. The authors show that this random walk can be used to improve the performance of clustering algorithms in the context of semi-supervised learning.   The main contribution of this paper is to propose a new algorithm for clustering that uses random walks to improve clustering performance. ,"This paper proposes a new algorithm for local clustering based on random walks. The main idea of the paper is to use a random walk as a way to speed up the clustering process. The authors show that their algorithm can be applied to a variety of clustering problems, including semi-supervised learning and clustering in the context of graph clustering. They also show that it can be used to improve the performance of existing clustering algorithms. "
7832,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,statistical mechanics USED-FOR replica method. typical sample complexity EVALUATE-FOR ` 1 - LinR. paramagnetic phase FEATURE-OF random regular graphs. ` 1 - LinR USED-FOR model selection. order of sample complexity EVALUATE-FOR model selection. order of sample complexity EVALUATE-FOR ` 1 - LinR. method USED-FOR nonasymptotic behavior. precision CONJUNCTION recall. recall CONJUNCTION precision. method USED-FOR ` 1 - LinR. nonasymptotic behavior FEATURE-OF ` 1 - LinR. precision HYPONYM-OF nonasymptotic behavior. recall HYPONYM-OF nonasymptotic behavior. ` 1 - LogR CONJUNCTION interaction screening. interaction screening CONJUNCTION ` 1 - LogR. method USED-FOR ` 1 - regularized M -estimators. interaction screening HYPONYM-OF ` 1 - regularized M -estimators. ` 1 - LogR HYPONYM-OF ` 1 - regularized M -estimators. Task is Ising model selection. OtherScientificTerm is model misspecification. Metric is M. Method is Ising model. ,"This paper studies the problem of model selection in the Ising model, where the goal is to select a model that maximizes the probability that the model is correct. The main contribution of this paper is to propose a replica method for model selection, which is based on the LinR replica method. The method is shown to have a typical sample complexity of $O(1/\sqrt{T})$ in terms of the number of samples required to select the correct model.   The main contributions of the paper are as follows:  1. The authors show that the replica method can be viewed as a variant of the Lipschitz method.  2. They show that it can be seen as a generalization of the Lin-R method, which can be used to select random regular graphs with a paramagnetic phase.  3. They prove that the method is non-asymptotic with respect to precision and recall. ",This paper proposes a replica method for model selection for random regular graphs with paramagnetic phase. The method is based on the fact that the sample complexity of model selection is bounded by the order of sample complexity. The main contribution of the paper is to show that the model selection process is non-asymptotic and that the precision and recall of the model is not affected by the model misspecification. The paper also proposes a method to select the correct model from the sample selection process.   
7848,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"k - means USED-FOR datasets. fuzzy or soft k - means objective HYPONYM-OF kmeans problem. framework USED-FOR clustering. similarity queries USED-FOR polynomial - time approximation algorithm. algorithms USED-FOR fuzzy clustering. k - means USED-FOR fuzzy k - means objective. non - negative matrix factorization HYPONYM-OF nonconvex problem. Lloyd - type algorithms CONJUNCTION alternating - minimization algorithms. alternating - minimization algorithms CONJUNCTION Lloyd - type algorithms. similarity queries USED-FOR problem. real - world applications EVALUATE-FOR algorithms. real - world datasets EVALUATE-FOR algorithms. Method is semisupervised active clustering framework. OtherScientificTerm are similarity, O(poly(k ) log n ) similarity queries, and local minima. Metric is polynomialtime - complexity. ","This paper studies the problem of fuzzy clustering with k-means, i.e. clustering over a set of k data points, where the goal is to cluster the data points in such a way that they are close to each other in terms of similarity. The main contribution of this paper is to provide a polynomial-time approximation algorithm for this problem. The algorithm is based on a semi-supervised active clustering framework, where each cluster is sampled from a distribution over k points, and the objective is to find a cluster that is close enough to all the clusters in the distribution.   The main contributions of the paper are as follows:  1. The authors show that the problem is a non-convex non-negative matrix factorization problem, which can be solved using a combination of alternating-minimization and Lloyd-type algorithms.  2. They show that under certain assumptions on the number of clusters and the size of the distribution, they can obtain an approximate solution to the problem in polynomially time.  3. They provide an approximation to the original problem by using an alternating-Minimization-based algorithm.  4. They also show that they can get an approximation of the true problem using the proposed algorithm.","This paper studies the problem of fuzzy or soft k-means clustering, where the objective is to find a set of samples with high similarity (i.e., the number of samples that are close enough to be classified as ""fuzzy"" or ""soft"" in the dataset) and then to cluster the samples in such a way that they are close to each other. The main contribution of this paper is to provide a polynomial-time approximation algorithm for this problem. The algorithm is based on a semisupervised active clustering framework, which is motivated by the fact that it is polynomially time-consuming to find samples with low similarity. The authors show that their algorithm can be approximated by a Lloyd-type algorithm and an alternating-minimization algorithm."
7864,SP:a8057c4708dceb4f934e449080043037a70fabf7,"models USED-FOR planning. value functions CONJUNCTION policies. policies CONJUNCTION value functions. computation USED-FOR policies. computation USED-FOR value functions. model CONJUNCTION value function. value function CONJUNCTION model. approach COMPARE planning methods. planning methods COMPARE approach. Dyna HYPONYM-OF planning methods. self - consistency USED-FOR policy evaluation. self - consistency USED-FOR control. policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. tabular and function approximation settings EVALUATE-FOR these. Method are reinforcement learning ( RL ) agents, model - based RL, and self - consistency updates. OtherScientificTerm is environment interactions. ","This paper proposes to use self-consistency updates in reinforcement learning (RL) to improve the performance of model-based RL agents. In particular, the authors propose to update the value function of the policy in a way that is consistent with the value of the model. The authors show that this is equivalent to using self- consistency updates for policy evaluation and control. The proposed method is evaluated in tabular and function approximation settings.  ",This paper proposes a new method for planning in reinforcement learning (RL) with model-based RL. The main idea is to use self-consistency updates to improve the performance of the agent in the planning process. The authors show that their method outperforms the state-of-the-art Dyna-based planning methods in a variety of tabular and function approximation settings.
7880,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"Episodic training USED-FOR models. few - shot learning USED-FOR models. Episodic training PART-OF few - shot learning. method USED-FOR episode sampling distributions. difficulty USED-FOR method. curriculum HYPONYM-OF sampling schemes. few - shot learning accuracies EVALUATE-FOR episodic training algorithms. algorithms CONJUNCTION network architectures. network architectures CONJUNCTION algorithms. network architectures CONJUNCTION protocols. protocols CONJUNCTION network architectures. few - shot learning datasets CONJUNCTION algorithms. algorithms CONJUNCTION few - shot learning datasets. network architectures EVALUATE-FOR method. algorithms EVALUATE-FOR method. few - shot learning datasets EVALUATE-FOR method. protocols EVALUATE-FOR method. Material is limited labelled data. Method are episodic training, and sampling method. OtherScientificTerm is episode difficulty. ",This paper studies episodic training in few-shot learning. The authors propose a new sampling scheme for episodic learning based on the difficulty of the episode distribution. They show that this sampling scheme can be used to improve the performance of the model. They also provide a theoretical analysis of the effect of the difficulty on the performance.   ,"This paper proposes a new sampling method for episodic few-shot learning. The sampling method is based on the notion of ""difficulty"", which is defined as the number of episodes per episode in a training set. The authors show that the difficulty of the episode sampling distribution is proportional to the difficulty in the training set, and propose a curriculum-based sampling scheme. The method is evaluated on a variety of datasets and architectures. "
7896,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"logistic bandits USED-FOR binary rewards. logistic bandits HYPONYM-OF ones. ones PART-OF generalized linear bandits. logistic bandits HYPONYM-OF generalized linear bandits. algorithms USED-FOR logistic bandits. unknown parameter FEATURE-OF MNL probabilistic model. MNL - UCB HYPONYM-OF upper confidence bound ( UCB)-based algorithm. MNL - UCB USED-FOR problem. Generic is extension. Method is multinomial logit ( MNL ). OtherScientificTerm are revenue parameter, problemdependent constants, and loose regret bounds. ","This paper studies the problem of logistic bandits, a class of generalized linear bandits in which the reward function is assumed to be a multinomial-logit (MNL) function. The authors show that under certain assumptions on the model parameters, the regret of the bandit is bounded by the upper confidence bound (UCB) of the logistic bandit. They also provide a UCB-based algorithm for the problem.  ","This paper proposes a new upper confidence bound (UCB-based) algorithm for generalized linear bandits. The UCB is based on the multinomial logit (MNL) probabilistic model, and the authors show that the UCB can be extended to the problem of estimating the regret of a logistic bandit with unknown parameters. They show that under certain assumptions, the upper confidence bounds can be derived for the problem. The authors also provide a theoretical analysis of the upper bounds. "
7912,SP:0eaf058ed224464f6682cbbd80f716c89759f467,max - min entropy framework USED-FOR reinforcement learning ( RL ). maximum entropy RL USED-FOR model - free sample - based learning. soft actor - critic ( SAC ) algorithm USED-FOR maximum entropy RL. maximum entropy RL USED-FOR policies. entropy USED-FOR exploration. entropy FEATURE-OF low - entropy states. max - min entropy framework USED-FOR algorithm. algorithm USED-FOR Markov decision processes ( MDPs ). algorithm COMPARE RL algorithms. RL algorithms COMPARE algorithm. ,"This paper studies the problem of model-free sample-based reinforcement learning with maximum entropy. The authors propose a soft actor-critic (SAC) algorithm for maximum entropy RL, where the entropy of low-entropy states is used to guide exploration. They show that the SAC algorithm can be used to learn policies that maximize the entropy in low-enlightened states. They also provide a theoretical analysis of the performance of the proposed algorithm. ","This paper proposes a new approach to model-free sample-based learning for maximum entropy RL. The authors propose a soft actor-critic (SAC) algorithm for maximum-entropy RL, which can be applied to a variety of RL algorithms. The SAC algorithm is based on the max-min entropy framework, which is a generalization of the SAC framework. The main contribution of the paper is to propose a new algorithm that can be used to learn a policy that is robust to low-entropic MDPs. The algorithm is shown to be competitive with the state-of-the-art in terms of performance. "
7928,SP:19107a648d3d23403a8693b065ee842833a0b893,"cross - sectional data USED-FOR learning task. continuous - time Markov chains USED-FOR problem. approximate likelihood maximization method USED-FOR continuous - time Markov chains. synthetic and real cancer data EVALUATE-FOR approach. OtherScientificTerm are genetic mutations, time order, and underspecification. Task is biomedical applications. Generic is methods. ",This paper studies the problem of learning from cross-sectional data with time-evolving Markov chains. The authors propose an approximate likelihood maximization method based on continuous-time Markov chain to solve the problem. The proposed method is evaluated on synthetic and real-world cancer data.   ,"This paper proposes a method for learning a continuous-time Markov chain from cross-sectional data. The main idea is to learn a continuous time-varying distribution over the cross-sections of the data, and then use a likelihood maximization method to maximize the likelihood of the distribution. The authors show that their approach outperforms the state-of-the-art methods on synthetic and real cancer data. "
7944,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,Document intelligence USED-FOR business applications. self - supervised learning methods USED-FOR annotation efforts. large - scale unlabeled document datasets USED-FOR self - supervised learning methods. self - supervised objectives FEATURE-OF models. models USED-FOR annotation efforts. unified pretraining framework USED-FOR document understanding. UDoc HYPONYM-OF unified pretraining framework. UDoc USED-FOR document understanding tasks. Transformer USED-FOR UDoc. multimodal embeddings USED-FOR Transformer. semantic region USED-FOR words and visual features. it USED-FOR generic representation. representation USED-FOR similarities. self - supervised losses USED-FOR it. self - supervised losses USED-FOR generic representation. pretraining procedure USED-FOR joint representations. pretraining procedure USED-FOR downstream tasks. Material is documents. Method is document pretraining methods. ," is a self-supervised learning method for document classification. This paper proposes a novel pretraining framework for document understanding based on the Transformer architecture. The Transformer is used to learn a joint representation of words and visual features, which is then used for downstream tasks. The proposed method achieves state-of-the-art performance on several document classification tasks.",", the paper proposes a unified pretraining framework, UDoc, for document understanding. UDoc is based on a Transformer-based framework, where the Transformer encodes the words and visual features into a generic representation, which is then used to train a self-supervised model. The proposed method is evaluated on a variety of document understanding tasks. "
7960,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"individual fairness FEATURE-OF data clustering problems. lp - norm objectives FEATURE-OF data clustering problems. k - MEDIAN HYPONYM-OF data clustering problems. k - MEDIAN HYPONYM-OF lp - norm objectives. objective guarantees FEATURE-OF l∞ or k - CENTER objective. clustering algorithm USED-FOR l∞ or k - CENTER objective. objective guarantees FEATURE-OF clustering algorithm. local - search algorithm USED-FOR lp - norms. kMEDIAN CONJUNCTION k - MEANS. k - MEANS CONJUNCTION kMEDIAN. algorithms USED-FOR problem. Linear Programming ( LP ) techniques USED-FOR algorithms. theoretical fairness guarantees COMPARE MV20. MV20 COMPARE theoretical fairness guarantees. sparsification technique USED-FOR algorithm. run - time EVALUATE-FOR algorithm. run - time EVALUATE-FOR sparsification technique. Generic are dataset, concept, and objective. OtherScientificTerm is individual fairness constraint. Metric are fairness, and worst - case guarantee. Method is LP rounding techniques. ",This paper studies the problem of data clustering with k-median objective under individual fairness constraint. The authors propose a clustering algorithm for the l-norm objective and a local search algorithm for lp-norm objectives. They show that the proposed algorithm achieves the worst-case performance of MV20 in terms of individual fairness. They also provide a theoretical analysis of the convergence of the algorithm.,This paper studies the problem of individual fairness in data clustering with lp-norm objective. The main contribution of the paper is to provide a theoretical guarantee of the lp norm objective for the l-median and k-center objective.  The main idea is to use a local-search algorithm to find the optimal lp norms for the k-Median objective. This is done by using a local search algorithm to search for the best lpnorms for each dataset. The paper also provides theoretical guarantees for the worst-case fairness of the proposed algorithm. 
7976,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"MAX - K - CUT CONJUNCTION correlation clustering. correlation clustering CONJUNCTION MAX - K - CUT. MAX - K - CUT HYPONYM-OF graph partitioning problems. correlation clustering HYPONYM-OF graph partitioning problems. MAX - K - CUT CONJUNCTION MAX - AGREE variant of correlation clustering. MAX - AGREE variant of correlation clustering CONJUNCTION MAX - K - CUT. methods USED-FOR MAX - K - CUT. methods USED-FOR approximation guarantees. methods USED-FOR SDPs. approximation guarantees CONJUNCTION MAX - K - CUT. MAX - K - CUT CONJUNCTION approximation guarantees. O(n ) constraints USED-FOR SDPs. polynomial - time Gaussian sampling - based algorithms USED-FOR problems. O(n + |E| ) memory USED-FOR polynomial - time Gaussian sampling - based algorithms. approach CONJUNCTION sparsification. sparsification CONJUNCTION approach. OtherScientificTerm are memory bottleneck, and dense graphs. Metric are storage complexity, and approximation ratio. ","This paper studies the problem of graph partitioning in the form of MAX-K-CUT and MAX-AGREE, where the goal is to partition a graph into a set of subgraphs. The main contribution of this paper is to provide a memory-efficient algorithm for solving the problem.   The main contributions of the paper are as follows:  1. Theoretical analysis of the memory complexity of the problem is provided.  2. A polynomial-time Gaussian sampling-based algorithm is proposed to solve the problem with O(n + |E|) memory.  3. The memory complexity is improved to O(1/\sqrt{n} + \epsilon^2) for the proposed method. ",This paper studies the problem of graph partitioning with O(n) constraints on the number of nodes in a graph. The main contribution of the paper is to study the memory bottleneck of the problem. The authors show that the problem can be solved with a polynomial-time Gaussian sampling-based algorithm. They also provide a theoretical analysis of the approximation ratio of the proposed method. 
7992,SP:cfd6cf88a823729c281059e179788248238a6ed7,predicting inter - frame motion information USED-FOR video prediction tasks. Motion - Aware Unit ( MAU ) USED-FOR inter - frame motion information. temporal receptive field FEATURE-OF predictive units. attention module CONJUNCTION fusion module. fusion module CONJUNCTION attention module. modules PART-OF MAU. fusion module HYPONYM-OF modules. attention module HYPONYM-OF modules. attention module PART-OF MAU. fusion module PART-OF MAU. attention module USED-FOR attention map. historical temporal states PART-OF augmented motion information ( AMI ). attention map USED-FOR historical temporal states. predictive unit USED-FOR temporal dynamics. receptive field USED-FOR predictive unit. receptive field USED-FOR temporal dynamics. fusion module USED-FOR augmented motion information ( AMI ). unit USED-FOR predictive models. encoders CONJUNCTION decoders. decoders CONJUNCTION encoders. information recalling scheme USED-FOR encoders. information recalling scheme USED-FOR decoders. video prediction CONJUNCTION early action recognition tasks. early action recognition tasks CONJUNCTION video prediction. early action recognition tasks EVALUATE-FOR MAU. video prediction EVALUATE-FOR MAU. MAU COMPARE methods. methods COMPARE MAU. tasks EVALUATE-FOR methods. tasks EVALUATE-FOR MAU. OtherScientificTerm is historical spatial states. ,"This paper proposes a method for predicting inter-frame motion information for video prediction tasks. The proposed method is based on an attention module and a fusion module. The attention module maps historical spatial states to a temporal receptive field, which is then used to train a predictive unit that predicts temporal dynamics. The fusion module is used to combine historical temporal states from the encoders and decoders. The method is evaluated on video prediction and early action recognition tasks.","This paper proposes a motion-aware unit (MAU) for video prediction. The MAU is composed of two modules: an attention module and a fusion module. The attention module maps historical temporal states into a map of historical spatial states, while the fusion module maps spatial states into an attention map. The fusion module encodes historical spatial information into a temporal receptive field, which is then used to predict the temporal dynamics of the video. The proposed method is evaluated on a variety of video prediction tasks, including video prediction and early action recognition tasks."
8008,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"neural net approximation of the Q function USED-FOR Deep Reinforcement Learning ( RL ). neural net approximations USED-FOR nonlinear RL. ReLU and polynomial activation functions PART-OF two - layer neural networks. two - layer neural networks USED-FOR function approximation. algorithm USED-FOR generative model setting. algebraic dimension FEATURE-OF sample complexity. Task is RL. OtherScientificTerm are neural net function class, and deterministic dynamics. Method is linear ( or eluder dimension ) methods. ","This paper studies the problem of learning non-linear Q function approximations in two-layer neural networks with ReLU and polynomial activation functions. The main contribution of this paper is to show that the sample complexity of learning the Q function is bounded by the algebraic dimension of the neural network function class, and deterministic dynamics.    The main contributions of the paper are as follows:   1. The authors show that learning nonlinear Q functions in two layer neural networks is equivalent to learning a generative model in the deterministic model setting.  2. They provide an algorithm for learning the function approximation in the generative models setting, and show that it can be done with sample complexity bounded by a constant factor.  3. They also provide a sample complexity bound for the detergent model setting, showing that their algorithm can be used to learn the Q-function in a deterministic manner. ","This paper studies the problem of nonlinear neural net approximations of the Q function in deep reinforcement learning (DRL). The main contribution of the paper is to study the sample complexity of a two-layer neural network with ReLU and polynomial activation functions in the generative model setting. The main result is that the number of samples in the two layer neural network is bounded by the algebraic dimension of the function approximation. The authors show that for a deterministic model with deterministic dynamics, they can find a sample complexity that is lower than that of linear methods. "
8024,SP:cac881243abde92a28c110f5bd84d115ed189bda,"representations USED-FOR zero - shot transfer. Deep Metric Learning ( DML ) USED-FOR representations. priori unknown test distributions USED-FOR zero - shot transfer. ooDML benchmark USED-FOR generalization. ooDML USED-FOR generalization. ooDML USED-FOR train - to - test distribution shifts. benchmark EVALUATE-FOR DML methods. few - shot DML USED-FOR generalization. unknown test shifts FEATURE-OF generalization. OtherScientificTerm are distribution shifts, train - test splits, out - of - distribution shifts, and distribution shift. Method are DML, and ooDML1. Generic is methods. ","This paper proposes a new benchmark for zero-shot transfer learning in the presence of distribution shifts between test and training sets. The benchmark is designed to evaluate the generalization ability of deep metric learning (DML) methods on distribution shifts. The main contribution of the paper is the introduction of the ooDML benchmark, which aims to measure the performance of DML methods on test-to-test distribution shift. The authors show that DML models trained on the new benchmark outperform the state-of-the-art few-shot learning methods. ","This paper proposes a new benchmark for zero-shot transfer learning, called ooDML, which aims to measure the generalization performance of a few-shot DML method in the presence of distribution shifts in the test distribution. The authors show that the performance of the few shot DML methods is affected by the out-of-distribution shifts. They also show that a training-to-test distribution shift can be a significant factor in the lack of generalization.  The authors also provide a theoretical analysis of the effect of the distribution shifts."
8040,SP:bacff3685476855a32549d03095375649fd89df2,"outlier detection algorithm CONJUNCTION hyperparameter(s ). hyperparameter(s ) CONJUNCTION outlier detection algorithm. model HYPONYM-OF hyperparameter(s ). data - driven approach USED-FOR UOMS. METAOD HYPONYM-OF data - driven approach. meta - learning USED-FOR data - driven approach. meta - learning USED-FOR METAOD. model selection USED-FOR clustering. UOMS problem COMPARE model selection. model selection COMPARE UOMS problem. model evaluations CONJUNCTION model comparisons. model comparisons CONJUNCTION model evaluations. model USED-FOR dataset. METAOD USED-FOR model. detection models USED-FOR METAOD. historical outlier detection benchmark datasets EVALUATE-FOR detection models. meta - learning framework USED-FOR task similarity. metafeatures USED-FOR task similarity. meta - learning techniques USED-FOR UOMS. model COMPARE model selection. model selection COMPARE model. METAOD COMPARE model selection. model selection COMPARE METAOD. model selection COMPARE meta - learning techniques. meta - learning techniques COMPARE model selection. METAOD USED-FOR model. meta-)training EVALUATE-FOR METAOD. METAOD CONJUNCTION meta - learning database. meta - learning database CONJUNCTION METAOD. METAOD USED-FOR UOMS problem. meta - learning database USED-FOR UOMS problem. Task are unsupervised outlier detection task, unsupervised outlier model selection ( UOMS ) problem, model evaluation, and model comparison. OtherScientificTerm is universal objective function. Generic is task. ",This paper proposes a data-driven approach to unsupervised outlier detection using meta-learning. The idea is to train a model for each outlier using a meta-training dataset and then use that model to select the best outlier from a set of outlier models. The proposed approach is evaluated on two benchmark datasets and compared with model selection and model comparison. ,This paper proposes a data-driven approach to unsupervised outlier detection using meta-learning. The main idea of the paper is to use meta-training to learn a universal objective function that can be used to select the best outlier model for each dataset. The authors show that the proposed method outperforms model selection and model comparison in terms of performance on historical outlier datasets.
8056,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"Prediction+optimization HYPONYM-OF real - world paradigm. SPO+ CONJUNCTION direct optimization. direct optimization CONJUNCTION SPO+. direct optimization HYPONYM-OF decision - focused prediction approaches. SPO+ HYPONYM-OF decision - focused prediction approaches. max operator USED-FOR real - world objectives. max operator USED-FOR soft constraints. framework USED-FOR closed - form solution. predictive parameters CONJUNCTION gradients. gradients CONJUNCTION predictive parameters. predictive parameters USED-FOR closed - form solution. gradients USED-FOR closed - form solution. synthetic linear programming CONJUNCTION portfolio optimization. portfolio optimization CONJUNCTION synthetic linear programming. portfolio optimization CONJUNCTION resource provisioning. resource provisioning CONJUNCTION portfolio optimization. method COMPARE decision - focused approaches. decision - focused approaches COMPARE method. two - staged methods CONJUNCTION decision - focused approaches. decision - focused approaches CONJUNCTION two - staged methods. method COMPARE two - staged methods. two - staged methods COMPARE method. applications EVALUATE-FOR method. method COMPARE method. method COMPARE method. soft constraints USED-FOR method. soft constraints FEATURE-OF applications. applications EVALUATE-FOR method. resource provisioning HYPONYM-OF applications. synthetic linear programming HYPONYM-OF applications. portfolio optimization HYPONYM-OF applications. Task are optimization problem, and downstream optimization problem. Method are prediction model, and analytically differentiable surrogate objective framework. Generic is they. OtherScientificTerm are soft linear and non - negative hard constraints, and theoretical bounds. ","This paper proposes a novel surrogate objective framework for prediction-optimization, where the goal is to find a closed-form solution to the optimization problem with a prediction model. The main idea is to use a surrogate objective that is differentiable and analytically differentiable. The proposed surrogate objective is a combination of two components: (1) a predictive parameter that is used to estimate the gradients of the loss function, and (2) a set of soft constraints that are used to constrain the solution to be close to the true solution. The paper shows that this surrogate objective can be used to solve the optimization problems with both soft and hard constraints. Theoretical analysis is provided to show that the surrogate objective converges to a closed form solution with high probability.  ","This paper proposes a surrogate objective framework for prediction-optimization (prediction+optimization) where the goal is to find a closed-form solution to a set of soft and hard constraints. The main idea is to use the max operator for the soft constraints, and the hard constraints can be either linear or non-negative. The paper provides theoretical bounds on the convergence of the proposed surrogate objective. Experiments on synthetic linear programming, portfolio optimization, and resource provisioning show that the proposed method outperforms the state-of-the-art. "
8072,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"graph USED-FOR GNN. GNN USED-FOR DropGNNs. DropGNNs USED-FOR graph neighborhoods. GNN benchmarks EVALUATE-FOR DropGNNs. Method are Dropout Graph Neural Networks ( DropGNNs ), GNN frameworks, and message passing GNNs. Generic is approach. OtherScientificTerm is theoretical bounds. Metric is expressiveness. ","This paper proposes DropGNNs, a new GNN architecture that uses drop-out GNNs to improve the expressiveness of message passing GNN models. The authors show that the proposed method is computationally efficient and achieves state-of-the-art performance on standard GNN benchmarks. The main contribution of the paper is a theoretical analysis of the dropout graph neural network (DropGNN) architecture. Theoretical analysis shows that the drop out GNN can be used to reduce the number of nodes in a GNN by a factor of at least 1. The paper also provides theoretical guarantees on the expressivity of the proposed model. ","This paper studies the expressiveness of Dropout Graph Neural Networks (DropGNNs), a new GNN framework for graph neural networks (GNN). The authors show that DropGNN can be more expressive than other GNNs in terms of the number of nodes in the graph. They also provide theoretical bounds on the expressive power of the Dropout GNN. Theoretical results are provided for the first time in the literature. "
8088,SP:090dc0471d54e237f423034b1e1c46a510202807,"Transformers USED-FOR visual tasks. global representation capacities FEATURE-OF Transformers. local and global pattern features USED-FOR image classification. representation capacity FEATURE-OF local and global pattern features. DS - Net USED-FOR fine - grained and integrated features. DS - Net USED-FOR them. Inter - Scale Alignment module USED-FOR information interaction. Intra - scale Propagation module CONJUNCTION Inter - Scale Alignment module. Inter - Scale Alignment module CONJUNCTION Intra - scale Propagation module. Intra - scale Propagation module USED-FOR resolutions. Intra - scale Propagation module USED-FOR information interaction. contextual information USED-FOR downstream dense predictions. Vision Transformers CONJUNCTION ResNets. ResNets CONJUNCTION Vision Transformers. DS - Net COMPARE DeiT - Small. DeiT - Small COMPARE DS - Net. DS - Net COMPARE Vision Transformers. Vision Transformers COMPARE DS - Net. DS - Net COMPARE ResNets. ResNets COMPARE DS - Net. ImageNet-1k EVALUATE-FOR DeiT - Small. top-1 accuracy EVALUATE-FOR DeiT - Small. ImageNet-1k EVALUATE-FOR DS - Net. top-1 accuracy EVALUATE-FOR DS - Net. object detection CONJUNCTION instance segmentation. instance segmentation CONJUNCTION object detection. DS - Net - Small COMPARE ResNet-50. ResNet-50 COMPARE DS - Net - Small. DS - Net - Small USED-FOR object detection. mAP EVALUATE-FOR DS - Net - Small. DS - Net - Small USED-FOR instance segmentation. OtherScientificTerm are high - level local pattern information, features, and dual scales. Material is MSCOCO 2017. Generic is state - of - the - art scheme. Task is vision tasks. ","This paper proposes a novel architecture for image classification that combines local and global pattern features. The proposed architecture consists of two modules: an inter-scale alignment module and an intra-scale propagation module. The alignment module is used to align the features from different resolutions, while the propagation module uses contextual information to generate dense predictions for downstream dense predictions. Experiments on ImageNet-1k show that the proposed architecture achieves state-of-the-art performance on object detection and instance segmentation.","This paper proposes a new method for fine-grained and integrated features for image classification. The proposed method, called DS-Net, is based on two modules: Intra-scale Propagation module and Inter-Scale Alignment module. The two modules are designed to capture both local and global pattern features, and the proposed method achieves state-of-the-art performance on ImageNet-1k. "
8104,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,unified framework USED-FOR visual concepts. unified framework USED-FOR physics models of objects. visual concepts CONJUNCTION physics models of objects. physics models of objects CONJUNCTION visual concepts. videos USED-FOR physics models of objects. visual perception module CONJUNCTION concept learner. concept learner CONJUNCTION visual perception module. concept learner CONJUNCTION differentiable physics engine. differentiable physics engine CONJUNCTION concept learner. differentiable physics engine PART-OF components. visual perception module PART-OF components. concept learner HYPONYM-OF components. latent scene representations USED-FOR them. color CONJUNCTION shape. shape CONJUNCTION color. shape CONJUNCTION material. material CONJUNCTION shape. prior knowledge USED-FOR physics engine. concept learner USED-FOR visual concepts. language USED-FOR object - centric representations. object - centric representations USED-FOR visual concepts. material HYPONYM-OF visual concepts. color HYPONYM-OF visual concepts. shape HYPONYM-OF visual concepts. mass CONJUNCTION restitution. restitution CONJUNCTION mass. restitution CONJUNCTION velocity. velocity CONJUNCTION restitution. differentiable physical simulation USED-FOR physical properties. differentiable physical simulation USED-FOR differentiable physics model. video observations USED-FOR simulated trajectories. impulse - based differentiable rigid - body simulator USED-FOR differentiable physics model. grounded concepts USED-FOR differentiable physical simulation. velocity HYPONYM-OF physical properties. mass HYPONYM-OF physical properties. restitution HYPONYM-OF physical properties. concepts CONJUNCTION physical models. physical models CONJUNCTION concepts. differentiable physics PART-OF dynamic reasoning framework. VRDP COMPARE counterpart. counterpart COMPARE VRDP. accuracy EVALUATE-FOR predictive and counterfactual questions. physics models USED-FOR dynamics prediction. predictive and counterfactual questions EVALUATE-FOR VRDP. synthetic and real - world benchmarks EVALUATE-FOR dynamics prediction. accuracy EVALUATE-FOR VRDP. VRDP USED-FOR concepts. physical parameters USED-FOR VRDP. OtherScientificTerm is object - centric trajectories. Metric is interpretability. ,"This paper proposes a framework for learning to predict the dynamics of objects from video observations. The method is based on a combination of a visual perception module, a concept learner, and an impulse-based differentiable rigid-body simulator, which is trained to simulate the physics of an object. The model is able to generate object-centric trajectories from a set of video observations, which are then used to answer counterfactual questions.   ","This paper proposes a dynamic reasoning framework for visual concepts and physics models of objects based on video observations. The concept learner is a visual perception module and the physics engine is a differentiable rigid-body simulator. The physics model is based on an impulse-based differentiable physics model, which is used to simulate the physical properties of an object. The dynamics prediction is done by predicting the trajectory of the object based on the dynamics of the video observations, which are then used to answer a series of counterfactual questions."
8120,SP:c511066c38f9793bacb4986c564eafa36e032f39,"Active learning USED-FOR minimizing labeling costs. out - of - distribution data CONJUNCTION redundancy. redundancy CONJUNCTION out - of - distribution data. submodular information measures ( SIM ) USED-FOR acquisition functions. acquisition functions USED-FOR unified active learning framework. submodular information measures ( SIM ) USED-FOR unified active learning framework. one - stop solution USED-FOR active learning. SIMILAR USED-FOR active learning. SIMILAR COMPARE active learning algorithms. active learning algorithms COMPARE SIMILAR. MNIST CONJUNCTION ImageNet. ImageNet CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification tasks. CIFAR-10 HYPONYM-OF image classification tasks. MNIST HYPONYM-OF image classification tasks. DISTIL toolkit USED-FOR SIMILAR. Method are active learning methods, and Submodular Information Measures based actIve LeARning. OtherScientificTerm are imbalance or rare classes, and rare classes. Material is large real - world datasets. ","This paper proposes a novel active learning method based on submodular information measures (SIMs) for active learning. The main idea is to learn a set of acquisition functions that can be used to select a subset of data points from the training set. The proposed method is evaluated on CIFAR-10, MNIST, and ImageNet.  ","This paper proposes a unified active learning framework based on submodular information measures (SIM) and actIve LeARning. SIMILAR is a one-stop solution for active learning. The main idea is to use the DISTIL toolkit to learn a set of acquisition functions that can be combined with existing active learning methods. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet datasets."
8136,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"identity tests USED-FOR ranking data. Mallows model USED-FOR asymptotic and non - asymptotic settings. Mallows model USED-FOR ranking data. algorithms USED-FOR spread parameter. spread parameter FEATURE-OF Mallows model. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR asymptotic setting. sample - optimal non - asymptotic identity test USED-FOR it. Uniformly Most Powerful Unbiased ( UMPU ) test USED-FOR one. distribution of the sufficient statistic USED-FOR it. optimal learning algorithm USED-FOR Mallows model. optimal learning algorithm USED-FOR nonasymptotic test. Mallows models USED-FOR unknown central ranking case. asymptotic setting USED-FOR case. OtherScientificTerm is central ranking. Generic are test, and tests. Material is medium sized data. ","This paper studies identity tests for ranking data in the asymptotic and non-asymptotic settings. The authors propose a uniformly most powerful unbiased (UMPU) test, which is a sample-optimal non-asymptotically identity test for the unknown central ranking case, and an optimal learning algorithm for the Mallows model. They show that the distribution of the sufficient statistic is sufficient for the test to be valid. They also provide an algorithm to learn the distribution for the identity test.","This paper proposes a new identity test for asymptotic and non-asymptotic testing of the Mallows model. The main contribution of the paper is a sample-optimal non-asymptotically optimal identity test that can be applied to both the asymptic and nonasymptic setting. The proposed test is based on the Uniformly Most Powerful Unbiased (UMPU) test, which is an extension of the standard Mallows test. The authors show that the test can be used for both asymption and nonasymption. They also provide an optimal learning algorithm for the test.  "
8152,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"sparse multi - view cameras USED-FOR free - viewpoint video. pixel - aligned features USED-FOR radiance fields. heavy occlusions CONJUNCTION dynamic articulations of body parts. dynamic articulations of body parts CONJUNCTION heavy occlusions. parametric human body model USED-FOR robust performance capture. approach USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR generalizable neural radiance fields. parametric human body model USED-FOR approach. temporal transformer USED-FOR tracked visual features. skeletal body motion USED-FOR temporal transformer. skeletal body motion USED-FOR tracked visual features. multi - view transformer USED-FOR cross - attention. temporally - fused features CONJUNCTION pixel - aligned features. pixel - aligned features CONJUNCTION temporally - fused features. ZJU - MoCap and AIST datasets EVALUATE-FOR method. method COMPARE generalizable NeRF methods. generalizable NeRF methods COMPARE method. ZJU - MoCap and AIST datasets EVALUATE-FOR generalizable NeRF methods. OtherScientificTerm is appearance. Method are generalization approaches, and Neural Human Performer. ","This paper proposes a method for generalizing neural radiance fields (NeRF) from sparse multi-view cameras. The proposed method is based on a parametric human body model and a temporal transformer. The temporal transformer is used to capture the tracked visual features from skeletal body motion, and the cross-attention is used for pixel-aligned features. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method outperforms other generalizable NeRF methods.",This paper proposes a method to capture generalizable neural radiance fields from video. The proposed method is based on a parametric human model and a temporal transformer. The model is trained to capture the generalization properties of the radiance field. The method is evaluated on the ZJU-MoCap and AIST datasets. 
8168,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"recognition CONJUNCTION detection. detection CONJUNCTION recognition. Vision Transformer USED-FOR vision tasks. detection HYPONYM-OF vision tasks. recognition HYPONYM-OF vision tasks. neural architecture search USED-FOR process. architecture CONJUNCTION search space. search space CONJUNCTION architecture. E - T Error USED-FOR search dimensions. weight - sharing supernet USED-FOR E - T Error. weight - sharing supernet USED-FOR search dimensions. Swin CONJUNCTION DeiT. DeiT CONJUNCTION Swin. DeiT CONJUNCTION ViT. ViT CONJUNCTION DeiT. searched models COMPARE models. models COMPARE searched models. searched space COMPARE models. models COMPARE searched space. searched space USED-FOR searched models. S3 HYPONYM-OF searched models. ViT HYPONYM-OF models. Swin HYPONYM-OF models. DeiT HYPONYM-OF models. ImageNet EVALUATE-FOR models. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. semantic segmentation CONJUNCTION visual question answering. visual question answering CONJUNCTION semantic segmentation. S3 USED-FOR vision and vision - language tasks. semantic segmentation EVALUATE-FOR S3. visual question answering EVALUATE-FOR S3. object detection EVALUATE-FOR S3. Generic is architectures. Method is vision transformers. Task are space searching process, and vision transformer. OtherScientificTerm is Search Space. ","This paper proposes a new architecture search method for vision transformers. The main idea is to use a weight-sharing supernet to share the E-T error between the search space and the search dimensions. The proposed method is evaluated on image classification, object detection, semantic segmentation and visual question answering tasks. ","This paper proposes a method to search the search space of vision transformers. The main idea is to use a weight-sharing supernet to share the E-T Error between the search dimensions. The search space is then used to find the best model for a given task. The proposed method is evaluated on a variety of vision tasks, including object detection and semantic segmentation."
8184,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"learning from label proportions ( LLP ) framework USED-FOR linear threshold functions ( LTFs ). algorithm USED-FOR LTF. algorithm USED-FOR LTF. d - dimensional boolean vectors USED-FOR OR. accuracy EVALUATE-FOR LTF. LTFs USED-FOR monotone ORs. LTF HYPONYM-OF algorithm. unit - sized bags HYPONYM-OF supervised learning setup. accuracy EVALUATE-FOR algorithm. linear programming USED-FOR LTFs. techniques USED-FOR LLP setting. LTFs USED-FOR LLP learning of LTFs. LLP CONJUNCTION supervised learning. supervised learning CONJUNCTION LLP. inapproximability EVALUATE-FOR LLP learning LTFs. OtherScientificTerm are bags of feature - vectors, label proportions, bags, labeled feature - vectors, non - monochromatic bags, monotone OR, and non - monochromatic bags case. Metric are algorithmic bounds, and complexity. Generic is bound. ","This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In this setting, the class of LTFs is defined in terms of a set of d-dimensional boolean vectors, and the goal is to learn a linear threshold function from the set of labeled feature vectors. The main contribution of this paper is to provide an algorithm for learning the LTF in this setting. The algorithm is based on learning from the label proportions framework, which is an extension of the learning from labels (LPs) framework.   The main contributions of the paper are as follows:  1. The authors provide a new algorithm for the learning of the LTs in the case of monotone or monochromatic bags.  2. They show that the algorithm is equivalent to a linear programming algorithm in the non-monotone bags case.  3. They provide an inapproximability bound for the algorithm.  4. They also provide a generalization of their algorithm to the case with unit-sized bags and show that their algorithm can be used to train a classifier with the same accuracy. ","This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In particular, the authors propose a new algorithm for learning LTFs, which is based on the notion of “label proportions” (i.e., the size of the bag of labels). The authors show that under certain conditions, they can learn a LTF that is monotone in the case of monochromatic bags, and non-monochromatically in the non-Monochrome bags case. The authors also provide an algorithmic bound on the complexity of the algorithm.  "
8200,SP:2eb193c76355aac08003c9b377895202fd3bd297,extreme computational resources USED-FOR neural architecture search ( NAS ). benchmarks EVALUATE-FOR multi - fidelity techniques. learning curve extrapolation HYPONYM-OF multi - fidelity techniques. NAS - Bench-111 CONJUNCTION NAS - Bench-311. NAS - Bench-311 CONJUNCTION NAS - Bench-111. method USED-FOR surrogate benchmarks. NAS - Bench-311 CONJUNCTION NAS - Bench - NLP11. NAS - Bench - NLP11 CONJUNCTION NAS - Bench-311. singular value decomposition and noise modeling USED-FOR method. NAS - Bench-111 HYPONYM-OF surrogate benchmarks. NAS - Bench - NLP11 HYPONYM-OF surrogate benchmarks. NAS - Bench-311 HYPONYM-OF surrogate benchmarks. learning curve extrapolation framework USED-FOR single - fidelity algorithms. it COMPARE single - fidelity algorithms. single - fidelity algorithms COMPARE it. Material is tabular and surrogate benchmarks. Generic is architecture. OtherScientificTerm is architectures. Metric is validation accuracy. ,This paper proposes a learning curve extrapolation framework for multi-fidelity neural architecture search (NAS). The main idea is to use the singular value decomposition and noise modeling to estimate the learning curve of the architecture. The method is evaluated on NAS-bench-311 and NAS-Bench-NLP on tabular and surrogate benchmarks. The results show that the proposed method outperforms the single fidelity methods.,"This paper proposes a learning curve extrapolation framework for multi-fidelity neural architecture search (NAS). The method is based on singular value decomposition and noise modeling. The authors show that the proposed method outperforms the state-of-the-art on NAS-bench-111, NAS- Bench-311, and NAS-Bench-NLP11 benchmarks."
8216,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"user - centred method USED-FOR example - based explanations. SimplEx HYPONYM-OF user - centred method. corpus USED-FOR SimplEx. Integrated Jacobian HYPONYM-OF approach. mortality prediction CONJUNCTION image classification. image classification CONJUNCTION mortality prediction. tasks EVALUATE-FOR decompositions. image classification HYPONYM-OF tasks. mortality prediction HYPONYM-OF tasks. Method are machine learning models, convoluted latent representations, mixture of corpus latent representations, and model representations. Generic are latent representations, model, and mixture. OtherScientificTerm are latent space, post - hoc explanations, and features. ","This paper proposes SimplEx, a method for post-hoc explanations of machine learning models. The idea is to use a mixture of examples from the training set to generate explanations for the model. The method is based on the idea that the model is trained to predict the distribution of features in the latent space, which is then used as an explanation. The authors show that SimplEx is able to learn from the example-based explanations.   ","This paper proposes SimplEx, a user-centred method for post-hoc explanations of machine learning models. SimplEx is based on the Integrated Jacobian (i.e., a mixture of corpus latent representations and model representations), which is an extension of SimplEx. The authors show that SimplEx can be used to decompose the latent representations of a model into two parts: (1) the model representations, and (2) the mixture of model representations and the corpus representations.  The authors also show that their method can be applied to a variety of tasks, including mortality prediction and image classification. "
8232,SP:c8f82ec90f891d7394933483b7f926155ac363ef,image - text pairs USED-FOR multi - modal representations. Transformer USED-FOR images. CNN USED-FOR images. CNN - Transformer architecture USED-FOR VLP models. Visual relationship between visual contents USED-FOR image understanding. CNNs USED-FOR visual relation learning. visual relation CONJUNCTION inter - modal alignment. inter - modal alignment CONJUNCTION visual relation. objectives PART-OF Transformer network. learning visual relation PART-OF Transformer network. inter - modal alignment PART-OF Transformer network. learning visual relation HYPONYM-OF objectives. inter - modal alignment HYPONYM-OF objectives. design USED-FOR inter - modal alignment learning. Transformer USED-FOR inter - modal alignment learning. fully Transformer visual embedding USED-FOR inter - modal alignment. fully Transformer visual embedding USED-FOR VLP. fully Transformer visual embedding USED-FOR visual relation. Inter - Modality Flow ( IMF ) HYPONYM-OF metric. masking optimization mechanism USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR inter - modality learning. Masked Feature Regression ( MFR ) USED-FOR Transformer. masking optimization mechanism PART-OF Transformer. Masked Feature Regression ( MFR ) HYPONYM-OF masking optimization mechanism. Transformer USED-FOR visual feature learning in VLP. Visual Entailment CONJUNCTION Visual Reasoning. Visual Reasoning CONJUNCTION Visual Entailment. Visual Question Answering ( VQA ) CONJUNCTION Visual Entailment. Visual Entailment CONJUNCTION Visual Question Answering ( VQA ). Image - Text Retrieval CONJUNCTION Visual Question Answering ( VQA ). Visual Question Answering ( VQA ) CONJUNCTION Image - Text Retrieval. vision - language tasks EVALUATE-FOR method. Visual Reasoning HYPONYM-OF vision - language tasks. Image - Text Retrieval HYPONYM-OF vision - language tasks. Visual Question Answering ( VQA ) HYPONYM-OF vision - language tasks. Visual Entailment HYPONYM-OF vision - language tasks. approach COMPARE V,"This paper proposes to use a fully-transformer architecture for visual feature learning (VLP) in image-text pairs. The authors propose two objectives: visual relation learning and inter-modal alignment. The visual relation is learned by learning the visual relation between images and text pairs, and the inter-Modality Flow (IMF) is a metric to measure the flow of information between the two modalities. The paper also proposes a masking optimization mechanism to improve the performance of the model. The experimental results show that the proposed method outperforms the state-of-the-art in VQA and Visual Entailment tasks.","This paper proposes a new architecture for visual feature learning (VLP) in the context of CNN-Transformer architecture. The main idea is to use a fully Transformer visual embedding to learn inter-modal alignment (i.e., visual relation) between images and text pairs. The authors propose a new metric called Inter-Modality Flow (IMF) to measure the intermodality alignment between images. They also propose Masked Feature Regression (MFR) as a masking optimization mechanism to improve the performance of the model. The experimental results show that the proposed method outperforms the state-of-the-art in VQA and Visual Entailment tasks."
8248,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"information leakage FEATURE-OF iterative randomized learning algorithm. model USED-FOR information leakage. noisy gradient descent algorithms USED-FOR problem. Rényi divergence FEATURE-OF probability distributions. probability distributions FEATURE-OF models. smooth and strongly convex loss functions COMPARE composition theorems. composition theorems COMPARE smooth and strongly convex loss functions. noisy gradient descent algorithms USED-FOR optimal utility. gradient complexity FEATURE-OF optimal utility. Generic is algorithm. OtherScientificTerm are dynamics of Rényi differential privacy loss, privacy loss, and intermediate gradient computations. ","This paper studies the problem of information leakage in an iterative randomized learning setting, where the goal is to learn a distribution over a set of probability distributions with respect to the privacy loss. The authors show that the optimal utility of the proposed algorithm depends on the complexity of the loss function and the number of intermediate gradient computations.    The main contributions of the paper are:  - The authors prove the existence of a new composition theorems that characterize the dynamics of the Rényi differential privacy loss, and show that it is possible to learn the distribution over the distribution of the probability distributions in this setting.  - They show that for any smooth and strongly convex loss functions, the authors prove that their algorithm achieves optimal utility with a gradient complexity of $O(1/\epsilon^2)$. ","This paper studies the problem of information leakage in an iterative randomized learning algorithm. The authors study the dynamics of the Rényi differential privacy loss (RDPL) in the context of a randomized learning problem. They show that the optimal utility of a noisy gradient descent algorithm is bounded by the number of intermediate gradient computations. They also show that for smooth and strongly convex loss functions, they can achieve optimal utility under certain conditions. "
8264,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,large - scale machine learning CONJUNCTION embedded optimal control. embedded optimal control CONJUNCTION large - scale machine learning. First - order methods USED-FOR large - scale machine learning. First - order methods USED-FOR quadratic optimization. First - order methods USED-FOR embedded optimal control. OSQP HYPONYM-OF First - order methods. OSQP HYPONYM-OF quadratic optimization. manual hyperparameter tuning CONJUNCTION convergence time. convergence time CONJUNCTION manual hyperparameter tuning. Reinforcement Learning ( RL ) USED-FOR policy. RL policy COMPARE QP solvers. QP solvers COMPARE RL policy. RLQP COMPARE QP solvers. QP solvers COMPARE RLQP. RLQP HYPONYM-OF RL policy. RLQP USED-FOR problems. RLQP USED-FOR applications. Maros - Mészáros problems HYPONYM-OF applications. QPLIB HYPONYM-OF applications. Generic is methods. Material is QP benchmarks. ,This paper proposes a new method for quadratic optimization in embedded optimal control. The main idea is to use reinforcement learning to learn a policy to solve the QP problem. The proposed method is based on a combination of reinforcement learning (RL) and gradient-based methods. The authors show that the proposed method outperforms the state-of-the-art methods on the QPLIB and Maros-Mészáros problems. ,"This paper proposes a new method for quadratic optimization (QP) solvers for large-scale machine learning with embedded optimal control. The main idea is to use Reinforcement Learning (RL) to learn a policy for QP solvers, which is then used to solve the problem. The authors show that RLQP outperforms the state-of-the-art in terms of convergence time and hyperparameter tuning. They also show that their method can be applied to the Maros-Mészáros problem. "
8280,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"convergence rate EVALUATE-FOR model. PC - bias USED-FOR linear and non8 linear networks. PC - bias USED-FOR early stopping. early stopping USED-FOR PCA. random labels USED-FOR deep networks. Method are convolutional neural networks, and over - parametrized deep linear network model. OtherScientificTerm are asymptotic analysis, hidden layers, principal components, singular 6 values, convergence pattern, Principal Components bias ( PC - bias ), and spectral bias. Task is learning. Generic is biases. ","This paper studies the convergence of deep linear networks with over-parameterized hidden layers. The authors show that the convergence rate of the model converges linearly with respect to the number of hidden layers in the network. They show that this is due to the principal components bias (PC-Bias) and show that it is a special case of the spectral bias (SPB), which is a generalization of the PCA.    The authors also show that PCA can be used as a way to improve the performance of deep neural networks with random labels.  The main contributions of this paper are:  1. A theoretical analysis of the asymptotic convergence of PCA and SPB.  2. An empirical study of the effect of the bias on the performance. ","This paper studies the convergence rate of a deep linear network with over-parametrized principal components (PC-Bias). The authors show that under certain assumptions on the number of hidden layers, the model converges to singular 6 values. They show that this convergence pattern is due to the Principal Components Bias (PCB) bias. They also show that PCB bias can be used as an early stopping mechanism to improve the performance of the model.  "
8296,SP:1598bad835a657e56af3261501c671897b7e9ffd,"Backdoor attack HYPONYM-OF deep neural networks ( DNNs ). defense methods USED-FOR detecting or erasing backdoors. anti - backdoor learning USED-FOR clean models. backdoor - poisoned data USED-FOR clean models. dual - task USED-FOR learning process. models USED-FOR backdoored data. backdoored data USED-FOR model. Anti - Backdoor Learning ( ABL ) USED-FOR backdoor attacks. learning scheme USED-FOR backdoor attacks. Anti - Backdoor Learning ( ABL ) HYPONYM-OF learning scheme. two - stage gradient ascent mechanism USED-FOR ABL. ABL - trained models COMPARE they. they COMPARE ABL - trained models. backdoor - poisoned data USED-FOR ABL - trained models. clean data USED-FOR ABL - trained models. clean data USED-FOR they. OtherScientificTerm are backdoor triggers, backdoor task, and backdoor target class. ","This paper proposes an anti-backdoor learning (ABL) method for detecting and erasing backdoors in deep neural networks. The main idea is to use backdoor-poisoned data to train models that can detect and erase backdoored data. The proposed method is based on a two-stage gradient ascent mechanism: first, the model is trained on a set of backdoor triggers, and then the backdoor task is used to train a clean model on the poisoned data. Second, the clean models are trained on the clean data and the backdoor target class.  ","This paper proposes a new anti-backdoor learning (ABL) method for detecting and erasing backdoors in deep neural networks (DNNs). The authors propose a two-stage gradient ascent mechanism for ABL, where the first stage learns a model that can detect and erases backdoor-poisoned data, and the second stage trains a clean model on the clean data. The authors show that ABL-trained models can outperform clean models in terms of accuracy and robustness to backdoor attacks."
8312,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"generative radiance fields USED-FOR 3Daware image synthesis. multi - view constraint USED-FOR 3D radiance fields. regularization USED-FOR 3D radiance fields. methods USED-FOR 3D radiance fields. multi - view constraint USED-FOR methods. 2D images USED-FOR 3D radiance fields. they USED-FOR 3D shapes. shading - guided generative implicit model USED-FOR shape representation. 3D shape USED-FOR realistic rendering. lighting conditions FEATURE-OF realistic rendering. lighting conditions FEATURE-OF shading. discriminator USED-FOR Gradients. surface tracking USED-FOR volume rendering strategy. approach USED-FOR photorealistic 3D - aware image synthesis. approach USED-FOR 3D shapes. approach COMPARE methods. methods COMPARE approach. approach USED-FOR image relighting. approach USED-FOR 3D shape reconstruction. OtherScientificTerm are shapecolor ambiguity, multi - lighting constraint, illumination, computational burden, and surface normals. Metric is training and inference time. ","This paper proposes a method for 3D image synthesis with 3D radiance fields. The proposed method is based on a shading-guided generative implicit model that learns a shape representation from a 2D image and a 3D shape, which is then used for rendering 3D shapes. The method is evaluated on image relighting and shape reconstruction tasks.  ","This paper proposes a generative implicit model for 3D-aware image synthesis. The main idea is to use a shading-guided implicit model to generate a 3D shape representation, which can be used to represent 3D shapes. The shape representation is learned by training a discriminator to discriminate between 3D radiance fields and surface normals. The discriminator is then used to train a volume rendering strategy, which is based on the surface tracking strategy. The proposed method is evaluated on a variety of image datasets."
8328,SP:4b3dad77d79507c512877867dfea6db87a78682d,"flexible machine learning models USED-FOR instrumental variable ( IV ) regression. quasi - Bayesian procedure USED-FOR IV regression. kernelized IV models USED-FOR quasi - Bayesian procedure. Bayesian modeling USED-FOR IV. Bayesian modeling COMPARE approach. approach COMPARE Bayesian modeling. approach USED-FOR approximate inference algorithm. approximate inference algorithm COMPARE point estimation methods. point estimation methods COMPARE approximate inference algorithm. time cost EVALUATE-FOR point estimation methods. time cost EVALUATE-FOR approximate inference algorithm. algorithm USED-FOR neural network models. Method is uncertainty quantification methodology. OtherScientificTerm are data generating process, and quasi - posterior. Generic is method. ",This paper proposes a quasi-Bayesian procedure for instrumental variable regression with a kernelized IV model. The proposed method is based on Bayesian methods for Bayesian estimation of instrumental variables. The main contribution of the paper is a new uncertainty quantification methodology for IV regression. The method is evaluated on synthetic and real-world datasets. ,"This paper proposes a quasi-Bayesian method for instrumental variable (IV) regression with flexible machine learning models. The method is based on the notion of quasi-posteriority, which is defined as the uncertainty quantification of the data generating process. The authors show that the proposed method can be used to approximate the posterior of the instrumental variable using a kernelized IV model. The proposed method is shown to outperform the state-of-the-art in terms of time and accuracy. "
8344,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,language - specific annotated data CONJUNCTION knowledge sources. knowledge sources CONJUNCTION language - specific annotated data. translation CONJUNCTION in - language retrieval modules. in - language retrieval modules CONJUNCTION translation. multilingual autoregressive generation model CONJUNCTION CORA. CORA CONJUNCTION multilingual autoregressive generation model. annotated data USED-FOR iterative training method. high - resource languages FEATURE-OF annotated data. multilingual open QA benchmarks EVALUATE-FOR CORA. cross - lingual retrieval CONJUNCTION generation. generation CONJUNCTION cross - lingual retrieval. Method is dense passage retrieval algorithm. OtherScientificTerm is low - resource ones. Material is low - resource settings. Generic is model. ," in multilingual open QA. This paper proposes a novel multi-lingual autoregressive QA model called CORA. The proposed model is based on a dense passage retrieval algorithm, which is trained on annotated data from multiple languages. The authors show that the proposed model outperforms the baselines in terms of QA performance.   ","This paper proposes a novel multi-lingual autoregressive generation model (CORA) for multilingual open QA. CORA is trained on high-resource languages (English, French, German, Spanish) and low-resource ones (Chinese, Chinese, Japanese). The authors propose an iterative training method for CORA based on dense passage retrieval (DQR) and show that CORA outperforms the state-of-the-art on a variety of open-QA benchmarks. The authors also show that the model can be used for cross-language retrieval."
8360,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"specialized training algorithms USED-FOR domain generalization. deep neural networks COMPARE specialized training algorithms. specialized training algorithms COMPARE deep neural networks. distribution shift FEATURE-OF deep neural networks. Empirical Risk Minimization ( ERM ) USED-FOR deep neural networks. domain generalization datasets USED-FOR ERM models. Fisher information CONJUNCTION predictive entropy. predictive entropy CONJUNCTION Fisher information. predictive entropy CONJUNCTION maximum mean discrepancy. maximum mean discrepancy CONJUNCTION predictive entropy. measures USED-FOR out - of - distribution generalization. measures CONJUNCTION predictive entropy. predictive entropy CONJUNCTION measures. out - of - distribution generalization EVALUATE-FOR ERM models. Fisher information FEATURE-OF measures. maximum mean discrepancy FEATURE-OF measures. deep networks USED-FOR out - of - distribution. ERM USED-FOR deep networks. Method are domain adaptation theory, and ERMs. Generic is theory. Task are out - of - domain generalization, and generalization. ","This paper studies the problem of out-of-domain generalization, i.e., generalization in the presence of distribution shift in the training data. The authors propose to use Empirical Risk Minimization (ERM) as a proxy for domain generalization. They show that ERM models trained with ERM can generalize well to new domains. They also show that the Fisher information and predictive entropy can be used to evaluate the generalization ability of ERM-trained deep neural networks.","This paper studies the problem of out-of-distribution generalization (OODG) in deep neural networks with Empirical Risk Minimization (ERM) models. The authors propose three measures of OODG generalization: Fisher information, predictive entropy and maximum mean discrepancy. They show that ERM models with Fisher information and predictive entropy are more general than ERMs without Fisher information or predictive entropy, and that ERMs with predictive entropy have better generalization performance than ERM without predictive entropy or Fisher information. They also provide a theoretical analysis of the relationship between the two measures and show that they can be combined to improve generalization."
8376,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,backdoor data poisoning attack HYPONYM-OF adversarial attack. watermarked examples USED-FOR model. backdoor data poisoning attacks USED-FOR classification problems. formal theoretical framework USED-FOR backdoor data poisoning attacks. this USED-FOR statistical and computational issues. statistical and computational issues FEATURE-OF attacks. intrinsic vulnerability FEATURE-OF learning problem. learning problem USED-FOR backdoor attack. memorization capacity HYPONYM-OF parameter. robustness FEATURE-OF natural learning problems. backdoor attacks FEATURE-OF natural learning problems. natural problem settings USED-FOR backdoor attacks. adversarial training USED-FOR backdoors. backdoor filtering CONJUNCTION robust generalization. robust generalization CONJUNCTION backdoor filtering. robust generalization HYPONYM-OF problems. backdoor filtering HYPONYM-OF problems. Generic is assumptions. Method is learning algorithm. ,"This paper studies the problem of backdoor data poisoning attacks in classification problems. The authors propose a theoretical framework to study the intrinsic vulnerability of the learning problem to backdoor attacks. Theoretical analysis is provided for the following reasons:  1. The learning problem can be viewed as a learning problem with intrinsic vulnerability to backdoors.  2. Backdoor attacks are robust to adversarial training and adversarial generalization.  3. In addition to the theoretical analysis, the authors show that backdoor attacks can be used to improve the robustness of natural learning problems.   ",This paper proposes a theoretical analysis of backdoor data poisoning attacks for natural learning problems. The main idea is to study the intrinsic vulnerability of the learning problem in the setting of backdoor attacks. The authors show that the memorization capacity of the model is a critical factor in the robustness of the backdoor attack. They also provide a theoretical framework to analyze the statistical and computational complexity of the attacks. 
8392,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"networks COMPARE ones. ones COMPARE networks. Large width limits PART-OF deep learning research. representational power FEATURE-OF networks. capacity CONJUNCTION width. width CONJUNCTION capacity. neural networks USED-FOR Deep Gaussian Processes ( Deep GP ). Deep Gaussian Processes ( Deep GP ) HYPONYM-OF nonparametric hierarchical models. neural nets HYPONYM-OF nonparametric hierarchical models. they USED-FOR modeling task. width USED-FOR neural networks. nonparametric Deep GP USED-FOR Gaussian processes. mixture of data - adaptable basis functions FEATURE-OF posterior. width CONJUNCTION depth. depth CONJUNCTION width. depth USED-FOR model. non - Gaussianity FEATURE-OF model. hidden units USED-FOR neural networks. L2 regularization USED-FOR neural networks. OtherScientificTerm are computational practicalities, GP behavior, adaptability, and Gaussian prior on parameters. Method are Deep GP, and hierarchical models. ","This paper proposes to use deep Gaussian processes (Deep GP) as the basis for deep neural networks (DNNs), which are non-parametric hierarchical models. The authors show that DNNs can be viewed as a mixture of data-adaptable basis functions with a Gaussian prior on parameters. They show that the width and depth of a DNN can be controlled by the depth of the posterior, and that the depth is independent of the number of hidden units. They also show that L2 regularization can be used to improve the performance of Deep GP. ","This paper proposes a new nonparametric hierarchical model, Deep Gaussian Processes (Deep GP), which is based on a mixture of data-adaptable basis functions (i.e., the posterior of the posterior is composed of a Gaussian prior on parameters) and a neural network. The authors show that the depth and width of the model can be controlled by L2 regularization. They also show that Deep GP can be used as a generalization of Deep Neural Networks (DNNs). "
8408,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"algorithmic framework USED-FOR challenges. systems heterogeneity CONJUNCTION infrequent and imprecise communication. infrequent and imprecise communication CONJUNCTION systems heterogeneity. objective heterogeneity CONJUNCTION systems heterogeneity. systems heterogeneity CONJUNCTION objective heterogeneity. challenges PART-OF FL. FedLin HYPONYM-OF algorithmic framework. infrequent and imprecise communication HYPONYM-OF challenges. objective heterogeneity HYPONYM-OF challenges. systems heterogeneity HYPONYM-OF challenges. speed - accuracy conflict FEATURE-OF FL algorithms. FedLin USED-FOR linear convergence. matching upper and lower bounds FEATURE-OF convergence rate. convergence rate FEATURE-OF FedLin. matching upper and lower bounds FEATURE-OF FedLin. compression level FEATURE-OF convergence rate. gradient sparsification USED-FOR FedLin. linear convergence rates FEATURE-OF FedLin. gradient sparsification USED-FOR FL. Task is federated learning ( FL ) setup. Method is statistical model. Generic are framework, and they. OtherScientificTerm are global minimum, sub - linear rate, fast convergence, clients ’ local loss functions, objective and systems heterogeneity, infrequent, periodic communication, and tight linear convergence rate guarantees. Metric is accuracy. ","This paper studies the problem of federated learning in the setting where there are multiple clients with different local loss functions, and the goal is to learn a model that is robust to both objective and systems heterogeneity. The authors propose a new algorithm called FedLin to solve this problem. They show that FedLin achieves linear convergence to the global minimum with a sub-linear convergence rate, which is faster than existing algorithms. They also show that gradient sparsification can be used to improve the convergence rate of FedLin.","This paper proposes FedLin, a framework for solving the speed-accuracy conflict problem in federated learning (FL). The main contribution of the paper is to propose a new algorithm, FedLin. FedLin is based on gradient sparsification, and the authors show that FedLin converges at a sub-linear rate, which is faster than the global minimum. The authors also provide upper and lower bounds on the convergence rate of FedLin at the compression level. "
8424,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,Sliced - Wasserstein distance ( SW ) USED-FOR machine learning applications. Sliced - Wasserstein distance ( SW ) COMPARE Wasserstein distance. Wasserstein distance COMPARE Sliced - Wasserstein distance ( SW ). Monte Carlo USED-FOR SW. perspective USED-FOR SW. one - dimensional projections FEATURE-OF highdimensional random vector. concentration of measure phenomenon USED-FOR perspective. concentration of measure phenomenon USED-FOR SW. deterministic approximation USED-FOR SW. method COMPARE Monte Carlo approximation. Monte Carlo approximation COMPARE method. weak dependence condition FEATURE-OF data distribution. nonasymptotical guarantees USED-FOR approach. generative modeling problem EVALUATE-FOR approximation. Generic is it. OtherScientificTerm is random projections. Metric is approximation error. Material is synthetic datasets. ,This paper studies the Sliced-Wasserstein distance (SW) from the perspective of one-dimensional projections of a high-dimensional random vector. The main contribution of this paper is to show that the Wasserstein-Sliced distance (WSW) can be approximated by a deterministic approximation of the random vector via a concentration of measure phenomenon. The authors show that this approximation is non-asymptotical and can be used for generative modeling problems with weak dependence conditions.,"This paper proposes a new Sliced-Wasserstein distance (SW) method for estimating the Wasserstein distances between two random projections of a highdimensional random vector. The main contribution of the paper is a new perspective on the S sliced-Wassstein distance, which is based on the concentration of measure phenomenon. The authors provide a deterministic approximation of the SW with a weak dependence condition on the data distribution. They also provide a nonasymptotical guarantee for the approximation error of the method."
8440,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,neural language models CONJUNCTION translation models. translation models CONJUNCTION neural language models. translation models CONJUNCTION language tagging tasks. language tagging tasks CONJUNCTION translation models. language tagging tasks USED-FOR representations. neural language models USED-FOR representations. translation models USED-FOR representations. networks USED-FOR language tasks. hidden representations USED-FOR networks. computer vision USED-FOR encoder - decoder transfer learning method. hidden representations USED-FOR feature spaces. language models CONJUNCTION translation models. translation models CONJUNCTION language models. word embeddings CONJUNCTION syntactic and semantic tasks. syntactic and semantic tasks CONJUNCTION word embeddings. syntactic and semantic tasks CONJUNCTION word embeddings. word embeddings CONJUNCTION syntactic and semantic tasks. method USED-FOR low - dimensional structure. it USED-FOR NLP ( natural language processing ) tasks. language representation embedding USED-FOR low - dimensional structure. feature space USED-FOR human brain responses. representation embedding USED-FOR feature space. natural language stimuli USED-FOR human brain responses. fMRI USED-FOR natural language stimuli. fMRI USED-FOR human brain responses. metric USED-FOR brain ’s natural language processing hierarchy. principal dimension USED-FOR metric. principal dimension FEATURE-OF structure. structure USED-FOR metric. embedding USED-FOR brain ’s natural language representation structure. ,"This paper proposes a method to learn a low-dimensional representation of the input word embeddings in a language model. The method is based on a computer vision-based encoder-decoder transfer learning method, where the encoder and decoder networks are trained using computer vision. The proposed method is evaluated on a variety of NLP tasks, including word embedding, syntactic and semantic tasks, and language tagging tasks. The results show that the proposed method can learn low-dimensions of the feature space, which are then used to predict the human brain responses to natural language stimuli.  ","This paper presents a method for learning a low-dimensional representation of the human brain’s natural language representations. The proposed method is based on an encoder-decoder transfer learning method, where the encoder and decoder are encoders and decoders, respectively, of the same language, and the decoder is used to learn a low dimensional representation. The method is evaluated on a variety of tasks, including word embeddings, syntactic and semantic tasks, and language tagging tasks. The results show that the proposed method can be used to improve the performance of language models and translation models."
8456,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"paradigm USED-FOR unconditional variational autoencoders ( VAEs ). unconditional variational autoencoders ( VAEs ) USED-FOR few - shot conditional image generation. Diffusion - Decoding models CONJUNCTION Contrastive representations ( D2C ). Contrastive representations ( D2C ) CONJUNCTION Diffusion - Decoding models. diffusion - based prior USED-FOR generation. diffusion - based prior USED-FOR latent representations. D2C USED-FOR generation. contrastive selfsupervised learning USED-FOR representation quality. contrastive selfsupervised learning USED-FOR D2C. diffusion - based prior USED-FOR D2C. D2C USED-FOR generation tasks. D2C COMPARE diffusion models. diffusion models COMPARE D2C. D2C USED-FOR conditional generation. D2C generations COMPARE StyleGAN2 ones. StyleGAN2 ones COMPARE D2C generations. double - blind study EVALUATE-FOR human evaluators. D2C generations USED-FOR conditional image manipulation. double - blind study EVALUATE-FOR D2C generations. Method are Conditional generative models of high - dimensional images, and d2c. OtherScientificTerm are supervision signals, and manipulation constraints. ","This paper proposes a method for few-shot conditional image generation using contrastive self-supervised learning and diffusion-decoding models. The main idea is to use contrastive learning to improve the quality of the latent representation, and then use a diffusion-based prior to learn the latent representations. Experiments show that the proposed method outperforms the state-of-the-art StyleGAN2 models on conditional image manipulation tasks.","This paper proposes a method for conditional few-shot conditional image generation using contrastive representations (D2C). D2C is an extension of contrastive self-supervised learning (DSC) that uses contrastive learning to improve the quality of the latent representations. The authors propose a diffusion-based prior for the generation of latent representations, which is based on a contrastive prior. They show that the proposed method outperforms the state-of-the-art StyleGAN2 on a number of image manipulation tasks."
8472,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"contrastive learning paradigm USED-FOR representations. Edges PART-OF graph. ground - truth classes PART-OF connected sub - graphs. contrastive learning objective USED-FOR neural net representations. population augmentation graph USED-FOR spectral decomposition. spectral decomposition USED-FOR loss. contrastive learning objective USED-FOR loss. objective USED-FOR features. linear probe evaluation FEATURE-OF features. generalization bounds USED-FOR accuracy guarantees. objective COMPARE baselines. baselines COMPARE objective. features COMPARE baselines. baselines COMPARE features. objective USED-FOR features. benchmark vision datasets EVALUATE-FOR objective. benchmark vision datasets EVALUATE-FOR baselines. Task is self - supervised learning. OtherScientificTerm are conditional independence of the positive pairs, correlated positive pairs, conditional independence of positive pairs, and augmentation graph. Method is contrastive learning. Metric is training contrastive loss. ",This paper proposes a new contrastive loss for self-supervised learning. The proposed loss is based on a population augmentation graph. The authors show that the proposed loss can improve the performance of the learned representations in terms of generalization error. The main contribution of the paper is that the authors propose a new loss function that is independent of the positive pairs and correlated with the ground truth classes. ,This paper proposes a new contrastive learning objective for self-supervised learning. The main idea is to use a population augmentation graph to learn representations for the ground truth classes of the neural net representations. The proposed objective is based on a spectral decomposition of the positive pairs of positive pairs. The authors show that the proposed objective can be used to improve the generalization of the learned representations.   
8488,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"parameterized complexity EVALUATE-FOR Bayesian Network Structure Learning ( BNSL ). complexity EVALUATE-FOR BNSL. parameterization USED-FOR fixed - parameter tractability. feedback edge set HYPONYM-OF parameterization. lower bounds USED-FOR complexity classification of BNSL. complexity classification EVALUATE-FOR BNSL. complexity EVALUATE-FOR BNSL. additive representation USED-FOR BNSL. OtherScientificTerm are superstructure, graph parameters, and treewidth. Task are fixed - parameter tractable, and Polytree Learning. Method is non - zero representation. ","This paper studies the Bayesian network structure learning (BNSL) problem, where the goal is to learn a Bayesian neural network with a fixed-parameterized complexity of $O(\sqrt{T})$ where $T$ is the number of nodes in the network. The authors show that BNSL is tractable with $O(T)$ complexity under certain assumptions, including the existence of an additive representation of the network, and that the treewidth of the graph is bounded by a constant $T$. The authors also provide a lower bound on the complexity of Bayesian Network Structure Learning with Polytree Learning.   ","This paper studies the complexity of Bayesian Network Structure Learning (BNSL) in the context of fixed-parameter tractability. The authors provide lower bounds on the complexity classification of BNSL. They show that the complexity is bounded by the number of parameters and the treewidth of the feedback edge set. They also show that for a given parameterization, the complexity can be bounded by a non-zero representation of the input graph."
8504,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,active learning algorithm USED-FOR binary classification tasks. active learning algorithm USED-FOR streaming setting. streaming setting USED-FOR binary classification tasks. model USED-FOR surrogate loss. algorithm USED-FOR model. labeled and weak - labeled points USED-FOR surrogate loss. weak labels USED-FOR algorithm. theoretical guarantees FEATURE-OF general agnostic setting. Uncertainty Sampling HYPONYM-OF active learning algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. Margin Algorithm CONJUNCTION Uncertainty Sampling. Uncertainty Sampling CONJUNCTION Margin Algorithm. generalization and label complexity bounds EVALUATE-FOR algorithm. Margin Algorithm HYPONYM-OF baselines. Uncertainty Sampling HYPONYM-OF baselines. Material is real - world datasets. ,"This paper studies the problem of active learning for binary classification in the streaming setting. In this setting, a model is trained on a set of labeled and weak-labeled points, and a surrogate loss is used to estimate the loss function. The main contribution of this paper is to provide a generalization and label complexity bound for the proposed method, which is based on the Margin Algorithm. ",This paper proposes a new active learning algorithm for binary classification in the streaming setting. The main idea is to use a surrogate loss between labeled and weak-labeled points in the training process. The surrogate loss is defined as the difference between the number of labeled points and weak labels. The paper provides theoretical guarantees for general agnostic generalization and label complexity bounds for the surrogate loss. 
8520,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"classifiers USED-FOR invariant feature representations. classifier ’s function space USED-FOR generalization. complexity USED-FOR generalization. complexity FEATURE-OF classifier ’s function space. KC FEATURE-OF functions. measure USED-FOR generalization error bounds. complexity EVALUATE-FOR measure. complexity USED-FOR generalization error bounds. Kolmogorov Growth ( KG ) HYPONYM-OF measure. Occam ’s razor USED-FOR neural networks. generalization ability EVALUATE-FOR classifiers. approach USED-FOR classifiers. generalization ability EVALUATE-FOR approach. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. KG CONJUNCTION test accuracies. test accuracies CONJUNCTION KG. Kolmogorov Growth HYPONYM-OF function complexity prior. Method are classifier, complexity theory, network - to - network regularization, N2N regularization, and cross - entropy baselines. Metric is Kolmogorov complexity ( KC ). OtherScientificTerm are classification function, network trajectory, low KG zone, and training data sizes. Generic is bounds. Task is learning. ",This paper studies the generalization ability of classifiers in terms of the Kolmogorov complexity of the classifier’s function space. The authors show that the complexity of a classifier's function space is a function of the size of the training set and the number of training samples. They show that this complexity can be used to derive generalization error bounds for a class of functions that are invariant to changes in the training data. They also provide a generalization bound for the classifiers trained with N2N regularization.,"This paper studies the generalization ability of classifiers in terms of the Kolmogorov complexity (KG) of the classifier’s function space, which is a measure of the complexity of the function space of a classifier. The authors propose a new measure of KG, which they call Kolmov Complexity (KCC) and show that it can be used to derive generalization error bounds for classifiers. They also show that the KCC can also be used as a generalization bound for classifier-to-classifier regularization.  The authors also provide a theoretical analysis of the KGC bounds."
8536,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"self - supervised methods USED-FOR image representation learning. self - supervised methods USED-FOR embedding vectors. encoders USED-FOR constant or non - informative vectors. regularizations terms USED-FOR embeddings. term CONJUNCTION term. term CONJUNCTION term. variance FEATURE-OF term. threshold FEATURE-OF variance. term HYPONYM-OF regularizations terms. term HYPONYM-OF regularizations terms. batch normalization CONJUNCTION feature - wise normalization. feature - wise normalization CONJUNCTION batch normalization. feature - wise normalization CONJUNCTION output quantization. output quantization CONJUNCTION feature - wise normalization. output quantization CONJUNCTION stop gradient. stop gradient CONJUNCTION output quantization. stop gradient CONJUNCTION memory banks. memory banks CONJUNCTION stop gradient. weight sharing CONJUNCTION batch normalization. batch normalization CONJUNCTION weight sharing. approaches USED-FOR problem. approaches COMPARE VICReg. VICReg COMPARE approaches. output quantization CONJUNCTION memory banks. memory banks CONJUNCTION output quantization. techniques USED-FOR VICReg. downstream tasks EVALUATE-FOR VICReg. stop gradient HYPONYM-OF techniques. weight sharing HYPONYM-OF techniques. memory banks HYPONYM-OF techniques. feature - wise normalization HYPONYM-OF techniques. output quantization HYPONYM-OF techniques. batch normalization HYPONYM-OF techniques. variance regularization term USED-FOR methods. Generic is method. OtherScientificTerm are collapse problem, and branches. ","This paper proposes a self-supervised method for image representation learning. The proposed method is based on two regularization terms: a batch normalization term and a variance regularization term. The variance term is designed to encourage the embeddings to be non-correlated to the constant or non-informative vectors. The authors show that the variance term can be used to prevent the collapse of the embedding, which is a common problem in self-Supervised learning.   The authors also propose to use the variance regularized embedding as a regularizer in the training process. ","This paper proposes a new regularization term for self-supervised image embeddings, called Variance-invariant Variance Normalization (VICReg), which aims to reduce the variance of the embedding. The variance term is based on the fact that the variance is a function of the number of embedding layers, and that it depends on the size of the training set. The authors show that VICReg can be used to improve the performance of a number of existing methods, including batch normalization, weight sharing, feature-wise normalization and output quantization. They also provide a theoretical analysis of the variance term and show that it can be applied to a variety of downstream tasks."
8552,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"model USED-FOR RL algorithms. model USED-FOR reward. expected returns EVALUATE-FOR RL algorithms. Bayesian model USED-FOR reward. Bayesian model USED-FOR Information Directed Reward Learning ( IDRL ). prior active reward learning methods COMPARE IDRL. IDRL COMPARE prior active reward learning methods. reward model USED-FOR policy. Task are reinforcement learning ( RL ) applications, and RL setting. OtherScientificTerm are binary preferences, expert queries, and reward approximation error. Metric is information gain. Generic is it. ","This paper studies the problem of information-based reinforcement learning, where the goal is to learn a policy that maximizes the expected return on a set of tasks. In this setting, the reward function is modeled using a Bayesian model of the reward distribution. The authors show that the reward approximation error is a function of the number of expert queries, and show that IDRL can learn a reward function that is close to the true reward function in terms of the expected returns on the tasks.    The main contribution of this paper is a novel method for learning the reward for a given task. The method is based on the idea of Bayesian Bayes, which is an extension of Bayes Bayes. The main idea is to use a Bayes model to model the reward of a task, and then use it as a prior for the policy learning. ","This paper proposes Information Directed Reward Learning (IDRL), a Bayesian model for information-driven reinforcement learning (IRL). IDRL aims to improve the performance of existing active reward learning methods in the context of binary preferences and expert queries. The main contribution of IDRL is the use of a Bayes-based reward model to estimate the expected return of an RL algorithm. The authors show that IDRL outperforms the state-of-the-art in terms of expected return on a variety of tasks. "
8568,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,Deep learning USED-FOR features. Deep learning USED-FOR machine learning pipelines. features PART-OF machine learning pipelines. algorithms USED-FOR neural network parameters. deep learning USED-FOR parameters. it USED-FOR parameter prediction. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. large - scale dataset USED-FOR parameter prediction. diverse computational graphs of neural architectures FEATURE-OF large - scale dataset. DEEPNETS-1 M HYPONYM-OF large - scale dataset. ImageNet USED-FOR parameter prediction. CIFAR-10 USED-FOR parameter prediction. DEEPNETS-1 M HYPONYM-OF diverse computational graphs of neural architectures. graph neural networks USED-FOR hypernetwork. accuracy EVALUATE-FOR it. CIFAR-10 EVALUATE-FOR it. ImageNet EVALUATE-FOR networks. top-5 accuracy EVALUATE-FOR networks. task CONJUNCTION model. model CONJUNCTION task. model USED-FOR neural architectures. OtherScientificTerm is CPU. Method is ResNet-50. Task is training networks. ,"This paper proposes a method for parameter prediction from large-scale neural networks. The method is based on graph neural networks (GNNs), which are trained to predict the parameters of a neural network. The proposed method is evaluated on CIFAR-10 and ImageNet, where it achieves state-of-the-art performance.  ",This paper proposes a method for predicting the parameters of a neural network from a large-scale dataset. The method is based on a graph neural network (GNN) trained on a dataset of diverse computational graphs of neural architectures. The authors show that their method can achieve top-5 accuracy on CIFAR-10 and ImageNet datasets. 
8584,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"distortion EVALUATE-FOR estimator. perception constraint FEATURE-OF minimal distortion. closed form expression USED-FOR distortion - perception ( DP ) function. mean squared - error ( MSE ) distortion CONJUNCTION Wasserstein-2 perception index. Wasserstein-2 perception index CONJUNCTION mean squared - error ( MSE ) distortion. closed form expression USED-FOR Wasserstein-2 perception index. mean squared - error ( MSE ) distortion EVALUATE-FOR distortion - perception ( DP ) function. closed form expression USED-FOR estimators. closed form expression USED-FOR Gaussian setting. global MSE minimizer CONJUNCTION minimizer. minimizer CONJUNCTION global MSE minimizer. global MSE minimizer CONJUNCTION MSE. MSE CONJUNCTION global MSE minimizer. minimizer FEATURE-OF MSE. perfect perceptual quality constraint FEATURE-OF minimizer. perfect perceptual quality constraint FEATURE-OF MSE. minimizer HYPONYM-OF tradeoff. global MSE minimizer HYPONYM-OF tradeoff. estimators USED-FOR estimators. stochastic transformation of the former USED-FOR latter. Metric are perception - distortion tradeoff, fidelity, and perceptual quality. Task is image restoration. OtherScientificTerm are statistics of natural images, perception - distortion plane, DP function, DP curve, and geodesic in Wasserstein space. ","This paper studies the connection between mean squared error (MSE) and Wasserstein-2 perception index (WPI) in the perception-distortion plane of natural images. The authors show that the MSE-DP function is a closed form expression of the WPI in the DP curve, and that it can be expressed in closed form in the Gaussian setting. They show that MSE and WPI can be approximated by a stochastic transformation of the former and a global MSE minimization of the latter. They also show that for Gaussian images, MSE/WPI estimators of the DP function can be computed in the closed form.  ","This paper studies the trade-off between mean squared error (MSE) distortion and Wasserstein-2 perception index (WPI) in the perception-distortion plane. The authors propose a closed-form expression for estimating the WPI, which is based on the closed form expression for the distortion-perception (DP) function. They show that the MSE-DP function can be approximated by a Gaussian estimator. They also show that a global MSE minimizer and a minimizer-based minimizer can be used to estimate the W PI. "
8600,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,textual features CONJUNCTION neighbourhood information. neighbourhood information CONJUNCTION textual features. low - dimensional embeddings USED-FOR nodes. textual features USED-FOR low - dimensional embeddings. neighbourhood information USED-FOR low - dimensional embeddings. pretrained language models CONJUNCTION graph neural networks. graph neural networks CONJUNCTION pretrained language models. graph neural networks USED-FOR techniques. textual features FEATURE-OF nodes. language models USED-FOR textual features. graph neural networks USED-FOR textual embeddings. layerwise GNN components CONJUNCTION transformer blocks of language models. transformer blocks of language models CONJUNCTION layerwise GNN components. layerwise GNN components PART-OF GraphFormers. text encoding CONJUNCTION graph aggregation. graph aggregation CONJUNCTION text encoding. graph aggregation PART-OF iterative workflow. text encoding PART-OF iterative workflow. manipulated data CONJUNCTION original data. original data CONJUNCTION manipulated data. model PART-OF progressive learning strategy. manipulated data USED-FOR model. original data USED-FOR model. GraphFormers COMPARE SOTA baselines. SOTA baselines COMPARE GraphFormers. large - scale benchmark datasets EVALUATE-FOR GraphFormers. running efficiency EVALUATE-FOR SOTA baselines. running efficiency EVALUATE-FOR GraphFormers. OtherScientificTerm is textual graph. Method is cascaded model architecture. Generic is architecture. Task is independent modeling of textual features. ,"This paper proposes a novel model architecture for learning low-dimensional embeddings of a textual graph. The proposed model is based on a combination of GNNs and transformer blocks of language models. The model is trained using a progressive learning strategy, where the model learns from manipulated data and the original data. The experimental results show that the proposed model achieves state-of-the-art performance on a variety of benchmark datasets.","This paper proposes a new model for the task of learning low-dimensional embeddings of textual features from text. The proposed model is based on a combination of GNNs, transformer blocks of language models, and layerwise GNN components. The model is trained using a progressive learning strategy, where the original data is used to train the model, and the manipulated data is fed to the model to improve the performance of the model. Experiments show that the proposed model outperforms SOTA baselines on several benchmark datasets. "
8616,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,algorithms USED-FOR learning tasks. userlevel differential privacy constraints FEATURE-OF learning tasks. empirical risk minimization CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION empirical risk minimization. stochastic convex optimization CONJUNCTION learning hypothesis classes. learning hypothesis classes CONJUNCTION stochastic convex optimization. high - dimensional mean estimation CONJUNCTION empirical risk minimization. empirical risk minimization CONJUNCTION high - dimensional mean estimation. smooth losses FEATURE-OF empirical risk minimization. finite metric entropy FEATURE-OF learning hypothesis classes. O(1 / n ) rate FEATURE-OF privacy cost. mean estimation CONJUNCTION stochastic convex optimization. stochastic convex optimization CONJUNCTION mean estimation. algorithms USED-FOR mean estimation. algorithms USED-FOR stochastic convex optimization. minimax optimality FEATURE-OF algorithms. lower bounds USED-FOR minimax optimality. lower bounds USED-FOR algorithms. techniques USED-FOR private mean estimation. techniques USED-FOR arbitrary dimension. error scaling FEATURE-OF techniques. private mean estimation USED-FOR algorithms. arbitrary dimension FEATURE-OF private mean estimation. techniques USED-FOR algorithms. Method is user - level DP. OtherScientificTerm is information leaks. ,"This paper studies the problem of private mean estimation under differential privacy constraints in stochastic convex optimization. The main contributions are:  1. The authors provide a new algorithm for mean estimation in the setting of user-level DP.  2. They provide a lower bound on the privacy cost of the algorithm, which is O(1/n) in terms of the DP cost.  3. They show that the algorithm is minimax optimally efficient.    4. They also show that their algorithm is efficient in the sense that the error scales linearly with the dimension.","This paper studies the problem of user-level differential privacy in the context of stochastic convex optimization and mean estimation. In particular, the authors consider the case where the data is collected at user level, and the privacy cost is O(1/n), where O(n) is the number of users and n is the dimension of the dataset. The authors provide a lower bound on the privacy of the algorithms for mean estimation under the assumption of a finite metric entropy. They also provide lower bounds on the lower bounds for stochastically convex optimality of the mean estimation algorithms. "
8632,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"they USED-FOR deep learning. infinite width / channel limit FEATURE-OF Deep neural networks ( DNNs ). deep learning USED-FOR finite DNNs. self - consistent Gaussian Process theory USED-FOR finite - DNN and feature learning effects. noisy gradient descent USED-FOR DNNs. this USED-FOR toy model. feature learning regime CONJUNCTION lazy learning regime. lazy learning regime CONJUNCTION feature learning regime. CIFAR-10 USED-FOR Myrtle5 CNN. self - consistent theory USED-FOR finite - DNN effects. self - consistent theory USED-FOR feature learning. feature learning HYPONYM-OF finite - DNN effects. Method is Gaussian Processes ( GPs ). Generic are model, and theory. ",This paper studies the effect of feature learning on finite-DNNs in the infinite-width/channels limit of deep neural networks. The authors propose a self-consistent Gaussian Process (GP) theory to study the finite-NN and feature learning effects. The main contribution of the paper is to show that the feature learning effect is independent of the number of neurons in the network and the width of the network. ,This paper studies the effect of finite-DNN and feature learning effects in deep neural networks (DNNs) under the infinite width/channels limit of Gaussian Processes (GP). The main contribution of the paper is a self-consistent Gaussian process theory for finite DNNs. The main result is that the feature learning effect of a DNN can be seen as a function of the width of the network and the number of parameters of the model. The authors show that this effect is independent of the size of the DNN. They also show that the effect can be independent of whether the model is noisy or not. 
8648,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"inductive biases USED-FOR compositional communication. training framework USED-FOR inductive biases. signaling games USED-FOR compositionality. noise levels USED-FOR compositionality. conflict count CONJUNCTION context independence. context independence CONJUNCTION conflict count. topographical similarity CONJUNCTION conflict count. conflict count CONJUNCTION topographical similarity. context independence HYPONYM-OF compositionality metrics. topographical similarity HYPONYM-OF compositionality metrics. conflict count HYPONYM-OF compositionality metrics. Task is Communication. OtherScientificTerm are complex signals, and noisy channel. Generic is model. ","This paper studies the problem of compositional communication in the context of signaling games, where the goal is to learn a communication model that is compositional to a noisy channel. The authors propose two metrics to measure compositionality: topographical similarity and conflict count, which measure the distance between the source and target channels. They show that these metrics can be used to evaluate the compositional performance of the model. They also show that the proposed metrics are robust to noise levels.","This paper studies the problem of communication in the context of signaling games, where the goal is to communicate with a noisy channel. The authors propose two metrics to measure the compositionality of the communication: conflict count and topographical similarity. They show that these metrics can be used to quantify the compositional bias of a communication model. They also provide a theoretical analysis of the relationship between the two metrics.   "
8664,SP:9d326254d77a188baf5bde39229c09b3966b5418,"ResMLP HYPONYM-OF architecture. multi - layer perceptrons USED-FOR image classification. architecture USED-FOR image classification. multi - layer perceptrons USED-FOR ResMLP. multi - layer perceptrons USED-FOR architecture. It HYPONYM-OF residual network. heavy data - augmentation CONJUNCTION distillation. distillation CONJUNCTION heavy data - augmentation. training strategy USED-FOR it. distillation USED-FOR training strategy. heavy data - augmentation USED-FOR training strategy. ImageNet EVALUATE-FOR it. self - supervised setup USED-FOR ResMLP models. labelled dataset USED-FOR priors. model USED-FOR machine translation. Timm library CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Timm library. OtherScientificTerm are image patches, and channels. Method is two - layer feed - forward network. "," image classification. The paper proposes a two-layer feed-forward network, ResMLP, for image classification, which is based on ResNet and ResNet2. The main idea is that ResNet is a residual network, which can be trained with heavy data-augmentation and distillation. The proposed method is evaluated on ImageNet with self-supervised training and machine translation.","This paper proposes a two-layer ResMLP architecture for image classification. The main idea is to use a multi-layer perceptron network, where each layer is composed of a set of channels, and each channel is trained by distillation. The training strategy is based on heavy data-augmentation and distillation, which is a combination of heavy data augmentation and data-distillation. Experiments on ImageNet show that the proposed method outperforms the state-of-the-art in terms of image classification accuracy."
8680,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"regret guarantees USED-FOR geometric problem of contextual search. multiclass classification CONJUNCTION binary classification. binary classification CONJUNCTION multiclass classification. Task is multi - class classification. Metric is misclassification rate. Method are nearest neighbor partition, and reduction technique. OtherScientificTerm is Euclidean distance. ","This paper studies the problem of contextual search in multi-class classification, where the goal is to minimize the misclassification rate. The main contribution is a reduction to the problem in terms of the Euclidean distance of the nearest neighbor partition. Theoretical results are given for the case where the nearest neighbors are in the same neighborhood as the training set.   ",This paper studies the problem of contextual search for multi-class classification in the context of Euclidean distance. The main contribution of the paper is a reduction of the distance between the nearest neighbors of a set of classes to the nearest neighbor of the same class. The authors show that this distance can be reduced to a lower bound on the number of misclassifications. They also show that the distance is bounded by the Euclideans of the neighbors.
8696,SP:5c0114535065d5125349f00bafdbccc911461ede,"Methods USED-FOR Visual Question Anwering ( VQA ). dataset biases COMPARE reasoning. reasoning COMPARE dataset biases. attention layers PART-OF VQA model. attention layers FEATURE-OF reasoning patterns. perfect ( oracle ) visual inputs USED-FOR they. method USED-FOR knowledge transfer. regularization term PART-OF loss function. regularization term USED-FOR method. sample complexity EVALUATE-FOR program prediction. PAC - learning USED-FOR theoretical analysis. GQA dataset EVALUATE-FOR approach. Task is generalization. Method is deep neural networks. Generic are models, and transfer. OtherScientificTerm is reasoning operations. ","This paper proposes a method for visual question answering (VQA) based on PAC-learning. The proposed method is based on the observation that prior work on VQA has shown that reasoning is more important than reasoning in generalization. To address this issue, the authors propose to add an additional regularization term to the loss function of the model to improve the generalization performance. Theoretical analysis is provided to show that the proposed method can improve the sample complexity of program prediction. Experiments on the GQA dataset demonstrate the effectiveness of the proposed methods.","This paper proposes a new method for Visual Question Anwering (VQA) that uses PAC-learning to improve the generalization of VQA models. The main idea is to use an oracle to predict the perfect (oracle) visual inputs for each question, and then use an attention layer to learn the reasoning patterns of the oracle. The authors show that the proposed method can improve the transferability of knowledge across different datasets. They also provide theoretical analysis on the sample complexity of the method. "
8712,SP:40fd96105e77063de4a07d4b36fe19385434c533,"neurons of fixed precision FEATURE-OF dynamically growing memory module. 54 - neuron bounded - precision RNN USED-FOR Universal Turing Machine. growing memory modules PART-OF 54 - neuron bounded - precision RNN. Turing completeness FEATURE-OF unbounded - precision and boundedprecision RNNs. Method are recurrent neural networks ( RNNs ), RNNs, memory module, and stack - augmented RNNs. OtherScientificTerm are unbounded precision, simulated machine ’s time, and memory size. Metric is time complexity. ",This paper studies RNNs with unbounded precision and bounded precision. The authors show that the memory of a bounded-precision RNN with bounded precision is bounded by the number of neurons in the memory module. They show that this is equivalent to having a fixed number of memory modules in the RNN. They also show that a fixed memory module can be added to a bounded precision RNN in a time-efficient manner.  ,"This paper studies the problem of unbounded precision and bounded precision RNNs. The authors show that unbounded-precision RNN with fixed precision can be approximated by a 54-neurons bounded precision network with fixed memory size. They also show that bounded precision and unbounded RNN can be combined to form a universal Turing machine with bounded precision. They show that the bounded precision of the bounded RNN is bounded by the number of neurons of fixed precision, and the unbounded accuracy of the fixed RNN cannot be bounded by any fixed precision. Finally, they provide a theoretical analysis of the time complexity of bounded precision bounded precision in the context of the Universal Turing Machine."
8728,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"Estimating the data uncertainty USED-FOR regression tasks. quantile function USED-FOR Estimating the data uncertainty. vanilla algorithm USED-FOR quantiles. uncertainty estimation algorithms USED-FOR quantiles. vanilla setting USED-FOR realizable linear quantile function. under - coverage bias FEATURE-OF quantile regression. α CONJUNCTION d / n. d / n CONJUNCTION α. quantile regression USED-FOR α - quantile. highdimensional parameter estimation error USED-FOR under - coverage bias. sample size CONJUNCTION model capacity. model capacity CONJUNCTION sample size. simulated and real data EVALUATE-FOR theory. model capacity FEATURE-OF under - coverage bias. OtherScientificTerm are asymptotic guarantees, coverage level, and noise distribution. ",This paper studies the under-covering bias of quantile regression with linear quantile function. The authors show that under-completeness bias is a result of the high-dimensional parameter estimation error in the quantile estimation problem. They show asymptotic guarantees and show that the coverage level depends on the noise distribution and the sample size.   ,"This paper studies the under-covering bias of quantile regression in the setting of estimating the data uncertainty of a linear quantile function. The main contribution of the paper is a theoretical analysis of the asymptotic guarantees of the quantile estimation problem. The authors show that under-covered quantiles have a high-dimensional parameter estimation error, which can be explained by the high sample size and model capacity of the model. They also show that the high-sample-size bias is due to the noise distribution of the data.  "
8744,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"strict memory budget USED-FOR classifiers. learning USED-FOR incremental phase. static and ad hoc strategy USED-FOR memory allocation. dynamic memory management strategy USED-FOR incremental phases. incremental phases CONJUNCTION object classes. object classes CONJUNCTION incremental phases. dynamic memory management strategy USED-FOR object classes. reinforcement learning USED-FOR reinforced memory management ( RMM ). it USED-FOR tasks. RMM USED-FOR pseudo CIL tasks. policy function USED-FOR pseudo CIL tasks. policy function USED-FOR RMM. tasks HYPONYM-OF pseudo CIL tasks. it USED-FOR replaying - based CIL method. it USED-FOR memory management. LUCIR+AANets CONJUNCTION POD+AANets. POD+AANets CONJUNCTION LUCIR+AANets. RMM USED-FOR top - performing baselines. POD+AANets HYPONYM-OF top - performing baselines. LUCIR+AANets HYPONYM-OF top - performing baselines. ImageNet - Subset CONJUNCTION ImageNet - Full. ImageNet - Full CONJUNCTION ImageNet - Subset. CIFAR-100 CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION CIFAR-100. CIFAR-100 HYPONYM-OF benchmarks. ImageNet - Subset HYPONYM-OF benchmarks. ImageNet - Full HYPONYM-OF benchmarks. Method are Class - Incremental Learning ( CIL ), and RMM training. Task are replaying, and CIL. ",This paper proposes a reinforcement learning-based reinforcement learning method for incremental learning. The main idea is to use reinforcement learning to improve the memory allocation in the incremental phase of class-incremental learning (CIL). The proposed method is based on reinforcement learning with reinforcement learning (RL). The main contribution of the paper is the use of reinforcement learning for memory allocation.    The main contributions of this paper are as follows:  1. The paper proposes to use RL to improve memory allocation for CIL.  2. The method is evaluated on three image classification tasks. ,This paper proposes a reinforcement learning-based replaying-based reinforcement learning method for memory management in class-incremental learning (CIL). The main idea is to use reinforcement learning to learn a dynamic memory management strategy for each incremental phase of CIL. The authors show that this strategy can be applied to both static and ad hoc memory allocation strategies. They also show that it can be used to improve the performance of existing baselines. 
8760,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"it USED-FOR speeding up stochastic gradient descent ( SGD ). Ω ( √ T ) communications USED-FOR local gradient steps. Ω ( √ T ) communications USED-FOR it. linear speed - up EVALUATE-FOR √ N or N communications. optimal convergence rate EVALUATE-FOR one - shot averaging. Method are stochastic gradient descent ( SGD ), Local SGD method, Local SGD, and Local SGD scheme. OtherScientificTerm are SGD steps, stochastic gradients, communication, parallelism, Ω(N ) communications, and twice differentiability. Task is linear reduction in the variance. Metric is error. ","This paper studies the convergence of local gradient descent (SGD) in the presence of two different types of communication: one-shot averaging and two-shot parallelism. The authors show that under certain assumptions, the convergence rate of local SGD converges linearly to a stationary point with probability $\tilde{O}(\sqrt{T}^T)$, where $T$ is the number of local gradients, and $\Omega(T)$ is a measure of the variance of the local gradient. They also show that the convergence is linear in $T$. The main contribution of the paper is a theoretical analysis of the convergence rates of the two communication methods. ","This paper studies the problem of speeding up stochastic gradient descent (SGD) with local communication. The main contribution of the paper is to study the convergence rate of a local SGD method, which can be used to speed up the rate of SGD. The authors show that the convergence of the proposed method is linear in the number of communication steps, and that it converges to the optimal convergence rate for one-shot averaging. The convergence rate is shown to be in the region of 1/\sqrt{T} in terms of the variance."
8776,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"Online Lazy Gradient Descent USED-FOR optimisation. strongly convex domain FEATURE-OF optimisation. O ( √ N ) regret EVALUATE-FOR algorithm. expected regret EVALUATE-FOR it. pseudo - regret CONJUNCTION expected regret. expected regret CONJUNCTION pseudo - regret. order bounds FEATURE-OF strongly convex domains. order bounds FEATURE-OF expected regret. order bounds FEATURE-OF pseudo - regret. OtherScientificTerm are adversarial opponents, i.i.d opponents, O(logN ) bounds, and simplex. Method is metaalgorithm. ","This paper studies online lazy gradient descent in the strongly convex case. The authors show that the online gradient descent algorithm can achieve O(N^2) regret in the convex setting, and O(log N) in the simplex setting. They also show that online lazy descent can achieve a pseudo-regret of $O(\sqrt{N})$ in the strong-convex case, and $O(logN)$ for the weak-concave case.   ",This paper studies the problem of online lazy gradient descent in the strongly convex domain. The main result is a meta-algorithm with O(N) expected regret bounds for the online Lazy Gradient Descent algorithm. The authors show that the expected regret of the algorithm is O(log N) with respect to the pseudo-regret of the opponent. They also provide an upper bound on the expected order of the pseudo regret.  
8792,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"Affine - Invariant ( AI ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry USED-FOR Riemannian optimization. Bures - Wasserstein ( BW ) geometry COMPARE Affine - Invariant ( AI ) geometry. Affine - Invariant ( AI ) geometry COMPARE Bures - Wasserstein ( BW ) geometry. symmetric positive definite ( SPD ) matrix manifold FEATURE-OF Riemannian optimization. linear dependence FEATURE-OF BW metric. BW metric USED-FOR Riemannian optimization problems. ill - conditioned SPD matrices USED-FOR Riemannian optimization problems. non - negative curvature FEATURE-OF BW geometry. OtherScientificTerm are SPD matrices, AI metric, and non - positively curved AI geometry. Metric is convergence rates. Method are cost functions, and AI geometry. Generic is applications. ","This paper studies the Affine-Invariant (AI) geometry for Riemannian optimization in the symmetric positive definite (SPD) matrix manifold. In particular, the authors show that the Bures-Wasserstein (BW) geometry is a non-positively curved version of the Affinine-invariant geometry (AI). The main contribution of the paper is to prove that the non-negative curvature of the BW metric is independent of the SPD matrix. The authors also show that for the case where the cost functions are ill-conditioned SPD matrices, the Bw metric has linear dependence on the cost function.  ","This paper studies the convergence of Affine-Invariant (AI) and Bures-Wasserstein (BW) metrics for Riemannian optimization problems. The authors show that under certain assumptions on the SPD matrices, the Affine Invariant metric converges faster than the Bures Wasserstein metric. They also show that the non-positively curvature of the Bure-wasserstein-AI metric does not affect the convergence rate. The main contribution of the paper is a theoretical analysis of the convergence rates of the two metrics. "
8808,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"Dynaboard HYPONYM-OF evaluation - as - a - service framework. evaluation - as - a - service framework USED-FOR holistic model comparison. Dynaboard CONJUNCTION Dynabench platform. Dynabench platform CONJUNCTION Dynaboard. platform USED-FOR NLP models. reproducibility CONJUNCTION accessibility. accessibility CONJUNCTION reproducibility. accessibility CONJUNCTION backwards compatibility. backwards compatibility CONJUNCTION accessibility. benchmarking USED-FOR NLP. memory use CONJUNCTION throughput. throughput CONJUNCTION memory use. throughput CONJUNCTION robustness. robustness CONJUNCTION throughput. robustness HYPONYM-OF metrics. memory use HYPONYM-OF metrics. throughput HYPONYM-OF metrics. Dynascore HYPONYM-OF utility - based aggregation. NLP models COMPARE benchmarks. benchmarks COMPARE NLP models. OtherScientificTerm is selfreported metrics. Generic are paradigm, models, and task. Material is leaderboards. ","This paper proposes a new evaluation-as-a-service framework, Dynaboard, for holistic NLP model comparison. The main idea is to use self-reported metrics (e.g., memory use, throughput, robustness) to evaluate the performance of different NLP models on a set of tasks. The authors also propose a utility-based aggregation method, called Dynascore, to aggregate the results of different models on different tasks. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy and robustness.","This paper proposes a new evaluation-as-a-service framework, Dynaboard, for holistic model comparison between NLP models. The authors propose Dynascore, a utility-based aggregation method to measure the performance of different models on different tasks. They also propose Dynabench, a platform that can be used to compare different models across different tasks and tasks. The proposed method is evaluated on a variety of metrics, including memory use, throughput, robustness, and robustness."
8824,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"filmmaking CONJUNCTION video production. video production CONJUNCTION filmmaking. re - recording actors ’ dialogues USED-FOR filmmaking. re - recording actors ’ dialogues USED-FOR video production. re - recording actors ’ dialogues USED-FOR Dubbing. Neural Dubber HYPONYM-OF neural network model. Neural Dubber USED-FOR automatic video dubbing ( AVD ) task. neural network model USED-FOR automatic video dubbing ( AVD ) task. Neural Dubber USED-FOR synthesizing human speech. synthesizing human speech HYPONYM-OF automatic video dubbing ( AVD ) task. lip movement USED-FOR Neural Dubber. Neural Dubber USED-FOR speech. timbre FEATURE-OF Neural Dubber. timbre FEATURE-OF speech. chemistry lecture single - speaker dataset CONJUNCTION LRS2 multi - speaker dataset. LRS2 multi - speaker dataset CONJUNCTION chemistry lecture single - speaker dataset. Neural Dubber COMPARE TTS models. TTS models COMPARE Neural Dubber. chemistry lecture single - speaker dataset EVALUATE-FOR Neural Dubber. LRS2 multi - speaker dataset EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR speech audios. speech audios COMPARE TTS models. TTS models COMPARE speech audios. speech quality EVALUATE-FOR TTS models. speech quality EVALUATE-FOR Neural Dubber. Neural Dubber USED-FOR high - fidelity speech. qualitative and quantitative evaluations EVALUATE-FOR Neural Dubber. Generic is It. Material are pre - recorded videos, and multi - speaker setting. ","This paper proposes a neural network model for automatic video dubbing (AVD) task. The proposed method, called Neural Dubber, is able to synthesize human speech from pre-recorded videos. The method is based on the observation that the timbre of speech is affected by the lip movement of the speaker. The authors show that the proposed method can produce high-quality speech audios.   ","This paper proposes a neural network model for automatic video dubbing (AVD) task that synthesizes human speech from pre-recorded videos. The proposed method, called Neural Dubber, is able to synthesize high-fidelity speech in a multi-speaker setting. The method is evaluated on a chemistry lecture single speaker dataset and LRS2 multi speaker dataset. The results show that the proposed method outperforms the state-of-the-art TTS models on both qualitative and quantitative evaluations."
8840,SP:24ea12428bd675459f0509aa7cee821fa236382e,"Federated learning USED-FOR healthcare sector. neural network training USED-FOR COVID-19 diagnosis. chest X - ray ( CXR ) images USED-FOR neural network training. chest X - ray ( CXR ) images USED-FOR COVID-19 diagnosis. Vision Transformer USED-FOR split learning. Vision Transformer HYPONYM-OF deep learning architecture. decomposable configuration FEATURE-OF deep learning architecture. framework COMPARE data - centralized training. data - centralized training COMPARE framework. CXR datasets USED-FOR framework. CXR datasets USED-FOR non - independent and identically distributed data distribution. framework CONJUNCTION heterogeneous multi - task clients. heterogeneous multi - task clients CONJUNCTION framework. Transformer USED-FOR collaborative learning. collaborative learning USED-FOR medical imaging. Transformer USED-FOR medical imaging. Method are neural network, and network architecture. Generic are it, network, and methods. Material are decentralized data, and patient CXR data. OtherScientificTerm are data privacy, and network bandwidth. Task is diagnosis of COVID-19. ","This paper proposes a method for distributed training of vision transformers on CXR images for COVID-19, a medical imaging dataset. The method is based on the idea of split learning, i.e., each client is given a set of images, and the goal is to train a vision transformer on each of them. The proposed method is evaluated on a variety of datasets, and compared to a number of baselines.   ","This paper proposes a method for federated learning of chest X-ray (CXR) images for medical imaging. The proposed method is based on the Transformer architecture, which is a decomposable architecture that can be used for split learning and multi-task learning. The authors show that the proposed method can achieve state-of-the-art performance on CXR datasets with heterogeneous data distribution and heterogeneous clients. "
8856,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"neural implicit representations USED-FOR 3D reconstruction. expressiveness CONJUNCTION flexibility. flexibility CONJUNCTION expressiveness. flexibility EVALUATE-FOR neural implicit representations. expressiveness EVALUATE-FOR neural implicit representations. oriented point cloud USED-FOR indicator function. differentiable PSR layer USED-FOR explicit 3D point representation. 3D mesh USED-FOR explicit 3D point representation. Chamfer distance HYPONYM-OF surface reconstruction metrics. oriented point clouds USED-FOR shapes. points CONJUNCTION patches. patches CONJUNCTION points. patches CONJUNCTION meshes. meshes CONJUNCTION patches. SAP USED-FOR topology - agnostic, watertight manifold surfaces. explicit representations COMPARE SAP. SAP COMPARE explicit representations. meshes HYPONYM-OF explicit representations. points HYPONYM-OF explicit representations. patches HYPONYM-OF explicit representations. SAP USED-FOR surface reconstruction. SAP USED-FOR learning - based reconstruction. unoriented point clouds CONJUNCTION learning - based reconstruction. learning - based reconstruction CONJUNCTION unoriented point clouds. learning - based reconstruction USED-FOR surface reconstruction. unoriented point clouds USED-FOR surface reconstruction. Metric are slow inference time, and inference time. Method are ubiquitous point cloud representation, differentiable point - to - mesh layer, and Poisson Surface Reconstruction ( PSR ). OtherScientificTerm is implicit indicator field. ","This paper proposes Poisson Surface Reconstruction (SAP), a differentiable point-to-mesmer (PSR) method for 3D surface reconstruction. The main idea is to use an oriented point cloud as an indicator function to estimate the Chamfer distance between a point and a mesh, which is then used to compute the surface reconstruction distance between the point and the mesh. The paper shows that the proposed method is computationally efficient and can be applied to any surface reconstruction problem.   ","This paper proposes a differentiable Poisson Surface Reconstruction (PSR) layer for 3D surface reconstruction. The PSR layer is differentiable from the point-to-point (P2P) layer, which is an explicit 3D point representation. The main idea is to use the Chamfer distance between points and patches as an indicator function for the surface reconstruction task. The paper shows that the differentiable PSR layers can be used to improve the performance of surface reconstruction on topology-agnostic, watertight manifold surfaces."
8872,SP:76b64e6b104818ed26e9331d134df0125d84291c,inverse problems USED-FOR recovering representations of corrupted data. pre - trained representation learning network R(x ) USED-FOR clean images. CLIP HYPONYM-OF clean images. representations USED-FOR corrupted images. supervised inversion method USED-FOR representations. forward operator USED-FOR corrupted version A(x ). contrastive objective USED-FOR supervised inversion method. blurring CONJUNCTION additive noise. additive noise CONJUNCTION blurring. linear probe USED-FOR robust representations. additive noise CONJUNCTION random pixel masking. random pixel masking CONJUNCTION additive noise. end - to - end supervised baselines USED-FOR classifying images. accuracy EVALUATE-FOR end - to - end supervised baselines. linear probe USED-FOR classifying images. linear probe COMPARE end - to - end supervised baselines. end - to - end supervised baselines COMPARE linear probe. distortions FEATURE-OF classifying images. random pixel masking HYPONYM-OF distortions. blurring HYPONYM-OF distortions. additive noise HYPONYM-OF distortions. ImageNet EVALUATE-FOR method. distortion FEATURE-OF method. method COMPARE end - to - end baselines. end - to - end baselines COMPARE method. forward operators FEATURE-OF labeled data. labeled data USED-FOR method. Material is images. ,This paper proposes a method for recovering representations of corrupted data from a pre-trained representation learning network R(x) from clean and corrupted images. The proposed method is based on a supervised inversion method to recover representations from corrupted images using a forward operator. The method is evaluated on ImageNet and achieves state-of-the-art performance.  ,"This paper proposes a supervised inversion method for recovering representations of corrupted data from a pre-trained representation learning network R(x) trained on clean images. The proposed method is based on a contrastive objective, which is used to recover representations from a corrupted version of A(x). The method is evaluated on ImageNet, where it outperforms end-to-end baselines in terms of accuracy and robustness."
8888,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,backpropagation USED-FOR local training of nodes. neural networks USED-FOR Structural credit assignment. reinforcement learning method USED-FOR node. REINFORCE USED-FOR node. global reward signal USED-FOR reinforcement learning method. REINFORCE HYPONYM-OF reinforcement learning method. reinforcement learning approaches USED-FOR learning. finite - horizon reinforcement learning problem USED-FOR neural network. off - policy learning HYPONYM-OF reinforcement learning. on - policy REINFORCE approach USED-FOR suboptimal solutions. on - policy REINFORCE approach CONJUNCTION variance reduction approaches. variance reduction approaches CONJUNCTION on - policy REINFORCE approach. networks of agents USED-FOR correlated samples. Generic is approach. Method is off - policy approach. OtherScientificTerm is stochasticity. ,"This paper proposes a reinforcement learning method for credit assignment in neural networks. The proposed method is based on the idea that credit assignment can be viewed as a structure learning problem, where each node in the network is responsible for assigning credit to a set of agents. The authors propose to learn a reward function for each node, which is a weighted sum of the rewards of the agents. This reward function is then used to train a neural network with backpropagation.   The authors show that the proposed method outperforms existing methods in terms of performance and variance reduction. ","This paper proposes a new reinforcement learning method REINFORCE for structural credit assignment. The main contribution of the paper is to propose a new approach to solve the finite-horizon reinforcement learning problem. The proposed method is based on the idea of backpropagation, which is an off-policy reinforcement learning approach. The authors show that the proposed method outperforms the state-of-the-art in terms of variance reduction and on-policy performance."
8904,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"parallel, hierarchical specialized pathways PART-OF visual system. pathways USED-FOR behaviours. visual recognition and movement FEATURE-OF behaviours. deep neural networks USED-FOR ventral, recognition pathway. deep ANN USED-FOR pathways. model USED-FOR ventral and the dorsal pathways. loss function USED-FOR ventral and the dorsal pathways. loss function USED-FOR model. models USED-FOR mouse visual cortex. self - supervised predictive loss function USED-FOR parallel pathways. parallel pathways USED-FOR deep neural network architecture. self - supervised predictive loss function USED-FOR deep neural network architecture. self - supervised predictive learning approach USED-FOR functional specialization. self - supervised predictive learning approach USED-FOR parallel pathway architectures. functional specialization FEATURE-OF mammalian visual systems. Material is mice. Task is recognition and movement behaviours. OtherScientificTerm is dorsal and ventral pathways. ","This paper proposes a self-supervised predictive loss function for parallel pathways in the mouse visual cortex. The proposed loss function is based on a combination of two loss functions, one for the dorsal and the ventral pathways, and the other for the inter-parallel and inter-dual pathways. The authors show that the proposed method is able to learn parallel pathways with the same number of parameters as the original dorsal and ventral networks. The method is evaluated on a variety of tasks in mice, including visual recognition and locomotion tasks.  ","This paper proposes a self-supervised predictive learning approach for functional specialization of the dorsal and ventral visual pathways in the mouse visual cortex. The authors propose a new loss function for the dorsal/ventral and dorsal/dorsal visual pathways, which is based on a deep neural network architecture. The proposed method is tested on a variety of tasks, including visual recognition and movement. The results show that the proposed method outperforms the state of the art."
8920,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"deep hierarchical topic models USED-FOR semantically meaningful topics. them PART-OF topic hierarchy. text corpus USED-FOR deep hierarchical topic models. text corpus USED-FOR semantically meaningful topics. prior belief USED-FOR learning of the topic hierarchy. knowledge graph HYPONYM-OF prior belief. TopicNet HYPONYM-OF deep hierarchical topic model. symmetric and asymmetric similarities FEATURE-OF Gaussian embedding vectors. TopicNet USED-FOR symmetric and asymmetric similarities. Gaussian - distributed embedding vector USED-FOR TopicNet. evidence lower bound CONJUNCTION regularization term. regularization term CONJUNCTION evidence lower bound. auto - encoding variational inference network USED-FOR model parameters. stochastic gradient descent USED-FOR regularization term. regularization term USED-FOR model parameters. evidence lower bound USED-FOR model parameters. stochastic gradient descent USED-FOR model parameters. deep topic models USED-FOR discovering deeper interpretable topics. TopicNet COMPARE deep topic models. deep topic models COMPARE TopicNet. TopicNet USED-FOR discovering deeper interpretable topics. TopicNet USED-FOR document representations. deep topic models USED-FOR document representations. OtherScientificTerm are prior structural knowledge, inductive bias, shared embedding space, and prior semantic hierarchies. Task is learning. ","This paper proposes TopicNet, a deep hierarchical topic model that learns to identify semantically meaningful topics in a text corpus. The model is based on a Gaussian-distributed embedding vector with symmetric and asymmetric similarities. The authors propose to use a knowledge graph as a prior belief for learning the topic hierarchy. The paper also proposes an auto-encoding variational inference network to learn the model parameters. Experiments show that TopicNet achieves state-of-the-art performance on the topic classification task.","This paper proposes TopicNet, a deep hierarchical topic model with a Gaussian-distributed embedding vector. The model is trained using an auto-encoding variational inference network and a stochastic gradient descent-based regularization term. The authors show that the model is able to learn symmetric and asymmetric similarities between two Gaussian embedding vectors. They also provide a lower bound on the evidence lower bound for the model parameters."
8936,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,Image - level contrastive representation learning USED-FOR transfer learning. generality USED-FOR transfer learning. specificity FEATURE-OF downstream task. design principle USED-FOR alignment. self - supervised pretext task CONJUNCTION downstream task. downstream task CONJUNCTION self - supervised pretext task. alignment USED-FOR self - supervised pretext task. alignment USED-FOR downstream task. pretraining method USED-FOR object detection. object - level translation invariance CONJUNCTION scale invariance. scale invariance CONJUNCTION object - level translation invariance. dedicated modules USED-FOR detection pipeline. selective search bounding boxes USED-FOR object proposals. object detection properties FEATURE-OF pretraining. FPN HYPONYM-OF detection pipeline. dedicated modules PART-OF pretraining network architecture. FPN HYPONYM-OF dedicated modules. scale invariance HYPONYM-OF object detection properties. object - level translation invariance HYPONYM-OF object detection properties. selective search bounding boxes USED-FOR object - level representations. COCO detection EVALUATE-FOR transfer. Selective Object COntrastive learning ( SoCo ) HYPONYM-OF method. transfer EVALUATE-FOR method. Mask R - CNN framework USED-FOR COCO detection. ,This paper proposes a novel pretraining method for object detection in image-level contrastive learning. The key idea is to use a self-supervised pretext task as a pretext task and a downstream task as an object detection task. The proposed method is evaluated on COCO and Mask R-CNN datasets.   ,"This paper proposes a novel method for transfer learning from image-level contrastive representation learning to object-level transfer learning. The key idea is to use the self-supervised pretext task as a pretext task, and then use the downstream task as an object detection task. The proposed method is based on the idea of selective search bounding boxes (SPB), which is used to select the object proposals for object detection. The method is evaluated on COCO detection and Mask R-CNN, where the proposed method outperforms the baseline method."
8952,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"Vehicle routing problems ( VRPs ) HYPONYM-OF combinatorial problems. heuristic or learning - based works USED-FOR decent solutions. small problem instances EVALUATE-FOR decent solutions. learning - augmented local search framework USED-FOR large - scale VRP. linear number of subproblems COMPARE exponential. exponential COMPARE linear number of subproblems. spatial locality USED-FOR linear number of subproblems. regression USED-FOR subproblem selection. method USED-FOR VRPs. solution qualities EVALUATE-FOR VRPs. method USED-FOR VRP solvers. solution qualities EVALUATE-FOR method. subproblem selection COMPARE heuristic or random selection. heuristic or random selection COMPARE subproblem selection. variants CONJUNCTION solvers. solvers CONJUNCTION variants. VRP distributions CONJUNCTION variants. variants CONJUNCTION VRP distributions. OtherScientificTerm are subproblems, and black box subsolver. Method is Transformer. ","This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs), where the number of subproblems grows linearly with the size of the problem instances. The proposed method is based on a regression-based method to select a set of possible solutions for a given VRP problem, which is then used to train a black-box subsolver. Experiments show that the proposed method outperforms heuristics and random selection methods in VRPs.","This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs). The main idea is to use regression to select the subproblems from a set of possible solutions, and then use a transformer-based subsolver to solve the subproblem selection problem. The proposed method is evaluated on a variety of VRP solvers, and the proposed method outperforms the state-of-the-art in terms of performance."
8968,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"dynamic data distributions USED-FOR Continual learning. learning - triggered synaptic expansion CONJUNCTION synaptic convergence. synaptic convergence CONJUNCTION learning - triggered synaptic expansion. learning - triggered synaptic expansion USED-FOR biological neural networks. synaptic Expansion - Convergence ( AFEC ) FEATURE-OF Active Forgetting. visual classification tasks CONJUNCTION Atari reinforcement tasks. Atari reinforcement tasks CONJUNCTION visual classification tasks. CIFAR-10 regression tasks CONJUNCTION visual classification tasks. visual classification tasks CONJUNCTION CIFAR-10 regression tasks. AFEC USED-FOR learning of new tasks. continual learning benchmarks EVALUATE-FOR AFEC. continual learning benchmarks EVALUATE-FOR AFEC. Atari reinforcement tasks HYPONYM-OF continual learning benchmarks. visual classification tasks HYPONYM-OF continual learning benchmarks. CIFAR-10 regression tasks HYPONYM-OF continual learning benchmarks. OtherScientificTerm are knowledge transfer, and forward knowledge transfer. Task is continual learning. Method are biological active forgetting, and Bayesian continual learning. Generic are approach, method, and them. ","This paper proposes a Bayesian continual learning method called Active Forgetting with Synaptic Expansion-Convergence (AFEC) to address the problem of active forgetting in continual learning. The method is based on the observation that in biological neural networks, learning-triggered synaptic expansion and learning-induced convergence are responsible for active forgetting. The authors show that AFEC can be viewed as an extension of the active forgetting phenomenon observed in biological networks. Theoretical analysis is provided to show that the proposed method is able to learn new tasks in a continual learning setting. Experiments are conducted on Atari and CIFAR-10 tasks.   ",This paper proposes a Bayesian continual learning approach for active forgetting in biological neural networks. The main idea is to use the learning-triggered synaptic expansion-convergence (AFEC) as a way to transfer knowledge between tasks. The authors show that AFEC can be used to improve the performance of the current task on a variety of continual learning benchmarks. They also show that the AFEC method can be applied to the task of Bayesian continuous learning. 
8984,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,Zeroth - order ( ZO ) optimization USED-FOR tasks. query - based black - box adversarial attacks CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION query - based black - box adversarial attacks. query - based black - box adversarial attacks HYPONYM-OF tasks. reinforcement learning HYPONYM-OF tasks. prior information PART-OF gradient estimation procedure. finite differences USED-FOR gradient estimation procedure. finite differences USED-FOR prior information. greedy descent framework CONJUNCTION gradient estimators. gradient estimators CONJUNCTION greedy descent framework. convergence FEATURE-OF prior - guided ZO algorithms. greedy descent framework USED-FOR prior - guided ZO algorithms. prior information USED-FOR accelerated random search ( ARS ) algorithm. convergence analysis USED-FOR accelerated random search ( ARS ) algorithm. numerical benchmarks CONJUNCTION adversarial attacks. adversarial attacks CONJUNCTION numerical benchmarks. OtherScientificTerm is convergence guarantee. Method is greedy descent methods. ,"This paper proposes a greedy descent algorithm for zeroth-order optimization (ZO) that uses prior information to estimate the gradient of the objective function. The method is based on the greedy descent framework, where the gradient estimator is estimated using finite differences. The authors show that the proposed method converges to the optimal solution in a time-varying manner. They also provide a convergence analysis for the proposed algorithm.","This paper studies the convergence of prior-guided Zeroth-order optimization (ZO) algorithms in the context of greedy descent. In particular, the authors propose a greedy descent framework for ZO algorithms that uses the prior information to guide the gradient estimation procedure. The authors provide convergence analysis for a variety of greedy-guided ZO methods. They also provide a convergence analysis of an accelerated random search (ARS) algorithm. "
9000,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ","This paper studies the lottery ticket hypothesis (LTH) in deep neural networks. The authors show that a pruned neural network with LTH can achieve better generalization performance than an unpruned network without LTH. In particular, the authors prove that the generalization error of the pruned network is bounded by a convex function, and that the LTH is guaranteed to be zero when the weights of the network are pruned.   ",This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that pruned neural networks can be more general than unpruned ones in terms of generalization error. They show that the generalization of a pruned network is bounded by the number of samples and the sample complexity of the model. They also provide an algorithm to prune the weights in the hidden layer of a neural network. 
9016,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"private synthetic data generation USED-FOR query release. differential privacy FEATURE-OF sensitive dataset. algorithmic framework USED-FOR iterative algorithms. computational bottlenecks PART-OF algorithms. generative networks CONJUNCTION exponential mechanism ( GEM ). exponential mechanism ( GEM ) CONJUNCTION generative networks. MWEM HYPONYM-OF algorithms. neural networks USED-FOR generative models. MWEM USED-FOR private entropy projection ( PEP ). PEP COMPARE algorithms. algorithms COMPARE PEP. GEM COMPARE algorithms. algorithms COMPARE GEM. GEM CONJUNCTION PEP. PEP CONJUNCTION GEM. prior information USED-FOR GEM. public data USED-FOR prior information. public data USED-FOR state - of - the - art method. OtherScientificTerm is statistical queries. Generic are framework, methods, and method. Method are gradient - based optimization, and PMWPub. Metric is accuracy. ","This paper proposes a new method for releasing private synthetic data generated by a generative model in the presence of differential privacy. The method is based on the exponential mechanism (GEM) algorithm, which is an extension of the private entropy projection (PEP) algorithm. The main contribution of the paper is to propose a new algorithm, called MWEM, which uses GEM to generate synthetic data in an iterative fashion. Theoretical analysis is provided to show that the proposed method is computationally efficient. Experiments are conducted on two synthetic datasets and show that it outperforms PEP and GEM.","This paper proposes a new algorithm for private synthetic data generation for differential privacy. The proposed method is based on the exponential mechanism (GEM) framework, which can be applied to any generative model. The authors show that the proposed method can be used to perform private entropy projection (PEP) on public data. They also show that their method is more accurate than the state-of-the-art methods. "
9032,SP:d789e92c1e4f6a44de373210cd732198a6f809be,per - pixel classification task USED-FOR semantic segmentation. mask classification USED-FOR instance - level segmentation. mask classification USED-FOR semanticand instance - level segmentation tasks. model USED-FOR mask classification. training procedure USED-FOR mask classification. MaskFormer HYPONYM-OF mask classification model. mask classification model USED-FOR binary masks. single global class label prediction USED-FOR binary masks. approaches USED-FOR semantic and panoptic segmentation tasks. mask classification - based method USED-FOR approaches. mask classification - based method USED-FOR semantic and panoptic segmentation tasks. MaskFormer COMPARE per - pixel classification baselines. per - pixel classification baselines COMPARE MaskFormer. , semantic and panoptic segmentation tasks. This paper proposes a mask classification method for instance-level semantic segmentation. The proposed method is based on a single global class label prediction for each mask. The method is evaluated on a variety of tasks and achieves state of the art performance.,This paper proposes a new per-pixel mask classification method for semantic and instance-level segmentation tasks. The proposed method is based on a single global class label prediction and a training procedure for mask classification. The method is evaluated on a variety of semantic and panoptic segmentation benchmarks.
9048,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,gradient descent USED-FOR random undercomplete two - layers ReLU neural networks. smooth activation function FEATURE-OF subexponential width random neural network. Material is overcomplete case. OtherScientificTerm is dimension. ,This paper studies the problem of random undercomplete two-layer ReLU neural networks with a smooth activation function. The authors show that gradient descent can be used to solve the problem in the undercomplete case. The main contribution is to show that undercomplete ReLU networks with smooth activation functions can be solved by gradient descent.  ,"This paper studies the problem of random undercomplete two-layer ReLU neural networks in the subexponential width case. The authors prove that undercomplete ReLU networks with smooth activation function have a subexponentially width random neural network. They show that the undercomplete case is the case where the width of the network is underexponential in dimension. They also show that in the overcomplete case, the width is sub-exponential.  "
9064,SP:220db9ed147bbe67de5d82778720a1549656e48d,"sample quality CONJUNCTION distribution coverage. distribution coverage CONJUNCTION sample quality. distribution coverage EVALUATE-FOR Score - based generative models ( SGMs ). sample quality EVALUATE-FOR Score - based generative models ( SGMs ). network evaluations USED-FOR sampling. data space FEATURE-OF they. approach USED-FOR SGMs. latent space FEATURE-OF SGMs. variational autoencoder framework USED-FOR approach. non - continuous data USED-FOR SGMs. score - matching objective USED-FOR LSGM setting. SGM USED-FOR mismatch of the target distribution. Normal one USED-FOR mismatch of the target distribution. LSGM COMPARE generative results. generative results COMPARE LSGM. dataset EVALUATE-FOR LSGM. dataset EVALUATE-FOR generative results. FID score EVALUATE-FOR LSGM. CIFAR-10 EVALUATE-FOR LSGM. LSGM COMPARE SGMs. SGMs COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR LSGM. LSGM COMPARE them. them COMPARE LSGM. CelebA - HQ-256 EVALUATE-FOR SGMs. sampling time EVALUATE-FOR them. sample quality EVALUATE-FOR SGMs. sample quality EVALUATE-FOR LSGM. LSGM USED-FOR binary images. binarized OMNIGLOT dataset EVALUATE-FOR LSGM. Method are generative models, and LSGMs. OtherScientificTerm is score function. Task is variance reduction of the training objective. Generic is implementation. ","This paper proposes a score-based generative model (SGM) for non-continuous data with a score matching objective. The main idea is to use a variational autoencoder (VAE) to model the distribution of the target distribution in the latent space of the SGMs. The VAE is used to learn the score-matching objective, which is then used as the training objective in the SGM setting. The proposed method is evaluated on CIFAR-10, CelebA-HQ-256, and binarized OMNIGLOT datasets. ",This paper proposes a score-matching generative model (SGM) for non-continuous data. The main idea is to use a variational autoencoder (VAE) framework to model the latent space of SGMs. The authors propose a score matching objective to match the score of the target distribution with the score function of the source distribution. They show that the proposed SGM can improve the sample quality and distribution coverage of SGM. They also show that it can reduce the variance of the training objective. 
9080,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"Neural networks COMPARE kernel methods. kernel methods COMPARE Neural networks. neural tangent kernels HYPONYM-OF kernel methods. hypothesis class USED-FOR realistic data. noise FEATURE-OF sparse signal. convolutional neural network USED-FOR noise. high - variance noise FEATURE-OF sparse signal. convolutional neural network USED-FOR data distribution. sparse signal FEATURE-OF data distribution. stochastic gradient descent USED-FOR convolutional neural network. predetermined features USED-FOR neural tangent kernel. CNN COMPARE neural tangent kernel. neural tangent kernel COMPARE CNN. CIFAR-10 and MNIST images EVALUATE-FOR CNN. neural networks COMPARE kernel methods. kernel methods COMPARE neural networks. OtherScientificTerm are complex hypothesis class, background noise, and local signal adaptivity ( LSA ) phenomenon. Task is image classification setting. ","This paper proposes to use a convolutional neural network to model the data distribution in the presence of high-variance noise. The proposed method is based on the idea of local signal adaptivity (LSA), which is an extension of the local signal adaptation (LSA) phenomenon. Theoretical results show that the LSA phenomenon can be explained as a result of the high variance of the data. The paper also shows that the proposed method can be used in conjunction with neural tangent kernels. ","This paper proposes a new neural tangent kernel (NTK) method for image classification. NTK is based on the idea of local signal adaptivity (LSA), which is the ability of a neural network to adapt to the data distribution. The authors show that NTK can adaptively adapt the data to different data distributions. They also show that the LSA phenomenon can be used to improve the performance of neural networks. "
9096,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"network USED-FOR decentralized machine learning. model USED-FOR local loss functions. known convergence rates EVALUATE-FOR GT algorithms. negative eigenvalues FEATURE-OF connectivity matrix. Method are stochastic model updates, gradient tracking ( GT ) algorithms, and GT method. OtherScientificTerm are workers ’ local data distributions, mixing parameter p, and O(p−3/2 ). Material are noiseless case, and stochastic case. ","This paper studies the convergence of gradient tracking (GT) algorithms for decentralized decentralized learning. The authors show that the convergence rate of the gradient tracking algorithm is O(p-3/2) in the noisy case, and O(1/3) for the noiseless case. The convergence rate is also shown in the stochastic case, where the mixing parameter p is unknown. ","This paper studies the convergence of gradient tracking (GT) algorithms for decentralized machine learning. The authors show that the convergence rate of GT algorithms converge to O(p-3/2) when the mixing parameter p is large enough. They show that this convergence rate is O(1/\sqrt{O(p+1/2), where p is a negative eigenvalue of the connectivity matrix. They also show that under certain assumptions on the mixing parameters p and the number of workers, the convergence rates of the GT algorithms are O(2/3) when p is small enough."
9112,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"Upper Confidence Bound ( UCB ) policy HYPONYM-OF optimism - based MAB algorithms. O ( log n ) regret EVALUATE-FOR it. arm - sampling behavior FEATURE-OF UCB. UCB USED-FOR arm - sampling rates. O p n log n minimax regret EVALUATE-FOR UCB. process - level characterization FEATURE-OF MAB problem. diffusion scaling FEATURE-OF UCB. diffusion scaling FEATURE-OF MAB problem. UCB USED-FOR MAB problem. UCB CONJUNCTION Thompson Sampling. Thompson Sampling CONJUNCTION UCB. incomplete learning phenomenon FEATURE-OF latter. Metric are complexity, and problem complexity. OtherScientificTerm are mean rewards, instance gap, and small ” gap worst - case lens. ","This paper studies optimism-based MAB algorithms with upper confidence bound (UCB) policy. The authors show that UCB has a regret of $O(\log n)$ regret, where $n$ is the number of arms and $p$ the reward distribution. They show that the regret of UCB is bounded by $O(1/\sqrt{n})$ when $p = 1$. They also show that this regret can be bounded by $\Omega(n^2)$.   ",This paper studies the Upper Confidence Bound (UCB) policy for optimism-based MAB algorithms. The main contribution of the paper is to provide a process-level characterization of the UCB policy. The authors show that UCB can be viewed as an incomplete learning phenomenon. They also provide a theoretical analysis of UCB and Thompson Sampling. 
9128,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"Cross - Domain Recommendation ( CDR ) USED-FOR cold - start problem. domain knowledge USED-FOR cold - start problem. cold - start problem PART-OF recommender systems. domain knowledge USED-FOR Cross - Domain Recommendation ( CDR ). cold - start CONJUNCTION CDR. CDR CONJUNCTION cold - start. approaches USED-FOR CDR. approaches USED-FOR cold - start. cross - domain recommendation framework USED-FOR CDCSR problem. DisAlign HYPONYM-OF cross - domain recommendation framework. rating and auxiliary representations USED-FOR recommendation. rating and auxiliary representations USED-FOR DisAlign. Stein path alignment USED-FOR latent embedding distributions. proxy Stein path HYPONYM-OF version. DisAlign COMPARE models. models COMPARE DisAlign. Douban and Amazon datasets EVALUATE-FOR DisAlign. CDCSR setting EVALUATE-FOR models. CDCSR setting EVALUATE-FOR DisAlign. OtherScientificTerm are latent embedding discrepancy, and model degradation. Metric is efficiency. ",This paper proposes a method to address the cross-domain recommendation problem in recommender systems. The authors propose to use Stein path alignment to align the latent embedding distributions between the source and target domains. The proposed method is evaluated on the Douban and Amazon datasets.   ,This paper proposes a new approach for cross-domain recommendation (CDR) based on the Stein path alignment (Stein path alignment) method. The key idea is to use a proxy Stein path to align the latent embedding distributions between the two domains. The proposed approach is evaluated on Douban and Amazon datasets. 
9144,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,self - attention CONJUNCTION multi - layer perceptrons ( MLP ) models. multi - layer perceptrons ( MLP ) models CONJUNCTION self - attention. multi - layer perceptrons ( MLP ) models USED-FOR vision. self - attention USED-FOR vision. self - attention CONJUNCTION MLP. MLP CONJUNCTION self - attention. complexity EVALUATE-FOR MLP. complexity EVALUATE-FOR self - attention. Global Filter Network ( GFNet ) HYPONYM-OF architecture. Global Filter Network ( GFNet ) USED-FOR long - term spatial dependencies. architecture USED-FOR long - term spatial dependencies. frequency domain FEATURE-OF long - term spatial dependencies. 2D discrete Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication CONJUNCTION 2D discrete Fourier transform. global filters CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION global filters. self - attention layer PART-OF vision transformers. element - wise multiplication USED-FOR frequency - domain features. element - wise multiplication CONJUNCTION 2D inverse Fourier transform. 2D inverse Fourier transform CONJUNCTION element - wise multiplication. element - wise multiplication USED-FOR global filters. operations PART-OF architecture. self - attention layer PART-OF architecture. 2D discrete Fourier transform HYPONYM-OF operations. global filters HYPONYM-OF operations. element - wise multiplication HYPONYM-OF operations. 2D inverse Fourier transform HYPONYM-OF operations. accuracy / complexity trade - offs EVALUATE-FOR models. ImageNet and downstream tasks EVALUATE-FOR models. efficiency CONJUNCTION generalization ability. generalization ability CONJUNCTION efficiency. generalization ability CONJUNCTION robustness. robustness CONJUNCTION generalization ability. GFNet COMPARE transformer - style models. transformer - style models COMPARE GFNet. GFNet COMPARE CNNs. CNNs COMPARE GFNet. transformer - style models CONJUNCTION CNNs. CNNs CONJUNCTION transformer - style models. efficiency EVALUATE-FOR CNNs. generalization ability EVALUATE-FOR CNNs. robustness EVALUATE-FOR CNN,"This paper proposes a new architecture for vision transformers that combines 2D discrete Fourier transform with 2D inverse Fourier transforms and 2D element-wise multiplication to improve efficiency and robustness. The proposed architecture, called Global Filter Network (GFNet), is able to capture long-term spatial dependencies in the frequency domain. Experiments on ImageNet and downstream tasks show that the proposed method achieves state-of-the-art accuracy and complexity trade-offs.","This paper proposes a new architecture for multi-layer vision transformers, called Global Filter Network (GFNet), which is based on the idea of long-term spatial dependencies in the frequency domain. GFNet consists of two operations: 1) global filters and 2D inverse Fourier transform; 2) element-wise multiplication; and 3) 2D discrete Fourier transforms. The proposed GFNet achieves state-of-the-art performance on ImageNet and downstream tasks."
9160,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"high - dimensional features CONJUNCTION visual concepts. visual concepts CONJUNCTION high - dimensional features. real - world large - scale datasets USED-FOR predicting trustworthiness. focal loss CONJUNCTION true class probability confidence loss. true class probability confidence loss CONJUNCTION focal loss. cross entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross entropy loss. cross entropy loss HYPONYM-OF trustworthiness predictors. focal loss HYPONYM-OF trustworthiness predictors. cross entropy loss HYPONYM-OF prior - art loss functions. true class probability confidence loss HYPONYM-OF prior - art loss functions. focal loss HYPONYM-OF prior - art loss functions. prior - art loss functions USED-FOR trustworthiness predictors. steep slope loss USED-FOR features. steep slope loss USED-FOR trustworthiness predictors. Vision Transformer CONJUNCTION ResNet. ResNet CONJUNCTION Vision Transformer. deep learning models USED-FOR trustworthiness predictors. trustworthiness predictors EVALUATE-FOR loss. deep learning models EVALUATE-FOR loss. Vision Transformer HYPONYM-OF deep learning models. ResNet HYPONYM-OF deep learning models. loss USED-FOR trustworthiness predictors. Method are classifier, and AI models. Material are small - scale datasets, and ImageNet. Generic is task. Metric are data complexity, and generalizability of trustworthiness predictors. OtherScientificTerm is slide - like curves. ","This paper studies the problem of trustworthiness prediction on ImageNet, a large-scale image classification task. The authors propose two loss functions, namely true class probability confidence loss and steep slope loss, to improve the generalizability of the trustworthiness predictors. The main contribution of the paper is to propose a new loss function, which is based on the concept of slide-like curves. Theoretical analysis is provided to show that the proposed loss can be used to improve trustworthiness predictions. Experiments are conducted to show the effectiveness of the proposed method. ",This paper proposes a new trustworthiness prediction loss for large-scale datasets. The proposed loss is based on the popular cross-entropy loss and true class probability confidence loss. The authors also propose a new steep slope loss to improve the generalizability of trustworthiness predictors. They show that the proposed loss can be used in combination with the cross entropy loss and the true class confidence loss to achieve better generalization.
9176,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"adversarial examples USED-FOR Adversarial robustness. robustness FEATURE-OF adversarial attacks. robust models USED-FOR adversarial attacks. robustness EVALUATE-FOR robust models. linear components USED-FOR adversarial robustness. batch normalization CONJUNCTION maximum pooling. maximum pooling CONJUNCTION batch normalization. maximum pooling CONJUNCTION activation layers. activation layers CONJUNCTION maximum pooling. activation layers HYPONYM-OF non - linear components. maximum pooling HYPONYM-OF non - linear components. batch normalization HYPONYM-OF non - linear components. domain adaption CONJUNCTION robustness boosting. robustness boosting CONJUNCTION domain adaption. it USED-FOR tasks. it USED-FOR domain adaption. it USED-FOR robustness boosting. robustness boosting HYPONYM-OF tasks. domain adaption HYPONYM-OF tasks. OtherScientificTerm are statistical properties, and linearized sub - networks. Method is clustering strategy. ",This paper studies adversarial robustness in the presence of adversarial examples. The authors show that linearized sub-networks with batch normalization and maximum pooling are more robust to adversarial attacks than linearized networks with no batch regularization. They further propose a clustering strategy to improve the robustness.   ,"This paper studies the problem of robustness against adversarial attacks. The authors show that the robustness of linearized sub-networks can be improved by adding non-linear components such as batch normalization, maximum pooling, and activation layers. They propose a clustering strategy to improve robustness, and show that this strategy can be applied to domain adaptation and robustness boosting tasks. "
9201,SP:590b67b1278267e966cf0b31456d981441e61bb1,approach USED-FOR end - to - end reconstruction operators. unpaired training data USED-FOR ill - posed inverse problems. unpaired training data USED-FOR approach. expected distortion CONJUNCTION Wasserstein-1 distance. Wasserstein-1 distance CONJUNCTION expected distortion. variational framework CONJUNCTION iterative unrolling. iterative unrolling CONJUNCTION variational framework. measurement space FEATURE-OF expected distortion. variational framework USED-FOR method. regularizer USED-FOR variational setting. deep neural network USED-FOR regularizer. unrolled reconstruction operator USED-FOR regularizer. reconstruction network USED-FOR variational problem. it COMPARE variational methods. variational methods COMPARE it. unrolled operator USED-FOR initialization. initialization USED-FOR it. well - posedness CONJUNCTION noise - stability guarantees. noise - stability guarantees CONJUNCTION well - posedness. noise - stability guarantees FEATURE-OF variational setting. well - posedness FEATURE-OF variational setting. end - to - end unrolled reconstruction USED-FOR approach. well - posedness FEATURE-OF approach. well - posedness FEATURE-OF end - to - end unrolled reconstruction. it COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE it. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. approach COMPARE it. it COMPARE approach. approach COMPARE supervised data - driven reconstruction approaches. supervised data - driven reconstruction approaches COMPARE approach. OtherScientificTerm is reconstruction. Material is X - ray computed tomography ( CT ). ,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems with unpaired training data. The proposed method is based on the variational framework and iterative unrolling. The main contribution of the paper is the use of a deep neural network as a regularizer for the reconstruction operator. The method is shown to achieve state-of-the-art performance in terms of well-posedness and noise stability. ,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on a variational framework with a deep neural network and an unrolled operator. The authors show that the proposed method can achieve well-posedness and noise-stability guarantees in the well-probability setting. They also show that their method can be applied to unsupervised data-driven reconstruction.
9226,SP:115d679338ab35829dbc594472d13cc02be5ed4c,Large - scale vision and language representation learning USED-FOR vision - language tasks. transformer - based multimodal encoder USED-FOR word tokens. transformer - based multimodal encoder USED-FOR methods. multimodal encoder USED-FOR image - text interactions. visual tokens CONJUNCTION word tokens. word tokens CONJUNCTION visual tokens. contrastive loss USED-FOR image and text representations. cross - modal attention USED-FOR vision and language representation learning. methods COMPARE method. method COMPARE methods. bounding box annotations CONJUNCTION high - resolution images. high - resolution images CONJUNCTION bounding box annotations. high - resolution images USED-FOR method. momentum distillation HYPONYM-OF self - training method. pseudo - targets USED-FOR self - training method. momentum model USED-FOR pseudo - targets. momentum model USED-FOR self - training method. downstream visionlanguage tasks EVALUATE-FOR ALBEF. ALBEF COMPARE methods. methods COMPARE ALBEF. image - text retrieval EVALUATE-FOR ALBEF. image - text retrieval EVALUATE-FOR methods. NLVR2 EVALUATE-FOR ALBEF. VQA EVALUATE-FOR ALBEF. ALBEF COMPARE state - ofthe - art. state - ofthe - art COMPARE ALBEF. VQA CONJUNCTION NLVR2. NLVR2 CONJUNCTION VQA. Material is noisy web data. Task is training tasks. OtherScientificTerm is image - text pair. ," image-text retrieval is an important task in large-scale vision and language representation learning. This paper proposes a self-training method called ALBEF, which uses a transformer-based multimodal encoder to model image-language interactions and a contrastive loss for image and text representations. The proposed method achieves state-of-the-art performance on downstream vision-language tasks. ","This paper proposes a self-training method for large-scale vision and language representation learning for vision-language tasks. The main idea is to use a transformer-based multimodal encoder for image-text interactions and a contrastive loss for image and text representations. The proposed method is evaluated on VQA, NLVR2, and text-to-text tasks."
9251,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,Markov decision processes ( MDPs ) USED-FOR offline policy evaluation ( OPE ). static datasets USED-FOR decisionmaking policies. OPE COMPARE realizable setting. realizable setting COMPARE OPE. unrealizability USED-FOR OPE. unrealizability USED-FOR OPE method. linear direct method ( DM ) HYPONYM-OF OPE method. doubly robust form FEATURE-OF OPE error. nonparametric consistency FEATURE-OF tile - coding estimators. OtherScientificTerm is approximate ) realizability assumptions. Method is hypothetical models. Task is real - world applications. ,"This paper studies offline policy evaluation (OPE) in MDPs, where the goal is to evaluate the performance of a policy in an offline setting. The authors propose a new offline OPE method based on a linear direct method (LDP), which is shown to be doubly robust to non-parametric consistency. They also show that the OPE error can be estimated from tile-coding estimators.   ",This paper proposes a new offline policy evaluation (OPE) method for offline MDPs. The main idea is to use a linear direct method (DM) to estimate the OPE error in a doubly robust form. The authors show that the proposed method is more robust than existing OPE methods in the realizable setting. They also show that their method is consistent with nonparametric consistency of the estimators. 
9276,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"stochastic 1 first - order methods USED-FOR large - scale machine learning models. theoretical guarantees FEATURE-OF expectation of the objective value. algorithms USED-FOR small objective residual. Existing methods USED-FOR non - smooth stochastic convex optimization. complexity 7 bounds FEATURE-OF Existing methods. logarithmic dependence FEATURE-OF high - probability convergence. stepsize rules USED-FOR stochastic methods. gradient 14 clipping USED-FOR stochastic methods. extension USED-FOR strongly convex problems. Hölder - continuous gradients FEATURE-OF generalized smooth objectives. extension USED-FOR methods. non - smooth setting FEATURE-OF one. iteration and oracle complexity EVALUATE-FOR accelerated ) 17 method. OtherScientificTerm are Random behavior, suboptimal objective value, confidence level, and negative - power. Generic is algorithm. Task are NLP tasks, and non - smooth convex stochastic 12 optimization problems. ","This paper studies non-smooth convex stochastic convex optimization problems, where the objective function is a convex convex function with a small objective residual. The authors propose to use gradient clipping to improve the convergence rate of existing methods in this setting. The main contribution of this paper is to extend the gradient clipping method to strongly convex problems.    The main contributions of this work are:  1) The authors show that gradient clipping can be used to speed up the convergence of the proposed methods.  2) The proposed method is shown to converge to the optimal solution of the convex problem with high probability.",This paper studies the problem of non-smooth stochastic convex optimization in the setting of strongly convex problems. The main contribution of the paper is a theoretical analysis of the high-probability convergence of existing methods in this setting. The authors show that the complexity of the existing methods is bounded by a logarithmic dependence on the confidence level of the objective value. They also provide an extension of the Hölder-Continuous Gradient clipping (HCC) method to the non-Smooth convex setting.
9301,SP:a22a893e25ce739dc757861741014764e78aa820,"extreme weather early warning CONJUNCTION long - term energy consumption planning. long - term energy consumption planning CONJUNCTION extreme weather early warning. self - attention mechanisms USED-FOR long - range dependencies. Transformerbased models USED-FOR long - range dependencies. self - attention mechanisms USED-FOR Transformerbased models. point - wise self - attentions USED-FOR long series efficiency. point - wise self - attentions USED-FOR Transformers. decomposition architecture USED-FOR Autoformer. Auto - Correlation mechanism USED-FOR decomposition architecture. Auto - Correlation mechanism USED-FOR Autoformer. pre - processing convention PART-OF series decomposition. design USED-FOR Autoformer. Autoformer USED-FOR complex time series. progressive decomposition capacities USED-FOR complex time series. progressive decomposition capacities FEATURE-OF Autoformer. dependencies discovery CONJUNCTION representation aggregation. representation aggregation CONJUNCTION dependencies discovery. stochastic process theory USED-FOR Auto - Correlation mechanism. series periodicity USED-FOR dependencies discovery. representation aggregation PART-OF sub - series level. series periodicity USED-FOR Auto - Correlation mechanism. Auto - Correlation COMPARE self - attention. self - attention COMPARE Auto - Correlation. efficiency EVALUATE-FOR Auto - Correlation. accuracy EVALUATE-FOR Auto - Correlation. traffic CONJUNCTION economics. economics CONJUNCTION traffic. energy CONJUNCTION traffic. traffic CONJUNCTION energy. energy CONJUNCTION economics. economics CONJUNCTION energy. long - term forecasting EVALUATE-FOR Autoformer. practical applications FEATURE-OF benchmarks. energy HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR Autoformer. accuracy EVALUATE-FOR Autoformer. traffic HYPONYM-OF benchmarks. economics HYPONYM-OF benchmarks. energy HYPONYM-OF practical applications. traffic HYPONYM-OF practical applications. economics HYPONYM-OF practical applications. OtherScientificTerm are forecasting time, and information utilization bottleneck. Task is long - term forecasting problem of time series. Generic are model, and it. Method is deep models. ","This paper proposes a Transformer-based architecture for long-term forecasting of time series. The proposed architecture is based on a series decomposition architecture, which uses an auto-correlation mechanism to discover long-range dependencies. The authors show that the proposed method is more efficient than self-attention in terms of time-series efficiency. The method is evaluated on time series forecasting tasks in traffic, energy, and economics.","This paper proposes a new Transformer-based model for long-term forecasting of time series. The main idea is to decompose the time series into sub-series using a series decomposition convention. The proposed method is based on the Auto-Correlation mechanism, which uses the series periodicity to discover long-range dependencies. The authors show that the proposed method outperforms the state-of-the-art self-attention mechanism in terms of accuracy and efficiency."
9326,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,NLP systems USED-FOR compositional language. Cryptic crosswords HYPONYM-OF dominant crossword variety. character - level manipulations USED-FOR wordplay cipher. creative intelligence USED-FOR cryptics. NLP systems USED-FOR compositional language. cryptic clues USED-FOR NLP systems. dataset USED-FOR NLP systems. dataset USED-FOR compositional language. cryptic clues FEATURE-OF dataset. model USED-FOR tasks. T5 HYPONYM-OF neural language model. non - neural approaches CONJUNCTION T5. T5 CONJUNCTION non - neural approaches. unscrambling words HYPONYM-OF tasks. meta - linguistic capabilities FEATURE-OF subword - tokenized models. T5 COMPARE human solving strategies. human solving strategies COMPARE T5. wordplay part of clues USED-FOR model systematicity. curricular approach COMPARE T5 baseline. T5 baseline COMPARE curricular approach. cryptic crosswords PART-OF NLP systems. OtherScientificTerm is Cryptic clues. Method is curriculum approach. ,"This paper introduces a new dataset of cryptic crossword clues, which is a collection of word-play-based clues that can be manipulated at character-level. The authors propose a new neural language model, T5, that is trained to solve the cryptic clues. They show that T5 outperforms the state-of-the-art methods on the task of unscrambling cryptic words. They also show that the model is able to learn to solve cryptic clues in a curriculum-based manner.","This paper presents a new dataset of cryptic crossword clues for NLP systems. The dataset consists of a set of cryptic clues that can be used to train a neural language model to solve crossword puzzles. The key idea of the dataset is to use the wordplay part of the clues to improve the model's systematicity. The model is trained using a curriculum approach, where it is shown to be able to solve the crossword puzzle more efficiently than humans. "
9351,SP:7693974b70806d9b67920b8ddd2335afc4883319,"Convolutional neural networks ( CNNs ) USED-FOR visual data. image classification tasks EVALUATE-FOR ( Vision ) Transformer models ( ViT ). Vision Transformers USED-FOR tasks. ViTs CONJUNCTION CNNs. CNNs CONJUNCTION ViTs. internal representation structure USED-FOR ViTs. internal representation structure USED-FOR CNNs. uniform representations USED-FOR ViT. image classification benchmarks EVALUATE-FOR internal representation structure. image classification benchmarks EVALUATE-FOR ViTs. ViT HYPONYM-OF architectures. image classification benchmarks EVALUATE-FOR CNNs. self - attention CONJUNCTION ViT residual connections. ViT residual connections CONJUNCTION self - attention. self - attention USED-FOR aggregation of global information. ViTs USED-FOR input spatial information. intermediate features CONJUNCTION transfer learning. transfer learning CONJUNCTION intermediate features. ( pretraining ) dataset scale USED-FOR transfer learning. ( pretraining ) dataset scale USED-FOR intermediate features. MLP - Mixer HYPONYM-OF architectures. Generic is they. Method are convolutional networks, and classification methods. OtherScientificTerm are visual representations, and features. Task are early aggregation of global information, and spatial localization. ", image classification tasks. This paper proposes to use self-attention and ViT residual connections for early aggregation of global information and spatial localization. Experiments show that the proposed method achieves state-of-the-art performance on ImageNet and CIFAR-10 datasets. ,"This paper presents a theoretical analysis of the internal representation structure of Vision Transformer models (ViTs) and CNNs. The authors show that ViTs are able to learn a uniform representation of the input images, and that the internal representations of CNNs are not uniform. They also show that the self-attention and ViT residual connections are responsible for the early aggregation of global information, and spatial localization. Finally, the authors propose a new dataset scale to train ViTs.  "
9376,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"Thompson sampling ( TS ) USED-FOR bandit area. approximation oracle USED-FOR TS. convergence analysis USED-FOR TS. exact oracle PART-OF CMAB. greedy oracle HYPONYM-OF common ( approximation ) oracle. theoretical guarantees FEATURE-OF common ( approximation ) oracle. TS USED-FOR CMAB problems. problemdependent regret lower bound USED-FOR TS. greedy oracle USED-FOR TS. TS USED-FOR CMAB. approximation oracles USED-FOR TS. Generic are It, and oracle. OtherScientificTerm are optimal solutions, reward gap, and almost matching regret upper bound. Task is combinatorial optimization problems. ","This paper studies Thompson Sampling (TS) in combinatorial bandit problems, where the goal is to maximize the bandit reward while minimizing the reward gap. The authors show that TS is equivalent to a greedy oracle in the original bandit problem, and that the regret of Thompson sampling with greedy oracles is the same as that of the exact oracle. They also show that Thompson sampling can be used as an approximation oracle to approximate the true bandit oracle, and provide a regret lower bound for TS with a problem-dependent regret.   ","This paper studies Thompson sampling (TS) in combinatorial bandits (CMAB) where the goal is to find an optimal bandit solution that maximizes the reward gap between the bandit and the target bandit. In particular, the authors consider the problem of Thompson sampling where the target and target bandits are in the same bandit area. They show that the regret upper bound for Thompson sampling in CMAB is almost matching the regret lower bound for the exact bandit oracle. They also provide a lower bound on the regret of the exact oracle for the Thompson sampling problem. "
9401,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"Federated learning HYPONYM-OF distributed learning paradigm. accuracy rates EVALUATE-FOR federated learning. total error HYPONYM-OF social good properties. hedonic game USED-FOR federated learning. average error rates USED-FOR optimality. algorithm USED-FOR optimal ( error minimizing ) arrangement of players. stability CONJUNCTION optimality. optimality CONJUNCTION stability. stability EVALUATE-FOR arrangement. optimality EVALUATE-FOR arrangement. Method are global model, and game - theoretic approach. OtherScientificTerm are error - minimizing players, federating coalitions, stable coalition partitions, stable arrangements, Price of Anarchy, and constant - factor bound. Generic is stable solutions. ","This paper studies federated learning in a hedonic game, where the goal is to maximize the total error incurred by each player in a federating coalition. The authors show that in this setting, there exists an optimal (error minimizing) arrangement of players that maximizes the average error rates. They show that this optimal arrangement can be found by a game-theoretic algorithm, and they show that it is stable. ","This paper studies the social good properties of federated learning in a hedonic game. The authors consider the problem of federating coalitions, where each player is trying to find an optimal (error-minimizing) arrangement of players. They show that the average error rates of each player can be lower than the total error of the total players, which is a social good property. They also provide a constant-factor bound for the stability of the optimal arrangement. "
9426,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"self - supervised capsule architecture USED-FOR 3D point clouds. capsule decompositions of objects USED-FOR capsule decompositions. permutation - equivariant attention USED-FOR capsule decompositions of objects. these USED-FOR decomposition. capsule invariance / equivariance properties FEATURE-OF decomposition. canonicalization operation USED-FOR object - centric reasoning. classification labels CONJUNCTION manually - aligned training datasets. manually - aligned training datasets CONJUNCTION classification labels. classification labels USED-FOR neural network. manually - aligned training datasets USED-FOR neural network. canonicalization CONJUNCTION unsupervised classification. unsupervised classification CONJUNCTION canonicalization. method COMPARE state - of - the - art. state - of - the - art COMPARE method. 3D point cloud reconstruction CONJUNCTION canonicalization. canonicalization CONJUNCTION 3D point cloud reconstruction. object - centric representation USED-FOR method. canonicalization EVALUATE-FOR method. unsupervised classification EVALUATE-FOR method. 3D point cloud reconstruction EVALUATE-FOR state - of - the - art. 3D point cloud reconstruction EVALUATE-FOR method. self - supervised manner USED-FOR object - centric representation. Generic is process. OtherScientificTerm are attention masks, and semantic keypoints. Method is semantically consistent decomposition. ","This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The capsule architecture is based on capsule decompositions of objects, and the capsule decomposition of objects is a permutation-equivariant attention. The authors show that capsule invariance and equivariance properties can be obtained from capsule invariant attention masks, which can be used for object-centric reasoning. The proposed method achieves state-of-the-art results on 3D Point Cloud reconstruction and unsupervised classification. ","This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The proposed method is based on the idea of permutation-equivariant attention, where the attention is applied to a set of capsule decompositions of objects. The authors show that the capsule decomposition is semantically consistent with respect to the semantic keypoints, and that it is invariant and equivariant to capsule invariance and capsule equivariance properties. They also show that canonicalization operation can be used for object-centric reasoning. The experimental results demonstrate that the proposed method can achieve state-of-the-art performance on 3D 3D Point Cloud reconstruction. "
9451,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,conformal method USED-FOR prediction intervals. prediction intervals USED-FOR nonparametric regression. conformal method USED-FOR nonparametric regression. black - box machine learning algorithms USED-FOR conditional distribution. approximate conditional coverage FEATURE-OF prediction intervals. histograms USED-FOR black - box machine learning algorithms. conditional coverage CONJUNCTION optimal length. optimal length CONJUNCTION conditional coverage. finite samples FEATURE-OF marginal coverage. marginal coverage FEATURE-OF prediction intervals. conformalized quantile regression CONJUNCTION distributional conformal prediction approaches. distributional conformal prediction approaches CONJUNCTION conformalized quantile regression. simulated and real data EVALUATE-FOR state - of - the - art alternatives. distributional conformal prediction approaches HYPONYM-OF state - of - the - art alternatives. conformalized quantile regression HYPONYM-OF state - of - the - art alternatives. Material is skewed data. Method is black - box model. ,This paper proposes a conformal method for non-parametric regression. The main idea is to use the conditional coverage of the conditional distribution to estimate the prediction intervals. The proposed method is based on quantile regression with conformalized quantile estimation. The authors show that the proposed method achieves the best conditional coverage in terms of the marginal coverage and the optimal length of the prediction interval.,"This paper proposes a new conformal method for nonparametric regression. The main idea is to use the conditional coverage of the conditional distribution as a histogram to estimate the prediction intervals of a black-box model. The proposed method is based on the notion of marginal coverage, which is defined as the distance between the prediction interval and the marginal distribution of the true distribution. The authors show that the proposed method can be applied to both simulated and real data, and show that it can achieve state-of-the-art performance."
9476,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"invariance USED-FOR generalisation. incorporating invariance PART-OF kernel ridge regression. effective dimension USED-FOR generalisation. feature averaging USED-FOR invariance. reproducing kernel Hilbert space CONJUNCTION kernel. kernel CONJUNCTION reproducing kernel Hilbert space. Generic is approach. OtherScientificTerm are function space perspective, and group. ","This paper studies the effect of invariance in kernel ridge regression on the generalization performance. The authors show that the effective dimension of the kernel is a function of the invariance of the function space, and that it is invariant to the number of samples in the kernel. They show that this invariance can be used to improve the generalisation performance of the model.   ",This paper studies the generalization properties of kernel ridge regression with invariance. The authors show that the effective dimension of generalization is bounded by the invariance of the function space. They also show that invariance can be improved by using feature averaging.  The authors also provide a theoretical analysis of the effect of invariance on generalization.
9501,SP:97fac361b69ed5871a60dc40e51900747a453df9,"assertion statements USED-FOR erroneous behavior. software programs USED-FOR they. applications EVALUATE-FOR deep learning programs. generative model USED-FOR neural network activations. DecNN HYPONYM-OF Decodable Neural Network. DecNN USED-FOR ensemble - like model. compositionality FEATURE-OF neural networks. uncertainty FEATURE-OF ensemble - like model. out - of - distribution detection CONJUNCTION adversarial example detection. adversarial example detection CONJUNCTION out - of - distribution detection. adversarial example detection CONJUNCTION calibration. calibration CONJUNCTION adversarial example detection. uncertainty USED-FOR out - of - distribution detection. uncertainty USED-FOR adversarial example detection. uncertainty USED-FOR calibration. accuracy EVALUATE-FOR neural networks. DecNN CONJUNCTION pretrained models. pretrained models CONJUNCTION DecNN. protected features USED-FOR neural networks. OtherScientificTerm is program logic. Generic are programs, and design. ","This paper proposes a generative model called DecNN to model the uncertainty of neural network activations in deep learning programs. The proposed DecNN is an ensemble-like model that uses the uncertainty to model compositionality of neural networks. The uncertainty is used for out-of-distribution detection, adversarial example detection, and calibration. Experiments show that DecNN can improve the accuracy of deep learning models trained with adversarial examples. ","This paper proposes a generative model for deep learning programs that can be used for out-of-distribution detection, adversarial example detection, and calibration. The proposed method is based on the DecNN model, which is a Decodable Neural Network (DecNN) that is composed of an ensemble of neural networks. The authors show that the ensemble-like model can be applied to a variety of applications, such as out of distribution detection, calibration, and adversarial examples detection. They also provide a theoretical analysis of the uncertainty of the ensemble."
9526,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"Optimal transport maps USED-FOR machine learning and statistics. probability distributions FEATURE-OF Optimal transport maps. Plugin estimators USED-FOR transport maps. Plugin estimators USED-FOR computational optimal transport. rates of convergences EVALUATE-FOR plug - in estimators. barycentric projections USED-FOR plug - in estimators. stability estimate USED-FOR plug - in estimators. stability estimate USED-FOR barycentric projections. rates of convergence EVALUATE-FOR plug - in estimators. rates of convergence FEATURE-OF Wasserstein barycenter. asymptotic detection thresholds USED-FOR optimaltransport based tests of independence. probability distributions FEATURE-OF Wasserstein barycenter. Generic is maps. OtherScientificTerm are minimal smoothness assumptions, smoothness assumptions, curse of dimensionality, and Wasserstein distance. ",This paper studies the convergence of plug-in estimators for optimal transport maps. The authors propose to use barycentric projections to estimate the stability of the optimal transport map. They show that the stability is a function of the Wasserstein distance between the probability distributions. They also provide asymptotic detection thresholds for testing the independence of optimal transport based tests of independence. ,This paper studies the convergence rates of plug-in estimators for optimal transport maps. The authors show that the rates of convergence of the Wasserstein barycenter of the optimal transport map are asymptotically bounded by the barycentric projection of the probability distributions. They also provide a stability estimate for the stability of barycenters. The stability estimate is based on the stability estimate of the stability estimates of the brycentric projections. The paper also provides a test of independence for the optimal-transport based tests of independence. 
9551,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,training efficiency CONJUNCTION useful feature extraction. useful feature extraction CONJUNCTION training efficiency. useful feature extraction EVALUATE-FOR dataset distillation methods. training efficiency EVALUATE-FOR dataset distillation methods. distributed kernel - based meta - learning framework USED-FOR dataset distillation. infinitely wide convolutional neural networks USED-FOR distributed kernel - based meta - learning framework. infinitely wide convolutional neural networks USED-FOR dataset distillation. test accuracy EVALUATE-FOR CIFAR10 image classification task. Fashion - MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Fashion - MNIST. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Fashion - MNIST CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION Fashion - MNIST. MNIST HYPONYM-OF settings. Fashion - MNIST HYPONYM-OF settings. CIFAR-100 HYPONYM-OF settings. CIFAR-10 HYPONYM-OF settings. they COMPARE naturally occurring data. naturally occurring data COMPARE they. distilled datasets COMPARE naturally occurring data. naturally occurring data COMPARE distilled datasets. Method is machine learning algorithms. ," distillation is a popular method to improve the performance of image classification models. This paper proposes to use a distributed kernel-based meta-learning framework to distill the features from the training set to the test set. The proposed distillation method is evaluated on CIFAR-10, Fashion-MNIST, MNIST, and Cifar-100. ","This paper proposes a meta-learning framework for dataset distillation. The proposed method is based on a distributed kernel-based meta-Learning framework. The authors show that the proposed method outperforms the state-of-the-art distillation methods on CIFAR-10, Fashion-MNIST, Cifar-100, and SVHN datasets. "
9576,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,Semi - supervised learning ( SSL ) USED-FOR model. unlabeled data USED-FOR model. unlabeled data USED-FOR Semi - supervised learning ( SSL ). label space FEATURE-OF labeled and unlabeled data. FixMatch HYPONYM-OF SSL methods. Learning representations of inliers USED-FOR OSSL. FixMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION FixMatch. OpenMatch USED-FOR FixMatch. OpenMatch CONJUNCTION novelty detection. novelty detection CONJUNCTION OpenMatch. threshold USED-FOR outliers. OVA - classifier USED-FOR confidence score. open - set soft - consistency regularization loss USED-FOR outlier detection. smoothness FEATURE-OF OVA - classifier. open - set soft - consistency regularization loss USED-FOR smoothness. OpenMatch COMPARE supervised model. supervised model COMPARE OpenMatch. CIFAR10 EVALUATE-FOR supervised model. Method is SSL algorithms. ,"This paper proposes a semi-supervised learning method that uses unlabeled data to learn representations of inliers. The proposed method is based on OpenMatch, which is an extension of FixMatch. The main contribution of this paper is to introduce an open-set soft-consistency regularization loss to improve the outlier detection. The method is evaluated on CIFAR-10 and ImageNet.","This paper proposes a method for semi-supervised learning with unlabeled data. The main idea is to learn representations of inliers in the label space of labeled data and outlier data, and then use them to improve the performance of the model. The method is based on OpenVA-classifier, which is an open-set soft-consistency regularization loss for outlier detection. Experiments on CIFAR-10 show that the proposed method outperforms the state-of-the-art."
9601,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"it USED-FOR achiever policy. it USED-FOR explorer. explorer CONJUNCTION achiever policy. achiever policy CONJUNCTION explorer. Latent Explorer Achiever ( LEXA ) HYPONYM-OF agent. imagined rollouts USED-FOR it. image inputs USED-FOR world model. imagined rollouts USED-FOR achiever policy. prior methods COMPARE explorer. explorer COMPARE prior methods. LEXA USED-FOR tasks. goal images zero - shot FEATURE-OF tasks. approaches USED-FOR unsupervised goal reaching. LEXA COMPARE approaches. approaches COMPARE LEXA. LEXA USED-FOR unsupervised goal reaching. prior benchmarks CONJUNCTION benchmark. benchmark CONJUNCTION prior benchmarks. test tasks EVALUATE-FOR benchmark. robotic manipulation and locomotion domains FEATURE-OF test tasks. benchmark EVALUATE-FOR approaches. test tasks EVALUATE-FOR LEXA. benchmark EVALUATE-FOR LEXA. prior benchmarks EVALUATE-FOR approaches. prior benchmarks EVALUATE-FOR LEXA. Method is artificial agents. OtherScientificTerm are complex visual environments, supervision, and achiever. ","This paper proposes Latent Explorer Achiever (LEXA), a method for unsupervised goal reaching in robotic manipulation and locomotion tasks. LEXA consists of two components: an explorer and an achiever policy. The explorer is trained using an image-based world model, and the achiever is trained via imagined rollouts. Experiments show that the proposed method outperforms the state-of-the-art methods on a variety of robotic manipulation tasks.","This paper proposes Latent Explorer Achiever (LEXA), a new method for unsupervised goal reaching in robotic manipulation and locomotion tasks. LEXA is based on a world model that is trained to predict the rollouts of an explorer and an achiever policy. The explorer and the achiever are trained using a series of imagined rollouts, and the explorer is used to select the best rollouts from a set of rollouts. Experiments are conducted on a variety of tasks, including robotic manipulation, locomotion, and goal reaching. "
9626,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"trainable parameters USED-FOR Language models. networks USED-FOR tasks. task EVALUATE-FOR networks. parameter sharing CONJUNCTION factorized representations. factorized representations CONJUNCTION parameter sharing. model compression CONJUNCTION parameter sharing. parameter sharing CONJUNCTION model compression. factorized representations CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION factorized representations. reshaped and rearranged original matrix USED-FOR low - rank factorized representation. expressiveness EVALUATE-FOR low - rank layers. embedding CONJUNCTION attention. attention CONJUNCTION embedding. attention CONJUNCTION feed - forward layers. feed - forward layers CONJUNCTION attention. approach USED-FOR Transformer models. OtherScientificTerm are lottery ticket hypothesis, parameter space, self - attention layers, parameter matrix, and architecture of the network. Generic is models. Method is factorized representations of matrices. Task is deep networks. Metric is on - task performance. ",This paper studies the lottery ticket hypothesis in deep neural networks. The authors propose to use low-rank factorized representations of matrices to improve the expressiveness of self-attention layers. The proposed method is based on the fact that the original matrix can be reshaped and rearranged to make the representation more expressive. Experiments show that the proposed method improves the performance of the model on a variety of tasks. ,"This paper studies the lottery ticket hypothesis, which claims that low-rank factorized representations of matrices (i.e., the original matrix) are more expressive than high-rank ones. The authors propose a method to reshape and rearrange the original matrices of the input matrix, and then use the reshaped matrix as a factorized representation. They show that this method can improve the expressiveness of the low-ranked representations. They also show that their method can be applied to Transformer models.  "
9651,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"structured source code representations USED-FOR models. syntax trees HYPONYM-OF structured source code representations. them PART-OF attention module of Transformer. path encoding methods PART-OF attention module of Transformer. path encoding PART-OF them. them PART-OF unified Transformer framework. TPTrans COMPARE baselines. baselines COMPARE TPTrans. code summarization EVALUATE-FOR approaches. Task is Learning distributed representation of source code. Method is positional encoding. OtherScientificTerm are pairwise path, tree root, and syntax tree. Generic is paths. ","This paper proposes a new Transformer-based architecture for code summarization. The proposed architecture is based on the idea of path encoding, which is an extension of the Transformer architecture. The main idea is to add a path encoding module to the attention module of Transformer. The authors show that the proposed architecture can be combined with existing Transformer architectures and achieve state-of-the-art results.   ","This paper proposes a unified Transformer-based approach to learn structured source code representations. The authors propose a new Transformer module, called Transformer Path Encoding (TPTrans), that learns a positional encoding of the source code representation. The positional encoding is based on the syntax tree representation of a pairwise path, where each pair of paths is represented as a pair of nodes in a syntax tree, and each node is represented by a tree root. The proposed method is evaluated on a variety of tasks, including code summarization, summarization of source code, and code-to-code translation."
9676,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"them USED-FOR high - resolution image generation. Attention - based models USED-FOR long range dependency. Transformer HYPONYM-OF Attention - based models. Generative Adversarial Networks ( GANs ) USED-FOR high - resolution image generation. multi - axis blocked self - attention USED-FOR mixing of local and global attention. global self - attention COMPARE multi - axis blocked self - attention. multi - axis blocked self - attention COMPARE global self - attention. implicit neural function FEATURE-OF multi - layer perceptrons. cross - attention USED-FOR self - modulation component. model USED-FOR synthesizing high definition images. linear computational complexity EVALUATE-FOR model. unconditional ImageNet CONJUNCTION FFHQ 256 × 256. FFHQ 256 × 256 CONJUNCTION unconditional ImageNet. FFHQ 256 × 256 EVALUATE-FOR HiT. FID scores EVALUATE-FOR HiT. unconditional ImageNet EVALUATE-FOR HiT. OtherScientificTerm are quadratic complexity of self - attention operation, low - resolution stages of the generative process, self - attention, image size, and convolutions. Method are generative process, and GANs. Task is high - resolution stages. ",This paper proposes a new method for high-resolution image generation based on self-attention in GANs. The proposed method is based on the idea of mixing local and global attention in the self-modulation component. The authors show that the proposed method achieves better FID scores on ImageNet and FFHQ compared to previous methods. ,"This paper proposes a multi-axis blocked self-attention for high-resolution image generation in GANs. The proposed method is based on the idea of cross-modulation, which allows for mixing of local and global attention. The authors show that the proposed method has linear computational complexity with respect to the number of convolutions and the size of the image. They also show that their method is able to synthesize high-res images in the low-resolution stages of the generative process."
9701,SP:41a6753bc56eb16040600666a859294ae36cfa9c,"query complexity EVALUATE-FOR learning geodesically convex halfspaces on graphs. Geodesic convexity HYPONYM-OF Euclidean convexity. treewidth CONJUNCTION minimum hull set size. minimum hull set size CONJUNCTION treewidth. query complexity CONJUNCTION VC dimension. VC dimension CONJUNCTION query complexity. query complexity EVALUATE-FOR Radon number. cut size FEATURE-OF labelling. approach COMPARE active learning algorithms. active learning algorithms COMPARE approach. ground - truth communities PART-OF real - world graphs. OtherScientificTerm are convex sets, diameter, and separation axioms. Material is unlabelled graph. ","This paper studies the query complexity of learning geodesically convex halfspaces on graphs with geodesic convexity. In particular, the authors show that for any convex set of size $\mathcal{O}(\sqrt{T})$ and radius $\Omega(T)$, the query cost is bounded by the Radon number of the half-spaces. The authors also show that if the diameter of the set is larger than $O(T^2)$, then the query costs are bounded by a Radon factor of $O(\log T)$.    The main contribution of the paper is to show that under certain assumptions on the geometry of the convex sets, the proposed method can learn a geodesical half-space with a query cost of $\log T$ and a query complexity that is logarithmic in the number of queries.  ","This paper studies the problem of learning geodesically convex halfspaces on graphs with geodesic convexity, i.e., a set of vertices that are convex in the Euclidean plane. The authors show that the query complexity of learning Geodesically Convex Halfspaces (GC) is bounded by the treewidth, the minimum hull set size, and the radius of the cut size. They show that this is a good trade-off between query complexity and the VC dimension. They also show that their method can be used to learn Geodesic HalfSpaces on unlabelled graphs. "
9726,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"action localization dataset EVALUATE-FOR TAL head. large action classification dataset EVALUATE-FOR video encoder. transfer learning pipeline USED-FOR temporal action localization ( TAL ) methods. video encoder USED-FOR action classification. video encoder USED-FOR task discrepancy problem. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. TAL head USED-FOR joint optimization. video encoder USED-FOR joint optimization. this USED-FOR TAL. video encoder CONJUNCTION TAL head. TAL head CONJUNCTION video encoder. temporal, spatial or spatio - temporal resolution FEATURE-OF mini - batch composition. LoFi optimization approach USED-FOR TAL methods. ResNet18 based video encoder USED-FOR method. single RGB stream FEATURE-OF ResNet18 based video encoder. OtherScientificTerm are encoder, GPU memory constraints, mid - range hardware budget, gradients, and TAL supervision loss. Method are TAL learning, and feature representations. ","This paper proposes a video encoder-based temporal action localization (TAL) method. The proposed method is based on a video-to-video transfer learning pipeline, where the video is used to learn a TAL head, which is a combination of a temporal encoder and a spatio-temporal encoder. The video is then used to train an action classification model. The main contribution of the paper is to propose a new LoFi optimization approach for TAL methods. The method is evaluated on a large action localization dataset and achieves state-of-the-art performance. ",This paper proposes a transfer learning pipeline for temporal action localization (TAL) methods. The key idea is to combine the video encoder and the TAL head in a mini-batch composition. The proposed method is based on the LoFi optimization approach. The method is evaluated on a large action classification dataset. 
9751,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"Gaussian design matrix CONJUNCTION arbitrary 2 noise distribution. arbitrary 2 noise distribution CONJUNCTION Gaussian design matrix. Gaussian design matrix FEATURE-OF convex penalty in linear models. arbitrary 2 noise distribution FEATURE-OF convex penalty in linear models. gradient - Lipschitz loss function USED-FOR M - estimators. Huber loss CONJUNCTION Elastic - Net penalty. Elastic - Net penalty CONJUNCTION Huber loss. heavy - tails FEATURE-OF noise distribution. Elastic - Net penalty USED-FOR robust M - estimator. Huber loss USED-FOR robust M - estimator. differentiability structure FEATURE-OF convex regularized M - estimators. adaptive criterion USED-FOR regularized M - estimators. criterion USED-FOR out - of - sample error. noise distribution CONJUNCTION covariance of the design. covariance of the design CONJUNCTION noise distribution. criterion USED-FOR out - of - sample error. OtherScientificTerm are differentiation, intermediate high - dimensional 9 regime, dimension, distribution of the residuals, and out - of - sample 14 error. Generic is derivatives. Material is Simulated data. Method is M - estimator. ",This paper studies the convergence of M-estimators in linear models with Gaussian design matrix and arbitrary 2-neighboring noise distribution. The main contribution is to show that the convergence rate of the estimator is bounded by the gradient-Lipschitz loss function with respect to the gradients of the residuals. The paper also provides an adaptive criterion for estimating the out-of-sample error.   ,"This paper studies the convergence of M-estimators in the intermediate high-dimensional 9-dimensional regime, where the dimension of the residuals is the same as that of the data. The authors provide an adaptive criterion for the out-of-sample error of the regularized M- estimators. The criterion is based on the Huber loss, Elastic-Net penalty, and heavy-tailed noise distribution. The paper also provides a differentiability analysis of the differentiability structure of the M-Estimators."
9776,SP:be53bc4c064402489b644332ad9c17743502d73c,"calibrated beam - based algorithm USED-FOR neural abstractive summarization. calibrated beam - based algorithm USED-FOR local optimality problem. beam search USED-FOR local optimality problem. global attention distribution FEATURE-OF calibrated beam - based algorithm. attention distribution USED-FOR global protocol. global scoring mechanism USED-FOR beam search. global scoring mechanism USED-FOR beam search. global ( attention)-aware inference COMPARE summarization models. summarization models COMPARE global ( attention)-aware inference. empirical hyper - parameters USED-FOR summarization models. empirical hyper - parameters USED-FOR global ( attention)-aware inference. Generic are design, and algorithm. Task is inference. OtherScientificTerm is corrupted attention distributions. ","This paper proposes a beam-based algorithm for neural abstractive summarization. The proposed method is based on the attention-aware inference, where the attention distribution is computed using a global scoring mechanism. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of accuracy and computational complexity.  ","This paper proposes a new algorithm for neural abstractive summarization. The proposed method is based on a calibrated beam-based algorithm, where the goal is to find a global attention distribution that maximizes the global optimality of the global protocol. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of local optimality. The main contribution of the paper is that it proposes a global scoring mechanism for beam search, which can be used to improve the performance of summarization models. "
9801,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"Attention mechanism USED-FOR deep learning models. relative position encoding PART-OF deep learning models. canonical local coordinate system USED-FOR neighborhoods. attention USED-FOR manifolds. method USED-FOR feature vectors. regular field of cyclic groups USED-FOR feature fields. regular field of cyclic groups USED-FOR intermediate layers. feature fields USED-FOR intermediate layers. feature vectors USED-FOR fields. method USED-FOR expressive ability. regular field of cyclic groups USED-FOR expressive ability. position vector USED-FOR orientation of the coordinate system. ambient space FEATURE-OF orientation of the coordinate system. local coordinate system USED-FOR position vector. global coordinate system HYPONYM-OF orientation of the coordinate system. gauge equivariance USED-FOR self - attention. triangle meshes USED-FOR Gauge Equivariant Transformer ( GET ). common recognition tasks EVALUATE-FOR GET. Method are equivariant transformer, and multi - head selfattention. OtherScientificTerm are orientation of local coordinate systems, gauge equivariant, position - based and content - based information, and rotation invariance. ","This paper proposes a method for self-attention in deep neural networks based on equivariant transformers. The main idea is to use a regular field of cyclic groups to represent the feature vectors in the intermediate layers of the network. The authors show that the proposed method is able to capture the position-based and content-based information, and is rotation invariant. Experiments on image classification and object detection tasks demonstrate the effectiveness of the method.","This paper proposes a new method for self-attention in deep learning models. The key idea is to use a cyclic group of cyclic groups to represent the position vectors of the feature fields in the intermediate layers of a deep learning model. This is done by using the local coordinate system and the global coordinate system of the ambient space to encode the position of the coordinate system. The local coordinate is represented by a position vector, and the ambient coordinate system is represented as a triangle mesh. The triangle mesh is then used as the input to a gauge equivariant transformer (Gauge Equivariant Transformer). The authors show that their method can be applied to a variety of tasks, including image recognition, image classification, and object recognition."
9826,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"methods USED-FOR unsupervised learning of finite mixture models. expectation maximization CONJUNCTION Metropolis - Hastings algorithm. Metropolis - Hastings algorithm CONJUNCTION expectation maximization. Metropolis - Hastings algorithm USED-FOR approach. expectation maximization PART-OF approach. it USED-FOR shallow and deep mixture 8 models. mixtures of normalizing flows CONJUNCTION sum - product ( transform ) networks. sum - product ( transform ) networks CONJUNCTION mixtures of normalizing flows. synthetic and real - data 10 contexts EVALUATE-FOR deep models. sum - product ( transform ) networks HYPONYM-OF deep models. mixtures of normalizing flows HYPONYM-OF deep models. synthetic and real - data 10 contexts EVALUATE-FOR method. Method is finite mixture models. OtherScientificTerm are mixture, and complex, and possibly nonlinear, transformations. Metric is computational cost. ",This paper proposes a method for unsupervised learning of finite mixture models. The method is based on expectation maximization and Metropolis-Hastings algorithm. The main idea is to learn a mixture of normalizing flows and sum-product (transformer) networks. The authors show that the method is computationally efficient and achieves good performance on both synthetic and real-data settings. ,"This paper proposes a new method for unsupervised learning of finite mixture models. The main idea is to use the Metropolis-Hastings algorithm to learn a mixture 8 model, where the mixture is a mixture of mixtures of normalizing flows and a sum-product (transformer) network. The method is based on the expectation maximization and Metropolis Hastings algorithm. The authors demonstrate the effectiveness of the method on synthetic and real-data datasets. "
9851,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"Sparse training USED-FOR deep neural networks. dense computation USED-FOR backward propagation step. sparse forward and backward passes USED-FOR sparse training method. global sparsity constraint FEATURE-OF continuous minimization problem. continuous minimization problem USED-FOR training process. weight update CONJUNCTION structure parameter update. structure parameter update CONJUNCTION weight update. structure parameter update HYPONYM-OF steps. weight update HYPONYM-OF steps. chain rule USED-FOR step. sparse structure USED-FOR chain rule. variance reduced policy gradient estimator USED-FOR sparse training. forward passes CONJUNCTION backward propagation. backward propagation CONJUNCTION forward passes. chain rule based gradient estimators USED-FOR variance reduced policy gradient estimator. variance reduced policy gradient estimator USED-FOR step. chain rule based gradient estimators USED-FOR step. forward passes USED-FOR variance reduced policy gradient estimator. algorithm USED-FOR training process. real - world datasets EVALUATE-FOR algorithm. OtherScientificTerm is memory usage. Method are neural networks, and gradient estimator. Generic is methods. Task is optimization process. ","This paper proposes a method for sparse training of deep neural networks with dense forward and backward passes. The forward pass is based on a dense computation for the backward propagation step, while the backward pass is a sparse update of the weights and parameters of the network with a global sparsity constraint. The authors propose to use a variance-reduced policy gradient estimator for the forward pass and a chain rule based gradient estimators for backward propagation. The proposed method is shown to be computationally efficient in terms of memory usage.   ",This paper proposes a new method for sparse training for neural networks. The main idea is to use a global sparsity constraint for the forward and backward propagation steps of the training process. The forward passes are sparse and the backward propagation step is sparse. The authors propose a variance-reduced policy gradient estimator for each forward pass. They show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and memory usage.
9876,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"importance samplers ( IS ) CONJUNCTION Markov chain Monte Carlo ( MCMC ) samplers. Markov chain Monte Carlo ( MCMC ) samplers CONJUNCTION importance samplers ( IS ). iterated sampling - importance resampling mechanism USED-FOR π. NEO - IS CONJUNCTION iterated sampling - importance resampling mechanism. iterated sampling - importance resampling mechanism CONJUNCTION NEO - IS. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. NEO - MCMC USED-FOR π. NEO - IS USED-FOR NEO - MCMC. iterated sampling - importance resampling mechanism USED-FOR NEO - MCMC. NEO - MCMC USED-FOR multimodal targets. T USED-FOR conformal Hamiltonian system. NEO - IS COMPARE NEO - MCMC. NEO - MCMC COMPARE NEO - IS. discrete - time integrator USED-FOR conformal Hamiltonian system. T USED-FOR NEO - IS. T USED-FOR discrete - time integrator. NEO - MCMC USED-FOR explicit mixing time estimates. OtherScientificTerm are complex distribution π, intractable normalizing constant, invertible map T, forward and backward Orbits, proposal distribution ρ, map T, NEO, Non - Equilibrium Orbits, and normalizing constant. Generic are schemes, and methods. ","This paper proposes a novel method for sampling from the distribution of the Hamiltonian of a Hamiltonian with non-equilibrium orbits. The proposed method is based on an iterated sampling-importance sampling (IS) scheme and a Markov chain Monte Carlo (MCMC) scheme. The main contribution of the paper is to show that the proposed method, called NEO-IS, is equivalent to the state-of-the-art methods in terms of the mixing time.   ","This paper proposes a novel method to sample from a Hamiltonian Hamiltonian system (i.e., the Hamiltonian of the proposal distribution ρ) and use it as a Markov chain Monte Carlo (MCMC) sampler. The main contribution of the paper is to propose a new method, called NEO-IS, which is an iterated sampling-importance resampling (IS) method for the proposed Hamiltonian. The authors show that the proposed method outperforms the state-of-the-art in terms of the mixing time of the proposed scheme.   "
9901,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,training CONJUNCTION inference. inference CONJUNCTION training. permutation invariance CONJUNCTION equivariance. equivariance CONJUNCTION permutation invariance. permutation invariance FEATURE-OF set - function constraints. equivariance FEATURE-OF set - function constraints. property USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) USED-FOR large scale mini - batch set encoding. Mini - Batch Consistency ( MBC ) HYPONYM-OF property. attention - based set encoding mechanism USED-FOR set representations. mini - batch processing of sets USED-FOR attention - based set encoding mechanism. symmetries of invariance CONJUNCTION equivariance. equivariance CONJUNCTION symmetries of invariance. MBC USED-FOR method. symmetries of invariance FEATURE-OF method. method USED-FOR rich set encoding representations. rich set encoding representations USED-FOR set - structured data. Method is set encoding algorithms. OtherScientificTerm is computational and memory resources. Generic is assumptions. Task is large - scale set encoding. ,"This paper proposes Mini-Batch Consistency (MBC) which is a new property of set encoding based on permutation invariance and equivariance to set-function constraints. The authors show that MBC can be used for mini-batch set encoding, which is an efficient way to encode large-scale set data. The proposed method is based on the attention-based set encoding mechanism, which uses mini-batches of sets to encode the set representations.  ","This paper proposes a method for large-scale mini-batch set encoding based on the Mini-Batch Consistency (MBC) property. The authors show that the MBC property is invariant to permutation invariance, equivariance, and set-function constraints. They also show that MBC can be used to improve the performance of the set encoding algorithms. "
9926,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"Diplomacy HYPONYM-OF game. human data USED-FOR policy. Diplomacy CONJUNCTION StarCraft. StarCraft CONJUNCTION Diplomacy. StarCraft CONJUNCTION Dota. Dota CONJUNCTION StarCraft. branching factors FEATURE-OF games. Diplomacy HYPONYM-OF branching factors. StarCraft HYPONYM-OF branching factors. Dota HYPONYM-OF games. Diplomacy HYPONYM-OF games. StarCraft HYPONYM-OF games. action exploration CONJUNCTION equilibrium approximation. equilibrium approximation CONJUNCTION action exploration. algorithm USED-FOR action exploration. algorithm USED-FOR equilibrium approximation. algorithm USED-FOR policy proposal network. value iteration USED-FOR algorithm. policy USED-FOR model training. equilibrium search procedure USED-FOR policy. equilibrium search procedure USED-FOR model training. algorithm USED-FOR agent. DORA USED-FOR two - player variant of Diplomacy. agent USED-FOR two - player variant of Diplomacy. DORA HYPONYM-OF agent. methods USED-FOR full - scale no - press Diplomacy. human data USED-FOR agent. agent COMPARE human - data bootstrapped agents. human - data bootstrapped agents COMPARE agent. self play USED-FOR superhuman performance. multiple equilibria FEATURE-OF Diplomacy. Diplomacy FEATURE-OF superhuman performance. Task is complex games. Method are handcrafted reward shaping, and double oracle step. OtherScientificTerm are combinatorial action spaces, and policy proposals. Generic is it. ","This paper studies the problem of learning to play Diplomacy in the presence of multiple equilibria in a combinatorial action space. The authors propose a method to learn a policy proposal network that is able to explore the action space and find the optimal policy. The proposed method is based on a two-step process: first, the policy is learned by a value iteration, and then it is used to find the best policy from a set of policy proposals. The method is shown to outperform the state-of-the-art methods in a variety of games, including Dota and StarCraft. ","This paper proposes a new method for learning a policy proposal network for no-press Diplomacy. The proposed method, DORA, is based on a two-step process: first, it proposes a policy for each player in a combinatorial action space, and then it uses a value iteration algorithm to find the optimal policy. Then, it uses the proposed policy to explore the action space and train a model to find a policy that maximizes the optimal action space. Finally, the proposed method is tested on two-player Diplomacy, where it is shown to outperform human-data bootstrapped agents. "
9951,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,it USED-FOR sequence modeling. attention heads PART-OF Multi - head attention. positive transfer CONJUNCTION negative interference. negative interference CONJUNCTION positive transfer. Multilingual and multi - domain learning USED-FOR sequence modeling. generalization EVALUATE-FOR non - selective attention sharing. attention sharing strategies USED-FOR multilingual and multi - domain sequence modeling. approach USED-FOR shared and specialized attention heads. attention sharing strategies USED-FOR sequence models. tasks EVALUATE-FOR attention sharing strategies. tasks EVALUATE-FOR sequence models. speech recognition HYPONYM-OF tasks. multi - head attention USED-FOR sequence models. BLEU EVALUATE-FOR multi - domain setting. BLEU EVALUATE-FOR speech - to - text translation. approach USED-FOR speech - to - text translation. multilingual setting EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. BLEU EVALUATE-FOR approach. ,"This paper proposes a multi-head attention method for sequence modeling in multi-domain learning. The proposed method is based on the observation that attention heads can be divided into shared and specialized attention heads, which can be used for both positive transfer and negative interference. The method is evaluated on speech-to-text translation and BLEU tasks.   ","This paper proposes a multi-head attention sharing strategy for multi-domain sequence modeling. The main idea is to share attention heads among different attention heads in the same sequence. The proposed approach is evaluated on a variety of tasks, including speech recognition, speech-to-text translation, and multi- domain sequence modeling, where it outperforms the state-of-the-art in terms of performance."
9976,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,covariate shift HYPONYM-OF distribution shift. covariate shift FEATURE-OF real - world applications. high - dimensional asymptotics FEATURE-OF random feature regression. limiting test error CONJUNCTION bias. bias CONJUNCTION limiting test error. covariate shift USED-FOR random feature regression. robustness EVALUATE-FOR overparameterized models. Method is machine learning models. OtherScientificTerm is conditional label distributions. Task is machine learning. ,"This paper studies the effect of covariate shift in random feature regression on the robustness of overparameterized models in the presence of distribution shift. The authors show that under some assumptions on the distribution shift, the test error of the model is bounded by the limiting test error and the bias of the data distribution. They show that this is the case for overparametrized models. They also show that in the case of over-parametrization, this is not the case.   ","This paper studies the effect of covariate shift on the robustness of overparameterized models in the context of random feature regression. The authors provide a theoretical analysis of the asymptotic properties of the variance of the data distribution. They show that the variance is bounded by a limiting test error and a bias. They also show that under certain conditions, the variance can be reduced to zero under the assumption that the covariate distribution is covariate-shifted.  "
10001,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"Thompson sampling CONJUNCTION Bayesian sequential decision - making algorithms. Bayesian sequential decision - making algorithms CONJUNCTION Thompson sampling. Bayesian sequential decision - making algorithms USED-FOR explore / exploit trade - offs. Thompson sampling USED-FOR explore / exploit trade - offs. explore / exploit trade - offs FEATURE-OF ( contextual ) bandits. prior USED-FOR algorithms. expected reward EVALUATE-FOR Thompson sampling ( TS ). misspecified prior USED-FOR Thompson sampling ( TS ). well - specified prior USED-FOR TS. parametric form FEATURE-OF prior. universal constants FEATURE-OF it. bounded support FEATURE-OF priors. algorithms USED-FOR Bayesian meta - learning setting. generic PAC guarantees USED-FOR algorithms. Bayesian POMDPs HYPONYM-OF Bayesian decision - making setting. knowledge gradient algorithm ( KG ) HYPONYM-OF Bayesian decision - making algorithms. multi - armed and contextual bandits USED-FOR meta - learning. structured and correlated priors FEATURE-OF multi - armed and contextual bandits. structured and correlated priors USED-FOR meta - learning. OtherScientificTerm are domain knowledge, misspecification, total - variation distance, learning horizon, cardinality or structure of the action space, sensitivity analysis, and prior misspecification. Generic is bound. Method are contextual bandits, and KG ). ","This paper studies Bayesian meta-learning in the context of Thompson sampling and contextual bandits. In this setting, the authors show that Thompson sampling is equivalent to Bayesian POMDPs with a misspecified prior, and show that the expected reward for Thompson sampling (TS) can be approximated by a well-specified prior with a parametric form with universal constants. The authors also show that Bayesian knowledge gradient algorithms (KG) and Bayesian contextual bandits (BCB) are Bayesian decision-making algorithms with bounded support.  ","This paper studies the generalization of Thompson sampling (TS) to multi-armed and contextual bandits in the context of Bayesian meta-learning. The authors provide a general bound on the total variation distance between the learned priors and the true priors, and show that it is bounded by a parametric form of the prior. They also provide a bound on bounded support of the priors. The paper also provides a generalization bound for Bayesian POMDPs. "
10026,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"PAC - learning model CONJUNCTION Equivalence - Query - learning model. Equivalence - Query - learning model CONJUNCTION PAC - learning model. sample / query complexity EVALUATE-FOR PAC - learning model. exponential separation FEATURE-OF sample / query complexity. adversarial training COMPARE training. training COMPARE adversarial training. adversarial training USED-FOR generalization. on - manifold adversarial examples USED-FOR adversarial training. Method are PAC model, Equivalence - Query model, adversarial model, and Equivalance - Query model. OtherScientificTerm are teacher, learner, PAC bound, adversarial examples, norm constraint, and adversary. Metric are adversarial robustness, and robustness. Generic is model. ",This paper studies the adversarial robustness of PAC-equivalence-queries (PAC-Q) and adversarial-PAC-learning models. The authors show that adversarial training with on-manifold adversarial examples improves generalization performance compared to training with only adversarial samples. They also show that the PAC-learning model with adversarial perturbations has an exponential separation from the Equivalent-Query model in terms of sample/query complexity.  ,This paper studies the sample complexity of PAC-learning and Equivalence-Query-learning (EQ) models. The authors show that the sample/queries complexity of a PAC-trained model is exponential in terms of the number of adversarial examples. They also show that adversarial training with on-manifold examples can improve the generalization performance of the PAC model. 
10051,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"models USED-FOR transfer learning. benchmarks EVALUATE-FOR task. techniques USED-FOR algorithms. methods USED-FOR PARC. PARC COMPARE methods. methods COMPARE PARC. methods USED-FOR diverse model selection. diverse model selection EVALUATE-FOR PARC. model selection USED-FOR transfer learning. Method is pretrained deep learning models. Material is large model banks. OtherScientificTerm is diversity of off - the - shelf models. Generic are model, and setting. Task is Scalable Diverse Model Selection. ","This paper studies the problem of diverse model selection in transfer learning, where the goal is to select a diverse set of off-the-shelf models from a large model bank for a given task. The authors propose a new task called Scalable Diverse Model Selection (PARC), which aims to select models that are diverse enough to be transferable across different tasks. The main contribution of the paper is to propose a novel algorithm for model selection, which is based on the idea that diverse models can be more transferable than one-hot models. The proposed method is evaluated on a variety of transfer learning benchmarks and compared with a number of baselines.","This paper proposes a new task called Scalable Diverse model selection (PARC), which aims to improve the diversity of off-the-shelf models in a transfer learning setting. The authors propose a new algorithm for diverse model selection, which they call PARC, and show that it outperforms existing methods in terms of transfer learning performance on a variety of benchmarks. They also show that PARC outperforms the state-of-the art on a number of datasets. "
10076,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"low - dimensional binary codes USED-FOR compression of high - dimensional neural representations. large bit - codes USED-FOR compression of high - dimensional neural representations. method USED-FOR Low - dimensional binary Codes ( LLC ). method USED-FOR low - dimensional binary codes. annotated attributes CONJUNCTION label meta - data. label meta - data CONJUNCTION annotated attributes. label meta - data HYPONYM-OF side - information. annotated attributes HYPONYM-OF side - information. it USED-FOR image retrieval. it USED-FOR codes. binary codes COMPARE 10 dimensional real representations. 10 dimensional real representations COMPARE binary codes. binary codes COMPARE HashNet. HashNet COMPARE binary codes. ImageNet-100 retrieval problem EVALUATE-FOR binary codes. Material is ImageNet-1 K. Metric is classification accuracy. Method is ResNet50. Task is OOD detection. Generic are baseline, and Code. OtherScientificTerm is threshold. ","This paper proposes a method for learning low-dimensional binary codes for image retrieval. The proposed method is based on the idea that binary codes can be used to compress high-dimensional neural representations. The method is evaluated on ImageNet-1K and ImageNet100, where it is shown to outperform HashNet in terms of classification accuracy.",This paper proposes a low-dimensional binary code (LLC) compression method for image retrieval. The proposed method is based on the idea of low dimensional binary codes (LDBC) which is used to compress high-dimensional neural representations into binary codes. The authors show that LDBC can be used to improve the performance of ImageNet-1K and ResNet-50 on the OOD detection task. They also show that the proposed method outperforms HashNet on the retrieval task.
10101,SP:07def8c80d05f86402ce769313480b30cd99af43,"computational / storage costs EVALUATE-FOR convolutional neural networks ( CNNs ). model compression techniques CONJUNCTION adversarial training. adversarial training CONJUNCTION model compression techniques. adversarial perturbations FEATURE-OF robustness. throughput ( frames - per - second ) EVALUATE-FOR methods. GDWS USED-FOR pre - trained network. throughput EVALUATE-FOR pre - trained network. real - life hardware FEATURE-OF pre - trained network. robustness EVALUATE-FOR GDWS. throughput EVALUATE-FOR GDWS. pre - trained models USED-FOR it. algorithms USED-FOR GDWS convolutions. 2D convolution approximator USED-FOR GDWS. complexity and error constraints USED-FOR algorithms. ImageNet datasets EVALUATE-FOR GDWS. CIFAR-10 EVALUATE-FOR GDWS. Task is robust model compression. Method are Generalized Depthwise - Separable ( GDWS ) convolution, and 2D convolution. ","This paper proposes a generalized depthwise-separable convolution (GDWS) convolution for robust model compression. The proposed method is based on a 2D convolution approximator, which can be used to reduce the complexity and error constraints of the original convolution. Theoretical and empirical results show that the proposed method outperforms existing methods on ImageNet and CIFAR-10. ","This paper proposes a new method for robust model compression, called Generalized Depthwise-Separable (GDWS) convolution. GDWS is a generalization of depthwise-separable convolution (GDW) that can be applied to any pre-trained network. The main idea is to use a 2D convolution approximator to approximate the depth of the input image. The authors show that GDWS can be used to improve the robustness of a pre-training network against adversarial perturbations. The proposed method is evaluated on CIFAR-10 and ImageNet datasets."
10126,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"Retrosynthesis prediction HYPONYM-OF organic synthesis. neural models USED-FOR task. model USED-FOR graph edits. model USED-FOR synthons. top-1 accuracy EVALUATE-FOR model. OtherScientificTerm are precursor molecules, graph topology, and chemical reaction. Method are model design, graph - based approach, and manual correction. Generic is architecture. ","This paper proposes a graph-based approach for predicting the chemical reactions that will lead to the synthesis of a given compound. The proposed method is based on the observation that the graph topology of a compound can be manipulated during the synthesis process, which can lead to incorrect predictions. To address this issue, the authors propose to use a graph editing method to edit the graph structure of the compound, which is then used as input to a neural network to predict the chemical reaction. The authors show that the proposed method outperforms the state-of-the-art methods in predicting chemical reactions.  ","This paper proposes a graph-based method for predicting the topology of a graph, which is used to predict the chemical reaction that will occur in a given graph. The method is based on the idea that the graph topology can be used as an input to a neural network to predict a chemical reaction. The authors show that the proposed method can be applied to the task of predicting the reaction of a synthetic molecule. The proposed method is evaluated on synthetic and real-world datasets. "
10151,SP:772277d969c95924755113c86663fb0e009f24cc,"Bayesian formulation of deconditioning USED-FOR reproducing kernel Hilbert space formulation. deconditioning USED-FOR downscaling setup. conditional mean embedding estimator USED-FOR multiresolution data. solution USED-FOR deconditioning problem. posterior USED-FOR deconditioning problem. posterior USED-FOR latent field. posterior USED-FOR solution. minimax optimal convergence rate FEATURE-OF it. its EVALUATE-FOR methods. OtherScientificTerm are high - resolution ( HR ) information, LR samples, mediating variable, conditional expectation, conditional expectations, and inter - domain features. Task are statistical downscaling, and recovery of the underlying fine - grained field. Material is spatial datasets. ","This paper studies the problem of statistical downscaling in the presence of high-resolution (HR) information. The authors propose a Bayesian formulation of deconditioning in the context of the reproducing kernel Hilbert space formulation. The proposed method is based on a conditional mean embedding estimator for multiresolution data, which is used to recover the underlying fine-grained field.   The authors show that the proposed method has a minimax optimal convergence rate of $O(1/\sqrt{T})$ with respect to the posterior of the latent field. They also show that their method is able to recover inter-domain features in spatial datasets.",This paper proposes a Bayesian formulation of deconditioning for reproducing kernel-Hilbert space formulation. The main idea is to use a conditional mean embedding estimator for multiresolution data to recover the underlying fine-grained field. The authors show that the proposed method converges to a minimax optimal convergence rate. The paper also provides a theoretical analysis of the convergence rate of the proposed approach.
10176,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"Deep sparse networks ( DSNs ) USED-FOR high - order feature interactions. highsparsity features FEATURE-OF prediction task. prediction task EVALUATE-FOR Deep sparse networks ( DSNs ). computation efficiency EVALUATE-FOR models. feature - interaction layer PART-OF DSNs. neural architecture search USED-FOR problem. distilled search space USED-FOR architectures. progressive search algorithm USED-FOR sparse prediction tasks. progressive search algorithm USED-FOR order - priority property. order - priority property FEATURE-OF sparse prediction tasks. Task is model inference. Material is real - world benchmark datasets. Metric are accuracy, and efficiency. Method is search algorithm. ","-based deep neural networks (DNNs) have been shown to achieve state-of-the-art performance on a wide range of prediction tasks. However, they are computationally expensive to compute due to the large number of high-order features. This paper proposes to use a feature-interaction layer in DSNs to reduce the computational cost of inference. The proposed method is based on the observation that the order-prioritization property of DNNs is not satisfied in sparse prediction tasks, and proposes a progressive search algorithm to address this issue.  ","This paper proposes a new search algorithm for deep sparse networks (DSNs) for sparse prediction tasks with high-order feature interactions. The main idea is to use a distilled search space for DSNs to find the best architecture for the sparse prediction task. The search space is defined as a set of DNNs that are trained to predict the high-sparsity features of the prediction task, and the goal is to find a DSN architecture with the best performance on the prediction tasks. The authors propose a progressive search algorithm to find an architecture that is best suited for the task of sparse prediction. They show that the search space can be partitioned into two parts: (1) a search space that is partitioned by the order of the feature-interaction layer, and (2) an order-prioritized search space. They also show that their algorithm can find the optimal search space in terms of the order-priority property. "
10201,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,algorithm USED-FOR transfer learning. pre - trained model USED-FOR task. fine - tuning HYPONYM-OF algorithm. labeled data USED-FOR pre - trained model. labeled data USED-FOR task. fine - tuning USED-FOR overfitting. noise FEATURE-OF robustness. generalization properties EVALUATE-FOR fine - tuning. noise stability FEATURE-OF fine - tuned model. self label - correction CONJUNCTION label - reweighting. label - reweighting CONJUNCTION self label - correction. layer - wise regularization CONJUNCTION self label - correction. self label - correction CONJUNCTION layer - wise regularization. interpolation between regularization and self - labeling methods PART-OF regularized self - labeling. layer - wise regularization PART-OF interpolation between regularization and self - labeling methods. self label - correction PART-OF regularized self - labeling. layer - wise regularization PART-OF regularized self - labeling. pre - trained model architectures USED-FOR image and text data sets. image and text data sets EVALUATE-FOR approach. pre - trained model architectures USED-FOR approach. image classification tasks CONJUNCTION few - shot classification task. few - shot classification task CONJUNCTION image classification tasks. approach COMPARE baseline methods. baseline methods COMPARE approach. few - shot classification task EVALUATE-FOR approach. image classification tasks EVALUATE-FOR baseline methods. image classification tasks EVALUATE-FOR approach. approach COMPARE baseline methods. baseline methods COMPARE approach. Metric is PAC - Bayes generalization bound. OtherScientificTerm is noisy labels. ,This paper proposes a method to improve transfer learning by fine-tuning a pre-trained model on labeled data with noisy labels. The proposed method is based on a combination of self-labeling and label reweighting. The main contribution is a regularization term that is added to the self-correction term to improve the robustness of the model to label noise. Experiments on image classification and few-shot classification tasks show that the proposed method outperforms the baselines.  ,This paper proposes a method for transfer learning from a pre-trained model to a dataset with noisy labels. The main idea is to fine-tune the model on the labeled data to prevent overfitting. The authors show that the fine-tuning can improve the generalization properties of the model. They also provide a generalization bound on the PAC-Bayes generalization of their method. 
10226,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"value - at - risk ( VaR ) HYPONYM-OF tail - risk measures. finance and insurance industries HYPONYM-OF tail - risk measures. weighted sum of CVaR CONJUNCTION mean. mean CONJUNCTION weighted sum of CVaR. VaR CONJUNCTION weighted sum of CVaR. weighted sum of CVaR CONJUNCTION VaR. CVaR CONJUNCTION VaR. VaR CONJUNCTION CVaR. VaR CONJUNCTION mean. mean CONJUNCTION VaR. latter USED-FOR risk - return trade - off. risk - return trade - off FEATURE-OF finance. optimal δcorrect algorithm USED-FOR arms. heavy - tailed distributions FEATURE-OF arms. non - convex optimization problem USED-FOR algorithm. probability measures FEATURE-OF non - convex optimization problem. OtherScientificTerm are probability distributions, and arm. ","This paper studies the risk-return trade-off between value-at-risk (VaR) and mean-weighted sum of CVaR in finance and insurance. The authors propose a non-convex optimization problem with heavy-tailed distributions, where the probability distributions are weighted by the weighted sum of VaR and the mean. They show that the optimal algorithm for arms with heavy tailed distributions can be found by minimizing the risk.   ","This paper studies the problem of value-at-risk (VaR) and mean-VAR (CVaR) in finance. The authors show that the trade-off between VaR and CVaR can be approximated by a non-convex optimization problem. They provide an optimal algorithm for this problem, which is shown to be optimal for the case of heavy-tailed distributions. They also provide an analysis of the risk-return trade-offs between the two arms."
10251,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"Transformers USED-FOR computer vision tasks. Transformers USED-FOR modeling long - range dependency. self - attention mechanism USED-FOR modeling long - range dependency. intrinsic inductive bias ( IB ) USED-FOR modeling local visual structures. vision transformers USED-FOR image. 1D sequence of visual tokens USED-FOR image. 1D sequence of visual tokens USED-FOR vision transformers. training schedules USED-FOR IB. large - scale training data CONJUNCTION training schedules. training schedules CONJUNCTION large - scale training data. training schedules USED-FOR they. large - scale training data USED-FOR they. intrinsic IB USED-FOR Vision Transformer. ViTAE HYPONYM-OF convolutions. convolutions USED-FOR intrinsic IB. dilation rates FEATURE-OF convolutions. spatial pyramid reduction modules PART-OF ViTAE. it USED-FOR robust feature representation. it USED-FOR intrinsic scale invariance IB. convolution block PART-OF multi - head selfattention module. ViTAE PART-OF transformer layer. convolution block PART-OF transformer layer. convolution block PART-OF ViTAE. local features CONJUNCTION global dependencies. global dependencies CONJUNCTION local features. it USED-FOR global dependencies. it USED-FOR local features. intrinsic locality IB FEATURE-OF it. ImageNet EVALUATE-FOR ViTAE. downstream tasks EVALUATE-FOR ViTAE. ViTAE COMPARE concurrent works. concurrent works COMPARE ViTAE. ViTAE COMPARE baseline transformer. baseline transformer COMPARE ViTAE. downstream tasks EVALUATE-FOR concurrent works. downstream tasks EVALUATE-FOR baseline transformer. ImageNet CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet. baseline transformer CONJUNCTION concurrent works. concurrent works CONJUNCTION baseline transformer. ImageNet EVALUATE-FOR baseline transformer. ImageNet EVALUATE-FOR concurrent works. OtherScientificTerm are local visual structures, scale variance, and rich multi - scale context. Method are feed - forward network, and pretrained models. ","This paper proposes a novel self-attention module for vision transformers, named ViTAE, to model long-range dependency in images. The proposed method is based on the idea of intrinsic inductive bias (IB), which is used to model local visual structures in an image. The authors propose to use convolutional layers to model the intrinsic scale invariance of the convolution. The method is evaluated on ImageNet and CIFAR-10.   ","This paper proposes ViTAE, a method to model intrinsic inductive bias (IB) in vision transformers, which is a self-attention mechanism for modeling long-range dependency. The authors propose to use a convolution block in the transformer layer as well as a spatial pyramid reduction module in the multi-head selfattention module. The proposed method is evaluated on ImageNet and CIFAR-10. "
10276,SP:5e3572a386f890c5864437985cf63b13844f338f,fine - tuning USED-FOR NLP fields. pre - trained language models USED-FOR NLP fields. pre - trained language models USED-FOR fine - tuning. adversarial examples USED-FOR it. synonyms USED-FOR word substitution attacks. word substitution attacks HYPONYM-OF adversarial examples. adversarial training HYPONYM-OF defense technique. adversarial training USED-FOR fine - tuning scenario. catastrophic forgetting FEATURE-OF it. pre - trained model USED-FOR generic and robust linguistic features. Robust Informative Fine - Tuning ( RIFT ) HYPONYM-OF adversarial fine - tuning method. objective model USED-FOR features. RIFT USED-FOR objective model. pre - trained model USED-FOR features. pre - trained weights USED-FOR one. sentiment analysis CONJUNCTION natural language inference. natural language inference CONJUNCTION sentiment analysis. RIFT COMPARE state - of - the - arts. state - of - the - arts COMPARE RIFT. NLP tasks EVALUATE-FOR state - of - the - arts. NLP tasks EVALUATE-FOR RIFT. natural language inference HYPONYM-OF NLP tasks. sentiment analysis HYPONYM-OF NLP tasks. Method is BERT - based sentiment analysis model. Task is fine - tuning process. ," and sentiment analysis. The paper proposes Robust Informative Fine-tuning (RIFT), a method for adversarial fine-tuned language models. The method is based on the idea that adversarial training can cause catastrophic forgetting. To mitigate this catastrophic forgetting, the paper proposes to use synonyms as word substitution attacks. Experiments on sentiment analysis and NLP tasks show that the proposed method outperforms the state-of-the-arts.","This paper proposes Robust Informative Fine-tuning (RIFT), a method for fine-tuneing a pre-trained language model for NLP tasks. The idea is to use a BERT-based sentiment analysis model to train a model that is robust to word substitution attacks. The authors show that RIFT can improve the performance of NLP models on a variety of tasks, including sentiment analysis, natural language inference, and natural language modeling."
10301,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"acceleration method USED-FOR fixed - point iterations. Anderson mixing ( AM ) HYPONYM-OF acceleration method. convergence theory FEATURE-OF AM. Stochastic Anderson Mixing ( SAM ) scheme USED-FOR nonconvex stochastic optimization problems. damped projection and adaptive regularization USED-FOR AM. damped projection and adaptive regularization USED-FOR Stochastic Anderson Mixing ( SAM ) scheme. almost sure convergence CONJUNCTION worst - case iteration complexity. worst - case iteration complexity CONJUNCTION almost sure convergence. convergence theory FEATURE-OF SAM. almost sure convergence FEATURE-OF stationary points. almost sure convergence PART-OF convergence theory. worst - case iteration complexity PART-OF convergence theory. variance reduction technique PART-OF SAM. preconditioned mixing strategy USED-FOR SAM. faster convergence CONJUNCTION generalization ability. generalization ability CONJUNCTION faster convergence. preconditioned mixing strategy USED-FOR faster convergence. generalization ability EVALUATE-FOR preconditioned mixing strategy. DenseNet CONJUNCTION LSTM. LSTM CONJUNCTION DenseNet. ResNeXt CONJUNCTION DenseNet. DenseNet CONJUNCTION ResNeXt. vanilla CNN CONJUNCTION ResNets. ResNets CONJUNCTION vanilla CNN. WideResNet CONJUNCTION ResNeXt. ResNeXt CONJUNCTION WideResNet. ResNets CONJUNCTION WideResNet. WideResNet CONJUNCTION ResNets. SAM method USED-FOR neural networks. LSTM HYPONYM-OF neural networks. DenseNet HYPONYM-OF neural networks. vanilla CNN HYPONYM-OF neural networks. ResNeXt HYPONYM-OF neural networks. WideResNet HYPONYM-OF neural networks. ResNets HYPONYM-OF neural networks. image classification and language model EVALUATE-FOR method. Task are scientific computing, and machine learning problems. Metric is complexity bound. ","This paper studies the Anderson mixing (AM) method for nonconvex stochastic optimization problems. The authors propose to use Anderson mixing as an acceleration method for fixed-point iterations, which is an extension of Anderson Mixing (AM). The main contribution of the paper is to show that the convergence of the proposed method is guaranteed to be almost sure, and that the worst-case iteration complexity of the method is O(1/\sqrt{T}) times faster than that of AM. The main contributions of this paper are:  1) The authors show that Anderson mixing can be viewed as a special case of Anderson mixing with damped projection and adaptive regularization.  2) A variance reduction technique is proposed to reduce the variance of the mixing strategy.  3) The proposed method achieves almost sure convergence to stationary points. ",This paper proposes a Stochastic Anderson Mixing (SAM) scheme for nonconvex stochastic optimization problems with damped projection and adaptive regularization. The main idea is to use Anderson mixing (AM) as an acceleration method for fixed-point iterations. The authors provide a convergence analysis of SAM and show that it converges to almost sure convergence with worst-case iteration complexity. They also provide a variance reduction technique to improve the generalization ability of SAM. 
10326,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"posterior distribution USED-FOR linear inverse problem. SNIPS HYPONYM-OF stochastic algorithm. Langevin dynamics CONJUNCTION Newton ’s method. Newton ’s method CONJUNCTION Langevin dynamics. Newton ’s method USED-FOR solution. Langevin dynamics USED-FOR solution. singular value decomposition ( SVD ) USED-FOR degradation operator. singular value decomposition ( SVD ) PART-OF posterior score function. paradigm USED-FOR image deblurring. image deblurring CONJUNCTION super - resolution. super - resolution CONJUNCTION image deblurring. super - resolution CONJUNCTION compressive sensing. compressive sensing CONJUNCTION super - resolution. paradigm USED-FOR super - resolution. paradigm USED-FOR compressive sensing. OtherScientificTerm are additive white Gaussian noise, and noisy observation. Generic are approach, and algorithm. Method is iterative algorithm. Task is inverse problem. ","This paper proposes a stochastic algorithm for solving the linear inverse problem with additive white Gaussian noise. The proposed algorithm is based on the SNIPS algorithm, which is an extension of SNIPS to the inverse problem. The main contribution of the paper is the use of Langevin dynamics and Newton's method to solve the problem.  ","This paper proposes a stochastic algorithm for solving the linear inverse problem with additive white Gaussian noise. The main contribution of the paper is the use of Langevin dynamics and Newton’s method to solve the inverse problem. The paper also proposes a new paradigm for image deblurring, super-resolution and compressive sensing."
10351,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"Instagram HYPONYM-OF social media. techniques USED-FOR illicit drug trades. meta - learning technique USED-FOR MetaHG. multimodal content CONJUNCTION relational structured information. relational structured information CONJUNCTION multimodal content. holistic framework USED-FOR illicit drug traffickers. MetaHG USED-FOR illicit drug trafficker detection. relational structured information USED-FOR illicit drug trafficker detection. MetaHG USED-FOR multimodal content. MetaHG USED-FOR relational structured information. MetaHG USED-FOR illicit drug traffickers. social media FEATURE-OF relational structured information. MetaHG HYPONYM-OF holistic framework. social media USED-FOR illicit drug traffickers. Instagram HYPONYM-OF social media. heterogeneous graph ( HG ) USED-FOR MetaHG. relation - based graph convolutional neural network USED-FOR node ( i.e., user ) representations. graph structure refinement USED-FOR sparse connection among entities. graph structure refinement USED-FOR node representation learning. sparse connection among entities PART-OF HG. graph structure refinement USED-FOR HG. HG USED-FOR relation - based graph convolutional neural network. HG USED-FOR node ( i.e., user ) representations. meta - learning algorithm USED-FOR model optimization. self - supervised module CONJUNCTION knowledge distillation module. knowledge distillation module CONJUNCTION self - supervised module. unlabeled data USED-FOR model. knowledge distillation module USED-FOR model. knowledge distillation module USED-FOR unlabeled data. self - supervised module USED-FOR model. self - supervised module USED-FOR unlabeled data. MetaHG COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MetaHG. real - world data EVALUATE-FOR MetaHG. real - world data EVALUATE-FOR state - of - the - art methods. Instagram FEATURE-OF real - world data. Task are crime of drug trafficking, online drug trafficking, and model training. Material is post content. ","This paper proposes a novel framework for detecting drug traffickers on Instagram. The proposed method is based on a heterogeneous graph (HG) model, which is composed of a relation-based graph convolutional neural network and a knowledge distillation module. The self-supervised module is used to generate unlabeled data for the model optimization, and the knowledge distillers are used to distill the knowledge from the unlabelled data. The experimental results show that the proposed method outperforms state-of-the-art methods on Instagram in terms of detection performance.","This paper proposes a meta-learning framework for detecting illicit drug traffickers on Instagram. The proposed method is based on a heterogeneous graph (HG) model, where each node is represented as a relation-based graph convolutional neural network with sparse connection among entities. The model is trained with a self-supervised module and a knowledge distillation module, which is used to distill unlabeled data from Instagram data. Experimental results show that the proposed method outperforms other state-of-the-art methods on Instagram drug trafficking detection. "
10376,SP:242da1384f48260d58a0e7949438611c05079197,"ReLU activations CONJUNCTION architecture. architecture CONJUNCTION ReLU activations. architecture USED-FOR neural network. ReLU activations USED-FOR neural network. neural network USED-FOR class of functions. polyhedral theory CONJUNCTION tropical geometry. tropical geometry CONJUNCTION polyhedral theory. mixed - integer optimization CONJUNCTION polyhedral theory. polyhedral theory CONJUNCTION mixed - integer optimization. hidden layer USED-FOR learning tasks. techniques USED-FOR mathematical counterbalance. mathematical counterbalance USED-FOR universal approximation theorems. polyhedral theory USED-FOR techniques. mixed - integer optimization USED-FOR techniques. upper bounds FEATURE-OF neural networks. neural networks USED-FOR neural hypothesis classes. OtherScientificTerm are layers, and neural network literature. Task is algorithmic and statistical aspects. ",This paper studies the universal approximation of neural networks with ReLU activations. The authors propose to use mixed-integer optimization and polyhedral theory to find a mathematical counterbalance between universal approximation theorems. They show that this counterbalance can be used to improve the generalization performance of the neural networks. They also show that the proposed method can be applied to a variety of neural hypothesis classes.,This paper studies the problem of learning neural networks with ReLU activations. The authors study the problem from the perspective of polyhedral theory and tropical geometry. They show that the number of layers in a neural network can be reduced to a fixed number of hidden layers. They also provide upper bounds on the upper bounds of neural networks that can be used to learn neural hypothesis classes.  
10401,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"worst - case training principle USED-FOR maximal adversarial loss. min - max optimization USED-FOR AT. min - max optimization PART-OF adversarial context. framework USED-FOR adversarial attacks. min - max optimization USED-FOR adversarial attacks. framework USED-FOR min - max optimization. probability simplex FEATURE-OF domain weights. unified framework USED-FOR attack generation problems. unified framework USED-FOR crafting attacks. crafting attacks USED-FOR data transformations. crafting attacks HYPONYM-OF attack generation problems. attacking model ensembles HYPONYM-OF attack generation problems. approach COMPARE heuristic strategies. heuristic strategies COMPARE approach. robustness EVALUATE-FOR defense methods. heuristic strategies COMPARE defense methods. defense methods COMPARE heuristic strategies. approach COMPARE defense methods. defense methods COMPARE approach. robustness EVALUATE-FOR heuristic strategies. robustness EVALUATE-FOR approach. self - adjusted domain weights USED-FOR difficulty level of attack. min - max framework USED-FOR self - adjusted domain weights. Method is adversarial training ( AT ). Task are adversarial robustness, and min - max problem. OtherScientificTerm are risk sources, and universal perturbation. Metric is worst - case attack loss. ",This paper studies adversarial robustness in the worst-case setting. The authors propose a new adversarial training method based on minimizing the maximal adversarial loss. The main idea is to use the worst case training principle to estimate the adversarial perturbation and then use this information to generate adversarial attacks.   The authors show that this is equivalent to minimizing the min-max optimization problem in the general adversarial context. They also show that the self-adjustment of domain weights can be used to control the difficulty level of attack. ,This paper proposes a new framework for adversarial training based on the worst-case training principle. The key idea is to use the min-max optimization principle to estimate the maximal adversarial loss in the adversarial context. The authors show that the worst case training principle can be used to estimate a probability simplex of the domain weights. They also propose a unified framework for attacking model ensembles and crafting attacks.   
10426,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. model USED-FOR sparse PCA. model USED-FOR tensor PCA. Wigner form FEATURE-OF sparse PCA. polynomial - time algorithm CONJUNCTION exponential - time exhaustive search algorithm. exponential - time exhaustive search algorithm CONJUNCTION polynomial - time algorithm. polynomial - time algorithm USED-FOR algorithms. exponential - time exhaustive search algorithm USED-FOR algorithms. algorithms USED-FOR sparse vector. signal - tonoise ratio λ FEATURE-OF sparse vector. algorithms USED-FOR sparse vectors. algorithms COMPARE algorithms. algorithms COMPARE algorithms. λ FEATURE-OF sparse vectors. algorithms USED-FOR sparse PCA. signal - to - noise ratio CONJUNCTION running time. running time CONJUNCTION signal - to - noise ratio. sparse PCA CONJUNCTION tensor PCA. tensor PCA CONJUNCTION sparse PCA. lower bound USED-FOR lower bounds. lower bounds USED-FOR sparse PCA. lower bound USED-FOR sparse PCA. lower bounds USED-FOR tensor PCA. Task is sparse tensor principal component analysis. OtherScientificTerm are i.i.d. Gaussian entries, k - sparse unit vector, k - sparse signals, and sparsity k. Generic is matrix settings. Metric is low - degree likelihood ratio. ",This paper studies sparse PCA and tensor PCA in the Wigner form. The authors propose two algorithms for computing the signal-to-noise ratio of a sparse unit vector. The first algorithm is a polynomial-time algorithm and the second one is an exponential-time search algorithm. The running time of both algorithms is exponential in the number of samples and polynomially in the size of the data set.  The authors show that the running time and the signal to noise ratio of the sparse unit vectors can be improved by a factor of k. ,This paper studies the problem of sparse tensor principal component analysis (Sparse PCA) and tensor PCA (Tensor PCA). The main contribution of the paper is a lower bound on the signal-to-noise ratio for sparse PCA. The lower bound is based on the Wigner form of the sparse vectors. The authors show that the lower bound can be used for both sparse PCAs and tensors. They also provide lower bounds for tensors as well. 
10451,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"Multilayer - perceptrons ( MLP ) USED-FOR learning functions of high - frequencies. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR MLP networks. spatially adaptive progressive encoding ( SAPE ) scheme USED-FOR them. feedback loop USED-FOR neural optimization process. feedback loop USED-FOR progressive exposure of frequencies. regression of low dimensional signals CONJUNCTION images. images CONJUNCTION regression of low dimensional signals. representation learning of occupancy networks CONJUNCTION geometric task of mesh transfer. geometric task of mesh transfer CONJUNCTION representation learning of occupancy networks. regression of low dimensional signals CONJUNCTION representation learning of occupancy networks. representation learning of occupancy networks CONJUNCTION regression of low dimensional signals. SAPE USED-FOR applications. geometric task of mesh transfer HYPONYM-OF applications. regression of low dimensional signals HYPONYM-OF applications. images HYPONYM-OF applications. representation learning of occupancy networks HYPONYM-OF applications. OtherScientificTerm are wide frequency bands, and 3D shapes. Metric is training stability. Method is domain specific preprocessing. ","This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for learning functions of high-frequency in multilayer-perceptron (MLP) networks. The proposed SAPE is based on the observation that MLP networks learn functions of wide frequency bands, and 3D shapes. The authors propose to use a feedback loop to optimize the progressive exposure of frequencies in the input space. The experiments show that SAPE improves the performance of MLP models on a variety of tasks, including mesh transfer, regression of low dimensional signals, and representation learning. ","This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for multilayer-perceptron (MLP) networks. The proposed SAPE scheme is based on the idea of progressive exposure of frequencies in a feedback loop. The authors show that SAPE can be used to improve the training stability of MLP networks in a variety of applications, including regression of low dimensional signals and mesh transfer. "
10476,SP:b03063fa82d76db341076e5f282176f4c007a202,"probability simplex constraints FEATURE-OF constrained saddle - point optimization problem. constrained saddle - point optimization problem USED-FOR equilibrium of competitive games. methods USED-FOR constrained settings. unconstrained setting FEATURE-OF extragradient methods. entropy regularization USED-FOR single - agent reinforcement learning and game theory. entropy regularization FEATURE-OF zero - sum two - player matrix games. algorithms USED-FOR approximate Nash equilibrium. approximate Nash equilibrium FEATURE-OF unregularized matrix game. knob of entropy regularization USED-FOR algorithms. policy extragradient algorithms USED-FOR entropy - regularized zero - sum Markov games. methods USED-FOR policy extragradient algorithms. linear rate FEATURE-OF policy extragradient algorithms. entropy regularization USED-FOR accelerating convergence. logarithm factors FEATURE-OF state and action spaces. OtherScientificTerm are multiplicative updates, objective function, sublinear rate, and Nash equilibrium. Method is symmetric and multiplicative updates. Metric is convergence rates. ","This paper studies the policy extragradient methods in the constrained saddle-point optimization problem of competitive games with probability simplex constraints. The main contributions are:  1. The authors show that under certain assumptions on the state and action spaces of the game, there exists an algorithm that converges to an approximate Nash equilibrium at a sublinear rate.  2. They show that this is the case even in the unconstrained setting.  3. They also show that in the two-player matrix game with entropy regularization, the algorithm converges at a linear rate to the Nash equilibrium.  ","This paper studies the convergence rate of policy extragradient algorithms for zero-sum two-player matrix games with entropy-regularized Markov games. The authors show that the rate of convergence is linear in the unconstrained setting and sublinear in the constrained setting. They also show that under certain assumptions on the state space and action space, the rate converges at a sublinear rate. The main contribution of the paper is to provide a theoretical analysis of the convergence rates of the algorithms."
10501,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"remote cooperation CONJUNCTION online education. online education CONJUNCTION remote cooperation. screen sharing CONJUNCTION remote cooperation. remote cooperation CONJUNCTION screen sharing. HR display USED-FOR super - resolution ( SR ). image SR methods USED-FOR natural images. image SR methods USED-FOR SCIs. pixel values USED-FOR continuous SR. implicit transformer USED-FOR image features. image features USED-FOR pixel values. LR and HR SCI pairs USED-FOR SCI1 K and SCI1K - compression datasets. continuous and discrete SR methods USED-FOR compressed and uncompressed SCIs. ITSRN COMPARE continuous and discrete SR methods. continuous and discrete SR methods COMPARE ITSRN. ITSRN USED-FOR compressed and uncompressed SCIs. Material is screen contents. OtherScientificTerm are limited terminal bandwidth, high - resolution ( HR ) screen contents, and image characteristics. Task is SCI browsing. Method are SCISR, and implicit position encoding scheme. ",This paper proposes an implicit position encoding scheme (ITSRN) for high-resolution (HR) SCI browsing. The proposed method is based on a transformer-based implicit transformer that encodes the pixel values of the HR SCI pairs. The authors show that the proposed method outperforms existing SR methods in both compressed and uncompressed SCIs. ,This paper proposes a new SCISR-based SCI browsing method based on the implicit position encoding scheme (ITSRN). The proposed method is based on a transformer-based implicit transformer that encodes the image features into pixel values. The authors show that the proposed method outperforms the state-of-the-art continuous and discrete SR methods on both compressed and uncompressed SCIs. 
10526,SP:3751625929b707ced417c3eb10064e4917866048,"probabilistic models USED-FOR causality. sumproduct networks ( SPNs ) USED-FOR learning interventional distributions. gate functions USED-FOR sumproduct networks ( SPNs ). neural networks HYPONYM-OF gate functions. gate function USED-FOR SPN. structural causal model USED-FOR interventional SPNs. personal health FEATURE-OF structural causal model. generative and causal modelling USED-FOR methods. Generic is so. Task is intractability of inference. Method is causal models. OtherScientificTerm are interventional distributions, arbitrarily intervened causal graph, and Pearl ’s do - operator. ","This paper proposes to use sumproduct networks (SPNs) to learn interventional distributions in probabilistic models of causality. The main idea is to learn a causal model of the interventional distribution in the presence of an intervening causal graph. The proposed method is based on the idea of using a gate function to learn the causal model.   The main contribution of the paper is a theoretical analysis of the problem of learning interventional causal distributions. The authors show that the proposed method can learn the distribution in a deterministic manner, and that it is equivalent to learning the distribution with the help of Pearl’s do operator. ",This paper studies the intractability of learning interventional distributions using sumproduct networks (SPNs). The authors propose a structural causal model (Pearl’s do-operator) to model the interventional distribution. They show that this model is intractable due to the use of arbitrarily intervened causal graph and Pearl's do- operator. They also show that the proposed model can be used to learn interventional SPNs.  
10551,SP:c857ff674ca05c1d949337cb885f056b82d981d6,"Factor analysis methods USED-FOR low dimensional, ideally interpretable representations. Factor analysis methods USED-FOR neuroimaging. Factor analysis methods USED-FOR high dimensional imaging data. high dimensional imaging data USED-FOR low dimensional, ideally interpretable representations. deep Markov factor analysis ( DMFA ) HYPONYM-OF generative model. Markov property USED-FOR low dimensional temporal embeddings. Markov property CONJUNCTION spatial inductive assumptions. spatial inductive assumptions CONJUNCTION Markov property. temporal dynamics FEATURE-OF functional magnetic resonance imaging ( fMRI ) data. Markov property USED-FOR generative model. discrete latent USED-FOR DMFA. DMFA USED-FOR fMRI data. low dimensional temporal embedding USED-FOR DMFA. DMFA USED-FOR interpretable clusters. DMFA USED-FOR nonlinear temporal dependencies. synthetic and real fMRI data EVALUATE-FOR DMFA. nonlinear temporal dependencies FEATURE-OF high dimensional imaging data. Generic is methods. OtherScientificTerm are nonlinear and complex temporal dynamics of neural processes, high spatial dimensionality, and subject and cognitive state variability. Material is imaging data. Method is neural networks. Task are fMRI - driven neuroscientific hypotheses, and capturing nonlinear temporal dependencies. ","This paper proposes a novel method for learning temporal embeddings of functional magnetic resonance imaging (fMRI) data with nonlinear temporal dependencies. The method is based on deep Markov factor analysis (DMFA), which is a generative model that uses a discrete latent variable to model the temporal dynamics of neural processes. The authors show that DMFA is able to learn a low-dimensional temporal embedding of the fMRI data, which is then used to construct interpretable clusters. The proposed method is evaluated on both synthetic and real-world fMRI datasets. ","This paper proposes a generative model for understanding the temporal dynamics of functional magnetic resonance imaging (fMRI) data. The authors propose a deep Markov factor analysis (DMFA) model for generating low dimensional embeddings of fMRI data that capture the nonlinear temporal dependencies of neural processes. They show that the DMFA can be used to generate interpretable clusters of high spatial dimensionality, and that it can be applied to both synthetic and real-world fMRI datasets."
10576,SP:855dcaa42868a29a14619d63221169495ed5dd54,"spheres CONJUNCTION tori. tori CONJUNCTION spheres. generative models USED-FOR complex geometries. tori CONJUNCTION implicit surfaces. implicit surfaces CONJUNCTION tori. manifolds USED-FOR complex geometries. implicit surfaces HYPONYM-OF manifolds. spheres HYPONYM-OF complex geometries. spheres HYPONYM-OF manifolds. tori HYPONYM-OF complex geometries. tori HYPONYM-OF manifolds. Moser Flow ( MF ) HYPONYM-OF generative models. continuous normalizing flows ( CNF ) FEATURE-OF generative models. MF USED-FOR CNF. source ( prior ) density CONJUNCTION divergence. divergence CONJUNCTION source ( prior ) density. CNF methods COMPARE model ( learned ) density. model ( learned ) density COMPARE CNF methods. divergence PART-OF neural network ( NN ). source ( prior ) density USED-FOR model ( learned ) density. divergence HYPONYM-OF local, linear differential operator. CNFs COMPARE MF. MF COMPARE CNFs. divergence USED-FOR model density. NN USED-FOR model density. MF USED-FOR universal density approximator. flow models USED-FOR sampling from general curved surfaces. training complexity EVALUATE-FOR CNFs. sample quality EVALUATE-FOR CNFs. sample quality CONJUNCTION training complexity. training complexity CONJUNCTION sample quality. synthetic geometries CONJUNCTION real - world benchmarks. real - world benchmarks CONJUNCTION synthetic geometries. density estimation CONJUNCTION sample quality. sample quality CONJUNCTION density estimation. flow models COMPARE CNFs. CNFs COMPARE flow models. density estimation EVALUATE-FOR CNFs. earth and climate sciences FEATURE-OF synthetic geometries. earth and climate sciences FEATURE-OF real - world benchmarks. earth and climate sciences EVALUATE-FOR CNFs. real - world benchmarks EVALUATE-FOR CNFs. synthetic geometries EVALUATE-FOR CNFs. Method are Euclidean ) generative models, and ODE solver. OtherScientificTerm are change - of -","This paper proposes continuous normalizing flows (CNFs), a new generative model for complex manifolds. CNFs are an extension of the Moser flow (MF) family of generative models. The main idea is to use the divergence between the source and the learned density of the model as a universal density approximator, which can be used to estimate the density of samples from general curved surfaces. The authors show that the convergence rate of the proposed method is faster than that of the original Moser Flow (MF). The authors also show that their method is universal in the sense that it is able to sample from any curved surface.   ",This paper proposes a new continuous normalizing flow (CNF) method for generating complex geometries. The main contribution of the paper is the use of a universal density approximator to approximate the source and prior density of the model. This is done by using the divergence of the source density and the prior density as a local differential operator. The authors show that their method can be used to estimate the model density and sample quality better than previous methods. They also provide a theoretical analysis of the convergence rate of their method.
10601,SP:545554de09d17df77d6169a5cc8f36022ecb355c,"framework USED-FOR unsupervised representation learning. Independent component analysis USED-FOR unsupervised representation learning. observed variables PART-OF generative process. independent causal mechanisms USED-FOR causality. assumptions USED-FOR independent causal mechanisms. approach USED-FOR nonidentifiability issues. nonidentifiability issues FEATURE-OF nonlinear blind source separation. OtherScientificTerm are latent code, mixing, statistical independence, Identifiability, and mixing process. Generic is model. Method is independent mechanism analysis. ",This paper studies the problem of unsupervised representation learning in the presence of mixing in a generative process. The authors propose an independent component analysis (ICA) framework to analyze the mixing process in the latent space. The main contribution of the paper is the identification of non-identifiability issues in the non-linear blind source separation.   ,This paper proposes a new framework for unsupervised representation learning for generative models. The key idea is to use independent component analysis (ICA) to identify the independent causal mechanisms in the generative process. The authors show that the mixing process of the latent code is independent of the observed variables in the latent space. They also show that there are non-identifiability issues in the case of blind source separation.   
10626,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"Annealed Importance Sampling ( AIS ) HYPONYM-OF method. Hamiltonian MCMC USED-FOR Annealed Importance Sampling ( AIS ). non - differentiable transition kernels USED-FOR it. AIS - like procedure USED-FOR framework. Uncorrected Hamiltonian MCMC USED-FOR AIS - like procedure. Uncorrected Hamiltonian Annealing HYPONYM-OF AIS - like procedure. method COMPARE approaches. approaches COMPARE method. method USED-FOR tight and differentiable lower bounds. method COMPARE approaches. approaches COMPARE method. OtherScientificTerm are unnormalized target distribution, tight lower bound, and reparameterization gradients. ","This paper proposes a new method for Hamiltonian MCMCMC based on Annealed Importance Sampling (AIS) that uses non-differentiable transition kernels to sample from the target distribution. The proposed method is based on the uncorrected Hamiltonian Annealing (UHA) method, which is a generalization of AIS. The main contribution of the paper is to show that UHA can be used as an alternative to AIS in the sense that it does not require reparameterization of the gradients.  ","This paper proposes a novel Hamiltonian MCMC-based method for Annealed Importance Sampling (AIS). The main idea is to use a non-differentiable transition kernel for the AIS-like procedure, which can be applied to the target distribution. The authors prove a tight lower bound on the reparameterization gradients of the transition kernels. They also show that their method is differentiable and tight and differentiable lower bounds can be obtained."
10651,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"deep neural networks USED-FOR safetycritical applications. training algorithms USED-FOR neural network. training algorithms USED-FOR robustness. Certified robustness FEATURE-OF deep neural networks. robustness EVALUATE-FOR neural network. Lipschitz constant FEATURE-OF global bound. global bound USED-FOR training algorithms. non - convexity FEATURE-OF network. natural and certified accuracy EVALUATE-FOR tighter Lipschitz bound. activation functions CONJUNCTION weight matrices. weight matrices CONJUNCTION activation functions. induced norm FEATURE-OF weight matrix. global Lipschitz constant FEATURE-OF neural network. method USED-FOR plug - in module. plug - in module USED-FOR Lipschitz bound. method USED-FOR Lipschitz bound. Lipschitz bound FEATURE-OF certifiable training algorithms. upper threshold CONJUNCTION sparsity loss. sparsity loss CONJUNCTION upper threshold. ReLU CONJUNCTION MaxMin. MaxMin CONJUNCTION ReLU. sparsity loss USED-FOR network. network USED-FOR local Lipschitz bound. upper threshold USED-FOR activation functions. MaxMin HYPONYM-OF activation functions. ReLU HYPONYM-OF activation functions. method COMPARE methods. methods COMPARE method. network architectures USED-FOR method. TinyImageNet datasets EVALUATE-FOR methods. clean and certified accuracy EVALUATE-FOR methods. MNIST EVALUATE-FOR methods. TinyImageNet datasets EVALUATE-FOR method. clean and certified accuracy EVALUATE-FOR method. MNIST EVALUATE-FOR method. Generic are bound, and it. Metric is natural accuracy. OtherScientificTerm are local Lipschitz upper bound, and activation function. ",This paper proposes a method for certifying the Lipschitz upper bound of deep neural networks. The method is based on a plug-in module that is used to compute the global bound of the network. The main idea is to use the induced norm of the weight matrix as a regularization term. The authors show that this regularization can be used to improve the certified accuracy of deep networks.  ,"This paper proposes a method to improve the Lipschitz bound for certified robustness of deep neural networks. The key idea is to use a plug-in module that can be applied to any neural network to compute the global bound. The proposed method is based on the notion of induced norm, where the induced norm is the weight matrix of the weight matrices of the network. The authors show that their method can improve the certified accuracy of deep networks by a factor of 1.5. "
10676,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"scalable methods USED-FOR conformal Bayesian predictive intervals. Bayesian posterior predictive distributions USED-FOR subjective beliefs. finite sample frequentist guarantees FEATURE-OF predictive confidence intervals. conformal inference USED-FOR predictive confidence intervals. conformal inference USED-FOR finite sample frequentist guarantees. add - one - in ’ importance sampling USED-FOR conformal Bayesian predictive intervals. re - weighted posterior samples of model parameters USED-FOR conformal Bayesian predictive intervals. refitting of models CONJUNCTION data - splitting. data - splitting CONJUNCTION refitting of models. approach COMPARE conformal methods. conformal methods COMPARE approach. refitting of models USED-FOR conformal methods. computational efficiency EVALUATE-FOR conformal methods. data - splitting USED-FOR conformal methods. hierarchical models HYPONYM-OF partially exchangeable settings. OtherScientificTerm are finite sample calibration guarantees, predictors, predictive intervals, and model fidelity. Method is Bayesian prediction. Metric is empirical coverage. Generic is examples. ",This paper studies the problem of computing conformal predictive intervals for Bayesian posterior predictive distributions. The authors propose to use add-one-in-importance sampling to compute conformal Bayesian predictive intervals with finite sample frequentist guarantees. The proposed method is based on re-weighting the posterior samples of model parameters and re-sampling the posterior distributions of the predictors of the model parameters.  The authors show that the proposed method can be used to compute the predictive intervals in a scalable manner and achieves better computational efficiency compared to previous methods.  ,"This paper studies conformal Bayesian predictive intervals, which are defined as the posterior predictive distributions of Bayesian posterior distributions of the model parameters. The authors show that conformal predictive intervals have finite sample frequentist guarantees, and propose an add-one-in-the-importance sampling (add-1-in) approach for conformal posterior predictive intervals. They show that this approach can be used to improve the computational efficiency of conformal methods. They also show that it can be applied to partially exchangeable settings."
10701,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"denoisers USED-FOR general inverse problems. priors FEATURE-OF explicit likelihood functions. regularization - by - denoising ( RED ) HYPONYM-OF frameworks. RED USED-FOR imaging tasks. RED CONJUNCTION PnP. PnP CONJUNCTION RED. PnP USED-FOR imaging tasks. convolutional neural networks ( CNNs ) HYPONYM-OF denoisers. maximum a posteriori ( MAP ) CONJUNCTION minimum mean square error ( MMSE ) estimators. minimum mean square error ( MMSE ) estimators CONJUNCTION maximum a posteriori ( MAP ). convergence FEATURE-OF RED and PnP methods. CNN denoisers USED-FOR maximum a posteriori ( MAP ). Lipschitz constant FEATURE-OF CNN denoisers. denoisers PART-OF RED and PnP schemes. denoisers USED-FOR MAP and MMSE estimators interpretation. symmetric Jacobians FEATURE-OF denoisers. backtracking step size USED-FOR RED and PnP schemes. backtracking step size USED-FOR denoisers. denoisers USED-FOR inversion method. method COMPARE RED and PnP methods. RED and PnP methods COMPARE method. imaging experiments EVALUATE-FOR RED and PnP methods. imaging experiments EVALUATE-FOR method. Method are denoising algorithms, MAP or MMSE estimators, inverse algorithms, and image denoisers. Generic is they. OtherScientificTerm are potentials, and objective function. ",This paper proposes to use image denoisers to improve the convergence of regularization-by-denoising (RED) and regularization by regularization (PnP) methods for inverse problems. The main contribution of this paper is to show that the Lipschitz constant of the denoiser used in RED and PnP can be reduced to the same as that used in CNNs. The paper also shows that the backtracking step size of denoising can be controlled by the number of backtracking steps. The proposed method is shown to converge to the optimal solution of the inverse problem.   ,"This paper proposes a new denoising method for image denoisers. The main contribution of the paper is to study the convergence of the proposed method to the maximum a posteriori (MAP) and minimum mean square error (MMSE) estimators. The authors show that their method converges faster than the existing RED and PnP methods. They also show that the Lipschitz constant of the denoiser can be used to estimate the MAP and MMSE estimators, and that the backtracking step size can be reduced."
10726,SP:da92e936f88b3842ca82c2914413b129ca35890f,rhythmic features FEATURE-OF activities. rhythmic features FEATURE-OF musical soundtrack. system USED-FOR soundtrack. them USED-FOR rhythmic sounds. models USED-FOR rhythmic sounds. human movements USED-FOR RhythmicNet. skeleton keypoints USED-FOR RhythmicNet. rhythm CONJUNCTION melody. melody CONJUNCTION rhythm. natural process of music improvisation USED-FOR RhythmicNet. RhythmicNet USED-FOR rhythm. RhythmicNet USED-FOR style pattern. body keypoints USED-FOR rhythm. body keypoints USED-FOR RhythmicNet. body keypoints USED-FOR style pattern. U - net based model USED-FOR velocity. U - net based model USED-FOR it. transformerbased model USED-FOR it. inherit sound association FEATURE-OF body movements. body movements PART-OF large scale video datasets. dance HYPONYM-OF body movements. dance HYPONYM-OF inherit sound association. large scale video datasets EVALUATE-FOR RhythmicNet. Task is video. OtherScientificTerm is free body movements. Generic is method. ,This paper proposes a method to learn to predict the rhythm and melody of human body movements in videos. The method is based on a skeleton keypoint model that predicts the skeleton keypoints of the human body. The key idea is to use the body keypoints as a musical instrument and use a transformer-based model to model the velocity of the skeleton. The proposed method is evaluated on a variety of video datasets and shows good results.   ,"This paper proposes a method to learn the rhythmic features of human body movements from video. The key idea is to learn a skeleton keypoint for each body movement, and use it to predict the rhythm of the body movements. The skeleton keypoints are learned using a transformer-based model, and a U-net based model is used to model the velocity. The method is evaluated on a variety of video datasets, and the results show that the proposed method can learn the rhythm and melody of human movements."
10751,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"approaches USED-FOR offline reinforcement learning ( RL ). iterative actor - critic approach USED-FOR approaches. off - policy evaluation PART-OF iterative actor - critic approach. on - policy Q estimate USED-FOR behavior policy. on - policy Q estimate USED-FOR constrained / regularized policy improvement. onestep algorithm COMPARE iterative algorithms. iterative algorithms COMPARE onestep algorithm. D4RL benchmark EVALUATE-FOR iterative algorithms. D4RL benchmark EVALUATE-FOR onestep algorithm. one - step baseline COMPARE iterative algorithms. iterative algorithms COMPARE one - step baseline. OtherScientificTerm is hyperparameters. Method are iterative approaches, and one - step algorithm. Task is repeated optimization of policies. Generic is estimates. ","This paper proposes an iterative actor-critic approach for offline reinforcement learning. The main idea is to use an on-policy Q estimate to estimate the Q-value of the behavior policy, which is then used to improve the performance of the policy. The proposed method is evaluated on the D4RL benchmark and shows improved performance compared to existing methods. ","This paper proposes an iterative actor-critic approach for offline reinforcement learning (D4RL). The main idea is to use the on-policy Q estimate to estimate the Q of the behavior policy, which is then used to improve the performance of the policy. The proposed method is evaluated on the D4RL benchmark, where it is shown to outperform the state-of-the-art. "
10776,SP:0346eba4f587acbe3492d039066f1737360fd870,"statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. tasks PART-OF machine learning. Low - rank and nonsmooth matrix optimization problems USED-FOR tasks. tasks PART-OF statistics. Low - rank and nonsmooth matrix optimization problems USED-FOR statistics. methods USED-FOR smooth low - rank optimization problems. convex relaxations USED-FOR problems. extragradient method USED-FOR optimal solution. maximum of smooth functions USED-FOR nonsmooth objective. initializations USED-FOR extragradient method. full - rank SVDs CONJUNCTION SVDs of rank. SVDs of rank CONJUNCTION full - rank SVDs. OtherScientificTerm are high - rank matrices, high - rank SVDs, natural generalized strict complementarity condition, low - rank SVDs, and SVDs. Task are nonsmooth problems, and nonsmooth low - rank matrix recovery tasks. Generic is method. ","This paper studies the problem of low-rank matrix recovery from nonsmooth matrices. The authors propose a novel method for solving the problem, which they call the extragradient method. The main idea is to use the maximum of smooth functions to solve the problem. They show that the optimal solution of the problem can be obtained by using the max-squared method. They also show that this method can be used to recover the matrix with high rank.",This paper studies the problem of low-rank and nonsmooth matrix recovery from high-rank SVDs. The authors propose an extragradient method to find the optimal solution of the problem. The main contribution of the paper is to provide a generalization of the generalization result of the previous work.  The authors show that the maximum of smooth functions can be used to find an optimal solution for the nonsmoothed problem. They also provide a theoretical analysis of their method.
10801,SP:d39f1d77d9919f897ccf82958b71be8798523923,graphs CONJUNCTION images. images CONJUNCTION graphs. images CONJUNCTION texts. texts CONJUNCTION images. texts HYPONYM-OF structured treatments. graphs HYPONYM-OF structured treatments. images HYPONYM-OF structured treatments. arbitrary models USED-FOR learning. generalized Robinson decomposition USED-FOR causal estimand. mild assumptions FEATURE-OF quasi - oracle convergence guarantee. approach COMPARE prior work. prior work COMPARE approach. small - world and molecular graphs EVALUATE-FOR approach. prior work USED-FOR CATE estimation. CATE estimation EVALUATE-FOR approach. OtherScientificTerm is regularization bias. ,"This paper proposes a method to estimate the causal effect of a given treatment on a set of data points. The proposed method is based on a generalized Robinson decomposition of the causal estimand, which is then used to learn a causal estimator. The authors show that under mild assumptions, the proposed method achieves quasi-oracle convergence with a regularization bias. The method is shown to outperform existing methods on small-world and molecular graphs.","This paper proposes a method for estimating the causal estimand of a causal estimator (CATE) for a set of structured data (graphs, images, texts, and graphs) with arbitrary models. The method is based on a generalized Robinson decomposition (GRC), which is a generalization of the original CATE estimator. The authors provide a quasi-oracle convergence guarantee for the GRC estimator under mild assumptions, and show that the estimator converges to the true estimator with mild assumptions. They also provide an empirical evaluation of the proposed method on small-world and molecular graphs."
10826,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"qualitative assumptions CONJUNCTION distributions. distributions CONJUNCTION qualitative assumptions. system USED-FOR distributions. causal graph HYPONYM-OF qualitative assumptions. probability axioms CONJUNCTION do - calculus. do - calculus CONJUNCTION probability axioms. do - calculus CONJUNCTION c - factorization. c - factorization CONJUNCTION do - calculus. probability axioms USED-FOR graphical criteria. graphical criteria USED-FOR identification algorithms. matrix equations USED-FOR proxy variables. graphical criteria CONJUNCTION matrix equations. matrix equations CONJUNCTION graphical criteria. graphical criteria USED-FOR causal identification algorithm. matrix equations USED-FOR causal identification algorithm. graphically - driven formulae CONJUNCTION matrix multiplications. matrix multiplications CONJUNCTION graphically - driven formulae. enriched matrix - based criteria PART-OF graphical identification approach. marginal, conditional, and interventional distributions USED-FOR causal effect identification algorithm. Task is Causal effect identification. OtherScientificTerm are causal effect, proxy variable based identification conditions, and intermediary criteria. ","This paper studies the problem of causal effect identification, where the goal is to identify the causal effect from a set of observations. The authors propose a method based on matrix-based criteria to identify proxy variables. The main contribution of the paper is the introduction of matrix equations for proxy variables, which can be used to improve the identification performance of existing methods.","This paper proposes a new method for causal effect identification based on a set of matrix-based criteria. The proposed method is motivated by the fact that it is possible to identify the causal effect under a number of proxy variables (e.g., marginal, conditional, and interventional distributions). The authors propose a method for identifying the proxy variables based on matrix equations. The method is based on the use of matrix multiplications and do-scalars. "
10851,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"Unsupervised Environment Design ( UED ) HYPONYM-OF selfsupervised RL paradigm. random levels USED-FOR PLR. Dual Curriculum Design ( DCD ) HYPONYM-OF UED methods. PLR CONJUNCTION UED algorithm. UED algorithm CONJUNCTION PLR. UED algorithm CONJUNCTION PAIRED. PAIRED CONJUNCTION UED algorithm. PLR CONJUNCTION PAIRED. PAIRED CONJUNCTION PLR. PAIRED PART-OF DCD. PLR PART-OF DCD. UED algorithm PART-OF DCD. theory USED-FOR PLR. robustness guarantee FEATURE-OF Nash equilibria. theory USED-FOR PLR. Nash equilibria FEATURE-OF convergence. PLR⊥ USED-FOR PAIRED. PLR⊥ HYPONYM-OF method. Method are Deep reinforcement learning ( RL ) agents, Prioritized Level Replay ( PLR ), UED, and theoretical framework. OtherScientificTerm are environment and task configurations, diverse training environments, randomly - generated training levels, and theoretical guarantees. Generic is it. ","This paper studies the problem of unsupervised environment design in self-supervised reinforcement learning. The authors propose Prioritized Level Replay (PLR), which is a variant of the Dual Curriculum Design (DCD) algorithm. The main contribution of the paper is a theoretical analysis of the convergence of PLR to Nash equilibria. Theoretical results show that PLR converges to a Nash equilibrium at a rate of $O(1/\sqrt{T})$ with probability $1/T$, where $T$ is the number of randomly generated training levels, and $\tilde{O}(T)$ is a function of $T$. ","This paper presents a theoretical analysis of Prioritized Level Replay (PLR) and Dual Curriculum Design (DCD) in the context of Unsupervised Environment Design (UED). PLR and DCD are two popular self-supervised RL paradigms, and the authors show that they can converge to Nash equilibria in the presence of diverse training environments. They also provide a theoretical guarantee for the robustness of PLR. "
10876,SP:9ed528da4b67f22678303cfd975aafe678db6411,"( ε, δ)-differentially private algorithm USED-FOR multi - armed bandit ( MAB ) problem. shuffle model FEATURE-OF multi - armed bandit ( MAB ) problem. upper bound COMPARE regret. regret COMPARE upper bound. Metric are distribution - dependent regret, and distribution - independent regret. OtherScientificTerm is suboptimality gap. Method are centralized model, and local model. ","This paper studies the multi-armed bandit (MAB) problem in the shuffle model, where the arms are distributed over a set of arms, and the goal is to minimize the suboptimality gap between the arms. The main contribution of this paper is to provide a (\epsilon, \delta)-differentially private algorithm for the MAB problem. The algorithm is based on a shuffle-based algorithm, and it is shown to have a regret of $O(1/delta^2)$ where $delta$ is the number of arms and $d$ is a random variable. The regret of the algorithm is also shown to be independent of $d$. ","This paper studies the multi-armed bandit (MAB) problem with a shuffle model. The authors show that the regret of the MAB problem is bounded by the suboptimality gap between the number of arms and the total number of bandits. They show that under certain assumptions, the regret is bounded in terms of the subvariability of the arms. They also provide an upper bound for the regret. "
10901,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"probabilistic forecasts USED-FOR decision rules. threshold calibration HYPONYM-OF calibration. algorithm USED-FOR threshold - calibrated forecaster. uncalibrated forecaster USED-FOR algorithm. threshold loss function USED-FOR threshold decision. hospital scheduling decisions CONJUNCTION resource allocation decisions. resource allocation decisions CONJUNCTION hospital scheduling decisions. threshold calibration USED-FOR decision loss prediction. real - world settings EVALUATE-FOR threshold calibration. resource allocation decisions HYPONYM-OF real - world settings. hospital scheduling decisions HYPONYM-OF real - world settings. OtherScientificTerm are forecasted probabilities, predicted losses, cutoff, decision loss, and threshold decisions. Task are regression setting, and loss of threshold decisions. Generic is procedure. ","This paper studies the problem of threshold calibration for decision rules in regression. In this setting, the goal is to estimate the decision loss of a set of threshold decisions. The authors propose a method to calibrate the loss of the threshold decisions in the regression setting. The main idea is to use a threshold-calibrated forecaster, which is trained to predict the threshold decision. The method is shown to be computationally efficient.   ","This paper proposes a new method to calibrate a threshold-calibrated forecaster for predicting the loss of threshold decisions in a regression setting. The method is based on the idea of threshold calibration, where the threshold is defined as the difference between the predicted loss and the true loss of the threshold decision. The authors show that threshold calibration can be used to improve the performance of the decision-making process in a real-world setting.  "
10926,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,method USED-FOR centroid approximation. centroid approximation USED-FOR random function. argmax distribution HYPONYM-OF random function. method USED-FOR argmax distribution. centroid points USED-FOR argmax distribution. centroid points USED-FOR method. objective function USED-FOR method. objective function USED-FOR argmax distribution. personalized dialogue systems CONJUNCTION multi - target domain adaptation. multi - target domain adaptation CONJUNCTION personalized dialogue systems. few - shot image classification CONJUNCTION personalized dialogue systems. personalized dialogue systems CONJUNCTION few - shot image classification. real - world multitask learning applications EVALUATE-FOR method. few - shot image classification HYPONYM-OF real - world multitask learning applications. multi - target domain adaptation HYPONYM-OF real - world multitask learning applications. personalized dialogue systems HYPONYM-OF real - world multitask learning applications. Task is machine learning. Method is argmax centroid method. OtherScientificTerm is Wasserstein distance. ,"This paper proposes a method for centroid approximation to the argmax distribution of a random function. The method is based on the fact that the Wasserstein distance between the distribution and the true distribution can be approximated by centroid points. The authors show that this method is computationally efficient and can be applied to a wide range of machine learning problems, including few-shot image classification and multi-target domain adaptation. ","This paper proposes a new method for centroid approximation of the argmax distribution. The main idea is to use the Wasserstein distance between the centroid points of a random function and the argMax distribution as an objective function. The authors show that the proposed method can be applied to a wide range of real-world applications, including few-shot image classification, personalized dialogue systems, and multi-target domain adaptation. "
10951,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"adversarial manner USED-FOR preferences. preference vector USED-FOR reward function. pre - specified multi - objective reward functions FEATURE-OF preference vector. episodic learning problem USED-FOR problem. Markov decision process USED-FOR episodic learning problem. nearly minimax optimal regret bound EVALUATE-FOR model - based algorithm. nearly optimal trajectory complexity EVALUATE-FOR algorithm. Task are multi - objective reinforcement learning, and online setting. OtherScientificTerm are transitions, ( adversarial ) preference, policies, and preference - free exploration. ","This paper studies the problem of online multi-objective reinforcement learning in the online setting, where the goal is to learn a policy that maximizes a reward function that is a function of the preference vector of the reward function. In this setting, the authors propose to use an adversarial manner to learn the preferences in an online setting. The authors show that the regret bound of the proposed algorithm is O(1/\sqrt{T}) with a nearly optimal trajectory complexity of O(T).  ","This paper studies the problem of multi-objective reinforcement learning (MORL) in the online setting, where the goal is to learn a policy that maximizes a reward function over a set of transitions (e.g., preferences) in an online setting. The authors propose a model-based algorithm for MORL with nearly minimax optimal regret bound. They show that their algorithm is nearly optimal in terms of trajectory complexity and regret. "
10976,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"classification models COMPARE explanation of sequence generation models. explanation of sequence generation models COMPARE classification models. model - agnostic explanations USED-FOR text generation task. dialogue response generation HYPONYM-OF text generation task. open - ended sentences USED-FOR Dialog response generation. LERG USED-FOR sequence prediction. LERG USED-FOR explanations. unbiased approximation CONJUNCTION consistency. consistency CONJUNCTION unbiased approximation. consistency CONJUNCTION cause identification. cause identification CONJUNCTION consistency. explanation USED-FOR text generation. LERG USED-FOR text generation. explanation USED-FOR LERG. consistency HYPONYM-OF explanation. automaticand humanevaluation metrics EVALUATE-FOR task. method COMPARE methods. methods COMPARE method. task EVALUATE-FOR methods. task EVALUATE-FOR method. automaticand humanevaluation metrics EVALUATE-FOR methods. automaticand humanevaluation metrics EVALUATE-FOR method. LERG USED-FOR explicit and implicit relations. Method are generation model, and local explanation of response generation ( LERG ). OtherScientificTerm is human response. ","This paper proposes a method for dialogue response generation, where the goal is to generate a sequence of open-ended sentences with a human response. The authors propose a method called Local Explanation of Response Generation (LERG) which is a model-agnostic explanation of sequence generation models. LERG is based on a local explanation of response generation (LIRG) model, which is trained to predict the human response to a given sentence. LIRG is evaluated on the task of dialog response generation and achieves state-of-the-art performance.","This paper proposes a model-agnostic explanation of response generation (LERG) for dialogue response generation, which is a text generation task with open-ended sentences. LERG is based on the idea of local explanation (LERG), which is an extension of LERG to the dialogue generation task. The main idea is to use a model agnostic explanation of the response generation task, where the model is trained to predict the response of the dialogue. The model is then used to generate a sequence of explanations for the response generated by the generation model, and the explanation is used to guide the model to generate the correct response. The method is evaluated on a variety of metrics, including consistency, unbiased approximation, cause identification, and human response."
11001,SP:965413b1726617006317bbbec55673dd5d21812a,"distributed methods USED-FOR compressor. contraction property FEATURE-OF compressor. RandK HYPONYM-OF biased compressors. error compensation CONJUNCTION error feedback. error feedback CONJUNCTION error compensation. gradient compression CONJUNCTION acceleration. acceleration CONJUNCTION gradient compression. error compensation CONJUNCTION acceleration. acceleration CONJUNCTION error compensation. error compensation USED-FOR gradient compression. method COMPARE error compensated algorithms. error compensated algorithms COMPARE method. communication rounds EVALUATE-FOR method. Method are Gradient compression, error compensated gradient compression methods, and error compensated loopless Katyusha method. Metric are communication cost, and accelerated linear convergence rate. OtherScientificTerm is divergence. ","This paper studies the convergence of error-compressed gradient compression methods. The authors propose a new error-computed gradient compression method that combines error compensation and accelerated linear convergence. The convergence rate is shown to be O(1/\epsilon^2), which is faster than existing error-based gradient compression algorithms. The main contribution of the paper is a theoretical analysis of the convergence rate of the proposed method.  ","This paper proposes a new method for error compensated gradient compression. The main idea is to use an error-compressed version of RandK, which is a biased version of the standard RandK. The authors show that the convergence rate of their method is linear in the number of communication rounds. They also show that their method converges faster than other error compensated methods. "
11026,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"machine learning framework USED-FOR edge and neuromorphic computing paradigms. training complexity CONJUNCTION biological plausibility. biological plausibility CONJUNCTION training complexity. machine learning framework USED-FOR it. biological plausibility FEATURE-OF liquid state machine ( LSM ). training complexity EVALUATE-FOR liquid state machine ( LSM ). LSM USED-FOR internal weights. LSM COMPARE multi - layer neural networks. multi - layer neural networks COMPARE LSM. LSM USED-FOR model of brain computation. synaptic plasticity CONJUNCTION brain dynamics. brain dynamics CONJUNCTION synaptic plasticity. astrocytes USED-FOR synaptic plasticity. astrocytes USED-FOR brain dynamics. neuron - astrocyte liquid state machine ( NALSM)1 USED-FOR under - performance. self - organized near - critical dynamics USED-FOR neuron - astrocyte liquid state machine ( NALSM)1. self - organized near - critical dynamics USED-FOR under - performance. astrocyte model USED-FOR global feedback. NALSM COMPARE LSM methods. LSM methods COMPARE NALSM. accuracy EVALUATE-FOR LSM methods. accuracy EVALUATE-FOR NALSM. MNIST CONJUNCTION N - MNIST. N - MNIST CONJUNCTION MNIST. accuracy EVALUATE-FOR NALSM. N - MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION N - MNIST. braininspired machine learning methods USED-FOR deep learning. deep learning USED-FOR neuromorphic computing. Task is brain computation. Method are backpropagation of gradients, and backpropagation. OtherScientificTerm are brain networks, computationally optimal critical phase transition, neuronal activity, NALSM dynamics, branching factor, edge - of - chaos, and data - specific hand - tuning. ","This paper proposes a method for learning a neural network model of the brain using a self-organized near-critical dynamics. The proposed method is based on a neuron-astrocyte liquid state machine (NALSM) model, which is able to model the dynamics of neurons and astrocytes in the brain. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy on MNIST and Fashion-MNIST datasets.","This paper proposes a new model for learning a model of the brain, called neuron-astrocyte liquid state machine (NALSM), that can be used to model the dynamics of neurons and astrocytes in the brain. NALSM is based on the idea of edge-of-chaos (e.g., edge of chaos) and is trained by backpropagating the gradients of the internal weights of the LSM. The authors show that their model is able to learn a model that is computationally optimal in terms of the critical phase transition between neurons. They also show that the model can be applied to a variety of neuromorphic computing paradigms."
11051,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"class imbalance problem HYPONYM-OF learning node representations. asymmetric topological properties FEATURE-OF labeled nodes. graph data USED-FOR imbalance. Label Propagation algorithm USED-FOR node influence shift phenomenon. model - agnostic method ReNode USED-FOR topology - imbalance issue. influence conflict detection – based metric Totoro USED-FOR graph topology imbalance. method USED-FOR topology - imbalance issue. method USED-FOR semi - supervised node classification. topology - imbalance issue CONJUNCTION semi - supervised node classification. semi - supervised node classification CONJUNCTION topology - imbalance issue. graph neural networks ( GNNs ) USED-FOR topology imbalance. OtherScientificTerm are quantity imbalance, graph ( topology imbalance ), and class boundaries. Task are unknown topology - imbalance issue, semisupervised node classification learning, and quantityand topologyimbalance issues. ",This paper studies the class imbalance problem in graph representation learning. The authors propose to use influence conflict detection – based metric Totoro to measure the influence conflict between two classes of nodes in a graph. They then propose a label propagation algorithm to mitigate the influence shift phenomenon. The proposed method is applied to semi-supervised node classification tasks.  ,"This paper proposes a new method for learning graph representations based on topology-imbalance issue. The authors propose a label-propagation-based method ReNode, which uses the influence conflict detection-based metric Totoro to identify the topology imbalance issue. They also propose a method for semi-supervised node classification based on the influence shift phenomenon. The proposed method can be applied to both quantity and topology issues."
11076,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"d - dimensional lattice FEATURE-OF additive Gaussian noise. additive Gaussian noise USED-FOR piece - wise constant signals. partition recovery USED-FOR partition of the lattice. DCART - based procedure USED-FOR partition. regularity conditions USED-FOR DCART - based procedure. recursive dyadic partitions USED-FOR signal partition. recursive dyadic partitions USED-FOR rectangular sub - graphs. NP - hard exhaustive search method USED-FOR one. optimal regression tree estimator ( ORT ) USED-FOR partition estimator. DCART USED-FOR partition recovery. Task are signal detection or testing, and estimation. OtherScientificTerm are constancy regions of the unknown signal, and noise variance. Generic is method. ",This paper studies the problem of finding the partition of a d-dimensional lattice with additive Gaussian noise in the presence of piece-wise constant signals. The authors propose a DCART-based procedure to recover the partition under regularity conditions. The main contribution of the paper is to show that the partition recovery problem is NP-hard under certain regularity assumptions.  ,This paper studies the problem of learning the partition of a d-dimensional lattice of additive Gaussian noise. The authors propose a method to recover the signal partition of the lattice using a DCART-based procedure under regularity conditions. They show that the partition recovery is NP-hard in terms of the variance of the noise variance. They also provide an exhaustive search method for finding a partition estimator that recovers the partition.
11101,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"tasks EVALUATE-FOR deep learning models. causality - based training framework USED-FOR spurious correlations. interventional distribution COMPARE observational distribution. observational distribution COMPARE interventional distribution. Maximum Likelihood Estimation ( MLE ) USED-FOR interventional distribution. algorithms USED-FOR causal predictions. Implicit CMLE CONJUNCTION Explicit CMLE. Explicit CMLE CONJUNCTION Implicit CMLE. Implicit CMLE USED-FOR causal predictions. algorithms USED-FOR deep learning models. Explicit CMLE USED-FOR causal predictions. deep learning models USED-FOR causal predictions. Implicit CMLE HYPONYM-OF algorithms. Explicit CMLE HYPONYM-OF algorithms. observational data USED-FOR algorithms. observational data USED-FOR deep learning models. observational data USED-FOR causal predictions. observational data USED-FOR interventional distribution. Natural Language Inference ( NLI ) CONJUNCTION Image Captioning. Image Captioning CONJUNCTION Natural Language Inference ( NLI ). simulated data CONJUNCTION real - world tasks. real - world tasks CONJUNCTION simulated data. Natural Language Inference ( NLI ) HYPONYM-OF real - world tasks. Image Captioning HYPONYM-OF real - world tasks. CMLE methods COMPARE regular MLE method. regular MLE method COMPARE CMLE methods. CMLE methods USED-FOR spurious correlations. out - of - domain generalization EVALUATE-FOR regular MLE method. out - of - domain generalization EVALUATE-FOR CMLE methods. Generic is they. OtherScientificTerm are predictive clues, observed confounders, and expected negative log - likelihood. Method are general Structural Causal Model ( SCM ), and Counterfactual Maximum Likelihood Estimation ( CMLE ). ","This paper proposes a method to train deep neural networks in a causal-based training framework, where the goal is to avoid spurious correlations between the observed confounders and the predicted confounder. The authors propose two methods: (1) Counterfactual Maximum Likelihood Estimation (CMLE), which uses the interventional distribution to estimate the expected negative log-likelihood, and (2) Explicit CMLE, which is a variant of CMLE that is based on the Structural Causal Model (SCM). The authors show that the proposed methods outperform the regular MLE method in terms of out-of-domain generalization. ","This paper proposes two new CMLE algorithms, Explicit CMLE and Implicit CMLE, to improve the generalization performance of deep learning models in the context of causality-based training. The main contribution of the paper is to propose two CMLE-based algorithms that are based on the Structural Causal Model (SCM) and Counterfactual Maximum Likelihood Estimation (CMLE). The main idea is to use the observed confounders and the expected negative log-likelihood of the interventional distribution to estimate the CMLE of the observational distribution. The empirical results show that the proposed algorithms outperform the existing CMLE methods in terms of generalization. "
11126,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"multi - task learning USED-FOR learning. multi - task learning COMPARE single task learning. single task learning COMPARE multi - task learning. learning COMPARE single task learning. single task learning COMPARE learning. multi - task learning objective USED-FOR average loss. gradients FEATURE-OF task objectives. heuristics USED-FOR problem. heuristics USED-FOR task gradients. Conflict - Averse Gradient descent ( CAGrad ) USED-FOR average loss function. regular gradient descent ( GD ) CONJUNCTION multiple gradient descent algorithm ( MGDA ). multiple gradient descent algorithm ( MGDA ) CONJUNCTION regular gradient descent ( GD ). multiple gradient descent algorithm ( MGDA ) PART-OF multi - objective optimization ( MOO ) literature. multiple gradient descent algorithm ( MGDA ) PART-OF It. regular gradient descent ( GD ) PART-OF It. CAGrad COMPARE multi - objective gradient manipulation methods. multi - objective gradient manipulation methods COMPARE CAGrad. OtherScientificTerm are model structures, conflicting gradients, average gradient direction, convergence guarantee, Pareto - stationary point, and worst local improvement. Generic is objective. Method is multi - task model. ","This paper studies the multi-objective multi-task learning, where the objective is to minimize the average loss between tasks. The authors propose a method called CAGrad, which is based on the conflict-averse gradient descent (CAGrad) algorithm. The main contribution of the paper is the convergence analysis of the proposed method to a Pareto-stationary point. ","This paper studies the problem of multi-objective multi-task learning, where the objective is to learn the average of the gradients of all the tasks in a set of tasks. The main contribution of the paper is to propose a new method, Conflict-Averse Gradient Descent (CAGrad), which is based on the idea of conflict-averse gradient descent (CAGrad). CAGrad is a generalization of the popular multiple gradient descent algorithm (MGDA) that is used in the multi-agent optimization literature. The authors show that the proposed method can converge to a Pareto-stationary point, which is the worst local improvement of the worst-performing task. "
11151,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"Large language models USED-FOR few - shot learning. language models USED-FOR simple algorithmic concepts. strong priors FEATURE-OF teaching problem. program induction systems CONJUNCTION humans. humans CONJUNCTION program induction systems. GPT architectures CONJUNCTION program induction systems. program induction systems CONJUNCTION GPT architectures. complexity EVALUATE-FOR concept. GPT architectures CONJUNCTION humans. humans CONJUNCTION GPT architectures. artificial intelligence CONJUNCTION machine learning. machine learning CONJUNCTION artificial intelligence. language models CONJUNCTION machine teaching. machine teaching CONJUNCTION language models. machine teaching USED-FOR artificial intelligence. OtherScientificTerm are patterns of algorithmic nature, and Occam ’s razor. Generic is models. Task is learning. ",This paper studies the problem of few-shot learning in which the goal is to learn algorithmic concepts from a large language model. The authors show that the problem is difficult to solve in the presence of strong priors. They show that this problem can be solved in a way that is similar to the one faced by GPTs and GPT-based induction systems. They also show that it is possible to solve the problem in a similar way to a program induction system.   ,This paper studies the problem of few-shot learning in the context of language models. The authors show that the complexity of the learning problem is bounded by the strong priors of the language model. They also show that there is a strong correlation between the complexity and the number of examples in the training set. They show that this is due to the fact that there are many different types of examples that can be used to train the model.    The authors also provide a theoretical analysis of the problem. 
11176,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"perturbation USED-FOR Adversarial examples. feature representation USED-FOR robust and non - robust features. Information Bottleneck USED-FOR feature representation. Information Bottleneck USED-FOR way. noise variation USED-FOR feature unit. information flow PART-OF feature representation. noise variation magnitude USED-FOR information flow. human - perceptible semantic information FEATURE-OF they. attack mechanism USED-FOR gradient of non - robust features. OtherScientificTerm are adversarial examples, feature space, and distilled features. Task are adversarial prediction, model prediction, and model robustness. ","This paper studies the problem of adversarial training, where the goal is to improve the robustness of the model against adversarial examples. The authors propose to use the Information Bottleneck (IB) method to learn a feature representation that can distinguish between robust and non-robust features in the feature space. The proposed method is based on the idea of distilling the features into a set of distilled features, which can then be used to improve robustness.  The authors show that the information flow between the features is affected by the noise variation magnitude of the feature unit. They also show that this information flow can be used as a way to improve human-perceptible semantic information.","This paper studies the problem of adversarial prediction in the context of human-perceptible semantic information. In particular, the authors propose a novel information-bottleneck-based adversarial attack method to improve the robustness of the model. The proposed method is based on the Information Bottleneck (IB) method, which is a generalization of the information bottleneck. The main contribution of the paper is that the proposed method can be applied to both robust and non-robust features in the feature space.  "
11201,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"support vector machine ( SVM ) CONJUNCTION minimum Euclidean norm least squares regression. minimum Euclidean norm least squares regression CONJUNCTION support vector machine ( SVM ). models USED-FOR high - dimensional data. support vector machine ( SVM ) HYPONYM-OF approaches. approaches USED-FOR linear models. minimum Euclidean norm least squares regression HYPONYM-OF approaches. they PART-OF models. support vector proliferation USED-FOR independent feature models. super - linear lower bound USED-FOR support vector proliferation. dimension USED-FOR support vector proliferation. super - linear lower bound FEATURE-OF dimension. sharp phase transition FEATURE-OF Gaussian feature models. geometric characterization of the problem USED-FOR lp case. l1 variant PART-OF SVM. OtherScientificTerm are support vector, upper bounds, and phase transition. Generic is transition. ",This paper studies the problem of support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) with Gaussian feature models. The authors show that the support vector proliferation of independent feature models can be characterized by a super-linear lower bound on the dimension of the support vectors. They also provide a geometric characterization of the problem in the lp case and show the phase transition of Gaussian features.,This paper studies the problem of support vector proliferation in support vector machine (SVM) and minimum Euclidean norm least squares regression (MNNS) models. The main contribution of the paper is a new lower bound on the number of support vectors that can be generated by a Gaussian feature model. The lower bound is based on a geometric characterization of the problem. The upper bound is derived for the l1 variant of SVM and the l2 variant of the MNNS model. 
11226,SP:99f226a63902863c429cb7baefab09626d13921e,"Markov Decision Processes USED-FOR active pure exploration problem. instance - specific sample complexity EVALUATE-FOR algorithm. algorithm USED-FOR communicating MDPs. reduced exploration rate CONJUNCTION faster convergence. faster convergence CONJUNCTION reduced exploration rate. reduced exploration rate FEATURE-OF variant. ergodicity assumption USED-FOR variant. online setting USED-FOR navigation constraints. ergodic theorem USED-FOR non - homogeneous Markov chains. ergodic theorem USED-FOR analysis. OtherScientificTerm are system trajectory, and problem - dependent lower bound. Task are generative setting, and analysis of Markov Decision Processes. ","This paper studies the active exploration problem in Markov Decision Processes (MDPs), where the goal is to find an MDP with instance-specific sample complexity of $O(1/\sqrt{T})$. The main contribution of this paper is to provide an instance-dependent sample complexity lower bound for the MDP in the generative setting. The lower bound is based on the ergodicity assumption of the Markov chain, which is proved to be true for non-homogeneous Markov chains.  ","This paper studies the problem of communicating Markov Decision Processes (MDPs) in a generative setting, where the goal is to find an instance-specific MDP with instance specific sample complexity. The main contribution of the paper is to provide a lower bound on the sample complexity of the MDPs, which is based on the ergodic theorem. The lower bound is obtained by assuming that the system trajectory is non-homogeneous, and that the exploration rate is reduced by a factor of at least 1/\sqrt{1/2}. The paper also provides a variant of the lower bound for the online MDP setting, which has a reduced exploration rate and faster convergence. "
11251,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"entities CONJUNCTION first - order logical ( FOL ) queries. first - order logical ( FOL ) queries CONJUNCTION entities. low - dimensional spaces FEATURE-OF first - order logical ( FOL ) queries. low - dimensional spaces FEATURE-OF entities. conjunction CONJUNCTION disjunction. disjunction CONJUNCTION conjunction. disjunction CONJUNCTION negation. negation CONJUNCTION disjunction. geometry - based QE model USED-FOR FOL operations. query embedding model USED-FOR FOL operations. Cone Embeddings ( ConE ) HYPONYM-OF geometry - based QE model. Cone Embeddings ( ConE ) HYPONYM-OF query embedding model. negation HYPONYM-OF FOL operations. disjunction HYPONYM-OF FOL operations. conjunction HYPONYM-OF FOL operations. geometric complement operators USED-FOR negation operations. embedding space FEATURE-OF geometric complement operators. ConE COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ConE. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR ConE. Task is multi - hop reasoning over knowledge graphs. OtherScientificTerm are knowledge graphs, geometric shapes, Cartesian products of two - dimensional cones, conjunction and disjunction operations, closure of complement of cones, and cones. Method is geometry - based models. ",This paper proposes a method for embedding entities and first-order logical (FOL) queries into low-dimensional spaces. The proposed method is based on a geometry-based QE model that embeds FOL operations into two-dimensional cones and uses geometric complement operators to represent the conjunction and disjunction operations in the embedding space. Experiments show that the proposed method achieves state-of-the-art performance on several benchmark datasets. ,"This paper proposes a geometry-based query embedding model for first-order logical (FOL) queries. The model is based on Cone Embeddings (ConE), which embeds FOL operations in a low-dimensional space. ConE uses geometric complement operators to embed FOL operators in the embedding space. The authors show that ConE achieves state-of-the-art performance on several benchmark datasets. "
11276,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"linear - time Legendre transform USED-FOR numerical scheme. numerical scheme USED-FOR value iteration ( VI ) algorithm. linear - time Legendre transform USED-FOR value iteration ( VI ) algorithm. conjugate domain FEATURE-OF value iteration ( VI ) algorithm. time complexity EVALUATE-FOR algorithm. error EVALUATE-FOR algorithm. convergence EVALUATE-FOR algorithm. convergence CONJUNCTION time complexity. time complexity CONJUNCTION convergence. time complexity CONJUNCTION error. error CONJUNCTION time complexity. minimization operation PART-OF primal domain. discretization USED-FOR state and input spaces. discretization USED-FOR approach. time complexity EVALUATE-FOR approach. Task is stochastic nonlinear systems. OtherScientificTerm are state and input variables, and O(X + U ). Method is VI algorithm. ","This paper studies the value iteration (VI) algorithm for stochastic nonlinear systems. In this setting, the state and input variables are assumed to be Gaussian, and the goal is to estimate the value of the system. The authors propose to use the Legendre transform to compute the value in the conjugate domain of the state space. They show that this method converges to the optimal solution in O(X + U) time complexity. They also show that the error of this method is O(U^2) in the primal domain.",This paper studies the value iteration (VI) algorithm for stochastic nonlinear systems in the conjugate domain. The authors propose a linear-time Legendre transform-based method for value iteration in the primal domain. They show convergence and time complexity guarantees for the proposed method. They also provide a theoretical analysis of the convergence of the method. 
11301,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"framework USED-FOR multimodal representations. unlabeled data USED-FOR framework. unlabeled data USED-FOR multimodal representations. convolution - free Transformer architectures USED-FOR framework. multimodal representations USED-FOR downstream tasks. VideoAudio - Text Transformer ( VATT ) USED-FOR multimodal representations. raw signals USED-FOR VideoAudio - Text Transformer ( VATT ). audio event classification CONJUNCTION image classification. image classification CONJUNCTION audio event classification. image classification CONJUNCTION text - to - video retrieval. text - to - video retrieval CONJUNCTION image classification. video action recognition CONJUNCTION audio event classification. audio event classification CONJUNCTION video action recognition. video action recognition HYPONYM-OF downstream tasks. text - to - video retrieval HYPONYM-OF downstream tasks. image classification HYPONYM-OF downstream tasks. audio event classification HYPONYM-OF downstream tasks. multimodal contrastive losses USED-FOR VATT. convolution - free VATT COMPARE ConvNet - based architectures. ConvNet - based architectures COMPARE convolution - free VATT. ConvNet - based architectures USED-FOR downstream tasks. convolution - free VATT USED-FOR downstream tasks. Kinetics-400 EVALUATE-FOR VATT ’s vision Transformer. top-1 accuracy EVALUATE-FOR VATT ’s vision Transformer. videos CONJUNCTION images. images CONJUNCTION videos. VATT ’s audio Transformer USED-FOR waveform - based audio event recognition. mAP EVALUATE-FOR VATT ’s audio Transformer. Method are modality - agnostic, single - backbone Transformer, and supervised pre - training. Material are Kinetics-600, Kinetics-700, ImageNet, and AudioSet. Generic are Transformer, and model. ","This paper proposes VideoAudio-Text Transformer (VATT), a method for learning multimodal representations from unlabeled video and text data. VATT is a convolution-free Transformer architecture that uses raw raw signals to learn a representation of the data. The method is applied to audio event recognition, image classification, text-to-video retrieval, and video action recognition tasks. The proposed method achieves state-of-the-art performance on Kinetics-400 and ImageNet. ","This paper proposes a video-audio-text transformer (VATT) framework for learning multimodal representations from unlabeled data. The proposed method is based on a single-backbone Transformer architecture, which is trained with supervised pre-training. The method is evaluated on Kinetics-600, Kinetics700, and AudioSet, where it achieves state-of-the-art performance in terms of top-1 accuracy."
11326,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"quadratic time and space complexity EVALUATE-FOR self - attention mechanism. computation bottleneck FEATURE-OF pairwise dot products. computation bottleneck FEATURE-OF kernel machines. computational cost EVALUATE-FOR approximation schemes. accuracy EVALUATE-FOR approximation schemes. computation methods USED-FOR kernel machines. Nyström method USED-FOR computation. Nyström method USED-FOR non - positive semidefinite matrix. softmax structure CONJUNCTION Gaussian kernel. Gaussian kernel CONJUNCTION softmax structure. computation methods USED-FOR computational cost. matrix approximation error EVALUATE-FOR method. spectral norm FEATURE-OF matrix approximation error. spectral norm EVALUATE-FOR method. method COMPARE full self - attention. full self - attention COMPARE method. Long Range Arena benchmark EVALUATE-FOR method. Long Range Arena benchmark EVALUATE-FOR full self - attention. Method are Transformers, and Skyformer. OtherScientificTerm is computation resources. ",This paper proposes a new method for computing matrix approximation in the presence of a non-positive semidefinite matrix. The main idea is to use the Nyström method to compute the non-negative matrix in the form of a pairwise dot product. The authors show that the proposed method is computationally efficient compared to self-attention in terms of time and space complexity. ,"This paper proposes a new method for computing a non-positive semidefinite matrix with a softmax structure and a Gaussian kernel. The method is based on the Nyström method, which is used to compute a pairwise pairwise dot product. The authors show that the proposed method is computationally efficient compared to the full self-attention mechanism. They also show that their method is more accurate than full self attention. "
11351,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"setting PART-OF problems. image - based data augmentation USED-FOR invariance. image - based data augmentation CONJUNCTION expert - aware offline data augmentation approach. expert - aware offline data augmentation approach CONJUNCTION image - based data augmentation. expert - aware offline data augmentation approach USED-FOR feedback - sensitivity. image - based data augmentation USED-FOR image perturbations. image - based data augmentation PART-OF augmented policy cloning ( APC ) approach. method USED-FOR transfer 12 of complex high - DoF behaviors. method USED-FOR policy cloning. data - efficiency EVALUATE-FOR policy cloning. approach USED-FOR algorithms. policy cloning USED-FOR transfer 12 of complex high - DoF behaviors. policy cloning PART-OF algorithms. data - efficiency EVALUATE-FOR method. Method are data - augmentation technique, and behavioral cloning. Task is policy 3 cloning setting. Metric is data efficiency. OtherScientificTerm are expert, student policy, and expert trajectories. ","This paper proposes a data-augmented policy cloning (APC) approach for policy cloning in the policy cloning setting, where the goal is to learn a student policy that maximizes the transfer of high-DoF behaviors from expert trajectories to the student trajectories. The main idea is to use an image-based data augmentation technique to improve the robustness of the student policy to image perturbations. The proposed method is shown to be efficient in terms of data efficiency and transfer efficiency.  ","This paper proposes a data-augmented policy cloning (APC) approach for policy cloning. The main idea is to use an image-based data augmentation technique to improve the performance of policy cloning in the high-DoF setting, where the goal is to transfer high-doF behaviors from expert to student. The proposed method is based on a combination of two existing methods: (1) an expert-aware offline data augmentation approach, and (2) an image augmentation approach that augments the data with an image perturbation. The method is evaluated on a variety of high-DOF environments, and the proposed APC method is shown to be more efficient than the state-of-the-art."
11376,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,benchmarks CONJUNCTION simulated robotics environment. simulated robotics environment CONJUNCTION benchmarks. simulated robotics environment EVALUATE-FOR framework. benchmarks EVALUATE-FOR framework. Task is computer vision settings. Method is deep networks. ,"This paper proposes a method for training deep neural networks for image classification in computer vision settings. The proposed method is based on the observation that the performance of deep networks in vision tasks is highly dependent on the number of parameters of the input image. To address this issue, the authors propose to use a combination of a two-stage training strategy: 1) a pre-trained network that learns to predict the parameters of an input image, and 2) a network that predicts the parameters from the input images. The authors show that the proposed method outperforms the baselines on a variety of image classification tasks. ",This paper proposes a new framework for training deep neural networks for computer vision tasks. The proposed framework is based on the idea that the goal is to train a deep neural network that can be used in a variety of computer vision settings. The authors show that the proposed framework can achieve state-of-the-art performance on a range of benchmarks and simulated robotics environments.
11401,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"agents USED-FOR tasks. Reinforcement Learning ( RL ) USED-FOR agents. visual observations USED-FOR agents. visual observations USED-FOR tasks. data augmentation USED-FOR generalization. generalization FEATURE-OF RL. data augmentation USED-FOR off - policy RL algorithms. data augmentation USED-FOR instability. technique USED-FOR algorithms. ConvNets CONJUNCTION Vision Transformers ( ViT ). Vision Transformers ( ViT ) CONJUNCTION ConvNets. benchmarks CONJUNCTION robotic manipulation tasks. robotic manipulation tasks CONJUNCTION benchmarks. benchmarks EVALUATE-FOR Vision Transformers ( ViT ). benchmarks EVALUATE-FOR image - based RL. DeepMind Control Suite USED-FOR benchmarks. Vision Transformers ( ViT ) USED-FOR image - based RL. ConvNets USED-FOR image - based RL. state - of - the - art methods USED-FOR image - based RL. stability CONJUNCTION sample efficiency. sample efficiency CONJUNCTION stability. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. generalization EVALUATE-FOR state - of - the - art methods. stability FEATURE-OF ConvNets. sample efficiency FEATURE-OF ConvNets. method USED-FOR ConvNets. augmentation FEATURE-OF ConvNets. sample efficiency EVALUATE-FOR method. stability EVALUATE-FOR method. generalization EVALUATE-FOR method. method USED-FOR RL. ViT - based architectures USED-FOR method. ViT - based architectures USED-FOR RL. OtherScientificTerm are divergence, and high - variance Q - targets. Generic is problems. ","This paper proposes to use data augmentation to improve the generalization performance of off-policy reinforcement learning algorithms. The proposed method is based on the observation that high-variance Q-targets can lead to instability in the training process. To address this issue, the authors propose to use convolutional neural networks (convNets) and vision transformers (ViTs) to augment the training data. Experiments on the DeepMind Control Suite show that the proposed method outperforms the state-of-the-art methods. ","This paper proposes a data augmentation method to improve the generalization performance of off-policy RL algorithms. The proposed method is based on the observation that the performance of convolutional neural networks (ConvNets) can be affected by the high variance of high-variance Q-targets. To address this issue, the authors propose a method that augments ConvNets with ViT-based architectures. The method is evaluated on the DeepMind Control Suite and robotic manipulation tasks, where it outperforms state-of-the-art methods."
11426,SP:f8ca9d92c45adc4512381035856b445029e3080a,"local data USED-FOR joint model. minibatch sizes CONJUNCTION number of local updates. number of local updates CONJUNCTION minibatch sizes. WNs ’ and the server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and the server ’s update directions. communication rounds USED-FOR WNs. WNs USED-FOR local updates. algorithm USED-FOR ✏ -stationary solution. Õ ( ✏ 3/2 ) samples CONJUNCTION Õ ( ✏ 1 ) communication rounds. Õ ( ✏ 1 ) communication rounds CONJUNCTION Õ ( ✏ 3/2 ) samples. stochastic momentum estimator USED-FOR WN ’s and the server ’s directions. Õ ( ✏ 1 ) communication rounds USED-FOR algorithm. Õ ( ✏ 3/2 ) samples USED-FOR algorithm. near - optimal sample and communication complexities EVALUATE-FOR FL algorithm. WNs ’ and server ’s update directions CONJUNCTION minibatch sizes. minibatch sizes CONJUNCTION WNs ’ and server ’s update directions. Method are Federated Learning ( FL ), stochastic algorithms, and FL algorithms. Task is non - convex FL problem. Metric is sample and communication complexities. OtherScientificTerm is trade - off curve. ","This paper studies the Federated Learning (FL) problem in the non-convex setting. In this setting, the goal is to learn a joint model with the help of the server and the WNs. The main contribution of this paper is to study the sample complexity of the FL problem in this setting.    The main contributions of this work are:  1. The authors propose a new FL algorithm that achieves near-optimal sample complexity.  2. The proposed algorithm is based on a stochastic momentum estimator.  3. Theoretical analysis is provided to show the convergence of the proposed algorithm. ",This paper studies the problem of federated learning (FL) where the goal is to learn a joint model with the help of a server and a local network (NN). The authors propose a stochastic FL algorithm for this problem. The main contribution of the paper is to study the trade-off between sample complexity and communication complexity between the two parties. The authors show that the sample complexity is bounded by the number of local updates and the minibatch size of the local data. They then propose a new algorithm for FL that can be used to reduce the communication complexity. 
11451,SP:bd3eecb81a17af010f2d3555434990855c1810f2,"information - theoretical framework USED-FOR non - vacuous generalization bounds. non - vacuous generalization bounds USED-FOR large models. isotropic noise USED-FOR Stochastic Gradient Langevin Dynamics ( SGLD ). Stochastic Gradient Langevin Dynamics ( SGLD ) USED-FOR large models. noise structure USED-FOR SGLD. noise structure USED-FOR information - theoretical generalization bound. expected gradient covariance USED-FOR optimal noise covariance. optimal noise COMPARE empirical gradient covariance. empirical gradient covariance COMPARE optimal noise. information - theoretical bound USED-FOR optimization analysis. matrix analysis USED-FOR optimal noise covariance. OtherScientificTerm are constraint, and prior. ","This paper proposes a generalization bound for stochastic gradient Langevin dynamics (SGLD) with isotropic noise. The generalization bounds are based on an information-theoretic framework. The authors show that the expected gradient covariance of the optimal noise covariance is close to the empirical covariance, which is a constraint on the covariance matrix of the empirical gradient.    The authors also show that optimal noise can be computed in terms of the expected covariance. ",This paper studies the generalization of stochastic gradient Langevin dynamics (SGLD) with isotropic noise. The authors provide an information-theoretic generalization bound for SGLD. The generalization bounds are based on the expected gradient covariance of the expected noise covariance. They show that the expected covariance depends on the noise structure of the model. They also provide a matrix analysis of the optimal noise. 
11476,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,Learned video compression methods COMPARE video codecs. video codecs COMPARE Learned video compression methods. prediction mode CONJUNCTION fixed network framework. fixed network framework CONJUNCTION prediction mode. prediction mode USED-FOR learned video compression schemes. fixed network framework USED-FOR learned video compression schemes. model USED-FOR prediction modes. 3D motion vector fields USED-FOR weighted 9 trilinear warping. voxel flows USED-FOR weighted 9 trilinear warping. spatial - temporal space FEATURE-OF weighted 9 trilinear warping. 3D motion vector fields USED-FOR motion compensation 8 module. motion compensation 8 module USED-FOR versatile compression. voxel flows HYPONYM-OF 3D motion vector fields. temporal reference position FEATURE-OF voxel flows. flow prediction module USED-FOR motion trajectories. flow prediction module USED-FOR multiple - reference - frame predic12 tion. unified polynomial function USED-FOR flow prediction module. unified polynomial function USED-FOR motion trajectories. flow prediction module USED-FOR voxel flows. VLVC USED-FOR versatile compression. VLVC COMPARE Versatile Video Coding 17 ( VVC ) standard. Versatile Video Coding 17 ( VVC ) standard COMPARE VLVC. R - D performance EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. MS - SSIM EVALUATE-FOR R - D performance. R - D performance EVALUATE-FOR VLVC. MS - SSIM EVALUATE-FOR Versatile Video Coding 17 ( VVC ) standard. OtherScientificTerm is inter prediction modes. ,"This paper proposes a method for video compression based on voxel flows. The proposed method is based on 3D motion vector fields, which are used to predict the motion trajectories of frames in a video. The method is evaluated on a variety of video compression tasks and achieves state-of-the-art results.",This paper proposes a new video compression method called Versatile Video Coding 17 (VLVC) for video compression. VLVC is based on a 3D motion vector field and a motion compensation 8 module. The motion vector fields are used to predict the motion trajectories of voxel flows and the temporal reference position of the voxels. The proposed method is evaluated on MS-SSIM and R-D video compression benchmarks.
11501,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"regret EVALUATE-FOR MT - OMD. geometry FEATURE-OF regularizer. geometry FEATURE-OF task 3 variance. OMDs USED-FOR √ NT bound. Online Gradient Descent CONJUNCTION Exponentiated 7 Gradient. Exponentiated 7 Gradient CONJUNCTION Online Gradient Descent. Exponentiated 7 Gradient HYPONYM-OF OMD. Method are Online Mirror 1 Descent ( OMD ), and closed - form updates. OtherScientificTerm are time horizon, and σ. Generic is them. Material is real - world datasets. ","This paper studies Online Mirror 1 Descent (OMD) and Exponentiated 7 Gradient (EDG). The authors show that under certain assumptions on the time horizon and the geometry of the task, MT-OMD has a regret of $O(1/\sqrt{T}^T)$ for $T \infty$, where $T$ is the number of tasks and $\tilde{O}(\epsilon^2)$ is a regularization term. They show that the regret is bounded by $\Omega(T^2/T)$, which is an upper bound on the regret of the OMD. The authors also show that this regret can be bounded by $O(\sqrt {T} \log T})$ for a certain class of updates.  ","This paper studies the regret of Online Mirror 1 Descent (OMD) and Exponentiated 7 Gradient (EDG). The authors show that under certain assumptions on the time horizon and geometry, OMDs can be used to reduce the variance of the task 3 variance. They also show that OMD can also be used as a regularizer to improve the performance of MT-OMD. They show that MT-MMD can be combined with OMD to improve performance. "
11526,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,ε - error USED-FOR approximating d - dimensional ULD. finite summation of N smooth components PART-OF stronglyconvex potential. stronglyconvex potential USED-FOR underdamped Langevin diffusion ( ULD ). gradient evaluations USED-FOR discretization method. method USED-FOR strongly - log - concave distribution. gradient complexity EVALUATE-FOR gradient based sampling algorithms. method COMPARE gradient based sampling algorithms. gradient based sampling algorithms COMPARE method. gradient complexity EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. method COMPARE ULD approaches. ULD approaches COMPARE method. synthetic and real - world data EVALUATE-FOR ULD approaches. ,This paper studies the problem of estimating the underdamped Langevin diffusion (ULD) in strongly-convex potentials. The main contribution is a discretization method for approximating the strongly log-concave distribution of the ULD. The method is based on the finite summation of N smooth components in the strongly-Convasso potential. Theoretical analysis is provided to show the convergence of the method to the true distribution. Empirical results on synthetic and real-world data demonstrate the effectiveness of the proposed method.,"This paper proposes a new method for approximating underdamped Langevin diffusion (UDL) under strongly-convex potential. The main contribution of the paper is a new discretization method for the strongly-log-concave distribution. The method is based on a finite summation of the N smooth components of the potential, which is the sum of two smooth components. The authors show that the proposed method can be used to approximate the d-dimensional ULD with a lower bound on the eigenvalue of the log-likelihood of the ULD."
11551,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"recurrent networks USED-FOR neural dynamics. neural dynamics USED-FOR biological continual learning. feedforward and recurrent neural networks EVALUATE-FOR methods. weight regularization CONJUNCTION projected gradient descent. projected gradient descent CONJUNCTION weight regularization. projected gradient descent PART-OF method. weight regularization PART-OF method. catastrophic forgetting FEATURE-OF optimization. prior precision USED-FOR catastrophic forgetting. gradient projection USED-FOR NCL. prior precision USED-FOR gradient projection. Bayesian weight regularization USED-FOR NCL. projection based approaches USED-FOR continual learning problems. weight regularization techniques CONJUNCTION projection based approaches. projection based approaches CONJUNCTION weight regularization techniques. method USED-FOR continual learning problems. method COMPARE weight regularization techniques. weight regularization techniques COMPARE method. method COMPARE projection based approaches. projection based approaches COMPARE method. feedforward and recurrent networks USED-FOR continual learning problems. networks USED-FOR task - specific dynamics. Method are Biological agents, artificial agents, specific parameter regularizers, and Natural Continual Learning ( NCL ). OtherScientificTerm are parameter space, gradients, and biological circuits. Task is optimization journey. ","This paper proposes a method for continual learning in recurrent neural networks based on Bayesian weight regularization and projected gradient descent. The proposed method is based on the idea of catastrophic forgetting, which is a term that refers to the fact that the gradient of a task-specific task can be forgotten during the optimization process. The authors show that this catastrophic forgetting is caused by the prior precision of the gradients, and propose a method to mitigate this issue. The method is evaluated on a variety of continual learning tasks, and compared with a number of baselines.",This paper proposes a method for continual learning in biological continual learning (NCL). The method is based on Bayesian weight regularization and projected gradient descent. The main idea is to use the prior precision of the gradient projection to prevent catastrophic forgetting in NCL. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and performance. 
11576,SP:26de056be14962312c759be5d284ef235d660f9c,"Normalizing flows HYPONYM-OF invertible neural networks. change - of - volume terms FEATURE-OF invertible neural networks. low - dimensional manifold PART-OF high - dimensional ambient space. heuristics USED-FOR approaches. heuristics USED-FOR term. methods USED-FOR gradient. automatic differentiation CONJUNCTION numerical linear algebra. numerical linear algebra CONJUNCTION automatic differentiation. gradient FEATURE-OF term. automatic differentiation USED-FOR methods. approaches USED-FOR end - to - end nonlinear manifold learning. manifolds CONJUNCTION distributions. distributions CONJUNCTION manifolds. Method is maximum likelihood. OtherScientificTerm are modelling mismatch, invertibility requirement, Injective flows, lowto high - dimensional spaces, and volume - change term. Generic are manifold, and model. Task is out - of - distribution detection. ",This paper studies the problem of nonlinear manifold learning with normalizing flows. Injective flows are a type of invertible neural networks where the input is a low-dimensional manifold and the output is a high-dimensional ambient space. The authors propose to use a change-of-volume term to model the mismatch between the low-dimension and high-dimension of the input space. They show that this term can be used to improve the performance of non-linear models.   ,This paper studies the problem of end-to-end nonlinear manifold learning for invertible neural networks. The main contribution of the paper is to study the change-of-volume term of the normalizing flows in the context of the invertibility requirement. The authors show that the gradient of the volume-change term depends on the number of dimensions of the low-dimensional manifold and the high-dimensional ambient space. They also provide a theoretical analysis of the convergence rate of the gradient.   
11601,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"response time FEATURE-OF physical computational elements. inference CONJUNCTION learning. learning CONJUNCTION inference. framework USED-FOR inference. framework USED-FOR learning. inference CONJUNCTION learning. learning CONJUNCTION inference. networks of slow components USED-FOR learning. principle USED-FOR quasi - instantaneous inference. phased plasticity CONJUNCTION network relaxation phases. network relaxation phases CONJUNCTION phased plasticity. prospective energy function USED-FOR disentangled neuron and synapse dynamics. continuous - time, leaky neuronal dynamics CONJUNCTION continuously active, local plasticity. continuously active, local plasticity CONJUNCTION continuous - time, leaky neuronal dynamics. error backpropagation USED-FOR deep cortical networks. continuous - time, leaky neuronal dynamics USED-FOR error backpropagation. benchmark datasets EVALUATE-FOR learning. fully - connected and convolutional architectures USED-FOR learning. robustness EVALUATE-FOR model. model USED-FOR physical realization. spatio - temporal substrate imperfections FEATURE-OF robustness. spatio - temporal substrate imperfections FEATURE-OF model. OtherScientificTerm are neurons, response lag, delayed processing of stimuli, timing mismatch, instructive signals, biological neurons, membrane potential, and network depth. Method are hierarchical models of cortical networks, physical dynamical systems, and Latent Equilibrium. ","This paper proposes a method to model the dynamics of neurons in deep neural networks. The method is based on the Latent Equilibrium (LE) framework, which is a framework for learning and inference in neural networks with slow components. Theoretical results show that the proposed method is quasi-instantaneous and can be used for inference and learning. Experiments are conducted on CIFAR-10 and ImageNet.   ","This paper proposes a new framework for learning deep neural networks of slow components that can be used for quasi-instantaneous inference. The proposed method is based on a disentangled neuron and synapse dynamics that is disentanglement of the dynamics of the neurons and their synapses. Theoretical analysis is provided to show that the proposed method can be applied to a variety of neural networks, including fully-connected and convolutional architectures. Experiments are conducted on several datasets to demonstrate the effectiveness of the method. "
11626,SP:b937901e3230b14e36975fbab0658a52bdac4977,"Graph neural network ( GNN ) USED-FOR graph classification. node representation USED-FOR rooted subtree. GNN USED-FOR node representation. 1 - WL USED-FOR node representation. 1 - WL CONJUNCTION GNN. GNN CONJUNCTION 1 - WL. representation USED-FOR graph. rooted subtree representations PART-OF representation. rooted subtrees USED-FOR nontree graph. NGNN USED-FOR graph. rooted subgraphs COMPARE rooted subtrees. rooted subtrees COMPARE rooted subgraphs. rooted subgraphs FEATURE-OF graph. GNN USED-FOR subgraph representation. NGNN USED-FOR subgraph representation. GNN USED-FOR subgraph. NGNN USED-FOR local subgraph. GNN USED-FOR NGNN. subgraph representations USED-FOR whole - graph representation. NGNN COMPARE 1 - WL. 1 - WL COMPARE NGNN. NGNN USED-FOR r - regular graphs. NGNN COMPARE GNNs. GNNs COMPARE NGNN. GNNs COMPARE NGNN. NGNN COMPARE GNNs. time complexity EVALUATE-FOR GNNs. time complexity EVALUATE-FOR NGNN. NGNN HYPONYM-OF plug - and - play framework. plug - and - play framework CONJUNCTION base GNNs. base GNNs CONJUNCTION plug - and - play framework. NGNN CONJUNCTION base GNNs. base GNNs CONJUNCTION NGNN. NGNN COMPARE base GNNs. base GNNs COMPARE NGNN. benchmark datasets EVALUATE-FOR base GNNs. benchmark datasets EVALUATE-FOR NGNN. OtherScientificTerm are neighboring node features, subtrees, and subtree. Method is Nested Graph Neural Networks ( NGNNs ). ","This paper proposes a new method for learning the representation of the nodes in a graph. The proposed method is based on the observation that the root node representation of a nontree graph can be decomposed into a set of subgraphs, which are then used to train a graph neural network (GNN) to predict the node features. The method is evaluated on a variety of graph classification tasks.   ",This paper proposes a new method for graph neural networks (GNNs) called Nested Graph Neural Networks (NGNNs). The main idea of NGNNs is to use a subgraph representation of each node in the graph as a representation of the whole graph. The subgraph representations are then used to train a GNN-based graph neural network. The proposed method is evaluated on a variety of benchmark datasets and compared with a number of baselines.
11651,SP:7b8284aa82022ce73802bfc57238b0d82031b226,"nesting FEATURE-OF forward or reverse KL divergence. NVI USED-FOR importance sampling strategies. heuristics USED-FOR sampler. NVI USED-FOR intermediate densities. NVI USED-FOR multimodal distribution. amortized inference USED-FOR hierarchical deep generative models. heuristics USED-FOR amortized inference. heuristics USED-FOR hidden Markov model. learned annealing path USED-FOR NVI. log average weight CONJUNCTION effective sample size. effective sample size CONJUNCTION log average weight. log average weight FEATURE-OF sample quality. effective sample size FEATURE-OF sample quality. sample quality EVALUATE-FOR nested objectives. Method are nested variational inference ( NVI ), and nested importance samplers. ","This paper proposes nested variational inference (NVI), which is a novel approach to sampling from a multi-modal distribution in deep generative models. NVI is based on the observation that the KL divergence between the forward and reverse KL divergences can be expressed as a function of the importance sampling strategy. The authors show that nested importance sampling strategies can be used to sample from intermediate densities of the multimodal distribution, which can then be used for amortized inference. The main contribution of the paper is a theoretical analysis of the effect of the number of importance samplers on the effective sample size. ","This paper proposes a new approach to amortized variational inference (NVI) for hierarchical deep generative models. The main contribution of the paper is to introduce nested importance samplers (i.e., importance sampling strategies) that can be combined with other heuristics to improve the quality of the samples. The authors show that nested importance sampling can improve the sample quality in terms of the log average weight, effective sample size, and the number of samples. They also show that a learned annealing path can be used to learn the importance sampling strategy. "
11676,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"packing bound USED-FOR Piyavskii - Shubert algorithm. packing bound USED-FOR upper bound. local worst - case analysis USED-FOR learning tasks. instance - dependent lower bound COMPARE worst - case lower bounds. worst - case lower bounds COMPARE instance - dependent lower bound. Lipschitz setting FEATURE-OF worst - case lower bounds. local worst - case analysis USED-FOR instance - dependent lower bound. OtherScientificTerm is Lipschitz function f. Metric is optimal sample complexity. Method are computationally tractable DOO algorithm, and packing and integral bounds. ","This paper studies the packing bound for the Piyavskii-shubert algorithm. The main result is a packing bound of the form $O(1/\sqrt{n})$ where $n$ is the number of samples and $\epsilon$ is a function of $f$, where $f$ is an Lipschitz function. The authors show that the packing and integral bounds are equivalent to the best-known packing bound and the worst-case lower bound, respectively.   The main contribution of the paper is to show that this packing bound is equivalent to an instance-dependent lower bound in the case where the LPschitz value of the function f is known. ",This paper studies the packing bound of the Piyavskii-shubert algorithm for the Lipschitz setting. The main contribution of the paper is to provide an instance-dependent lower bound for the worst-case lower bound. The lower bound is based on a local worst case analysis of the algorithm. The authors show that the worst case lower bound can be better than the best one for the best-case upper bound. 
11701,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"Deep neural networks ( DNNs ) USED-FOR tasks. Credible uncertainty estimation USED-FOR risk - sensitive applications. network USED-FOR uncertainty estimation. attack COMPARE adversarial attacks. adversarial attacks COMPARE attack. white - box setting USED-FOR scenario. Deep Ensembles CONJUNCTION MC - Dropout. MC - Dropout CONJUNCTION Deep Ensembles. vanilla softmax score CONJUNCTION Deep Ensembles. Deep Ensembles CONJUNCTION vanilla softmax score. uncertainty estimation methods USED-FOR attacks. vanilla softmax score HYPONYM-OF uncertainty estimation methods. MC - Dropout HYPONYM-OF uncertainty estimation methods. Deep Ensembles HYPONYM-OF uncertainty estimation methods. SelectiveNet CONJUNCTION selective classification architecture. selective classification architecture CONJUNCTION SelectiveNet. SelectiveNet USED-FOR attack. MobileNetV2 CONJUNCTION EfficientNetB0. EfficientNetB0 CONJUNCTION MobileNetV2. architectures EVALUATE-FOR attack. EfficientNetB0 HYPONYM-OF architectures. MobileNetV2 HYPONYM-OF architectures. OtherScientificTerm are network ’s capacity, and uncertainty estimation damage. Method is DNN. Material are black - box regime, and ImageNet. Task is uncertainty estimations. ","This paper proposes a method to attack uncertainty estimation in deep neural networks (DNNs). The method is based on the observation that uncertainty estimation is a useful tool for risk-sensitive applications, but it can be harmful to the accuracy of the model. The authors propose to use uncertainty estimation as a way to attack DNNs. They show that the uncertainty estimation can be used as an adversarial attack on DNN models, and that it is more effective than adversarial attacks. ","This paper proposes a method to attack uncertainty estimation in deep neural networks (DNNs). The method is based on the fact that the uncertainty estimation of a DNN can be affected by the network’s capacity. The authors show that the attack can be applied to a variety of DNNs, including Deep Ensembles, MC-Dropout, and SelectiveNet. They also show that their method can be used in a black-box setting, where the network is trained in a white-box regime. The attack is shown to be more effective than adversarial attacks."
11726,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"stochastic block models USED-FOR community detection. network information USED-FOR they. model USED-FOR networks. Task are community detection problem, and real - world applications. OtherScientificTerm are network, well - connected ‘ communities ’, and network structure. Method are detection algorithm, voting approaches, streaming stochastic block model ( StSBM ), voting algorithms, and streaming belief - propagation ( STREAMBP ) approach. Material is synthetic and real data. ","This paper studies the problem of community detection in deep neural networks, where the goal is to find a set of well-connected ‘communities’ in the network. The authors propose to use a streaming stochastic block model (StSBM) to model the community distribution in a streaming fashion. The proposed method is based on streaming belief propagation (STREAMBP), which is an extension of STREAMBP to the community detection problem. Theoretical results are provided to show that the proposed method outperforms existing community detection methods in terms of accuracy. ","This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on the streaming belief-propagation (STREAMBP) approach, which is a streaming belief propagation (SPG) method. The authors show that the proposed method outperforms the state-of-the-art in both synthetic and real-world datasets."
11751,SP:b1163857a6b06047c3531ab762642fcbed6dd294,predictor space FEATURE-OF regularization cost. l2 regularization USED-FOR regularization cost. l2 regularization USED-FOR predictor space. linear neural networks USED-FOR parameterizations of linear predictors. sparse linear ConvNets CONJUNCTION residual networks. residual networks CONJUNCTION sparse linear ConvNets. representation cost FEATURE-OF sparse linear ConvNets. representation cost FEATURE-OF residual networks. lp quasi - norms CONJUNCTION group quasi - norms. group quasi - norms CONJUNCTION lp quasi - norms. architecture CONJUNCTION parameterization. parameterization CONJUNCTION architecture. group quasi - norms CONJUNCTION k - support - norm. k - support - norm CONJUNCTION group quasi - norms. k - support - norm CONJUNCTION elastic net. elastic net CONJUNCTION k - support - norm. parameterization USED-FOR representation cost. regularizers USED-FOR linear predictors. architecture USED-FOR representation cost. group quasi - norms CONJUNCTION elastic net. elastic net CONJUNCTION group quasi - norms. l2 regularization USED-FOR representation cost. elastic net HYPONYM-OF regularizers. k - support - norm HYPONYM-OF regularizers. group quasi - norms HYPONYM-OF regularizers. lp quasi - norms HYPONYM-OF regularizers. Method is parameterizations. Task is reverse problem. ,"This paper studies the problem of regularization of linear predictors in linear neural networks. The authors show that the l2 regularization can be used to reduce the representation cost in the predictor space. They show that this regularization is equivalent to a l2-regularization in the residual space, and that it can be combined with other regularizers such as group quasi-norms, k-support-norm, and elastic nets. They also show that linear convolutional networks with residuals have the same representation cost as sparse linear convnets.","This paper studies the problem of estimating the representation cost of linear neural networks (LNNs) with different parameterizations of linear predictors. In particular, the authors show that the l2 regularization can be used to estimate the cost of the representation in the predictor space. They also show that l2-regularization can also be used as a regularizer for linear networks with different architecture and parameterization. Finally, they show that under certain assumptions on the architecture and the parameterization of the LNNs, they can estimate the representation of the predictor space.  "
11776,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"approach USED-FOR knowledge graphs ( KGs ). approach COMPARE case - based reasoning. case - based reasoning COMPARE approach. case - based reasoning USED-FOR artificial intelligence ( AI ). non - parametric approach USED-FOR crisp logical rules. graph path patterns USED-FOR non - parametric approach. NELL-995 CONJUNCTION FB-122. FB-122 CONJUNCTION NELL-995. accuracy EVALUATE-FOR method. NELL-995 EVALUATE-FOR models. FB-122 EVALUATE-FOR models. model USED-FOR low data settings. OtherScientificTerm are binary relation, and relation. ","This paper proposes a non-parametric approach to learn logical rules for knowledge graphs. The proposed approach is based on graph path patterns, which are used to model the relationship between nodes in a knowledge graph. The method is evaluated on two datasets, namely NELL-995 and FB-122, and achieves state-of-the-art performance. ",This paper proposes a non-parametric approach to learn logical rules for knowledge graphs (KGs). The key idea is to use graph path patterns to learn a set of logical rules that can be used to model the relationship between two KGs. The proposed method is evaluated on two datasets (NELL-995 and FB-122) and compared with the state-of-the-art methods.
11780,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,Transformer architecture PART-OF entity linking model. CoNLL CONJUNCTION TAC - KBP. TAC - KBP CONJUNCTION CoNLL. entity linking datasets EVALUATE-FOR model. Transformer architecture CONJUNCTION input perturbations. input perturbations CONJUNCTION Transformer architecture. negative entity candidates CONJUNCTION Transformer architecture. Transformer architecture CONJUNCTION negative entity candidates. end - to - end entity linking CONJUNCTION entity linking. entity linking CONJUNCTION end - to - end entity linking. entity linking HYPONYM-OF settings. end - to - end entity linking HYPONYM-OF settings. in - domain training data USED-FOR settings. Material is Wikipedia links. ,This paper proposes a Transformer-based entity linking model for entity linking. The proposed model is based on a transformer architecture and is trained on CoNLL and TAC-KBP datasets. The experiments show that the proposed model achieves state-of-the-art performance on entity linking datasets.  ,"This paper proposes a transformer-based entity linking model for entity linking. The proposed model is trained on two datasets, CoNLL and TAC-KBP, and it is shown to perform better than the state-of-the-art in terms of entity linking performance. The authors also show that the proposed model can be used for end-to-end entity linking as well as in-domain training. "
11784,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"ensemble Active Learning methods USED-FOR acquisition. intermediate training checkpoints USED-FOR ensembles. training data subset search USED-FOR large labeled datasets. acquisition functions CONJUNCTION ensemble configurations. ensemble configurations CONJUNCTION acquisition functions. initialization schemes CONJUNCTION acquisition functions. acquisition functions CONJUNCTION initialization schemes. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ImageNet HYPONYM-OF image classification benchmarks. CIFAR-10 HYPONYM-OF image classification benchmarks. CIFAR-100 HYPONYM-OF image classification benchmarks. ResNet-101 CONJUNCTION DenseNet121. DenseNet121 CONJUNCTION ResNet-101. data subsets USED-FOR deep models. ResNet-18 ensemble USED-FOR data subsets. DenseNet121 HYPONYM-OF deep models. ResNet-101 HYPONYM-OF deep models. training data distribution USED-FOR large scale vision tasks. Method are Deep Neural Networks ( DNNs ), and DNNs. Generic are datasets, they, approach, and dataset. Task is DNN ’s optimization. OtherScientificTerm is training distribution. Metric is training time. ","This paper proposes to use ensemble active learning (EAML) to improve the performance of deep neural networks (DNNs) on large-scale image classification tasks. The main idea is to use an ensemble of DNNs to search for training data subsets that are close to the training distribution. The authors show that this approach can improve performance on CIFAR-10, ImageNet, and ResNet-18 on ImageNet and Cifar-100, and on ResNet101 and DenseNet-121 on ResNets.","This paper proposes a new ensemble active learning method for training deep neural networks (DNNs) on large labeled datasets. The proposed method is based on the idea of training ensembles of data subsets, where each data subset is a subset of the training data, and each data set is a set of data points. The authors show that the proposed method outperforms the state-of-the-art on CIFAR-10 and ImageNet datasets.  "
11788,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"routing algorithm USED-FOR capsule networks. mechanism USED-FOR routing. sequential iterative routing CONJUNCTION concurrent iterative routing. concurrent iterative routing CONJUNCTION sequential iterative routing. inverted dot - product attention USED-FOR routing. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. routing algorithms COMPARE method. method COMPARE routing algorithms. method COMPARE CNN. CNN COMPARE method. it COMPARE CNN. CNN COMPARE it. method COMPARE it. it COMPARE method. CIFAR-10 HYPONYM-OF benchmark datasets. CIFAR-100 HYPONYM-OF benchmark datasets. benchmark datasets EVALUATE-FOR method. capsule model COMPARE CNNs. CNNs COMPARE capsule model. task EVALUATE-FOR CNNs. task EVALUATE-FOR capsule model. task EVALUATE-FOR recognizing digits. neurons per layer USED-FOR CNNs. capsule model USED-FOR recognizing digits. overlayed digit images USED-FOR task. overlayed digit images USED-FOR recognizing digits. capsule networks USED-FOR complex real - world tasks. Method are Layer Normalization, and Capsules - Inverted - Attention - Routing. OtherScientificTerm is normalization. ","This paper proposes Capsules-Inverted-Attention-Routing (CATR), a novel routing algorithm for capsule networks. CATR is based on the idea of normalizing the attention of each layer in a capsule network by using inverted dot-product attention. Theoretical analysis and experimental results show that the proposed method is able to achieve state-of-the-art performance on CIFAR-10/100 and ImageNet. ","This paper proposes Capsules-Inverted-Attention-Routing, a new routing algorithm for capsule networks. The proposed method is based on the inverted dot-product attention (i.e., the attention of each layer of a capsule network is divided into two parts, one for each layer, and the other for the last layer). The proposed approach is evaluated on CIFAR-10 and CIFar-100 datasets, where it outperforms the state-of-the-art CNN-based approach."
11792,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"statistical physics FEATURE-OF parallel tempering technique. history PART-OF joint hyperparameter / model - parameter space. method USED-FOR dropout and learning rate optimization. Task are Hyperparameter optimization, and training of deep architectures. Method is deep architectures. Generic are methods, and model. OtherScientificTerm are hyperparameter space, nonlocal paths, hyperparameters, correlated noise, temperature, and overfitting. Metric are computational cost, resistance, and absolute validation error. ","This paper studies hyperparameter optimization in deep neural networks. The authors propose a new method that scales linearly with the temperature of the hyperparameters in the model-parameter space. They show that the temperature is determined by the history of non-local paths in the joint hyperparametrization space, and show that this history is correlated with correlated noise. They then propose a parallel tempering technique that can be applied to both dropout and learning rate optimization. The proposed method is shown to be computationally efficient.","This paper studies the problem of hyperparameter optimization in deep neural networks. In particular, the authors propose a new temperature-tempering method to reduce the computational cost, resistance, and absolute validation error in hyperparameters optimization. The authors show that temperature tempering can be used to reduce dropout and learning rate optimization. They also provide a theoretical analysis of the effect of the temperature on the dropout rate.  "
11796,SP:beba754d96cc441712a5413c41e98863c8abf605,"Minimum Risk Training ( MRT ) CONJUNCTION Generative Adversarial Networks ( GAN ). Generative Adversarial Networks ( GAN ) CONJUNCTION Minimum Risk Training ( MRT ). Reinforcement learning ( RL ) USED-FOR text generation tasks. machine translation ( MT ) HYPONYM-OF text generation tasks. methods USED-FOR MT. RL methods USED-FOR MT. OtherScientificTerm are expected reward, pre - trained parameters, translation, training signal, and distribution curve. ",This paper proposes to use minimum risk training (MRT) and adversarial training to improve the performance of machine translation models. The authors show that MRT and GANs can be used together to improve performance on MT tasks. The main contribution of the paper is a theoretical analysis of the distribution of the expected reward in the training set and the distribution curve of the training signal. The analysis is based on the fact that the expected rewards of the model and its parameters are close to each other in terms of the log-likelihood of the target task. ,"This paper studies the problem of machine translation (MT) from the perspective of minimum risk training (MRT) and GANs. In particular, the authors study the problem in terms of the distribution curve of the expected reward, which is defined as the distance between the translation parameters and the training signal. The authors show that the distribution of the reward depends on the number of parameters of the training set and the amount of training data. They also provide a theoretical analysis of the trade-off between the reward distribution and the expected translation parameters.   "
11800,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,reinforcement learning algorithms CONJUNCTION applications. applications CONJUNCTION reinforcement learning algorithms. closed - form characterizations FEATURE-OF asymptotic variances. closed - form characterizations USED-FOR Q - value estimates. policies USED-FOR estimation errors. confidence regions USED-FOR Q - value and optimal value functions. exploration strategy COMPARE benchmark approaches. benchmark approaches COMPARE exploration strategy. Task is statistical inference. Method is policy exploration strategy. OtherScientificTerm is Q estimates. ,"This paper studies the problem of estimating the Q-values in reinforcement learning problems. In particular, the authors propose to use a policy exploration strategy to estimate the Q values in the confidence region of the optimal value function. The exploration strategy is based on the fact that the Q value estimates can be approximated using closed-form characterizations of the asymptotic variances. The authors show that this exploration strategy leads to a lower estimation error compared to existing methods. ",This paper studies the problem of estimating the Q-value of a Q-policy from a set of Q-values. The authors propose an exploration strategy to estimate the Q value of a given policy from the confidence region of the value function. The exploration strategy is based on a closed-form characterization of the asymptotic variances between the Q values and the optimal value functions. The method is evaluated on a variety of benchmark datasets and shows that the exploration strategy can improve the performance of the proposed method.
11804,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"Top - k recommendation USED-FOR largescale recommender systems. Cold - start and efficiency issues USED-FOR largescale recommender systems. Cold - start and efficiency issues FEATURE-OF Top - k recommendation. hybrid recommendation methods USED-FOR cold - start issues. efficiency EVALUATE-FOR online recommendation. online recommendation EVALUATE-FOR they. real latent space FEATURE-OF similarity search. cold - start items CONJUNCTION warm - start ones. warm - start ones CONJUNCTION cold - start items. cold - start users CONJUNCTION cold - start items. cold - start items CONJUNCTION cold - start users. collaborative generated hashing ( CGH ) USED-FOR efficiency. it USED-FOR recommendation settings. CGH USED-FOR hash functions. Minimum Description Length ( MDL ) principle USED-FOR CGH. Minimum Description Length ( MDL ) principle USED-FOR hash functions. CGH USED-FOR marketing strategy. generative step USED-FOR CGH. MDL principle USED-FOR compact and informative binary codes. content data USED-FOR compact and informative binary codes. recommendations COMPARE baselines. baselines COMPARE recommendations. public datasets EVALUATE-FOR recommendations. application USED-FOR marketing. public datasets EVALUATE-FOR baselines. OtherScientificTerm are side information, and binary codes. ",This paper proposes collaborative generated hashing (CGH) to address the cold-start and efficiency issues of top-k recommendation. CGH is based on the Minimum Description Length (MDL) principle and uses a generative step to generate compact and informative binary codes. It is shown that CGH can be used for cold-starting users and warm-start users in the online recommendation setting. Experiments are conducted on public datasets to demonstrate the effectiveness of the proposed method.,"This paper proposes collaborative generated hashing (CGH) to improve the cold-start and efficiency of Top-k recommendation systems. The authors propose a new method to generate a set of binary codes for each item in the top-k list, which can be used to improve cold start and efficiency. The proposed method is based on the Minimum Description Length (MDL) principle, which is used to generate compact and informative binary codes. Experiments on public datasets show that the proposed method outperforms baselines. "
11808,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"adversarial domain adaptation CONJUNCTION multi - task learning. multi - task learning CONJUNCTION adversarial domain adaptation. adversarial domain adaptation USED-FOR AITL. multi - task learning USED-FOR AITL. genomic information USED-FOR drug response. large pre - clinical pharmacogenomics datasets CONJUNCTION clinical datasets. clinical datasets CONJUNCTION large pre - clinical pharmacogenomics datasets. transfer learning USED-FOR large pre - clinical pharmacogenomics datasets. drug response outcome FEATURE-OF clinical data. cancer cell lines HYPONYM-OF large pre - clinical pharmacogenomics datasets. AITL USED-FOR input and output discrepancies. adversarial inductive transfer learning method USED-FOR input and output discrepancies. AITL HYPONYM-OF adversarial inductive transfer learning method. AITL USED-FOR precision oncology. AITL COMPARE pharmacogenomics and transfer learning baselines. pharmacogenomics and transfer learning baselines COMPARE AITL. Method is Adversarial Inductive Transfer Learning ( AITL ). Generic is method. OtherScientificTerm are input and output spaces, and output space. Task is pharmacogenomics. Material is pre - clinical and clinical datasets. ",This paper proposes an adversarial inductive transfer learning (AITL) method for pharmacogenomics and transfer learning for precision oncology. AITL is based on adversarial domain adaptation and multi-task learning. The main idea is to use adversarial data augmentation to improve the transfer learning performance. The proposed method is evaluated on two large pre-clinical and clinical datasets and compared with several transfer learning baselines.,This paper proposes a new method for transfer learning from pharmacogenomics to precision oncology. The proposed method is based on adversarial domain adaptation and multi-task learning. The authors show that the proposed method AITL outperforms the state-of-the-art in terms of accuracy on both input and output data. They also demonstrate that the method can be applied to both pre-clinical and clinical datasets.
11812,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,"sample complexity EVALUATE-FOR model - free approaches. fictitious trajectory rollouts USED-FOR dynamics model. fictitious trajectory rollouts USED-FOR model - free approaches. stochastic dynamics model USED-FOR uncertainty quantification. erroneously optimistic regions PART-OF dynamics model. uncertainty aware ensemble of dynamics models USED-FOR next state predictions. ensemble of dynamics models USED-FOR policy update. simulated robotic locomotion HYPONYM-OF benchmark tests. benchmark tests EVALUATE-FOR approach. approach COMPARE model - based one. model - based one COMPARE approach. approach COMPARE model - free algorithms. model - free algorithms COMPARE approach. model - free algorithms COMPARE model - based one. model - based one COMPARE model - free algorithms. learning rates CONJUNCTION asymptotic behaviour. asymptotic behaviour CONJUNCTION learning rates. asymptotic behaviour EVALUATE-FOR MBPGE. learning rates EVALUATE-FOR MBPGE. Method are RL methods, and policy gradient methods. OtherScientificTerm are next state prediction, and real and virtual total reward. ","This paper proposes MBPGE, a model-free policy gradient algorithm that uses an uncertainty-aware ensemble of dynamics models to estimate the next state predictions for policy update. The authors show that the proposed method achieves better sample complexity than model-based methods in simulated robotic locomotion tasks. The proposed method is evaluated on a variety of simulated robotic tasks.   ","This paper proposes a new model-free algorithm for the problem of estimating the next state prediction of a stochastic dynamics model. The key idea is to use an ensemble of dynamics models to estimate the uncertainty quantification of the dynamics model, and then use the ensemble to update the policy update at each time step. The authors show that the proposed method outperforms the state-of-the-art in terms of sample complexity, learning rate, and asymptotic behavior."
11816,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"neural network models USED-FOR visual features. adversarial examples CONJUNCTION corrupted images. corrupted images CONJUNCTION adversarial examples. class - conditional reconstruction of the input USED-FOR adversarial examples. class - conditional reconstruction of the input USED-FOR corrupted images. misclassification CONJUNCTION reconstruction error. reconstruction error CONJUNCTION misclassification. Reconstructive Attack USED-FOR detection mechanism. reconstructive attack USED-FOR undetected adversarial examples. success rate EVALUATE-FOR reconstructive attack. CapsNets COMPARE convolutional networks. convolutional networks COMPARE CapsNets. adversarial examples USED-FOR CapsNets. visual similarity USED-FOR reconstructive attack. features USED-FOR CapsNets. Material is Adversarial examples. Method is class - conditional reconstruction. Generic is attacks. OtherScientificTerm are perturbations, and human perception. ","This paper proposes a method to detect adversarial examples by reconstructing corrupted images from the original image. The proposed method is based on class-conditional reconstruction of the input, which can be used as a detection mechanism. Experiments show that the proposed method outperforms the state-of-the-art methods in terms of adversarial detection accuracy. ","This paper proposes a novel method for reconstructing adversarial examples from corrupted images. The proposed method is based on the class-conditional reconstruction of the input, which can be used as a detection mechanism. The authors show that the proposed method outperforms the state-of-the-art in terms of success rate and accuracy. "
11820,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"gradient descent USED-FOR parameter space. function space FEATURE-OF kernel gradient descent. gradient descent USED-FOR neural network. linear model USED-FOR wide networks. linear model USED-FOR neural network. full batch gradient descent USED-FOR neural network. Edge of Chaos HYPONYM-OF initialization. initialization CONJUNCTION activation function. activation function CONJUNCTION initialization. activation function USED-FOR NTK. initialization USED-FOR NTK. OtherScientificTerm are Neural Tangent Kernel ( NTK ), and network depth. ",This paper studies the gradient descent in the parameter space of a neural network in the function space of the Neural Tangent Kernel (NTK). The authors show that the NTK can be viewed as a function of the depth of the network and the initialization of the neural network. The authors also show that gradient descent can be done in the full batch gradient descent.    The main contribution of the paper is a theoretical result that shows that the neural networks with depth at least $d$ can be approximated by a linear model with a certain initialization. ,"This paper studies the problem of neural network initialization in the function space of a neural network. The authors show that the neural network can be initialized with a neural tangent kernel (NTK), which is a generalization of the Neural Tangent Kernel. They show that NTK can be used as an initialization for neural networks. They also provide a theoretical analysis of the effect of NTK on the depth of the network. "
11824,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"self - supervised method USED-FOR sentence representations. injection of linguistic knowledge USED-FOR self - supervised method. sentence structures USED-FOR semantic meaning. Multiple linguistic frameworks USED-FOR sentence structures. compositional words operations USED-FOR semantic meaning. embeddings USED-FOR semantic. linguistic views USED-FOR embeddings. OtherScientificTerm are linguist diversity, views, and sentence outward form. ",": This paper proposes a self-supervised method to learn sentence embeddings from multiple linguistic frameworks. The method is based on the idea that the embedding of a sentence can be learned from multiple views of the same sentence, i.e. from a variety of linguistic views. The authors propose to use a combination of two techniques to learn the sentence embedding. The first technique is to learn a set of embedding functions that are independent of the sentence structure. The second one is to use compositional words operations to extract the semantic meaning from the sentence.   ","This paper proposes a self-supervised method to learn sentence embeddings from a set of linguistic views. The authors propose to use multiple linguistic frameworks to learn the embedding embedding of a sentence. The embedding is composed of three components: (1) a sentence embedding, (2) a word embedding and (3) a syntactic embedding. The proposed method is evaluated on a variety of datasets.   "
11828,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"methods USED-FOR niche domains. product or movie review datasets EVALUATE-FOR sentiment classification solutions. finance HYPONYM-OF niche domains. Transfer learning USED-FOR new domains. NLP transfer learning USED-FOR financial sentiment classification. FinBERT HYPONYM-OF language model. financial sentiment classification task EVALUATE-FOR language model. FinancialPhrasebank dataset USED-FOR financial sentiment classification task. BERT USED-FOR language model. OtherScientificTerm is domainspecific language. Generic is models. Material are labeled data, specific domain, and large training data sets. ",This paper presents a transfer learning approach for sentiment classification in financial domain. The authors propose to use BERT language model to learn a new language model for financial sentiment classification. The proposed method is evaluated on the FinancialPhrasebank dataset. The results show that the proposed method outperforms the state-of-the-art methods on the task. ,"This paper proposes a new method for transfer learning for financial sentiment classification. The authors propose a new language model, FinBERT, which is trained on the FinancialPhrasebank dataset. The model is trained using a BERT-based language model. The proposed method is evaluated on a variety of sentiment classification tasks.  "
11832,SP:31c9c3a693922d5c3448e80ade920391dce261f9,musical scores CONJUNCTION text lyrics. text lyrics CONJUNCTION musical scores. Generative models USED-FOR singing voice. Generative models USED-FOR singing voice synthesis. singing voice synthesis USED-FOR singing voice waveforms. musical scores USED-FOR singing voice waveforms. text lyrics USED-FOR singing voice waveforms. pre - assigned scores CONJUNCTION lyrics. lyrics CONJUNCTION pre - assigned scores. pre - assigned scores USED-FOR singing voice generation. singing voice generation HYPONYM-OF alternative. training and inference time EVALUATE-FOR singing voice generation. pipeline USED-FOR tasks. source separation and transcription models USED-FOR data preparation. adversarial networks USED-FOR audio generation. source separation and transcription models CONJUNCTION adversarial networks. adversarial networks CONJUNCTION source separation and transcription models. adversarial networks CONJUNCTION metrics. metrics CONJUNCTION adversarial networks. Method is unconditioned or weakly conditioned singing voice generation schemes. ,This paper proposes a method for singing voice synthesis based on pre-assigned musical scores and text lyrics. The method is based on a pre-trained generative model that generates singing voice waveforms from text lyrics and musical scores. The authors propose to use adversarial training to improve the training and inference time of the singing voice generation model. Experiments show that the proposed method outperforms the state-of-the-art methods in terms of training time and inference cost.,"This paper proposes a new method for singing voice generation. The method is based on pre-assigned musical scores and text lyrics, which are used to generate singing voice waveforms. The authors show that the method can be applied to a wide range of tasks, such as singing voice synthesis, singing voice recognition, and singing voice identification. The proposed method is evaluated on a variety of tasks.   "
11836,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,computer vision CONJUNCTION audio - understanding. audio - understanding CONJUNCTION computer vision. deep neural networks USED-FOR computer vision. deep neural networks USED-FOR audio - understanding. adversarial attacks FEATURE-OF they. defensive tensorization HYPONYM-OF adversarial defence technique. latent high order factorization of the network USED-FOR adversarial defence technique. Randomization USED-FOR latent subspace. Randomization USED-FOR dense reconstructed weights. sparsity CONJUNCTION perturbations. perturbations CONJUNCTION sparsity. randomization USED-FOR perturbations. approach CONJUNCTION techniques. techniques CONJUNCTION approach. approach CONJUNCTION neural architecture. neural architecture CONJUNCTION approach. adversarial training HYPONYM-OF techniques. image classification benchmarks EVALUATE-FOR approach. audio classification task CONJUNCTION binary networks. binary networks CONJUNCTION audio classification task. binary networks EVALUATE-FOR approach. audio classification task EVALUATE-FOR approach. Generic is network. , adversarial training is an important problem in computer vision and audio-to-text tasks. This paper proposes a novel adversarial defense technique based on the latent high order factorization of the network. The proposed method is based on randomizing the latent subspace of the weights in a dense reconstructed weights. Experiments on image classification and audio classification tasks show the effectiveness of the proposed method. ,"This paper proposes a novel adversarial defense technique for deep neural networks. The proposed method is based on a latent factorization of the weights of the network, which is used to reconstruct the weights in the latent subspace. The authors show that the proposed method can be applied to both image classification and audio classification tasks. The method is evaluated on both synthetic and real-world datasets, and it is shown to be competitive with existing methods."
11840,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,Deep learning based approaches USED-FOR urban spatiotemporal forecasting problems. graph attention network CONJUNCTION transformer. transformer CONJUNCTION graph attention network. clustered graph transformer framework USED-FOR unsmoothness issue. transformer PART-OF encoder - decoder architecture. transformer PART-OF clustered graph transformer framework. graph attention network PART-OF clustered graph transformer framework. structural components USED-FOR architectures. structural components USED-FOR deep learning models. architectures PART-OF deep learning models. gradient - based clustering method USED-FOR feature extractors. gradient - based clustering method USED-FOR spatial domain. multi - view position encoding USED-FOR periodicity and closeness of urban time series data. multi - view position encoding USED-FOR temporal domain. real datasets EVALUATE-FOR method. real datasets EVALUATE-FOR baselines. method COMPARE baselines. baselines COMPARE method. ride - hailing business FEATURE-OF real datasets. OtherScientificTerm is unsmoothness issue of urban data. Material is urban data. ," time series forecasting is an important problem in the transportation industry. However, the unsmoothness issue of urban time series data is a major challenge. This paper proposes a novel clustering-based approach to address this issue. The proposed method is based on a graph attention network (GNN) and a transformer. The GNN encodes the time series into a set of embeddings, which are then used as input to a decoder. The transformer is then used to predict the position of the embedding embedding in time series.   The proposed approach is evaluated on two real-world datasets and compared with several baselines. The results show that the proposed method outperforms the baselines in terms of accuracy and time complexity.","This paper proposes a clustering-based clustering method for spatiotemporal forecasting of urban time series data. The proposed method is based on a graph attention network (GNN) and a graph transformer (GNT). The GNN is used to extract features from the time series, and the transformer is used as an encoder-decoder architecture for the encoder and decoder. The graph transformer is then used to encode the position of time series in the spatial domain. The authors show that the proposed method outperforms the state-of-the-art baselines on a variety of real datasets. "
11844,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. deep neural networks USED-FOR fitted Q iteration ( FQI ) algorithm. algorithmic and statistical rates of convergence FEATURE-OF action - value functions. action - value functions FEATURE-OF iterative policy sequence. FQI USED-FOR iterative policy sequence. geometric rate FEATURE-OF algorithmic error. deep neural network USED-FOR action - value function. experience replay CONJUNCTION target network. target network CONJUNCTION experience replay. target network USED-FOR DQN. Minimax - DQN algorithm USED-FOR zero - sum Markov game. DQN USED-FOR Minimax - DQN algorithm. Method are deep reinforcement learning, and deep Q - network ( DQN ) algorithm. OtherScientificTerm is statistical error. ","This paper studies the convergence of the fitted Q iteration (FQI) algorithm with deep Q-network (DQN) in reinforcement learning. In particular, the authors show that FQI converges at a geometric rate of $O(1/\sqrt{T})$ with a probability of $\Omega(T)$, where $T$ is the action-value function of the policy. The authors then show that DQN is equivalent to the Minimax-FQN algorithm, which is a generalization of the Minimalax-DQI algorithm. The main contribution of the paper is the theoretical analysis of the convergence rate of the algorithm.","This paper studies the convergence rate of the fitted Q iteration (FQI) algorithm for deep Q-network (DQN) in a zero-sum Markov game. The main contribution of the paper is a theoretical analysis of the geometric rate of convergence of the FQI algorithm. The authors show that the rate is bounded by the geometric error of the action-value function of the target network. They also show that for the Minimax DQN algorithm, the rate converges at a geometric rate. "
11848,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"weighted sum USED-FOR semantics. attention mechanism USED-FOR natural language. PhraseTransformer HYPONYM-OF attention architecture. second phase USED-FOR inductive bias. WMT16 English - German translation task EVALUATE-FOR Transformer. WMT16 English - German translation task EVALUATE-FOR PhraseTransformer. BLEU EVALUATE-FOR Transformer. BLEU EVALUATE-FOR PhraseTransformer. Transformer COMPARE PhraseTransformer. PhraseTransformer COMPARE Transformer. WMT16 English - German translation task EVALUATE-FOR BLEU. OtherScientificTerm are self - attention, word compositions, compositional attentions, hypernodes, and non - linearity. Task is attention. Generic is first phase. Method is non - linear attention. ","This paper proposes a new attention architecture for natural language translation. The proposed architecture is based on a two-stage attention mechanism. The first phase is a self-attention module that learns a weighted sum of the sum of words in a sentence, and the second phase is an inductive bias module that selects the words with the highest similarity. The authors show that the proposed method outperforms the state-of-the-art Transformer model on the WMT16 English-German translation task. ","This paper proposes a new attention architecture, PhraseTransformer, for natural language translation. The main idea is that the first phase of the attention is a weighted sum of the sum of word compositions, and the second phase is an inductive bias. The authors show that this second phase can be used to improve the performance of Transformer on the WMT16 English-German translation task. "
11852,SP:622b0593972296a95b630a4ece1e959b60fec56c,"modular neural network architecture MAIN USED-FOR algorithms. input - output examples USED-FOR algorithms. neural controller USED-FOR variable - length input tape. neural controller PART-OF MAIN. general domain - agnostic mechanism USED-FOR selection of modules. general domain - agnostic mechanism USED-FOR MAIN. input tape layout CONJUNCTION parallel history tape. parallel history tape CONJUNCTION input tape layout. parallel history tape USED-FOR It. input tape layout USED-FOR It. memoryless controller USED-FOR it. input - output examples USED-FOR reinforcement learning. reinforcement learning USED-FOR MAIN architecture. it USED-FOR policies. algorithmic tasks EVALUATE-FOR MAIN. OtherScientificTerm are modules, random access, and tape locations. ","This paper proposes a modular neural network architecture called MAIN, which is capable of learning from input-output examples in reinforcement learning. The proposed architecture is based on a neural controller that selects modules from a variable-length input tape and uses a memoryless controller to control the selection of modules. The architecture is trained using reinforcement learning and achieves state-of-the-art performance on several tasks. ","This paper proposes a modular neural network architecture MAIN, which can be applied to a wide range of tasks. The main idea is to use a neural controller to control the length of the input tape and the number of modules in the network. The controller is a memoryless controller that can be used to select the modules from a large number of input-output examples. The authors also propose a general domain-agnostic mechanism for selection of modules. The proposed method is evaluated on a variety of tasks, including reinforcement learning and machine learning. "
11856,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"Quantization USED-FOR Deep Neural Networks. Quantized floating point representations COMPARE fixed point representations. fixed point representations COMPARE Quantized floating point representations. Quantized floating point representations USED-FOR dynamic range. fixed point representations USED-FOR dynamic range. technique USED-FOR Deep Neural Networks. floating point arithmetic FEATURE-OF quantization. quantization FEATURE-OF Deep Neural Networks. floating point arithmetic USED-FOR Deep Neural Networks. Monte Carlo Arithmetic USED-FOR inference computation. relative standard deviation FEATURE-OF neural network loss. CIFAR-10 and ImageNet datasets EVALUATE-FOR pre - trained image classification models. pre - trained image classification models EVALUATE-FOR method. CIFAR-10 and ImageNet datasets EVALUATE-FOR method. loss of significance FEATURE-OF weight parameter sets. Metric is accuracy. OtherScientificTerm are parameter distributions, network topology, Monte Carlo trials, and network topologies. Method is MCDA. ",This paper proposes to use quantized floating point arithmetic to improve the accuracy of deep neural networks. The main idea is to use Monte Carlo arithmetic to compute the relative standard deviation of the neural network loss. Theoretical analysis is provided to show that quantization improves the performance of neural networks in terms of accuracy and loss of significance. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method. ,This paper proposes a new quantization technique for deep neural networks. The key idea is to use Monte Carlo arithmetic to compute the relative standard deviation of the neural network loss. The authors show that this can be used to estimate the accuracy of the weights of a neural network. They also show that their method can be applied to a wide range of network topologies.  
11860,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"Model - based reinforcement learning ( MBRL ) USED-FOR data - efficiently learning control of continuous tasks. function approximators CONJUNCTION planning schemes. planning schemes CONJUNCTION function approximators. function approximators USED-FOR MBRL. planning schemes USED-FOR MBRL. dynamics models USED-FOR task. method USED-FOR mismatch issue. dynamics model training USED-FOR method. Method are MBRL framework, and forward dynamics model. Task are objective mismatch issue, and downstream control task. OtherScientificTerm are Objective mismatch, and objective mismatch. Metric is downstream control performance. ","This paper studies model-based reinforcement learning (MBRL) in continuous control tasks, where the goal is to learn to solve a continuous control task with a forward dynamics model. The authors propose a method to address the objective mismatch issue in MBRL, which is that the downstream control task may not be the same as the original task. To address this issue, the authors propose to use a forward model to model the dynamics of the downstream task. The forward model is trained by minimizing the KL divergence between the forward model and the true dynamics model, and then the downstream model is used to estimate the objective. Theoretical analysis is provided to show that the proposed method can improve the downstream performance. Experiments are conducted on a variety of continuous control problems.  ","This paper proposes a model-based reinforcement learning (MBRL) framework for learning control of continuous tasks. The main idea is to use a forward dynamics model to model the dynamics of the downstream task, and then use the forward model to learn a planning scheme for downstream control. The authors show that the downstream control performance can be improved by learning a planning strategy that minimizes the objective mismatch between the forward and downstream tasks. They also show that this planning strategy can be used to improve downstream performance."
11864,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"CNN classifiers USED-FOR adversarial attacks. targeted blackbox transfer - based attack EVALUATE-FOR undefended ImageNet models. intermediate feature distributions FEATURE-OF CNNs. adversarial attacks USED-FOR intermediate feature distributions. Generic are network, and methodology. Task are adversarial attack, and attacking process. Method is CNN architecture. Metric is transferability. ",This paper studies the transferability of adversarial attacks on undefended ImageNet models. The authors propose a targeted blackbox transfer-based attack based on adversarial perturbations to the intermediate feature distributions of CNNs. They show that the attack is transferable in the sense that it can be applied to any CNN architecture. The transferability is defined as the number of attacks that can be transferred to a different CNN architecture in a given time step.  ,"This paper proposes a new blackbox transfer-based adversarial attack method for undefended CNNs. The main idea is to attack the intermediate feature distributions of the CNNs, which are the ones that are most likely to be vulnerable to adversarial attacks. The method is based on the fact that the intermediate features of CNNs can be easily transferred across different networks. The authors show that the transferability of the attack can be measured by the number of attacks that can be applied to the same network. They also show that this transferability can be controlled by the size of the network and the attack size. "
11868,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,approach USED-FOR reinforcement learning policies. latent behavioral space FEATURE-OF Wasserstein distances ( WDs ). Wasserstein distances ( WDs ) USED-FOR approach. dual formulation USED-FOR score functions. dual formulation USED-FOR WD. score functions USED-FOR policy optimization. dual formulation USED-FOR algorithms. smoothed WDs CONJUNCTION dual formulation. dual formulation CONJUNCTION smoothed WDs. WD regularizers USED-FOR algorithms. Behavior - Guided Policy Gradient CONJUNCTION Behavior - Guided Evolution Strategies. Behavior - Guided Evolution Strategies CONJUNCTION Behavior - Guided Policy Gradient. regularizers PART-OF on - policy algorithms. Behavior - Guided Evolution Strategies COMPARE methods. methods COMPARE Behavior - Guided Evolution Strategies. Behavior - Guided Policy Gradient HYPONYM-OF on - policy algorithms. Behavior - Guided Evolution Strategies HYPONYM-OF on - policy algorithms. Method is demo1. ,"This paper proposes to use Wasserstein distance (WD) as a regularizer for policy optimization in reinforcement learning. Specifically, the authors use a dual formulation of the score functions of the policy and policy gradient to compute the WSD. The authors show that this dual formulation can be used to improve the performance of existing on-policy optimization algorithms.  ",This paper proposes a new method for learning policies in the latent space of the Wasserstein distance (WD) space. The main idea is to use a dual formulation of the score function of the WD as well as the smoothed WDs as regularizers for policy optimization. The authors show that the proposed method outperforms the state-of-the-art on-policy algorithms in terms of performance. 
11872,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"deep neural networks PART-OF supervised learning. optimization algorithm USED-FOR deep learning. interpolation property USED-FOR optimization algorithm. it USED-FOR adaptive learning - rate. SGD USED-FOR ALI - G. learning - rate EVALUATE-FOR ALI - G. SGD COMPARE ALI - G. ALI - G COMPARE SGD. constant hyper - parameter USED-FOR ALI - G. constant hyper - parameter USED-FOR learning - rate. convergence guarantees FEATURE-OF ALI - G. ALI - G USED-FOR stochastic convex setting. convergence guarantees FEATURE-OF stochastic convex setting. wide residual networks CONJUNCTION densely connected networks. densely connected networks CONJUNCTION wide residual networks. architectures CONJUNCTION tasks. tasks CONJUNCTION architectures. SVHN data set EVALUATE-FOR wide residual network. CIFAR data sets EVALUATE-FOR wide residual networks. SNLI data set EVALUATE-FOR Bi - LSTM. CIFAR data sets EVALUATE-FOR densely connected networks. ALI - G COMPARE SGD. SGD COMPARE ALI - G. ALI - G COMPARE adaptive methods. adaptive methods COMPARE ALI - G. manually tuned learning - rate schedules USED-FOR SGD. ALI - G USED-FOR drop - in replacement. ALI - G PART-OF deep learning framework. OtherScientificTerm are empirical loss, Adaptive Learning - rates, and decay schedule. Method is differentiable neural computer. ","This paper proposes ALI-G, an adaptive learning rate schedule for deep neural networks. The main idea is to use a constant hyper-parameter to control the learning-rate of ALI, and then use the learning rate as a drop-in replacement for the empirical loss. The authors prove convergence guarantees for ALI in the stochastic convex setting, and show that ALI can be used as an efficient learning rate replacement for SGD.   ",This paper proposes a new adaptive learning rate for deep neural networks. The main idea of the paper is to use a differentiable neural computer with a constant hyper-parameter to compute the learning-rate of a neural network. The learning rate is computed by using the interpolation property of the neural computer. The authors prove convergence guarantees for the learning rate of the algorithm in the stochastic convex setting. They also show that the learning rates of ALI-G and SGD converge to the same convergence rate.
11876,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,Forming perceptual groups CONJUNCTION individuating objects in visual scenes. individuating objects in visual scenes CONJUNCTION Forming perceptual groups. Forming perceptual groups USED-FOR visual intelligence. computations USED-FOR ability. connections USED-FOR perceptual grouping. high - level object cues USED-FOR perceptual grouping. synthetic visual tasks EVALUATE-FOR neural network architectures. learning USED-FOR networks. bottom - up connections USED-FOR networks. Horizontal connections USED-FOR straining. top - down connections USED-FOR learning. high - level object cues USED-FOR learning. model USED-FOR perceptual groups. interactions USED-FOR perceptual groups. interactions PART-OF model. Material is visual scenes. Generic is task. OtherScientificTerm is Gestalt cues. Task is incremental grouping. ,"-based models are trained on a set of synthetic visual tasks, where the goal is to learn how to identify objects in a scene. The task is to identify the objects in the scene that have the most interactions with each other. The authors propose to use a combination of top-down and bottom-up connections to learn this task.    The authors show that their method outperforms baselines in terms of accuracy on the synthetic tasks. ","This paper proposes a new task of visual perceptual grouping, where the goal is to identify groups of objects in a visual scene that can be easily grouped into groups based on high-level object cues. The task is formulated as an incremental grouping task, where each object is represented as a set of objects, and the task is to find a group of objects that are most likely to belong to the same group. The authors propose a new network architecture for this task, which is based on a top-down network with horizontal connections and a bottom-up network with vertical connections. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method.  "
11880,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"` 1 or ` 0 regularizers USED-FOR weight sparsity. ` 0 regularizer USED-FOR parameter sparsity. complex optimization techniques USED-FOR it. gradient descent USED-FOR ` 1 regularizer. Hoyer measure USED-FOR compressed sensing problems. DeepHoyer HYPONYM-OF sparsity - inducing regularizers. DeepHoyer regularizers USED-FOR sparser neural network models. DeepHoyer USED-FOR element - wise and structural pruning. Task is sparse and efficient neural network models. Method is neural network models. OtherScientificTerm are scaling of parameter values, gradients, and sparsity. Metric are shrinking rate, and accuracy level. ","This paper studies the problem of weight sparsity in neural network models. The authors propose a new regularizer called DeepHoyer, which is based on the Hoyer measure. Theoretical results are provided to show that the proposed regularizer can be used to improve the accuracy of sparse neural networks. ","This paper studies the problem of weight sparsity in neural network models. The authors propose a new regularizer called DeepHoyer, which is based on the Hoyer measure. They show that it can be used to reduce the shrinking rate of the accuracy of neural networks. They also show that the proposed method can also be used for element-wise and structural pruning. "
11884,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"data - dependent O(log T ) regret bound FEATURE-OF strongly convex functions. Adam USED-FOR data - dependent O(log T ) regret bound. controlled step size USED-FOR strong convexity. hyperparameters USED-FOR SAdam. RMSprop USED-FOR strongly convex functions. SC - RMSprop HYPONYM-OF RMSprop. SC - RMSprop USED-FOR SAdam. optimizing strongly convex functions CONJUNCTION deep networks. deep networks CONJUNCTION optimizing strongly convex functions. OtherScientificTerm are O ( √ T ) regret bound, and time horizon. Metric is data - dependent logarithmic regret bound. Generic is method. ",This paper studies the problem of optimizing strongly convex functions with a fixed step size. The main result is a data-dependent O(log T) regret bound on the regret of the function with the fixed step-size. The regret is shown to depend on the time horizon and the number of samples.   The main contribution of the paper is to show that the regret is dependent on the step size and the hyperparameters. ,"This paper studies the data-dependent O(log T) regret bound for strongly convex functions. The main contribution of the paper is to provide a new regret bound that can be used to optimize strongly-convex functions with a controlled step size. The regret bound is obtained by using the SC-RMSprop method, which is based on the control of the step size of the function. The authors show that the regret bound can be obtained for a variety of hyperparameters, and that it can be achieved for any time horizon.  "
11888,SP:9f89501e6319280b4a14b674632a300805aa485c,BlockBERT HYPONYM-OF BERT model. BERT model USED-FOR modeling long - distance dependencies. BlockBERT USED-FOR modeling long - distance dependencies. memory consumption CONJUNCTION training time. training time CONJUNCTION memory consumption. attention heads USED-FOR shortor long - range contextual information. sparse block structures PART-OF attention matrix. BERT USED-FOR model. sparse block structures USED-FOR model. paragraph lengths FEATURE-OF benchmark question answering datasets. BlockBERT COMPARE BERT - based model. BERT - based model COMPARE BlockBERT. prediction accuracy EVALUATE-FOR BERT - based model. BlockBERT COMPARE RoBERTa. RoBERTa COMPARE BlockBERT. memory USED-FOR BlockBERT. RoBERTa HYPONYM-OF BERT - based model. training time EVALUATE-FOR BlockBERT. prediction accuracy EVALUATE-FOR BlockBERT. ,This paper proposes a novel BERT-based model for long-range contextualization. The proposed method is based on the idea of using sparse block structures in the attention matrix of BERT to model long-distance dependencies. The method is evaluated on a variety of benchmark question answering datasets and achieves state-of-the-art performance. ,"This paper proposes a new BERT-based model BlockBERT, which aims to reduce the memory consumption and training time of the BERT model. The proposed model is based on a sparse block structure, which is used to model the short- and long-range contextual information. The model is trained with a single attention head, and the attention heads are organized into two heads: one for short-term contextual information and one for long-term information.  The proposed method is evaluated on a number of benchmark question answering datasets, and compared with the state-of-the-art RoBERTa model."
11892,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"test accuracy EVALUATE-FOR pruning approaches. test accuracy EVALUATE-FOR pruning. generalization EVALUATE-FOR pruning. pruning CONJUNCTION regularizing. regularizing CONJUNCTION pruning. generalization FEATURE-OF over - parameterized networks. noise USED-FOR regularizing. noise USED-FOR pruning. OtherScientificTerm are Pruning neural network parameters, overfitting, and instability. Method is pruned model. ",This paper studies the effect of pruning on the generalization ability of over-parameterized neural networks. The authors show that pruning leads to a reduction in generalization error in the presence of overfitting. They further show that regularizing the weights of the pruned network improves generalization performance.  ,"This paper studies the generalization properties of over-parameterized neural networks. The authors study the effect of pruning and regularizing on generalization. They show that under certain conditions, pruning can lead to overfitting and instability. They also show that regularizing with noise can help to improve generalization performance.  "
11896,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"framework USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm USED-FOR automatic discovering process of neural architectures. neural architecture search ( NAS ) algorithm CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION neural architecture search ( NAS ) algorithm. neural architecture search ( NAS ) algorithm USED-FOR framework. architecture encoders CONJUNCTION decoders. decoders CONJUNCTION architecture encoders. reinforcement learning USED-FOR embedding space. reinforcement learning USED-FOR approach. decoders USED-FOR reinforcement learning. architecture encoders USED-FOR reinforcement learning. decoders USED-FOR approach. architecture encoders USED-FOR approach. NAS approaches USED-FOR image classification task. architecture network COMPARE NAS approaches. NAS approaches COMPARE architecture network. image classification task EVALUATE-FOR architecture network. CIFAR10 USED-FOR image classification task. CIFAR10 EVALUATE-FOR NAS approaches. NASES procedure USED-FOR architecture network. architecture - embedding searching CONJUNCTION pre - training controller. pre - training controller CONJUNCTION architecture - embedding searching. architecture - embedding searching USED-FOR NASES. pre - training controller USED-FOR NASES. parameter sharing HYPONYM-OF NAS tricks. architecture USED-FOR NASES procedure. Method are neural architectures, NAS in embedding space ( NASES ), and NAS with reinforcement learning approaches. OtherScientificTerm are noncontinuous and highdimensional search spaces, and discrete and high - dimensional architecture space. Task is optimization. ","This paper proposes NASES, a neural architecture search (NAS) algorithm with reinforcement learning (RL) and reinforcement learning in the embedding space. The main idea is to use RL to learn the architecture encoders and decoders, and then use reinforcement learning to optimize the architecture embedding. The proposed method is evaluated on image classification tasks on CIFAR-10 and ImageNet.   ","This paper proposes NASES, a neural architecture search (NAS) framework that combines reinforcement learning and reinforcement learning to find the best architecture in a discrete and high-dimensional search space. The main idea of NASES is to use a pre-training controller to learn the architecture encoders and decoders, and then use them to search the embedding space of the learned architecture. The authors show that NASES outperforms the state-of-the-art NAS methods on the CIFAR-10 image classification task."
11900,SP:e2e5bebccc76a51df3cb8b64572720da97174604,"Homotopy Training Algorithm ( HTA ) USED-FOR optimization problems. neural networks USED-FOR optimization problems. decoupled systems USED-FOR HTA. low dimensional structure FEATURE-OF decoupled systems. HTA USED-FOR continuous homotopy path. continuous homotopy path USED-FOR system. homotopy solution path USED-FOR convex case. HTA USED-FOR non - convex case. CIFAR-10 USED-FOR VGG models. VGG models HYPONYM-OF examples. accuracy EVALUATE-FOR HTA. examples EVALUATE-FOR HTA. HTA USED-FOR neural networks. HTA CONJUNCTION dropout technique. dropout technique CONJUNCTION HTA. dropout technique USED-FOR neural networks. OtherScientificTerm are high dimensional coupled system, and low dimensionality. ","This paper proposes a homotopy training algorithm (HTA) for training neural networks for non-convex optimization problems with decoupled systems. The main idea is to learn a continuous homotopic path through the low-dimensional coupled system, which is then used to train a neural network. The proposed HTA is shown to be computationally efficient in the convex case, and it is shown that it can be used to improve the performance of VGG models on CIFAR-10. ","This paper proposes a homotopy training algorithm (HTA) for decoupled systems with low dimensionality. HTA trains a neural network on a continuous homotopic path through a low dimensional coupled system. The authors show that HTA can be applied to the non-convex case and the convex case, and that it can be used to train neural networks on CIFAR-10 and VGG models. They also show that the HTA is able to improve the accuracy of neural networks trained on Cifar-10."
11904,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,attention USED-FOR entity representations. dot - product attention USED-FOR higher - dimensional attention. higher - dimensional attention PART-OF Transformer. tensor products of value vectors USED-FOR entity representations. inductive bias USED-FOR logical reasoning. architecture USED-FOR inductive bias. logical reasoning PART-OF deep reinforcement learning. Method is 2 - simplicial Transformer. ,This paper proposes a 2-simplicial Transformer architecture with dot product attention and tensor products of value vectors to learn entity representations. Theoretical analysis is provided to show that the proposed architecture can be used for logical reasoning in reinforcement learning tasks. Experiments are conducted on a variety of RL tasks.   ,This paper proposes a 2-simplicial Transformer architecture for deep reinforcement learning. The main idea is to use tensor products of value vectors to represent entity representations. The authors show that the tensor product of value vector can be used to learn higher dimensional attention for logical reasoning. They also show that dot product attention can also be used for higher-dimensional attention.   
11908,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,classification and regression approaches USED-FOR task. labelled data USED-FOR classification and regression approaches. labelled data USED-FOR pose estimation. circular latent representations USED-FOR 2D rotations. datasets USED-FOR method. labelled images FEATURE-OF datasets. Task is Pose estimation. OtherScientificTerm is fixed frame of reference. Method is Conditional Variational Autoencoders ( CVAEs ). ,This paper proposes a method for 2D pose estimation in the presence of a fixed frame of reference. The proposed method is based on conditional variational autoencoders (CVAEs) and uses circular latent representations to capture the 2D rotations. The method is evaluated on a variety of datasets and achieves state-of-the-art performance. ,"This paper proposes a new method for pose estimation from a fixed frame of reference. The proposed method is based on Conditional Variational Autoencoders (CVAEs), which is an extension of CVAE to the task of pose estimation. The main idea is to use circular latent representations to represent 2D rotations of a 2D image. The authors show that the proposed method can be applied to a wide range of datasets, and show that it can be used to perform pose estimation on a variety of datasets."
11912,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"Evolutionary Population Curriculum ( EPC ) HYPONYM-OF curriculum learning paradigm. evolutionary approach USED-FOR objective misalignment issue. evolutionary approach USED-FOR EPC. approach COMPARE baselines. baselines COMPARE approach. MARL algorithm EVALUATE-FOR EPC. MADDPG HYPONYM-OF MARL algorithm. Task is multi - agent games. Metric is complexity. OtherScientificTerm are policies, agent population, curriculum, and scaled populations. Method is MultiAgent Reinforcement Learning ( MARL ). ","This paper proposes a curriculum learning approach for multi-agent reinforcement learning, where the goal is to learn a curriculum of policies that can be used to improve the performance of the agent. The proposed approach is based on the idea that the population of policies should be scaled to avoid the objective misalignment issue. The authors show that the proposed method is able to achieve better performance compared to existing methods in terms of convergence and sample complexity. ","This paper proposes a curriculum learning approach for multi-agent reinforcement learning (MARL) where the goal is to learn a curriculum of policies that maximizes the diversity of the agent population. The proposed approach is based on the idea of evolutionary population curriculum learning (EPC), where the curriculum is designed to maximize the diversity in the population of agents. The main idea of EPC is that the curriculum should be adapted to each agent in a way that minimizes the objective misalignment issue. The authors propose a new algorithm MADDPG (Multi-Agent Dynamic Dynamic Population Curriculum) to solve this problem. They show that the proposed method outperforms the state-of-the-art MARL algorithm in terms of complexity. "
11916,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,MULTIPLEX GRAPH NETWORKS USED-FOR DIAGRAMMATIC REASONING. ,"This paper presents a set of experiments to investigate the effect of different types of images on human behavior. The experiments are conducted on a variety of datasets. The authors show that images from different datasets have different effects on human behaviour. They also show that different kinds of images (e.g. color, shape, texture, etc.) have different impact on the behavior of humans.   "," is a well-known and well-studied topic in the literature. This paper proposes a new method to study the effect of different types of visualizations of the same image. The main contribution of the paper is to propose a method that can be applied to multiple visualizations. The method is based on the idea that the visualizations can be used as a way to understand the relationship between different visualizations, and it is shown to be able to do so in the following way:  1. It is possible to use the same visualizations for all visualizations (i.e., the same one for each image), and it can be done in a way that makes sense for each visualisation.  2. It can also be done by using a different visualisation of each visualization (e.g., a different one for a different image).  3. It's possible to do it in a different way for different visualisations of the image. "
11920,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,"Bayesian inference USED-FOR deep neural networks. agent exploration CONJUNCTION prediction fairness. prediction fairness CONJUNCTION agent exploration. decision making CONJUNCTION agent exploration. agent exploration CONJUNCTION decision making. calibrated measure of uncertainty USED-FOR decision making. It USED-FOR calibrated measure of uncertainty. calibrated measure of uncertainty USED-FOR agent exploration. calibrated measure of uncertainty USED-FOR prediction fairness. It USED-FOR overfitting. decision making CONJUNCTION prediction fairness. prediction fairness CONJUNCTION decision making. Markov Chain Monte Carlo ( MCMC ) methods USED-FOR Bayesian inference. model parameters FEATURE-OF posterior distribution. optimization methods USED-FOR large scale deep learning tasks. sampling methods COMPARE optimization methods. optimization methods COMPARE sampling methods. MCMC CONJUNCTION optimization methods. optimization methods CONJUNCTION MCMC. ATMC HYPONYM-OF adaptive noise MCMC algorithm. momentum CONJUNCTION noise. noise CONJUNCTION momentum. noise USED-FOR parameter update. momentum USED-FOR parameter update. momentum USED-FOR ATMC. noise USED-FOR ATMC. classification accuracy CONJUNCTION test log - likelihood. test log - likelihood CONJUNCTION classification accuracy. Cifar10 benchmark CONJUNCTION large scale ImageNet benchmark. large scale ImageNet benchmark CONJUNCTION Cifar10 benchmark. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. ResNet architecture USED-FOR ATMC. ResNet architecture CONJUNCTION batch normalization. batch normalization CONJUNCTION ResNet architecture. test log - likelihood EVALUATE-FOR optimization baseline. large scale ImageNet benchmark EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR optimization baseline. Cifar10 benchmark EVALUATE-FOR ATMC. test log - likelihood EVALUATE-FOR ATMC. classification accuracy EVALUATE-FOR ATMC. ATMC COMPARE optimization baseline. optimization baseline COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR optimization baseline. ATMC USED-FOR overfitting. ATMC COMPARE ATMC. ATMC COMPARE ATMC. calibrated measure of uncertainty EVALUATE-FOR ATMC. OtherScientificTerm are hyperparameters, and","This paper proposes an adaptive noise MCMC algorithm for Bayesian inference. The proposed method is based on a mixture of momentum and noise, which is used to update the parameters of the posterior distribution of the model parameters. The authors show that the proposed method outperforms existing methods in terms of accuracy and test log-likelihood on the Cifar10 and ImageNet benchmarks.",This paper proposes an adaptive noise MCMC algorithm for Bayesian inference. The proposed method is based on the Markov Chain Monte Carlo (MCMC) method. The main idea is to use momentum and noise to update the parameters of the posterior distribution of the model. The authors show that the proposed method outperforms the state-of-the-art on the Cifar10 benchmark and ImageNet.
11924,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"accuracy EVALUATE-FOR latter. winning tickets USED-FOR dense, randomly initialized networks. small but critical subnetworks HYPONYM-OF winning tickets. early stopping CONJUNCTION low - precision training. low - precision training CONJUNCTION early stopping. large learning rates FEATURE-OF lowcost training schemes. low - precision training HYPONYM-OF lowcost training schemes. early stopping HYPONYM-OF lowcost training schemes. lowcost training schemes USED-FOR winning tickets. connectivity patterns FEATURE-OF neural networks. mask distance metric USED-FOR EB tickets. computational overhead EVALUATE-FOR mask distance metric. mask distance USED-FOR training methods. low - cost schemes USED-FOR EB tickets. method USED-FOR deep network training. mask distance USED-FOR them. deep networks CONJUNCTION datasets. datasets CONJUNCTION deep networks. accuracy EVALUATE-FOR training methods. Method is train - prune - retrain process. OtherScientificTerm is Early - Bird ( EB ) tickets. Metric is energy savings. Generic is state - ofthe - art training methods. ","This paper proposes a method for training deep neural networks with early-bird (EB) tickets. The method is based on the observation that EB tickets are more efficient than early-stopping or low-precision training in training dense networks with large learning rates. The authors propose to use a mask distance metric to measure the connectivity patterns of neural networks, which can then be used to estimate the early bird tickets. They show that the mask distance can be computed efficiently by computing the distance between the early birds and the early stopping tickets.  The authors show that using EB tickets can reduce the computational cost of training deep networks by a significant margin. ",This paper proposes a method for training deep neural networks with early-bird tickets (EB) tickets. EB tickets are tickets that are assigned to the first layer of a network that is trained with low-precision training with large learning rates. The authors show that EB tickets can be used to improve the performance of the network by reducing the computational overhead of training the network with EB tickets. The paper also proposes a new mask distance metric to measure the distance between EB tickets and the network's connectivity patterns. The proposed method is evaluated on a variety of datasets and datasets. 
11928,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"adversarial examples FEATURE-OF deep convolutional neural networks. intrinsic dimension COMPARE pixel space dimension. pixel space dimension COMPARE intrinsic dimension. low - dimensional space USED-FOR classification. intrinsic dimension FEATURE-OF image data. image data COMPARE pixel space dimension. pixel space dimension COMPARE image data. high - dimensional input images USED-FOR classification. high - dimensional input images USED-FOR low - dimensional space. vulnerability EVALUATE-FOR neural networks. robustness EVALUATE-FOR deep neural networks. adversarial robustness EVALUATE-FOR classifier. benchmark datasets EVALUATE-FOR framework. benchmark datasets EVALUATE-FOR strong adversarial attack methods. strong adversarial attack methods EVALUATE-FOR framework. OtherScientificTerm are input dimension, lowdimensional space, regularization, and embedding regularization. Method is Embedding Regularized Classifier ( ER - Classifier ). ","This paper studies the problem of adversarial robustness of deep neural networks against adversarial examples in low-dimensional space. The authors show that the intrinsic dimension of image data is more important than the pixel space dimension for classification, and propose a new regularization technique to improve the robustness. The proposed method is called Embedding Regularized Classifier (ER-Classifier) and is based on embedding regularization. Theoretical analysis shows that the proposed method improves robustness to adversarial attacks. Empirical results show that ER-classifier outperforms the state-of-the-art robustness methods on several benchmark datasets.","This paper proposes a method to improve the robustness of deep neural networks against adversarial examples. The proposed method is based on embedding regularization, where the input dimension of the input image is reduced to a low-dimensional space, and the classifier is trained in a low dimensional space. The method is evaluated on a variety of datasets, and it is shown to improve robustness against strong adversarial attack methods. "
11932,SP:efd68097f47dbfdd0208573071686a62240d1b12,"Named entity recognition ( NER ) CONJUNCTION relation extraction ( RE ). relation extraction ( RE ) CONJUNCTION Named entity recognition ( NER ). Named entity recognition ( NER ) HYPONYM-OF tasks. relation extraction ( RE ) HYPONYM-OF tasks. dependency parsers HYPONYM-OF external natural language processing ( NLP ) tools. external natural language processing ( NLP ) tools USED-FOR joint models. neural, end - to - end model USED-FOR jointly extracting entities. large, pre - trained language model USED-FOR neural, end - to - end model. recurrence USED-FOR self - attention. OtherScientificTerm is propagation of error. Method are pipeline - based systems, neural, end - to - end models, and external NLP tools. Generic are tools, and model. ",This paper proposes a method for jointly extracting entities in NER and relation extraction tasks. The proposed method is based on a pre-trained language model and a neural network architecture. The model is trained using self-attention and a recurrence mechanism. Experiments show that the proposed method achieves state-of-the-art performance on NER tasks.  ,"This paper proposes an end-to-end model for jointly extracting entities from a large, pre-trained language model. The model is trained using a pre-training language model and a neural network. The authors show that the model is able to jointly extract entities with the help of external NLP tools. They also show that their model can learn a recurrence-based self-attention mechanism, which can be used to improve the performance of the model."
11936,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"network USED-FOR task. informative representation USED-FOR tasks. network USED-FOR informative representation. image context FEATURE-OF RGB values of images. RGB values of images HYPONYM-OF low - level feature representation. DNNs USED-FOR Euclidean representation. DNNs USED-FOR algorithm. problem PART-OF machine learning. Ordinal Embedding HYPONYM-OF machine learning. approaches USED-FOR problem. approach COMPARE methods. methods COMPARE approach. real - world large datasets EVALUATE-FOR methods. real - world large datasets EVALUATE-FOR approach. Method are supervised / unsupervised DNNs, and neural networks. OtherScientificTerm is triplet comparisons. Task is unsupervised learning problems. ","This paper proposes a novel method to learn a low-level feature representation for image classification tasks. The proposed method is based on the idea that the representation learned by a neural network can be used to represent the context of the input image. The method is motivated by the observation that the Euclidean distance between the input and the output of the neural network is a function of the number of triplets in the input space. The authors propose to use this information to learn an embedding of the image that is independent of the context. This embedding is then used to train an encoder-decoder network that is trained to predict the input. The encoder and decoder are trained in an unsupervised manner, where the encoder is trained in a supervised manner and the decoder is used to learn the embedding.   The authors show that the proposed method can learn a representation that is similar to the original representation of the original image, but with the added information about the context in the image. This information is used as a regularization term in the loss function, which is used in the training of the network. In the experiments, the authors compare their method with a number of baselines and show that their method outperforms the baselines in terms of accuracy.","This paper proposes a novel method for unsupervised learning of the Euclidean representation of an image. The proposed method is based on the idea of using a low-level feature representation of the image to represent the context of the input image. This is done by training a neural network to learn a representation for the image, which is then used to learn an informative representation for each task. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy on a variety of datasets. "
11940,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"domain adaptation CONJUNCTION corrupted sample discovery. corrupted sample discovery CONJUNCTION domain adaptation. building insights about the learning task CONJUNCTION domain adaptation. domain adaptation CONJUNCTION building insights about the learning task. corrupted sample discovery CONJUNCTION robust learning. robust learning CONJUNCTION corrupted sample discovery. Data Valuation HYPONYM-OF meta learning framework. Reinforcement Learning ( DVRL ) USED-FOR meta learning framework. deep neural network USED-FOR data value estimator. reinforcement signal USED-FOR data value estimator. DVRL COMPARE methods. methods COMPARE DVRL. DVRL USED-FOR data value estimates. data value estimates EVALUATE-FOR methods. domain adaptation CONJUNCTION robust learning. robust learning CONJUNCTION domain adaptation. DVRL COMPARE state - of - the - art. state - of - the - art COMPARE DVRL. DVRL USED-FOR corrupted sample discovery. DVRL USED-FOR robust learning. DVRL USED-FOR domain adaptation. Task are machine learning, and Data valuation. OtherScientificTerm is data values. Method are task predictor model, and predictor model. ","This paper proposes a meta-learning framework for data value estimation based on reinforcement learning. The main idea is to use reinforcement learning to estimate the value of a set of data points, which is then used to train a task-specific predictor model. The proposed method is evaluated on two tasks: domain adaptation and corrupted sample discovery.   ","This paper proposes a meta-learning framework for data valuation. The authors propose a reinforcement-based approach for data value estimation. The main idea is to use a deep neural network as a data value estimator and use reinforcement learning to train a task predictor model to estimate the data value. The proposed method is evaluated on a variety of tasks, including domain adaptation, corrupted sample discovery and robust learning."
11944,SP:e2c3374629cfd654b7b35e88507e65646d70470e,"network ’s architecture CONJUNCTION initialization parameters. initialization parameters CONJUNCTION network ’s architecture. gradient FEATURE-OF random fully connected ReLU networks. finite sized networks FEATURE-OF per layer Jacobian. initialization parameters PART-OF network. ResNets CONJUNCTION DenseNets. DenseNets CONJUNCTION ResNets. vanilla networks CONJUNCTION ResNets. ResNets CONJUNCTION vanilla networks. vanilla networks HYPONYM-OF architectures. DenseNets HYPONYM-OF architectures. ResNets HYPONYM-OF architectures. arbitrary depths FEATURE-OF norm. architecture CONJUNCTION layer ’s size. layer ’s size CONJUNCTION architecture. Method are neural networks, initialization strategy, and deep networks. Task is gradient steps post - initialization. OtherScientificTerm are Jacobian squared norm, exploding or decaying gradients, per layer Jacobian norm, and layer ’s depth. ","This paper studies the per-layer Jacobian squared norm of random fully connected ReLU networks with random initialization parameters. The authors show that the per layer Jacobian norm depends on the architecture, initialization parameters, and initialization parameters of the network, and the depth of each layer. They show that this per layer norm is independent of the layer’s size and the architecture. They also show that under certain assumptions on the initialization parameters and the network architecture, the Jacobian of a random ReLU network can be approximated by a function of the depth and the layer size. ","This paper studies the per-layer Jacobian squared norm of deep neural networks. The authors show that the per layer Jacobian norm depends on the size of the network, the architecture, and the depth of the layers. They also show that exploding or decaying gradients can be caused by the depth and the architecture of the networks. They provide a theoretical analysis of this phenomenon.   "
11948,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"hand - optimized libraries CONJUNCTION compilation heuristics. compilation heuristics CONJUNCTION hand - optimized libraries. genetic algorithms CONJUNCTION stochastic methods. stochastic methods CONJUNCTION genetic algorithms. compilation heuristics CONJUNCTION genetic algorithms. genetic algorithms CONJUNCTION compilation heuristics. hand - optimized libraries USED-FOR neural networks. solution USED-FOR code optimization. reinforcement learning USED-FOR CHAMELEON. reinforcement learning USED-FOR solution. domain - knowledge inspired logic USED-FOR adaptive sampling algorithm. CHAMELEON COMPARE AutoTVM. AutoTVM COMPARE CHAMELEON. optimization time EVALUATE-FOR AutoTVM. real hardware EVALUATE-FOR CHAMELEON. inference time EVALUATE-FOR deep networks. optimization time EVALUATE-FOR CHAMELEON. Metric is compilation time. Generic are methods, and them. OtherScientificTerm are hardware measurements, design space, and real hardware measurements. Task is search. ",This paper proposes a method for code optimization in deep neural networks. The main idea is to use reinforcement learning to learn to sample from the design space in order to reduce the compilation time. The method is based on domain-knowledge inspired logic and uses an adaptive sampling algorithm. The proposed method is shown to be competitive with existing methods in terms of inference time. ,"This paper proposes a new method for code optimization for deep neural networks, called CHAMELEON, which is based on reinforcement learning. The main idea is to use a domain-knowledge inspired logic to learn a sampling strategy that can be applied to the design space of a neural network. The authors show that the proposed method outperforms AutoTVM in terms of optimization time, inference time, and real hardware measurements. "
11952,SP:df8483206bb88debeb24b04eb31e016368792a84,adversarial perturbations FEATURE-OF classifiers. certified robustnesses FEATURE-OF top-1 predictions. top - k predictions PART-OF real - world applications. certified robustness USED-FOR top - k predictions. randomized smoothing USED-FOR certified robustness. it USED-FOR large - scale neural networks. it USED-FOR classifier. randomized smoothing USED-FOR classifier. ` 2 norm FEATURE-OF topk predictions. tight robustness FEATURE-OF topk predictions. ` 2 norm FEATURE-OF tight robustness. Gaussian noise USED-FOR randomized smoothing. randomized smoothing USED-FOR tight robustness. top - k predictions FEATURE-OF certified robustness. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR method. CIFAR10 EVALUATE-FOR method. method USED-FOR ImageNet classifier. ` 2 - norms FEATURE-OF adversarial perturbations. top-5 accuracy EVALUATE-FOR ImageNet classifier. OtherScientificTerm is noise. ,"This paper proposes a method to improve the certified robustness of the top-k predictions in the presence of adversarial perturbations. The proposed method is based on randomized smoothing, where the smoothing is done by adding Gaussian noise to the input data. The method is evaluated on CIFAR-10 and ImageNet.   ","This paper proposes a method to improve the certified robustness of large-scale neural networks by using randomized smoothing. The proposed method is based on a Gaussian noise-based method, where the smoothing is applied to the top-k predictions of the classifier. The method is evaluated on CIFAR-10 and ImageNet datasets. "
11956,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,gradient signal to noise ratio ( GSNR ) USED-FOR DNNs. GSNR EVALUATE-FOR parameter. GSNR CONJUNCTION generalization gap. generalization gap CONJUNCTION GSNR. logistic regression CONJUNCTION support vector machines. support vector machines CONJUNCTION logistic regression. gradient descent optimization dynamics USED-FOR DNNs. gradient descent optimization dynamics USED-FOR GSNR. support vector machines HYPONYM-OF shallow models. logistic regression HYPONYM-OF shallow models. Method is deep neural networks ( DNNs ). OtherScientificTerm is data distribution. ,"This paper studies the gradient signal-to-noise ratio (GSNR) of deep neural networks (DNNs) in terms of the generalization gap. The authors show that the GSNR of DNNs is a function of the number of parameters in the network, and that it depends heavily on the parameters of the network. They also show that for a certain class of shallow models, the GSR can be reduced to a lower bound on the generalizability gap. ","This paper studies the gradient signal-to-noise ratio (GSNR) of deep neural networks (DNNs) in the context of the generalization gap. The authors show that the GSNR of DNNs depends on the parameters of the training data distribution, and that it is a function of the data distribution. They show that if the parameters are sufficiently large, then the generalisation gap can be reduced to a lower bound. They also provide a theoretical analysis of the dynamics of the DNN training process.  "
11960,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"approach USED-FOR problem. KG entities PART-OF vector space. conjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION conjunctions. existential quantifiers USED-FOR queries. conjunctions USED-FOR queries. logical disjunctions ( ∨ ) USED-FOR Handling queries. QUERY2BOX HYPONYM-OF embedding - based framework. embedding - based framework USED-FOR massive and incomplete KGs. QUERY2BOX USED-FOR arbitrary logical queries. Disjunctive Normal Form USED-FOR QUERY2BOX. Disjunctive Normal Form FEATURE-OF queries. ∧ FEATURE-OF arbitrary logical queries. QUERY2BOX COMPARE state of the art. state of the art COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. QUERY2BOX COMPARE QUERY2BOX. KGs EVALUATE-FOR QUERY2BOX. Task is Answering complex logical queries. OtherScientificTerm are complex query, hyper - rectangles, and disjunctions. ","This paper proposes a method to handle complex logical queries in the form of hyper-rectangles. The method is based on the notion of logical disjunctions, which are disjoint sets of logical conjunctions and existential quantifiers. The main idea is to embed the query embedding into a vector space, and then use the embedding to compute the queries. The proposed method is shown to be able to handle large and incomplete KGs.   ","This paper proposes a new embedding-based framework for solving complex logical queries. The main idea is to use a disjunctive normal form to handle complex queries, i.e., a set of logical disjunctions that can be represented as hyper-rectangles of a vector space. The authors show that the proposed method can be used to solve complex queries with large and incomplete KGs. The proposed method is evaluated on a variety of datasets, and it is shown that it outperforms the state-of-the-art."
11964,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"approaches USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) USED-FOR stochastic optimization. Stochastic gradient descent ( SGD ) HYPONYM-OF approaches. machine learning USED-FOR convex loss functions. machine learning USED-FOR nonconvex deep neural networks. machine learning USED-FOR SGD. unbiased estimator COMPARE full gradient. full gradient COMPARE unbiased estimator. consistent estimators COMPARE unbiased ones. unbiased ones COMPARE consistent estimators. convergence behavior EVALUATE-FOR consistent estimators. training algorithms USED-FOR large - scale graphs. consistent estimators USED-FOR SGD updates. Method are unbiased gradient estimator, empirical risk minimization, and consistent gradient estimator. OtherScientificTerm are graphs, and convex, convex, and nonconvex objectives. Material is synthetic and real - world data. ","This paper studies the convergence of stochastic gradient descent (SGD) under convex, convex and nonconvex objectives. In particular, the authors propose a new unbiased estimator for SGD updates that is consistent with the empirical risk minimization. They show that this unbiased gradient estimator is equivalent to the full gradient of SGD, and that it converges to the true gradient faster than the unbiased gradient. They also show that consistent estimators can be used to train SGD on large-scale graphs.   ","This paper studies the convergence of unbiased and non-unbiased gradient estimators for stochastic gradient descent (SGD) on large-scale graphs. The main contribution of the paper is a theoretical analysis of the convergence behavior of the unbiased estimator, which shows that it converges faster than the full gradient. The authors also provide empirical results on synthetic and real-world data to support their results. "
11968,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,resource constraints FEATURE-OF inference. neural architecture search ( NAS ) USED-FOR specialized neural network. neural architecture search ( NAS ) USED-FOR approaches. OFA network USED-FOR specialized sub - network. width CONJUNCTION kernel size. kernel size CONJUNCTION width. depth CONJUNCTION width. width CONJUNCTION depth. kernel size CONJUNCTION resolution. resolution CONJUNCTION kernel size. progressive shrinking algorithm HYPONYM-OF generalized pruning method. generalized pruning method COMPARE pruning. pruning COMPARE generalized pruning method. depth HYPONYM-OF pruning. progressive shrinking algorithm USED-FOR OFA networks. width HYPONYM-OF pruning. kernel size HYPONYM-OF pruning. It USED-FOR subnetworks. hardware platforms CONJUNCTION latency constraints. latency constraints CONJUNCTION hardware platforms. accuracy EVALUATE-FOR It. MobileNetV3 COMPARE EfficientNet. EfficientNet COMPARE MobileNetV3. OFA COMPARE MobileNetV3. MobileNetV3 COMPARE OFA. OFA COMPARE EfficientNet. EfficientNet COMPARE OFA. ImageNet top1 accuracy EVALUATE-FOR MobileNetV3. MobileNetV3 COMPARE MobileNetV3. MobileNetV3 COMPARE MobileNetV3. CO2 emission EVALUATE-FOR OFA. ImageNet top1 accuracy EVALUATE-FOR OFA. accuracy EVALUATE-FOR OFA. SOTA EVALUATE-FOR OFA. DSP classification track CONJUNCTION LPCVC. LPCVC CONJUNCTION DSP classification track. classification track CONJUNCTION detection track. detection track CONJUNCTION classification track. LPCVC CONJUNCTION classification track. classification track CONJUNCTION LPCVC. detection track HYPONYM-OF LPCVC. LPCVC EVALUATE-FOR OFA. DSP classification track EVALUATE-FOR OFA. Code CONJUNCTION pre - trained models. pre - trained models CONJUNCTION Code. Material is edge devices. Generic is it. OtherScientificTerm is diverse architectural settings. Metric is ImageNet top-1 accuracy. ,"This paper proposes OFA, a method to prune a large number of neurons in a deep neural network. OFA prunes neurons based on their width, depth, and kernel size. The method is evaluated on ImageNet and MobileNet and achieves state-of-the-art performance. ","This paper proposes a method to prune the width, depth, resolution, and kernel size of the OFA network. The proposed method is based on a progressive shrinking algorithm, which prunes the width and depth of the network. It is shown that the proposed method outperforms existing methods in terms of accuracy on ImageNet, MobileNetV3, and SOTA. "
11972,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"discrete, symbolic operations USED-FOR they. Neural module networks ( NMNs ) USED-FOR questions. Neural module networks ( NMNs ) USED-FOR executable programs. modules PART-OF executable programs. synthetic visual QA domains EVALUATE-FOR Neural module networks ( NMNs ). models USED-FOR non - synthetic questions. model USED-FOR reasoning. model USED-FOR natural language. open - domain text USED-FOR non - synthetic questions. sorting CONJUNCTION counting. counting CONJUNCTION sorting. arithmetic CONJUNCTION sorting. sorting CONJUNCTION arithmetic. modules USED-FOR symbolic reasoning. probabilistic and differentiable manner USED-FOR symbolic reasoning. counting HYPONYM-OF symbolic reasoning. arithmetic HYPONYM-OF symbolic reasoning. sorting HYPONYM-OF symbolic reasoning. heuristically - obtained question program CONJUNCTION intermediate module output supervision. intermediate module output supervision CONJUNCTION heuristically - obtained question program. inductive bias USED-FOR learning. heuristically - obtained question program USED-FOR learning. intermediate module output supervision USED-FOR inductive bias. heuristically - obtained question program USED-FOR inductive bias. intermediate module output supervision USED-FOR learning. model COMPARE models. models COMPARE model. DROP dataset EVALUATE-FOR models. DROP dataset EVALUATE-FOR model. Task is Answering compositional questions. Method are NMNs, and unsupervised auxiliary loss. ","This paper presents a method for answering compositional questions in natural language using neural module networks (NMNs). The method is based on the idea that modules in a neural network can be viewed as discrete, symbolic operations, which can be used to perform symbolic reasoning. The model is trained to answer questions in a probabilistic and differentiable manner. The authors show that the proposed method is able to answer compositional question in an open-domain text setting, where the question is composed of a question program and a set of symbolic operations. ","This paper proposes a method to learn symbolic reasoning from a set of open-domain questions in a probabilistic and differentiable manner. The proposed method is based on a neural module network (NMN) that is trained to predict the answer to a question from a question program. The model is trained using an unsupervised auxiliary loss and a heuristically obtained question program, which is combined with intermediate module output supervision. Experiments on the DROP dataset show that the proposed method outperforms the state-of-the-art in terms of performance."
11976,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"Network pruning USED-FOR compressing deep neural networks. approach USED-FOR pruning. connection sensitivity HYPONYM-OF saliency criterion. saliency criterion USED-FOR pruning. initialization conditions USED-FOR connection sensitivity measurements. gradient FEATURE-OF connection sensitivity. signal propagation properties FEATURE-OF pruned networks. image classification tasks EVALUATE-FOR network models. signal propagation perspective CONJUNCTION unsupervised pruning. unsupervised pruning CONJUNCTION signal propagation perspective. pruning USED-FOR arbitrarily - designed architectures. supervision USED-FOR pruning. Method are deep neural networks, data - free method, and pruning at initialization method. Generic is model. OtherScientificTerm is redundant parameters. ","This paper proposes a data-free pruning method that prunes the weights of the network at initialization. The method is based on the connection sensitivity metric, which is a simple yet effective way to evaluate the importance of each layer in the network. The authors show that pruning at initialization is equivalent to pruning the weights at initialization in terms of connection sensitivity. They also show that the pruned weights are more sensitive to changes in the input signal propagation.  ","This paper proposes a new method for pruning deep neural networks. The main idea is to prune the weights of the network at the beginning of training, and then prune at the end of training. The method is based on the saliency criterion, which is a generalization of the connection sensitivity criterion. The authors show that pruning the weights at the initialization stage can be done in a data-free manner. They also show that the pruning at the initialization stage can lead to better performance. "
11980,SP:d5899cba36329d863513b91c2db57675086abc49,"deep neural networks PART-OF machine learning research. training ‘ a priori ’ sparse networks USED-FOR method. training ‘ a priori ’ sparse networks USED-FOR layers. information bandwidth FEATURE-OF layers. sparse topology USED-FOR networks. data - free heuristic USED-FOR topology. Task is fast training. OtherScientificTerm are dense and convolutional layers, memory, and intra - layer topology. Method is sparse neural network initialization scheme. Generic are topologies, and network. ","This paper proposes a new initialization scheme for training sparse neural networks. The idea is to use a data-free heuristic to determine the intra-layer topology of the network, which is then used to compute the information bandwidth of each layer. Theoretical results show that the proposed initialization scheme can improve the performance of sparse networks in terms of accuracy and memory.","This paper proposes a new sparse neural network initialization scheme for training sparse neural networks. The main idea is to use a data-free heuristic to determine the intra-layer topology of a sparse network. The authors show that the information bandwidth of the layers depends on the number of layers. They also show that sparse networks with dense and convolutional layers have the same information bandwidth, and that sparse layers with dense layers have a different information bandwidth."
11984,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"recurrent neural networks ( RNNs ) USED-FOR long sequence tasks. exponential explosion CONJUNCTION vanishing of signals. vanishing of signals CONJUNCTION exponential explosion. LSTM CONJUNCTION GRU. GRU CONJUNCTION LSTM. RNN architectures COMPARE vanilla RNN cells. vanilla RNN cells COMPARE RNN architectures. GRU HYPONYM-OF RNN architectures. LSTM HYPONYM-OF RNN architectures. LSTMs CONJUNCTION GRUs. GRUs CONJUNCTION LSTMs. time scales CONJUNCTION spectral properties. spectral properties CONJUNCTION time scales. spectral properties FEATURE-OF state - to - state Jacobians. mean field theory USED-FOR signal propagation. mean field theory USED-FOR time scales. mean field theory USED-FOR GRUs. time scales USED-FOR signal propagation. LSTMs FEATURE-OF signal propagation. initialization scheme USED-FOR training instabilities. quantities USED-FOR initialization scheme. initialization hyperparameters USED-FOR quantities. it COMPARE initialization. initialization COMPARE it. it EVALUATE-FOR initialization scheme. multiple sequence tasks EVALUATE-FOR initialization scheme. generalization performance EVALUATE-FOR initialization. Generic are network, and they. Method is algorithmic and architectural modifications. OtherScientificTerm is instabilities. ","This paper studies the effect of initialization in recurrent neural networks (RNNs) on training instabilities. In particular, the authors propose to use the state-to-state Jacobians of RNNs with LSTMs, GRUs, and GRUs as the initializations. The authors show that the time-scale and spectral properties of the Jacobians are affected by the initialization hyperparameters, and propose a new initialization scheme to address this issue. Experiments on multiple sequence tasks demonstrate the effectiveness of the proposed method. ","This paper proposes a new initialization scheme for recurrent neural networks (RNNs) that is based on the mean field theory of signal propagation. The authors show that the time-scale and spectral properties of RNNs are related to the Jacobian of the state-to-state Jacobians. They show that this is the case for GRU, LSTM, GRU and LSTMs. They also show that their method can be used to improve the generalization performance on multiple sequence tasks."
11988,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"intra - class diversity CONJUNCTION inter - class similarity. inter - class similarity CONJUNCTION intra - class diversity. deep learning algorithms USED-FOR classification tasks. intra - class diversity FEATURE-OF remote sensing scene image data sets. neural network USED-FOR smoothing operation. neural network USED-FOR approaches. neighboring scene images USED-FOR deep features. siamese network USED-FOR discriminative power. siamese network USED-FOR convolutional neural networks. discriminative power FEATURE-OF convolutional neural networks. neighboring scene images USED-FOR convolutional neural networks. siamese network USED-FOR approach. semantic coherence USED-FOR feature vector. It USED-FOR feature vector. semantic coherence USED-FOR It. approach COMPARE methods. methods COMPARE approach. model COMPARE baseline. baseline COMPARE model. mean squared error value EVALUATE-FOR baseline. disease density estimation task EVALUATE-FOR baseline. mean squared error value EVALUATE-FOR model. disease density estimation task EVALUATE-FOR model. prediction accuracy EVALUATE-FOR model. Metric is accuracy. Method are post - classification methods, and cleanup model. OtherScientificTerm is overhead. Generic is task. ","This paper proposes a method to improve intra-class diversity and inter-class similarity in remote sensing scene image data sets. The proposed method is based on a siamese network, which is a convolutional neural network with a smoothing operation.   The main contribution of this paper is to use a siamee network to improve the discriminative power of deep neural networks for image classification tasks.  The method is evaluated on the disease density estimation task and achieves state-of-the-art performance on the task.","This paper proposes a method to improve intra-class diversity and inter-class similarity in remote sensing image data sets. The authors propose a siamese network to improve the discriminative power of convolutional neural networks. The proposed method is based on the idea of semantic coherence, which is used to improve feature coherence between neighboring scene images. The method is evaluated on a disease density estimation task, and the proposed method achieves state-of-the-art performance."
11992,SP:99c10e038939aa88fc112db10fe801b42360c8dc,"Self - supervised learning USED-FOR monocular depth estimation. geometry USED-FOR supervision. geometry USED-FOR Self - supervised learning. Depth networks USED-FOR representations. representations USED-FOR visual appearance. category - level patterns USED-FOR representations. semantic structure USED-FOR geometric representation learning. fixed pretrained semantic segmentation networks USED-FOR self - supervised representation learning. semantic labels CONJUNCTION proxy losses. proxy losses CONJUNCTION semantic labels. architecture USED-FOR self - supervised representation learning. semantic labels USED-FOR multi - task approach. proxy losses USED-FOR multi - task approach. semantic labels USED-FOR architecture. pixel - adaptive convolutions USED-FOR architecture. fixed pretrained semantic segmentation networks USED-FOR architecture. pixel - adaptive convolutions USED-FOR self - supervised representation learning. two - stage training process USED-FOR common semantic bias. common semantic bias FEATURE-OF dynamic objects. resampling USED-FOR two - stage training process. resampling USED-FOR common semantic bias. method COMPARE state of the art. state of the art COMPARE method. state of the art USED-FOR self - supervised monocular depth prediction. method USED-FOR self - supervised monocular depth prediction. OtherScientificTerm are 3D properties, self - supervised regime, and fine - grained details. ","This paper proposes a self-supervised monocular depth estimation method based on semantic segmentation networks. The proposed method is based on a two-stage training process, where the first stage uses semantic labels and the second stage uses pixel-adaptive convolutions to learn the semantic structure. The authors show that the proposed method outperforms the state-of-the-art methods in the self supervised setting.   ","This paper proposes a new method for self-supervised monocular depth estimation. The proposed method is based on a fixed pretrained semantic segmentation network and a pixel-adaptive convolutional neural network. The main contribution of the paper is a two-stage training process, where the first stage is to learn a set of semantic labels for each object, and the second stage is a resampling of the dataset. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and accuracy with respect to the number of samples."
11996,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,label - flipping attacks HYPONYM-OF data poisoning attack. deep features USED-FOR linear classifiers. test - time robustness EVALUATE-FOR technique. randomized smoothing USED-FOR approach. Dogfish binary classification task EVALUATE-FOR baseline undefended classifier. ImageNet FEATURE-OF Dogfish binary classification task. certified accuracy EVALUATE-FOR classifier. accuracy EVALUATE-FOR baseline undefended classifier. multi - class classification algorithm USED-FOR label - flipping attacks. Task is label - flipping. Method is classification. Material is multi - class case. ,"This paper studies the label-flipping attacks, a new data poisoning attack that aims to fool a classifier. The authors propose a new method to improve the robustness of the classifier against the label flipping attacks. The proposed method is based on the randomized smoothing. The method is evaluated on the Dogfish binary classification task and achieves state-of-the-art certified accuracy.","This paper proposes a novel method to improve the test-time robustness of a classifier against label-flipping attacks. The proposed method is based on a randomized smoothing approach, where the classifier is trained with a set of features that are randomly smoothed. The method is tested on the Dogfish binary classification task, where it outperforms the baseline undefended classifier. "
12000,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"Outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION Outlier detection. novelty detection USED-FOR anomaly detection. Outlier detection PART-OF anomaly detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. backdoor poisoning attacks USED-FOR machine learning models. Differential privacy USED-FOR aggregated analysis. dataset USED-FOR aggregated analysis. random noise USED-FOR It. differential privacy USED-FOR outlier detection. differential privacy USED-FOR novelty detection. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. extension USED-FOR poisoning samples. poisoning samples PART-OF backdoor attacks. outlier detection CONJUNCTION novelty detection. novelty detection CONJUNCTION outlier detection. novelty detection CONJUNCTION backdoor attack detection. backdoor attack detection CONJUNCTION novelty detection. differential privacy USED-FOR detection. differential privacy USED-FOR outlier detection. differential privacy USED-FOR backdoor attack detection. differential privacy USED-FOR novelty detection. OtherScientificTerm are distribution, Outliers, novelties, and outliers. Method is aggregation mechanism. ",This paper proposes to combine differential privacy with outlier detection and novelty detection for backdoor poisoning attacks. The main idea is to use differential privacy to aggregate outlier and novelty data from different sources and use the aggregated data to train a model with differential privacy. Theoretical analysis is provided to show that the proposed method is able to detect outliers and novelties in backdoor attacks. Experiments are conducted to validate the effectiveness of the method.,"This paper proposes a method for differential privacy-based outlier detection and novelty detection for backdoor poisoning attacks. The main idea is to use differential privacy as an aggregation mechanism to detect outliers and novelties in the dataset. The authors show that differential privacy can be used to detect outliers and novelty, and that it can also be used for backdoor attack detection. They also show that it is possible to detect backdoor poisoning samples in differential privacy."
12004,SP:a5f0e531afd970144169823971d2d039bff752fb,"applications CONJUNCTION safety - critical ones. safety - critical ones CONJUNCTION applications. calibration of uncertainty prediction USED-FOR regression tasks. real - world systems FEATURE-OF regression tasks. definition USED-FOR regression uncertainty. reliability diagrams USED-FOR classification tasks. histogram - based approach USED-FOR classification tasks. definition CONJUNCTION evaluation method. evaluation method CONJUNCTION definition. reliability diagrams USED-FOR histogram - based approach. histogram - based approach USED-FOR evaluation method. synthetic, controlled problem CONJUNCTION object detection bounding - box regression task. object detection bounding - box regression task CONJUNCTION synthetic, controlled problem. Generic are method, and examples. OtherScientificTerm are uncertainty prediction, and empirical uncertainty. Method is scaling - based calibration. ","This paper proposes a method for calibration of uncertainty prediction in regression tasks. The method is based on a histogram-based approach, which uses the reliability diagrams for classification tasks. It is shown that the proposed method can be applied to both synthetic and real-world regression problems. The proposed method is shown to outperform existing methods in the experiments.","This paper proposes a new method for calibration of uncertainty prediction for regression tasks. The method is based on a histogram-based approach, where the uncertainty is defined as the difference between the predicted uncertainty and the empirical uncertainty. The authors show that their method can be applied to both synthetic and real-world regression tasks, and demonstrate that it can be used to improve the accuracy of the calibration."
12008,SP:c422afd1df1ac98e23235830585dd0d45513064c,BERT HYPONYM-OF bidirectional Transformer language model. Tensor - Product Representations ( TPRs ) CONJUNCTION BERT. BERT CONJUNCTION Tensor - Product Representations ( TPRs ). structured - representational power CONJUNCTION Tensor - Product Representations ( TPRs ). Tensor - Product Representations ( TPRs ) CONJUNCTION structured - representational power. structured - representational power CONJUNCTION BERT. BERT CONJUNCTION structured - representational power. Tensor - Product Representations ( TPRs ) USED-FOR HUBERT1. BERT PART-OF HUBERT1. structured - representational power PART-OF HUBERT1. HUBERT COMPARE BERT. BERT COMPARE HUBERT. GLUE benchmark CONJUNCTION HANS dataset. HANS dataset CONJUNCTION GLUE benchmark. HANS dataset EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR model. general language structure USED-FOR untangling data - specific semantics. Material is NLP datasets. ,"This paper proposes a new Transformer-based language model called HUBERT, which combines BERT and Tensor-product representations (TPRs) to improve the performance of BERT on NLP tasks. The main idea is to use TPRs to model the general language structure of the input data, and then use BERT to learn a bidirectional Transformer model to predict the output of the TPR. The proposed model is evaluated on GLUE and HANS datasets, where it outperforms BERT.","This paper proposes a bidirectional Transformer language model (HUBERT1) that combines BERT and Tensor-Product Representations (TPRs) with structured-representational power. The proposed model is evaluated on GLUE benchmark, HANS dataset, and HANS-HANS dataset. HUBERT outperforms BERT on all three datasets. "
12012,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,Multi - agent reinforcement learning HYPONYM-OF problem. communication CONJUNCTION centralized training. centralized training CONJUNCTION communication. particle - based agents USED-FOR cooperative and competitive environments. communication USED-FOR particle - based agents. centralized training USED-FOR particle - based agents. dynamics CONJUNCTION humanoid navigation strategies. humanoid navigation strategies CONJUNCTION dynamics. interaction CONJUNCTION dynamics. dynamics CONJUNCTION interaction. Multi - Agent Reinforcement Learning CONJUNCTION Hierarchical Reinforcement Learning. Hierarchical Reinforcement Learning CONJUNCTION Multi - Agent Reinforcement Learning. multi - agent models USED-FOR simulated humanoid navigation. decentralized methods USED-FOR learning. structure USED-FOR optimization problem. goal - conditioned policies USED-FOR low - level physical controllers. balance CONJUNCTION walking. walking CONJUNCTION balance. low - level physical controllers USED-FOR balance. low - level physical controllers USED-FOR walking. lower - level controllers CONJUNCTION higher - level policies. higher - level policies CONJUNCTION lower - level controllers. decentralized heterogeneous policies USED-FOR multi - agent goal - directed collision avoidance. goal conditioned policies USED-FOR decentralized heterogeneous policies. RL techniques USED-FOR policies. methods USED-FOR RL techniques. Method is partial parameter sharing approach. OtherScientificTerm is hierarchy. Task is multi - agent problem. Material is multi - agent pursuit environment. ,"This paper presents a method for multi-agent reinforcement learning in the multi-goal-directed collision avoidance (MDPA) setting. The method is based on hierarchical reinforcement learning (HRL), where the goal is to avoid collision with a goal-conditioned goal conditioned on a set of low-level physical controllers. The authors propose a partial parameter sharing approach, which allows agents to share the parameters of their policies with each other. The proposed method is evaluated on a simulated humanoid navigation task, where it is shown that the proposed method outperforms baselines in terms of performance on the goal. ","This paper proposes a new method for multi-agent reinforcement learning in a multi-player pursuit environment. The authors propose a hierarchical approach to multi-players in which each player has a goal-conditioned goal-directed collision avoidance policy, and the goal is to avoid collision with other agents. The goal is that the agents should be able to communicate with each other in a cooperative and competitive manner. The agents are trained to navigate the environment in a hierarchical manner, where each agent has a low-level controller and a higher-level policy. The lower-level controllers control the dynamics of the environment, while the high-level policies control the interaction between the agents.  The authors show that this hierarchical approach can be applied to a variety of RL techniques, such as partial parameter sharing, hierarchical reinforcement learning, and Hierarchical Reinforcement Learning.  "
12016,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"spatial convolution layer USED-FOR Graph Neural Networks ( GNNs ). spatial convolution layer USED-FOR feature vector. convolution layer USED-FOR nodes. convolution layer USED-FOR feature vectors. continuous feature space FEATURE-OF feature vectors. GNN USED-FOR graphs. convolution layers USED-FOR local structures. solution USED-FOR GNNs. spatial representation of the graph USED-FOR approach. point - cloud representation of the graph USED-FOR spatial representation. graph embedding method USED-FOR spatial representation. GNN USED-FOR topological structure. local feature extractor USED-FOR GNN. approach USED-FOR local feature extractor. spatial distribution of the locally extracted feature vectors USED-FOR GNN. spatial distribution of the locally extracted feature vectors USED-FOR topological structure. spatial representation USED-FOR graph down - sampling problem. pooling method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE pooling method. Method are neural network, and graph pooling method. OtherScientificTerm is graph. ",This paper proposes a novel method to improve the performance of graph neural networks (GNNs) by introducing a spatial convolution layer. The proposed method is based on a point cloud representation of the graph and a graph embedding method to learn the spatial representation from the point cloud. The spatial representation is then used to compute the local feature extractor for each node in the graph. The method is evaluated on a variety of graph classification tasks.  ,"This paper proposes a new spatial convolution layer for graph neural networks (GNNs). The proposed method is based on the point-cloud representation of the graph, which is a graph embedding method. The authors show that the spatial representation can be used to reduce the down-sampling problem of GNNs. They also show that their method can be applied to graph pooling. "
12020,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,it USED-FOR image processing. shift - equivalent prior of images USED-FOR Convolutional layer. max - pooling CONJUNCTION averagepooling. averagepooling CONJUNCTION max - pooling. averagepooling CONJUNCTION stride convolution. stride convolution CONJUNCTION averagepooling. down sampling methods USED-FOR convolutional neural networks ( CNNs ). max - pooling HYPONYM-OF convolutional neural networks ( CNNs ). stride convolution HYPONYM-OF down sampling methods. max - pooling HYPONYM-OF down sampling methods. averagepooling HYPONYM-OF down sampling methods. down sampling USED-FOR shift - equivalent. image classifications EVALUATE-FOR frequency pooling. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. CNNs EVALUATE-FOR frequency pooling. accuracy EVALUATE-FOR frequency pooling. robustness EVALUATE-FOR frequency pooling. ,"This paper proposes to use frequency pooling to improve the accuracy and robustness of convolutional layers in deep neural networks. The proposed method is based on the observation that the shift-equivalent prior of images can be used as a down-sampling method to reduce the number of convolutions in a deep neural network. The authors propose to use a combination of max-pooling, averagepooling and stride convolution to achieve this goal. The experimental results show that the proposed method improves the accuracy of CNNs on image classification tasks.","This paper proposes a new frequency pooling method for down-sampling convolutional neural networks (CNNs). The proposed method is based on the shift-equivalent prior of images, which is used to reduce the number of samples needed for training a CNN. The authors show that the proposed method outperforms max-pooling, averagepooling and stride convolution in terms of accuracy, robustness, and robustness to shift."
12024,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"high - dimensional state spaces USED-FOR they. images HYPONYM-OF high - dimensional state spaces. technique USED-FOR deep RL agents. technique USED-FOR generalization ability. generalization ability EVALUATE-FOR deep RL agents. randomized ( convolutional ) neural network USED-FOR technique. Monte Carlo approximation USED-FOR inference method. 2D CoinRun CONJUNCTION 3D DeepMind Lab exploration. 3D DeepMind Lab exploration CONJUNCTION 2D CoinRun. 3D DeepMind Lab exploration CONJUNCTION 3D robotics control tasks. 3D robotics control tasks CONJUNCTION 3D DeepMind Lab exploration. it COMPARE regularization and data augmentation methods. regularization and data augmentation methods COMPARE it. 3D robotics control tasks EVALUATE-FOR method. 3D DeepMind Lab exploration EVALUATE-FOR method. 2D CoinRun EVALUATE-FOR method. Method is Deep reinforcement learning ( RL ) agents. Generic are agents, and It. OtherScientificTerm are robust features, varied and randomized environments, and randomization. ",This paper proposes a method to improve the generalization ability of reinforcement learning agents in the presence of diverse and randomized environments. The proposed method is based on a convolutional neural network with a randomized (convolutional) neural network and a Monte Carlo approximation method. The method is evaluated on CoinRun and 3D DeepMind Lab exploration tasks. ,"This paper proposes a novel method to improve the generalization ability of deep RL agents. The method is based on a convolutional neural network, which is trained with a Monte Carlo approximation. The proposed method is evaluated on a variety of tasks, including 2D CoinRun and 3D DeepMind Lab exploration. "
12028,SP:31772a9122ec998c7c829bc4813f6147cdc30145,"image classification CONJUNCTION visual question answering. visual question answering CONJUNCTION image classification. models USED-FOR tasks. visual question answering HYPONYM-OF tasks. image classification HYPONYM-OF tasks. explanation approach USED-FOR image similarity models. score USED-FOR model. saliency map USED-FOR explanation method. saliency map USED-FOR image regions. explanations USED-FOR attribute recognition. diverse domains FEATURE-OF datasets. Polyvore Outfits HYPONYM-OF datasets. datasets EVALUATE-FOR approach. Polyvore Outfits HYPONYM-OF diverse domains. Method are deep learning model, and saliency maps. Task is classification. Generic are task, and methods. ","This paper proposes an explanation approach for image similarity models. The proposed method is based on the saliency map, which is used to map the image regions to explain the similarity between two images. The method is evaluated on image classification and visual question answering tasks. The results show that the proposed method outperforms the baselines.","This paper proposes an explanation method for image similarity models. The proposed method is based on the idea of saliency maps, where the saliency map is a map of the regions of the image that are most important to the model. The authors show that the proposed method can be applied to a variety of tasks, including image classification, visual question answering, and attribute recognition. The method is evaluated on Polyvore Outfits, a dataset with diverse domains, and it is shown that it outperforms the state-of-the-art. "
12032,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,WaveFlow HYPONYM-OF small - footprint generative flow. auxiliary losses USED-FOR Parallel WaveNet. small - footprint generative flow USED-FOR raw audio. flowbased models USED-FOR raw audio. autoregressive flow CONJUNCTION bipartite flow. bipartite flow CONJUNCTION autoregressive flow. WaveNet HYPONYM-OF autoregressive flow. autoregressive flow PART-OF flowbased models. WaveGlow HYPONYM-OF bipartite flow. test likelihood CONJUNCTION speech fidelity. speech fidelity CONJUNCTION test likelihood. likelihood - based generative models USED-FOR raw waveforms. test likelihood EVALUATE-FOR likelihood - based generative models. speech fidelity EVALUATE-FOR likelihood - based generative models. WaveFlow USED-FOR high - fidelity speech. WaveFlow COMPARE WaveNet. WaveNet COMPARE WaveFlow. likelihood EVALUATE-FOR WaveFlow. small - footprint WaveFlow COMPARE real - time. real - time COMPARE small - footprint WaveFlow. GPU without engineered inference kernels USED-FOR real - time. Method is maximum likelihood. ,"This paper proposes a new generative model for high-fidelity speech synthesis. The proposed method, called WaveFlow, is based on parallelized autoregressive flow and bipartite flow with auxiliary losses. The authors show that the proposed method is able to achieve state-of-the-art results in terms of test likelihood and speech fidelity.","This paper proposes WaveFlow, a small-scale generative model for high-fidelity speech synthesis. The main idea is to use a parallelized version of WaveNet with auxiliary losses to generate high-quality speech samples. The authors show that WaveFlow is able to achieve state-of-the-art performance in terms of test likelihood and speech fidelity. They also show that the proposed method can be used in combination with other generative models such as WaveNet and WaveGlow. "
12036,SP:963e85369978dddcd9e3130bc11453696066bbf3,"GT - GAN USED-FOR translation mapping. graph translator USED-FOR translation mapping. graph convolution and deconvolution layers USED-FOR translation mapping. graph convolution and deconvolution layers PART-OF graph translator. graph convolution and deconvolution layers PART-OF GT - GAN. graph translator PART-OF GT - GAN. GT - GAN COMPARE baseline methods. baseline methods COMPARE GT - GAN. synthetic and realworld datasets EVALUATE-FOR GT - GAN. synthetic and realworld datasets EVALUATE-FOR baseline methods. scalability EVALUATE-FOR GT - GAN. effectiveness EVALUATE-FOR GT - GAN. scalability EVALUATE-FOR baseline methods. effectiveness EVALUATE-FOR baseline methods. GT - GAN COMPARE GraphRNN. GraphRNN COMPARE GT - GAN. GraphRNN CONJUNCTION RandomVAE. RandomVAE CONJUNCTION GraphRNN. GT - GAN COMPARE RandomVAE. RandomVAE COMPARE GT - GAN. Method are Deep graph generation models, unconditioned generative models, and conditional graph discriminator. OtherScientificTerm is global and local features. ",This paper proposes a new GAN architecture for graph generation. The main idea is to use graph convolution and deconvolution layers to map the global and local features of the input graph to the target graph. The proposed method is evaluated on both synthetic and real-world datasets and achieves state-of-the-art performance. ,This paper proposes a novel GAN-based GAN model for graph generation. The main idea is to use GANs to learn a translation mapping between the global and local features of a graph. The translation mapping is done by using graph convolution and deconvolution layers to map the global features to the local features. The proposed method is evaluated on synthetic and real-world datasets. The experimental results show that the proposed method outperforms the state-of-the-art baselines.
12040,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"METAGROSS ( Meta Gated Recursive Controller ) HYPONYM-OF neural sequence modeling unit. gating mechanisms PART-OF METAGROSS. inductive bias USED-FOR learning. hierarchically - structured sequence data USED-FOR learning. language HYPONYM-OF hierarchically - structured sequence data. code generation CONJUNCTION machine translation. machine translation CONJUNCTION code generation. tree traversal CONJUNCTION logical inference. logical inference CONJUNCTION tree traversal. sequential pixel - by - pixel classification CONJUNCTION semantic parsing. semantic parsing CONJUNCTION sequential pixel - by - pixel classification. sorting CONJUNCTION tree traversal. tree traversal CONJUNCTION sorting. semantic parsing CONJUNCTION code generation. code generation CONJUNCTION semantic parsing. machine translation CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION machine translation. logical inference CONJUNCTION sequential pixel - by - pixel classification. sequential pixel - by - pixel classification CONJUNCTION logical inference. tasks EVALUATE-FOR approach. polyphonic music modeling HYPONYM-OF recursive logic tasks. sequential pixel - by - pixel classification HYPONYM-OF recursive logic tasks. machine translation HYPONYM-OF recursive logic tasks. logical inference HYPONYM-OF recursive logic tasks. code generation HYPONYM-OF recursive logic tasks. semantic parsing HYPONYM-OF recursive logic tasks. sorting HYPONYM-OF recursive logic tasks. tree traversal HYPONYM-OF recursive logic tasks. Generic is unit. OtherScientificTerm are gating functions, and meta - gating. Method is recurrent model. ","This paper proposes a meta-gated recurrent neural sequence model called Meta-Gated Recursive Controller (Metagross), which is a neural sequence modeling unit with gating mechanisms. The proposed Metagross model is based on the meta gating mechanism, which is an extension of meta-Gating. The main contribution of the paper is the introduction of a new meta-regularization term, which encourages the model to learn a set of gating functions for each input sequence.  The proposed model achieves state-of-the-art performance on a variety of tasks, including code generation, machine translation, logical inference, and semantic parsing. ","This paper proposes a new neural sequence modeling unit, Meta-Gated Recursive Controller (MetAGROSS), which is based on meta-gating mechanism. Meta-gated mechanisms are gating mechanisms that control the inductive bias of the neural sequence model, which is used to train a recurrent model. The proposed method is evaluated on a variety of tasks, including code generation, machine translation, semantic parsing, logical inference, sorting, and tree traversal."
12044,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,local prior matching ( LPM ) HYPONYM-OF self - supervised objective. self - supervised objective USED-FOR speech recognition. language model USED-FOR learning signal. unlabeled speech USED-FOR learning signal. language model USED-FOR LPM objective. unpaired text CONJUNCTION speech. speech CONJUNCTION unpaired text. it USED-FOR LPM. unpaired text USED-FOR it. speech USED-FOR it. unpaired text USED-FOR LPM. speech USED-FOR LPM. language model USED-FOR LPM. model USED-FOR LPM. unlabeled data USED-FOR model. labeled speech USED-FOR model. WER EVALUATE-FOR LPM. clean and noisy test set EVALUATE-FOR LPM. test sets EVALUATE-FOR fully supervised model. WER EVALUATE-FOR fully supervised model. noisy test set EVALUATE-FOR WER. WER EVALUATE-FOR LPM. noisy data USED-FOR LPM. Method is self - supervised approach. ,"This paper proposes a self-supervised approach to improve the performance of speech recognition models with unlabeled speech. The proposed approach is based on the local prior matching (LPM) objective, which is an extension of the text-to-speech (T2S) objective. The authors propose to use a language model to model the similarity between text and speech, and use it as the prior for the T2S objective. They show that the proposed approach outperforms the state-of-the-art methods on both clean and noisy test sets. ","This paper proposes a self-supervised approach to local prior matching (LPM) for speech recognition. The authors propose to use a language model to model the unlabeled speech as well as the unlabelled speech as the learning signal for the LPM objective. The language model is trained using a combination of text-to-speech and speech-to speech pairs. The proposed method is evaluated on both a clean and noisy test set, and it outperforms the state-of-the-art."
12048,SP:e6af249608633f1776b608852a00946a5c09a357,"data poisoning USED-FOR fair and robust model training. FR - GAN USED-FOR fair and robust model training. generative adversarial networks ( GANs ) USED-FOR FR - GAN. generative adversarial networks ( GANs ) USED-FOR fair and robust model training. fairness discriminator CONJUNCTION robustness discriminator. robustness discriminator CONJUNCTION fairness discriminator. robustness discriminator HYPONYM-OF discriminators. fairness discriminator HYPONYM-OF discriminators. disparate impact CONJUNCTION equalized odds. equalized odds CONJUNCTION disparate impact. equalized odds CONJUNCTION equal opportunity. equal opportunity CONJUNCTION equalized odds. fairness measures FEATURE-OF framework. disparate impact HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. equal opportunity HYPONYM-OF fairness measures. FR - GAN USED-FOR fairness. FR - GAN COMPARE fairness methods. fairness methods COMPARE FR - GAN. fairness CONJUNCTION accuracy. accuracy CONJUNCTION fairness. accuracy EVALUATE-FOR FR - GAN. fairness EVALUATE-FOR FR - GAN. accuracy CONJUNCTION fairness. fairness CONJUNCTION accuracy. fairness EVALUATE-FOR FR - GAN. accuracy EVALUATE-FOR FR - GAN. OtherScientificTerm is bias. Method are model fairness techniques, and generator. Material is validation set. ","This paper proposes a framework for fair and robust model training based on generative adversarial networks (GANs). The proposed framework consists of two discriminators: a fairness discriminator and a robustness discriminator. The fair discriminator is designed to measure the impact of data poisoning, while the robust discriminator aims to ensure robustness. The authors show that the proposed framework can be used to improve the fairness and robustness of GANs.  ","This paper proposes a framework for fair and robust model training with generative adversarial networks (GANs). The proposed framework is based on the idea of data poisoning, where data is poisoned during training. The authors show that data poisoning can lead to unfair and robust training, and propose a framework to address this problem. The framework consists of three components: (1) a fairness discriminator, (2) a robustness discriminator and (3) an equal opportunity discriminator. The discriminator discriminator is used to measure the fairness of the training data, and the fairness measure is a combination of two existing fairness measures: (i) disparate impact and (ii) equalized odds. The fairness measure measures the impact of training data on the training set, and (iii) equal opportunity measures the equal opportunity and equal opportunity of the test set. The fair measures are used to evaluate the training accuracy and fairness of a GAN trained with the proposed framework."
12052,SP:6306417f5a300629ec856495781515c6af05a363,"physics - inspired deep learning approach USED-FOR point cloud processing. static background grid CONJUNCTION Lagrangian material space. Lagrangian material space CONJUNCTION static background grid. moving particles USED-FOR Lagrangian material space. Lagrangian material space USED-FOR learning architecture. moving particles USED-FOR learning architecture. static background grid USED-FOR learning architecture. Eulerian - Lagrangian representation USED-FOR particle features. generalized, high - dimensional force field USED-FOR flow velocities. generalized, high - dimensional force field USED-FOR particle features. flow velocities USED-FOR particle features. point cloud classification and segmentation problems EVALUATE-FOR system. geometric machine learning CONJUNCTION physical simulation. physical simulation CONJUNCTION geometric machine learning. PIC / FLIP scheme USED-FOR natural flow. Task is natural flow phenomena in fluid mechanics. OtherScientificTerm are Eulerian world space, and geometric reservoir. ","This paper presents a method for learning to predict the flow dynamics of a point cloud. The method is based on the observation that the Eulerian-Lagrangian representation of the point cloud can be used to model the dynamics of the system. The main idea is to use a generalized, high-dimensional force field to estimate the flow velocities of the particles in the system, which are then used to train a classifier that predicts the distribution of the flow in the point clouds.  ","This paper proposes a physics-inspired deep learning approach for point cloud processing. The main idea is to learn a Lagrangian-Lagrangian representation of the particles in a static background grid and a moving particles in the Lagrangians of the Eulerian material space. The authors propose a PIC/FLIP scheme to learn the flow velocities of the moving particles. They also propose a generalized, high-dimensional force field to capture the flow velocity of the particle features. The proposed method is evaluated on point cloud classification and segmentation problems. "
12056,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"clipping USED-FOR dynamics of iterates. local minimum FEATURE-OF rate of convergence. clipping COMPARE vanilla gradient descent. vanilla gradient descent COMPARE clipping. clipping USED-FOR noise. lens USED-FOR gradient clipping. robustness HYPONYM-OF lens. label noise FEATURE-OF classification. robustness EVALUATE-FOR gradient clipping. gradient clipping USED-FOR label noise. Method are Gradient clipping, deep networks, and optimisation lens. OtherScientificTerm are loss function, and cross - entropy loss. ",This paper studies the effect of gradient clipping on the dynamics of iterates in deep neural networks. The authors show that gradient clipping can be used to improve the robustness to label noise in classification tasks. They show that clipping can improve the convergence rate of gradient descent with respect to the local minimum of the loss function. They also show that it can improve robustness against label noise.,This paper studies the effect of gradient clipping on the dynamics of iterates in deep neural networks. The authors show that the rate of convergence of clipping is lower than that of vanilla gradient descent when the loss function is cross-entropy. They also show that clipping can be used to improve the robustness of the network against label noise. The paper also provides a theoretical analysis of the impact of clipping on robustness. 
12060,SP:414b06d86e132357a54eb844036b78a232571301,they USED-FOR imitating actions. imitation learning methods USED-FOR imitating actions. state alignment based imitation learning method USED-FOR imitator. expert demonstrations FEATURE-OF state sequences. them PART-OF reinforcement learning framework. local and global perspectives USED-FOR state alignment. regularized policy update objective USED-FOR them. regularized policy update objective USED-FOR reinforcement learning framework. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings CONJUNCTION imitation learning settings. imitation learning settings EVALUATE-FOR method. imitation learning settings EVALUATE-FOR method. Task is imitation learning problem. Method is dynamics models. ,This paper proposes a state alignment-based imitation learning method to learn from expert demonstrations of state sequences. The proposed method is based on state alignment between the expert demonstrations and the state sequences learned by the imitator. The authors propose to use both local and global perspectives to learn the state alignment. They also propose a regularized policy update objective to improve the performance of the learned policy. Experiments show that the proposed method achieves state-of-the-art performance in both imitation learning and reinforcement learning settings. ,"This paper proposes a state alignment-based imitation learning method for imitation learning. The key idea is to learn a state-action pair (e.g., an imitator and an expert) from a sequence of expert demonstrations, where the expert demonstrations are generated from a set of state sequences. The state sequences are generated by a dynamics model, and the imitators are trained using a regularized policy update objective. The authors show that the proposed method outperforms the state-of-the-art in a variety of imitation learning settings."
12064,SP:91761d68086330ce378507c152e72218ed7b2196,"Stochastic gradient descent ( SGD ) USED-FOR deep neural networks. deep gradient boosting ( DGB ) HYPONYM-OF SGD. pseudo - residual targets USED-FOR gradient boosting problem. chain rule USED-FOR back - propagated gradients. boosting problem USED-FOR weight update. linear base learner USED-FOR boosting problem. normalization procedure USED-FOR weight update formula. architecture COMPARE architecture. architecture COMPARE architecture. image recognition tasks EVALUATE-FOR architecture. input normalization layer ( INN ) USED-FOR architecture. normalization layers PART-OF architecture. image recognition tasks EVALUATE-FOR architecture. batch normalization ( BN ) COMPARE INN. INN COMPARE batch normalization ( BN ). CIFAR10 CONJUNCTION ImageNet classification tasks. ImageNet classification tasks CONJUNCTION CIFAR10. ImageNet classification tasks EVALUATE-FOR it. CIFAR10 EVALUATE-FOR it. Method are neural network, and DGB. OtherScientificTerm are intrinsic generalization properties, and forward pass. "," image classification tasks.   The paper proposes a new normalization method for training deep neural networks.  The proposed method is based on gradient boosting with pseudo-residual targets.  In the proposed method, the weight update formula is approximated by a linear base learner.  A chain rule is used to compute the back-propagated gradients, and a normalization procedure is used for weight update.  Experiments on CIFAR-10 and ImageNet classification tasks show the effectiveness of the proposed methods. ","This paper proposes a novel approach to deep gradient boosting (DGB) in the context of stochastic gradient descent (SGD). The main idea is to use pseudo-residual targets as pseudo-reinforcing targets in the training process, and then use a normalization procedure to normalize the weight update formula of the base learner. The proposed method is evaluated on CIFAR-10 and ImageNet classification tasks. "
12068,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"Differentiable architecture search ( DARTS ) USED-FOR network architectures. reduced memory cost USED-FOR PC - DARTS. training stability EVALUATE-FOR PC - DARTS. batch size USED-FOR PC - DARTS. GPU - days USED-FOR search. top-1 error rate EVALUATE-FOR search. ImageNet EVALUATE-FOR top-1 error rate. Metric are memory and computing overheads, and error rate. OtherScientificTerm are super - network, edges of super - net, and edge - level parameters. Generic are approach, strategy, method, and code. Method are Partially - Connected DARTS, operation search, edge normalization, and architecture search. Material is CIFAR10. ","This paper proposes PC-DARTS, a method to reduce the memory cost of differentiable architecture search for deep neural networks. The main idea is to perform operation search on the edges of the super-net, and then normalize the parameters of the edge-level parameters. Theoretical analysis is provided to show that this approach can reduce the training time and memory cost. Experiments are conducted on ImageNet and CIFAR-10. ","This paper proposes a new approach to differentiable architecture search (DARTS) for training deep neural networks. The main idea of the paper is to reduce the memory and computing overheads of DARTS by reducing the size of the batch size and the number of training days. The proposed approach is based on the Partially-Connected DARTS (PC-DATS) approach, which is a generalization of the DARTS approach. The authors show that the proposed PC-DATS can achieve a top-1 error rate in terms of training stability.   "
12072,SP:724870046e990376990ba9f73d63d331f61788d7,"control strategies USED-FOR complex control tasks. model - predictive control ( MPC ) CONJUNCTION trajectory optimization. trajectory optimization CONJUNCTION model - predictive control ( MPC ). Model - based control algorithms USED-FOR control tasks. gradients of underlying system dynamics USED-FOR control tasks. gradients of underlying system dynamics USED-FOR Model - based control algorithms. sample efficiency EVALUATE-FOR control tasks. trajectory optimization HYPONYM-OF Model - based control algorithms. model - predictive control ( MPC ) HYPONYM-OF Model - based control algorithms. gradient - based numerical optimization methods COMPARE model - based control methods. model - based control methods COMPARE gradient - based numerical optimization methods. sampling USED-FOR solution space. gradient - based methods CONJUNCTION DRL. DRL CONJUNCTION gradient - based methods. gradient - based methods USED-FOR hybrid method. DRL USED-FOR hybrid method. true gradients USED-FOR convergence rate. convergence rate FEATURE-OF actor. differentiable physical simulator USED-FOR true gradients. convergence rate EVALUATE-FOR modification. differentiable physical simulator USED-FOR modification. true gradients USED-FOR modification. deep deterministic policy gradients ( DDPG ) algorithm USED-FOR algorithm. differentiable half cheetah HYPONYM-OF one. 2D robot control tasks EVALUATE-FOR algorithm. hard contact constraints FEATURE-OF differentiable half cheetah. method USED-FOR DDPG. robustness EVALUATE-FOR method. OtherScientificTerm are initialization, and local minima. Method is Deep reinforcement learning ( DRL ). Metric is computational cost. ","This paper proposes a novel method to improve the sample efficiency of model-based control algorithms. The main idea is to use a differentiable simulator to estimate the true gradients of the underlying system dynamics, which can then be used to improve sample efficiency. The proposed method is based on deep reinforcement learning (DRL) and is able to achieve better sample efficiency compared to the state-of-the-art methods. ",This paper proposes a new method to improve the sample efficiency of model-based control algorithms. The proposed method is based on deep reinforcement learning (DRL) and a differentiable physical simulator. The key idea is to use the true gradients of the underlying system dynamics to improve sample efficiency. The method is evaluated on a variety of 2D robot control tasks. 
12076,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"task - agnostic world graphs USED-FOR complex environment. nodes PART-OF world graph. hierarchical RL framework USED-FOR exploration. world graph nodes CONJUNCTION edges. edges CONJUNCTION world graph nodes. structural and connectivity knowledge USED-FOR exploration. trajectory data USED-FOR binary recurrent variational autoencoder ( VAE ). world graph USED-FOR structural and connectivity knowledge. learning phases PART-OF framework. structural and connectivity knowledge USED-FOR hierarchical RL framework. hierarchical RL framework HYPONYM-OF learning phases. world graphs USED-FOR RL. reward CONJUNCTION learning. learning CONJUNCTION reward. maze tasks EVALUATE-FOR approach. OtherScientificTerm are complex environments, and agents. Method is reinforcement learning ( RL ) agents. ",This paper proposes a method to learn a task-agnostic world graph from trajectory data. The proposed method is based on a binary recurrent variational autoencoder (VAE) that learns to predict trajectories from trajectories generated by a set of nodes in the world graph. The method is evaluated on a number of maze tasks and achieves state-of-the-art performance.   ,"This paper proposes a hierarchical reinforcement learning framework for learning to explore complex world graphs. The proposed method is based on a binary recurrent variational autoencoder (VAE) that learns to predict the trajectories of agents in a world graph. The authors show that the learned trajectories can be used to learn the structure and connectivity of the world graph, which is used to guide the agent's exploration. They also show that their method can be applied to a variety of tasks, including a maze task."
12080,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,Neural networks USED-FOR real - world tasks. neural network USED-FOR approximation of any type of continuous functions. non - interpretable black box model USED-FOR neural network. white box network HYPONYM-OF function constructing network. network USED-FOR function blocks. discretized layers USED-FOR network. discretization USED-FOR end - to - end PathNet structure. OtherScientificTerm is continuous functions. Task is reverse engineering. Method is neural networks. ,This paper proposes a method to learn a function-constrained neural network that can be used to approximate any type of continuous functions. The proposed method is based on discretization of the input function and a non-interpretable black-box model. The method is shown to be able to learn functions that are close to the true function.  ,"This paper proposes a method for reverse engineering neural networks that can be applied to any type of continuous functions. The method is based on discretization of neural networks. The authors propose a new method called PathNet, which is a function constructing network with discretized layers. They show that PathNet can be used to construct a function-constrained neural network that is able to approximate any continuous function. "
12084,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"approach USED-FOR control policies. Imitation learning algorithms USED-FOR control policies. Imitation learning algorithms USED-FOR approach. supervised learning methods USED-FOR control policies. supervised imitation learning USED-FOR policies. imitation learning USED-FOR policies. algorithm USED-FOR behaviors. user - provided reward functions CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION user - provided reward functions. user - provided reward functions USED-FOR algorithm. method USED-FOR goal - reaching policies. approach USED-FOR method. approach USED-FOR goal - reaching policies. approach USED-FOR imitation learning settings. self - supervised imitation learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION self - supervised imitation learning. it COMPARE reinforcement learning methods. reinforcement learning methods COMPARE it. reinforcement learning methods USED-FOR goal reaching problems. goal reaching problems EVALUATE-FOR it. OtherScientificTerm are expert demonstrator, expert demonstrations, and demonstrations. Task is multi - task setting. Method is suboptimal policy. Generic is tasks. ",This paper proposes a self-supervised imitation learning method for goal-reaching tasks where the goal is to learn a goal-conditioned policy that can reach the goal faster than an expert policy. The method is based on the observation that the expert policy can be sub-optimal in some tasks. The authors propose a method that learns a reward function that encourages the agent to reach goals faster than the expert. The proposed method is evaluated on a variety of goal reaching tasks and achieves state-of-the-art performance.,This paper proposes an imitation learning algorithm for goal-reaching control policies in multi-task imitation learning settings. The main idea is to use a user-provided reward function to motivate the learner to reach the goal. The authors show that the proposed method outperforms other imitation learning methods on a variety of goal reaching tasks. 
12088,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"Zeno++ USED-FOR non - convex problems. Zeno++ COMPARE approaches. approaches COMPARE Zeno++. OtherScientificTerm are workerserver communications, Byzantine workers, candidate gradient, optimization progress, and Byzantine failures. ","This paper proposes a new algorithm for non-convex optimization problems with Byzantine workers. The main idea is to use Byzantine workers, where workers communicate with each other in order to find the candidate gradient of the problem. The proposed algorithm is based on the idea of Byzantine workers and is able to learn Byzantine workers that are independent of each other. The method is evaluated on a variety of non-Convex problems and is shown to outperform existing methods. ","This paper proposes Zeno++ for non-convex problems with Byzantine workers. The main idea is to use Byzantine workers to encourage the candidate gradient to converge to a candidate gradient, which is then used to guide the optimization progress. The authors show that the Byzantine workers can lead to Byzantine failures, and propose a new algorithm to address this problem. The proposed algorithm is evaluated on a variety of problems, and is shown to be competitive with the state-of-the-art."
12092,SP:d16ed9bd4193d99774840783347137e938955b87,"deep neural networks ( DNNs ) HYPONYM-OF Machine learning models. defenses USED-FOR adversarial impact. Lp norm USED-FOR adversarial perturbations. unrestricted ” perturbations USED-FOR image - based visual descriptors. feature squeezing CONJUNCTION adversarially trained model. adversarially trained model CONJUNCTION feature squeezing. JPEG compression CONJUNCTION feature squeezing. feature squeezing CONJUNCTION JPEG compression. semantically aware perturbations USED-FOR JPEG compression. semantically aware perturbations USED-FOR adversarially trained model. semantically aware perturbations USED-FOR feature squeezing. ImageNet CONJUNCTION MSCOCO. MSCOCO CONJUNCTION ImageNet. image classification CONJUNCTION image captioning tasks. image captioning tasks CONJUNCTION image classification. methods USED-FOR image captioning tasks. methods USED-FOR image classification. complex datasets USED-FOR methods. complex datasets USED-FOR image captioning tasks. MSCOCO HYPONYM-OF complex datasets. ImageNet HYPONYM-OF complex datasets. OtherScientificTerm are adversarial examples, and large magnitude perturbations. Method is user studies. Material is semantic adversarial examples. Generic is attacks. ",This paper studies the impact of adversarial perturbations on image classification and image captioning tasks. The authors show that the Lp norm of the perturbation can be used as a measure of the adversarial impact on image-based visual descriptors. They show that adversarially trained models are sensitive to large magnitude adversarial examples. They also show that feature squeezing and JPEG compression can help improve the performance of image classification models.,This paper studies the impact of semantic adversarial attacks on deep neural networks (DNNs) on image classification and image captioning tasks. The authors show that the Lp norm of the adversarial perturbations can be used as a measure of the impact on image-based visual descriptors. They also show that feature squeezing and adversarially trained model can have a large impact on DNNs. The paper also shows that the effect of feature squeezing can be mitigated by semantically aware perturbation. 
12096,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,large datasets USED-FOR latent representations. neural networks USED-FOR latent representations. large datasets USED-FOR neural networks. events CONJUNCTION locations. locations CONJUNCTION events. natural language understanding ( NLU ) system USED-FOR emerging entities. RL trainable controller USED-FOR representation learning. representation learning PART-OF neural encoder. neural encoder CONJUNCTION memory management role. memory management role CONJUNCTION neural encoder. controller USED-FOR read and write operations. external memory USED-FOR read and write operations. named Learning to Control ( LTC ) USED-FOR approach. memory plasticity USED-FOR few - shot learning. system USED-FOR few - shot learning of entity recognition. Stanford Task - Oriented Dialogue dataset USED-FOR few - shot learning of entity recognition. Metric is loss function. Generic is solution. ,"This paper proposes a method for few-shot learning of entity recognition in NLP tasks. The main idea is to use an external memory to store events and locations in an encoder-decoder architecture, which is then used to update the encoder during training. The method is evaluated on the Stanford Task-Oriented Dialogue (STD) dataset.   ","This paper proposes a method for few-shot learning of entity recognition in a natural language understanding (NLU) system. The method is based on the idea of learning to control (LTC), which is an RL-based controller that learns to control a neural encoder and a memory management role. The controller is trained to control the read and write operations of the encoder, and the memory is stored in an external memory. The proposed method is evaluated on the Stanford Task-Oriented Dialogue dataset, where it achieves state-of-the-art performance."
12105,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,approach USED-FOR recomposable motor primitives. large - scale and diverse manipulation demonstrations FEATURE-OF recomposable motor primitives. approaches USED-FOR primitives. manually defined primitives USED-FOR approaches. manually defined primitives USED-FOR primitives. approaches USED-FOR primitive discovery. complexity EVALUATE-FOR primitive. latent representation USED-FOR primitives. motor primitives CONJUNCTION latent representation. latent representation CONJUNCTION motor primitives. hierarchical reinforcement learning setup USED-FOR robotic manipulation tasks. primitives USED-FOR robotic manipulation tasks. primitives PART-OF hierarchical reinforcement learning setup. ,"This paper proposes a method for learning motor primitives from large-scale and diverse manipulation demonstrations. The method is based on a hierarchical reinforcement learning approach, where the goal is to learn a set of primitives that can be used to perform a given task. The main contribution of the paper is the use of a latent representation learning approach to learn the primitives. The proposed method is evaluated on a variety of robotic manipulation tasks.","This paper proposes a new method for discovering motor primitives for robotic manipulation tasks. The method is based on a hierarchical reinforcement learning framework, where the goal is to discover a set of primitives that can be used to perform robotic manipulations. The main idea is to use a latent representation of the primitives, which is then used to train a neural network to predict the primordial primitives. The authors show that the proposed method outperforms existing methods on a variety of tasks.   "
12114,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"Transfer CONJUNCTION adaptation. adaptation CONJUNCTION Transfer. adaptation USED-FOR unknown environmental dynamics. Transfer PART-OF reinforcement learning ( RL ). adaptation PART-OF reinforcement learning ( RL ). Transfer USED-FOR unknown environmental dynamics. methods USED-FOR adaptation. experience rollouts USED-FOR adaptation. experience rollouts USED-FOR methods. general algorithm USED-FOR probe. general algorithm USED-FOR inference model. general algorithm USED-FOR latent variables of test dynamics. general algorithm USED-FOR single episode transfer. algorithms USED-FOR RL. algorithms USED-FOR variational inference. variational inference CONJUNCTION RL. RL CONJUNCTION variational inference. algorithms USED-FOR modular approach. method COMPARE adaptive approaches. adaptive approaches COMPARE method. baselines USED-FOR robust transfer. method COMPARE baselines. baselines COMPARE method. method USED-FOR robust transfer. single episode test constraint EVALUATE-FOR method. OtherScientificTerm are dense rewards, and rewards. Method is universal control policy. Generic are approach, and it. ","This paper proposes a method for transfer learning in reinforcement learning, where the goal is to learn a universal control policy that is robust to perturbations in the environment. The method is based on variational inference, where a probe is used to estimate the latent variables of the test dynamics, and then a general algorithm is used for learning the latent variable of test dynamics. The proposed method is shown to be able to achieve robust transfer in the presence of a single episode test constraint.   ","This paper proposes a method for learning a universal control policy that can adapt to the environment in a single episode. The method is based on a modular approach, where a probe is used to learn the latent variables of the test dynamics, and a general algorithm is used for learning the latent variable of test dynamics. The authors show that the proposed method is robust to the single episode test constraint. They also show that their method is able to learn a universal policy that adapts well to the new environment. "
12123,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"AlphaZero COMPARE AlphaGo Zero. AlphaGo Zero COMPARE AlphaZero. generalization EVALUATE-FOR neural net. three - head network architecture USED-FOR action - value head. action - value head USED-FOR Monte Carlo tree search ( MCTS ). search efficiency EVALUATE-FOR action - value head. three - head network USED-FOR AlphaZero style learning paradigm. threehead network architecture USED-FOR AlpahZero learning. game of Hex EVALUATE-FOR threehead network architecture. architecture USED-FOR zero - style iterative learning. neural network models COMPARE those. those COMPARE neural network models. two - head counterpart PART-OF MCTS. MCTS EVALUATE-FOR neural network models. two - head counterpart USED-FOR those. two - head counterpart USED-FOR neural network models. Method are search - based reinforcement learning algorithm AlphaZero, two - head network architecture, and two - head net. Material is chess. OtherScientificTerm is policy. ","This paper proposes a three-head network architecture for reinforcement learning. The main idea is to replace the traditional two-head architecture in AlphaZero with a single action-value head, which is used for Monte Carlo Tree Search (MCTS). The proposed method is shown to improve the performance of AlphaZero in a variety of games, including chess, Hex, and Go. ","This paper proposes a three-head network architecture for zero-style iterative learning. The main idea is to use the action-value head of a two-head neural network to perform Monte Carlo tree search (MCTS) in a search-based reinforcement learning (RL) setting. The proposed method is evaluated on the game of Hex, where it is shown to outperform two-headed neural networks. "
12132,SP:89d6d55107b6180109affe7522265c751640ad96,policy transfer PART-OF reinforcement learning. warm initialization CONJUNCTION imitation. imitation CONJUNCTION warm initialization. Policy transfer USED-FOR Reinforcement Learning ( RL ) tasks. imitation USED-FOR Policy transfer. warm initialization USED-FOR Policy transfer. biological world FEATURE-OF behavior transfer. adaptation reward CONJUNCTION environmental reward. environmental reward CONJUNCTION adaptation reward. method USED-FOR policies. sample complexity EVALUATE-FOR policies. sample complexity EVALUATE-FOR method. Material is randomized instances. Task is transfer of policies. Generic is mechanism. OtherScientificTerm is transition differences. ,"This paper studies the problem of policy transfer in reinforcement learning. In particular, the authors propose to use warm initialization and imitation to improve the transfer of policies from one environment to another. The authors show that warm initialization can be used as a way to improve policy transfer. They also show that imitation can improve the policy transfer by using warm initialization.   ","This paper studies the problem of policy transfer in reinforcement learning. The authors propose a new method for policy transfer based on warm initialization, imitation, and warm initialization. They show that warm initialization and imitation can be used to improve the transfer of policies. They also show that imitation can improve the performance of the policy transfer.  "
12141,SP:626021101836a635ad2d896bd66951aff31aa846,"scale changes FEATURE-OF tasks. steerable filters USED-FOR scale - equivariant convolutional networks. numerical stability EVALUATE-FOR method. computational efficiency EVALUATE-FOR method. computational efficiency CONJUNCTION numerical stability. numerical stability CONJUNCTION computational efficiency. scale equivariance CONJUNCTION local scale invariance. local scale invariance CONJUNCTION scale equivariance. models COMPARE methods. methods COMPARE models. methods USED-FOR scale equivariance. methods USED-FOR local scale invariance. models USED-FOR scale equivariance. models USED-FOR local scale invariance. MNIST - scale dataset CONJUNCTION STL-10 dataset. STL-10 dataset CONJUNCTION MNIST - scale dataset. supervised learning setting FEATURE-OF STL-10 dataset. Method are Convolutional Neural Networks ( CNNs ), CNNs, and scale - convolution. OtherScientificTerm are translation equivariance, and scale - equivariant. Generic is transformations. ","This paper proposes a scale-equivariant convolutional networks with steerable filters that are scale-invariant and local scale invariant. The proposed method is based on the observation that scale-convolutional neural networks (CNNs) are equivariant to translation equivariance, which is a generalization of previous work on scale-adaptive convolutions. The authors propose to use a steerable filter to learn scale-evolutionary filters. Theoretical analysis is provided to show that the proposed method can be used to train scale-efficient CNNs. Experiments are conducted on MNIST-scale and STL-10 datasets.   ",This paper proposes a new method for scale-equivariant convolutional neural networks (CNNs) that is based on steerable filters. The main idea is to train a CNN that is scale equivariant and local scale-invariant. The proposed method is evaluated on MNIST-scale dataset and STL-10 dataset. The authors show that the proposed method outperforms the state-of-the-art in terms of numerical stability and computational efficiency.
12150,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,deep learning setups USED-FOR scan completion. paired training data FEATURE-OF supervision. supervision USED-FOR methods. synthetic data EVALUATE-FOR methods. real scans USED-FOR scan completion. point clouds USED-FOR approach. Matterport3D CONJUNCTION KITTI. KITTI CONJUNCTION Matterport3D. ScanNet CONJUNCTION Matterport3D. Matterport3D CONJUNCTION ScanNet. incompleteness USED-FOR realistic completions. ScanNet HYPONYM-OF real - world datasets. real - world datasets EVALUATE-FOR approach. 3D - EPN shape completion dataset EVALUATE-FOR approach. KITTI HYPONYM-OF real - world datasets. Matterport3D HYPONYM-OF real - world datasets. Task is 3D scanning solutions. Material is raw scans. OtherScientificTerm is partial scans. Generic is approaches. ,"This paper proposes a method for 3D shape completion on point clouds. The method is based on the observation that point clouds can be used as training data for deep neural networks to perform 3D scan completion. The proposed method is evaluated on 3D-EPN and Matterport3D datasets, and achieves state-of-the-art results.","This paper proposes a new method for 3D scan completion based on point clouds. The proposed method is based on the idea of combining two sets of training data, where the first set is a set of paired training data and the second set is an unsupervised set of data. The training data consists of two sets, one of which is generated from a single point cloud, and the other is generated by combining the two sets. The method is evaluated on 3D-EPN shape completion dataset and Matterport3D dataset."
12159,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"models USED-FOR systems. authentication CONJUNCTION anomaly detection. anomaly detection CONJUNCTION authentication. sensor data USED-FOR anomaly detection. sensor data USED-FOR authentication. systems USED-FOR authentication. systems USED-FOR anomaly detection. sensor data USED-FOR systems. learning systems USED-FOR private data. authentication system USED-FOR generative impersonation attacks. optimal strategies USED-FOR attacker. optimal strategies USED-FOR Gaussian source distributions. maximin game USED-FOR problem. they USED-FOR models. real - world data USED-FOR models. Method are Generative neural models, and practical learning approaches. Generic is it. Task are learning, and information theory. Material is nominally - looking artificial data. OtherScientificTerm is optimal strategy. ",This paper studies the problem of generative impersonation attacks on generative models. The authors propose a maximin game in which the attacker tries to fool a generative model by generating adversarial samples from a Gaussian source distribution. They show that the attacker can find an optimal strategy that maximizes the probability that the generated samples are from the same distribution as the original data distribution. Theoretical analysis is provided to show the existence of a minimization of the optimal strategy. The paper also shows that the optimal strategies can be found by minimizing the variance of the Gaussian distribution.  ,"This paper studies the problem of generative impersonation attacks against generative neural models. The authors propose a maximin game where the attacker tries to impersonate a generative model using a Gaussian source distribution. They show that the attacker can find the optimal strategies for generating the source distribution, and then use these strategies to fool the generative models. They also provide a theoretical analysis of the problem. "
12168,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,robustness CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robustness. natural accuracy CONJUNCTION robustness. robustness CONJUNCTION natural accuracy. limited data samples USED-FOR high dimensional distribution. sensible adversary USED-FOR defense model. Bayes rule HYPONYM-OF multi - class classifier. 0 - 1 loss FEATURE-OF Bayes rule. 0 - 1 loss FEATURE-OF multi - class classifier. sensible adversarial learning USED-FOR Bayes rule. algorithm USED-FOR robust model. sensible adversarial examples USED-FOR robust model. robust accuracy EVALUATE-FOR PGD attacks. model USED-FOR attacks. CIFAR10 EVALUATE-FOR model. perturbations USED-FOR attacks. Generic is problem. ," is a well-studied problem in machine learning, where the goal is to improve the robustness of the model against adversarial perturbations. In this paper, the authors propose to use the Bayes rule as the adversarial model. The proposed method is based on the idea of sensible adversarial learning (SADL), which is a variant of adversarial training in which the objective is to learn a robust model that is robust to adversarial attacks. The authors show that SADL can be used to improve robustness against PGD attacks. ","This paper studies the problem of robustness of a multi-class classifier against PGD attacks. The authors propose a method to learn a robust model that is robust to PGD attack. The method is based on the Bayes rule, which is a well-known defense model for PGD. The main contribution of the paper is to propose a new method for learning the robust model, which can be applied to any classifier. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets."
12177,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,image intensity CONJUNCTION spatial correlation. spatial correlation CONJUNCTION image intensity. spatial correlation FEATURE-OF Feature maps. image intensity FEATURE-OF Feature maps. class probabilities USED-FOR online knowledge distillation methods. adversarial training framework USED-FOR online knowledge distillation method. discriminators USED-FOR feature map distributions. discriminators USED-FOR networks. discriminator USED-FOR feature map. discriminator PART-OF network. network USED-FOR feature map distribution. network USED-FOR discriminator. Discriminators CONJUNCTION networks. networks CONJUNCTION Discriminators. Discriminators PART-OF minimax twoplayer game. minimax twoplayer game USED-FOR networks. cyclic learning scheme USED-FOR networks. method USED-FOR network architectures. method USED-FOR classification task. classification task EVALUATE-FOR network architectures. Generic is small network. ,"This paper proposes an online knowledge distillation method for online image classification. The proposed method is based on a minimax minimax game between a discriminator and a network, where the discriminator is trained to predict the feature map distribution of the input image. The discriminators are trained in an adversarial fashion, and the network is trained with a cyclic learning scheme. Experiments show that the proposed method achieves state-of-the-art performance on image classification tasks. ",This paper proposes a novel online knowledge distillation method based on a minimax minimax twoplayer game. The proposed method uses a cyclic learning scheme to train a discriminator for each feature map distribution. The discriminator is trained in a way that minimizes the number of discriminators in the network. The method is evaluated on a variety of classification tasks.   
12186,SP:e43fc8747f823be6497224696adb92d45150b02d,"natural language processing tasks EVALUATE-FOR word embedding models. word embedding models USED-FOR rich semantic meanings. maximum likelihood estimation CONJUNCTION Bayesian estimation. Bayesian estimation CONJUNCTION maximum likelihood estimation. maximum likelihood estimation USED-FOR model. Bayesian estimation USED-FOR model. model COMPARE baseline methods. baseline methods COMPARE model. baseline methods USED-FOR sentiment analysis. low - frequency words FEATURE-OF sentiment analysis. sentiment analysis EVALUATE-FOR model. it USED-FOR semantic and sentiment analysis tasks. OtherScientificTerm is sentiment information. Method are sentiment word embedding model, and parameter estimating method. Task is semantic and sentiment embeddings. ",This paper proposes a new word embedding model for sentiment word embeddings. The proposed model is based on a combination of Bayesian estimation and maximum likelihood estimation. Theoretical analysis is provided to show that the proposed method is Bayesian in nature. Experiments are conducted on both semantic and sentiment analysis tasks.   ,"This paper proposes a new word embedding model for semantic and sentiment word embeddings. The proposed model is based on a combination of two techniques: maximum likelihood estimation and Bayesian estimation. The model is evaluated on a variety of tasks, and it is shown to perform better than the state-of-the-art in terms of accuracy."
12195,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"Noisy labels PART-OF real - world training data. overfitting FEATURE-OF noisy labels. maximal safe set USED-FOR early stopped network. two - phase training method USED-FOR noise - free training. Prestopping HYPONYM-OF two - phase training method. image benchmark data sets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. image benchmark data sets EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR state - of - the - art methods. test error EVALUATE-FOR method. Method is deep neural network. OtherScientificTerm are label noise, and real - world noise. ","This paper proposes a two-phase training method for noise-free training. In the first phase, the authors propose to train an early-stopped network with noisy labels and then stop the network at the end of the second phase. The proposed method is evaluated on image classification tasks and achieves state-of-the-art performance. ",This paper proposes a two-phase training method for noise-free training. The main contribution of the paper is a new two-stage training method called Prestopping. Prestopping is based on the idea that the early stopped network should be trained with a maximal safe set. The authors show that Prestopping outperforms state-of-the-art methods in terms of test error on a variety of datasets.
12204,SP:8316872d8b388587bf25f724c80155b25b6cb68e,framework USED-FOR generalization. reinforcement learning USED-FOR actions. regularization metrics USED-FOR generalization. generalization USED-FOR policy. action representations USED-FOR reinforcement learning architecture. policy USED-FOR zero - shot generalization. representation learning method CONJUNCTION policy. policy CONJUNCTION representation learning method. representation learning method USED-FOR zero - shot generalization. sequential decision - making environments USED-FOR zero - shot generalization. Task is intelligence. OtherScientificTerm is action ’s functionality. Method is unsupervised representation learning. ,"This paper proposes a method for learning representations of actions in reinforcement learning tasks. The proposed method is based on the idea of learning a representation of the action space, which is then used as a regularization metric to evaluate the generalization ability of the policy. The method is evaluated on a set of MuJoCo environments, where it is shown that the proposed method outperforms baselines in terms of generalization.","This paper proposes a new framework for zero-shot generalization in reinforcement learning. The key idea is to use a representation learning method to learn a policy that is able to generalize well in a sequential environment. The method is based on the idea of unsupervised representation learning, where the goal is to learn an action representation that maximizes the generalization ability of the policy. The authors show that the proposed method can generalize better than the state-of-the-art in a variety of environments. "
12213,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"word2vec CONJUNCTION GloVe. GloVe CONJUNCTION word2vec. vector word embeddings USED-FOR Deep learning natural language processing models. word2vec HYPONYM-OF vector word embeddings. GloVe HYPONYM-OF vector word embeddings. continuous vectors USED-FOR it. embedding vectors USED-FOR dictionary. word embedding matrix USED-FOR inference. word2ket CONJUNCTION word2ketXS. word2ketXS CONJUNCTION word2ket. word embedding matrix USED-FOR training. quantum computing USED-FOR approaches. accuracy EVALUATE-FOR natural language processing tasks. natural language processing tasks EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. OtherScientificTerm are discrete sequence of words, and GPU memory. Material is text corpus. Generic is embeddings. ","This paper proposes to use word2vec and GloVe as word embeddings to improve the performance of NLP models. The main idea is to use a vector word embedding matrix as the input to a dictionary, which is then used to compute the embedding vectors for each word in a sequence of words. The proposed method is evaluated on a variety of text classification tasks.   ","This paper proposes a novel method for learning vector word embeddings for NLP tasks. The proposed method is based on the idea of learning a dictionary of word embedding vectors from a sequence of words. The dictionary is represented as a set of continuous vectors, and the embedding vector vectors are represented as continuous vectors. The embedding matrix of the dictionary is then used to train a neural network model. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy on a variety of tasks. "
12222,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"machine learning approach USED-FOR policy. Imitation Learning ( IL ) HYPONYM-OF machine learning approach. Imitation Learning ( IL ) USED-FOR policy. human players PART-OF video games. IL USED-FOR reinforcement learning ( RL ). they USED-FOR average ” policy. dataset USED-FOR average ” policy. behavioral descriptions USED-FOR state - action pairs. approach USED-FOR neural network policy. behavior description USED-FOR neural network policy. approach USED-FOR policy. human demonstrations USED-FOR build - order planning task. StarCraft II FEATURE-OF build - order planning task. StarCraft II FEATURE-OF human demonstrations. human demonstrations USED-FOR policy. Dimensionality reduction techniques USED-FOR lowdimensional behavioral space. high - dimensional army unit composition USED-FOR lowdimensional behavioral space. UCB1 algorithm USED-FOR policy. policy COMPARE IL baseline approach. IL baseline approach COMPARE policy. Generic is it. Method are IL approaches, and Behavioral Repertoire Imitation Learning ( BRIL ). OtherScientificTerm is repertoire of behaviors. ",This paper proposes a method for imitation learning in video games. The method is based on the idea that imitation learning is a good way to learn a policy that can imitate the behavior of human players in a video game. The main idea is to use a dataset of demonstrations from human players to train a model that is able to imitate the state-action pairs from the demonstrations. The model is trained using a UCB1 algorithm that learns a policy from the dataset. The proposed method is evaluated on the StarCraft II build-order planning task and shows improved performance compared to baseline methods.,"This paper proposes a new method for imitation learning in video games. The main idea is to use a dataset of human demonstrations to train a neural network policy that can imitate the behavior of a human player. The dataset consists of a high-dimensional state-action pairs and a low-dimensional behavioral space, which is represented by a high dimensional army unit composition. A UCB1 algorithm is used to train the policy. The authors show that the proposed method outperforms the state-of-the-art IL baseline on StarCraft II."
12231,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"stochastic gradient descent optimization USED-FOR lottery ticket. weight magnitude FEATURE-OF model. gradual pruning COMPARE one - shot pruning. one - shot pruning COMPARE gradual pruning. accuracy EVALUATE-FOR one - shot pruning. accuracy EVALUATE-FOR gradual pruning. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. ImageNet EVALUATE-FOR ResNet architectures. CIFAR10 EVALUATE-FOR ResNet architectures. ImageNet FEATURE-OF ResNet50. Task is lottery ticket hypothesis. Method are small, sparsified neural networks, sparsified model, iterative pruning, and memorization capacity analysis. Generic is network. OtherScientificTerm are rewinding, transferability of the winning lottery tickets, winning ticket, structure of winning lottery tickets, lottery tickets, Pruning, complex patterns, and pruning rate. Metric is Top-1 accuracy. ","This paper studies the lottery ticket hypothesis, which claims that the success of a model can be explained by the transferability of winning lottery tickets. The authors propose to use the lottery tickets as a measure of the memorization capacity of a network, and show that a network with a large weight magnitude is more likely to memorize the winning ticket than a model with a small weight magnitude. They then propose a method to prune the weight magnitude of the model, which they call ""gradual pruning"". They show that gradual pruning is more effective than one-shot pruning in terms of accuracy and transferability. ","This paper studies the lottery ticket hypothesis, which proposes a method to study the transferability of winning lottery tickets. The key idea is that the lottery tickets are more transferable if the weights of the winning tickets are larger than the weight of the sparsified model. The authors propose a method that prunes the weights at each round of pruning, and show that the pruning rate is proportional to the weight magnitude of the model. They also show that gradual pruning is better than one-shot pruning.  "
12240,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"low - confidence detections USED-FOR unknowns. CNNs USED-FOR product operation. UDN USED-FOR product operation. CNNs USED-FOR UDN. convolutional layers USED-FOR features. product operations USED-FOR UDN. UDN USED-FOR detecting unknowns. learning process USED-FOR UDN. information - theoretic regularization strategy USED-FOR learning process. information - theoretic regularization strategy USED-FOR UDN. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. SVHN HYPONYM-OF benchmark image datasets. MNIST HYPONYM-OF benchmark image datasets. CIFAR-100 HYPONYM-OF benchmark image datasets. CIFAR-10 HYPONYM-OF benchmark image datasets. UDN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE UDN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR UDN. classification accuracy EVALUATE-FOR UDN. Method is image classification systems. Generic are they, and problem. ","This paper proposes a novel low-confidence detection method for image classification. The proposed method is based on the idea that low confidence detections can be used to detect unknowns in the input images. The method uses a product operation on the features extracted from the convolutional layers and uses a regularization strategy to regularize the learning process. Experiments are conducted on CIFAR-10, Cifar-100, SVHN, and MNIST datasets. ","This paper proposes a novel method for low-confidence detection of unknowns in image classification. The method is based on the idea of unsupervised product operation (UDN), which is a simple but effective way to detect low confidence detections. The key idea is to use a convolutional layer to extract features from the input image, and then use a regularization strategy to improve the performance of the learning process. The proposed method is evaluated on CIFAR-10, Cifar-100, SVHN, and MNIST datasets. "
12249,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"approach USED-FOR approximate Bayesian inference. Variational inference ( VI ) USED-FOR approximate Bayesian inference. approximate Bayesian inference USED-FOR highly parameterized models. Variational inference ( VI ) HYPONYM-OF approach. deep neural networks HYPONYM-OF highly parameterized models. method USED-FOR highly flexible variational distributions. coarse approximation USED-FOR method. benchmark tasks EVALUATE-FOR method. log - likelihood CONJUNCTION ELBO. ELBO CONJUNCTION log - likelihood. variational inference methods USED-FOR deep learning. method COMPARE variational inference methods. variational inference methods COMPARE method. method USED-FOR deep learning. log - likelihood EVALUATE-FOR variational inference methods. ELBO EVALUATE-FOR variational inference methods. log - likelihood EVALUATE-FOR method. ELBO EVALUATE-FOR method. CIFAR10 EVALUATE-FOR residual networks. Task is variational inference. Generic are distribution, and it. OtherScientificTerm are variational families, and Evidence Lower BOund. Method are larger scale models, and VI. ","This paper proposes a method for approximate Bayesian inference in deep neural networks. The proposed method is based on variational inference (VI), which is an approach to approximate Bayes inference in highly parameterized models. The main contribution of this paper is to introduce a new variational family, which is a family of distributions that is more flexible than previous variational distributions. The authors show that the proposed method can be used to approximate the distribution of the variational families. The method is evaluated on a variety of benchmark tasks and achieves state-of-the-art performance.","This paper proposes a new approach to approximate Bayesian inference for variational inference (VI) in the context of deep neural networks. The main contribution of the paper is to propose a new family of variational families that can be used for approximate Bayes inference. This family is called Evidence Lower BOund. The authors show that this family can be applied to a variety of distributions, and that it is more flexible than the standard VI family. They also provide a coarse approximation for this family, and show that their method is able to perform well on a wide range of benchmark tasks."
12258,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,user - defined metrics USED-FOR reinforcement learning. reinforcement learning USED-FOR Sequence generation models. correlated Monte Carlo ( MC ) rollouts USED-FOR variance control. correlated Monte Carlo ( MC ) rollouts USED-FOR policy gradient estimator. policy gradient estimator USED-FOR contextual generation of categorical sequences. rollouts USED-FOR model uncertainty. correlated MC rollouts USED-FOR binary - tree softmax models. large vocabulary scenarios FEATURE-OF high generation cost. binary actions FEATURE-OF categorical action. neural program synthesis CONJUNCTION image captioning. image captioning CONJUNCTION neural program synthesis. image captioning EVALUATE-FOR methods. neural program synthesis EVALUATE-FOR methods. gradient variance EVALUATE-FOR methods. Generic is method. OtherScientificTerm is correlation. ,This paper proposes to use correlated Monte Carlo (MC) rollouts for policy gradient estimator for contextual generation of categorical sequences. The proposed method is based on correlated MC rollouts to reduce the model uncertainty and control the variance control. The authors show that the proposed method can improve the gradient variance control in binary-tree softmax models with large vocabulary scenarios.  ,This paper proposes a new method to improve the variance control of sequence generation models by using correlated Monte Carlo (MC) rollouts for the policy gradient estimator. The authors show that correlated MC rollouts can be used to control the variance of the gradient of the policy in the context of categorical sequences. They also show that the proposed method can be applied to binary-tree softmax models with large vocabulary scenarios. 
12267,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"Goal recognition USED-FOR applications. goal recognition design USED-FOR online goal recognition process. goal recognition control HYPONYM-OF stages. deceptive opponent modeling HYPONYM-OF stages. proactively static interdiction USED-FOR goal recognition control. worst case distinctiveness ( wcd ) USED-FOR nondistinctive path. approach USED-FOR goal recognition process. opponent ’s deceptive behavior USED-FOR approach. opponent ’s deceptive behavior USED-FOR goal recognition process. OtherScientificTerm are hard action removal, and wcd. Method is S - GRC. "," is an online goal recognition system, where the goal is to reach a goal in a goal-conditioned environment.  The paper proposes a novel goal recognition method, called S-GRC, which uses a deceptive opponent model to model the goal recognition process. The main idea is to use the worst-case distinctiveness (wcd) as a metric to measure the distance between the goal and a nondistinctive path. The paper shows that wcd can be used to estimate the distance of the goal from the nondistinguished path.   ","This paper proposes a method for online goal recognition based on the worst-case distinctiveness (wcd) of the opponent. The key idea is to model the opponent’s deceptive behavior and use it to improve the goal recognition process. The method is evaluated on a variety of tasks, including goal recognition, goal detection, and goal recognition control.   "
12276,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,large mini - batch sizes USED-FOR Generative Adversarial Networks ( GANs ). Coreset - selection USED-FOR active learning. Coreset - selection USED-FOR method. Coreset - selection USED-FOR batch. training time CONJUNCTION memory usage. memory usage CONJUNCTION training time. it USED-FOR GANs. GANs USED-FOR anomaly detection. training time EVALUATE-FOR GAN variants. it USED-FOR dropped modes. technique USED-FOR GAN variants. memory usage EVALUATE-FOR GAN variants. it USED-FOR anomaly detection. dropped modes PART-OF synthetic dataset. memory usage EVALUATE-FOR technique. training time EVALUATE-FOR technique. synthetic dataset EVALUATE-FOR it. Method is GAN. Material is real ’ images. Generic is them. ,-based GANs are known to suffer from large mini-batch sizes. This paper proposes a Coreset-based active learning method to reduce the training time and memory usage in GAN training. The coreset-selection is based on the fact that the number of samples in the mini-batches is limited by the size of the training set. The proposed method is evaluated on a synthetic dataset as well as a real-world dataset. ,"This paper proposes a method for training GANs with large mini-batch sizes. The method is based on Coreset-selection, which is an active learning method that selects a subset of the mini-batch to be used for active learning. The coreset selection is done by selecting a set of mini-batches that are similar in size to the original batch. The authors show that this method can be used to train a GAN with a large batch size. They also show that it can be applied to a variety of GAN variants. "
12285,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,artificial neurons PART-OF recurrent neural networks ( RNNs ). maximum likelihood USED-FOR recurrent neural networks ( RNNs ). restorative Brownian motion CONJUNCTION hand - drawn sketch dataset. hand - drawn sketch dataset CONJUNCTION restorative Brownian motion. information plane FEATURE-OF RNNs. hand - drawn sketch dataset HYPONYM-OF datasets. restorative Brownian motion HYPONYM-OF datasets. RNNs USED-FOR predictive information. maximum likelihood CONJUNCTION contrastive loss training. contrastive loss training CONJUNCTION maximum likelihood. noise USED-FOR hidden state. predictive information USED-FOR maximum likelihood. RNNs USED-FOR contrastive loss training. predictive information USED-FOR contrastive loss training. noise USED-FOR past information. Method is biological neurons. ,"This paper studies the use of maximum likelihood and contrastive loss training in recurrent neural networks. The authors propose to use maximum likelihood to train RNNs in the information plane, where the hidden state is represented as a mixture of the current state and the past state. They show that the proposed method is able to learn the hidden states of RNN models in a similar way as biological neurons. They also show that contrastive training can be used to improve the performance of the model. ","This paper proposes a novel approach to training RNNs with maximum likelihood and contrastive loss training. The main contribution of the paper is to study the relationship between maximum likelihood, contrastive training, and predictive information in the information plane of a RNN. The authors show that contrastive and maximum likelihood training can be combined to improve the performance of a recurrent neural network (RNN) in terms of predictive information. They also show that the proposed approach can be applied to biological neurons as well."
12294,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"techniques USED-FOR delusion. methods USED-FOR delusional bias. Q - approximators USED-FOR methods. penalization scheme USED-FOR Q - labels. search framework USED-FOR premature ( implicit ) policy commitments. search framework USED-FOR Q - approximators. methods USED-FOR Q - learning. Atari games EVALUATE-FOR Q - learning. Atari games EVALUATE-FOR methods. Task is Delusional bias. Method are approximate Q - learning, and tabular value estimates. OtherScientificTerm are greedy policy class, and expressible policy class. ","This paper studies the problem of ""delusional bias"" in approximate Q-learning in the presence of a greedy policy class. The authors propose a penalization scheme to penalize the Q-labels in the policy class, and a search framework to find Q-approximators that do not commit premature (implicit) policy commitments. The proposed methods are evaluated on a variety of Atari games.   ","This paper studies the problem of ""delusional bias"" in approximate Q-learning, where a greedy policy class is greedy and the goal is to learn a policy that maximizes the Q-label. The authors propose a new penalization scheme to penalize the greedy class, which penalizes the early (implicit) policy commitments. They also propose a search framework to find Q-approximators that can be used to find the best Q-labels. They evaluate their method on Atari games and show that their method outperforms the state-of-the-art in terms of accuracy."
12303,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"approaches USED-FOR unsupervised object - oriented scene representation learning. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. scene - mixture approaches USED-FOR approaches. spatial - attention USED-FOR approaches. spatial - attention CONJUNCTION scene - mixture approaches. scene - mixture approaches CONJUNCTION spatial - attention. generative latent variable model USED-FOR unified probabilistic modeling framework. scene - mixture approaches USED-FOR unified probabilistic modeling framework. spatial - attention USED-FOR unified probabilistic modeling framework. SPACE HYPONYM-OF generative latent variable model. factorized object representations USED-FOR foreground objects. SPACE USED-FOR factorized object representations. SPACE USED-FOR methods. SPACE USED-FOR scalability problems. parallel spatial - attention USED-FOR methods. IODINE CONJUNCTION GENESIS. GENESIS CONJUNCTION IODINE. SPACE COMPARE SPAIR. SPAIR COMPARE SPACE. Atari CONJUNCTION 3D - Rooms. 3D - Rooms CONJUNCTION Atari. SPACE COMPARE IODINE. IODINE COMPARE SPACE. SPAIR CONJUNCTION IODINE. IODINE CONJUNCTION SPAIR. SPACE COMPARE GENESIS. GENESIS COMPARE SPACE. OtherScientificTerm are complex multi - object scenes, higher - level cognition, and complex morphology. Task is modeling real - world scenes. Generic is models. ","This paper proposes a new method for unsupervised object-oriented scene representation learning. The proposed method is based on a generative latent variable model, which is able to model complex multi-object scenes. The model is trained using a combination of spatial-attention and scene-mixture approaches. The method is evaluated on Atari and 3D-Rooms. ","This paper proposes a generative latent variable model for unsupervised object-oriented scene representation learning. The authors propose a unified probabilistic modeling framework that combines spatial-attention and scene-mixture approaches. The proposed model, called SPACE, is able to model complex multi-object scenes with higher-level cognition and complex morphology. The experiments show that the proposed model outperforms IODINE, SPAIR, and GENESIS. "
12312,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,classification tasks EVALUATE-FOR Convolutional Neural Networks ( CNN ). depthwise convolution CONJUNCTION pointwise convolution. pointwise convolution CONJUNCTION depthwise convolution. convolution CONJUNCTION depthwise convolution. depthwise convolution CONJUNCTION convolution. convolution PART-OF depthwise separable convolution. accuracies EVALUATE-FOR convolution. method USED-FOR compressing CNN. FALCON HYPONYM-OF method. FALCON USED-FOR compressing CNN. mathematical formulation USED-FOR convolution kernel. FALCON USED-FOR convolution methods. depthwise separable convolution USED-FOR convolution methods. EHP USED-FOR convolution methods. compression CONJUNCTION computation reduction rates. computation reduction rates CONJUNCTION compression. computation reduction rates EVALUATE-FOR generalized version rank - k FALCON. accuracy EVALUATE-FOR generalized version rank - k FALCON. compression EVALUATE-FOR generalized version rank - k FALCON. FALCON PART-OF convolution unit ShuffleUnitV2. FALCON USED-FOR FALCON - branch. FALCON - branch COMPARE methods. methods COMPARE FALCON - branch. FALCON COMPARE methods. methods COMPARE FALCON. FALCON CONJUNCTION FALCON - branch. FALCON - branch CONJUNCTION FALCON. FALCON - branch COMPARE CNN models. CNN models COMPARE FALCON - branch. methods COMPARE CNN models. CNN models COMPARE methods. depthwise separable convolution USED-FOR methods. compression EVALUATE-FOR CNN models. accuracy EVALUATE-FOR CNN models. parameters CONJUNCTION floating - point operations. floating - point operations CONJUNCTION parameters. rank - k FALCON COMPARE convolution. convolution COMPARE rank - k FALCON. accuracy EVALUATE-FOR convolution. floating - point operations USED-FOR rank - k FALCON. parameters USED-FOR rank - k FALCON. accuracy EVALUATE-FOR rank - k FALCON. Generic is they. Method is heuristic approaches. ,"This paper proposes a new method to compress convolutional neural networks. The proposed method is based on depthwise separable convolution, which is an extension of depthwise convolution. The main idea is to use the convolution kernel as the input to a depth-separable separable layer. The method is evaluated on a variety of image classification tasks.   ","This paper proposes a new method for compressing convolutional neural networks (CNNs). The proposed method, called FALCON, is based on depthwise separable convolution (FALCON), which is a mathematical formulation of the convolution kernel. The main contribution of the paper is that the proposed method is able to compress CNNs more efficiently than previous methods. The method is evaluated on a variety of tasks, and it is shown that it can reduce the computational cost of training CNNs by a significant margin."
12321,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"Batch Normalization HYPONYM-OF normalization layers. normalization algorithm USED-FOR small batch sizes. Ghost Batch Normalization USED-FOR small and medium batch sizes. scaling and shifting parameters FEATURE-OF weight decay regularization. Batch and Group Normalization USED-FOR normalization algorithm. SVHN CONJUNCTION Caltech-256. Caltech-256 CONJUNCTION SVHN. CUB-2011 CONJUNCTION ImageNet. ImageNet CONJUNCTION CUB-2011. Oxford Flowers-102 CONJUNCTION CUB-2011. CUB-2011 CONJUNCTION Oxford Flowers-102. CIFAR-100 CONJUNCTION SVHN. SVHN CONJUNCTION CIFAR-100. Caltech-256 CONJUNCTION Oxford Flowers-102. Oxford Flowers-102 CONJUNCTION Caltech-256. ImageNet HYPONYM-OF datasets. CIFAR-100 HYPONYM-OF datasets. CUB-2011 HYPONYM-OF datasets. Caltech-256 HYPONYM-OF datasets. Oxford Flowers-102 HYPONYM-OF datasets. SVHN HYPONYM-OF datasets. Method are neural network architectures, and deep architectures. Generic are they, and method. Task is inference normalization statistics. OtherScientificTerm is training vs. inference discrepancy. ","This paper proposes a new normalization method called ""Ghost Batch Normalization"" to reduce the inference discrepancy between training and inference in neural networks. The proposed method is based on the idea of ""ghost normalization"", which is an extension of batch normalization. The main idea is to use a combination of Batch and Group Normalization, where Batch normalization is used to regularize the weight decay of the weights in the training set, and Group normalization acts as a weight decay regularization on the weights of the inference set. The authors show that the proposed method can reduce the training discrepancy between the two normalization methods.   ","This paper proposes a new normalization method for batch normalization, called Ghost Batch Normalization, which can be applied to both small and medium batch sizes. The main contribution of the paper is to propose a new regularization method that can be used for both batch and group normalization. The proposed method is based on the idea of weight decay regularization, where the weights of the weights are scaled and shifting parameters are shifted during training and inference. The authors show that the proposed method outperforms the state-of-the-art in terms of training accuracy on CUB-2011, SVHN, and Caltech-256 datasets."
12330,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"outliers CONJUNCTION misclassifications. misclassifications CONJUNCTION outliers. manual data inspection USED-FOR privacy - sensitive datasets. metrics CONJUNCTION model parameters. model parameters CONJUNCTION metrics. model parameters HYPONYM-OF aggregated outputs. metrics HYPONYM-OF aggregated outputs. federated methods CONJUNCTION formal differential privacy guarantees. formal differential privacy guarantees CONJUNCTION federated methods. formal differential privacy guarantees FEATURE-OF generative models. federated methods USED-FOR generative models. algorithm USED-FOR differentially private federated GANs. text with differentially private federated RNNs CONJUNCTION images. images CONJUNCTION text with differentially private federated RNNs. algorithm USED-FOR images. Task are machine learning, Manual inspection of raw data, and federated learning. Generic are models, two, and methods. OtherScientificTerm are modeling hypotheses, and human - provided labels. ",This paper studies the problem of federated learning in the presence of outliers and misclassifications. The authors propose a federated federated GAN framework that is differentially private with respect to the label distribution. The proposed method is based on the idea that the model parameters and the metrics are independent of each other and can be used to train a generative model in federated fashion. Theoretical analysis is provided to show that the proposed method can be viewed as an extension of existing federated generative models with differential privacy guarantees. ,"This paper studies the problem of differentially private federated GANs. The authors propose a new algorithm for federated generative models, which is based on the notion of ""differentially private"" data. The main contribution of the paper is that the authors provide a formal differential privacy guarantee for the generative model. They also provide a theoretical analysis of the performance of the proposed algorithm. "
12339,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"Learning diverse and natural behaviors USED-FOR intelligent characters. method USED-FOR generating long range diverse and distinctive behaviors. method USED-FOR motion of human. non - parametric techniques CONJUNCTION parametric ones. parametric ones CONJUNCTION non - parametric techniques. non - parametric techniques USED-FOR method. parametric ones USED-FOR method. memory bank USED-FOR motion references. deep network USED-FOR synthesis. skeleton datasets EVALUATE-FOR method. method COMPARE parametric and non - parametric baselines. parametric and non - parametric baselines COMPARE method. skeleton datasets EVALUATE-FOR parametric and non - parametric baselines. OtherScientificTerm are long range diverse and distinctive behaviors, and starting and ending state. Material is animated world. ",This paper proposes a method for generating long-range diverse and distinctive behaviors in animated characters. The method is based on a memory bank of motion references and a deep network to generate motion references for each frame. The proposed method is evaluated on a variety of skeleton datasets and achieves state-of-the-art performance. ,"This paper proposes a method for generating long-range diverse and distinctive behaviors in animated characters. The key idea is to learn diverse and natural behaviors by generating long range diverse behaviors, and starting and ending states. The method is based on a memory bank of motion references, which are stored in a deep network. The memory bank is used to store motion references for each character, and the motion references are stored by a deep neural network, which is then used to generate a sequence of skeleton animations. "
12348,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,neural networks USED-FOR natural language processing tasks. model USED-FOR semantic compositions. hierarchies FEATURE-OF non - additivity and context independent importance attributions. inconsistent explanation quality EVALUATE-FOR models. contextual decomposition HYPONYM-OF hierarchical explanations. Sampling and Contextual Decomposition ( SCD ) algorithm CONJUNCTION Sampling and Occlusion ( SOC ) algorithm. Sampling and Occlusion ( SOC ) algorithm CONJUNCTION Sampling and Contextual Decomposition ( SCD ) algorithm. algorithms COMPARE hierarchical explanation algorithms. hierarchical explanation algorithms COMPARE algorithms. LSTM models CONJUNCTION BERT Transformer models. BERT Transformer models CONJUNCTION LSTM models. Human and metrics evaluation EVALUATE-FOR BERT Transformer models. Human and metrics evaluation EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR LSTM models. BERT Transformer models EVALUATE-FOR algorithms. Human and metrics evaluation EVALUATE-FOR hierarchical explanation algorithms. algorithms USED-FOR semantic composition. algorithms USED-FOR classification rules. models USED-FOR semantic composition. Task is hierarchical explanation of neural network predictions. OtherScientificTerm is word and phrase compositions. ,"This paper studies the problem of hierarchical explanation of neural network predictions in natural language processing tasks. The authors propose a sampling and contextual decomposition (SCD) algorithm and a Sampling and Occlusion (SOC) algorithm for hierarchical explanation. The main contribution of the paper is that the authors show that hierarchical explanations are inconsistent in their explanation quality.   The authors also show that the explanation quality is inconsistent in the context independent importance attributions, which is a common problem in hierarchical explanations. ","This paper proposes two new hierarchical explanation algorithms for LSTM and BERT Transformer models. The authors propose Sampling and Contextual Decomposition (SCD) and SOC (Sampling and Occlusion (SOC) algorithms), which are based on the idea of contextual decomposition. The proposed methods are evaluated on both human and metrics evaluation. "
12357,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"AI saliency map method USED-FOR deep convolutional neural networks ( CNN ). AI saliency map method COMPARE gradient methods. gradient methods COMPARE AI saliency map method. deep convolutional neural networks ( CNN ) COMPARE gradient methods. gradient methods COMPARE deep convolutional neural networks ( CNN ). accuracy EVALUATE-FOR It. Saliency Map Order Equivalence USED-FOR saliency measures. Layer Ordered Visualization of Information USED-FOR scale / layer contributions. scale information contributions PART-OF network. it COMPARE Guided Backprop. Guided Backprop COMPARE it. layers PART-OF network. forward pass USED-FOR layers. forward pass USED-FOR method. Grad - CAM++ CONJUNCTION Smooth Grad - CAM++. Smooth Grad - CAM++ CONJUNCTION Grad - CAM++. Grad - CAM CONJUNCTION Grad - CAM++. Grad - CAM++ CONJUNCTION Grad - CAM. method COMPARE Guided Backprop. Guided Backprop COMPARE method. method COMPARE class activation methods. class activation methods COMPARE method. Guided Backprop COMPARE class activation methods. class activation methods COMPARE Guided Backprop. Smooth Grad - CAM++ HYPONYM-OF class activation methods. Grad - CAM HYPONYM-OF class activation methods. Grad - CAM++ HYPONYM-OF class activation methods. cell phones CONJUNCTION low cost industrial devices. low cost industrial devices CONJUNCTION cell phones. robots CONJUNCTION cell phones. cell phones CONJUNCTION robots. resource limited platforms USED-FOR methods. low cost industrial devices HYPONYM-OF resource limited platforms. robots HYPONYM-OF resource limited platforms. cell phones HYPONYM-OF resource limited platforms. method USED-FOR CNNs 1. Generic is technique. OtherScientificTerm are network scale, and memory footprint. Method are saliency map, and saliency map methods. Task is satellite image processing. ",This paper proposes a new saliency map method for deep convolutional neural networks (CNNs) based on order-equivalence between saliency measures. The proposed method is based on layer-ordered visualization of information (LIVI) and aims to estimate the scale information contributions of each layer in the network. The method is evaluated on image classification tasks on CIFAR-10 and ImageNet.   ,"This paper proposes a new saliency map method for deep convolutional neural networks (CNNs). The proposed method is based on the idea of order-equivalence between the scale and layer contributions of the network. The authors show that the proposed method can be used to improve the accuracy of CNNs. The method is evaluated on a variety of datasets, and it is shown to be competitive with the state-of-the-art."
12366,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"Non - autoregressive models USED-FOR text generation tasks. position modeling PART-OF non - autoregressive text generation. PNAT USED-FOR text generative process. positions USED-FOR PNAT. PNAT COMPARE baselines. baselines COMPARE PNAT. machine translation CONJUNCTION paraphrase generation tasks. paraphrase generation tasks CONJUNCTION machine translation. paraphrase generation tasks EVALUATE-FOR PNAT. machine translation EVALUATE-FOR PNAT. OtherScientificTerm are positions of generated words, and latent variable. ","This paper proposes a novel method for non-autoregressive text generation based on position modeling. The proposed method is based on the observation that the position of generated words is a latent variable in the text generation process, which can be used to model the generative process. The method is evaluated on two tasks: machine translation and paraphrase generation tasks.  ","This paper proposes a new method for non-autoregressive text generation based on position modeling. The proposed method is based on the idea that the position of generated words can be represented as a latent variable, and that the latent variable can be used to model the generative process. The method is evaluated on a variety of tasks, including machine translation and paraphrase generation. The results show that the proposed method outperforms the state of the art. "
12375,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,GANs USED-FOR generative model analysis. Random Path Generative Adversarial Network ( RPGAN ) HYPONYM-OF GANs. random paths PART-OF generator network. latent space FEATURE-OF GAN. latent space PART-OF RPGAN. random paths PART-OF latent space. design USED-FOR factors of variation. natural interpretability FEATURE-OF factors of variation. generator layers USED-FOR factors of variation. layers USED-FOR image generation process. RPGAN USED-FOR image generation process. RPGAN model USED-FOR incremental learning. interpretability EVALUATE-FOR RPGAN model. generation quality EVALUATE-FOR RPGAN model. OtherScientificTerm is Gaussian distribution. ,This paper proposes a novel random path generative adversarial network (RPGAN) to improve the interpretability of GANs. The main idea is to use random paths in the latent space of the generator network to capture the factors of variation in the image generation process. The generator layers are trained to generate images with random paths. The experimental results show that the proposed method outperforms the baselines in terms of generation quality and incremental learning.,"This paper proposes a novel approach to interpretability in GANs. The key idea is to use a random path generation network (RPGAN) to generate images from the latent space of a GAN, which is then used to train a generative model. The method is based on a Gaussian distribution, which can be viewed as a combination of two Gaussian distributions: (1) a Gaussian distribution, and (2) a random distribution. The authors show that the generated images are more interpretable than those generated by the original GAN. They also show that their method can be used for incremental learning."
12384,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"convolution USED-FOR spherical neural network. efficiency CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION efficiency. DeepSphere HYPONYM-OF method. graph representation of the sampled sphere USED-FOR method. graph USED-FOR equivariance. Generic is formulation. Method are anisotropic filters, and deepsphere. ","This paper studies the problem of training a convolutional neural network on a sphere with anisotropic filters. The authors propose a method called DeepSphere, which uses a graph representation of the sampled sphere as the input to the convolution network. The proposed method is shown to be efficient and equivariant to the rotation of the sphere. ","This paper proposes a new method for training a spherical neural network with anisotropic filters. The main idea is to use a graph representation of the sampled sphere as the basis for training the neural network. The proposed method, DeepSphere, is based on the idea of rotation equivariance. The authors show that the proposed method can be used to improve the efficiency of a spherical convolutional neural network, and that it can be applied to a variety of applications."
12393,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"Learning Invariant Representations USED-FOR deep classifiers. invariance USED-FOR compression of representations. weighting representations USED-FOR representation distributions. adaptability EVALUATE-FOR weighting representations. compression CONJUNCTION invariance of learned representations. invariance of learned representations CONJUNCTION compression. adaptability EVALUATE-FOR representation. compression risk EVALUATE-FOR representation. learning weighted representations USED-FOR constraint of invariance. Metric are minimal combined domain error, and risk of compression. OtherScientificTerm are representation invariance, and constraint. Generic is bound. Material is domain adaptation benchmark. Method is adaptation methods. ","This paper studies the problem of learning invariant representations for domain adaptation. The authors show that the risk of compression of learned representations can be controlled by the invariance of the learned representations. The main contribution of the paper is a theoretical analysis of the compression risk of invariance and adaptability of learned representation. They show that there is a trade-off between compression and invariance, and show that invariance leads to a lower compression risk. They also show that weighting representations can help reduce the compression of representations. ",This paper studies the problem of learning representations that are invariant to compression and adaptability to different domains. The authors provide a bound on the risk of compression of representations learned by weighting representations. They show that the invariance of learned representations is bounded by the combined domain error. They also provide an empirical analysis of the adaptation risk of weighting representation. 
12402,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"learning USED-FOR user interface attributes. it USED-FOR synthetic training dataset. colors CONJUNCTION border radius. border radius CONJUNCTION colors. border radius CONJUNCTION shadow or text properties. shadow or text properties CONJUNCTION border radius. shadow or text properties HYPONYM-OF attributes. border radius HYPONYM-OF attributes. colors HYPONYM-OF attributes. imitation learning USED-FOR neural policy. approach USED-FOR inferring Android Button attribute values. accuracy EVALUATE-FOR dataset. real - world Google Play Store applications FEATURE-OF dataset. dataset EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Task is user interface implementation. Method are black box rendering engine, and neural models. Metric is pixel - level accuracy. OtherScientificTerm are attribute space, and Android Button attribute values. ","This paper proposes a method for inferring user interface attributes such as colors, border radius, shadow or text properties. The method is based on imitation learning, where a neural network is trained on a synthetic training dataset and a real-world dataset of Android Button attribute values. The model is trained to predict pixel-level accuracy and pixel-wise accuracy. The paper shows that the method is able to achieve state-of-the-art accuracy on the synthetic dataset.","This paper proposes a method for inferring user interface attributes from a synthetic training dataset of Android Button attributes. The key idea is to use imitation learning to learn the attributes of the user interface, and then use the learned attributes to train a neural network to infer the attributes from the training data. The method is evaluated on real-world Google Play Store applications, and it achieves state-of-the-art accuracy."
12411,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"Transfer learning USED-FOR neural network classifiers. data scarcity CONJUNCTION computational limitations. computational limitations CONJUNCTION data scarcity. robust feature extractors PART-OF robust networks. feature extractors USED-FOR classifiers. strategies USED-FOR models. Generic are network, and model. Method are full - scale training, robust transfer learning, lifelong learning strategies, and adversarial training. Metric are robustness, accuracy, and generalization of adversarially trained models. ","This paper studies the problem of robust transfer learning in the context of adversarial training, where the goal is to transfer the robustness of a trained model to a new training set. The authors propose lifelong learning strategies to improve the generalization ability of adversarially trained models. The proposed methods are evaluated on a variety of image classification tasks, and compared with a number of baselines.",This paper studies the problem of robust transfer learning in the context of adversarially trained models. The authors propose a new metric to measure the robustness of a model trained with adversarial training. The metric is based on the number of robust features extracted from the training data. They show that a robust model is more robust than a fully-trained model with robust feature extractors. They also show that robust models with robust features are more robust to data scarcity and computational limitations. 
12420,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"natural language USED-FOR complex concepts. compositionality USED-FOR complex concepts. natural language USED-FOR compositionality. neural agents USED-FOR language games. compositionality FEATURE-OF language. neural agents USED-FOR communication protocols. neural iterated learning ( NIL ) algorithm USED-FOR structured type of language. neural iterated learning ( NIL ) algorithm USED-FOR interacting neural agents. OtherScientificTerm are limited vocabulary, and compositional language. Generic is languages. Method are NIL, probabilistic model of NIL, and neural agent communication. ",This paper proposes a neural iterated learning (NIL) algorithm for language games where the goal is to learn a compositionality of the language. The authors propose a probabilistic model of NIL and show that NIL is able to learn language with compositionality in a language game. They also show that the NIL algorithm can be used to learn communication protocols in language games.  ,"This paper proposes a probabilistic model of neural iterated learning (NIL) for language games. The authors show that NIL can be used to learn a structured type of language that is compositional in nature. They also show that this structured language can be combined with neural agents in a language game. They show that the proposed method can be applied to a variety of language games, including language games where the language is composed."
12429,SP:add48154b31c13f48aef740e665f23694fa83681,inference CONJUNCTION learning. learning CONJUNCTION inference. black - box algorithm USED-FOR inference. black - box algorithm USED-FOR learning. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR inference. Adversarial Variational Inference and Learning ( AdVIL ) USED-FOR learning. learning USED-FOR Markov random field ( MRF ). Adversarial Variational Inference and Learning ( AdVIL ) HYPONYM-OF black - box algorithm. variational distributions USED-FOR latent variables. AdVIL USED-FOR partition function. variational distributions USED-FOR partition function. partition function FEATURE-OF MRF. variational distributions USED-FOR AdVIL. variational distributions USED-FOR negative log - likelihood. negative log - likelihood FEATURE-OF MRF. stochastic gradient descent USED-FOR minimax optimization problem. minimax optimization problem USED-FOR negative log - likelihood. contrastive divergence COMPARE AdVIL. AdVIL COMPARE contrastive divergence. AdVIL USED-FOR MRFs. black - box methods COMPARE AdVIL. AdVIL COMPARE black - box methods. AdVIL USED-FOR log partition function. OtherScientificTerm is model structure. ,"This paper proposes AdVIL, a black-box algorithm for learning and inference in Markov random fields. The main idea is to use variational inference to learn the partition function of the MRF, which is then used to compute the negative log-likelihood of the latent variable. This is done by minimizing a minimax optimization problem with stochastic gradient descent. Theoretical results show that the proposed method outperforms the state-of-the-art contrastive learning algorithms in terms of the log-log likelihood. ","This paper proposes Adversarial Variational Variational Inference and Learning (AdVIL), a black-box algorithm for learning Markov random field (MRF) with negative log likelihood. AdVIL learns a variational distribution over the latent variables of the MRF using a stochastic gradient descent (SGD) algorithm. The authors show that the proposed method outperforms contrastive divergence in terms of log-likelihood. "
12438,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"low - dimensional state of the environment USED-FOR robot manipulation tasks. state estimator USED-FOR reward function. reinforcement learning USED-FOR policy. high - dimensional observations USED-FOR reward function. indicator reward function USED-FOR goal - conditioned reinforcement learning. continuous state spaces FEATURE-OF indicator reward function. reward balancing CONJUNCTION reward filtering. reward filtering CONJUNCTION reward balancing. methods USED-FOR convergence. indicator rewards USED-FOR methods. reward filtering HYPONYM-OF methods. indicator rewards USED-FOR convergence. reward balancing HYPONYM-OF methods. method USED-FOR tasks. continuous state spaces USED-FOR method. continuous state spaces FEATURE-OF tasks. RGB - D images HYPONYM-OF continuous state spaces. RGB - D images USED-FOR rope manipulation. rope manipulation HYPONYM-OF continuous state spaces. OtherScientificTerm are deformable objects, high - dimensional sensor inputs, positive reward, positive rewards, ground - truth state, and rewards. Method is end - to - end policy. ","This paper proposes an end-to-end reinforcement learning method for continuous state-space state estimation in robotic manipulation tasks with deformable objects. The proposed method is based on a state estimator that estimates the ground-truth state of the environment from high-dimensional sensor inputs, and uses a goal-conditioned reinforcement learning algorithm to learn a policy that maximizes the reward function. The method is evaluated on a variety of tasks with continuous state spaces, and achieves state-of-the-art performance. ","This paper proposes an end-to-end goal-conditioned reinforcement learning method for robot manipulation tasks. The goal is to learn a state estimator of the ground-truth state of the environment in a continuous state space, which is then used to train a policy that maximizes the reward function. The authors show that their method converges to the ground truth state in a state space with high-dimensional observations. They also show that the convergence rate of their method is better than that of reward balancing and reward filtering."
12447,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"Robustness verification USED-FOR prediction behavior. Robustness verification USED-FOR safety guarantees. Robustness verification USED-FOR neural networks. architectures USED-FOR neural networks. robustness verification problem USED-FOR Transformers. cross - nonlinearity CONJUNCTION cross - position dependency. cross - position dependency CONJUNCTION cross - nonlinearity. self - attention layers FEATURE-OF Transformers. robustness verification algorithm USED-FOR Transformers. method COMPARE naive Interval Bound Propagation. naive Interval Bound Propagation COMPARE method. method COMPARE those. those COMPARE method. certified robustness bounds COMPARE those. those COMPARE certified robustness bounds. naive Interval Bound Propagation USED-FOR those. naive Interval Bound Propagation USED-FOR certified robustness bounds. method USED-FOR certified robustness bounds. bounds USED-FOR Transformers. OtherScientificTerm is model behavior. Task are verification, and sentiment analysis. Generic is they. ",This paper studies the problem of robustness verification for neural networks with self-attention layers. The authors propose a new method for verifying the robustness of neural networks in the presence of cross-nonlinearity and cross-position dependency. The proposed method is based on the idea of interval bound propagation (IBP). The authors show that the proposed IBP can be used to obtain robustness bounds for Transformers. The main contribution of the paper is to show that IBP is equivalent to the naive IBP. ,"This paper studies the problem of robustness verification for neural networks with self-attention layers. In particular, the authors propose a new method for verifying the robustness of Transformers with cross-nonlinearity, cross-position dependency, and sentiment analysis. The proposed method is based on the naive Interval Bound Propagation (IBP) method. The authors show that the proposed method outperforms the naive IBP method in terms of certified robustness."
12456,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"self - supervised learning USED-FOR natural language processing ( NLP ) tasks. pretrained language models USED-FOR self - supervised learning. syntactic and semantic NLP tasks EVALUATE-FOR pretrained models. real - world knowledge FEATURE-OF tasks. tasks EVALUATE-FOR pretrained models. zero - shot fact completion task USED-FOR pretrained models. BERT HYPONYM-OF pretrained models. weakly supervised pretraining objective USED-FOR model. objective USED-FOR Models. fact completion task EVALUATE-FOR objective. fact completion task EVALUATE-FOR Models. TriviaQA CONJUNCTION SearchQA. SearchQA CONJUNCTION TriviaQA. SearchQA CONJUNCTION Quasar - T. Quasar - T CONJUNCTION SearchQA. WebQuestions CONJUNCTION TriviaQA. TriviaQA CONJUNCTION WebQuestions. model COMPARE BERT. BERT COMPARE model. model USED-FOR downstream tasks. FIGER HYPONYM-OF fine - grained entity typing dataset. fine - grained entity typing dataset EVALUATE-FOR model. entity - related question answering datasets EVALUATE-FOR BERT. entity - related question answering datasets EVALUATE-FOR model. WebQuestions HYPONYM-OF entity - related question answering datasets. Quasar - T HYPONYM-OF entity - related question answering datasets. TriviaQA HYPONYM-OF entity - related question answering datasets. SearchQA HYPONYM-OF entity - related question answering datasets. Method is large - scale language modeling. OtherScientificTerm are knowledge, and real - world entities. ",This paper proposes to use weakly supervised pretraining to improve the performance of self-supervised language models on syntactic and semantic NLP tasks. The authors propose to use the fact completion task as a weakly-supervisioned pretraining objective to train a model on the task. They show that the proposed method outperforms BERT in terms of performance on the tasks.  ,"This paper proposes a weakly supervised pretraining objective for self-supervised language models for syntactic and semantic NLP tasks. The authors propose a zero-shot fact completion task to test the performance of the model on the task of fact-completion. They show that the proposed objective can improve the performance on a number of downstream tasks, including entity-related question answering and entity typing."
12465,SP:4395d6f3e197df478eee84e092539dc370babd97,"image collection USED-FOR discovering novel classes. setting COMPARE semi - supervised learning. semi - supervised learning COMPARE setting. self - supervised learning USED-FOR representation. supervised classification of the labelled data CONJUNCTION clustering of the unlabelled data. clustering of the unlabelled data CONJUNCTION supervised classification of the labelled data. rank statistics USED-FOR model. rank statistics USED-FOR clustering the unlabelled images. model USED-FOR clustering the unlabelled images. labelled and unlabelled subsets of the data USED-FOR joint objective function. labeled data USED-FOR image representation. joint objective function USED-FOR data representation. classification benchmarks EVALUATE-FOR methods. approach COMPARE methods. methods COMPARE approach. methods USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR novel category discovery. approach USED-FOR novel category discovery. classification benchmarks EVALUATE-FOR approach. OtherScientificTerm is labelled images. Method is general - purpose clustering model. Generic is latter. Material are unlabelled data, and labelled and unlabelled data. "," is a semi-supervised learning setting where the labeled and unlabeled data are collected from different sources. The goal is to discover novel classes from the labelled data. In this setting, the authors propose a general-purpose clustering model that combines supervised classification of the labeled data and clustering of the unlabelled data. The proposed method is based on the rank statistics and uses a joint objective function to learn the data representation. The authors show that the proposed method achieves state-of-the-art performance on classification benchmarks.","This paper proposes a general-purpose clustering model for semi-supervised learning of unlabelled and labelled data. The proposed method is motivated by the observation that self-supervision of the unlabeled data can lead to poor representation of the labelled data, and that it is more important to learn the representation of labelled data than the unlabelling data. To address this issue, the authors propose a joint objective function for the labelled and unlabeled subsets of the data, which is based on the rank statistics. The model is trained using a combination of supervised classification and clustering, and is evaluated on a variety of classification benchmarks. "
12474,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"selfsupervised robot interaction USED-FOR images. images HYPONYM-OF dynamical system. VP algorithms USED-FOR robotic manipulation and navigation domains. data - driven perception and planning USED-FOR VP algorithms. approach USED-FOR VP. nodes PART-OF graph. connectivity PART-OF graph. image samples PART-OF graph. semiparametric topological memory ( SPTM ) method HYPONYM-OF approach. deep image classification USED-FOR connectivity. topological connectivity FEATURE-OF graph. graph search methods USED-FOR planning. loss function USED-FOR connectivity classifier. manual tuning USED-FOR loss function. loss function USED-FOR SPTM. discriminative classifier USED-FOR energy function. contrastive predictive coding USED-FOR discriminative classifier. contrastive predictive coding USED-FOR energy function. hallucinated samples USED-FOR connectivity graph. connectivity graph USED-FOR zero - shot generalization. domain changes FEATURE-OF zero - shot generalization. simulated domains EVALUATE-FOR HTM. SPTM CONJUNCTION visual foresight methods. visual foresight methods CONJUNCTION SPTM. simulated domains EVALUATE-FOR visual foresight methods. HTM COMPARE SPTM. SPTM COMPARE HTM. HTM COMPARE visual foresight methods. visual foresight methods COMPARE HTM. simulated domains EVALUATE-FOR SPTM. visual foresight methods USED-FOR long - horizon planning. long - horizon planning EVALUATE-FOR HTM. plan quality EVALUATE-FOR visual foresight methods. plan quality EVALUATE-FOR HTM. Task is visual planning ( VP ). Method are Hallucinative Topological Memory ( HTM ), and conditional VAE model. OtherScientificTerm is context image of the domain. ","This paper proposes a novel method for visual planning in robotic manipulation and navigation tasks. The method is based on topological memory (SPTM), which is a semiparametric topological graph that is used to model the topological structure of an image. The proposed method is evaluated on a variety of robotic manipulation tasks in simulated environments.   ",This paper proposes a new method for visual planning (VP) based on the semiparametric topological memory (SPTM) method. The main idea of SPTM is to use the topological connectivity graph to map the topology of an image to the context of the scene. The proposed method is based on a conditional VAE model and a discriminative classifier. Experiments on simulated domains show that the proposed method outperforms the state-of-the-art in terms of zero-shot generalization.
12483,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"Large - scale benchmark datasets USED-FOR AI. benchmarks USED-FOR realistic problem distributions. spurious biases FEATURE-OF them. similar ( thus non - tail ) problems FEATURE-OF benchmarks. long - tail problems PART-OF real world problems. Adversarial Filters USED-FOR model - based reduction of dataset biases. AFLITE HYPONYM-OF iterative greedy algorithm. AFLITE USED-FOR reduced dataset. realistic problem distributions CONJUNCTION spurious biases. spurious biases CONJUNCTION realistic problem distributions. spurious biases FEATURE-OF reduced dataset. realistic problem distributions FEATURE-OF reduced dataset. AFOPTIMUM USED-FOR optimum bias reduction. AFLITE USED-FOR task. MNLI CONJUNCTION QNLI. QNLI CONJUNCTION MNLI. SNLI CONJUNCTION MNLI. MNLI CONJUNCTION SNLI. ImageNet CONJUNCTION SNLI. SNLI CONJUNCTION ImageNet. it USED-FOR benchmarks. SNLI CONJUNCTION QNLI. QNLI CONJUNCTION SNLI. ImageNet HYPONYM-OF benchmarks. QNLI HYPONYM-OF benchmarks. SNLI HYPONYM-OF benchmarks. MNLI HYPONYM-OF benchmarks. AFLITE USED-FOR measurable dataset biases. measurable dataset biases FEATURE-OF synthetic and real datasets. synthetic and real datasets EVALUATE-FOR AFLITE. K - nearest - neighbors USED-FOR dataset biases. Generic are filtered counterparts, and model. Metric is human performance. Task is bias reduction. ","This paper proposes an iterative greedy algorithm for bias reduction on large-scale benchmark datasets with spurious biases. The proposed method is based on adversarial filters, where the bias reduction problem is formulated as a model-based reduction of dataset biases. It is shown that the proposed method achieves the best performance on MNLI, QNLI, SNLI, and ImageNet. ",This paper proposes an iterative greedy algorithm AFLITE for the task of bias reduction in large-scale benchmark datasets. The main idea is to use a model-based reduction of dataset biases to reduce the spurious biases in the dataset. The proposed method is based on the idea of adversarial filters. The authors show that AFLITE achieves the best performance on synthetic and real-world datasets.
12492,SP:82777947d2377efa897c6905261f5375b29a4c19,"decision metric USED-FOR models. matching networks CONJUNCTION prototypical networks. prototypical networks CONJUNCTION matching networks. prototypical networks PART-OF few - shot models. matching networks PART-OF few - shot models. softmax USED-FOR relative distance. batch normalization USED-FOR centering. Gaussian layer USED-FOR distance calculation. Gaussian layer USED-FOR prototypical network. support examples ’ distribution COMPARE centroid. centroid COMPARE support examples ’ distribution. distance calculation PART-OF prototypical network. support examples ’ distribution USED-FOR Gaussian layer. Method are Few - shot models, and prototypical few - shot models. Generic are They, and extension. OtherScientificTerm are class belongings, and null class ”. Material are Omniglot data set, matched test set, unmatched MNIST data, and MiniImageNet data set. Metric are classification accuracy, and test accuracy. ","This paper proposes to use matching networks and prototypical networks in few-shot models. The proposed method is based on the observation that matching networks can be seen as a decision metric for models, which can be used as a way to evaluate the performance of the model. The authors propose to use batch normalization to reduce the distance between support examples and centroids in the prototypical network, and to use a Gaussian layer to compute the distance from the support examples to the centroid of the prototype network.  ","This paper proposes a new decision metric for few-shot models. The proposed metric is based on the notion of class belongings, which is defined as the distance between support examples’ distribution and the centroid of the prototypical network’s distribution. The authors propose to use a Gaussian layer to calculate the distance from support examples to centroid. They also propose a batch normalization method for centering the support examples' distribution. Finally, the authors show that the proposed metric can be used to improve the accuracy of few shot models. "
12501,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"Graph embedding techniques USED-FOR applications. learning on non - Euclidean data USED-FOR applications. node attribute information PART-OF graph embedding models. computational complexity CONJUNCTION memory usage. memory usage CONJUNCTION computational complexity. graph fusion USED-FOR graph. topology FEATURE-OF graph. topology CONJUNCTION node attribute information. node attribute information CONJUNCTION topology. GraphZoom HYPONYM-OF multi - level framework. graph CONJUNCTION node attribute information. node attribute information CONJUNCTION graph. graph fusion USED-FOR GraphZoom. it USED-FOR embeddings. embedding methods USED-FOR coarsened graph. GraphZoom USED-FOR embedding methods. graph datasets USED-FOR transductive and inductive tasks. transductive and inductive tasks EVALUATE-FOR approach. graph datasets EVALUATE-FOR approach. GraphZoom COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE GraphZoom. GraphZoom USED-FOR graph embedding process. graph embedding process COMPARE unsupervised embedding methods. unsupervised embedding methods COMPARE graph embedding process. classification accuracy EVALUATE-FOR GraphZoom. OtherScientificTerm is node attribute noise. Metric are accuracy, and scalability. Generic is them. ","This paper proposes GraphZoom, a multi-level framework for embedding graphs on non-Euclidean graphs. The main idea is to combine graph topology and node attribute information in a coarsened graph. The proposed method is evaluated on a variety of graph classification and inductive tasks and achieves state-of-the-art performance.  ","This paper proposes GraphZoom, a multi-level framework for learning graph embeddings on non-Euclidean graphs. The main idea is to combine graph fusion and graph embedding methods to learn a coarsened graph. The authors show that GraphZoe can be used to improve the performance of unsupervised embedding models on both transductive and inductive tasks.  "
12510,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,predictive coding USED-FOR image compression models. neural autoregressive image generation models USED-FOR absolute pixel intensities. prediction COMPARE absolute counterpart. absolute counterpart COMPARE prediction. model USED-FOR sharp transitions. model USED-FOR smooth transitions. absolute predictor USED-FOR model. relative predictor USED-FOR generating smooth transitions. mechanism COMPARE absolute prediction counterparts. absolute prediction counterparts COMPARE mechanism. unconditional image generation CONJUNCTION image colorization. image colorization CONJUNCTION unconditional image generation. image colorization CONJUNCTION super - resolution. super - resolution CONJUNCTION image colorization. benchmarks EVALUATE-FOR mechanism. benchmarks EVALUATE-FOR super - resolution. likelihood EVALUATE-FOR absolute prediction counterparts. benchmarks EVALUATE-FOR unconditional image generation. benchmarks EVALUATE-FOR absolute prediction counterparts. image colorization HYPONYM-OF benchmarks. likelihood EVALUATE-FOR mechanism. Material is natural images. Method is unified probabilistic model. ,"This paper proposes a method for image compression based on a unified probabilistic model. The proposed method is based on an autoregressive image generation model that predicts the absolute pixel intensities and a relative predictor. The relative predictor is used to generate smooth transitions in the image, while the absolute predictor predicts the sharpness of the image. The method is evaluated on unconditional image generation, image colorization and super-resolution. ","This paper proposes a new method for predicting the absolute pixel intensities of an image. The proposed method is based on a unified probabilistic model, where the relative predictor is used to generate smooth transitions and the absolute predictor predicts the sharp transitions. The method is evaluated on unconditional image generation, image colorization, super-resolution, and image compression benchmarks."
12519,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"reinforcement learning CONJUNCTION Monte Carlo methods. Monte Carlo methods CONJUNCTION reinforcement learning. stationary distribution FEATURE-OF Markov chain. stationary distribution USED-FOR estimating quantities. Markov chain USED-FOR estimating quantities. estimation USED-FOR applications. variational divergence minimization USED-FOR constraint reformulations. off - line PageRank CONJUNCTION off - policy policy evaluation. off - policy policy evaluation CONJUNCTION off - line PageRank. off - policy policy evaluation HYPONYM-OF benchmark problems. off - line PageRank HYPONYM-OF benchmark problems. Task are real - world applications, and consistent estimation. OtherScientificTerm are transition operator, and stationary and empirical distributions. Generic are approach, and algorithm. Method are GenDICE, and error analysis. ",This paper studies the problem of estimating the transition operator of a Markov chain in a reinforcement learning problem. The authors propose to use a variational divergence minimization approach to minimize the divergence between the stationary distribution and the empirical distribution. They show that this approach is computationally tractable and can be used to solve off-line pageRank and off-policy policy evaluation problems.  ,This paper proposes a new method for estimating the stationary distribution of a Markov chain. The stationary distribution is defined as the transition operator between the stationary and empirical distributions. The authors show that it is possible to estimate a stationary distribution using a variational divergence minimization (VMD) method. They also show that their method can be applied to off-line PageRank and off-policy policy evaluation.
12528,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"machine learning systems USED-FOR spurious patterns. statistical frameworks USED-FOR term. models USED-FOR spurious patterns. sentiment analysis CONJUNCTION natural language inference tasks. natural language inference tasks CONJUNCTION sentiment analysis. classifiers USED-FOR natural language inference tasks. classifiers USED-FOR sentiment analysis. classifiers COMPARE models. models COMPARE classifiers. classifiers USED-FOR spurious features. original or manipulated data USED-FOR classifiers. OtherScientificTerm are spurious associations, confounding, direct or indirect causal effects, and internal coherence. Task is natural language processing. Method is Classifiers. ","This paper studies the problem of ""causal confounding"", i.e. the presence of spurious associations in natural language processing tasks. The authors propose a new notion of causal confounding, which they define as the existence of direct or indirect causal effects. They show that classifiers trained on the original data and manipulated data can produce spurious features that are not present in the original or manipulated data. They then show that the classifiers are able to identify the source of the spurious features. They also show that their method is able to recover the original and the manipulated data from each other.","This paper studies the problem of spurious associations in machine learning models. The authors propose a new term, ""internal coherence"", which is defined as the coherence between the original and manipulated data. They show that classifiers trained with internal coherence are more sensitive to the original data and the manipulated data, and that their classifiers can be more sensitive than other classifiers. They also show that their method can be applied to natural language inference tasks. "
12537,SP:b720eb5b6e44473a9392cc572af89270019d4c42,super - resolution CONJUNCTION image restoration. image restoration CONJUNCTION super - resolution. CNN based image quality assessment CONJUNCTION super - resolution. super - resolution CONJUNCTION CNN based image quality assessment. full - reference perceptual quality features USED-FOR CNN based image quality assessment. image restoration CONJUNCTION imageto - image translation problems. imageto - image translation problems CONJUNCTION image restoration. CNN based image quality assessment CONJUNCTION image restoration. image restoration CONJUNCTION CNN based image quality assessment. frequency and orientation tuning of channels PART-OF trained image classification deep CNNs. spatial frequencies FEATURE-OF grating stimuli. VGG-16 HYPONYM-OF trained image classification deep CNNs. CNN channels USED-FOR human visual perception models. CNN channels USED-FOR spatial frequency and orientation selective filters. deep CNN representations USED-FOR perceptual quality features. contrast masking thresholds FEATURE-OF human visual perception. orientation selectivity FEATURE-OF deep CNN channels. perceptual quality features FEATURE-OF deep CNN channels. contrast masking thresholds FEATURE-OF spatial frequencies. ,"This paper studies the spatial frequency and orientation tuning of channels in CNNs for image classification. The authors show that spatial frequency tuning and orientation selective filters are important for human visual perception. The experiments show that frequency tuning improves the performance of CNNs trained on VGG-16 on image classification tasks, while orientation tuning improves performance on image restoration tasks.","This paper proposes a method to improve the perceptual quality of CNN-based image classification by tuning the spatial frequency and orientation tuning of channels in CNNs. The proposed method is based on the observation that human visual perception is highly sensitive to spatial frequencies and orientation selectivity. The method is applied to VGG-16, which is a well-known image classification deep CNN. The experimental results show that the proposed method can improve the performance of CNNs on image classification and image restoration."
12546,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,drug development CONJUNCTION medical practice. medical practice CONJUNCTION drug development. graph neural networks USED-FOR task. nodes CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION nodes. drugs CONJUNCTION drug - drug interactions. drug - drug interactions CONJUNCTION drugs. drugs CONJUNCTION nodes. nodes CONJUNCTION drugs. graph neural networks USED-FOR DDI predictions. link prediction problems USED-FOR DDI predictions. graph energy neural network ( GENN ) USED-FOR link type correlations. structure prediction problem USED-FOR DDI prediction task. graph neural networks USED-FOR energy function. real world DDI datasets EVALUATE-FOR GENN. GENN COMPARE baselines. baselines COMPARE GENN. real world DDI datasets EVALUATE-FOR baselines. PR - AUC EVALUATE-FOR datasets. datasets EVALUATE-FOR GENN. PR - AUC EVALUATE-FOR GENN. GENN USED-FOR DDI correlations. GENN COMPARE baseline models. baseline models COMPARE GENN. Task is drug - drug interactions ( DDIs ). OtherScientificTerm is DDI types. Method is energy - based model. ,"This paper proposes a graph energy neural network (GENN) for predicting drug-drug interaction (DDI) correlations. The proposed method is based on a link prediction problem, where the goal is to predict link-type correlations between two DDI types. The main idea is to use an energy-based model to predict the energy function of the DDI type. Experiments on two real-world DDI datasets show that the proposed method outperforms baselines on PR-AUC.",This paper proposes a graph energy neural network (GENN) for predicting drug-drug interactions (DDIs) from graph neural networks (GNNs). The main idea is to use a link prediction problem to predict link type correlations between two DDI types. The model is trained using a graph neural network to predict the structure of the DDI prediction task. Experiments are conducted on three real-world DDIs datasets and show that the proposed model outperforms baselines. 
12555,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,vq - wav2vec USED-FOR discrete representations of audio segments. online k - means clustering USED-FOR dense representations. Gumbel - Softmax CONJUNCTION online k - means clustering. online k - means clustering CONJUNCTION Gumbel - Softmax. algorithm USED-FOR dense representations. online k - means clustering USED-FOR algorithm. Gumbel - Softmax USED-FOR algorithm. Discretization USED-FOR algorithms. discrete inputs USED-FOR algorithms. TIMIT phoneme classification EVALUATE-FOR BERT pre - training. ,This paper proposes a method to learn discrete representations of audio segments using vq-vq-wav2vec. The proposed method is based on Gumbel-Softmax and online k-means clustering. Theoretical analysis is provided to show that the proposed method achieves better performance than existing methods on TIMIT phoneme classification tasks.   ,"This paper studies the problem of learning discrete representations of audio segments. The authors propose a new algorithm called Gumbel-Softmax, which is based on online k-means clustering. They show that the proposed algorithm can learn dense representations of the audio segment. They also show that it can be used to improve the performance of BERT pre-training. "
12564,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,methods USED-FOR collaborative filtering models. methods USED-FOR ranking - based objective functions. actor - critic reinforcement learning USED-FOR methods. critic network USED-FOR ranking - based metrics. actor network USED-FOR metrics. critic - based method USED-FOR approximate ) ranking scores. critic - based method USED-FOR scoring process. learning - to - rank methods COMPARE critic - based method. critic - based method COMPARE learning - to - rank methods. neural network USED-FOR critic - based method. optimization procedure USED-FOR learning - to - rank methods. neural network USED-FOR scoring process. actor - critic COMPARE baselines. baselines COMPARE actor - critic. prediction models COMPARE baselines. baselines COMPARE prediction models. actor - critic USED-FOR prediction models. large - scale datasets EVALUATE-FOR actor - critic. large - scale datasets EVALUATE-FOR baselines. ,This paper proposes an actor-critic reinforcement learning method for collaborative filtering models. The proposed method is based on a critic network that estimates the ranking-based metrics and uses an actor network to compute the ranking scores. The method is evaluated on two large-scale datasets and achieves state-of-the-art performance. ,"This paper proposes an actor-critic reinforcement learning method for collaborative filtering models. The proposed method is based on a neural network-based approach, where a critic network is trained to estimate the ranking-based metrics, and an actor network is used to predict the metrics. The method is evaluated on a variety of large-scale datasets, where it outperforms the baselines. "
12573,SP:2444a83ae08181b125a325d893789f074d6db8ee,"off - policy reinforcement learning methods USED-FOR robot control. data - efficiency EVALUATE-FOR Deep Q - learning. multi - step TD - learning USED-FOR data - efficiency. Truncated Q - functions HYPONYM-OF TemporalDifference formulations. Shifted Q - functions USED-FOR farsighted return. Model - based Value Expansion CONJUNCTION TD3(∆ ). TD3(∆ ) CONJUNCTION Model - based Value Expansion. TD3 CONJUNCTION Model - based Value Expansion. Model - based Value Expansion CONJUNCTION TD3. approach COMPARE TD3. TD3 COMPARE approach. TD3(∆ ) HYPONYM-OF off - policy variant of TD(∆ ). approach COMPARE Model - based Value Expansion. Model - based Value Expansion COMPARE approach. approach USED-FOR function - approximation setting. function - approximation setting CONJUNCTION TD3. TD3 CONJUNCTION function - approximation setting. tabular case FEATURE-OF Composite Q - learning. simulated robot tasks EVALUATE-FOR Composite TD3. Composite TD3 COMPARE TD3. TD3 COMPARE Composite TD3. Composite TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE Composite TD3. simulated robot tasks EVALUATE-FOR off - policy multi - step approaches. simulated robot tasks EVALUATE-FOR TD3. TD3 COMPARE off - policy multi - step approaches. off - policy multi - step approaches COMPARE TD3. data - efficiency EVALUATE-FOR TD3. data - efficiency EVALUATE-FOR off - policy multi - step approaches. data - efficiency EVALUATE-FOR Composite TD3. Task is realworld applications. OtherScientificTerm are off - policy, target - policy rollout, truncated rollout, and shortand long - term predictions. Method is Composite Q - learning algorithm. ","This paper proposes a new off-policy Q-learning algorithm based on TD3(∆), which is a variant of TD3 with truncated Q-functions. The main idea is to use a model-based Value Expansion (MBO) to learn the Q-function and then use TD3 to update the target policy. Theoretical analysis is provided to show that TD3 is equivalent to TD3 in the function-approximation setting. Experiments on simulated robot control tasks show that the proposed method outperforms TD3. ","This paper proposes a new method for multi-step TD-based off-policy reinforcement learning for robot control. The main idea of the paper is to combine TD3, TD3(∆) and Model-based Value Expansion to improve the data-efficiency of TD3. The proposed method is evaluated on simulated robot tasks and real-world robot control tasks. "
12582,SP:64564b09bd68e7af17845019193825794f08e99b,"reinforcement learning USED-FOR real world robotics. dexterous manipulation USED-FOR system. manually designed resets USED-FOR learning. on - board perception USED-FOR manually designed resets. on - board perception USED-FOR learning. dexterous robotic manipulation tasks EVALUATE-FOR system. system USED-FOR vision - based skills. real - world three - fingered hand FEATURE-OF vision - based skills. Material is instrumented laboratory scenarios. Method are continuous learning, and robotic learning system. OtherScientificTerm are hand - engineered reward functions, and human intervention. Generic are solutions, and learning paradigm. ","This paper proposes a method for continuous reinforcement learning in robotic manipulation tasks. The method is based on the observation that human intervention is needed to train a robot to perform tasks that require dexterous manipulation. To this end, the authors propose to train the robot in a continuous fashion, where the goal is to learn a reward function that encourages the robot to execute tasks that are similar to the ones performed by humans. The proposed method is evaluated on a variety of tasks, including three-fingered manipulation, three-finger manipulation, and hand manipulation.   ","This paper proposes a robotic learning system that learns to perform dexterous robotic manipulation tasks with human intervention. The proposed method is based on the idea of continuous learning, where the goal is to learn a set of skills that can be used for dexterous manipulation tasks. The system is trained using a combination of hand-engineered reward functions and human intervention, and is evaluated on three-fingered hand manipulation tasks in the lab."
12591,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"adversarial examples USED-FOR Neural network robustness. labeled data USED-FOR adversarially robust generalization. perturbed test data EVALUATE-FOR networks. unlabeled data USED-FOR model. adversarially robust generalization FEATURE-OF model. classification accuracy EVALUATE-FOR accuracy part. accuracy part HYPONYM-OF parts. unlabeled data USED-FOR part. adversarially robust generalization COMPARE generalization. generalization COMPARE adversarially robust generalization. generalization PART-OF supervised learning. adversarially robust generalization USED-FOR Gaussian mixture problem. adversarial training algorithm USED-FOR adversarial robust generalization. MNIST CONJUNCTION Cifar-10. Cifar-10 CONJUNCTION MNIST. MNIST USED-FOR adversarial robust generalization. Cifar-10 USED-FOR adversarial robust generalization. unlabeled data USED-FOR adversarial training algorithm. Method is risk decomposition theorem. OtherScientificTerm are expected robust risk, prediction stability, perturbations, and label information. Metric is stability part. ","This paper studies the adversarial robust generalization in the presence of adversarial examples in the training data. The authors show that adversarial generalization is a function of the expected robust risk and prediction stability, which is a measure of the prediction stability of the model in the face of perturbations.    The authors prove a risk decomposition theorem, which shows that the accuracy part of adversarially robust generalisation is more important than prediction stability in terms of classification accuracy, and that the stability part is the most important part of generalization. They then propose an adversarial training algorithm to improve the generalization performance.  They show that the loss function is a Gaussian mixture problem, and they show that it is more sensitive to the label information than the accuracy. ","This paper studies the problem of adversarially robust generalization in supervised learning. In particular, the authors study the generalization of adversarial examples in the Gaussian mixture problem. They show that adversarial training with unlabeled data can improve generalization, but not generalization with perturbed test data. The authors also show that the stability of the model is the most important part of generalization.   "
12600,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"estimation of advantage USED-FOR reinforcement learning algorithms. promotion focus CONJUNCTION prevention focus. prevention focus CONJUNCTION promotion focus. order statistics FEATURE-OF path ensemble. promotion focus FEATURE-OF learning process. prevention focus FEATURE-OF learning process. promotion focus COMPARE prevention focus. prevention focus COMPARE promotion focus. Terrain locomotion CONJUNCTION Atari games. Atari games CONJUNCTION Terrain locomotion. Atari games CONJUNCTION sparse - reward environments. sparse - reward environments CONJUNCTION Atari games. MuJoCo continuous control CONJUNCTION Terrain locomotion. Terrain locomotion CONJUNCTION MuJoCo continuous control. benchmarks EVALUATE-FOR schemes. schemes COMPARE mainstream methods. mainstream methods COMPARE schemes. benchmarks EVALUATE-FOR mainstream methods. MuJoCo continuous control HYPONYM-OF benchmarks. sparse - reward environments HYPONYM-OF benchmarks. Atari games HYPONYM-OF benchmarks. Terrain locomotion HYPONYM-OF benchmarks. Generic are it, and formulation. OtherScientificTerm are regulatory focuses, regulatory focus, and sparse rewards. ","This paper studies the problem of estimating the advantage of a policy in reinforcement learning. The authors propose to use a combination of two strategies: (1) a path ensemble based on the order statistics of the path ensemble, and (2) a ""regulatory focus"" based on maximizing the expected reward of the policy. The proposed approach is evaluated on MuJoCo continuous control and two Atari games, and compared with a number of baselines.","This paper proposes a new approach to estimate the advantage of a given strategy in reinforcement learning. The main contribution of the paper is to propose a new framework for estimating the order statistics of a path ensemble, which can be used to improve the performance of the learning process. The proposed approach is evaluated on a variety of tasks, including MuJoCo continuous control, Atari games, and sparse-reward environments.   "
12609,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"precision gating ( PG ) USED-FOR deep neural networks. approach USED-FOR DNN architectures. computational cost EVALUATE-FOR DNN execution. PG USED-FOR CNNs. PG USED-FOR statically compressed mobile - friendly networks. statically compressed mobile - friendly networks HYPONYM-OF CNNs. ShuffleNet HYPONYM-OF statically compressed mobile - friendly networks. prediction - based quantization schemes COMPARE PG. PG COMPARE prediction - based quantization schemes. compute EVALUATE-FOR PG. ImageNet EVALUATE-FOR PG. accuracy EVALUATE-FOR PG. PG USED-FOR RNNs. 8 - bit uniform quantization COMPARE PG. PG COMPARE 8 - bit uniform quantization. computational cost reduction EVALUATE-FOR LSTM. Penn Tree Bank dataset EVALUATE-FOR LSTM. computational cost reduction EVALUATE-FOR PG. perplexity EVALUATE-FOR PG. Metric are precision, and accuracy loss. ","This paper proposes to use precision gating (PG) to reduce the computational cost of training deep neural networks (DNNs). The main idea is to use quantization to improve the precision and perplexity of the output of a DNN. The proposed quantization method is based on the idea that the precision of the input is computed by the quantized output of the DNN, and the perplexity is computed using the prediction of the predicted output. The paper shows that the proposed method is able to reduce computational cost in terms of accuracy, perplexity, and accuracy loss.   ","This paper proposes a new quantization scheme for precision gating (PG) in deep neural networks (DNNs). The main idea of PG is to reduce the computational cost of training DNNs by quantizing the precision of the output layer. The proposed method is based on the idea that the accuracy loss of a DNN is proportional to the accuracy of the input layer, and quantization is a measure of the precision. The authors show that the proposed method can reduce the precision by a factor of 1.5. They also show that PG can be applied to a wide range of DNN architectures.  "
12618,SP:0c2c9b80c087389168acdd42af15877fb499449b,"clean labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION clean labeled data. unlabeled data PART-OF TD. classifiers PART-OF unsupervised domain adaptation ( UDA ). unlabeled data USED-FOR classifiers. clean labeled data USED-FOR classifiers. noisy labeled data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION noisy labeled data. unlabeled data PART-OF TD. unlabeled data USED-FOR classifiers. noisy labeled data USED-FOR classifiers. Butterfly framework USED-FOR WUDA. solution USED-FOR WUDA. label noise FEATURE-OF SD. Butterfly framework HYPONYM-OF solution. WUDA COMPARE UDA methods. UDA methods COMPARE WUDA. classification USED-FOR TD. models PART-OF Butterfly. deep networks HYPONYM-OF models. Butterfly USED-FOR WUDA. Butterfly COMPARE baseline methods. baseline methods COMPARE Butterfly. WUDA EVALUATE-FOR Butterfly. WUDA EVALUATE-FOR baseline methods. OtherScientificTerm are noisy - to - clean, and SD - to - TD - distributional. ","This paper proposes a method for unsupervised domain adaptation (UDA) in which unlabeled data is used to train classifiers in the target domain. The proposed method, called ""Butterfly"", is based on the observation that label noise in the source domain can be used to improve the performance of classifiers trained on the target data. To this end, the authors propose to use a modified version of the Butterfly framework, where the label noise is added to the source and target domain distributions. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of datasets. ","This paper proposes a new method for unsupervised domain adaptation (UDA) in the setting of noisy-to-clean data (WUDA). The authors propose a new approach called ""Butterfly"" to solve the problem of WUDA. The main idea of the paper is to use a differentiable classifier for clean and noisy data. The proposed method is evaluated on a variety of datasets and compared with several baselines. "
12627,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"agent USED-FOR control tasks. high - dimensional images USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR agent. model - free reinforcement learning ( RL ) USED-FOR control tasks. high - dimensional images USED-FOR control tasks. control policy USED-FOR task. latent representation CONJUNCTION control policy. control policy CONJUNCTION latent representation. latent representation USED-FOR task. scarce reward signal USED-FOR high - capacity encoder. relevant features USED-FOR task. representation learning USED-FOR image - based RL. image reconstruction loss USED-FOR representation learning. auxiliary decoder USED-FOR end - to - end. auxiliary decoder USED-FOR off - policy actor - critic algorithm. control tasks EVALUATE-FOR model - free and model - based algorithms. OtherScientificTerm are suboptimal convergence, and latent features. Metric is sample efficiency. Method is off - policy algorithms. Task is image - based RL1. ","This paper proposes a model-free RL algorithm for image-based control tasks. The proposed method is based on a latent representation encoder and a control policy. The encoder is trained with an image reconstruction loss, and the policy is trained using an off-policy actor-critic algorithm. Experiments show that the proposed method achieves better sample efficiency compared to the baselines.   ","This paper proposes an off-policy actor-critic algorithm for image-based RL. The main idea is to use a high-capacity encoder to extract the relevant features of an image from a scarce reward signal, and then use a low-capacity decoder to reconstruct the features of the image. The authors show that the proposed algorithm achieves suboptimal convergence in terms of sample efficiency. They also provide a theoretical analysis of their algorithm."
12636,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"it USED-FOR tasks. Dropout HYPONYM-OF regularization technique. it USED-FOR DNNs. regularization technique USED-FOR generalization. regularization technique USED-FOR deep neural networks ( DNNs ). DNNs USED-FOR tasks. dropout USED-FOR training. dropout technique USED-FOR accelerating training. accelerating training CONJUNCTION generalization. generalization CONJUNCTION accelerating training. dropout technique USED-FOR generalization. multi - sample dropout HYPONYM-OF dropout technique. dropout USED-FOR generalization. multi - sample dropout USED-FOR dropout samples. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION ILSVRC 2012 ( ImageNet ). multi - sample dropout USED-FOR training. CIFAR-100 CONJUNCTION SVHN datasets. SVHN datasets CONJUNCTION CIFAR-100. multi - sample dropout USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION ILSVRC 2012 ( ImageNet ). CIFAR-10 USED-FOR image classification tasks. ILSVRC 2012 ( ImageNet ) USED-FOR image classification tasks. SVHN datasets USED-FOR image classification tasks. convolution layers CONJUNCTION dropout layer. dropout layer CONJUNCTION convolution layers. computation time FEATURE-OF convolution layers. training set CONJUNCTION validation set. validation set CONJUNCTION training set. error rates CONJUNCTION losses. losses CONJUNCTION error rates. validation set EVALUATE-FOR networks. losses EVALUATE-FOR networks. training set EVALUATE-FOR networks. error rates EVALUATE-FOR networks. multi - sample dropout USED-FOR networks. OtherScientificTerm are overfitting, dropout sample, sample losses, and fully connected layers. Metric is loss. Generic are technique, operator, and network. Method",This paper proposes to use multi-sample dropout as a regularization technique to improve the generalization performance of deep neural networks (DNNs) in image classification tasks. The proposed method is based on the observation that dropout can be used to accelerate the training of DNNs by reducing the computation time of convolutional layers. The main contribution of this paper is to show that the dropout technique is effective in accelerating training and generalization. The experiments on CIFAR-10 and ImageNet show the effectiveness of the proposed dropout method.,"This paper proposes a new dropout technique, called multi-sample dropout, to speed up the training of deep neural networks (DNNs) by reducing the number of dropout samples during training. The proposed dropout method is based on the idea that dropout can be used as a regularization technique to improve the generalization performance of DNNs. The authors show that the proposed method can accelerate the training time of a DNN by using multiple dropout layers. They also show that it can reduce the computation time of convolutional layers by using a single dropout layer. They show that their method can be applied to CIFAR-10 and SVHN datasets. "
12645,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"filters CONJUNCTION features. features CONJUNCTION filters. filter ambiguity FEATURE-OF CNNs. interpretability EVALUATE-FOR models. strategy USED-FOR interpretable CNNs. model USED-FOR disentangled filters. Label Sensitive Gate ( LSG ) structure USED-FOR model. supervised manner USED-FOR model. sparsity regularization USED-FOR LSG. LSG USED-FOR redundant filters. training strategy COMPARE model. model COMPARE training strategy. redundancy CONJUNCTION interpretability. interpretability CONJUNCTION redundancy. redundancy EVALUATE-FOR model. interpretability EVALUATE-FOR training strategy. interpretability EVALUATE-FOR model. Method is Convolutional neural networks ( CNNs ). Generic are pre - trained model, and method. OtherScientificTerm are redundant channels, and periodical shutdown. ",This paper proposes a label-sensitive gate (LSG) structure to improve the interpretability of CNNs by disentangling filters. The proposed LSG structure is based on the label sensitive gate (LSG) structure. The LSG is trained in a supervised manner with sparsity regularization. Experiments show that the proposed method achieves better interpretability compared to the existing methods. ,"This paper proposes a new training strategy for interpretable CNNs. The authors propose a label sensitive gate (LSG) structure to disentangle the disentangled filters in a supervised manner. The LSG structure is based on a sparsity regularization scheme, which is used to reduce the number of redundant channels in the model. Experiments are conducted to demonstrate the effectiveness of the proposed method. "
12654,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"scalability CONJUNCTION non - stationarity. non - stationarity CONJUNCTION scalability. value function factorization learning USED-FOR collaborative multiagent systems. communication USED-FOR tasks. framework USED-FOR nearly decomposable Q - functions ( NDQ ). communication minimization USED-FOR framework. value function factorization learning CONJUNCTION communication learning. communication learning CONJUNCTION value function factorization learning. value function factorization learning USED-FOR framework. communication learning USED-FOR framework. information - theoretic regularizers USED-FOR framework. mutual information EVALUATE-FOR regularizers. regularizers COMPARE value function factorization methods. value function factorization methods COMPARE regularizers. QMIX HYPONYM-OF value function factorization methods. StarCraft unit micromanagement benchmark EVALUATE-FOR framework. framework COMPARE baseline methods. baseline methods COMPARE framework. StarCraft unit micromanagement benchmark EVALUATE-FOR baseline methods. Method is Reinforcement learning. Task is multi - agent settings. OtherScientificTerm are decentralized value functions, coordination, action selection, and communication messages. ","This paper proposes a communication-minimizing Q-learning framework for decentralized multi-agent reinforcement learning. The proposed method is motivated by the observation that decentralized Q-functions are non-decomposable, which is a problem in many real-world settings. To address this problem, the authors propose a communication minimization approach to learn a nearly decomposable Q-function, which can be used to improve the communication efficiency. The authors also propose an information-theoretic regularization term to ensure the mutual information between the communication messages and the Q-values. Experiments on the StarCraft unit micromanagement benchmark show the effectiveness of the proposed method. ",This paper proposes a new framework for value function factorization learning in collaborative multiagent systems. The authors propose a communication minimization framework for nearly decomposable Q-functions (NDQ) and propose information-theoretic regularizers to improve the mutual information between agents. The proposed method is evaluated on the StarCraft unit micromanagement benchmark and shows promising results.
12663,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"AI USED-FOR complex programs. programming puzzles USED-FOR computers programming. programming puzzle HYPONYM-OF Boolean function. input - output pairs CONJUNCTION English problem descriptions. English problem descriptions CONJUNCTION input - output pairs. GAN - like algorithm USED-FOR puzzles. Troublemaker USED-FOR puzzles. Troublemaker HYPONYM-OF GAN - like algorithm. GAN - like algorithm USED-FOR automatic puzzle generation. Generic are problems, and it. OtherScientificTerm is Puzzles. Task is program synthesis. Method are puzzle - solver, and puzzle - solving techniques. ","This paper introduces a new programming puzzle problem, which is defined as a set of input-output pairs and a Boolean function. The authors propose a GAN-like algorithm to generate the puzzles, which are then used to train a solver to solve the problem. The main contribution of the paper is the introduction of the new problem, and the development of a new algorithm to solve it.  ","This paper proposes a GAN-like algorithm for solving programming puzzles. The main idea is to generate a set of programming puzzles from input-output pairs and English problem descriptions, and then solve the puzzles by solving them using the problem solver. The algorithm is based on the Troublemaker algorithm, which generates the puzzles from the set of input pairs and the English problem description. The paper is well-written and easy to follow."
12672,SP:627b515cc893ff33914dff255f5d6e136441d2e2,"structured decomposition of their behavior USED-FOR Reinforcement learning agents. lower - level primitives CONJUNCTION higher - level meta - policy. higher - level meta - policy CONJUNCTION lower - level primitives. lower - level primitives PART-OF policy. primitives PART-OF hierarchical reinforcement learning. primitives PART-OF policy design. information - theoretic mechanism USED-FOR decentralized decision. natural competition CONJUNCTION specialization. specialization CONJUNCTION natural competition. policy architecture COMPARE flat and hierarchical policies. flat and hierarchical policies COMPARE policy architecture. generalization EVALUATE-FOR policy architecture. Method is meta - policy. OtherScientificTerm are high - level meta - policy, and primitive. ","This paper proposes a hierarchical reinforcement learning approach where a meta-policy is composed of a set of primitive primitives and a higher-level policy. The primitives are learned in a hierarchical manner, and the meta-policies are trained to maximize the mutual information between the primitive and the high-level policies. The authors show that the primitives can be used to improve the performance of the policy in a variety of experiments. ","This paper proposes a hierarchical meta-policy architecture for reinforcement learning, where the lower-level primitives are represented as a set of primitives, and the higher-level meta-policies are represented by lower- level primitives. The paper proposes an information-theoretic mechanism for the decentralized decision making process, which is motivated by natural competition, specialization, and generalization. The authors show that the proposed policy architecture can outperform flat and hierarchical policies in terms of generalization, and that it can also outperform a flat policy. "
12681,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"models USED-FOR highdimensional state spaces. Model - based reinforcement learning methods USED-FOR models. rewards USED-FOR latent dynamics model. model - based planning framework USED-FOR latent reward prediction model. multi - step reward prediction USED-FOR planning. multi - step reward prediction USED-FOR latent representation. concise model - free representation USED-FOR framework. multi - pendulum and multi - cheetah environments FEATURE-OF framework. concise latent representation USED-FOR environments. method USED-FOR latent reward prediction model. method COMPARE model - based methods. model - based methods COMPARE method. sample efficiency EVALUATE-FOR model - free and model - based baselines. Planning COMPARE model - free and model - based baselines. model - free and model - based baselines COMPARE Planning. latent state - space FEATURE-OF Planning. sample efficiency EVALUATE-FOR Planning. Method are model - free reinforcement learning, and model - based algorithms. OtherScientificTerm are pendulums, and irrelevant information. Generic is them. ","This paper proposes a model-free reinforcement learning method for multi-pendulum and multi-cheetah environments. The proposed method is based on model-based reinforcement learning, where the reward function is modeled as a latent dynamics model. The authors propose to use a multi-step reward prediction model to model the latent state-space of the environment, which is then used for planning. The method is evaluated on a set of multi-Pendulum environments, where it is shown to outperform the baselines in terms of sample efficiency.","This paper proposes a model-free planning framework for multi-pendulum and multi-cheetah environments. The authors propose a new model-based planning framework that learns a latent state-space representation of the environment and a latent reward prediction model. The proposed method is evaluated on a variety of environments, and it is shown that the proposed method outperforms the state-of-the-art in terms of sample efficiency."
12690,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"GPU / TPU memory limitations CONJUNCTION training times. training times CONJUNCTION GPU / TPU memory limitations. parameterreduction techniques USED-FOR BERT. parameterreduction techniques USED-FOR memory consumption. training speed EVALUATE-FOR BERT. parameterreduction techniques USED-FOR training speed. methods USED-FOR models. models COMPARE BERT. BERT COMPARE models. it USED-FOR downstream tasks. self - supervised loss USED-FOR inter - sentence coherence. multi - sentence inputs USED-FOR it. multi - sentence inputs USED-FOR downstream tasks. model COMPARE BERT - large. BERT - large COMPARE model. GLUE, RACE, and SQuAD benchmarks EVALUATE-FOR model. OtherScientificTerm are model size, and parameters. Method are natural language representations, and pretrained models. "," training speed and memory consumption are two of the major challenges in NLP training. This paper proposes to reduce the number of parameters in BERT by using a combination of self-supervised loss and multi-sentence coherence. The proposed method is evaluated on GLUE, RACE, and SQuAD benchmarks.   ","This paper proposes a method to reduce the memory consumption of BERT by reducing the number of parameters in the model. The authors propose a self-supervised loss for inter-sentence coherence, which they call BERT-Large. They show that the proposed method outperforms BERT on GLUE, RACE, and SQuAD benchmarks."
12699,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"Transformer HYPONYM-OF neural network architecture. neural network architecture USED-FOR language understanding. Transformer USED-FOR language understanding. text CONJUNCTION videos. videos CONJUNCTION text. image CONJUNCTION text. text CONJUNCTION image. modalities FEATURE-OF tasks. image HYPONYM-OF modalities. text HYPONYM-OF modalities. videos HYPONYM-OF modalities. temporal input sequence FEATURE-OF hidden states. model USED-FOR tasks. architecture USED-FOR model. model USED-FOR asynchronous multi - task learning. multiple input modalities CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION multiple input modalities. model USED-FOR multiple input modalities. architecture USED-FOR tasks. tasks CONJUNCTION asynchronous multi - task learning. asynchronous multi - task learning CONJUNCTION tasks. multiple input modalities FEATURE-OF tasks. image captioning CONJUNCTION visual question answering. visual question answering CONJUNCTION image captioning. visual question answering CONJUNCTION video activity recognition. video activity recognition CONJUNCTION visual question answering. OmniNet USED-FOR tasks. part - of - speech tagging CONJUNCTION image captioning. image captioning CONJUNCTION part - of - speech tagging. OmniNet USED-FOR part - of - speech tagging. part - of - speech tagging CONJUNCTION visual question answering. visual question answering CONJUNCTION part - of - speech tagging. video activity recognition HYPONYM-OF tasks. part - of - speech tagging HYPONYM-OF tasks. visual question answering HYPONYM-OF tasks. image captioning HYPONYM-OF tasks. compressed model EVALUATE-FOR tasks. video captioning CONJUNCTION video question answering. video question answering CONJUNCTION video captioning. neural network USED-FOR tasks. modalities USED-FOR neural network. video question answering HYPONYM-OF tasks. video captioning HYPONYM-OF tasks. spatio - temporal cache PART-OF OmniNet. spatio - temporal cache USED-FOR self - attention mechanism. Method is spatio - temporal cache mechanism. Generic are it, and them. ","This paper proposes a Transformer-based model for multi-modal multi-task learning with multiple input modalities. The proposed model is based on the Transformer architecture and consists of a self-attention mechanism and a spatio-temporal cache to store the hidden states of the model. The model is trained with a compressed model and evaluated on image captioning, video activity recognition, and part-of-speech tagging tasks. ","This paper proposes a Transformer-based model for multi-modal multi-task language understanding. The proposed model is based on the Transformer architecture. The model is trained with a spatio-temporal cache to store the temporal input sequence of the hidden states of the input modalities. The authors show that the model is able to perform well on a variety of tasks, including video captioning, part-of-speech tagging, and visual question answering.  "
12708,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"tasks EVALUATE-FOR Deep learning models. predictive accuracy EVALUATE-FOR Deep learning models. methods USED-FOR uncertainty quantification. Bayesian neural networks USED-FOR methods. discriminative accuracy EVALUATE-FOR approximate posterior inference. frequentist approach USED-FOR uncertainty quantification. formal inference procedure USED-FOR predictive confidence intervals. discriminative jackknife ( DJ ) HYPONYM-OF formal inference procedure. formal inference procedure USED-FOR regression models. higher - order influence functions ( HOIFs ) FEATURE-OF model parameters. higher - order influence functions ( HOIFs ) USED-FOR DJ procedure. loss gradients CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION loss gradients. oracle access USED-FOR loss gradients. DJ USED-FOR HOIFs. oracle access CONJUNCTION Hessian - vector products. Hessian - vector products CONJUNCTION oracle access. oracle access USED-FOR recursive formula. model accuracy EVALUATE-FOR it. recursive formula USED-FOR DJ. recursive formula USED-FOR HOIFs. DJ COMPARE Bayesian and non - Bayesian baselines. Bayesian and non - Bayesian baselines COMPARE DJ. OtherScientificTerm are predictive uncertainty, and Bayesian credible intervals. Material is highand low - confidence prediction instances. Method is Bayesian methods. Metric is frequentist coverage. Task is model training. ",This paper proposes a new method for estimating uncertainty quantification in regression models. The proposed method is based on the use of higher-order influence functions (HOIFs) to estimate predictive confidence intervals. Theoretical results show that the proposed method can be used to estimate confidence intervals in high-confidence prediction instances. Experiments on synthetic and real-world datasets show the effectiveness of the method.,"This paper proposes a new approach to quantifying uncertainty quantification for Bayesian neural networks. The authors propose a new method called discriminative jackknife (DJ) to estimate confidence intervals between high-confidence and low-confidence predictions. The method is based on the notion of higher-order influence functions (HOIFs), which are defined as a function of the model parameters.  The authors show that the proposed method outperforms other Bayesian methods in terms of predictive confidence intervals. "
12717,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,Generative Adversarial Networks USED-FOR video samples. complexity CONJUNCTION fidelity. fidelity CONJUNCTION complexity. complex Kinetics-600 dataset EVALUATE-FOR Generative Adversarial Networks. decomposition of its discriminator USED-FOR model. video synthesis CONJUNCTION video prediction. video prediction CONJUNCTION video synthesis. baseline USED-FOR synthesis. Fréchet Inception Distance USED-FOR prediction. Inception Score USED-FOR synthesis. Fréchet Inception Distance CONJUNCTION Inception Score. Inception Score CONJUNCTION Fréchet Inception Distance. prediction USED-FOR Kinetics600. UCF-101 dataset EVALUATE-FOR Inception Score. UCF-101 dataset EVALUATE-FOR synthesis. Kinetics-600 USED-FOR baseline. Kinetics-600 USED-FOR synthesis. Method is Generative models of natural images. Task is video modeling. ,This paper proposes a method for video synthesis and video prediction using GANs. The method is based on a GAN with a discriminator that is trained to maximize the Inception Distance between the input video and the predicted video. The proposed method is evaluated on the complex Kinetics-600 dataset and UCF-101 dataset. ,"This paper proposes a novel method for video synthesis and video prediction in GANs. The method is based on the Fréchet Inception Distance (FID) and Inception Score (IID), which is a measure of the quality of a video. The authors show that FID can be used to improve the video synthesis performance of a GAN on Kinetics-600 dataset. They also show that it can improve the performance on UCF-101 dataset. "
12726,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"few - shot classification benchmarks EVALUATE-FOR representation. base class data USED-FOR representation. stages USED-FOR representation. spatial attention map USED-FOR background clutter. pre - trained classifier USED-FOR spatial attention map. Method are Few - shot learning, pre - trained network, and meta - learning. OtherScientificTerm is prior knowledge. Material is large - scale dataset. Generic is network. "," in few-shot learning. This paper proposes a meta-learning approach to improve the performance of the base classifier. The proposed approach is based on a spatial attention map, which is used to map the background data to the base data. The spatial attention maps are then used to train the meta-classifier. Experiments show that the proposed method outperforms baselines on several benchmarks.","This paper proposes a meta-learning method for few-shot learning, where a pre-trained classifier is trained on a large-scale dataset with base class data. The authors propose a spatial attention map, which maps the background data to a spatial representation of the data, and then use this representation to train the classifier on the base data. They show that their method outperforms the state-of-the-art methods on a variety of benchmark datasets. "
12735,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"molecules CONJUNCTION game maps. game maps CONJUNCTION molecules. generative adversarial networks ( GANs ) USED-FOR structured objects. molecules HYPONYM-OF structured objects. game maps HYPONYM-OF structured objects. structural requirements FEATURE-OF objects. molecules HYPONYM-OF structural requirements. constraints PART-OF model. approach USED-FOR logical constraints. knowledge compilation techniques USED-FOR expected disagreement. knowledge compilation techniques USED-FOR approach. setup USED-FOR hybrid logical - neural constraints. hybrid logical - neural constraints USED-FOR complex requirements. setup USED-FOR complex requirements. graph reachability HYPONYM-OF complex requirements. constrained images CONJUNCTION molecules. molecules CONJUNCTION constrained images. molecules CONJUNCTION video game levels. video game levels CONJUNCTION molecules. Method are constrained adversarial networks ( CANs ), generator, unconstrained GANs, and CANs. ","This paper proposes a novel method for generating molecules with constraints. The proposed method is based on a combination of GANs and logical constraints. In particular, the authors propose to use a GAN as a generator to generate a set of constraints for a given set of objects, and then use a neural network as a discriminator to distinguish between the generated objects and the objects in the set. The authors show that the proposed method outperforms the state-of-the-art methods in terms of the quality of generated molecules.","This paper proposes a novel approach to learn constraints for GANs that can be applied to structured objects (e.g., molecules, game maps, video game levels, etc). The proposed approach is based on a combination of two existing approaches: (1) a GAN-based approach that learns a set of logical constraints for each object, and (2) an unconstrained GAN model that learns constraints for the same set of objects. The authors show that the proposed approach can be used to learn complex constraints for a variety of structured objects, including molecules, video games, and graph reachability. "
12744,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"Deep neural networks COMPARE deep neural networks. deep neural networks COMPARE Deep neural networks. fixed activation functions USED-FOR deep neural networks. learnable activation functions USED-FOR Deep neural networks. adaptability of learnable activation functions USED-FOR model. expressive power FEATURE-OF model. expressive power FEATURE-OF adaptability of learnable activation functions. Adaptive Piecewise Linear units ( APL ) USED-FOR learnable activation function. stages PART-OF activation functions. gradient information PART-OF activation functions. Symmetric - APL activations USED-FOR deep neural networks. robustness EVALUATE-FOR deep neural networks. deep neural networks USED-FOR adversarial attacks. Network - in - Network CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION Network - in - Network. Lenet CONJUNCTION Network - in - Network. Network - in - Network CONJUNCTION Lenet. activation functions COMPARE ReLUs. ReLUs COMPARE activation functions. activation functions USED-FOR architectures. ResNet-18 HYPONYM-OF architectures. activation functions USED-FOR adversarial fooling. Lenet HYPONYM-OF architectures. Network - in - Network HYPONYM-OF architectures. OtherScientificTerm are positive and negative halves, zero - centered continuous non - linearity, and SymmetricAPL function. Task is ablation studies. ",This paper proposes Adaptive Piecewise Linear units (APL) as a new activation function for deep neural networks. APL is an extension of piecewise linear units (ReLUs) that allows the activation function to be learned in stages. The authors show that APLs are more robust to adversarial perturbations than ReLUs and can be used to improve the robustness of neural networks against adversarial fooling.  ,This paper proposes a new activation function called Symmetric APL (Adaptive Piecewise Linear units) for deep neural networks (DNNs) that can be used to improve the robustness of DNNs against adversarial attacks. The authors show that the proposed function is more expressive than ReLUs in terms of the expressive power of the model. They also show that it is more robust to adversarial fooling attacks. 
12753,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"models CONJUNCTION proprietary data. proprietary data CONJUNCTION models. third party components CONJUNCTION models. models CONJUNCTION third party components. application programming interfaces ( APIs ) USED-FOR pre - trained encapsulated models. wrapping deep learning model USED-FOR classification black - box. wrapping deep learning model USED-FOR measure of uncertainty. Dirichlet layer USED-FOR fusion layer. black - box classifier USED-FOR probabilistic neural network. Dirichlet layer USED-FOR probabilistic neural network. Dirichlet layer USED-FOR distribution. Dirichlet layer USED-FOR estimation of aleatoric uncertainty. multinomial output parameters USED-FOR distribution. uncertainty measure USED-FOR rejection system. NLP CONJUNCTION computer vision. computer vision CONJUNCTION NLP. technique CONJUNCTION methodology. methodology CONJUNCTION technique. wrapper USED-FOR uncertainty. Method are machine learning models, classifier, and simulated API based. Generic are external components, and components. Metric are auditability, and trustability. OtherScientificTerm are uncontrolled potential risks, black - box, aleatoric uncertainty, and misclassifications. ","This paper proposes a method to improve the auditability of black-box machine learning models. The method is based on the Dirichlet layer of a probabilistic neural network, which is used to estimate the aleatoric uncertainty of a classifier. This uncertainty is then used as a rejection signal for the classifier to reject samples with high uncertainty. The authors show that this method can be used to improve auditability in the presence of misclassifications.","This paper proposes a new method to improve the auditability and trustability of black-box deep learning models. The method is based on the Dirichlet layer of a probabilistic neural network, which is used to estimate the aleatoric uncertainty of a classifier and a fusion layer. The authors also propose a rejection system based on a multinomial distribution over the output parameters of the classifier. The proposed method is tested on a variety of applications, including NLP, computer vision, and NLP."
12762,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"Stochastic methods USED-FOR deep neural networks. RMSprop CONJUNCTION Adam. Adam CONJUNCTION RMSprop. coordinate - wise adaptive stepsize FEATURE-OF Stochastic methods. Adam HYPONYM-OF coordinate - wise adaptive stepsize. RMSprop HYPONYM-OF coordinate - wise adaptive stepsize. Adam HYPONYM-OF Stochastic methods. RMSprop HYPONYM-OF Stochastic methods. they COMPARE stochastic gradient descent. stochastic gradient descent COMPARE they. blockwise adaptivity COMPARE adaptivity. adaptivity COMPARE blockwise adaptivity. adaptivity CONJUNCTION generalization. generalization CONJUNCTION adaptivity. blockwise adaptive gradient descent USED-FOR optimizing nonconvex objective. convergence rate FEATURE-OF optimizing nonconvex objective. blockwise adaptive gradient descent USED-FOR online convex learning. regret CONJUNCTION convergence rate. convergence rate CONJUNCTION regret. convergence rate EVALUATE-FOR blockwise adaptive gradient descent. regret EVALUATE-FOR blockwise adaptive gradient descent. blockwise adaptivity COMPARE coordinate - wise adaptivity. coordinate - wise adaptivity COMPARE blockwise adaptivity. generalization error EVALUATE-FOR coordinate - wise adaptivity. generalization error EVALUATE-FOR blockwise adaptivity. Nesterov ’s accelerated gradient CONJUNCTION Adam. Adam CONJUNCTION Nesterov ’s accelerated gradient. blockwise adaptive gradient descent COMPARE Adam. Adam COMPARE blockwise adaptive gradient descent. blockwise adaptive gradient descent COMPARE Nesterov ’s accelerated gradient. Nesterov ’s accelerated gradient COMPARE blockwise adaptive gradient descent. Method is Adagrad. OtherScientificTerm are network parameters, blockwise adaptive stepsize, and nonconvex objective. Metric is uniform stability. ",This paper studies the convergence of blockwise adaptive gradient descent for online convex learning with nonconvex objective. The main contribution is to show that blockwise adaptivity improves the generalization error compared to coordinate-wise adaptive stepsize. The paper also shows that the convergence rate of the proposed method is faster than that of Adam.  ,This paper studies the convergence rate of blockwise adaptive gradient descent for online convex learning. The main contribution of the paper is a theoretical analysis of the generalization error of the blockwise adaptivity of Adam and Adam-RMSprop. The authors show that blockwise Adaptive Gradient Descent (BAD) can converge faster than coordinate-wise adaptive stepsize (CAGS) in the case of nonconvex optimization. They also show that the regret of Adagrad is lower than that of Adam.   
12771,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"large - scale supervised datasets USED-FOR deep features. datasets CONJUNCTION spatiotemporal models. spatiotemporal models CONJUNCTION datasets. scene and object structure FEATURE-OF implicit biases. implicit biases FEATURE-OF video datasets. observable and controllable object and scene bias FEATURE-OF video dataset. spatiotemporal understanding USED-FOR video dataset. 3D objects USED-FOR dataset. diagnostic tools USED-FOR spatiotemporal video architectures. CATER USED-FOR diagnostic tools. CATER HYPONYM-OF dataset. CATER USED-FOR deep video architectures. Task are Computer vision, static image analysis, and video understanding. Method are frame - by - frame classification methods, and long - term reasoning. OtherScientificTerm is temporal structure. ","This paper proposes a new video dataset called CATER, which is a video dataset with 3D objects. The dataset is designed to test the ability of deep neural networks to learn spatiotemporal representations of objects and scenes. The authors claim that the proposed dataset can be used as a diagnostic tool to evaluate the performance of deep networks on video datasets.   ","This paper proposes a new dataset, CATER, for spatiotemporal video understanding. CATER consists of a set of 3D objects and 2D scenes, and it is designed to be used as a diagnostic tool for deep video architectures. The authors show that CATER can be used to identify implicit biases in video datasets. They also show that the CATER dataset can also be used for long-term reasoning.  "
12780,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"distribution USED-FOR synthetic data. Generative Adversarial Networks ( GANs ) HYPONYM-OF models. distribution USED-FOR models. image data USED-FOR visual applications. model compatibility problem HYPONYM-OF task. generating near - boundary data USED-FOR classifiers. generating ‘ easier ’ synthetic data USED-FOR GANs. pre - trained classifiers USED-FOR boundary information. Boundary - Calibration GANs ( BCGANs ) USED-FOR GAN. boundary information USED-FOR Boundary - Calibration GANs ( BCGANs ). pre - trained classifiers USED-FOR Boundary - Calibration GANs ( BCGANs ). GAN variants USED-FOR model compatibility. BC - loss CONJUNCTION GAN variants. GAN variants CONJUNCTION BC - loss. BC - loss USED-FOR model compatibility. BCGANs COMPARE GANs. GANs COMPARE BCGANs. model compatibility EVALUATE-FOR GANs. BCGANs USED-FOR realistic images. BCGANs COMPARE GANs. GANs COMPARE BCGANs. GANs USED-FOR realistic images. model compatibility EVALUATE-FOR BCGANs. Material is near - boundary data. Method is generator of GAN. OtherScientificTerm are posterior distributions, and boundaries of the pre - trained classifiers. ",This paper proposes a method to improve the model compatibility of GANs by generating near-boundary data. The method is based on the idea that the boundary information of the pre-trained classifiers can be used to improve model compatibility. The proposed method is called Boundary-Calibration GAN (BCGANs) and it is shown that the BC-Loss is able to learn the boundaries of the classifiers. The authors also show that BC-loss can be combined with existing GAN variants and show that it improves model compatibility in terms of model accuracy.,"This paper proposes Boundary-Calibration GANs (BCGANs), a GAN-based method for generating near-boundary data to improve model compatibility between GAN and classifiers. The authors show that BCGANs can be used in conjunction with GAN variants to improve the model compatibility. They also show that the BC-loss can be combined with the GAN variant to achieve better model compatibility, and show that it can be applied to a variety of GAN models."
12789,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,approach USED-FOR robust neural networks. Adversarial training USED-FOR robust neural networks. Adversarial training USED-FOR approach. minimax robust optimization problem USED-FOR adversarial training. outer minimization USED-FOR robust classifier. inner maximization USED-FOR adversarial samples. outer minimization CONJUNCTION inner maximization. inner maximization CONJUNCTION outer minimization. hand - designed algorithms USED-FOR inner problem. convolutional neural network USED-FOR optimizer. robust classifier USED-FOR adversarial attack. optimizer USED-FOR adversarial attack. L2L COMPARE adversarial training methods. adversarial training methods COMPARE L2L. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR L2L. classification accuracy CONJUNCTION computational efficiency. computational efficiency CONJUNCTION classification accuracy. classification accuracy EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR adversarial training methods. computational efficiency EVALUATE-FOR L2L. classification accuracy EVALUATE-FOR L2L. L2L framework USED-FOR generative adversarial imitation learning. Task is minimax problem. OtherScientificTerm is convex - concave structure. Method is adversarial training method. ,"This paper proposes a new adversarial training method for robust neural networks. The proposed method is based on the minimax robust optimization problem, which is a convex-concave optimization problem with a convolutional neural network as the optimizer. The main idea is to use the inner minimization of the robust classifier to generate adversarial samples and then use the outer minimization to train the robust model. The method is evaluated on CIFAR-10 and Cifar-100 datasets and achieves state-of-the-art performance. ","This paper proposes a new adversarial training method for robust neural networks. The proposed method is based on the minimax robust optimization problem (L2L), where the inner minimization problem is solved by a convolutional neural network and the outer minimization is solved using a hand-designed algorithm. The method is evaluated on CIFAR-10 and CifAR-100 datasets. The results show that L2L outperforms the state-of-the-art in terms of classification accuracy and computational efficiency."
12798,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"approaches USED-FOR Inverse Reinforcement Learning ( IRL ). reward function USED-FOR expert agent ’s policy. reward function USED-FOR control task. reward CONJUNCTION hard constraints. hard constraints CONJUNCTION reward. reward USED-FOR behavior. Markov Decision Processes ( MDPs ) USED-FOR IRL. Maximum Entropy IRL framework USED-FOR approach. algorithm USED-FOR Maximum Likelihood Constraint. algorithm USED-FOR observed behavior. Maximum Likelihood Constraint USED-FOR observed behavior. OtherScientificTerm are cumulative rewards, nominal reward function, and MDP. Generic is method. Material is simulated behavior. ","This paper studies the problem of inverse reinforcement learning (IRL) in Markov Decision Processes (MDPs), where the goal is to learn a policy that satisfies a set of constraints on the reward function and the MDP. The authors propose a Maximum Entropy IRL framework to solve this problem. The main contribution of the paper is a new algorithm that uses a Maximum Likelihood Constraint to estimate the reward and hard constraints. The proposed algorithm is shown to outperform the state-of-the-art IRL algorithms in simulated experiments. ",This paper proposes a new method for inverse reinforcement learning (IRL) in Markov Decision Processes (MDPs) where the agent is given a control task and a reward function. The reward function is a function of the MDP and the agent’s policy. The authors propose a new algorithm for IRL based on the Maximum Entropy IRL framework. The main contribution of the paper is the use of the Maximum Likelihood Constraint (MCL) algorithm to estimate the observed behavior of the agent under the MCL. 
12807,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,deep recurrent neural network architecture USED-FOR visual cortical circuits. architecture USED-FOR contour detection tasks. architecture COMPARE feedforward networks. feedforward networks COMPARE architecture. sample efficiency EVALUATE-FOR feedforward networks. γ - Net HYPONYM-OF architecture. sample efficiency EVALUATE-FOR architecture. orientation - tilt illusion HYPONYM-OF perceptual illusion. lowlevel edges COMPARE high - level object boundary contours. high - level object boundary contours COMPARE lowlevel edges. it USED-FOR lowlevel edges. γ - Net contour detection accuracy EVALUATE-FOR illusion. neural circuits USED-FOR biological visual systems. circuits PART-OF artificial neural networks. circuits USED-FOR computer vision. neural circuits USED-FOR contour detection. artificial neural networks USED-FOR computer vision. biological visual systems USED-FOR contour detection. neural circuits USED-FOR orientation - tilt illusion. ,"This paper proposes a novel deep recurrent neural network architecture for contour detection tasks. The proposed architecture is based on a convolutional neural network with recurrent layers. The authors show that the proposed architecture achieves state-of-the-art performance on the object boundary detection task. They also show that this architecture is able to capture the orientation-tilt illusion, which is a perceptual phenomenon observed in biological visual systems.","This paper proposes a novel deep recurrent neural network architecture for contour detection tasks. The proposed method is based on a deep recurrent network architecture, called γ-Net, which is a combination of a feedforward network and a convolutional network. The main contribution of the paper is that the proposed method can be applied to a variety of visual tasks, such as object boundary detection and orientation-tilt illusion detection. The experimental results show that the method outperforms the state-of-the-art in terms of accuracy and sample efficiency. "
12816,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,object detection methods USED-FOR detecting and classifying objects. deep convolutional neural networks ( CNNs ) USED-FOR object detection methods. semantics context constraints FEATURE-OF CNN - based object detector. conditional random field ( CRF ) model USED-FOR CNN. mean - field inference method USED-FOR CRF. context - aware module USED-FOR mean - field inference method. context - aware module PART-OF conCNN. stack of common CNN operations USED-FOR mean - field inference method. stack of common CNN operations USED-FOR conCNN. It PART-OF region - based object detection paradigm. average precision ( AP ) EVALUATE-FOR object detection. COCO datasets EVALUATE-FOR conCNN. conCNN USED-FOR object detection. average precision ( AP ) EVALUATE-FOR conCNN. Generic is methods. OtherScientificTerm is semantic context. ,This paper proposes a method for object detection based on a conditional random field (CRF) model with semantic context constraints. The proposed method uses a context-aware module and a mean-field inference method to perform inference in the CRF model. The method is evaluated on COCO and ImageNet datasets. ,This paper proposes a new object detection method based on a conditional random field (CRF) model. The CRF model is trained with a context-aware module and a mean-field inference method. The proposed method is evaluated on COCO and CIFAR-10 datasets.
12825,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"Batch Normalization ( BatchNorm ) USED-FOR deep neural networks. adversarial perturbations FEATURE-OF it. adversarial vulnerability FEATURE-OF BatchNorm. training CONJUNCTION inference. inference CONJUNCTION training. normalization statistics USED-FOR training. normalization statistics USED-FOR inference. normalization statistics USED-FOR adversarial vulnerability. adversarial vulnerability FEATURE-OF BatchNorm layer. mini - batch statistics HYPONYM-OF normalization statistics. Method are neural network architectures, and Robust Normalization ( RobustNorm ). OtherScientificTerm is adversarial perturbation. ","This paper studies the adversarial vulnerability of batch normalization (BatchNorm) in the presence of adversarial perturbations in deep neural networks. The authors show that BatchNorm is vulnerable to adversarial attacks in terms of the number of normalization layers and the size of the batch size. They then propose RobustNorm (RobustNorm), a variant of BatchNormalization that is more robust against adversarial attack. They show that the robustness of the model is related to the normalization statistics of the training and inference. ","This paper studies the vulnerability of batch normalization (BatchNorm) to adversarial perturbations. The authors show that the adversarial vulnerability of BatchNorm is related to the number of mini-batch statistics (i.e., the size of the batch) of the training data. They show that this vulnerability is due to the fact that the mini-batches are not as large as the total training data, and that it is not the same for the training and inference data.  The authors propose Robust Normalization (RobustNorm), which is a new normalization technique that is based on the idea of robust normalization. RobustNorm is an extension of the previous RobustNormalization (RNN) framework, and it is shown that it can be used to improve the robustness of the normalization statistics. "
12834,SP:f16d3e61eda162dfee39396abbd594425f47f625,"Over - parameterized deep neural networks USED-FOR labeling of data. first - order methods USED-FOR Over - parameterized deep neural networks. over - fitting ability USED-FOR generalization. clean test data EVALUATE-FOR regularization methods. early - stopping HYPONYM-OF regularization methods. trainable auxiliary variable USED-FOR network output. regularization HYPONYM-OF regularization methods. generalization guarantee FEATURE-OF clean data distribution. gradient descent training USED-FOR generalization guarantee. methods USED-FOR gradient descent training. wide neural network CONJUNCTION neural tangent kernel ( NTK ). neural tangent kernel ( NTK ) CONJUNCTION wide neural network. noisily labeled datasets EVALUATE-FOR methods. OtherScientificTerm are network parameters, noisy labels, and label noise. Metric is generalization bound. ",This paper studies the over-parameterized deep neural networks in the presence of noisy labels. The authors propose two regularization methods to improve the generalization performance of ONNs with noisy labels: (1) early-stopping and (2) training with a trainable auxiliary variable. Theoretical results are provided to show that the proposed methods improve generalization on clean and noisy data distributions.  ,"This paper studies the generalization ability of over-parameterized deep neural networks with noisy labels. The authors provide a generalization bound for the overfitting ability of the network parameters. The generalization bounds are based on the following assumptions: (1) the data distribution is clean, (2) the training data is not noisy, and (3) there is a trainable auxiliary variable (e.g., NTK) that can be used to train the network output.  The authors show that under certain conditions, the over-fitting generalization can be reduced to a lower bound on the number of test data. They also show that this can be done by training the network with a wide neural network and a neural tangent kernel."
12843,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"convolutional neural networks ( CNNs ) USED-FOR regression. Convolutional Neural Tangent Kernel ( CNTK ) USED-FOR regression. algorithm USED-FOR CNTK. classification accuracy EVALUATE-FOR CNTK. CNTK COMPARE CNN architecture. CNN architecture COMPARE CNTK. classification accuracy EVALUATE-FOR CNN architecture. CIFAR-10 EVALUATE-FOR CNTK. Local Average Pooling ( LAP ) HYPONYM-OF operation. pixel shifts USED-FOR data augmentation. operation USED-FOR kernel. quadratic training cost FEATURE-OF kernel regression. CNN - GP CONJUNCTION CNTK. CNTK CONJUNCTION CNN - GP. full translation data augmentation USED-FOR CNTK. single convolutional layer USED-FOR pre - processing technique. random image patches PART-OF single convolutional layer. CIFAR-10 EVALUATE-FOR kernel. classifier COMPARE trained neural network. trained neural network COMPARE classifier. AlexNet COMPARE classifier. classifier COMPARE AlexNet. CNN - GP HYPONYM-OF kernel. horizontal flip data augmentation USED-FOR kernel. horizontal flip data augmentation USED-FOR CNN - GP. accuracy EVALUATE-FOR kernel. OtherScientificTerm are ` 2 loss, convolutional layers, and fixed kernel. Generic are layer, and kernels. Method are naive data augmentation, Global Average Pooling ( GAP ), and Fashion - MNIST. ","This paper proposes a novel convolutional neural tangent kernel (CNTK) method to improve the performance of deep neural networks (DNNs) on regression tasks. The proposed method is based on the idea of local average pooling (LAP), which is a simple but effective way of augmenting the input images with random patches. Theoretical analysis shows that LAP can be used to reduce the training cost of DNNs by a quadratic factor. Experiments on Fashion-MNIST and CIFAR-10 show the effectiveness of the proposed method.   ","This paper proposes a novel convolutional neural tangent kernel (CNTK) that can be used to improve the performance of deep neural networks (DNNs) on CIFAR-10 and Fashion-MNIST datasets. The proposed method is based on the Local Average Pooling (LAP) operation, which is an extension of Global Average Pool (GAP) to the convolution layer. The authors show that the proposed method can improve the accuracy of DNNs on Cifar-10 by a factor of quadratic training cost. "
12852,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"approach USED-FOR model - free Q - learning. MCTS USED-FOR state - action values. real experience USED-FOR prior. Q - estimates USED-FOR prior. Q - estimates CONJUNCTION real experience. real experience CONJUNCTION Q - estimates. model - free learning CONJUNCTION model - based search. model - based search CONJUNCTION model - free learning. MCTS USED-FOR value computation. physical reasoning tasks CONJUNCTION Atari. Atari CONJUNCTION physical reasoning tasks. agents USED-FOR physical reasoning tasks. agents USED-FOR Atari. it PART-OF agents. model USED-FOR Q - learning agent. Q - learning agent USED-FOR SAVE. SAVE COMPARE model - based search approaches. model - based search approaches COMPARE SAVE. training steps USED-FOR SAVE. real experience USED-FOR SAVE. model - free learning CONJUNCTION planning. planning CONJUNCTION model - free learning. Method is Search with Amortized Value Estimates. OtherScientificTerm are search budgets, and search. ",This paper proposes a method for model-free Q-learning with Amortized Value Estimates (SAVE) that uses MCTS to estimate the state-action values in a model-based search. The proposed method is based on the idea that the Q-values computed by the model should be amortized over time. The authors show that the proposed method outperforms existing methods on a variety of tasks.   ,"This paper proposes a method for model-free Q-based search with Amortized Value Estimates (MCTS) for state-action value computation. The main idea is to use a model-based Q-learning agent to estimate the Q-values of the agent, and then use MCTS to find the best value for the agent. The proposed method is evaluated on a variety of tasks, including Atari, physical reasoning, and planning."
12861,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"deep learning USED-FOR low - level vision tasks. it USED-FOR 3D visualization of the 2D input scenery. network architecture USED-FOR stereoscopic view synthesis. network architecture USED-FOR Deep 3D Pan. globally and locally adaptive dilations USED-FOR t - shaped ” adaptive kernels. local 3D geometries USED-FOR synthesis of naturally looking 3D panned views. globally and locally adaptive dilation FEATURE-OF t - shaped adaptive kernel. t - shaped adaptive kernel USED-FOR monster - net. t - shaped adaptive kernel USED-FOR network architecture. monster - net HYPONYM-OF network architecture. 2 - D input image USED-FOR synthesis of naturally looking 3D panned views. VICLAB STEREO indoors dataset EVALUATE-FOR method. PSNR CONJUNCTION SSIM. SSIM CONJUNCTION PSNR. RMSE CONJUNCTION PSNR. PSNR CONJUNCTION RMSE. PSNR EVALUATE-FOR monster - net. SSIM EVALUATE-FOR monster - net. RMSE EVALUATE-FOR monster - net. monster - net USED-FOR image structures. synthesized images FEATURE-OF image structures. coherent geometry FEATURE-OF synthesized images. coherent geometry FEATURE-OF image structures. SOTA USED-FOR unsupervised monocular depth estimation task. that USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE that. that COMPARE disparity information. disparity information USED-FOR unsupervised monocular depth estimation task. disparity information COMPARE SOTA. SOTA COMPARE disparity information. t - shaped ” kernel COMPARE SOTA. SOTA COMPARE t - shaped ” kernel. SOTA USED-FOR that. t - shaped ” kernel USED-FOR disparity information. Task is single - image - based view synthesis. OtherScientificTerm are parallel camera views, arbitrary camera positions, X - axis, and global camera shift. Material is KITTI. ","This paper proposes a method for 3D panning from a single 2D input image. The method is based on the idea of global and locally adaptive dilations, where the global dilations are computed globally and locally, respectively. The proposed method is evaluated on VIC-STEREO indoors and PSNR datasets.   ","This paper proposes a new neural network architecture for stereoscopic view synthesis. The main idea is to use a t-shaped adaptive kernel for the global and locally adaptive dilations. The proposed method is evaluated on PSNR, SSIM, RMSE and VICLAB STEREO indoors dataset. "
12870,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"representation CONJUNCTION generative model. generative model CONJUNCTION representation. Variational AutoEncoder ( VAE ) USED-FOR generative model. Variational AutoEncoder ( VAE ) USED-FOR representation. tasks PART-OF VAE. capacity FEATURE-OF variational network. information properties FEATURE-OF network. information properties USED-FOR tasks. network HYPONYM-OF information properties. generative model HYPONYM-OF information properties. theoretical objective USED-FOR maximal informative generative model. one HYPONYM-OF generative model. Capacity - Constrained InfoMax ( CCIM ) HYPONYM-OF generative model. one USED-FOR clustered and robust representation. one USED-FOR sharper samples. variational lower bound USED-FOR sharper samples. variational lower bound USED-FOR CCIM. variational lower bound USED-FOR one. one HYPONYM-OF VAE. OtherScientificTerm are informative one, and network capacity. Method is Variational InfoMax AutoEncoder ( VIMAE ). ",This paper proposes a variational auto-encoder (VAE) model with capacity-constrained information maximization (CCIM) objective to improve the performance of VAE on a variety of tasks. The main contribution of the paper is a theoretical analysis of the variational lower bound on the capacity of the model. The authors show that the proposed method is able to achieve better performance than the state-of-the-art variational autoencoders (VAEs). ,"This paper studies the capacity-constrained variational auto-encoder (VAE) of a generative model. The authors propose a theoretical objective to find the maximal informative generative models (i.e., the one that maximizes the capacity of the variational network) for each task. They show that the maximum informative model is the one with the highest capacity. They also provide a variational lower bound for the maximum capacity of a VAE."
12879,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"approach COMPARE Skip - gram. Skip - gram COMPARE approach. disconnected networks USED-FOR latent feature identification. latent feature identification HYPONYM-OF applications. embeddings USED-FOR matrices of node - feature pointwise mutual information. algorithms COMPARE models. models COMPARE algorithms. social, web and citation network datasets EVALUATE-FOR models. social, web and citation network datasets EVALUATE-FOR algorithms. Method are network embedding algorithms, random walks, and multiscale approach ( MUSAE ). OtherScientificTerm are local distribution, and attribute - neighborhood relationships. ","This paper studies the problem of learning the embedding matrix of node-feature pointwise mutual information in disconnected networks. The authors propose to learn the embeddings of nodes and feature matrices of nodes using random walks, which is an extension of the Skip-gram approach. They show that this approach is computationally efficient and achieves better performance than Skip-Gram on social networks and web-based datasets.","This paper proposes a new multiscale embedding method for latent feature identification in disconnected networks. The main idea is to learn a matrices of node-feature pointwise mutual information (i.e., the mutual information between node-features) and attribute-neighborhood relationships. The authors show that this matrices can be used to train a network embedding algorithm, which can be applied to a variety of applications, including social networks, web and citation networks. They show that their method outperforms Skip-Gram in terms of performance on social networks. "
12888,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"multiple phase transitions FEATURE-OF IB objective. definition USED-FOR IB phase transitions. formula USED-FOR IB phase transitions. secondorder calculus of variations USED-FOR formula. Fisher information matrix USED-FOR parameterized models. canonical - correlation analysis ( CCA ) USED-FOR linear settings. maximum ( nonlinear ) correlation USED-FOR IB phase transition. theory USED-FOR algorithm. theory USED-FOR phase transitions. algorithm USED-FOR phase transitions. algorithm USED-FOR prominent phase transitions. theory CONJUNCTION algorithm. algorithm CONJUNCTION theory. theory USED-FOR prominent phase transitions. algorithm USED-FOR class difficulty. class difficulty FEATURE-OF MNIST. categorical datasets EVALUATE-FOR theory. categorical datasets USED-FOR phase transitions. CIFAR10 USED-FOR prominent phase transitions. Task is Information Bottleneck ( IB ). Generic are terms, dataset, and transitions. OtherScientificTerm are encoding distribution, β, and IB loss landscape. Metric is prediction accuracy. ","This paper studies the Information Bottleneck (IB) problem, where the objective is to learn a model with multiple phase transitions. The main contribution is to define the IB phase transitions as the Fisher information matrix of parameterized models. The authors show that the IB objective can be viewed as a second-order calculus of variations, and derive a formula for IB phase transition in terms of the Fisher Information Matrix (FIM). The authors then propose an algorithm to find prominent phase transitions in the IB loss landscape, and show that this algorithm is computationally efficient. ","This paper studies the information bottleneck (IB) problem with multiple phase transitions (i.e., the Fisher information matrix) in the context of phase transitions. The main contribution of the paper is a new definition of the IB objective, which is based on the second-order calculus of variations (2nd order) of variations. The authors also propose a new algorithm for identifying prominent phase transitions in the IB problem. The proposed method is evaluated on MNIST and CIFAR-10 datasets. "
12897,SP:fecfd5e98540e2d146a726f94802d96472455111,"advantage function estimation methods USED-FOR reinforcement learning. independence property USED-FOR importance sampling advantage estimator. close - to - zero variance FEATURE-OF importance sampling advantage estimator. it COMPARE Monte - Carlo estimator. Monte - Carlo estimator COMPARE it. reward decomposition model USED-FOR Monte - Carlo estimator. method COMPARE advantage estimation methods. advantage estimation methods COMPARE method. sample efficiency EVALUATE-FOR advantage estimation methods. advantage estimation methods USED-FOR complex environments. complex environments EVALUATE-FOR method. sample efficiency EVALUATE-FOR method. OtherScientificTerm are time horizon, Monte - Carlo return signal, and estimation variance. Method is advantage estimation. Generic is estimator. ","This paper proposes a new method for advantage estimation in reinforcement learning. The main idea is to use the importance sampling advantage estimator to estimate the advantage function in the time horizon, which is independent of the reward decomposition model. The proposed method is shown to have a close-to-zero variance compared to the Monte-Carlo estimator in terms of the estimation variance. The experimental results show that the proposed method outperforms the state-of-the-art advantage estimation methods.",This paper proposes a new advantage estimation method for reinforcement learning. The main idea is to use the importance sampling advantage estimator (i.e. importance sampling is used to estimate the importance of the reward function) in the reward decomposition model. The authors show that the advantage estimation is independent of the time horizon and the variance of the estimator is close to zero. They also show that their method is competitive with the Monte Carlo estimator in terms of sample efficiency.
12906,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"approach USED-FOR infinite horizon off - policy evaluation. value function USED-FOR accuracy. value function USED-FOR bias - reduced augmentation of their method. density ratio CONJUNCTION value function estimation. value function estimation CONJUNCTION density ratio. method COMPARE methods. methods COMPARE method. Task is Infinite horizon off - policy policy evaluation. OtherScientificTerm are stationary density ratio, biases, and bias. Method is density ratio estimation. Generic is them. ",This paper proposes a bias-reduced off-policy evaluation method for infinite horizon policy evaluation. The proposed method is based on the assumption that the value function is a function of the density ratio and the bias is reduced by adding bias augmentation. The bias reduced augmentation consists of adding a bias term to the loss function that is proportional to the bias of the original value function. The authors show that the bias reduced loss function is equivalent to the original loss function in terms of the bias reduction term.  ,This paper proposes a new method for infinite horizon off-policy evaluation in which the bias-reduced augmentation of the value function is used to reduce the bias of the method. The bias reduction is based on the assumption that the density ratio and value function estimation are independent of the bias. The authors show that the bias reduced augmentation improves the accuracy of their method by a factor of 1.5. They also show that their method is more accurate than the state-of-the-art. 
12915,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"large - scale base classes USED-FOR generalization of prior knowledge. base data USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR task - agnostic feature. Metric - Softmax loss USED-FOR discriminative feature. Metric - Softmax loss COMPARE episodic training. episodic training COMPARE Metric - Softmax loss. episodic training USED-FOR discriminative feature. task - adaptive transformation USED-FOR classifier. classifier USED-FOR few - shot setting. task - adaptive transformation USED-FOR few - shot setting. approach COMPARE state - of - the - arts. state - of - the - arts COMPARE approach. mini - ImageNet CONJUNCTION CUB-200 - 2011 benchmarks. CUB-200 - 2011 benchmarks CONJUNCTION mini - ImageNet. CUB-200 - 2011 benchmarks EVALUATE-FOR state - of - the - arts. mini - ImageNet EVALUATE-FOR state - of - the - arts. CUB-200 - 2011 benchmarks EVALUATE-FOR approach. mini - ImageNet EVALUATE-FOR approach. Task is Few - shot classification. Generic are task, and two - stage framework. Method are Metric - Softmax classifier, and fine - tuning scheme. ","-softmax loss is proposed to improve the few-shot classification performance. The proposed method is based on a two-stage framework. First, a task-adaptive transformation is used to learn the task-agnostic feature from base data. Second, a fine-tuning scheme is used for fine-tune the classifier. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms the state-of-the-arts.",This paper proposes a two-stage framework for few-shot classification. The first stage is to learn a task-agnostic classifier that can adaptively learn the discriminative feature to the task-agnostic data. The second stage is a fine-tuning scheme to fine-tune the classifier. The proposed method is evaluated on mini-ImageNet and CUB-200-2011 benchmarks. 
12924,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,fluid flow USED-FOR simulations of complex flow phenomena. fluid flow HYPONYM-OF problems. data - driven approach USED-FOR numerical solvers. accuracy EVALUATE-FOR numerical solvers. fine simulation resolution FEATURE-OF numerical scheme. numerical scheme USED-FOR method. neural network USED-FOR correction. fully supervised learning methods CONJUNCTION unsupervised learning method. unsupervised learning method CONJUNCTION fully supervised learning methods. naive and an optimized data acquisition USED-FOR fully supervised learning methods. differentiable Navier - Stokes solver USED-FOR unsupervised learning method. fully supervised learning methods HYPONYM-OF learning approaches. learning approaches USED-FOR targeted learning problem. unsupervised learning method HYPONYM-OF learning approaches. approach USED-FOR arbitrary partial differential equation models. accuracy EVALUATE-FOR fluid flow simulations. Method is numerical methods. Task is nonlinear simulation problems. ,This paper proposes a data-driven approach to train a Navier-Stokes solver for nonlinear fluid flow problems. The main idea is to use a neural network to learn a correction function that is used to train the solver. The proposed method is shown to achieve state-of-the-art accuracy in fluid flow simulations.  ,"This paper proposes a data-driven approach to learn a Navier-Stokes solver for nonlinear fluid flow simulation problems. The method is based on a differentiable Navier Stokes solvers, which can be used to learn an arbitrary partial differential equation model. The proposed method is evaluated on a variety of nonlinear simulation problems, and it is shown to be competitive with other supervised learning methods."
12933,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"problem USED-FOR downstream online continual learning tasks. resource constrained data collection USED-FOR learning methods. discrete autoencoders PART-OF Quantization Modules ( SQM ). Quantization Modules ( SQM ) PART-OF architecture. discrete autoencoders PART-OF architecture. module USED-FOR latent space. latent space FEATURE-OF module. module USED-FOR module. methods COMPARE approach. approach COMPARE methods. method USED-FOR applications. episodic memory USED-FOR Experience Replay. episodic memory USED-FOR SQM. fixed memory budget USED-FOR continual learning benchmarks. method USED-FOR online compression of larger images. it USED-FOR modalities. Imagenet USED-FOR online compression of larger images. LiDAR data HYPONYM-OF modalities. Task is Online Continual Compression. Generic are learned representation, and modularity. OtherScientificTerm are moderate compressions, and pretraining. ","This paper proposes a method for online continual learning in the continual compression setting. The method is based on the idea of quantization modules (SQM), which is an autoencoder architecture with discrete autoencoders. Theoretical analysis is provided to show that the proposed method is able to achieve moderate compressions with a fixed memory budget. Experiments are conducted on a variety of continual learning benchmarks.  ","This paper proposes a new method for online continual learning with discrete autoencoder-based Quantization Modules (SQM). The idea is to use episodic memory to store the experience replay of the learned representations in the latent space of each module, which is then stored in an episodic manner. The method is evaluated on a variety of continual learning benchmarks, including Imagenet, LiDAR, and Experience Replay."
12942,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"framework USED-FOR multi - label problem. Metric Learning USED-FOR k nearest neighbor algorithms. k nearest neighbor algorithms USED-FOR multi - label problem. k nearest neighbor algorithms HYPONYM-OF framework. deep representation approach USED-FOR metric learning. neural networks USED-FOR feature data. raw image data USED-FOR neural networks. neural networks USED-FOR deep representation approach. deep convolutional networks USED-FOR image data. Bidirectional Representation learning USED-FOR network architecture. deep neural networks USED-FOR metric space. metric space FEATURE-OF testing data. deep neural networks USED-FOR model. approach COMPARE methods. methods COMPARE approach. multi - labels tasks EVALUATE-FOR approach. multi - labels tasks EVALUATE-FOR methods. systematic metric EVALUATE-FOR approach. systematic metric EVALUATE-FOR methods. Task is Multi - Label Learning task. Method are multiple - label metric learning, and multi - label metric learning. OtherScientificTerm are application restriction, and label dependency. Material is multi - label data set. ","This paper proposes a new method for multi-label metric learning. The proposed method is based on a bidirectional representation learning approach, where a deep neural network is used to represent the metric space. The method is shown to outperform existing methods on a variety of metric learning tasks.","This paper proposes a new framework for multi-label metric learning. The proposed method is based on the Bidirectional Representation Learning (DPR) framework, which is a deep representation approach for metric learning with deep convolutional networks. The main idea of DPR is to use a neural network architecture to represent the metric space in the test data, and then use a deep neural network to model the feature data. The method is evaluated on a variety of tasks, and the proposed method outperforms the state-of-the-art in terms of performance."
12951,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,large batch sizes CONJUNCTION data parallelism. data parallelism CONJUNCTION large batch sizes. large batch sizes USED-FOR neural networks training. convergence rate EVALUATE-FOR asynchronous methods. dynamical stability USED-FOR asynchronous training. asynchronous stochastic gradient descent algorithm USED-FOR minima. closed - form rules USED-FOR learning rate. momentum PART-OF analysis. momentum USED-FOR training stability. OtherScientificTerm is delay. Task is training. Metric is generalization. Generic is algorithm. ,"This paper proposes an asynchronous stochastic gradient descent (asynchronous SGD) method for training neural networks with large batch sizes. The main contribution is to provide a theoretical analysis of the convergence rate of the proposed method, which is shown to converge to the true minima of the training process. The theoretical analysis is based on the observation that the learning rate of asynchronous SGD converges to zero as the number of training epochs increases. The authors show that this is due to the dynamical stability of the algorithm, and show that the momentum of the method can be used as a measure of training stability.   ",This paper considers the problem of training a neural network with large batch sizes and data parallelism. The authors propose an asynchronous stochastic gradient descent algorithm that can be used to train a network with a large batch size. They show that the convergence rate of the proposed method is bounded by the number of minima of the learning rate. They also provide a theoretical analysis of the stability of the algorithm. 
12960,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"Value estimation PART-OF reinforcement learning ( RL ) paradigm. rich transition structure USED-FOR Model learning. weak scalar signal USED-FOR model - free methods. approach USED-FOR representation learning. representation learning USED-FOR RL. OtherScientificTerm are predictors, reward function, features, and value function. Generic are approaches, and task. Task are value prediction, and policy evaluation settings. Material is Atari 2600 games. ","This paper studies the problem of value estimation in reinforcement learning, where the goal is to estimate the value of a reward function given a set of predictors. The authors propose a model-free reinforcement learning approach that learns a representation of the reward function and uses this representation to learn a policy. They show that the proposed method outperforms model-based methods on Atari games.  ","This paper proposes a new method for value estimation in reinforcement learning (RL). The key idea is to use a weak scalar signal to model the transition structure between the reward function and the predictors. The transition structure is modeled as a series of transitions, where each transition is represented by a set of features, and the value function is represented as a function of these features. The authors show that this transition structure can be used to learn a model-free RL method, which can be applied to Atari 2600 games. They show that their method outperforms the state-of-the-art methods on the Atari games."
12969,SP:6388fb91f2eaac02d9406672760a237f78735452,node classification CONJUNCTION graph classification. graph classification CONJUNCTION node classification. Graph Neural Networks ( GNNs ) USED-FOR graph related tasks. graph classification HYPONYM-OF graph related tasks. node classification HYPONYM-OF graph related tasks. graph neural networks USED-FOR adversarial attacks. graph rewiring operation USED-FOR graph. graph rewiring operation COMPARE operators. operators COMPARE graph rewiring operation. reinforcement learning USED-FOR attack strategy. rewiring operation USED-FOR reinforcement learning. rewiring operation USED-FOR attack strategy. real world graphs EVALUATE-FOR framework. perturbation FEATURE-OF graph structure. OtherScientificTerm is edges. ,: This paper proposes a novel adversarial attack method based on graph rewiring. The main idea is to train a GNN to rewire the nodes in a graph to make it more robust to adversarial attacks. The rewinding operation is trained using reinforcement learning. The proposed method is evaluated on two real-world graph classification tasks. ,"This paper proposes a novel attack strategy for graph neural networks (GNNs) against adversarial attacks. The main idea is to use the rewiring operation of reinforcement learning to attack the GNNs. The key idea of the attack strategy is to perturb the structure of the graph to make it more vulnerable to adversarial attack. The attack strategy consists of two steps: (1) perturbing the graph structure with a perturbation of the edges, and (2) using a reinforcement learning algorithm to learn a rewired graph. The proposed method is evaluated on a variety of real-world graph datasets."
12978,SP:233b12d422d0ac40026efdf7aab9973181902d70,"selected data USED-FOR neural networks. Stein ’s unbiased risk estimator ( SURE ) USED-FOR denoising problems. divergence term PART-OF SURE. neural network framework USED-FOR divergence term. close form expression USED-FOR unbiased estimator. unbiased estimator USED-FOR prediction error. close form expression USED-FOR prediction error. piecewise linear representation USED-FOR encoderdecoder CNN. bootstrap and aggregation scheme USED-FOR neural network. close form representation USED-FOR bootstrap and aggregation scheme. neural network USED-FOR identity mapping. inverse problems EVALUATE-FOR algorithm. Generic are architectures, and it. ",This paper studies the unbiased estimator of Stein’s unbiased risk estimator (SURE) for denoising problems. The authors propose to use a piecewise linear representation of the encoder-decoder network as a bootstrap and aggregation scheme to estimate the prediction error. They show that this approach can be used to solve the inverse problems.   ,"This paper proposes a novel approach to solving Stein’s unbiased risk estimator (SURE) for denoising problems. The main idea is to use a piecewise linear representation of the data to represent the prediction error of the unbiased estimator, which is then used as a bootstrap and aggregation scheme in a neural network framework. The authors show that their approach can be applied to a variety of denoised problems, including inverse problems, and show that it can be used in combination with existing methods."
12987,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"meta - learning approaches USED-FOR few - shot classification. meta - knowledge CONJUNCTION task - specific learning. task - specific learning CONJUNCTION meta - knowledge. Bayesian inference framework USED-FOR objective. variational inference USED-FOR objective. variational inference USED-FOR it. it COMPARE meta - learning approaches. meta - learning approaches COMPARE it. balancing component CONJUNCTION Bayesian learning framework. Bayesian learning framework CONJUNCTION balancing component. OtherScientificTerm are distributional difference, task relatedness, and balancing variables. Method are meta - learning model, and meta - learning. Material is multiple realistic taskand class - imbalanced datasets. ","This paper proposes a meta-learning approach for few-shot classification, where the goal is to learn a task-specific model that is able to generalize well to new tasks. The proposed approach is based on meta-knowledge and meta-balance, which is a combination of meta-information and task-related information. The authors show that the proposed approach can generalize better than other meta-learned methods on imbalanced datasets.   ",This paper proposes a new meta-learning framework for few-shot classification. The main idea is to use a balancing component to balance the distributional difference between the two tasks. The balancing component is based on a Bayesian learning framework. The authors show that the balancing component can be combined with a variational inference framework to improve the performance of the meta-learners.
12996,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"high - dimensional, continuous observations CONJUNCTION unknown dynamics. unknown dynamics CONJUNCTION high - dimensional, continuous observations. distribution shift USED-FOR Supervised learning methods. behavioral cloning ( BC ) USED-FOR Supervised learning methods. inverse RL CONJUNCTION generative adversarial imitation learning ( GAIL ). generative adversarial imitation learning ( GAIL ) CONJUNCTION inverse RL. generative adversarial imitation learning ( GAIL ) HYPONYM-OF methods. inverse RL HYPONYM-OF methods. reinforcement learning ( RL ) USED-FOR methods. generative adversarial imitation learning ( GAIL ) HYPONYM-OF reinforcement learning ( RL ). inverse RL HYPONYM-OF reinforcement learning ( RL ). methods USED-FOR reward function. reward function USED-FOR task. adversarial training USED-FOR complex and brittle approximation techniques. complex and brittle approximation techniques USED-FOR reward function. complex and brittle approximation techniques USED-FOR methods. RL USED-FOR alternative. sparsity prior USED-FOR long - horizon imitation. regularized variant of BC USED-FOR SQIL. sparsity prior USED-FOR regularized variant of BC. SQIL COMPARE GAIL. GAIL COMPARE SQIL. Atari CONJUNCTION MuJoCo. MuJoCo CONJUNCTION Atari. Box2D CONJUNCTION Atari. Atari CONJUNCTION Box2D. SQIL COMPARE BC. BC COMPARE SQIL. Atari FEATURE-OF image - based and low - dimensional tasks. Box2D FEATURE-OF image - based and low - dimensional tasks. MuJoCo FEATURE-OF image - based and low - dimensional tasks. image - based and low - dimensional tasks EVALUATE-FOR SQIL. image - based and low - dimensional tasks EVALUATE-FOR GAIL. imitation method COMPARE methods. methods COMPARE imitation method. constant rewards FEATURE-OF RL. constant rewards USED-FOR imitation method. RL USED-FOR imitation method. learned rewards USED-FOR methods. OtherScientificTerm are expert behavior, error accumulation, out - of - distribution states, and constant reward. Method are RL agent, and soft Q imitation learning ( SQIL ). Generic is method. ","This paper proposes an imitation learning method for reinforcement learning in the presence of distribution shift. The main idea is to use reinforcement learning to learn a reward function that can be used to approximate the reward function of an expert agent. The reward function is approximated by a soft Q-function, which is a combination of a sparsity prior and a regularized version of behavioral cloning (BC). The proposed method is evaluated on MuJoCo, Box2D, and Atari tasks. ","This paper proposes a new method for imitation learning based on reinforcement learning (RL). The main idea is to use a sparsity prior for long-horizon imitation learning, which is a regularized variant of behavioral cloning (BC). The authors show that this method can be applied to both high-dimensional and low-dimensional tasks. They also show that the proposed method outperforms the state-of-the-art in terms of performance on Atari and MuJoCo tasks."
13005,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"Point clouds HYPONYM-OF Lagrangian representation. deep - learning method USED-FOR stable and temporally coherent feature spaces. stable and temporally coherent feature spaces FEATURE-OF points clouds. temporal loss function USED-FOR mingling. higher time derivatives USED-FOR temporal loss function. super - resolution method CONJUNCTION truncation approach. truncation approach CONJUNCTION super - resolution method. techniques CONJUNCTION truncation approach. truncation approach CONJUNCTION techniques. techniques PART-OF super - resolution method. method USED-FOR large, deforming point sets. Generic are approaches, and approach. OtherScientificTerm are time dimension, flickering, undesirable local minima, halo structures, and halos. ",This paper proposes a method to learn stable and temporally coherent feature spaces for Lagrangian point clouds. The proposed method is based on super-resolution and truncation techniques. The main contribution of the paper is a temporal loss function that allows for mingling of time-varying points in the feature space. The authors show that the proposed method can achieve better performance than the state-of-the-art methods.,"This paper proposes a method to learn stable and temporally coherent feature spaces for Lagrangian point clouds. The main idea is to use a temporal loss function to prevent mingling between time-varying points clouds. This is achieved by using higher time derivatives of the temporal loss. The authors also propose a super-resolution method and a truncation method. The proposed method is shown to be effective for large, deforming point sets. "
13014,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"hand - crafted or Euclidean cost USED-FOR cost functions. side information USED-FOR cost function. side information USED-FOR subset correspondence. annotated cell type PART-OF single - cell data. side information USED-FOR cost function. marriage - matching CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION marriage - matching. images CONJUNCTION marriage - matching. marriage - matching CONJUNCTION images. images EVALUATE-FOR method. single - cell RNA - seq EVALUATE-FOR method. method COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE method. marriage - matching EVALUATE-FOR method. images CONJUNCTION single - cell RNA - seq. single - cell RNA - seq CONJUNCTION images. Generic is it. OtherScientificTerm is confounding. Method are Optimal transport ( OT ), OT, transport cost function, and Sinkhorn algorithm. ",This paper proposes a method for learning the optimal transport function in the context of single-cell RNA-seq data. The main idea is to use the Sinkhorn algorithm to learn the transport cost function in a subset correspondence with respect to the cell type. The proposed method is based on the observation that the cost function can be expressed as a function of the side information of a subset of the data points. The authors show that the proposed method outperforms the state-of-the-art methods on the marriage-matching and single-coding benchmarks. ,"This paper proposes a new method for learning the optimal transport function for a subset of single-cell RNA-seq data. The proposed method is based on the Sinkhorn algorithm, which uses the side information of the cost function to learn the subset correspondence. The authors show that the proposed method outperforms the state-of-the-art on a variety of benchmarks. "
13023,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,large datasets USED-FOR method. algorithm USED-FOR representation underlying normal data. clustering technique USED-FOR high dimensional data. clustering CONJUNCTION representation learning. representation learning CONJUNCTION clustering. clustering USED-FOR hypothesizing normal candidate subset. normal data subset USED-FOR autoencoder. representation learning USED-FOR hypothesizing normal candidate subset. reconstruction error USED-FOR scoring function. autoencoder USED-FOR reconstruction error. public benchmark datasets EVALUATE-FOR method. method COMPARE semi - supervised techniques. semi - supervised techniques COMPARE method. method COMPARE unsupervised techniques. unsupervised techniques COMPARE method. public benchmark datasets EVALUATE-FOR unsupervised techniques. unsupervised techniques COMPARE semi - supervised techniques. semi - supervised techniques COMPARE unsupervised techniques. Task is normal data selection. , data is used to train an autoencoder for normal data selection. The authors propose to use a clustering technique to learn the representation underlying normal data and then use this representation to select a subset of normal data from the original data. The reconstruction error of the normal data subset is used as a scoring function to select the normal candidate subset. Experiments on several public benchmark datasets show the effectiveness of the proposed method.   ,This paper proposes a new method to learn the representation of high dimensional data from a subset of data. The proposed method is based on a clustering technique and a representation learning algorithm. The authors show that the proposed method outperforms the state-of-the-art semi-supervised methods on several public benchmark datasets.
13032,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"learning control policies USED-FOR reward function. local reward improvement update USED-FOR first step. L norm CONJUNCTION Kullback - Leibler divergence. Kullback - Leibler divergence CONJUNCTION L norm. convergence FEATURE-OF PCPO. metrics EVALUATE-FOR PCPO. L norm HYPONYM-OF metrics. Kullback - Leibler divergence HYPONYM-OF metrics. metrics USED-FOR convergence. control tasks EVALUATE-FOR PCPO. constraint violation CONJUNCTION reward. reward CONJUNCTION constraint violation. reward EVALUATE-FOR PCPO. OtherScientificTerm are constraints, fairness, policy, constraint set, and policy update. Generic are algorithm, and second step. Method is iterative method. Metric is reward improvement. ","This paper studies the problem of learning a policy that is fair in the presence of constraints. The authors propose PCPO, which is a local reward improvement update method for learning a reward-based policy. The main idea is to update the reward function in the first step, and then update the policy in the second step. The first step updates the reward for each state, while the second one updates the policy for each constraint in the constraint set.  The authors show that PCPO converges to the optimal policy in terms of the L norm and the Kullback-Leibler divergence. They also show that the convergence rate of PCPO is better than the L-norm and the KL-divergence.  ","This paper studies the problem of learning control policies that improve the fairness of the reward function in the context of a local reward improvement update (PCPO). The authors propose a new metric, the Kullback-Leibler divergence (KLD), to measure the convergence of PCPO. They show that PCPO converges to a state-of-the-art state of the art in terms of the L norm and KLD. They also show that the KLD can be used as a metric for the reward improvement.  "
13041,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"analogy structure FEATURE-OF embedding space. low rank transformation USED-FOR embedding. embedding transformation USED-FOR relative distances. α USED-FOR word embedding. Method is word embedding methods. Generic are inner mechanism, and method. OtherScientificTerm is word - context co - occurrence space. Material is real datasets. ","This paper studies the problem of word embedding in the context co-occurrence space. The authors propose a low-rank transformation on the embedding space and show that it can be used to improve the performance of existing word embeddings methods. The proposed method is based on the observation that the word embedded space can be viewed as analogy of the context space, and that it is possible to use a low rank transformation to reduce the distance between the two embedding spaces.  ",This paper proposes a low-rank transformation for word embeddings. The authors show that the embedding space of a word embedding can be represented by a low rank transformation. They show that such a transformation can be used to represent the relative distances between two words in a word-context co-occurrence space. They also show that it is possible to use this transformation to obtain a low ranking embedding.   
13050,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"Similarity measurement USED-FOR data mining and machine learning tasks. approximate Random Projection Trees PART-OF X - Forest. RP Trees USED-FOR similarity measurement. layers PART-OF tree. randomness USED-FOR partition. real - world datasets EVALUATE-FOR model. Euclidean distance - based similarity metrics USED-FOR clustering tasks. model COMPARE RP Trees. RP Trees COMPARE model. X - Forest COMPARE RP Trees. RP Trees COMPARE X - Forest. X - Forest HYPONYM-OF model. efficiency EVALUATE-FOR model. accuracy EVALUATE-FOR RP Trees. Method is similarity measurement solution. Metric are speed, and exalted speed. OtherScientificTerm are prior knowledge, and projection vectors. Task is similarity measurements. ","This paper proposes a method to improve the efficiency of similarity measurement in data mining and machine learning tasks. The proposed method is based on approximate Random Projection Trees (RP Trees), which is a tree-based approach to measure the similarity between two sets of data points. The main idea is to partition the data points into a set of projection vectors, which are then used to estimate the distance between the projection vectors of the two sets. The method is evaluated on a variety of clustering tasks, and compared with existing methods. ","This paper proposes a novel approach to the task of similarity measurement for data mining and machine learning tasks. The authors propose a new approach called Random Projection Trees (RP Trees), which is based on the idea of approximate Random Projections Trees (RPT). The proposed method is based upon the idea that the similarity between two data sets is a function of the number of layers in the tree, and that the distance between two layers is a measure of the similarity of the data. The proposed approach is evaluated on a variety of datasets, and the proposed method outperforms the state-of-the-art in terms of accuracy. "
13059,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"Statistical inference methods USED-FOR machine learning. Markov chain Monte Carlo ( MCMC ) CONJUNCTION variational inference ( VI ). variational inference ( VI ) CONJUNCTION Markov chain Monte Carlo ( MCMC ). Markov chain Monte Carlo ( MCMC ) USED-FOR inference algorithms. variational inference ( VI ) USED-FOR inference algorithms. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. simulation bias FEATURE-OF finite - length MCMC chains. hybrid method USED-FOR VI. gradient - based optimisation USED-FOR hybrid method. gradient - based optimisation USED-FOR simulation bias. method USED-FOR low - biased samples. approximation bias CONJUNCTION computational efficiency. computational efficiency CONJUNCTION approximation bias. method USED-FOR MCMC hyper - parameters. method COMPARE hybrid methods. hybrid methods COMPARE method. MCMC CONJUNCTION VI. VI CONJUNCTION MCMC. hybrid methods USED-FOR MCMC. hybrid methods USED-FOR VI. Generic is methods. Method are MCMC methods, and VI methods. OtherScientificTerm is MCMC simulation. ",This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation. The main contribution of the paper is to show that the approximation bias of MCMC with finite-length MCMC chains can be reduced by using the gradient of the hyper-parameters of the MCMC hyper- parameters. The proposed method is shown to be computationally efficient in terms of the number of iterations and the computational complexity.  ,"This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) for the simulation bias of finite-length MCMC chains. The main contribution of the paper is to propose a hybrid approach for the approximation bias of MCMC and VI. The method is based on a gradient-based optimisation approach to reduce simulation bias. The authors show that their method can be applied to both MCMC- and VI-based methods, and show that it can be used for both."
13068,SP:64f2744e938bd62cd47c1066dc404a42134953da,"treatment USED-FOR Inferring causal effects. observational data USED-FOR Inferring causal effects. state - of - the - art methods USED-FOR causal inference. adapted unconfoundedness hypothesis USED-FOR they. variational autoencoders USED-FOR missing values. variational autoencoders USED-FOR latent confounders. them PART-OF multiple imputation strategy. methodology COMPARE competitors. competitors COMPARE methodology. methodology USED-FOR non - linear models. non - linear models COMPARE competitors. competitors COMPARE non - linear models. OtherScientificTerm are covariates, and Missing data. Task are real - world analyses, and causal inference procedures. Generic is They. ","This paper proposes a method for causal inference in the presence of missing data. The proposed method is based on variational autoencoders, where the latent confounders are assumed to be non-linear in the covariate space. The authors propose to use the unconfoundedness hypothesis as a proxy for the missing data, and then use a multiple imputation strategy to estimate the missing values. The experimental results show that the proposed method outperforms the state-of-the-art methods in terms of causal inference.",This paper proposes a new method for causal inference based on the unconfoundedness hypothesis. The main idea is to use a variational autoencoder (VAE) to estimate the latent confounders and use them as a multiple imputation strategy. The authors show that the proposed method outperforms the state-of-the-art methods in terms of the number of missing data and the accuracy of the missing data. 
13077,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"neural architecture search algorithm USED-FOR compact reinforcement learning ( RL ) policies. ENAS CONJUNCTION ES. ES CONJUNCTION ENAS. combinatorial search space FEATURE-OF NAS. edge - partitionings USED-FOR compact architectures. vanilla policies COMPARE compact policies. compact policies COMPARE vanilla policies. compression EVALUATE-FOR vanilla policies. compression EVALUATE-FOR compact policies. colorings USED-FOR policies. colorings USED-FOR RL tasks. Toeplitz matrices USED-FOR compact policies. structured neural network architectures USED-FOR RL problems. approach USED-FOR structured neural network architectures. mobile robotics FEATURE-OF RL problems. limited storage and computational resources FEATURE-OF mobile robotics. OtherScientificTerm are edge - partitionings ( colorings ), same - weight classes, and weight parameters. ","This paper proposes a method to search for compact reinforcement learning policies in reinforcement learning. The method is based on edge-partitioning, where each edge is partitioned into a set of weight classes, and each weight class is represented by a Toeplitz matrix. The authors show that the proposed method is able to find compact policies in the combinatorial search space of NAS. They also show that their method can be applied to a variety of RL tasks, including reinforcement learning in mobile robotics. ","This paper proposes a method for finding compact reinforcement learning (RL) policies in the combinatorial search space of neural network architectures. The main idea is to use a Toeplitz matrix to search for compact policies in a combinatorially search space. The paper shows that the proposed method outperforms vanilla policies in terms of performance and compression. The proposed method is tested on a variety of RL tasks, including mobile robotics."
13086,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"time - series USED-FOR representation learning. Group Transform approach USED-FOR representation learning. framework USED-FOR time - frequency transformations. Wavelet Transform HYPONYM-OF time - frequency transformations. approach USED-FOR non - linear transformations. affine transformations of a mother filter USED-FOR Wavelet Transform filter - bank. transformations USED-FOR signal representations. maps USED-FOR signal representations. maps USED-FOR transformations. parameterization USED-FOR non - linear map. Deep Neural Network USED-FOR Group Transform. time - series datasets EVALUATE-FOR framework. Generic is representation. OtherScientificTerm are invertible maps, and strictly increasing and continuous functions. ","This paper proposes a novel framework for time-series representation learning based on time-frequency transformations. The proposed method is based on the Wavelet Transform, which is a family of affine transformations of a mother filter. The authors show that the proposed method can be used to learn non-linear transformations of time-frequencies using a deep neural network.   ",This paper proposes a new framework for time-series representation learning based on the Wavelet transform. The key idea is to use the affine transformations of a mother filter to represent the time-frequency transformations of the wavelet filter bank. This is done by using a deep neural network (DNN) to learn the representations of a wavelet transform bank. The proposed method is evaluated on a variety of time series datasets. 
13095,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"inductive biases USED-FOR real - world data properties. scale - free HYPONYM-OF real - world data properties. hyperbolic or spherical HYPONYM-OF non - Euclidean spaces. Euclidean geometry CONJUNCTION vector space operations. vector space operations CONJUNCTION Euclidean geometry. Euclidean geometry USED-FOR graph neural networks. vector space operations USED-FOR graph neural networks. graph convolutional networks ( GCN ) USED-FOR ( products of ) constant curvature spaces. gyro - barycentric coordinates USED-FOR Euclidean concept of the center of mass. models USED-FOR Euclidean counterparts. node classification CONJUNCTION distortion minimization. distortion minimization CONJUNCTION node classification. Euclidean GCNs USED-FOR node classification. Euclidean GCNs USED-FOR distortion minimization. distortion minimization USED-FOR symbolic data. non - Euclidean behavior FEATURE-OF symbolic data. Method is unified formalism. OtherScientificTerm are geometries of constant curvature, curvature, and discrete curvature. ","This paper proposes to use graph convolutional networks (GCNs) with constant curvature to model the center of mass in non-Euclidean spaces. In particular, the authors propose to use gyro-barycentric coordinates to represent the center in the space of constant curvatures. The authors show that such a method can be used to train GNNs on non-euclidian graphs. They also show that their method is able to generalize to Euclidean graphs.   ",This paper proposes a unified formalism for graph convolutional neural networks (GCN) for constant curvature spaces. The main idea is to use the gyro-barycentric coordinates of the center of mass of the Euclidean concept of the centre of mass as the basis for the construction of the GCN. The authors show that the proposed GCN can be used to compute the product of the products of constant curvatures in the space of continuous and discrete curvatures. They also show that their method can be applied to graph classification and distortion minimization tasks.
