paper_id,summary
SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"This paper proposes sqSGD (selective quantized stochastic gradient descent) for federated learning under local differential privacy constraints. The proposed algorithm is based on a novel privacy-preserving quantization scheme that uses a constant number of bits per dimension per client. It improves the base algorithm in two ways: first, it applies a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, it utilizes randomized rotation as a preprocessing step to reduce quantization error."
SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a simple and general representation method to consider prior knowledge related to language representation from the beginning of training. The proposed method allows SANs to leverage prior knowledge in a universal way compatible with neural networks. The authors apply it to one prior word frequency knowledge for monolingual data and other prior translation lexicon knowledge for the bilingual data, thereby enhancing the language representation. Experimental results on WMT14 English-to-German and WMT17 Chinese-toEnglish translation tasks demonstrate the effectiveness and universality of the proposed method over a strong Transformer-based baseline."
SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,"This paper proposes a Bayesian Stackelberg Markov game-theoretic model (BSMG) to model uncertainty over attacker types and the nuances of a moving target defense (MTD) system. BSMGs can be used to model various MTD scenarios, capturing the uncertainty over the attacker type and the sequential impacts of attacks and switching defenses, and characterize the notion of strong equilibrium in the landscape of incomplete-information Markov games. The authors also propose a BSM-Q-learning approach that can learn the optimal movement policy for MTD systems within a reasonable time. Experiments show that the learned movement policy improves the state-of-the-art in MTD."
SP:97911e02bf06b34d022e7548beb5169a1d825903,This paper proposes a VAE ensemble framework for unsupervised disentangled representation learning. The ensemble consists of multiple VAEs. The latent variables in every pair of these models are connected through linear transformations to force the ensemble to be similar to a linear transformation. Theoretical analysis shows that the linear transformations connect the models in the ensemble tend to connect the latent representations of the individual models. Experiments show that the proposed method outperforms the state-of-the-art disentanglement methods.
SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The proposed method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The authors train a graph neural network in which each node represents a dataset to predict the best machine learning pipeline for a new test dataset. The approach generalizes to new datasets and new sets of datasets.
SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the relationship between compositionality and gradient descent. The authors argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. They find that the optimization process imposes a bias toward non-compositional solutions. This is caused by gradient descent, trying to use all available and redundant information from input, violating the conditional independence property of compositionality. Based on this finding, they suggest that compositionality-learning approaches considering only model architecture design are unlikely to achieve complete compositionality, and suggest that only manual or search-based approaches with model structure design (manual or searching) alone are not likely to achieve full compositionality in general."
SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper provides a theoretical analysis of embedding-based entity alignment (EEA) methods. The authors show that the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function for embedding learning. However, such a margin cannot guarantee to be tight enough for alignment learning. To mitigate this problem, the authors propose a new approach that explicitly learns KG-invariant and principled entity representations, while preserving the original infrastructure of existing methods. "
SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor algorithm co-design framework to develop more energy-efficient CNN-powered PhlatCam. The proposed SACoD jointly optimizes the phase mask and the convolution kernel in the Phlatcam sensor and the backend CNN model via differential neural architecture search. Extensive experiments including both simulation and physical measurement on manufactured masks show that the proposed method achieves aggressive model compression and energy savings while maintaining or even boosting task accuracy, when benchmarking over two state-of-the-art designs with six datasets on four different tasks."
SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a temporal matrix factorization model for learning the trajectories of individuals in two honey bee colonies over multiple generations in two generations. The authors propose to jointly learn the average developmental path and structured variations of individuals over their entire lives. Their method yields interpretable embeddings that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they live. The method provides a quantitative framework for understanding behavioral heterogeneity in complex social systems applicable in fields such as behavioral biology, social sciences, neuroscience, and information science."
SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation pipeline for image reconstruction tasks arising in medical imaging and explores its effectiveness at reducing the required training data in a variety of settings. In particular, the authors focus on accelerated magnetic resonance imaging, where the goal is to reconstruct an image from a few under-sampled linear measurements. The proposed pipeline is specifically designed to utilize the invariances present in the medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. The authors demonstrate the effectiveness of the proposed pipeline by showing that for some problem regimes, DA can achieve comparable performance to the state of the art on the fastMRI dataset while using significantly fewer training data."
SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a clustering algorithm for order learning. The clustering is based on the order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. Then, the object instances are grouped into clusters according to their identity features using a repulsive term. Moreover, the rank of a test instance is estimated by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show the effectiveness of the proposed algorithm."
SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,This paper proposes a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behavior. The proposed method outperforms the state-of-the-art intrinsic reward strategies in terms of sample efficiency and performance.
SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes Isometric Propagation Network (IPN) for zero-shot learning (ZSL), which learns to strengthen the relation between classes within each space and align the class dependency in the two spaces. Specifically, IPN learns to propagate the class representations on an auto-generated graph within the semantic space and the visual space, and regularizes the two dynamic propagation procedures to be isometric in terms of the two graphs’ edge weights per step by minimizing a consistency loss between them. The proposed method achieves state-of-the-art performance on three popular ZSL benchmarks."
SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a task-conditioned hypernetwork for controlling the feed-forward layers of the Transformer. Specifically, the authors propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. The authors conduct an extensive set of experiments on GLUE/SuperGLUE and show that the proposed method can match the performance of the SOTA while being 16 times more parameter efficient."
SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper proposes an approach to improve the performance of the task of “learning to steer” for self-driving cars by simulating image degradations at training time. The approach builds a dataset of adversarially degraded images by apply evolutionary optimization within the space of possible degredations during training. On each training iteration, the parameters are updated to generate a new degradation combination so that system performance is (approximately) minimized. Experiments show that the approach improves the task performance up to 48% over strong baselines."
SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), a framework for applying deep learning to optimization problems with hard constraints. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. The authors demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid."
SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,This paper proposes a growing L2 regularization scheme for neural network pruning. The authors show that the growing regularization can be used to improve the performance of two problems: pruning schedule and weight importance scoring. They also propose a Hessian-based method to exploit the Hessian information for more accurate pruning without knowing the weights' values. The proposed method is evaluated on CIFAR-10 and ImageNet datasets and achieves competitive results.
SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper systematically studies the role of planning and its algorithmic design choices in MuZero, a model-based reinforcement learning algorithm. The authors perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. The results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization."
SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes a new value iteration network (VIN) for implicit planning in reinforcement learning (RL). The authors combine ideas from contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning to improve VIN-style models on generic environments. The authors show that XLVINs match the performance of VINs when the underlying MDP is discrete, fixed and known, and provide significant improvements to model-free baselines across three general MDP setups."
SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the inductive bias of learning read-once DNFs, a subset of functions that are known to be efficiently learnable by neural networks in the distribution-free setting. The authors first observe empirically that the learned neurons are aligned with the terms of the DNF, despite the fact that there are many zero-error networks that do not have this property. Then, the authors show that minimizing the population risk can be minimized by multiple networks: from ones that memorize data to ones that compactly represent the functions. Finally, they analyze why gradient descent “chooses the compact representation”."
SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes a method for goal-oriented reinforcement learning (GRL) that leverages graph structure in historical trajectories to slowly adjust exploration directions and rapidly update value function estimation with related experiences. The proposed method constructs a dynamic graph on top of state transitions and develops an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. The authors also leverage graph structure to sample related trajectories for efficient value learning. Experimental results show that the proposed method outperforms the state-of-the-art algorithms in terms of sample efficiency on benchmarks with sparse reward functions."
SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method for sampling procedurally generated (PCG) environments for testing systematic generalization of reinforcement learning agents. The authors claim that different levels provide different learning progress for an agent at specific times during training. They introduce Prioritized Level Replay, a general framework for estimating the future learning potential of a level given the current state of the agent’s policy. They find that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning progress when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it."
SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a method to perform fine-grained manipulation of the auxiliary task gradients. The authors propose to decompose auxiliary updates into directions which help, damage or leave the primary task loss unchanged. This allows weighting the update directions differently depending on their impact on the problem of interest. The method leverages efficient automatic differentiation procedures and randomized singular value decomposition for scalability."
SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper presents a method for one-shot learning of objects in a 3D environment. The agent is trained with a combination of conventional RL and predictive (semi-supervised) learning. It is shown that it is able to learn to fast-map novel names to novel objects, and that this ability is enhanced by meta-learning, episodic memory, and a novel dual-coding external memory. Experiments show that the method generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects."
SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the effect of class-imbalance (CI) on the performance of few-shot learning (FSL) methods in the low-data regime. In particular, the authors consider three regimes of CI: (1) dataset vs. support set imbalance, (2) step, (3) random CI, and (4) meta-training CI. The authors compare 10 state-of-the-art FSL methods using backbones of different depths on multiple datasets and show that the performances of CI-imbalanced methods always drop, by up to 18.0% for optimization-based methods, although feature-transfer and metric based methods generally suffer less. Moreover, they also show that strategies used to mitigate imbalance in supervised learning can be adapted to the FSL case resulting in better performances."
SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a new convolution layer for graph convolutional neural networks (GCNs). The proposed method is based on the idea of Polynomial Graph Convolution (PGC) layer, which independently considers neighbouring nodes at different topological distances (i.e. arbitrarily large receptive fields). The authors prove that the proposed PGCN can represent a richer set of functions compared to the linear stacking of two or more GCN layers, i.e., it is more expressive. Moreover, the linear PGC design allows to consider large receptive field without incurring in typical issues related to training deep networks. The authors empirically evaluate the proposed method on eight commonly adopted graph classification benchmarks."
SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes Sim2SG, a technique for sim-to-real transfer for scene graph generation. It decomposes the domain gap into appearance, label and prediction discrepancies between the two domains, and uses pseudo statistic based self-learning and adversarial techniques to handle these discrepancies. The experiments demonstrate significant improvements over baselines in reducing the domain gaps."
SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"This paper introduces a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), for continuous-action reinforcement learning (RL). The authors show that REDQ can achieve the performance of Model-Based Policy Optimization (MBPO) on the MuJoCo benchmark with fewer parameters than MBPO, and with less wall-clock run time. The key ingredients of REDQ are 1) a high Update-To-Data (UTD) ratio, 2) an ensemble of Q-functions, and 3) in-target minimization across a random subset of Q functions from the ensemble. The authors also provide a theoretical analysis providing insights into the algorithm."
SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a distribution-based invariant deep architecture (DIDA), which extends the work of (Maron et al., 2020) to handle continuous distributions instead of discrete ones. The proposed architecture inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. The authors also empirically demonstrate the merits of the approach on two tasks defined at the dataset level. On both tasks, DIDA learns meta-features supporting the characterization of a labeled dataset."
SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes a spectral graph densification approach (GRASPEL) for graph learning from data. The key idea is to learn ultra-sparse undirected graphs from potentially high-dimensional input data. By limiting the precision matrix to be a graphLaplacian-like matrix in graphical Lasso, the approach aims to learn the graph sparse and spectrally-robust. Compared with prior state-of-the-art graph learning approaches, the proposed method is more scalable and allows substantially improving computing efficiency and solution quality of a variety of data mining and machine learning applications."
SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a method to learn a goal-conditioned policy with intrinsic motivation (GPIM), which jointly learns an abstract-level policy conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals. The learned discriminator serves as an intrinsic reward function to imitate the trajectory induced by the abstract level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed method which substantially outperforms prior techniques."
SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"This paper studies the problem of policy switching in deep reinforcement learning (DQN) with low switching cost, i.e., a small number of policy switches during training. Such a requirement is ubiquitous in many applications, such as medical domains, recommendation systems, education, robotics, dialogue agents, etc., where the deployed policy that actually interacts with the environment cannot change frequently. The paper investigates different policy switching criteria based on deep Q-networks and further proposes an adaptive approach based on the feature distance between the deployed Q-network and the underlying learning Q-Network. Through extensive experiments on a medical treatment environment and a collection of Atari games, the authors find that the feature-switching criterion substantially decreases the switching cost while maintaining a similar performance to the case without the switching-cost constraint. They also complement this empirical finding with a theoretical justification from a representation learning perspective."
SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a first-order stochastic algorithm based on a combination of homotopy methods and SGD, called Homotopy-Stochastic Gradient Descent (H-SGD), which finds interesting connections with some proposed heuristics in the literature, e.g. optimization by Gaussian continuation, training by diffusion, mollifying networks. Under some mild assumptions on the problem structure, the authors conduct a theoretical analysis of the proposed algorithm. Their analysis shows that, with a specifically designed scheme for thehomotopy parameter, H-SGd enjoys a global linear rate of convergence to a neighborhood of a minimum while maintaining fast and inexpensive iterations. Experimental evaluations confirm the theoretical results and show that the proposed method outperforms standard SGD."
SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning method for the task of shape completion, i.e., estimating the 3D shape of an object from sparse observations. The proposed method is based on the Bayesian approach of (Maeda et al., 2020) whose posterior estimate behaves asymptotically well with respect to the size of contextual dataset, and combine their method with Implicit Geometrical Regularization (IGR) (Gropp et al. 2020). They use IGR as the baseline, and demonstrate its efficacy on two benchmark datasets (ShapeNet and ICL-NUIM), especially when the observations are very sparse."
SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper studies adversarial examples from the channel-wise activation perspective, which is motivated by the observation that the activation magnitudes of adversarial images are higher than that of natural images, and that the channels are activated more uniformly by adversarial attacks than by natural images. The authors propose a method called Channel-wise Activation Suppressing (CAS) to suppress the redundant activation from being activated by the adversarial perturbations. They show that CAS can train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness."
SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper provides a non-asymptotic analysis of the gradient flow dynamics of overparametrized single-hidden layer linear networks, which provides disentangled conditions on initialization that lead to acceleration and generalization. Specifically, it shows that imbalanced initialization ensures acceleration, while orthogonal initialization ensures that trajectories remain close to the generalization manifold. The paper also provides a novel upper-bound on the operator norm distance between the trained network and the min-norm solution."
SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of deep ReLU networks. The authors show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their “shallow” two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures."
SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper studies the overestimation and underestimation issues in deep reinforcement learning (DRL). The authors propose a novel algorithm that can minimize overestimation, avoid the underestimation bias and retain the policy improvement during the whole training process. Specifically, they add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which they provide a novel method to provide theoretical and experimental guarantee for future policy improvement."
SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper proposes a generalization of the inverse reinforcement learning (IRL) problem to a stochastic expectation optimization problem, where the goal is to recover a probability distribution over the reward function from expert demonstrations. The authors propose to use the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the probability distribution as the first solution to the SIRL problem. The solution is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem."
SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervisory learning. Under a simple and realistic expansion assumption on the data, the authors show that using input consistency regularization, the self training with input consistency with polynomial margin and Lipschitzness can achieve high accuracy with respect to ground-truth labels. "
SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a hierarchical model for long-term video prediction. The proposed method predicts future frames by first estimating a sequence of semantic structures and then translating the structures to pixels by videoto-video translation. The authors evaluate the method on three challenging datasets involving car driving and human dancing, and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (i.e., thousands frames), setting a new standard of video prediction with orders of magnitude longer prediction time than existing approaches."
SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new framework for graph neural networks (GNNs) that can deal with undirected graph and directed graph in a unified way. The core component of IGNNS is the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS for iterating, and then obtain the high-level representation of graph nodes. The experimental results show that the performance of our method is obviously better than the related methods."
SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a geometric graph (GG) generative adversarial network (GAN), which is a Wasserstein GAN that addresses the above challenges. The proposed method is permutation equivariant and easily scales to generate graphs of tens of thousands of nodes. It strikes a good trade-off between novelty and modeling the distribution statistics, being competitive or surpassing the state-of-the-art methods that are either slower or non-equivariant."
SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a novel method for exploring how neurons within a neural network interact. In particular, they consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y, where X and Y are sets of neurons in different layers. To ensure we obtain a small and non-redundant set of high quality rules, they identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that their rules give clear insight in how networks perceive the world: they identify shared, resp. class-specific traits, compositionality within the network, as well as locality in convolutional layers."
SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies the problem of fixed-dataset policy optimization (FDPO), in which a dataset of transitions from an environment is used to find a policy with high return. The main contribution of this paper is a theoretical justification of the pessimism principle in FDPO, based on a bound that characterizes the suboptimality incurred by an FDLO algorithm. It further shows how this bound may be used to derive principled algorithms. "
SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a memory-efficient method to train Neural ODEs. The proposed method is based on the asynchronous leapfrog (ALF) solver, which has a constant memory cost w.r.t number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). Extensive experiments are conducted to validate the effectiveness of the proposed method."
SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provides an in-depth analysis that assesses the ability of each model to (1) fit the training distribution and perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) to generate images from conditionings that are composed of unseen object combinations. The authors claim that the methods are able to generate recognizable scenes, and exploit compositionality to generalise to unseen conditionsings with seen objects. However, all methods suffer from noticeable image quality degradation when asked to generate image from unseen conditions. Through the analysis, the authors identify the advantages of different pipeline components."
SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper studies the expressive power of Graph-Augmented Multi-Layer Perceptrons (GA-MLPs) and Graph Neural Networks (GNNs) from the perspective of graph isomorphism testing and community detection. The paper shows that GA-MLP can distinguish almost all non-isomorphic graphs, just like the WL test and GNNs. However, it also shows that there is an exponential-in-depth gap in expressive power between GNN and GAMLPs in terms of the equivalence classes on rooted graphs that they induce. In addition, the paper also shows via community detection experiments that GNN can be limited by their choice of operator family, whereas GNN has higher flexibility in learning."
SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes an actor-learner distillation (ALD) method for distributed reinforcement learning (RL) that transfers learning progress from a large capacity learner model to a small capacity actor model. The main idea is to use transformer models as the learner and LSTMs as the actor. The proposed ALD method is evaluated in the context of partially-observable environments, where transformer models have had large improvements over LSTM recently, at the cost of significantly higher computational complexity. The authors demonstrate that ALD can recover the sample-efficiency gains of the transformer model while maintaining the fast inference and reduced total training time."
SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,"This paper proposes a Universal Representation Transformer (URT) layer for multi-domain few-shot image classification. The URT layer learns to leverage universal features by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, the authors show that URT sets a new state-of-the-art result on Meta-Dataset and achieves top-performance on the highest number of data sources compared to competing methods."
SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes an algorithm for the Unsupervised Progressive Learning (UPL) problem, where the number of classes is gradually increasing over time, and the goal is to learn salient representations from a non-stationary stream of unlabeled data that can be associated with specific classes, thus enabling classification tasks. The authors propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM), which learns based on a combination of clustering, novelty detection, forgetting outliers, and storing prototypical representations rather than specific examples. The proposed algorithm is evaluated on synthetic and real-world datasets."
SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between centralized and decentralized training of deep learning models. The authors identify the changing consensus distance between devices as a key parameter to explain the gap between decentralized and centralized training. They empirically investigate what is the desirable level of consensus distance during different phases of training, in order to ensure high generalization performance. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. They highlight the intimate interplay between network topology and learning rate at different training phases and discuss the implications for communication efficient training schemes."
SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,"This paper proposes a method for learning a similarity metric between sequences using siamese recurrent neural networks (RNNs). The proposed method is based on the idea of dynamical system theory, which is motivated by the analogy between synchronized trajectories produced by dynamical systems and the distance between similar sequences processed by RNNs. The authors propose a new neural network model that implements this coupling with a new gate integrated into the classical Gated Recurrent Unit architecture. This model is able to simultaneously learn the similarity metric and the synchronization of unaligned multi-variate sequences in a weakly supervised way. The experiments show that introducing such a coupling improves the performance of the proposed method."
SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of distributed data parallelism (codistillation) on the performance of neural network models trained in a distributed setting. The authors propose to use a distillation-like loss to encourage the models to represent the same function through an auxiliary loss. They show that even at moderate batch sizes, models trained with codistillation can perform as well as those trained with synchronous data-parallel methods, despite using a much weaker synchronization mechanism. The findings hold across a range of batch sizes and learning rate schedules."
SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. The authors claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters, SGD iterates will converge to a heavy-tailed stationary distribution. They rigorously prove this claim in the setting of quadratic optimization. They show that even in a simple linear regression problem with independent and identically distributed Gaussian data, the iterates can be heavy-tail with infinite variance. They further characterize the behavior of the tails with respect to the algorithm parameter, the dimension, and curvature. They finally support their theory with experiments conducted on both synthetic data and fully connected neural networks."
SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the impact of bandpass filtering on the performance of GCNs for community detection. The authors empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection, and it is possible to obtain state-of-the-art accuracy with simple classifiers that rely only on a few low frequencies. This is surprising because contrary to GCNs, no cascade of filtering along the graph structure is involved and it indicates that the important spectral components for the supervised community detection task are essentially in the lower frequency domain."
SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper studies the problem of semi-supervised graph representation learning, where the graph structure is not available. The authors propose a method called SLAPS, which learns the adjacency matrix and GNN parameters with self-supervision. The proposed method is based on the observation that the edges between pairs of nodes that are far from labeled nodes receive insufficient supervision, which results in learning poor structures away from labelled nodes and hence poor generalization. To solve this problem, the authors propose to use a multi-task learning framework, in which they supplement the classification task with a self supervised task. They show that the proposed method can outperform existing approaches on various benchmarks."
SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel method for learning in an incremental learning setting, where the exposure identities are unknown. The novelty detection method leverages network confusion caused by training incoming data as a new class. The proposed method is evaluated on a set of image classification benchmarks. The results show that incorporating a class-imbalance during the detection method substantially enhances performance."
SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a framework for learning to interpret natural language constraints for safe reinforcement learning. Specifically, the authors introduce a new multi-task benchmark, called HazardWorld, that requires an agent to optimize reward while not violating constraints specified in free-form text. They then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Their model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. They show that their method achieves higher rewards (up to 11x) and fewer constraint violations compared to existing approaches."
SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a class-agnostic few-shot semantic edge detection network (CAFENet), which aims to localize boundaries of novel categories using only a few labeled samples. The proposed method employs a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. The authors also propose a new regularization method based on multi-split matching. Two new datasets, FSE-1000 and SBD-5, are constructed to evaluate the performance of the proposed method."
SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method, Causal Screening, for generating causal explanations for graph neural networks (GNNs). The proposed method incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, it can be used to generate faithful and concise explanations for any GNN model. Experiments on three graph classification datasets show that the proposed method achieves significant improvements over state-of-the-art approaches w.r.t. predictive accuracy, contrastivity, and safely passes sanity checks."
SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper proposes a new measure of model simplicity, i.e., the fraction of parameters that can be kept while pruning without adversely affecting the training loss. The authors show that this measure is highly predictive of a model’s generalization performance across a large set of convolutional networks trained on CIFAR-10. They also show the mutual information between the predictions of their new measure and strong existing measures based on models’ margin, flatness of minima and optimization speed."
SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,"This paper proposes a method to learn discrete representations for long-horizon planning from video data in an unsupervised fashion via a mutual information maximization objective. DORP plans a sequence of abstract states for a low-level model-predictive controller to follow. The proposed method is evaluated on a set of tasks and is shown to be able to learn to solve tasks end-to-end, but struggles to solve long-term tasks."
SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit level sparsity. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer to induce all-zero bits across a group of weight elements and realize the dynamic precision reduction. The method enables the exploration of the full mixed precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. "
SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the robustness of binary neural networks (BNNs) against adversarial attacks. The authors claim that BNNs suffer from gradient vanishing issues and show a fake sense of robustness. To address this issue, the authors propose a simple temperature scaling approach to mitigate this issue while preserving the decision boundary. Experiments on CIFAR-10/100 datasets with multiple network architectures demonstrate the effectiveness of the proposed method."
SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel interpretable recurrent neural network (RNN) model, called ProtoryNet, in which it introduces a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, the proposed method makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each of the sentences to the prototypes. Prototype trajectories enable intuitive, fine-grained interpretation of how the model reached to the final prediction, resembling the process of how humans analyze paragraphs. The proposed method not only is more interpretable but also is more accurate."
SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper proposes a hidden Markov recurrent neural network (HMRNN) that is a special case of recurrent neural networks (RNNs). The authors prove that each HMRNN has the same likelihood function as a corresponding discrete-observation HMM. The authors also show that HMRnn parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. Finally, the authors demonstrate how the HMRNets can be combined with other neural networks to improve parameter estimation, using an Alzheimer’s disease dataset."
SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a method for analyzing single-cell RNA-Seq data in the context of neuronal phenotypes. The authors propose a linear transformation of gene expression data that varies highly with only one phenotypic factor and minimally with the others. They further leverage a sparsity-based regularization algorithm, which selects a few genes important to a specific feature or feature combination. They applied this approach to a single- cell RNA-seq dataset of Drosophila T4/T5 neurons. The analysis confirms results obtained by conventional methods but also points to new genes related to the phenotypes and an intriguing hierarchy in the genetic organization of these cells."
SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper proposes a method for generating saliency maps (saliency maps) to explain the behavior of an image classifier. The saliency map is modeled as a random variable and the posterior distribution over it is calculated using variational Bayesian methods. The prior distribution is modeled using a soft-TV Gaussian prior. The likelihood function is designed to measure the distance between the classifier’s predictive probability of an images and that of a perturbed image. The authors show that the approximate posterior is effective in providing uncertainty over the explanation, and also has benefits of providing auxiliary information to experts on how much the explanation is trustworthy."
SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a neural debiasing method for pretrained sentence encoders, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, the authors introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoder, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoder, further enlarging FairFil's application space."
SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a method for certifying neural networks with provable l2-robustness. The proposed method is based on randomized smoothing, which assigns different noise levels to different samples. Specifically, the authors propose a pretrain-to-finetune framework that first pretrains a model and then adjusts the noise levels for higher performance based on the model’s outputs. For certification, they carefully allocate specific robust regions for each test sample. The experimental results on CIFAR-10 and MNIST datasets demonstrate the effectiveness of the proposed method."
SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a variational auto-encoder (VAE) based method for unsupervised data recovery from corrupted data. The authors propose a reduced entropy condition approximate inference method that results in rich posteriors of clean values, which allows the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The proposed method is evaluated on a data recovery task under the common setting of missing values and noise, and shows superior performance to existing variational methods for imputation and de-noising with different real data sets."
SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a method to incorporate multi-hop context information into the self-attention mechanism in graph neural networks (GNNs). Specifically, the authors propose to compute attention between nodes that are not directly connected but provide important network context, enabling long-range interactions at every layer of the GNN. The proposed method, called Multi-hop Attention Graph Neural Network (MAGNA), uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. Experimental results on node classification and knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results."
SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new benchmark for evaluating the performance of NLP models on a variety of tasks. The benchmark is a combination of the SuperGLUE (Wang et al. 2018) and the GPT-3 (Raffel et al., 2019) benchmarks. The main contribution of this paper is the introduction of a new set of 57 tasks that are designed to evaluate the ability of a model to perform well on a diverse set of tasks across a range of domains. The paper also proposes a few-shot question-and-answer (ZPO) and prediction (PPO) task that is designed to measure the model's ability to solve problems in zero-shot/few-shot settings. The results show that the GLUE-3 model is the best-performing model on the new benchmark, but that it is still far from the expert level."
SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,"This paper proposes a pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. The authors construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). They also include masked language modeling (MLM) on several table-and-language datasets to regularize the model’s ability to represent real-world data. When incorporated with strong base semantic parsers, GRAPPA achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parse tasks."
SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper studies the performance of the least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large (p) and numerous (n) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance is shown to converge, as n, p→\infty, to a deterministic limit involving simple (small-dimensional) statistics of the data. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that the proposed method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi- task and transfer learning techniques."
SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper proposes a group equivariant conditional neural process (EquivCNP), a meta-learning method with permutation invariance in a data set as in conventional conditional neural processes (CNPs) and transformation equivariance in data space. The authors give a decomposition theorem for permutation-invariant and group-equivariant maps, which leads them to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. In this paper, they build architecture using Lie group convolutional layers for practical implementation. Experiments are conducted on 1D regression task and zero-shot generalization for an image-completion task."
SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes an approach for offline reinforcement learning for text generation. The authors frame text generation as an offline RL problem with expert demonstrations (i.e., the reference), where the goal is to maximize quality given model-generated histories. They propose an algorithm called GOLD (generation by off-policy learning from demonstrations): an easy-to-optimize algorithm that learns from the demonstrations by importance weighting. The algorithm upweights confident tokens and downweights unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. "
SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. The authors show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, the authors propose an InfoPro-based local learning approach, where a network is split into gradient-isolated modules and trained with local supervision. Extensive empirical results on five datasets (CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to end-to-end training."
SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes a method to improve the expressive power of graph neural networks (GNNs) for node classification and graph classification tasks. The proposed method is based on the idea of diverse sampling, i.e., the representation of the target node is obtained by aggregating the representations of diverse neighborhoods obtained using any GNN model. Experiments are conducted at multi-class node classification tasks on three benchmark datasets and multi-label node classification task on a dataset collected in this paper."
SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the robustness of video models to bit-level network and file corruptions, which can arise from network transmission failures or hardware errors. The authors explore two types of defenses against such corruptions: corruption-agnostic and corruption-aware defenses. They find that corruption-neutral defenses such as adversarial training have limited effectiveness, performing up to 11.3 accuracy points worse than a no-defense baseline. In response, they propose Bit-corruption Augmented Training (BAT), a corruptionaware baseline that exploits knowledge of bit- level corruptions to enforce model invariance to such corruptionions. The experimental results show that BAT recovers up to 7.1 accuracy points on highly-corrupted videos and 8.6 points on clean data."
SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes to use skip-gram embedding to perform implicit tensor factorization on different higher-order representations of time-varying graphs. The proposed method is able to disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. Experiments on time-resolved face-to-face data show that the learned representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction and predicting the outcomes of a process over time."
SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper studies the question of whether self-supervised language modeling applied to mathematical formulas enables logical reasoning. To measure the logical reasoning abilities of language models, the authors formulate several evaluation (downstream) tasks, such as inferring types, suggesting missing assumptions, and completing equalities, for training language models for formal mathematics. They also analyze the models’ ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs. They find that models trained on the skip-tree task show surprisingly strong mathematical reasoning abilities, and outperform models training on standard skip-sequence tasks."
SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper introduces a new type of gradient masking called imbalanced gradients, where the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. The authors formulate a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. They introduce two variants of PGD and MT and conduct extensive evaluations on 12 state-of-the-art defense models and find that their MD attacks can decrease their PGD robustness (evaluated by PGD attack) by more than 9% against the MD attacks."
SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes an incremental graph-to-sequence neural network system for proving semantic equivalence between two programs. The system is trained to produce the proof steps that lead to rewriting one program into another, instead of producing directly the result of this reasoning (e.g., the transformed expression). The paper proposes a method for generating training samples using probabilistic applications of production rules within a formal grammar, and then develops a graph to-sequence (G2S) network that learns to learn and combine rewrite rules to rewrite one programs into another. The paper provides the first implementation of such G2S systems in the OpenNMT framework. "
SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes an end-to-end trainable bidirectional transformer architecture that learns contextualized audio-visual representations of long videos. The authors propose to decompose the Transformer into modality-specific and modality shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. "
SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes an online contextualized few-shot learning (OCL) setting, which extends the standard framework of FSL to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. The authors also propose a new dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. They convert popular FSL approaches into online versions and propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) in the presence of distribution shift in temporal graphs. The authors systematically analyze these issues by incrementally training and evaluating GNNs in a sliding window over temporal graphs, where new vertices, edges, and even classes appear and disappear over time. They experiment with three representative GNN architectures and two scalable GNN techniques, on three new datasets. Their results show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy."
SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"This paper proposes a cross-state self-constraint (CSSC) technique to regularize the representation feature space by comparing representation similarity across different pairs of state. Based on the implicit feedback between state and action from the agent’s experience, this constraint helps reinforce the general feature recognition during the learning process and thus enhance the generalization to unseen environment. The proposed method is tested on the OpenAI ProcGen benchmark and see significant improvement on generalization performance across most of Procgen games."
SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting, where the attacker has no access to model parameters and model predictions. The authors formulate the adversarial attack as an optimization problem to maximize the mis-classification rate over the selected set of nodes, and they carry out formal analysis regarding this optimization problem. The proposed optimization problem is combinatorial and seems hard to solve in its original form. However, the authors mitigate these difficulties by rewriting it and connecting it with influence maximization on a special linear threshold model related to the original graph structure. They show that, under certain distribu-tional assumptions about the GNN, the expected mis-classified rate is submodular with respect to the set of node to perturb. Therefore, by specifying concrete distributions, they are able to derive a group of near black-box attack maximizing the expected class-mislead rate, which can be efficiently optimized by a greedy algorithm thanks to its submodularity. The connection also provides us nice interpretations regarding the attack on GNNs."
SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of learning causal structures from directed acyclic graphs (DAGs). The authors propose to exploit a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to mitigate this problem. They demonstrate how to adapt existing methods for causal structure learning to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low-rank assumption. They also provide empirical evidence for the utility of their low rank adaptations, especially on relatively large and dense graphs."
SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML) that performs co-optimization of the neural architectures, training hyper-parameters and data augmentation policies in an end to end fashion without the need of model retraining. Experiments show that DiffAutoML achieves state-of-the-art results on ImageNet and achieves superior performance compared with multi-stage AutoML algorithms with higher computational efficiency."
SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper extends the Prior Networks and Ensemble Distribution Distillation (EnD) to regression tasks by considering the Normal-Wishart distribution over the parameters of multivariate normal distributions. The authors derive all measures of uncertainty, reverse KL-divergence training objective, and the Ensemble distribution Distillation objective in closed form. Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets, and two monocular depth estimation tasks. They yield performance competitive with ensemble approaches."
SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper proposes a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The framework incorporates MAML into a Bayes online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed framework can effectively prevent catastrophic forgetting.
SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a new type of GNN, called Recursive Neighborhood Neighborhood Pooling Neural Graph Networks (RNP-GNNs), which is a variant of the Message Passing Graph Neural Networks (MPNNs) and Local Relational Pooling (LRP) networks. The authors show that the proposed model can count subgraphs of size $k$ and overcomes a known limitation of low-order GNNs. Moreover, they prove that, in several cases, RNP-Gs can greatly reduce the computational complexity compared to the existing higher-order kGNN and LRP networks."
SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the Lyapunov chaos of two-player zero-sum games in the context of evolutionary game theory (EGT). The authors show that if the agents employ one of the well-known learning algorithms, Multiplicative Weights Update (MWU) or Follow-the-Regularized-Leader (FTRL) algorithms, then the resulting games are chaotic almost everywhere in the cumulative payoff space. The authors derive the characterizations by extending the volume-expansion argument of Cheung & Piliouras (2019; 2020) via the canonical game decomposition."
SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"This paper proposes a new class of adaptive algorithms, called AMX, which is based on the idea of marginal regret bound minimization. Theoretical results show that AMX can converge with a regret bound of size O(\sqrt{T}(T), where T is the number of gradients. In the worst case, AMX is shown to converge at least as fast as AMSGrad and AdaGrad under the same assumptions. Empirical results are also provided to show the superiority of AMX. "
SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes a new framework for policy gradient style reinforcement learning (RL) algorithms with mean-variance control. The framework is based on the expected quadratic utility maximization (EQUM), which is a common objective of risk management in finance and economics. The proposed EQUM has several interpretations, such as reward-constrained variance minimization and regularization, as well as agent utility maximisation. In experiments, the authors demonstrate the effectiveness of the EQUM in benchmark setting of RL and financial data."
SP:bf9d66f713b6502d274143c6273b2d071a0c045e,This paper proposes a framework for multi-task learning based on implicit differentiation. The main idea is to learn a network that combines all losses into a single coherent objective function. This network can learn nonlinear interactions between tasks. The proposed method is evaluated on image segmentation and learning with attributes in the low-data regime.
SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,This paper proposes a new measure of epistemic uncertainty for neural machine translation (NMT) models. The measure is based on the length of the input sentence and the number of discrete random variables in the output sentence. The proposed measure is used to detect out-of-distribution sentences in NMT using the Bayesian Deep Learning (BDL) equivalent of Transformer models. It is shown that the proposed measure can identify when a sentence is given to the model as German instead of English.
SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the performance of pruning neural networks at initialization. It compares SNIP (Lee et al., 2019), GraSP (Wang et al. 2020), SynFlow (Tanaka et al, 2020), and magnitude pruning (Renda et al 2020) and shows that random pruning within each layer or sampling new initial values preserves or improves accuracy. It also shows that the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights."
SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper proposes FedLearning, a federated learning protocol that can simultaneously defend against both a semi-honest server and Byzantine malicious clients. The main challenge stems from the incompatibility between secure aggregation and robust mean estimator. To solve this issue, the authors propose to split the clients into shards, securely aggregate each shard’s updates and run FilterL2 on the updates from different shards. The evaluation shows that FEDLearning consistently achieves optimal or close-to-optimal performance under three attacks among five robust FL protocols."
SP:9f89ff90b203d86a569e3d5148546942f5bf2093,This paper proposes a benchmark for offline black-box model-based optimization (MBO) problems. The benchmark consists of a set of MBO tasks that can be used to evaluate the performance of a number of existing offline MBO methods and baselines. The authors also provide reference implementations of these methods for each of the proposed tasks. The paper also provides an empirical evaluation of the effectiveness of the baselines on the proposed benchmarks. 
SP:073958946c266bf760d1ad66bd39bc28a24c8521,"This paper proposes a new ELBO formulation for multimodal data that combines the benefits of Product-of-Experts and MMVAE as special cases and combines their benefits without compromises. The proposed method models the joint posterior approximation as a mixture of experts, which encompasses the previous models, and the proposed model approximates the posterior for all subsets of modalities, an advantage that is validated empirically in Section 4."
SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a new Bayesian Optimization (BO) method that leverages the prior knowledge of domain experts to improve the performance of BO. The prior knowledge is in the form of priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). The proposed method combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. The authors show that the proposed method is around 12x faster than state-of-the-art methods without user priors and 10,000x better than random search on a common suite of benchmarks."
SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper proposes a binary weight normalization technique to reduce the computational cost of deep generative models. The authors show that binary weights and activations can reduce the model size and speed up the execution time while maintaining the performance. The proposed method is evaluated on two models, namely, the ResNet VAE and Flow++. The experiments show that the proposed method can achieve better performance than batch normalization."
SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning to address the gap between robustness against natural, out-of-distribution shifts in the data distribution in a general context. The main idea is to obtain models of natural variation, which vary the data over a range of natural conditions, and then develop three novel model based robust training algorithms that improve the robustness of DL with respect to natural variation. Extensive experiments show that the proposed algorithms outperform classifiers trained via ERM, adversarial training, and domain adaptation techniques."
SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the training of CNNs with ReLU activations and introduces exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, number of neurons, and data dimension. The authors first prove that two-layer CNNs can be globally optimized via an `2 norm regularized convex program. They then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an `1 norm regularization. They also extend these results to three-layer convolutional neural networks with two ReLU layers. Furthermore, they present extensions of their approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers."
SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,This paper proposes a method for interpretable learning from demonstration (LfD) that uses a probabilistic generative model with a high-capacity neural network. The latent variables are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. The method is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot – that of dabbing liquids with a sponge and moving it along a surface.
SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes to use Variational Information Bottleneck (VIB) to suppress irrelevant features in pretrained sentence representations when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting. The authors also show that the VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. The paper is well-written and easy to follow."
SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method to recover the 3D shape of an object from a single 2D image using an off-the-shelf pre-trained 2D generative adversarial network (GAN). The main idea is to generate pseudo-images of different viewpoints and lighting conditions in the GAN manifold, which are then used to guide the reconstruction of the original image. The method is evaluated on the task of shape reconstruction and face rotation. The results show that the method is able to recover 3D shapes with high precision."
SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a new long-tailed classifier called RIDE, which reduces the model variance with multiple experts, reduces model bias with a distribution-aware diversity loss, and reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is also a universal framework that can be applied to various re-balancing or re-weighting methods for consistent performance gains."
SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper studies the effectiveness of different pruning criteria for layer-wise and global pruning of convolutional neural networks. The authors claim that there are two blind spots: (1) Similarity: There are some strong similarities among several primary pruning criterion that are widely cited and compared. According to these criteria, the ranks of filters’ Importance Score are almost identical, resulting in similar pruned structures. (2) Applicability: The filters' Importance scores are too close to distinguish the network redundancy well. Based on these observations, the authors propose a Convolutional Weight Distribution Assumption (CWDA) that the well-trained convolutionsal filters in each layer approximately follow a Gaussian-alike distribution. This assumption has been verified through systematic and extensive statistical tests."
SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for programming language that considers the inherent structure of code. Specifically, the authors use data flow in the pre-training stage, which is a semantic-level structure that encodes the relation of “where-the-value-comes-from” between variables. The proposed model is evaluated on four tasks, including code search, clone detection, code translation, and code refinement. Results show that the proposed model can improve the performance on the downstream tasks."
SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method to improve the accuracy of regression models that are trained using a skewed dataset. The method forces the regression outputs to follow the true distribution; the forcing algorithm regularizes the regression results while keeping the information of the training data. The authors created skewed datasets by selecting data that exceeded a certain threshold from four real-world datasets (pLogP, Diamond, House, Elevators), then evaluated the proposed approach using these skewed datasets. The proposed approach reduced the root mean squared error (RMSE) of the regression model derived using each of the datasets compared to regression model that had been trained using only the skewed data."
SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of out-of-distribution generalization, i.e., learning compositional representations from the training distribution. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, the authors propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than a 20% absolute increase in various experiments compared with baselines."
SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper studies the problem of poisoning attacks on deep reinforcement learning (RL) systems. The authors propose a poisoning method that works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. They propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for any RL agent without any prior knowledge of the MDP. They also propose a new metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple RL agents and multiple environments show that the poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy."
SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper proposes a new checkpointing method for deep learning models. The proposed method, Dynamic Tensor Rematerialization (DTR), is a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. They prove that DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N})$ memory budget with only $O(N)$ tensor operations, which is within a constant factor of optimal and matches the offline bound of the static checkpointing technique of Chen et al. (Chen et al., 2016)."
SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collects new manually annotated evaluation sets for this task. It also introduces a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach – it achieves an average F1 of around 0.6 across all the benchmark datasets."
SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a neural architecture search (NAS) algorithm to automatically design class-aware generators for conditional generative adversarial networks (cGANs). The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data for each class generator. The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability."
SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper introduces a regularization framework for estimating the causal effect of treatments from observational data. The authors formalize unconfoundedness as an orthogonality constraint, which is used during the estimation of the model parameters to ensure that the outcomes are orthogonal to the treatment assignment. They provide theoretical guarantees that this yields an asymptotically normal estimator for the average causal effect. Based on this framework, the authors develop a deep learning method called DONUT, which learns outcomes that are orthonormal to the underlying treatment assignment, and demonstrate that DONUT outperforms the state-of-the-art substantially."
SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper investigates the expressive power of BatchNorm feature normalization. The authors train only the $\gamma$ and $\beta$ parameters and freeze all weights at their random initializations. They show that training only these parameters leads to surprisingly high performance considering the significant limitations that this style of training imposes. For example, sufficiently deep ResNets reach 82% (CIFAR-10) and 32% (ImageNet, top-5) accuracy in this configuration."
SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a test-time adaptation method based on test entropy minimization (tent1), which aims to optimize the model for confidence as measured by the entropy of its predictions. Tent estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent can achieve less error given only the target data."
SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate the uncertainty in RNNs via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, the proposed method offers several advantages in different settings. The proposed method can learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the explorationexploitation trade-off in reinforcement learning."
SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper proposes a stochastic differential equation principled residual perturbation for privacy-preserving DL, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that the proposed method guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, it outperforms DPSGD in both membership privacy protection and maintaining the DL models’ utility."
SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes an extension of the PoWER-BERT algorithm, which gradually decreases the length of a sequence as it is passed through layers. The proposed extension enables to train a large-scale transformer, called Length-Adaptive Transformer, once and use it for various inference scenarios without re-training it. To do so, the authors train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length at each layer. They then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. The experiments on SQuAD, MNLI-m, and SST-2 show the effectiveness of the proposed method."
SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressive power of neighborhood aggregation in graph neural networks (GNNs). The authors propose two GNN layers: ExpandingConv and CombConv, and evaluate them on general graph classification and graph regression tasks. Theoretically, the authors analyze the requirements of the aggregation coefficient matrix for building more powerful aggregators and even injective aggregators. They also show the necessity of applying nonlinear units ahead of aggregation, which is different from most aggregation-based GNNs."
SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,This paper proposes a method to evaluate the disentanglement of generative models by measuring the topological similarity of conditional submanifolds in the learned representation. The method is based on topology and density analysis of samples conditioned on disentangled latent variables. Experiments on CIFAR-10/100 and CelebA show that the proposed method performs similarly to existing methods.
SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make training examples ""unlearnable"" for deep neural networks (DNNs). This is achieved by adding imperceptible ""error-minimizing noise"" to the training data, which is intentionally designed to reduce the error of one or more of the training examples close to zero, which can trick the model into believing there is ""nothing to learn from these example(s)"". The authors empirically verify the effectiveness of the proposed method in both sample-wise and class-wise settings."
SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper presents Nondeterministic MuZero (NDMZ), an extension of MuZero for nondeterministic, two-player, zero-sum games of perfect information. NDMZ formalizes chance as a player in the game and incorporates the chance player into the MuZero network architecture and tree search. Experiments are conducted on the Nannon, a simplified variant of backgammon."
SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper introduces Hindsight off-policy options (HO2), a data-efficient option learning algorithm for hierarchical reinforcement learning (HRL). The algorithm learns the policy via critic-weighted maximum-likelihood (similar to Abdolmaleki et al. (2018b); Wulfmeier et al (2020)) and combines these with an efficient dynamic programming procedure to infer option probabilities along trajectories and update all policy parts via backpropagation through the inference graph (conceptually related to (Rabiner, 1989; Shiarlis et al, 2018; Smith et al., 2018)). Intuitively, the approach can be understood as inferring option and action probabilities for off policy trajectories in hindsight and maximizing the likelihood of good actions and options by back-propagating through an inference procedure. To stabilize updates, HO2 uses adaptive trust-region constraints, demonstrating the importance of robust policy optimization for HRL in line with recent work (Zhang & Whiteson, 2019)."
SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper studies the problem of synthesizable molecule generation using reinforcement learning. The authors propose a novel functional form of the Bellman equation, called max-Bellman, to optimize for the maximum reward in an episode. They also introduce the corresponding evaluation and optimality operators, and prove the convergence of Q-learning with the max-bellman formulation. Finally, they demonstrate the effectiveness of the proposed method on a toy task and a real-world drug discovery task."
SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes a method for few-shot learning, where new features are added to the dataset with few or no associated observations. The proposed method, Contextual HyperNetwork (CHN), is an auxiliary model that generates parameters for extending the base model to a new feature by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. The CHN is applied to augment a partial variational autoencoder (P-VAE), a deep generative model which can impute the values of missing features in sparsely-observed data."
SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method for Bayesian deep learning that performs inference over only a small subset of the model parameters while keeping all others as point estimates. This enables the method to use expressive posterior approximations that would otherwise be intractable for the full model. In particular, the proposed method first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The proposed method is evaluated on CIFAR-10 and ImageNet and compared with point-estimated and other Bayesian methods."
SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a new approach for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the authors use a model of the environment to obtain embedding for state and action and present a generic architecture that uses these to learn a policy. In this way, the embedded representations obtained via the approach enable better generalization over both states and action by capturing similarities in the embedding spaces. Evaluations of the approach on several gaming, robotic control, and recommender systems show it significantly outperforms state-of-the-art models."
SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a generative model for unsupervised representation learning that learns to produce diverse and useful views for contrastive learning. The model is trained adversarially to create views that increase the contrastive loss of the encoder network. The views are generated by generating and then adding an $\ell_p$-bounded perturbation to the input, and are trained with respect to the main encoder. The proposed method achieves comparable transfer accuracy to the well-tuned SimCLR augmentations on CIFAR-10, and outperforms the state-of-the-art contrastive methods on speech recordings and wearable sensor data."
SP:ef7735be9423ad53059505c170e75201ca134573,This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. The authors demonstrate how different existing detection approaches fail to detect certain types of outliers. They utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outlier.
SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model that is able to match the performance of autoregressive models on CIFAR-10, ImageNet, and FFHQ datasets. The authors show that VAEs with more stochastic layers are able to generate samples faster and achieve higher likelihoods than the PixelCNN on these datasets. This is achieved by scaling the number of layers of the VAE to greater depth than previously explored. The paper also provides a theoretical justification for why greater depth can improve the performance."
SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a new contrastive representation learning method based on the noise-contrastive estimation (NCE) framework. The authors propose to sample the negative samples conditionally in a ""ring"" around the positive samples, which they call Conditional-NCE (CNCE). They show that CNCE is a looser bound on the mutual information than NCE and has a lower variance. They also show that the proposed method can be applied to existing contrastive learning methods such as IR, CMC, and MoCo and improve their performance by 2-5% absolute points on four standard image benchmarks."
SP:613a0e2d8cbe703f37c182553801be7537333f64,"This paper studies the problem of large-batch data leakage in federated learning (FL). The authors propose an attack to recover batch data from the shared aggregated gradients. The attack is based on the idea of data index alignment and internal representation alignment in FL, which can significantly improve the recovery performance. Experimental results on vertical and horizontal FL settings have validated the effectiveness of the proposed attack. "
SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for inferring multi-agent interactions from trajectories. The model is based on a variational auto-encoder, where the latent code represents the interaction graph and the reconstruction is done using graph neural networks. The proposed model is evaluated on simulated physical systems and real-world motion capture data."
SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes an inductive collaborative filtering framework that learns a hidden relational graph among users from the rating matrix. The key advantage of the proposed model is the capability for inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate the model achieves state-of-the-art performance for inductive learning on several matrix completion benchmarks, provides very close performance to transductive models when given many training ratings and exceeds them significantly on cold-start users."
SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage approach for learning disentangled representations of images. First, the disentanglement factors are learned using a pre-trained representation learning method (beta-TCVAE). Then, a generative model is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangling factors. The proposed approach can be applied to a variety of model classes including likelihood-based models such as variational autoencoders, generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. The authors demonstrate that the proposed approach has much higher reconstruction quality than current state-of-the-art methods with equivalent disentaglement performance across multiple standard benchmarks."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations learned by maximizing mutual information (MI) between random variables in the context of deep reinforcement learning (RL). In particular, the authors focus on representations obtained by maximizing the mutual information between states, actions, and rewards at different time-steps. The authors provide a theoretical analysis of two commonly used objectives for representation learning based on mutual information maximization, and show that one of them is insufficient for the general class of MDPs, in the most general case, and prove that another typical objective is sufficient. The theoretical results are corroborated with empirical experiments on a simulated game environment. "
SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output neural network training problem. In particular, the authors show that the non-convex neural networks training problem is equivalent to a finite-dimensional convex copositive program. The authors also provide the first algorithms for provably finding the global minimum of the vector output neural network learning problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. In the case of convolutional architectures, the computational complexity is exponential in only the filter size and polynomorphic in all other parameters."
SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The proposed method, Language-mediated, Object-centric Representation Learning (LORL), builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. LORL learns to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. Experiments show that the proposed method improves the performance of MONet/Slot Attention on two datasets."
SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a method for link prediction based on rule-based reasoning. In particular, the authors propose EM-RBR (embedding and rule based reasoning), which combines the advantages of reasoning based on rules and the state-of-the-art models of embedding. The proposed method is evaluated on FB15k, WN18 and a new dataset FB15K-R, where it achieves better performance compared with previous models."
SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes an approach to model object-oriented (OOP) knowledge, which consists of active modules called object files that maintain the state of a single object and passive external knowledge sources called schemata that prescribe state updates. The authors propose to use attention to determine which object files to update, the selection of Schemata, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type."
SP:42a3c0453ab136537b5944a577d63412f3c22560,This paper proposes a neural module network (NMN) approach for visual question answering (VQA) and video-grounded dialogues (VGL) tasks. The approach is based on neural module networks (NMNs) that decompose the question into sub-sequences called program and assemble a network of neural operations on spatial and temporal-level visual features. The proposed approach is evaluated on VQA and VGL tasks and shows promising results.
SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variants of the policy-space response oracles (PSRO) framework for learning policies in multi-agent reinforcement learning (RL). In particular, the authors propose mixed-oracles and mixed-opponents, which modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy. The first method, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy, while the second method, mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy’S action-value estimates, instead of their policies. The authors demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game."
SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper presents a non-attentive variant of the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. The authors also propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training. "
SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method for stereo bird’s eye view (BEV) layout estimation from a pair of stereo images. The main idea is to use inverse perspective mapping (IPM) to map the input images and their features to the bird's eye view. The proposed method is evaluated on KITTI (Geiger et al. 2013) and CARLA (Dosovitskiy et al., 2017) datasets and shows state-of-the-art performance."
SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,This paper proposes a relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup.
SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method for model explanation based on integrated gradients (IG) and Satisfiability Modulo Theory (SMT) solvers. The idea is to identify minimal regions in an input that are most relevant for a neural network’s prediction. The approach uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows the technique to scale to large networks. The proposed method is evaluated on three datasets MNIST, ImageNet, and Beer Reviews."
SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a method for 3D pose estimation that is robust to partial occlusion and unseen pose. The proposed method is based on a generative model of neural feature activations at each vertex on a dense 3D mesh and differentiable rendering to estimate the 3D object pose by minimizing the reconstruction error between NeMo and the feature representation of the target image. To avoid local optima in the reconstruction loss, the feature extractor is trained to maximize the distance between the individual feature representations on the mesh using contrastive learning. Experiments on occluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to the occlusions and unseen poses compared to standard deep networks."
SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes an approach for non-inherent feature compatible learning (FCL), i.e., feature-compatible learning without inheriting old classifier and training data. The authors propose a unified framework for FCL, and extend it to handle the case where the old model is a black-box. Specifically, they learn a simple pseudo classifier in lieu of the old models and further enhance it with a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 data proved the efficacy of the proposed approach."
SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the generalization performance of deep neural networks (DNNs) for hyper-parameter optimization. The authors propose to use accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200\�20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm, as one of the hyperparameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). The results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalisation error is not always consistent across phases of training process."
SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes a method to retrieve entities from large knowledge bases (KBs) by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. The authors claim that this approach enables sub-linear search using modern maximum-inner-product-search libraries (Johnson et al., 2019) and hence supports retrieving from large entity databases. The method is evaluated on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint."
SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The routing requests are supplied adversarially. For each edge e in the selected path, the algorithm incurs a cost ce = fe(x t e) + \eta t e, where x t e is the flow on edge e at time t, fe is the congestion function, and \eta is a noise sample drawn from an unknown distribution. The algorithm observes ce, and can use this observation in future routing decisions. The authors present an algorithm with cumulative regret Á(|E|t), where the regret on each time step is defined as the difference between the total cost incurred by our chosen path and the minimum cost among all valid paths. "
SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes a new masking strategy based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. The authors show that such uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masksing, entity/phrase masking and random-span masking. Specifically, the authors show experimentally that the proposed method reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training."
SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the amortized variational inference for generative models. The authors show that the ELBO objective forces partially-conditioned amortised variational posteriors to approximate products of smoothing posteriors instead of the true posteriors. They show that this leads to the learned generative model is compromised. They demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. "
SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF), and obtain optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only $O(|D|)$ memory and $O(\|D^2$)$ time. Then, beyond the generalisation bounds in expectation that demonstrate the average information for multiple trails, the authors derive the learning rates in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance of DKRR-RD and validate the theoretical bounds via numerical experiments."
SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address the drawbacks of RandomNAS. Specifically, it introduces a proxy search space (PS) that is only a small subset of the global search space to improve RandomNAS’s search efficiency while at the same time keeping a good correlation for the top-performing architectures. Experiments on the NASBench-201 benchmark show that the proposed approach can achieve near-optimal NAS performance and surpass all existing state-of-the-art."
SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes a method to combine imitation learning and meta-RL to enable an agent to quickly adapt to new tasks at test time. The proposed method is called Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). The method is able to interpolate from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, under sparse rewards. The authors show how PERIL is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties."
SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the generalization properties of overparameterized convolutional neural networks (CNNs) trained with stochastic gradient descent (SGD) in the setting of orthogonal patches. The authors show that the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set. They call this phenomenon Pattern Statistics Inductive Bias (PSI) and empirically verify it in a large number of instances. They also prove that if a learning algorithm satisfies PSI then its sample complexity is O(d log(d)) where d is the filter dimension. In contrast, they show a VC dimension lower bound which is exponential in d."
SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper studies the problem of document classification in the presence of latent structure in the form of a topic model. The authors show that contrastive learning is able to recover a representation of documents that reveals their underlying topic posterior information to linear models. They apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.
SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive generative framework for multimodal variational autoencoder (VAE) models. The key idea is to use contrastive learning to distinguish between ""related"" and ""unrelated"" multi-modal data. The proposed method is evaluated on CIFAR-10 and CelebA datasets. The results show that the proposed method outperforms the baselines."
SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes an energy-based prior for variational autoencoders (VAEs) that aims to improve the generative quality of VAEs. Specifically, the authors propose a reweighting factor, which is the product of a base prior distribution and a reweighed factor, to bring the base prior closer to the aggregate posterior. The reweighing factor is estimated using noise contrastive estimation. The authors also propose a hierarchical VAE variant that generalizes the reweighted prior to hierarchical VAEs with many latent variable groups. Experiments are conducted on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Results show that the proposed reweigtings improve the quality of the generated images."
SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper studies the problem of regularized inverse reinforcement learning (IRL) in the setting of strongly convex regularized Markov decision processes (MDPs). In particular, the authors propose tractable solutions for regularized IRL problems that can be derived from policy regularization and adversarial IRL. Theoretical results show that the proposed solutions are tractable for Tsallis entropy regularization with multi-variate Gaussian policies in discrete control problems and continuous control problems. The authors also propose a practical sample-based method for policy imitation and reward learning in regularized MDPs, which generalizes adversarial inverse RL (AIRL, Fu et al. 2018) and generalizes the minimization perspective (Ke et al., 2019; Ghasemour et al, 2019; Dadashi et al 2020) to the regularized setting. Empirical results demonstrate the effectiveness of the proposed method."
SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many to one translation."
SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,This paper proposes a Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification. The authors split the data into uncertain and certain samples based on small loss criteria. They impose geometric constraints on the uncertain samples by normalizing them into the Wasserstein ball centered on certain samples. Experimental results demonstrate that our WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets.
SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a method for quantifying the uncertainty of convolutional neural networks (CNNs) with respect to the true label with a user-specified probability, such as 90%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. The method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes. The proposed method outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scale."
SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper proposes a method to compute Wasserstein-2 barycenters of continuous distributions based on a novel regularized dual formulation where the convex potentials are parameterized by input convex neural networks (Amos et al., 2017). The algorithm is straightforward without introducing bias or requiring minimax optimization. This is made possible by combining a new congruence regularizing term combined with cycle-consistency regularization (Korotin et al. 2019a). The paper provides theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach in low dimensional qualitative scenarios and high-dimensional quantitative experiments."
SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the problem of binary classification on two disjoint manifolds of the unit sphere, where the classifier is a deep fully-connected ReLU network of depth L and width n trained on N i.i.d. samples from a distribution supported on one of the two manifolds. The main result of the paper is that when the network depth L is large relative to certain geometric and statistical properties of the data, the network width n grows as a sufficiently large polynomial in L, and the number of samples from the manifolds is polynomials in L. The paper also provides a nonasymptotic framework for establishing generalization of networks trained in the NTK regime. "
SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines, while also being able to leverage off-policy data. The proposed approach, which the authors refer to as advantage-weighted regression (AWR), consists of two standard learning steps: one to regress onto target values for a value function, and another to regress on weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code. The authors also provide a theoretical motivation for AWR and analyze its properties when incorporating off-Policy data from experience replay. Experimental results show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms."
SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes a method for deep quantization of neural networks with heterogeneous bitwidths. The method is based on adding a sinusoidal regularization term to the loss function, which is a parametrized version of the sinusoid regularizer (Sin2). This regularizer pushes the weights to the quantization levels while simultaneously learning the bitwidth of each layer separately. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of quantized neural networks. "
SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"This paper proposes a novel data augmentation method for neural machine translation (NMT) by using only the original training data without extra data. More accurately, it randomly replaces words or mixup with their aligned alternatives in another language when training NMT models. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models."
SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,This paper proposes a real-time contribution measurement method for federated learning. The method defines the impact of each agent and comprehensively considers the current round and the previous round to obtain the contribution rate. The paper conducts pseudo-distributed training and an experiment on the Penn Treebank dataset. The experimental results show that the proposed method is more sensitive to both data quantity and data quality.
SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"This paper studies the problem of learning Bayesian networks where a fraction of the samples are adversarially corrupted. The authors propose a nearly-linear time algorithm for this problem with a dimension-independent error guarantee. The algorithm and analysis are simpler than those in previous work. They achieve this by establishing a direct connection between robust learning of Bayesian network and robust mean estimation. As a subroutine in the algorithm, they develop an algorithm whose runtime is nearly linear in the number of nonzeros in the input samples."
SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-horizon planning in model-based reinforcement learning (RL). The method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. The proposed method (LatCo) is evaluated on challenging visual control tasks with sparse rewards and long-term goals."
SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model to explain the effect of ""tempered"" or ""cold"" posterior likelihoods for Bayesian neural networks (BNNs) for image classification. The authors argue that BNNs use the wrong likelihood for standard image classification datasets such as CIFAR-10. The generative models assumes that a dataset is included in the dataset only if there is unanimous agreement on the class among multiple labellers. This model naturally multiplies the effects of each datapoint, and hence gives posteriors that closely match tempered or cold posteriors. The paper also shows that toy data drawn from the model can give rise to optimal temperatures being smaller than 1."
SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the speed-quality trade-off between autoregressive and non-autoregressive machine translation (NAR) models. The authors argue that the speed disadvantage of NAR models has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. They provide extensive experiments to show that given a sufficiently deep encoder and shallow decoder, a single-layer auto-regressive decoder can substantially outperform strong NAR baselines with comparable inference speed."
SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper investigates epoch-wise double descent, i.e., the test error of a DNN also shows double descent as the number of training epoches increases. Specifically, the authors extend the bias-variance analysis to epochs and reveal that the variance also contributes the most to the zero-one loss. Inspired by this result, they propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) test error."
SP:8d8b738c676938952e62a6b2aea42e79518ece06,"This paper studies adversarial robustness in model-agnostic meta-learning (MAML), a bi-leveled learning procedure in which the outer loop optimizes a task-specific meta-initialization of the model parameters and the inner loop learns a task specific meta-model. The authors show that robustifying the meta-update stage is sufficient to make the robustness adapted to the task specific fine-tuning stage even if the latter uses a standard training protocol. Furthermore, the authors investigate how robust regularization can be designed in MAML. They propose a general but easily-optimized robustness-regularized meta learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine tuning."
SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the problem of tuning the step size for quadratic loss in the learning-to-learn (LTL) setting. The authors show that there is a way to design the meta-objective so that the metagradient remain polynomially bounded, but computing the meta gradient directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. They also characterize when it is necessary to compute meta-gradient on a separate validation set instead of the original training set. Finally, they verify their results empirically and show that a similar phenomenon appears even for more complicated learned optimizers parametrized by neural networks."
SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a graph view-consistent learning network (GVCLN) for semi-supervised node classification. The main idea is to use the neighborhood aggregation capability of GVCLN and use dual views to obtain different representations. The two views have different viewing angles, but their observation objects are the same, so their observation representations need to be consistent. To achieve this, two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while the two views are applied to obtain the consistent representation. The pseudo-label loss is designed by using the common high-confidence predictions. Experiments on three citation network datasets of Cora, Citeseer, and PubMed show that the proposed method achieves state-of-the-art performance."
SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper proposes a method for learning the dynamics of physical systems with additional constraints, i.e., the symmetries of the system, in order to identify conserved quantities such as angular momentum, momentum, the splitting into decoupled subsystems, etc. The main idea is to use the cyclic coordinates of the dynamics as input to the Hamiltonian neural network (HNN) to predict the underlying dynamics. The authors show that such coordinates can be searched for automatically with appropriate loss functions which naturally arise from Hamiltonian dynamics. They test their method on standard classical physics systems using synthetic and experimental data where our network identifies the conserved quantity in an unsupervised way and find improved performance on predicting the dynamics."
SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes to combine gradient boosted decision trees (GBDT) and graph neural networks (GNN) for graph representation learning. GBDT deals with heterogeneous features, while GNN accounts for the graph structure. The proposed model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. The experimental results show that the proposed model outperforms GBDT and GNN models on a variety of graphs with tabular features."
SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper studies the role of data splitting in meta-learning. In particular, the authors consider the setting where the number of tasks goes to infinity. They show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. They also show that if the data are generated from linear models (the realizable regime), they show that both the splitting and the non splitting methods converge to the best prior."
SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard confident examples from the noisy training data for learning with noisy labels. The method is built on the memorization effect of deep neural networks that they would first learn simple patterns, i.e., which are defined by these shared by multiple training examples. To extract hard examples that contain non-simple patterns and are entangled with the inaccurately labeled examples, this paper borrows the idea of momentum from physics. Specifically, it alternately update the confident examples and refine the classifier. The extracted confident examples in the previous round can be exploited to learn a better classifier and that the improved classifier will help identify better (and hard) confident examples."
SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the problem of certifiably robustness against data poisoning attacks. In particular, the authors propose two algorithms, namely kNN and rNN, which they claim to be intrinsic majority vote mechanisms for certified robustness. The main idea is to use a majority vote mechanism to predict the label of a testing example, where each voter is a base classifier trained on a subset of the training dataset. The number of corrupted voters depends on the gap between the largest and second largest number of votes. The authors show that the intrinsic certified accuracy of the proposed algorithms is better than the certified accuracy achieved by the state-of-the-art certified defenses."
SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for training graph neural network (GNN) with SGD method. The authors propose a metric that combines both the variance of gradients and compute time for each mini-batch. They theoretically analyze how batch-size influence such a metric and propose the formula to evaluate some rough range of optimal batch size. The empirical results show that in contrast to conventional deep learning models, GNNs benefit from large batch sizes."
SP:30d97322709cd292a49f936c767099f11b0e2913,This paper proposes a method to calibrate the classifier’s inherent confidence indicators and estimate the uncertainty of the calibrated confidence scores using Gaussian Processes. The proposed method is evaluated on UCI datasets. The main limitation of this method is scalability. The authors propose to use the confidence scores of the original classifier and a modified nearest-neighbor classifier. 
SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. The approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. The authors evaluate their method on question answering, obtaining state-of-the-art results."
SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper proposes a method for specifying constraints in formal languages to enable safety-aware reinforcement learning. Constraints are represented as finite automata, which are then used to augment the underlying MDP state and to learn a dense cost function. The proposed method is evaluated on the Safety Gym, MuJoCo, and Atari environments. "
SP:e18cfc1502c4087422d3baf655c244d4f3924a76,This paper proposes a cascading decision tree approach to improve the comprehensibility of binary classification. The idea is to build several smaller decision subtrees and cascade them in sequence. The cascading approach is designed to specifically target explanations for positive classifications. The authors evaluate their algorithm on standard datasets as well as new real-world applications and find that their model shortens the explanation depth by over 40.8% for positive classification compared to the classic decision tree model.
SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of the number of parameters and the width of the network on the performance of neural networks. The authors compare different ways of increasing model width while keeping the parameters constant. They show that for models initialized with a random, static sparsity pattern in the weight tensors, the network width is the determining factor for good performance. The number of weights is secondary, as long as the model achieves high training accuarcy. "
SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text, while the language module generates context-aware initial embedding for entities and relations in the graph. Experiments on several knowledge-aware NLP tasks show that the proposed framework achieves superior performance by effectively leveraging knowledge in language understanding."
SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes an unsupervised learner that can be trained in a fully-parallelizable manner to discover high-quality autoregressive orders in a data-driven way without a domain-specific prior. The learner is a neural network that performs variational inference with the auto-regressive ordering as a latent variable. Since the corresponding variational lower bound is not differentiable, the authors develop a practical algorithm for end-to-end optimization using policy gradients."
SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction scheme to accelerate sampling-based methods for training graph convolutional neural networks (GCNs). The main idea is to decompose node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance (first-order variances) during backward propagation. Theoretical convergence analysis is provided to show that the proposed scheme enjoys an O(1/T) convergence rate. Empirical results are also provided to demonstrate the effectiveness of the proposed method.
SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a method for training deep conditional generative models from a single image. The basic idea is to use a standard adversarial conditional image mapping network to learn to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. At manipulation time, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network. The proposed method is evaluated extensively and displays remarkable results."
SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes a method to detect the maximum common subgraph (MCS) between two input graphs. The main idea is to replace the node selection heuristics used by state-of-the-art MCS solvers with a novel task-specific Deep Q-Network (DQN), which is trained under reinforcement learning to make the best decision at each step in order to quickly find the best solution. The proposed method is evaluated on synthetic and real-world graph datasets."
SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes an end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The network takes as input an unordered set of 3D points sampled from the surface of some object, and outputs a sparse set of corner points linked by line segments. The architecture gradually builds up the model: It starts by encoding the points into feature vectors, it identifies a pool of candidate vertices, then prunes those candidates to a final set of corners and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe. All steps are trainable, and errors can be backpropagated through the entire sequence."
SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies stochastic optimization under weaker assumptions on the distribution of noise than those used in usual analysis. The assumptions are motivated by empirical observations in training neural networks. The authors address this nonstationary behavior of noise by analyzing convergence rates of stochastically gradient methods subject to changing second moment (or variance). When the noise variation is known, the authors show that it is always beneficial to adapt the step-size and exploit the noise variability. The results reveal why adaptive step size methods can outperform SGD."
SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. The authors introduce an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. The method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean, English-to-German and English-Romanian translation tasks."
SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper presents a benchmark for Reinforcement Learning (RL) algorithms with various dimensions of hardness that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. The authors consider and allow control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range. They define a parameterised collection of fast-to-run toy benchmarks in OpenAI Gym by varying these dimensions. Despite their toy nature and low compute requirements, these benchmarks present substantial challenges to current RL algorithms."
SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes an approach to calibrate regression models. The main idea is to use the idea of quantile calibration (Kuleshov et al. 2018) and recast it as entropy estimation, and leverage the new formulation to construct a novel quantile regularizer, which can be used as a blackbox to calibrated any probabilistic regression model. The authors provide a detailed formal analysis of the side-effects of Isotonic Regression when used for regression calibration. They provide empirical results demonstrating that their approach improves calibration for regression models trained on diverse architectures that provide uncertainty estimates, such as Dropout VI, Deep Ensembles."
SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a method for learning representations of spatial environments, perceived through RGB-D and inertial sensors, such as in mobile robots, vehicles or drones. The authors propose a deep state-space model that leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. The combination of variational inference, neural networks and a differentiable raycaster ensures that the model is amenable to end-to-end gradient-based optimisation. The proposed method is evaluated on realistic unmanned aerial vehicle flight data, and achieves the performance of state-of-the-art visual-inertial odometry systems."
SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper presents a method for learning representations of entities and their dynamics from text descriptions. The authors propose a model, EMMA, which uses a multi-modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. The model is end-to-end differentiable and can learn a latent grounding of entities from text to observations using environment rewards as the only source of supervision. The experiments demonstrate that EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards."
SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes an actor-critic approach for training the critic in the actor critic framework. The critic uses a new state-value function approximation that learns the value of the states relative to their mean value rather than the absolute value as in conventional actor critic. The authors also prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms.
SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper studies the effect of depth on the generalization performance of deep neural networks in the overparametrized regime. The authors introduce local and global labels as abstract but simple classification rules, which are used to measure the performance of the network. They show that deeper is better for local labels, while shallower is good for global labels. They also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with a proper initialization and infinitesimal learning rate, and show that the NTK does not correctly capture the depth dependence of the generalisation performance."
SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies representation learning for few-shot learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2(n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. First, they study the case where this common representation is low-dimensional and provide a risk bound of $\tilde{O}(\delta^k n1T + k n2)$ on the target task, where d is the ambient input dimension and k(d) is the dimension of the representation. This result bypasses the $\Omega(1 T)$ barrier under the i.i.d. task assumption, and can capture the desired property that all n1$ samples from source tasks can be pooled together for representation learning. The authors also extend this result to handle a general representation function class and obtain a similar result. Next, they consider the setting when the common representation may be high-dimensional but is capacity-constrained (say in norm); here, they again demonstrate the advantage of representation learning in both high dimensional linear regression and neural networks, and show that representation learning can fully utilize all n2$ samples of source tasks from target tasks."
SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,This paper studies the robustness of deep neural networks (DNNs) to perturbations of semantic features. The authors propose a black-box approach to determine features for which a network is robust or weak. They leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. They evaluate their approach with PCA features. 
SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes an approach to automatically cluster together similar tasks during training for multi-task reinforcement learning. The approach is inspired by the expectation-maximization algorithm, which finds clusters of related tasks and uses these to improve sample complexity. The method is intuitive, simple to implement and orthogonal to other multi- task learning algorithms. The authors show the generality of the approach by evaluating on simple discrete and continuous control tasks, as well as complex bipedal walker tasks and Atari games."
SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised framework for learning representations for non-stationary time series. The proposed method, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. The motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients’ latent states in settings where labeling data is practically impossible. TNC is efficient, easily scalable to high dimensions, and can be used in different time series settings."
SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"This paper proposes a novel approach for learning modular networks for multi-task learning, transfer learning and domain adaptation. The proposed approach is based on the idea that the modules can be invoked repeatedly and allow knowledge transfer to novel tasks by adjusting the order of computation. This allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors show that their method leads to interpretable self-organization of modules in the case of multi- task learning and transfer learning while achieving competitive results on those tasks."
SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of posterior collapse in variational autoencoders (VAEs), which is the phenomenon that the learned latent space becomes uninformative. The authors suggest that the variance parameter regularizes the VAE and affects its smoothness, and that an inappropriate choice of this parameter causes oversmoothness and leads to posterior collapse. This is shown theoretically by analysis on the linear approximated objective function and empirically in general cases. Based on this hypothesis, the authors propose AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameters, and thus avoids oversmoothing the model."
SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes an unsupervised global VAE, which combines a clustering inducing mixture model in the local space and a global Gaussian latent variable to capture global dependencies among observations. The authors show that the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). The model performs domain alignment to find correlations and interpolate between different databases."
SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"This paper proposes a self-supervised representation learning method that uses human interaction and attention cues to learn better representations compared to visual-only representations. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives. The experiments show that the proposed method outperforms the state-of-the-art method MoCo (He et al., 2020) on a variety of target tasks: scene classification, action recognition, depth estimation, dynamics prediction, and walkable surface estimation."
SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the problem of negative pretraining, where a pretrained model obtains a worse generalization performance than a model that is trained from scratch when either are trained on a target task. The authors conceptualize the ingredients of this problem setting and examine the negative preraining effect experimentally by providing three interventions to remove and fix it. First, the learning process, altering the learning rate after pretraining can yield better results than training directly on the target task, on the learning task-level, increasing the discretization of data distribution changes from start to target task instead of “jumping” to the target tasks, and at the model-level."
SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"This paper studies the problem of adversarial robustness, i.e., the robustness against adversarial perturbations of the input images. The authors propose a method to search for safe spots in the input space that are resistant to adversarial attacks. They also propose a new out-of-distribution detection method to detect outliers. The proposed method is evaluated on CIFAR-10 and ImageNet datasets."
SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to model raw point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequence. Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution are used to model the dynamics of the spatial regions along the time dimension. Furthermore, the proposed PSTconvolution is incorporated into a deep network, namely PSTNet, to extract features of point cloud in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet."
SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes an adaptive text-to-speech (TTS) system for high-quality and efficient customization of new voices. The authors propose AdaSpeech to address the two challenges in custom voice: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions which could be very different from source speech data, and 2) the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. To handle different acoustic conditions, the authors model the acoustic information in both utterance and phoneme level. Specifically, they use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning."
SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the gradient flow of training sparse neural networks. The authors propose a new metric, effective gradient flow (EGF), to measure the flow of gradients in sparse networks. They show that the default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks, and show that gradient flow can be improved by reconsidering aspects of the architecture design and the training regime. The paper also shows that initialization is only one piece of the puzzle and a wider view of tailoring optimization to sparse networks yields promising results."
SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"This paper studies the relationship between Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) for the task of node classification in graphs. In particular, the authors analyze the feature/label smoothing where they analyze how the features/labels of one node are spread over its neighbors; and, how much the initial feature of one nodes influences the label of another node. Based on the theoretical analysis, they propose an end-to-end model that unifies GCN and LPA for node classification. The edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. "
SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of classifiers in the setting where the ground truth label generating function (LGF) is fixed. The authors show that the R-Complexity of both the classifier and the generator function spaces (classifier and generator) controls the generalisation gap. They also propose a joint entropy-like measure of complexity (called co-complexity) between the two function spaces, which leads to tighter bounds on the generalizability error in this setting. "
SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a novel post-training quantization (PTQ) method, BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. The main idea is to reconstruct the basic building blocks in neural networks and reconstruct them one-by-one. To further employ the power of quantization, mixed precision technique is incorporated in the framework by approximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks."
SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the effect of dataset properties, rather than network architecture, on the calibration of deep neural networks. Specifically, the authors focus on the problem of class imbalanced datasets and show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. The authors also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which the authors motive via results on network expressivity. "
SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper studies the problem of emergent communication in referential games, in which agents learn to communicate via actuating their joints in a 3D environment. The agents are assumed to have a non-uniform distribution of intents and a common knowledge energy cost. The authors show that under realistic assumptions, the agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice."
SP:5ba686e2eef369fa49b10ba3f41f102740836859,"This paper proposes a meta-modeling approach for uncertainty quantification in sequential regression tasks (SRT). In particular, the authors propose a method that can generate symmetric and asymmetric uncertainty estimates, makes no assumptions about stationarity, and outperforms competitive baselines on both drift and non-dwindling scenarios. The proposed approach is evaluated on the MNIST, CIFAR-10, and Fashion MNIST datasets."
SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,This paper proposes two unbalanced Gromov-Wasserstein formulations for the comparison of metric measure spaces (i.e. metric spaces endowed with a probability distribution) up to isometry. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. The second is a distance between mm-spaces based on the conic lifting. The authors show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme.
SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a new pruning method, called all-alive pruning (AAP), which aims to find the well-trainable networks with sparse parameters (e.g., <10% of the parameters remaining) at high compression ratios. The main idea is that dead connections, which do not contribute to model capacity, appear regardless of the pruning methods. To this end, the authors propose a novel pruning algorithm, which produces the pruned networks with only trainable weights. The proposed method is applicable to various saliency-based pruning algorithms and model architectures."
SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for semantic image editing using a generative adversarial network (GAN). The proposed method is based on a GAN with a regressor that predicts the attributes of the image and a latent variable that controls the degree of the transformation. The model is trained with two losses: a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. The paper also proposes a quantitative evaluation strategy for measuring controllable editing performance, which is different from prior work that primarily focuses on qualitative evaluation. The method is evaluated on both natural and synthetic images."
SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"This paper proposes a feature alignment regularization method for Gumbel-Softmax-based GAN for discrete sequence generation. Specifically, the proposed method aligns the mean statistics of the fake data distribution with that of real data as close as possible in a finite-dimensional feature space. Experiments on synthetic and real-world benchmark datasets show the superior performance in quantitative evaluation. "
SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, the proposed method is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, SpectralDQN remains more than competitive."
SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper introduces a notion of “usable information”, which measures the amount of information that can be extracted from a representation by a learned decoder, and is scalable to high dimensional realistic tasks. The authors use this notion to quantify how relevant and irrelevant information is represented across layers of the network throughout the training process, and how this is affected by the optimization algorithms and the network pretraining. They also evaluate how perturbing the initial part of training impacts the learning dynamics and the resulting representations. They show that SGD training with high learning rate and small batch size leads to learning minimal sufficient representations."
SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper proposes a variant of the SREDA algorithm for nonconvex-strongly-concave min-max optimization. The main contribution of this paper is a theoretical analysis of the complexity of the proposed algorithm, which shows that it achieves the optimal complexity dependence on the accuracy level and the step-size. The paper also proposes a zeroth-order variance reduction algorithm named ZO-SREDA-Boost for the scenario that has access only to the information about function values not gradients, and shows that this algorithm outperforms the best known complexity dependency on accuracy."
SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper investigates the effect of increasing the number of object categories used during training on the generalization of few-shot object detection. The authors show that increasing number of categories improves generalization from seen to unseen classes from 45% to 89% and improves the state-of-the-art on COCO by 5.4 %AP50 (from 22.0 to 27.5). They verify that the effect is caused by the number categories and not the number training samples, and that it holds for different models, backbones and datasets. This result suggests that the key to strong few shot detection models may not lie in sophisticated metric learning approaches, but instead simply in scaling the number category."
SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes to use the spatial gradients of the occupancy field and signed distance field (SDF) as a source of supervision for single-view object reconstruction. The authors propose a novel closed-form Differentiable Gradient Sampling (DGS) solution that enables back-propagation of the spatial gradient to the feature maps, thus allowing training on large-scale scenes without dense 3D supervision. They demonstrate single view implicit surface reconstructions on real-world scenes via learning directly from a scanned dataset. Their model performs well when generalizing to unseen images from Pix3D or downloaded from the internet."
SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a training strategy called “Pseudo-to-real” for extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. The authors demonstrate a practice of pretraining an unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 NVIDIA-V100 GPUs and lasted around 10 days. Besides, the authors also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities."
SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper considers the problem of training energy-based generative models (EBMs) with shallow overparametrized neural networks (NNs). The authors propose a dual formulation of the EBM training problem, which is based on the variational principles of Fenchel duality. In particular, the authors show that the dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. The authors also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies the lower bounds of differentially private empirical risk minimization (ERM) for general convex functions. In particular, the paper considers the constrained and unconstrained cases of DP-ERM. In the constrained case, the lower bound is $\tilde{O}(\sqrt{\sqrt{p}log(1/\delta) n)$, which is believed to be tight, but is off by some logarithmic terms. The paper proposes to use a novel $\ell_2$ loss function instead of linear functions, and achieve the first $\Omega(p n)$ lower bound for both cases. The authors also introduce an auxiliary dimension to simplify the computation. "
SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key of the method is a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The framework covers all the classical Wassersteins gradient flows including the heat equation and the porous medium equation. The authors demonstrate the performance and scalability of the algorithm with several numerical examples."
SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed meta-feature with the space of distributions on the hyperparameter configurations. Experiments on the OpenML CC-18 benchmark demonstrate that the proposed approach outperforms the state-of-the-art AutoML systems, AutoSkLearn and Probabilistic Matrix Factorization."
SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a method for federated learning (FL) with heterogeneous participants that provides in-situ customization of model sizes and robustness. Specifically, the proposed method learns a set of base sub-networks of different sizes and adversarial robustness levels, which are then aggregated on-demand according to inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate that the method provides better in-Situ customization than the existing heterogeneous FL methods."
SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper proposes a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. It also converges globally even in settings where the underlying operator exhibits limit cycles. Moreover, a variant with stochastic oracles is proposed."
SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies neural contextual bandits, a general class of contextual bandits where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The authors prove that under standard assumptions, their proposed algorithm achieves $O(\sqrt{T})$ regret, where $T$ is the learning time horizon."
SP:a9a2c21110e00f19882d27bef0063c422a15e576,"This paper proposes a Shapley-inspired methodology for training action space categorization and ranking. The methodology includes a Monte Carlo simulation to reduce the search space by 80% and categorizes the training action sets into dispensable and indispensable groups. Additionally, it ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The proposed data-driven methodology is extensible to different domains, use cases, and reinforcement learning algorithms."
SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"This paper proposes a novel approach to construct probably approximately correct (PAC) prediction sets in the presence of covariate shift. The authors propose to use importance weights to capture the likelihood of a source example under the target domain. When the importance weights are known, it uses rejection sampling (von Neumann, 1951) to construct the prediction sets. When only given confidence intervals around importance weights, it constructs prediction sets that are robust to this uncertainty. The proposed approach is evaluated on covariate shifts based on DomainNet and ImageNet."
SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount of unlabelled data to refine the model parameters. In particular, they seek to understand the behaviour of the generalisation error of iterative SSL algorithms using information-theoretic principles. They first show that when the class conditional variances are not too large, the upper bound on the generalizability error decreases monotonically with the number of iterations, but quickly saturates. The theoretical results on the simple model are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR datasets. The results suggest that the proposed upper bound serves as a useful guide to understand how changes across the training iterations and it can be used to establish conditions under which unlabeled data can help in terms of generalization generalization."
SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method for model-free reinforcement learning (RL) that can generate actions not only for the current step, but also for a number of future steps. The authors claim that the generated multi-step plans can be used for temporally coordinated exploration towards high-value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at a single step level, whose consistent movement decays exponentially with the number of exploration steps. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods."
SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorizations to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix/tensor completion problems and prove that their methods have tighter generalization error bounds than conventional matrix & tensor decomposition methods. The experiments on synthetic data and real datasets showed that the proposed methods have much higher recovery accuracy than many baselines.
SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes an approach to interpret the behavior of structured output models, i.e. models that learn mappings between an input vector to a set of output variables simultaneously. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. They assume an arbitrary structured output model is available as a black box and argue how considering the correlations between output variables can improve the explanation performance. They introduce an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained."
SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a method for risk-sensitive deep reinforcement learning (DRL) based on policy gradients. The authors propose to optimize the cumulative distribution function (CDF) of the full-episode outcomes instead of the expected reward. The CDF is defined as the sum of the cumulative rewards of all episodes, and the objective is to maximize the CDF for each episode. The proposed method is based on sampling-based optimization and variance reduction, and is shown to be asymptotically consistent for a broad class of CDF-based objectives. Experiments on the OpenAI Safety Gym show that the proposed method outperforms PPO and Lagrangian constrained methods."
SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning framework to proactively learn a deep learning surrogate model and accelerate simulation for large-scale, spatiotemporal, age-structured epidemic models. The proposed method is based on the integration of neural process, deep sequence model, and active learning. The model automatically infers the latent process which describes the intrinsic uncertainty of the simulator. A new acquisition function, Latent Information Gain (LIG), is designed to select the parameters with the highest LIG, queries the simulator to generate new simulation data, and continuously updates our model. Theoretical analysis and empirical results demonstrate the efficacy of the proposed method."
SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"This paper studies the problem of fine-tuning large pretrained NLP models for privacy-guaranteed NLP tasks. The authors show that the performance drop can be mitigated by (1) the use of larger pretrained models, (2) hyperparameters that suit DP optimization, and (3) fine-tuneing objectives aligned with the pretraining procedure. In addition, the authors propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model."
SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method to optimize the skeletal structure and joint attributes of a robotic agent. The main idea is to learn a conditional policy that first applies a sequence of transform actions to modify an agent’s skeletal structure, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods in terms of convergence speed and final performance."
SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a new split MLP architecture to accelerate inference and training of coordinate-based MLPs for implicit neural representations. The proposed method splits the initial layers to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP."
SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to learn object-centric representations of visual scenes from a single image. The method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The authors empirically show that INFERNO discovers objects in a scene without supervision. They also validate the interpretability of the learned representations by manipulating inferred scenes and showing the corresponding effect in the rendered output. They demonstrate the usefulness of our 3D object representations in a visual reasoning task using the CATER dataset."
SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method for evaluating the subgraph explanations of GNNs. The authors argue that a distribution shift exists between the full graph and subgraphs, which causes the out-of-distribution problem. They propose a generative model to generate plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of DSE in terms of explanation fidelity."
SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper studies the problem of active learning for few-shot image and text classification. The authors propose to use pretrained models to select examples that are informative and disambiguate between the possible tasks a user may be trying to specify. The proposed approach is evaluated on a variety of datasets with spurious correlations, latent minority groups, or domain shifts. The results show that the selected examples are preferentially minority classes or informative examples where the spurious feature and class label are decorrelated."
SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper presents a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The proposed model, called GRAPHIX, leverages the abstract syntax structure of code and represents the code using a multi-head graph encoder, along with an autoregressive tree decoder, the model learns to perform graph edit actions for automated program repair. The authors devise a novel pre-training strategy, namely deleted sub-tree reconstruction, to enrich the model with implicit knowledge of program structures from unlabeled source code, which is made consistent with the bug fixing task to facilitate the downstream learning. The model is evaluated on the Patches in The Wild Java benchmark, using both abstract and concrete code. Experimental results show that the proposed model significantly outperforms a wide range of baselines including CodeBERT and BART."
SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper proposes an alpha-weighted Federated Adversarial Training (alpha-WFAT) method, which relaxes the inner-maximization optimization of adversarial training into a lower bound friendly to Federated Learning. Theoretical analysis about the convergence of the proposed method is provided, and extensive experiments are conducted to comprehensively understand the characteristics of alpha-Weighted FedAvg. Experimental results on three benchmark datasets demonstrate that alpha-Wat outperforms FAT under different adversarial learning methods and federated optimization methods."
SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a method for semi-supervised lifelong learning (LML) by leveraging data programming. The main idea is to use data programming to label new data under the supervision of a set of weak labeling functions trained from a limited number of labeled data points. The proposed method is evaluated on various partially labeled LML task sequences generated from commonly used datasets including MNIST, Cifar-10 and CIFAR-100 (LeCun & Cortes, 2010; Krizhevsky, 2009; Smith et al., 2021)."
SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes two methods to generate adversarial examples that simultaneously (a) be misclassified by the model and (b) be detected as non-adversarial. The first method, Selective Projected Gradient Descent, is a modification of standard gradient descent (MMS+17) where instead of taking gradient descent steps that optimize the joint loss function f + l_g, the authors propose to select steps that maximize the loss on either f or g. The second method, Orthogonal Projected gradient Descent (OGD), is a variant of MMS +17 where the gradients of f and g are orthogonal to each other. The authors show that the proposed method is able to evade four state-of-the-art detection defenses, reducing their accuracy to 0% while maintaining a 0% detection rate."
SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes an energy-based treatment for cooperative games with the maximum entropy principle, which recovers classical game-theoretic valuation criteria by conducting one-step fixed point iteration for maximizing the ELBO objective. Theoretical analysis shows that under uniform initializations, the variational valuations satisfy a set of game theoretic axioms. Empirical results show that the proposed Variational Index enjoys lower decoupling error and better valuation performance on some synthetic and real-world valuation problems."
SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method to directly estimate epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator is useful in interactive learning environments arising in active learning or reinforcement learning. The proposed method is evaluated on downstream tasks including sequential model optimization and reinforcement learning and probabilistic classification of images."
SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes a new method to learn the rotation matrix of the space of embeddings for approximate nearest neighbor (ANN) search. The proposed method is based on geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n) and block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. Compared to the state-of-the-art SVD method, the proposed algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to experimental studies. They further improve upon vanilla product quantization significantly in an end-to-end training scenario."
SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices, an abstract visual reasoning test of fluid intelligence. The framework uses (1) a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain, and (2) a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The authors claim that the proposed approach (NSM) is able to isolate the relational structure from source domain with high accuracy and (b) successfully utilizes this structure for analogical reasoning in the targets domain."
SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a contrastive self-supervised method, namely Self-GenomeNet, for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force the framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the method with self/semi-supervision settings outperforms state-of-the-art deep learning methods. The learned representations generalize well and can be transferred to new datasets and tasks."
SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how our model parameters are fully steerable at inference time. The proposed spherical filter banks enable making equivariant and invariant class predictions for known point sets in unknown orientations."
SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper compares the performance of 5 pretrained language models (PLMs) and 4 CL approaches on 3 benchmarks in 2 typical incremental learning settings. The experiments reveal interesting performance differences across PLMs and across CL methods. Furthermore, the representativeness probing analyses dissect PLMs’ performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches."
SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,This paper proposes a defense against untargeted model poisoning attacks in federated learning. The proposed defense is based on the intuition that certain patterns of gradient flips are indicative of an attack. The paper proposes to assign reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. The authors show that TESSERACT provides robustness against even a white-box version of the attack.
SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a method for estimating linear functionals of high-dimensional or non-parametric regression functions. The authors propose an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Their method solely requires value query oracle access to the linear function. They propose a multi-tasking Neural Net debiase method with stochastic gradient descent minimization of a combined Riesze representer and regression loss, while sharing representation layers for the two functions. They also propose a Random Forest method which learns a locally linear representation of Rieszi function. The proposed method outperforms the state-of-the-art estimator of Shi et al. (2019) for the average treatment effect functional."
SP:96e1da163020441f9724985ae15674233e0cfe0d,This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. The authors propose a mini-batch Markovian sampled actor critic algorithm and analyze its finite-time convergence and sample complexity. They show that the sample complexity of this algorithm is O(N/\ell_\infty) and O(O(\ell^{-1}log^1) for the stationary point. This sample complexity matches that of the state-of-the-art single-agent actor critic algorithms. 
SP:8475e89f143c727e33147b652c2d0b3cdb420382,This paper provides a theoretical understanding of contrastive learning (CL) from the point of view of learning class-separated representations. The main idea is to view CL as an instance discrimination task (differing each image from others) instead of a classification task (clustering images from the same class together and differing with other classes). The paper provides upper and lower bounds for the downstream performance and show how the two asymptotically converge with our assumptions on the overlapping data. The paper also shows that the theory aligns well with existing contrastive methods on both synthetic and real-world datasets.
SP:b491314336c503b276e34e410cf461cb81294890,This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The authors also propose a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The experimental results show that the proposed method achieves a 0.499 higher mean opinion score (MOS) than the speech denoising SSR model.
SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a method to model the long-term dependencies in the variable space of multivariate time series. The main idea is to use a tensor network based on the idea of low-rank approximation to model variable space. The tensor components are shared to ensure the translation invariance of the network. The series variable encoder is designed to improve the quality of variable space, and the skip-connection layer is used to achieve the scale information. "
SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper proposes a variant of SCO algorithms with sparse moving averages for GNN training. Theoretical results show that the proposed method preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. Experiments validate the theoretical results and show the algorithm outperforms the traditional Adam SGD.
SP:72e0cac289dce803582053614ec9ee93e783c838,"This paper proposes Circulant MinHash (C-MinHash), a variant of MinHash that uses two independent random permutations in a circulant manner to approximate the Jaccard similarity in binary data. Theoretical results show that using only two independent permutations leads to uniformly smaller variance than the classical MinHash. Experiments are conducted to show the effectiveness of the proposed method. The paper also proposes a more convenient C-minHash variant which reduces two permutations to just one."
SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a simple and efficient training scheme to achieve adversarial robustness against the union of lp-threat models. The proposed E-AT scheme is based on geometric considerations of the different Lp-balls and costs as much as normal adversarial training against a single threat model. Moreover, it can fine-tune with just 3 epochs any lp robust model (for p \in {1,2,\infty}) and achieve multiple norm robustness. The authors show that the proposed method can boost the state-of-the-art for multiple-norm robustness to more than 51% on CIFAR-10 and report up to our knowledge the first ImageNet models with multiple norms robustness on Cifar-10."
SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed to allow the gate to choose either the public Encoder or the corresponding private Encoder. Moreover, a variant ofSMTL is also proposed to place all the gates before the decoder of each task. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper argues that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions, which makes model selection not only difficult, but also undecidable. Specifically, the authors show that there are no good deterministic or randomized estimates of the partition functions. The authors then propose to consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness. "
SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes a new family of distance metrics called augmented sliced Wasserstein distances (ASWDs), which are constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on the hypersurface would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The ASWDs can be optimized by gradient ascent efficiently. The condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. "
SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a general framework for improving coordination and performance of multi-agent reinforcement learners (MARL). The framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator, that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. The Generator determines the best states to learn to add intrinsic rewards which leads to a highly efficient learning process. LIGS can be easily integrated into existing MARL algorithms and the theory shows that it ensures convergence to joint policies that deliver higher system performance."
SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a hierarchical attention mechanism for multi-hop question answering. The proposed model, called DOCHOPPER, iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. The model is able to retrieve either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. The experiments on four different QA tasks that require reading long and complex documents show that the proposed model achieves state-of-the-art results on three datasets."
SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a semi-supervised learning method for character-based characteristic extraction for voice casting for audiovisual productions. The proposed method first uses a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm to finally train a refined representation extractors. The method is validated on recordings from the MassEffect 3 video game."
SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks, allowing clients to learn multiple tasks with their private data. The key idea is to disentangle representation of local and non-local features using task-agnostic Vision Transformer and a task-specific head/tail. Each client learns a translation from its own task to a common representation, while the Transformer body learns global attention between the features embedded in the representation. The proposed method is evaluated on various low-level and high-level computer vision including medical image data."
SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking, where RL agents exploit gaps in misspecified reward functions. The authors construct four RL environments with reward misspecifications and investigate how reward hacking arises as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. They find that more capable agents exploit the misspecification, achieving higher proxy reward and lower true reward than less capable agents. Moreover, they find instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, the authors propose an anomaly detection task for aberrant policies and offer several baseline detectors for this task."
SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence variational objective (f-TVO) that generalizes the TVO by replacing Kullback-Leibler (KL) divergence with arbitary differeitiable f divergence. In particular, it approximates dual function of model evidence f*(p(x)) rather than the log model evidence log p(x) in TVO. It is derived from a deformed χ-geometry perspective, which is the deformed geodesic between the variational posterior distribution and the true posterior distribution. Experiments on VAE and Bayesian neural network show that the proposed f -TVO performs better than the cooresponding baseline."
SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper presents an empirical study of the interplay between different components of the deep reinforcement learning (RL) toolbox in the continuous control setting. In particular, the authors propose a new algorithm, ED2, which is a combination of several existing RL tools, such as (1) ensemble exploration, (2) weighted Bellman backup, (3) perturbation, (4) inverse sampling, (5) ensemble policy gradient, and (6) critics’ initialization. The empirical results show that the proposed ED2 algorithm outperforms the existing methods on the Ant and Humanoid continuous control tasks. "
SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of finding a fair predictor for the Equalized Loss (EL) constraint, which requires the prediction error/loss to be equalized across different demographic groups. The authors propose two algorithms that can leverage off-the-shelf convex programming tools and efficiently find the global optimum of this non-convex problem. Experiments on real-world data show the effectiveness of the proposed algorithms."
SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper revisits systematic generalization from the perspective of meaningful learning. The authors propose to reassess models’ compositional skills conditioned on the semantic connections between new and old concepts. They augment a training dataset in either an inductive or deductive manner to exposure such semantic links to models. Their observations on SCAN and two real-world datasets on semantic parsing suggest that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking."
SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper presents a method for 3D shape representation learning using multi-scale wavelet decomposition. It decomposes 3D shapes into sub-bands components at multiple scales and all scales form a decomposition tree in a principled manner rooted in multi-resolution wavelet analysis. The proposed Adaptive Wavelet Transformer Network (AWT-Net) firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-band components, using lifting scheme at multiple scale recursively and hierarchically. Then, it exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions."
SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes to combine the benefits of full and lightweight finetuning to achieve strong performance for both in-distribution (ID) and out-of distribution (OOD) fine-tuning. The authors show that an ensemble of the lightweight and full finetuned models achieves the best of both worlds: performance matching the better of full-finetuning, both ID and OOD. Moreover, they show that they can achieve similar improvements using a single model instead of two with the proposed cocktail finetune."
SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes Active Refinement of Weakly Supervised Models (WARM), a method for active improvement of weakly supervised models via active learning. Specifically, at each iteration WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s probabilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling functions in the weak supervision model. Experiments on multiple real-world medical classification datasets reveal that WARM can substantially improve the accuracy of probablistic labels used to train downstream classifiers, with as few as 30 queries to experts."
SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper considers the problem of training a classification model with group annotated training data. The authors propose a new algorithm that explicitly encourages learning of features that are shared across various groups. The key insight is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group- DRO. Theoretically, the authors show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. "
SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method for black-box models that can capture feature interactions in a higher-order. The proposed method is based on Covert et al. (2020a), which provides a unifying mathematical framework capturing a broad array of explainability techniques, termed Removal-based Explanation methods. The authors propose a method to extend any given univariate removal-based explanation to capture asymmetrical feature interactions, represented as a directed graph. They also provide theoretical justification for these definitions in the context of SHAP, the Shapley value explanation map introduced by Lundberg & Lee (2017)."
SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a framework for interpretable policy learning based on probabilistic decision trees. The main idea is to learn a representation of patient history through recurrence, and learn decision tree policies that adapt over time with patient information. The proposed method is evaluated on both synthetic and real-world data sets, and compared with state-of-the-art methods. "
SP:5630707c9d0d9e21fce2efddef874e373bfed026,"This paper proposes a multi-agent reinforcement learning (MARL) approach for data augmentation, where each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. The proposed method is evaluated on multiple benchmark datasets of image classification, fine-grained image recognition and object detection."
SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes an adversarial distribution alignment method to eliminate the difference between natural and adversarial distributions by considering spurious correlations between labels and nuisance factors. The authors construct a causal graph to model the generation process of adversarial examples to formalize the intuition of the adversarial vulnerability. The proposed method is evaluated on MNIST, CIFAR-10, and Fashion MNIST datasets."
SP:9f09449a47464efb5458d0732df7664865558e6f,This paper proposes a method for continual learning of convolutional neural networks. The method is based on decomposing convolution filters within each layer over a small set of filter atoms and then performing continual learning with filter atom swapping for each task. The proposed method can be applied to a wide range of optimization schemes and CNN structures. The effectiveness of the proposed method is illustrated both empirically and theoretically. 
SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance of Stein Variational Gradient Descent (SVGD), a particle-based variational inference algorithm, in high-dimensional settings. The authors show that SVGD is prone to variance collapse in high dimensions, and connect SVGD with MMD-descent, a kernel-based particle inference algorithm that performs maximum mean discrepancy (MMD). They show that the variance collapse phenomenon relates to the bias from deterministic updates present in the “driving force” of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation."
SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the relationship between adversarial training (AT) and noisy labels (NL) in the presence of label noise. In particular, it shows that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training, which suggests that AT itself is an NL correction. In addition, AT with NL is helpful for improving even the natural accuracy."
SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method allows us to provide formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The approach can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods."
SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) that learns representations from non-i.i.d sequential data by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM, and demonstrate that HCM learns meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. Furthermore, the interpretability of the learned chunks enables flexible transfer between environments that share partial representational structure."
SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems. The authors show that reparametrizing the optimization variables as the output of a neural network can lead to significant speedup. To obtain the maximum speed up, the neural network architecture needs to be a specially designed graph convolutional network (GCN). The aggregation function of the GCN is constructed from the gradients of the loss function and reduces to the Hessian in early stages of the optimization. The paper shows the utility of our method on two optimization problems: network synchronization and persistent homology optimization, and find an impressive speedup, with our method being 4 \times\times faster."
SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method based on a layered structure of Support Vector Machine (SVM) ensembles for non-parametric image classification. By utilizing the quick learning of SVMs compared to neural networks, the proposed method can reach higher accuracy than DCNNs when the training set is small. Experimental results show that conventional DCNN architectures such as ResNet-50 outperform SVMnet when the number of training samples is large."
SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG) to reduce the communication and memory overhead. Each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. A cascade of “exits” is attached to the model to make multiple predictions and direct further computation. To reduce total computation, and make full use of prior predictions, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. The full model with redesigned exit architecture and spatial adaptivity enables anytime inference, achieves the same level of final accuracy and significantly reduces total computation."
SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a new bootstrapping approach for learning stochastic processes. The proposed approach builds on top of Bootstrapping Neural Processes (B(A)NP) by Lee et al. (2020) and adapts the meta-learning framework of Attentive Neural processes (Kim et al., 2018) by injecting multiple random weights into the encoder and the loss function, instead of memory-inefficient resampling and contrived heuristics employed in BANP. Experiments on Bayesian optimization and contextual multi-armed bandit tasks show that the proposed approach achieves the best performance in the sequential decision-making tasks among NP methods."
SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a multi-modal and self-supervised approach for pre-training genome data for regulatory genome modeling. Specifically, the authors propose to simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input to improve the robustness and generalizability of the model. They pre-train their model on the ATAC-seq dataset with 17 million genome sequences. They evaluate their GeneBERT on regulatory downstream tasks including promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction."
SP:841b12443d0274e34b78940f220b17d36798899b,This paper proposes a new method for detecting out-of-distribution (OOD) samples. The method is based on the geodesic distance between the underlying data distributions. The discriminator is able to combine confidence scores from the logits outputs and the learned features of a deep neural network. The proposed method is evaluated on CIFAR-10 and SVHN datasets.
SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper provides a generalization of Cover’s Function Counting Theorem to equivariant representations of objects. The authors show that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action. They show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. Finally, they test the theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement."
SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The approach is validated on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample."
SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a method to improve the efficiency and quality of Kernel Point Convolution (KPConv) for 3D point cloud classification and segmentation. Specifically, the proposed method employs a depthwise kernel to reduce resource consumption, and re-calibrates the contribution of kernel points towards each neighbor point via Neighbor-Kernel attention to improve representation power. A predictor-based Neural Architecture Search (NAS) approach is used to automate the design of efficient 3D networks based on KPConv. The experimental results show the effectiveness of the proposed approach."
SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies the problem of robust overfitting, robustness overestimation, and robustness-accuracy trade-off in adversarial training. The authors propose a method to measure the data quality based on the learning behaviors of the data and find that low-quality data may not be useful and even detrimental to the adversarial robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in adversarially robust learning. The experiments cover various sample sizes, training methods, and neural architectures."
SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper analyzes the number of neurons and training parameters that a neural network needs to approximate Korobov functions of bounded second mixed derivatives. The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. The bounds hold for general activation functions, including ReLU. They also show that these bounds nearly match the minimal number of parameters any continuous function approximator needs to approximate Korobev functions, showing that neural networks are near-optimal function approximation."
SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the effect of population size on the Lewis Game of Lewis (Lewis, 1969) in the context of reinforcement learning. The authors first show that increasing the number of agents does not improve the emergent language properties, e.g. success rate, compositionality, generalization, etc. They then show that the relative difference of factors between speaker and listener is more important than the absolute value of speaker capacity and network capacity. Finally, they propose to further distribute the learning speeds across the agents by introducing training speed heterogeneities. "
SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper proposes a polynomial filter-based graph neural network (GNN) method for heterophilic node classification. The proposed method is inspired by Chien et al. (2021), which proposes to learn a sum of polynomials over different subsets of eigenvalues of the graph. The authors show that the proposed method outperforms the existing methods in terms of accuracy and scalability on several datasets. "
SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper proposes a novel graph shortest distance embedding method called Betweenness Centrality-based Distance Resampling (BCDR) for shortest distance query (SDQ) in graphs. BCDR is based on truncated random walk followed by Pointwise Mutual Information (PMI)-based optimization to embed local structural features into a dense vector on each node and integrates with a subsequent predictor for global extraction of nodes’ mutual shortest distance. Theoretical analysis shows that BCDR can occupy a wider distance range measured by the intrinsic metric in the graph domain due to its awareness of the path structure. Second, it performs distance sampling from original walk paths before maximum likelihood optimization instead of the PMI-based optimization and proves that this strategy preserves distance relation with respect to any calibrated node via steering optimization objective to reconstruct a global distance matrix. "
SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL), where the goal is to maintain the invariant knowledge of the pretrained language model (LM) on new corpora. The authors categorize the knowledge categories into three main categories: Invariant knowledge, Updated knowledge and New knowledge. They also propose a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. Through extensive experiments, the authors show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously."
SP:639fd88482330389019fb5be7446a909b99a8609,"This paper proposes a stochastic approach for the criterion minimization. The proposed algorithm is faster than conventional exhaustive search by several orders of magnitude. The algorithm is compared with several other related state-of-the-art decision tree learning methods, including the baseline non-stochastic approach. It outperforms all other methods in terms of accuracy and computational cost."
SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper proposes a new learning rate schedule for SGD on quadratic objectives. The proposed schedule is based on the assumption that the eigenvalue distribution of the underlying Hessian matrix is skewed, which is quite common in practice. Theoretical results show that the proposed schedule can achieve minimax optimal convergence rates (up to a constant). The paper also proposes two simple learning rate schedulers for practical applications that can approximate eigencurve. Experiments are conducted on CIFAR-10 and ImageNet."
SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the uncertainty heuristics for offline model-based reinforcement learning (MBRL). In particular, the authors compare the P-MDP-based approaches (Kidambi et al., 2020; Rafailov et al. 2020; Pan et al, 2020; Cowen-Rivers et al 2020) with respect to a number of hyperparameters, such as the number of models, rollout horizon, and number of rollouts. They also explore the interaction with a series of other hyperparameter choices (i.e., rollout horizon and model size). The authors show that Bayesian Optimization (BO) provides superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance."
SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a method for prioritizing experience replay in off-policy model-free reinforcement learning (MfRL). The authors propose to use learnable features driven from components in model-based RL (MbRL) to calculate the scores on experiences. Specifically, the authors modify the critic network and the transition elements, which they call Modelaugmented Critic Network (MaCN), and propose a modified experience replay scheme, which prioritizes the past experiences having both model estimation errors and the TD-errors (see Figure 1). The proposed MaPER brings the effect of curriculum learning for predicting Q-values better by the critic networks with negligible memory and computational overhead."
SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a method for human-in-the-loop reinforcement learning (HACO) in which a human expert can take over the control and demonstrate to the agent how to avoid probably dangerous situations or trivial behaviors. The proposed method extracts proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that HACO achieves a substantially high sample efficiency in the safe driving benchmark.
SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes a new hierarchical meta imitation learning method, called Dual Meta Imitation Learning (DMIL), which uses the likelihood of state-action pairs from each sub-skill as the supervision for the high-level network adaptation, and use the adapted high level network to determine different data set for each sub skill adaptation. The authors theoretically prove the convergence of the iterative training process of DMIL, and establish the connection between DMIL and the Expectation-Maximization algorithm. Empirically, the proposed method achieves state-of-the-art performance on the meta-world benchmark and comparable results on the Kitchen environment."
SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a node embedding compression method to compress the node embeddings of graph neural networks (GNNs) into bit vectors instead of floating-point vectors. The idea is to convert the compositional code vector of each node to a bit vector and concatenate the bit vectors into a binary vector, which is then converted into a floating point vector. The authors claim that the proposed method can be trained end-to-end with GNNs and achieves superior performance compared to the alternatives. "
SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper proposes to replace gradient descent with high-order ODE solvers (i.e., Runge-Kutta solvers) in the domain adversarial learning (DAL) framework. The authors first show that gradient descent in DAL can violate the asymptotic convergence guarantees of the optimizer. Then, the authors propose to replace the gradient descent algorithm with the RKHS solver, which is more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers."
SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularizer, called individual Flood (denoted as iFlood), which encourages the trained models to better fit the under-fitted instances while suppressing the confidence on the over-fitted ones. The authors also theoretically show that the design of the loss function can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. Experiments on both image classification and language understanding tasks confirm that models learned with iFlOOD can stably converge to solutions with better generalization ability, and behave consistently at instance-level."
SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a Hierarchical reinforcement learning (HRL) approach for long-horizon tasks. In particular, the authors propose Value Function Spaces (VFS), a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that the approach improves long-term performance and enables better zero-shot generalization than alternative model-based methods."
SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a one-shot probabilistic generative model that maps a vector-shaped prior to a distribution over sets or graphs. The authors propose a deterministic, non-exchangeable set creation mechanism that learns to select the most relevant points from a trainable reference set. The proposed method can replace i.i.d. generation in any VAE or GAN and better captures complex dependencies in the data. "
SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, the authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. They establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. The authors also propose a modified version of the DeepRitz Method and prove that PINN and the modified version can achieve minimax optimal bounds over Sobolev spaces."
SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper analyzes the relationship between robustness and generalization in the context of domain generalization. The authors provide sufficient conditions for this phenomenon considering different factors that could affect both, such as norm of the last layer, Jacobian norm, and data augmentation. They show that robustness induced by adversarial training is a by-product of such function class regularization. They then discuss in details about different properties of DA and prove that under certain conditions, DA can be viewed as regularization and therefore improve generalisation."
SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the memorization effect in meta-learning, where the meta-knowledge simply memorizes all meta-training tasks discourages task-specific adaptation and poorly generalizes. The authors claim that the universal label space as a confounder to be the causing factor of memorization and frame the two lines of prevailing methods as different deconfounder approaches. The proposed causal perspective not only brings in the two deconfounder algorithms that surpass previous works in four benchmark datasets towards combating memorization, but also opens a promising direction for meta learning."
SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper tackles the problem of ad hoc teamwork, i.e., when agents need to collaborate with previously unknown teammates on the fly. This paper relaxes the assumptions such as full observability, fixed and predefined teammates’ types, and allows the agent to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a finite set of predefined types, ODITS automatically learns latent variables of teammates' behaviors to infer how to cooperate with new teammates effectively. An information-based regularizer is introduced to overcome partial observability. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc team tasks."
SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The proposed method, called EMFlow, is based on an online version of Expectation-Maximization (EM) algorithm by using a normalizing flow (NF) model which maps the data space to a latent space. The method is iterative, involving updating the parameters of EM and NF alternatively. Extensive experimental results are presented to illustrate the superior performance of the EMFlow compared to a couple of recent methods in terms of predictive accuracy and speed of algorithmic convergence."
SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a deep linear gated networks (DLGN) for deep neural networks (DNNs) with rectified linear units (ReLUs) that disentangles the computations into two ‘mathematically’ interpretable linearities: (i) the primal linearity between the input and the preactivations in the gating network and (ii) the ‘dual’ linearity in the path space in the weights network characterised by the neural path kernel (NPK). The authors extend the recently developed dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates can be characterised analytically via the so called neural path kernels. In this paper, the authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the Neural Path Kernels. The authors also propose a novel interpretable counterpart of DNNs with ReLUs namely deep linearly-gated networks(DLGN): the pre-activations to the gates are generated by deep linear networks, and then applied as external masks to learn the weights in a different network."
SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes a new normalizer for vision transformers, termed dynamic token normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (intertoken). The authors claim that DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead."
SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper investigates the effect of regularization and data augmentation on the frequency of the learned functions in deep neural networks (DNNs). The authors propose two methods to measure the spectral bias of DNNs: (1) label noise, and (2) linear interpolation between test examples. The authors show that regularization methods such as weight augmentation and mixup can affect the learned function frequency in different ways. They also show that larger models learn high frequencies more readily than smaller ones, but many forms of regularisation inhibit the learning of high frequencies."
SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper investigates the question of when to switch between exploration and exploitation modes in RL. The authors propose a set of algorithmic components to make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. They report a promising and detailed analysis on Atari using two-mode exploration and switching at sub-episodic time-scales."
SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median clustering problem in the general metric space, based on the construction of metric embedding tree structure of the data. The authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Theoretically, the authors show that the initial centers from HST initialized can achieve lower error than those from another popular initialization method, k-Median++, in the non-DP setting. Moreover, with privacy constraint, the error of applying DP local search followed by our private HST initial centers improves previous results, and approaches the known lower bound within a small factor."
SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper introduces a new architecture, named FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and how they can be mitigated using existing image augmentation techniques. The proposed method outperforms the current SOTA models across four different video prediction benchmarks."
SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the influence of data structure on the test loss dynamics of stochastic gradient descent (SGD) on the generalization performance of a machine learning algorithm. The authors propose an exactly solveable model of SGD that predicts test loss when training on features with arbitrary covariance structure. They show that the optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure, demonstrating the computational benefits of using small batch sizes. They also extend their theory to the more usual setting of standard batch gradient descent on a fixed subsampled training set."
SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the non-convex non-linear optimization setting, where the loss function is nonlinear and the landscape is non-vanishing. The setting is similar to the one studied by Ziyin et al. (2021), where the noise is assumed to be Gaussian and the learning rate is held constant throughout training. The main contribution of this paper is to show that SGD converges to local maxima, but at the rate of $O(1/\sqrt{T})$, which is not the case for SGD when the loss is strongly convex. The paper also shows that AMSGrad may not converge to the maxima. "
SP:22d01913b78ef447b064c65a646fa301b861d3f7,This paper proposes a method for hyperparameter optimization in gradient-based meta-learning. The main idea is to use a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second-order term for every online HO step. The method allows online optimization and also is scalable to the hyperparameters and the horizon length. The proposed method is evaluated on two different meta learning methods and three benchmark datasets.
SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a clustered task aware meta-learning (CTML) framework with task representation learned from both features and learning path. CTML first conduct rehearsed task learning from the common initialization, and collect a set of geometric quantities that adequately describes the learning path, by inputting this set of values into a meta path learner, we automatically abstract path representation optimized for the downstream clustering and modulation. To further save the computational cost incurred by the additional rehearsed learning, we devise a shortcut tunnel to directly map between the path and feature cluster assignments. Extensive experiments on two real-world application domains demonstrate the superiority of CTML compared to state-of-the-art baselines."
SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near-OOD data. The proposed method relies on regularization to promote diversity on the OOD data while preserving agreement on ID data. Extensive comparisons of the proposed approach to state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical images data sets reveal significant gains with negligible increase in computational cost.
SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,This paper proposes an encoder-decoder architecture for multi-agent trajectory prediction. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal and Social dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The proposed model is evaluated on the nuScenes vehicle motion prediction leaderboard and Argoverse vehicle prediction challenge.
SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper presents a user study on the effectiveness of interpretable image classification methods for bias discovery. The authors propose a synthetic dataset (TWO4TWO) that can be used to identify biases in an image classification model. A baseline explanation technique is designed to allow users to inspect only the model’s output and only the logit predictions. The baseline is evaluated against concept-based and counterfactual explanations, and the results show that the baseline outperformed concept based explanations. "
SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes an approach to defend against backdoor data poisoning attacks. The proposed approach consists of two steps. First, it first trains an ensemble of weak learners to automatically discover distinct subpopulations in the training set, and then leverage a boosting framework to exclude the poisoned data and recover the clean data. The algorithm is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. Empirically, the method successfully defends against a state-of-the-art dirty label backdoor attack."
SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, and maps the contextual encodings of these latent labels to the actual labels cooperatively. The correlations between labels, and between labels and the text are modeled indirectly through these latent-label representations and their correlations. The method is conceptually simple but quite effective. It improves the state-of-the-art results on two widely used benchmark datasets by a large margin."
SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of convolutional kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers. The authors show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities."
SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of computing the Neural Tangent Kernel (NTK) matrix for finite width neural networks. The NTK is the outer product of the Jacobians of the weights of the neural network, and it is a central object of study in deep learning. In the infinite width limit, the NTK can be computed analytically and is useful for understanding training and generalization of NN architectures. However, the finite width NTK matrix is notoriously expensive to compute, which severely limits its practical utility. This paper provides the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. It further proposes two novel algorithms that change the exponent of the computations of NTK, which dramatically improves efficiency."
SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper considers the problem of offline constrained reinforcement learning, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. The authors propose an algorithm, called COptiDICE, that directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. The proposed algorithm is evaluated on the Mujoco environment, and compared with baselines. "
SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a novel parallel training scheme (called parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach.
SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a manifold-regularized multi-decoder autoencoder (MRMD-AE) that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. The authors assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. They show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, and improves cross-subject translation of fMRI signal."
SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper introduces new benchmarks for out-of-distribution detection in multi-class and multi-label settings. It also introduces a new dataset of anomalous species for ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Finally, it introduces a segmentation benchmark for anomaly segmentation by introducing a road anomalies. The authors conduct extensive experiments in these more realistic settings for out of distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-classes, multi-labels, and segmentation tasks."
SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,This paper studies the representation of tournaments in terms of sign-rank matrices. The authors show that tournaments can be represented in d dimensions if there exists a skew symmetric matrix M \in R^n of rank d such that a directed edge from i to j is present in T if and only if Mij > 0. They also show that the flip class associated with a coned-doubly regular tournament of size O(\sqrt{d}^d) must necessarily be a union of flip classes. They further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments.
SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a method to improve the performance of neural processes (NP) by adjusting the stochasticity of the attention weights. The authors claim that the proposed method encourages the context embedding to be differentiated from the target dataset. The proposed method is evaluated on 1D regression, predator-prey model, and image completion tasks, and is also validated on MovieLens-10k dataset."
SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a method for interpretability of transformer language models (LMs). The main idea is to use prototype networks to explain the reasoning process behind the network’s decisions. The authors also propose a novel interactive prototype learning setup (iProto-Trex) to incorporate human capabilities to incorporate knowledge outside of the rigid range of purely data-driven approaches. Experimental results demonstrate that the proposed method performs on par with noninterpretable baselines.
SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes a method for continual learning (CL) based on a notion of ‘trust region’ to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to reuse the frozen weights of the selected old tasks in the trust region. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets."
SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper studies the connection between optimization and generalization in machine learning. The authors propose a framework to connect optimization with generalization by analyzing the generalization error based on the length of optimization trajectory under the gradient flow algorithm after convergence. They show that, with proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalization bound, showing that short optimization paths after convergence indicate good generalization. Their framework can be applied to broad settings. For example, they use it to obtain generalization estimates on three distinct machine learning models: underdetermined lp linear regression, kernel regression, and overparameterized two-layer ReLU neural networks."
SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples, supported by theoretical and empirical findings. The analysis shows that adversarial example are neither in high-frequency nor in low-frequency components, but are simply dataset dependent. Particularly, it highlights the glaring disparities between models trained on CIFAR-10 and ImageNet-derived datasets. Utilizing this framework, the authors analyze many intriguing properties of training robust models with frequency constraints, and propose a frequency explanation for the commonly observed accuracy vs robustness trade-off."
SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper studies the effect of heterophily in graph neural networks (GNNs). The authors first show that not all cases of homophily are harmful for GNNs with aggregation operation. Then, they propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNN. Based on the metrics and the observations, the authors propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophilies. The authors validate the ACM-augmented baselines with 10 real-world node classification tasks."
SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a method for generalizing deep reinforcement learning (RL) solvers to larger-sized instances of the traveling salesman problem (TSP). The main idea is to use equivariance, local search, and curriculum learning to improve the generalizability of the RL solver. The proposed method is evaluated on random and realistic TSP problems against relevant state-of-the-art deep RL methods."
SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a framework for federated learning (FL) that uses differentially private synthetic data. Specifically, each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation. To ensure privacy, the local GANs are trained with differential privacy by adding artificial noise to the local model gradients. Extensive experiments demonstrate the effectiveness of the proposed framework."
SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a method to improve the certified robustness of smoothed classifiers. The main idea is to use the accuracy under Gaussian noise as an easy-to-compute proxy of adversarial robustness for each input, and differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. Experiments on standard benchmarks show that the proposed method, despite its simplicity, exhibits improved certified defense upon existing state-of-the-art training methods."
SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper proposes two packing algorithms to reduce the amount of padding in BERT pre-training. The first algorithm, shortest-pack-first histogram-packing (SPFHP), determines the packing order for the Wikipedia dataset of over 16M sequences in 0.03 seconds. The second algorithm, non-negative least-squares histogram packing (NNLSHP), converges in 28.4 seconds but produces solutions that are more depth efficient. The authors also propose an alternative algorithm, NNLSHP, which is able to get near optimal packing by combining a maximum of 3 sequences in one sample. The experiments show that the proposed methods can achieve a 2x speed-up in terms of sequences/sec."
SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm, which is a deterministic variant of Monte Carlo tree search (MCTS) that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, the algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in auto-regressive models. "
SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes a method for anomaly detection based on Energy Based Model (EBM) and contrastive loss. The main idea is to learn to associate low energy to normal samples and high energy to abnormal samples, which is achieved by using Langevin Dynamics (LD) in generating incorrect samples based on an iterative optimization procedure, alleviating the intractable problem of modeling the world of anomalies. An adaptive sparse coding layer is also proposed to avoid training an anomaly detector for every task, which can be used to quickly update what is normal during inference time and avoid tedious data collection."
SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper presents a large-scale dataset for Question Answering (QA) that contains 10K human-written and model-generated contradicting pairs of contexts. The dataset is constructed on top of SQuAD 1.1 (Rajpurkar et al., 2016). For each context paragraph P in the dataset, the authors create a fake version of P by modifying information in P, such that 1) certain information contradicts with the information in the original context, and 2) P is fluent, consistent, and looks realistic. Experiments show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such threat, they build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation learning that uses the distance between states between the different spaces of the agents to align and compare states. The paper provides a theoretical analysis of the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. The authors demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformations of the state-action space."
SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a hierarchical cross-level contrastive learning (HCCL) method for self-supervised learning (SSL) of visual representation. HCCL uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. Experiments on ImageNet classification, detection, segmentation, and few-shot learning tasks demonstrate the effectiveness of the proposed method."
SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a multi-agent deep reinforcement learning (RL) framework to learn Nash equilibria for dynamic general equilibrium models (DGE) with heterogeneous, interacting strategic agents of various agent types, such as consumers, firms, and governments. The authors show that the proposed approach can discover stable solutions that are -Nash equilibrium for a meta-game over agent types in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training. The proposed approach is more flexible and does not need unrealistic assumptions, e.g., market clearing, that are commonly used for analytical tractability. The GPU implementation enables training and analyzing economies with a large number of agents within reasonable time frames."
SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method to impede model extraction by requiring users to complete a proof-of-work before they can read the model’s predictions. The method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen. The proposed method can greatly increase the computational effort needed to leverage query access for model extraction. However, this only introduces a slight overhead for regular users."
SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a multi-resolution variant of Continuous Normalizing Flows (CNFs) for generative models of images. CNFs have been shown to be capable of modelling complex distributions such as those associated with images. This paper characterizes the conditional distribution over the additional information required to generate a fine image that is consistent with the coarse image. The authors introduce a transformation between resolutions that allows for no change in the log likelihood. They show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time."
SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect label noise in deep neural networks (DNNs). The main idea is to first identify “neighbor” instances for each training example whose representation extractor can be fully independent of noisy supervisions by referring to unsupervised learning, self-supervised or other tasks. Based on the neighborhood information, they propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations. "
SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a new adversarial training method called Policy Adversarial Director (PA-AD) for adversarial reinforcement learning (DRL). In particular, the authors propose to train an actor and a ""director"" to find the optimal adversarial state perturbations for a given policy perturbation direction. The actor and the director are trained in an adversarial learning framework, where the actor is trained to maximize the reward of the policy, while the director learns to minimize the cost of the attack. Theoretical analysis shows that the optimal policy in PAMDP induces an optimal state adversary, and the proposed algorithm is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that PA-AD outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments."
SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy-seeking method for multi-agent reinforcement learning (MARL) based on the idea of diversity-driven exploration. Specifically, the authors propose a novel metric to measure the difference between policies, and then propose a constrained optimization formulation for novel policy generation and design algorithm, named Interior Policy Differentiation (IPD), which is derived from the interior point method commonly known in the constrained optimization literature. The proposed method is evaluated on several continuous control benchmarks, showing the strengths of the proposed method."
SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes an end-to-end approach called Visually-Inually-formed Dereverberation of Audio (VIDA) that learns to remove reverberation based on both the observed sounds and visual scene. The visual environment surrounding a human speaker reveals important cues about the room geometry, materials, and speaker location, all of which influence the precise reverberation effects in the audio stream. The authors develop a large-scale dataset that uses realistic acoustic renderings of speech in real-world 3D scans of homes offering a variety of room acoustics. The proposed method achieves state-of-the-art performance and substantially improves over traditional audio-only methods."
SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper studies the problem of extrapolation in transformer language models, i.e., the ability of the model to generalize to sequences that are longer than the ones it was trained on during training. The authors propose a simple and efficient position method, Attention with Linear Biases (ALiBi), which does not add positional embeddings to the word embedding; instead, it biases query-key attention scores with a penalty that is proportional to their distance. They show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory."
SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the multi-objective online convex optimization problem. The authors propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or an L1-regularized solver. They derive regret bounds of both variants and show that the L1 regularized variant enjoys a lower bound. Extensive experiments demonstrate the effectiveness of the proposed algorithm."
SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a generative negative replay approach for continual learning. The proposed approach is based on the idea that the generated data can be used as negative examples (or antagonists) to learn the new classes, especially when the learning experiences are small and contain examples of just one or a few classes. The approach is validated on the CORe50 and ImageNet-1000 benchmark datasets. "
SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,This paper proposes an inductive graph partitioning (IGP) framework across multiple associated graphs of a system or scenario to alleviate the NP-hard challenge. IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. The trained model is then generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines.
SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a multiresolution equivariant message passing graph variational autoencoder (MGVAE) that is end-to-end permutation-equivariant with respect to node ordering. The proposed framework is applied to several graph generation tasks including general graph generation, molecular generation, unsupervised molecular representation learning to predict molecular properties, link prediction on citation graphs, and graph-based image generation. The experiments show that having a flexible clustering procedure from MGN enables MGVAE to generate important graph substructures, especially functional groups."
SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper proposes a framework for nonlinear independent component analysis (ICA) in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. The authors provide an insightful proof of the identifiability of the proposed framework, and verify the theory by experiments on artificial data and synthesized images. "
SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional operator, termed BankGCN, which extends the capabilities of most MPGCNs beyond single ‘low-pass’ features and simplifies spectral methods with a carefully designed sharing scheme between filters. The authors decompose multi-channel signals on arbitrary graphs into subspaces and share adaptive filters to represent information in each subspace. The filters of all subspace differ in frequency response and together form a filter bank. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture."
SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short, to improve the transferability of supervised pretraining methods to downstream tasks. The authors argue that the worse transferability arises from the negligence of valuable intra-class semantic difference. To alleviate this problem, the authors propose a new method that only requires each image to share its class label with most of its k nearest-neighbors, thus allowing each class to exhibit a multi-mode distribution and preserving part of intra class difference for better transferring to the downstream task. Extensive empirical studies on multiple downstream tasks show that the proposed method outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a method to generate facial details that are consistent with any desired target expression from a single image. Given a single input image, a target expression (in this case ‘Happy’), and an initial detailed geometry extracted using FDS (Chen et al., 2019) as input, the proposed method hallucinates facial geometric details consistent with the target expression. The hallucinated details are added to the smooth proxy geometry (marked as ‘Proxy Shading’, also extracted from Chen et al. 2019) to give a detailed geometry with facial details. The detailed geometry is then rendered using Neural Rendering to give the final image."
SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) for disentangling syntactic roles (i.e., subject, object, and syntactic role) from the content of a sentence. The model is trained on the SNLI dataset (Schmidt et al. 2020) and evaluated on the English dataset (SNLI). The authors show that the model is able to disentangle subject and object roles better than classical sequence VAEs and Transformer VAEs, and that realizations of core roles can be separately modified in sentences by modifying the associated latent variables."
SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a method to improve the coordination among agents in multi-agent reinforcement learning (MARL). The authors propose a mechanism for achieving sufficient exploration and coordination in a team of agents. Specifically, agents are rewarded for contributing to a more diversified team behavior by employing proper intrinsic motivation functions. The authors structure agents’ interactions by introducing a novel framework, where at each timestep, an agent simulates counterfactual rollouts of its policy and, through a sequence of computations, assesses the gap between other agents' current behaviors and their targets. Actions that minimize the gap are considered highly influential and are rewarded."
SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of large pre-trained neural network models. The proposed method, Model Editor Networks with Gradient Decomposition (MEND), learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre- trained model. The experiments show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters."
SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper introduces a physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in a compositional manner. Results on both one- and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper presents an analysis of the brain representations of code generated by unsupervised machine learning (ML) models and representations of computer programs in the human brain. They analyze recordings from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They discover brain representations that encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also map brain representations to representations of a suite of ML models that vary in their complexity. They find that the Multiple Demand system, a system of brain regions previously shown to respond to code, contains information about multiple specific code properties and machine learned representations."
SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural direct speech-to-speech translation model, which consists of a speech encoder, phoneme decoder, mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The authors also propose a new method for retaining the source speaker’s voice in the translated speech. The proposed method is based on concatenation-based data augmentation, which is able to retain each speaker's voice for input with speaker turns. Experiments show that the proposed method outperforms the original Translatron by a large margin."
SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of few-shot attribute learning, i.e., the task of learning a set of attributes that are not present in the training set, but which are predicted to be useful for the test set. In particular, the authors propose to use a self-supervised pre-training approach, where the model is trained on the test attributes, and then fine-tuned on the new attributes. The authors also propose random splits of the attribute space to study the generalization ability of the learned attribute representations."
SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle-based method for sampling from unnormalized distributions. The proposed method is based on the Wasserstein gradient flow of relative entropy, which is used to define a path of probability distributions that interpolates the reference distribution and the target distribution. This gradient flow is characterized by an ODE system with velocity fields depending on the density ratios of the evolving particles and the unnormalised target density. The authors propose a nonparametric approach to estimate the logarithmic density ratio using neural networks. Extensive simulation studies on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate the effectiveness of the proposed method."
SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a framework to classify larger, realistic images using quantum systems. The approach relies on a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. The framework is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop and obtains accuracy comparable to classical neural networks with the same number of learnable parameters. The authors also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of final performance."
SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated learning approach to improve the privacy-utility paradox in face recognition. The proposed approach is composed of two components. First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss is introduced to encourage global consensuses among clients, which results in more discriminative features."
SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a new method for transfer learning, called head-to-toe (head2toe) probing, that selects features from all layers of the source model to train a classification head for the target-domain. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows that it outperforms the state-of-the-art fine-tuning method (finetuning) in terms of performance on the target domain. The paper also shows that the proposed method outperforms finetuning on OOD transfer. "
SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes an object-oriented text dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. To improve the robustness of dynamics, the authors identify the objects influenced by input actions and predicts beliefs of object states with independently parameterized transition layers. Experiments show that the proposed model outperforms model-free baselines in terms of sample efficiency and running scores."
SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification, where a label taxonomy has a cost associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. The authors propose a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. They prove that there is a bijective mapping between the original hierarchical cost and the set of layer-wise abstaining losses under symmetry assumptions. They employ the distributionally robust learning framework to solve the learning to abstain problems in each layer. They conduct experiments on large-scale bird dataset and on cell classification problems."
SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes to learn a group of parameterized synperiodic filter banks to process multi-channel sound waveforms in a unified, parameter-frugal and computation-efficient manner. The proposed method is based on a Transformer-like backbone with two parallel soft-stitched branches to learn semantic identity label and spatial location representation semi-independently. Experiments on both direction of arrival estimation task and the physical location estimation task shows that the proposed method outperforms existing methods by a large margin."
SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of unsupervised domain adaptation, where the goal is to gradually shift the model towards the target distribution rather than learning domain invariant representations. The authors propose a method called “Gradual Interpolation of Features toward Target” (GIFT) that creates virtual samples from intermediate distributions by interpolating representations of examples from source and target domains. They show that under two assumptions: (i) access to intermediate distributions, and (ii) samples are annotated with the amount of change from the source distribution, self-training can be successfully applied on gradually shifted samples to adapt the model toward the target distributions."
SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for zero-shot video-text retrieval, where the goal is to retrieve the correct class description prompt from a video. The authors propose a hierarchical attention mechanism to learn representations from videos, video titles, and comments. The model is trained with a transformer-based architecture, with a second layer of attention that is applied per input to assess the amount of information at a higher level of features. Experiments show that the proposed method is able to learn better representations from user comments, and achieves competitive results on standard benchmarks. "
SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes a new intrinsic reward for unsupervised skill discovery. The authors argue that the exploration problem is intrinsic to the entire variational skill discovery algorithms, which can inhibit the discovery of new skills. To address this problem, the authors propose an ensemble of discriminators and reward the policy for their disagreement. The proposed method is evaluated on a tabular grid world and 57 games of the Atari Suite."
SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a molecular graph generation framework based on a construction of a spanning tree and the residual edges. The authors also design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Fréchet ChemNet distance, and fragment similarity."
SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral properties of two-layer polynomial neural networks (PNNs) in the NTK regime. The authors show that the p-th order polynomials (i.e., a parametrization of the PNN) have a spectral bias towards high-frequency components, which leads to a speed-up in learning higher frequencies. They also show that this spectral bias can be verified empirically by extensive experiments. "
SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method to find sparse subnetworks in randomly initialized neural networks (NNs) by searching for a subnetwork at random initialization and then unmasking the disguise by learning to transform the subnetwork’s remaining weights. The method is based on the Lottery Ticket Hypothesis (LTH) of Ramanujan et al. (2020), which shows that dense NNs contain sparse matching subnetwork (called “winning tickets”) capable of training in isolation to achieve high accuracy. This paper extends the LTH by defining a class of subnetwork that is not only “hidden” in the random networks but also “disguised” – hence can only be “unmasked” with certain transformations on weights. It proposes a two-stage algorithm that plays a Peek-a-Boo (PaB) game to identify the disguised subnetwork with a combination of two operations: (1) searching efficiently for a sparse subnetwork; (2) unmasking the disguise. Extensive experiments on CIFAR-10/100 datasets demonstrate the competency of PaB over edge-popup and other counterparts."
SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,This paper proposes a method to defend against adversarial attacks on graph neural networks (GNNs) by jointly cleaning the graph and denoising features of data. The authors propose a General Unified Graph Neural Network (GUGNN) framework to jointly clean the graphs and denoise features. They further extend it by introducing two operations and develop a robust GNN model(R-GNN). Experiments on four real-world datasets demonstrate that the proposed method improves the overall robustness over the state-of-the-art baselines.
SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method learns surface parameterization by learning a continuous bijective mapping between 3d surface positions and 2D texture-space coordinates. The surface parameterisation network can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. The paper shows that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios.
SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes Adaptive Region Pooling (ARP), a novel downsampling algorithm for fine-grained recognition. The idea is to make the network only focus on a smaller but more critical region, and simultaneously increase the resolution of sub-sampled feature. The paper claims that ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. Extensive experiments qualitatively and quantitatively validate the effectiveness and efficiency of the proposed pooling operation."
SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper studies the problem of out-of-distribution (OOD) generalization for node-level prediction on graphs. The authors propose a new domain-invariant learning approach, named Explore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage invariant graph features for prediction. The key difference to existing invariant models is that the authors design multiple context explorers (specified as graph editers in the case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. This design enables the model to extrapolate from a single observed environment which is the common case for node level prediction. "
SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning approach that adaptively selects optimal data augmentation strategies for contrastive representation learning. The proposed approach is based on two criteria: high fidelity and variety. The paper also provides a theoretical analysis on the criteria for selecting feasible data augmentations. Experiments on various datasets show that InfoTS outperforms the baselines."
SP:6bc677d060ba4ab09f6da61458680e7a7976644b,This paper studies the lottery ticket hypothesis (LTH) for deep neural networks (DNNs) and the universality of lottery tickets. The authors show that iterative magnitude pruning (IMP) is a renormalization group (RG) scheme. They show that IMP satisfies the properties required to be an RG operator. They also show that successful transfer experiments for winning tickets have successful transfer properties.
SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the performance gap between cross-attention (CA) and dual-encoder (DE) models for re-ranking in the context of information retrieval. The authors claim that the gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, the authors propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark neural reranking datasets."
SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the relationship between importance sampling (IS) and variance minimization (VM) in policy optimization (PO). In particular, it shows that in the off-policy learning (off-PL) setting, importance sampling can be used as a variance reduction technique, with the goal of finding an optimal behavioral policy. In contrast, in the policy optimization setting, the goal is to estimate the performance of a target policy, given samples collected with a different behavior policy. Theoretical results show that the variance of the importance sampling estimator can be lower than the one achievable when sampling from the target distribution. Based on these theoretical results, the paper proposes a new algorithm, Policy Optimization via Optimal Policy Evaluation (POPE), that employs variance minimisation as an inner loop. Finally, empirical results are presented on continuous RL benchmarks, with a particular focus on the robustness to small batch sizes. "
SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes a method to scale up graph neural networks (GNNs) for predicting per-atom forces and energies on the Open Catalyst 2020 (OC20) dataset. The authors introduce Graph Parallelism, a method that splits up the input graphs across multiple GPUs, enabling them to train very large GNNs with hundreds of millions or billions of parameters. They empirically evaluate the method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude on the OC20 dataset. They show that graph-parallelized models lead to relative improvements of 15% and 21% on the force MAE and AFbT metrics on the IS2RS and S2EF datasets, respectively."
SP:352c177d89b9460acee0c78364e6d9c153c6a93c,This paper proposes an approach to generate long-range text by learning a representation that maps the dynamics of how text changes in a document to dynamics of a stochastic process of interest. This representation is used to generate text by first implicitly generating a document plan and then generating text that is consistent with this latent plan. The proposed approach is evaluated on a variety of text generation tasks and compared to domain-specific methods and fine-tuning GPT2.
SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for object-centric representation learning (OCRL) by learning to predict future scenes in the presence of moving objects. The authors treat objects as latent causes whose function to an agent is to facilitate efficient prediction of the coherent motion of their parts in visual input. The model learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together. The proposed method requires no supervision or pre-training of any part of the network."
SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based model for disentangled representation learning for spatio-temporal data for mobility forecasting. The proposed model is able to achieve state-of-the-art performance on multiple ST datasets. The authors also propose to remove the non-informative features from the learned representations and show that models can benefit from this operation.
SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"This paper proposes a new deep learning framework for probabilistic interpolation of irregularly sampled time series that the authors call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE). The proposed method is based on the Multi-Time Attention Network (UnTAN) model, which is a VAE-based model for continuous-time interpolation. The UnTAN is a multi-time attention network that is able to capture the relationships between dimensions and time. The proposed HeTVAE generalizes the previously introduced mTAN with an additional intensity network that can more directly encode information about input sparsity using an uncertainty-aware multi-head attention layer. The experiments show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models."
SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two measures of modularity, i.e., importance and coherence, to assess the modularity exhibited by a partitioning of a network’s neurons. Specifically, the authors propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and Coherence, that reflects how consistently their neurons associate with features of the inputs. To measure these proxies, they develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. They apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. They show that these partitionings reveal groups of neurons that are important and coherent."
SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. They obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights."
SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones entirely. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and one entirely. This has many benefits such as improving reproducibility, reducing the variance over different experimental runs, and allowing network training without batch normalization. The experiments show that ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, suggesting random weights may be unnecessary for modern network initialization."
SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. The Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. The authors theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data.
SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the question of whether random permutations are optimal in the context of permutation-based SGD. The authors first show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist permutations that offer exponentially faster convergence compared to random. However, for general strongly-convex functions and quadratic functions, the authors show that there are easy-to-construct permutations which lead to accelerated convergence."
SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes an automated normalizing flow (NF) architecture search method to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient optimization of deep flow models."
SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes to measure the expressiveness and learnability of the representations learned by self-supervised learning (SSL) methods by the Intrinsic Dimension (ID) of the dataset in representation space. The authors also introduce Cluster Learnability (CL), defined in terms of the learning speed of a KNN classifier trained to predict K-means cluster labels for held-out representations. They show that high ID and CL correlate with high downstream performance for 30 different pretrained checkpoints across different architectures. To further demonstrate the utility of the framework, they propose modifying DeepCluster (Caron et al., 2018) to improve the learningability of representations."
SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box adversarial attacks such that the perturbations are limited in the salient region. The authors show that state-of-the-art blackbox attacks equipped with segmentation pretors can achieve much better imperceptibility performance with little reduction in query efficiency and success rate. They further propose the Saliency Attack, a new gradient-free black box attack that can further improve the imperceptible performance. The proposed method is evaluated on CIFAR-10 and ImageNet datasets. The results show that the adversarial examples generated by the proposed method are much more imperceptable than the ones generated by other attacks, and are interpretable to some extent."
SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a graph-to-sequence architecture Graph2SMILES to solve the tasks of retrosynthesis prediction and reaction outcome prediction in organic chemistry. The proposed method combines the power of Transformer models for text generation with the permutation invariance of molecular graph encoders that mitigates the need for input data augmentation. It can be used as a drop-in replacement for the Transformer in any task involving molecule(s)-to-molecule(s) transformations. In the encoder, an attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding."
SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical reinforcement learning approach for disease diagnosis in task-oriented dialogues setting. The high level policy consists of a master model that is responsible for triggering a low level model, which consists of several symptom checkers and a disease classifier. The proposed framework imitates a group of doctors from different departments to diagnose a patient together, and each worker acts like a doctor from a specific department. Experimental results on both real-world and synthetic datasets demonstrate that the hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised and personalized federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing the challenges of label deficiency at the edge and data heterogeneity. The authors also develop a distributed training system and related evaluation protocol for SSFL. They conduct experiments on a synthetic non-I.I.D. dataset based on CIFAR-10 and an intrinsically non-i.i.d. dataset GLD-23K. The performance comparison indicates that representation regularization-based personalization method is able to outperform other variants."
SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper attempts to derive a general form of partial differential equations (PDEs) for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show that the adjustment operator is the solution operator of PDEs. Based on this, they design a training method to train DNN models for better robustness and less chance of overfitting. Theoretically, the robustness of DNN trained with our method is certifiable and our training method reduces the generalization gap for DNN."
SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper investigates the emergence of emergent languages in referential games, where agents interact and develop an emergent language to solve a task. The authors propose to use contrastive loss to alleviate the problem of message type collapse, and measure the expressivity of the languages by their generalization performance across different referentially games. The results show that the language's expressivity is a trade-off between the complexity and unpredictability of the context used in the games."
SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a new exploration strategy for reinforcement learning based on the Sample Average Uncertainty (SAU) method developed for bandit problems. The main advantage of SAU is that it only depends on the value predictions, meaning that it does not need to rely on maintaining model posterior distributions. The authors extend the exploration strategy of Rigotti & Zhu (2021) to the general sequential RL setting, which results in a class of exploration strategies called $\delta_2-exploration$ that is implementable with minimal computational overhead and can be applied to both tabular and deep Q-learning settings. Empirical results show the effectiveness of the proposed exploration strategies."
SP:2f6e266b03939c96434834579999707d3268c5d6,This paper proposes a dynamics-aware implicit generative adversarial network (DIGAN) for video generation. DIGAN is based on implicit neural representations (INRs) that encodes a continuous signal into a parameterized neural network. The authors propose two key components: (1) an INR-based video generator that improves the motion dynamics by manipulating the space and time coordinates differently and (2) a motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences. The proposed method is evaluated on UCF-101 and CIFAR-10 datasets and achieves state-of-the-art performance.
SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of differentially private multi-label classification, i.e. the task of releasing the k-hot binary vectors that satisfy a bounded differential privacy guarantee. The authors propose three different mechanisms: Binary, τ, and Powerset voting. Binary voting operates independently per label through composition, while $\tau$ voting operates over the entire binary vector by viewing the possible outcomes as a power set. They theoretically analyze tradeoffs showing that powerset voting requires strong correlations between labels to outperform Binary voting. They also extend the canonical single-label technique: PATE. They empirically evaluate their algorithms on large-scale datasets."
SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions in the computational cost. The authors also discover a non-adaptive learning rate correction to SGD which allows it to train a BERT model to state-of-the-art performance. The invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning."
SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning in sparse-reward settings, where the reward function can only indicate whether the task is completed partially or fully. The authors propose an algorithm, called Learning Online with Guidance Offline (LOGO), which merges a policy improvement step with an additional policy guidance step by using the offline demonstration data. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimal policy, while being able to learn beyond and approach optimality. Theoretical analysis of the algorithm is provided, and a lower bound on the performance improvement in each learning episode is also provided."
SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This paper proposes a hybrid neural Pareto Front (HNPF) method for multi-task learning (MTL) solvers. The proposed method is based on two stages: Stage-1 and Stage-2. The first stage uses a neural network to extract the weak weak front, and the second stage extracts the strong strong front from this weak front. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. "
SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper proposes a method to learn a consolidated image feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks. The authors propose a multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher is sufficient to consolidate representations from task specific teacher(s) and improve downstream performance, outperforming the teacher (or best of all teachers) as well as the strong baseline of ImageNet pre-trained features. The method almost reaches the performance of a multi task joint training oracle, reaping the benefit of the teachers without replaying their training data."
SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. Conformalized Quantile Regression (Romano et al. 2019) is used to learn a single width adjustment parameter that turns a two-sided quantile predictor into a prediction interval of valid coverage. However, it does not offer a way of further optimizing its length or the cardinality of the prediction set. In this paper, the authors propose a meta-algorithm that generalizes the conformalized quantile regression algorithm to the ERM problem by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments show that the proposed algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches."
SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes an approach for the problem of query object localization, in which an agent is trained to localize objects of interest specified by a small set of exemplary images. The authors propose a transferable reward signal formulated using the exemplary set by ordinal metric learning. The proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of the approach."
SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a transformer-based feature matching method for image classification, stereo matching, and object detection. The proposed method builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top-K patches. Experimental results show that the proposed method achieves state-of-the-art performance in various vision tasks."
SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes a method for learning termination conditions of options by maximizing mutual information between options and corresponding state transitions. The authors derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards, combined with intrinsic rewards."
SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a semantic topology approach for open-world object detection (OWO). The idea is to assign all objects from the same category to their corresponding node in the semantic topologies, including the ‘unknown’ category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed approach outperforms the current state-of-the-art OO detectors by a large margin."
SP:97f618558f4add834e5930fd177f012a753247dc,"This paper studies the problem of selecting a subset of the unlabeled dataset for training deep learning models. The authors propose to use the maximization of the submodular function of a convex function to select the subset. They propose a greedy algorithm based on matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. They show that the proposed algorithm can identify informative and diverse subsets of data that lead to deep learning model with similar performance as the ones trained with the original dataset."
SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper proposes two adaptive inference strategies for semantic segmentation that adapts the model to the test sample before producing the final prediction. The first strategy, Instance-adaptive Batch Normalization (IaBN), combines the feature statistics acquired at training time with those of the test samples. The second strategy, Seg-TTT (Test-time Training-Adaptive Training), adapts model parameters to test samples using a self-supervised loss. The authors conduct an extensive empirical study following a rigorous evaluation protocol, allowing them to establish that the two techniques are complementary. The results show that the proposed methods consistently and significantly outperform the baseline and attain a new state of the art, substantially improving in accuracy over previous generalization methods."
SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper considers the task of finding out-of-distribution samples in tabular data, where little can be assumed on the structure of the data. The proposed method learns mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss, which considers only one sample at a time. Once learned, it can score a test sample by measuring whether the learned mappings lead to a small contrastive losses using the masked parts of this sample. The experiments show that the method leads by a sizable accuracy gap in comparison to the literature and that the same default set rule on data from data from a single class provides the best results."
SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,This paper proposes a low-dimensional functional connectivity embedding method for neuropsychiatric neuroimaging. The authors propose a conditional variational auto-encoder that incorporates dual utilisation of diagnostic information in learning the optimal embedding space. The proposed method is evaluated on two datasets and shows promising results in challenging the conventional approaches in low dimensional density estimation.
SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper introduces a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of a parametric tensor-train network for feature extraction and a tensor product encoding (Tensor product encoding). The authors claim that QTN enables an end-to-end parametric model pipeline, from the generation of quantum embeddings to the output measurement. The experiments on the MNIST dataset demonstrate the advantages of the proposed approach over other quantumembedding approaches."
SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method to construct low-dimensional manifolds of the space of neural networks, where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The proposed method, called DYNAMO, takes as input a collection of pre-trained neural networks and outputs a meta-model that emulates the dynamics of the hidden states as well as the outputs of any model in the collection. The specific model to be emulated is determined by a model embedding vector that the meta-models take as input; these model embeddings constitute a manifold corresponding to the given population of models. The authors apply the proposed method to both RNNs and CNNs, and find that the resulting model embedden spaces enable novel applications: clustering, averaging, and semi-supervised learning."
SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a framework for constraint-based learned physical simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The authors implement their method using a graph neural network as the constraint function and gradient descent as a constraint solver. They test the model on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids. The model achieves better or comparable performance to top learned simulators."
SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes a new evolutionary diversity optimization algorithm with clustering-based selection. In each iteration, the policies are divided into several clusters based on their behaviors, and a high-quality policy is selected from each cluster for reproduction. The authors also adaptively balance the importance between quality and diversity in the reproduction process. Experiments on deceptive and multi-modal continuous control tasks show the superior performance of EDO-CS over previous methods."
SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper considers a distributed linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction. The interconnection structure among the agents is described by a time-varying directed graph. The paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. The equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication."
SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes Graph Mechanics Network (GMN) which is equivariant, constraint-aware, and constraint aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. Moreover, the authors developed a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed Equivariant formulation is proved to be universally expressive under certain conditions. Extensive experiments support the advantages of the proposed GMN."
SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposes two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. Theoretical analyses of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that (a) partial model personalization can obtain most of the benefit of full model personalisation with a small fraction of personalized parameters, and, (b) the alternating update algorithm often outperforms the simultaneous update algorithm."
SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a framework for unsupervised object representation learning based on contrastive learning. The main idea is to replace the contrastive augmentation operations in contrastive representation learning by generating viewing sequences in a procedurally generated way, which allows perfect control over the temporal structure of the input. The proposed method, called CLTT, is evaluated on the task of linear classification on the ThreeDWorld (TDW) dataset and compared with several state-of-the-art contrastive methods. The performance of the proposed method is compared with the performance of fully-supervised learning."
SP:2fb4af247b5022710b681037faca2420207a507a,"This paper extends the AlphaZero with Hindsight Experience Replay (HER) to tackle goal-directed planning tasks. HER is a reinforcement learning algorithm that can be used with any policy-based RL algorithm to tackle the problem of sparse-reward functions. In particular, the authors propose to use a reward function based on state distance in the policy space to generate local-optimal solutions since a reward based on a state distance from the goal state is not possible for every task. The proposed algorithm is evaluated in several simulated domains, including a novel application to a quantum compiling domain."
SP:e2d33c7331db7f52b84ad1018152564d91a9f126,This paper proposes a method for continual learning (CL) of neural networks without access to historical data. The proposed method is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer (FEL) that represents different network structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines and achieves new state-of-the-art performance.
SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper investigates the properties and applications of aligned generative models for image-to-image translation. The authors empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, they find that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Then, equipped with this better understanding, they leverage aligned models to solve a diverse set of tasks. In addition to image translation, they demonstrate fully automatic cross-domain image morphing. They further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain."
SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a new Gromov-Wasserstein (GW) distance based on Optimal Transport (OT) distance, which relaxes the coupling between all the nodes from the two considered graphs. The authors argue that this property can be detrimental for tasks such as graph dictionary or partition learning, and propose a new semi-relaxed GW divergence. Theoretical properties of this divergence are discussed, and it is shown that it can lead to an efficient graph dictionary learning algorithm. Empirically, the authors demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion."
SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method to model systematic suboptimality in human behavior, i.e. when the deviations from optimal behavior are not independent, but instead consistent over time. The authors introduce the Boltzmann policy distribution (BPD) as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data."
SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free quantization (DFQ) framework for convolutional neural networks (CNNs) that can be applied to inference-only devices with low computation and memory requirements. The authors propose a novel objective function called Constrained Absolute Sum of Error (CASE) that does not need any dataset and is aware of network architecture. They also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver. Finally, without fine-tuning and synthetic datasets, the proposed SQuant accelerates the data free quantization process to a sub-second level with > 30% accuracy improvement over the existing DFQ works."
SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a novel approach to time series segmentation, i.e., splitting time series into segments that correspond to given categories. The proposed approach is based on a bi-pass neural network architecture with several structures that can process information in a multi-scale fashion. In particular, the proposed approach finds precise breakpoints, obviates sliding windows, handles long-term dependencies, and it is insensitive to the label changing frequency. Experiments are conducted to evaluate the effectiveness of the approach."
SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a new explanation method for graph neural networks (GNNs) based on decomposition. Specifically, the authors propose to decompose the information generation and aggregation mechanism of GNNs, which allows tracking the contributions of specific components of the input graph to the final prediction. They further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. Finally, they conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of their method."
SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper introduces DiffStride, the first downsampling layer with learnable strides. This layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of the solution."
SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on a novel localized randomized smoothing approach, where the random perturbation strength is proportional to their importance for the outputs. The resulting locally smoothed model yields strong collective guarantees while maintaining high prediction quality on both image segmentation and node classification tasks."
SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain-specific inductive biases into normalizing flows for density estimation. The method consists of two parts: (1) a bijective transformation layer that converts differentiable probabilistic models into equivalent transformations, and (2) a gated layer that bypasses the parts of the models that fail to capture the statistics of the data. The paper shows that the proposed method is able to induce desirable properties such as multimodality, hierarchical coupling and continuity, and enables a high performance form of variational inference."
SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the ability of large pre-trained language models (LMs) to learn to map an entire conceptual domain (e.g., direction or color) onto a grounded world representation given only a small number of examples. The authors show that the largest model (a GPT-3 model containing 175B parameters) can indeed learn groundings in the conceptual worlds. They also show that although the smaller models struggle to perform this mapping, the largest models can learn to ground the concepts that it is explicitly taught, but appears to generalize to several instances of unseen concepts as well."
SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the effect of shaped rewards in emergent language research. In particular, the authors propose a simple navigation game where a sender passes a one-word message to the receiver and the receiver receives a message from the sender. They show that shaped rewards can bias the semantics of the learned language, significantly change the entropy of the language, and mask the potential effects of other environmental variables of interest. They also propose a mathematical model for understanding the role of experience buffer size in the entropy and semantics of emergent languages."
SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. Experiments show that IWDA outperforms the more popular semi-supervised learning methods under large prior shifts, and can be additionally combined with them for further performance gains. "
SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes an algorithm for meta-learning, which first bootstraps a target from the learner, then optimizes the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. The paper provides conditions under which the proposed algorithm achieves better performance and shows that the metric can control meta-optimization. The authors also show that the bootstrapping mechanism can extend the effective meta learning horizon without requiring backpropagation through all updates. The algorithm achieves a new state-of-the art for model-free agents on the Atari ALE benchmark."
SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper investigates the generalization ability of model-based RL agents in comparison to their model-free counterparts. The authors evaluate the performance of MuZero on both procedural and task generalization. They identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, MuZero achieves state-of-the-art generalization performance and data efficiency on Procgen and ML-1/ML-45. However, they find that these factors do not always provide the same benefits for task generalisation in Meta-World, indicating that transfer remains a challenge."
SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph deconvolution network (GDN) to learn the distribution of latent graphs from data. The authors formulate the graph learning task as a network inverse (deconvolution) problem, and unroll and truncate proximal gradient iterations to arrive at a parameterized neural network architecture that the authors call a Graph Deconservolution Network (G DN). GDNs can learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. Experiments on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the robustness and representation power of the model."
SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes a framework for reward shaping in reinforcement learning (RL). The framework is based on the idea of two-player nonzero-sum Markov decision process (MDP) between two independent learners with distinct objectives. In particular, the two agents have distinct learning agendas, cooperate to achieve the other agent’s objective and learn to construct shaping rewards by observing Controller (the agent whose goal is to solve the environment task) while Controller learns to solve its task. The framework generates tailored shaping rewards without the need for domain knowledge or manual engineering. The authors demonstrate the effectiveness of the proposed framework in three carefully designed experiments in challenging sparse reward environments."
SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a novel training and inference framework for vertical federated learning (VFL) to improve the robustness of VFL. In particular, the authors propose a novel feature subspace recovery (RVFR) method to recover the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. The authors also defend against inferencephase adversarial and missing feature attacks. Experiments on NUS-WIDE and CIFAR-10 datasets demonstrate the effectiveness of the proposed method."
SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper proposes to use contrastive learning as a pretext task to train unsupervised dense retrievers for document retrieval. The paper shows that the proposed method outperforms BM25 on 8 out of 14 datasets on the BEIR benchmark. It also shows that fine-tuning the model on a few thousand examples of the MS MARCO dataset leads to strong improvements compared to BM25.
SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the trade-off between fine-tuning and linear probing when transferring a pretrained model to a downstream task. The authors show that fine-tuneing can achieve worse accuracy than linear probing in-distribution (ID), especially when the pretrained features are good and distribution shift is large. They theoretically analyze the tradeoff arising in overparameterized two-layer linear networks, characterizing how fine tuning can distort high-quality pre-trained features which leads to low OOD accuracy. Their analysis suggests that the simple two-step strategy of linear probing then full fine tuning could achieve better ID and OOD performance. "
SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning to discover novel classes (L2DNC), where the goal is to cluster novel classes into a set of known classes and unlabeled data from unseen classes, and train a clustering model for the unseen classes. The authors show that the problem is well-defined if the novel and known classes are sampled in causality, i.e., the novel classes and the known classes share high-level semantic features. Then, the authors propose a meta-learning algorithm that has exactly the same assumption as L2dNC. Empirical results show the effectiveness of the proposed algorithm. "
SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper studies the problem of combining offline and online data in the Partially-Observable Markov Decision Process (POMDP) setting. In this setting, the learning agent has the ability to collect online experiences through direct interactions with the environment (interventional data) and also has access to a large collection of offline experiences obtained from observing another agent (observational data). The observed agent is allowed to act based on privileged information, hidden from the learner, and the goal is to learn a causal transition model that explains both the interventional and observational regimes. The authors propose a general yet simple methodology for safely leveraging offline data during learning. In a nutshell, the method relies on learning a latent-based causal model and then inferring the standard POMDP transition model via deconfounding using the recovered latent variable. "
SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper proposes a new approach for open-ended question answering (QA) and abstract generation (EG) tasks. The authors propose to use an additional guide retriever that is allowed to use the target output and “in hindsight” retrieve relevant passages during training. They model the guidance retriever after the posterior distribution Q of passages given the input and the output, and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q. They show that the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passages (19% relative improvements), and the end-to-end system produces better overall output (6.4% overall improvement)."
SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper studies the problem of influence maximization, i.e., estimating the number of nodes influenced by a given seed set. The authors propose a graph neural network (GNN) to parameterize the upper bound of the influence estimation and train it on small simulated graphs and then apply it to large graphs. They show that the GNN is able to provide accurate influence estimation for real graphs up to 10 times larger than the train set. They also develop a version of Cost Effective Lazy Forward optimization with GLIE instead of simulated influence estimation, surpassing the benchmark for influence maximisation."
SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper considers the problem of active learning for general loss functions under the assumption of Lipschitz functions. The authors propose to use the concept of discrepancy distance between source and target distributions to restrict the maximization over the hypothesis class to a localized class of functions that are performing accurate labeling on the source domain. They derive generalization error bounds for such active learning strategies in terms of Rademacher average and localized discrepancy for general losses which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds.
SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks. The approximation relies on a central result from singular learning theory according to which the posterior distribution over the parameters of a singular model, following an algebraic-geometrical transformation known as a desingularization map, is asymptotically a mixture of standard forms. From here, the authors demonstrate that a generalized gamma mean-field variational family, following desedularization, can recover the leading order term of the model evidence."
SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper presents a learning-theoretic generalization bound for domain generalization (DG) that bounds novel DG performance in terms of the model’s Rademacher complexity. The authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. The analysis suggests that domain generalisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective."
SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper introduces the maximum n-times coverage problem, which is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. The authors also introduce NTIMES-ILP and MARGINALGREEDY, efficient algorithms for solving the problem on both synthetic data and real-world vaccine design. They show that the proposed method achieves superior predicted population coverage when compared to 29 previous published vaccines for COVID-19 with less than 150 peptides."
SP:11ad277db038a77d5935e7504cc640e74bfc4efe,This paper proposes a new weight initialization method for spiking neural networks (SNNs) based on the slant asymptotic formula for the response curve of spiking neurons. The proposed method is evaluated on the MNIST and CIFAR-10 datasets and compared with other SNN initialization methods. The authors claim that the proposed method can improve the training speed and the model accuracy compared with the existing methods.
SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper studies the problem of distribution shift in federated learning. The authors propose to model the distribution shift with a mixture of distributions that gradually changes between daytime and nighttime modes. They propose a Federated Expectation-Maximization algorithm enhanced by Temporal priors of the shifting distribution (FedTEM) that jointly learns a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments for image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of the distribution shifts and significantly improve the final model performance."
SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a method for pruning deep neural networks (DNs) based on the analysis of the spline of the input-output mapping (splines) of the network. The authors show that the splines of DNs exhibit an early-bird (EB) phenomenon, i.e., the partition of splines converges at early training stages. Based on this insight, the authors propose a pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate the effectiveness of the proposed method."
SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper studies the problem of fair representation learning, i.e., learning representations that are invariant with respect to different subgroups. The authors formulate the problem as a bi-level optimization, where the representation is learned in the outer-level and invariant optimal group predictors are updated in the inner-level. To avoid the high computational and memory cost, the authors propose the implicit path alignment algorithm, which only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. The proposed method is demonstrated to fulfill the sufficiency rule, which is desirable in various practical scenarios but was not commonly studied in fair representations learning. The error gap of the implicit approach is analyzed and empirically validated in both classification and regression settings."
SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the importance of different design decisions in reinforcement learning via supervised learning (RvS) methods, such as policy architectures and how the conditioning variable is constructed, and empirically investigates the design decisions, performance, and limits of such methods. The authors find that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data. The most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (goal or rewards)."
SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method to predict human exploration behavior by leveraging Bayesian inference about the structure of unobserved space (which they term map induction). They also show that map induction can improve performance of a Partially Observable Markov Decision Process (POMDP) planner during exploration. The method is based on a distribution of strong spatial priors. The authors also introduce a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models. "
SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"This paper presents a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. Existing NBP methods rely on domain-specific features encoded in the probablistic factors of a graphical model. In this work, the authors replace each crafted factor with a neural network enabling the factors to be learned using an efficient optimization routine from labeled data. The method learns to maintain a set of marginal posterior samples using end-to-end training. The effectiveness of the proposed method is demonstrated on two simulated articulated tracking tasks and on a real-world hand pose tracking tasks."
SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based generative model for molecule optimization. The proposed model is able to support scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches."
SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper studies the problem of imitation learning for deterministic experts. In particular, the authors propose to reduce the imitation learning problem to a single instance of reinforcement learning problem, where the reward is defined to be one for state-action pairs from the expert trajectory and zero for other state-actions. Theoretical analysis shows that the proposed method recovers the expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments on continuous control tasks confirm that the reduction works well in practice."
SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the over-parameterized worst-group performance of various reweighting algorithms for fairness. The authors prove that for almost all reweighted algorithms, the model always converges to the same ERM interpolator that fits all training samples, which means that the implicit biases of these algorithms are equivalent to that of ERM. Then, the authors analyze whether adding regularization helps fix the issue, and show that for regularization to work, it must be large enough to prevent the model from achieving small training error. "
SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes to use the Rational Inattention (RI) model to model the cost of cognitive information processing in multi-agent reinforcement learning (MARL). The proposed RIRL framework generalizes and is more flexible than prior work by allowing for multi-timestep dynamics and information channels with heterogeneous processing costs. The authors evaluate RirL in Principal-Agent (specifically manager-employee relations) problem settings of varying complexity, where RI models information asymmetry (e.g. it may be costly for the manager to observe certain information about the employees). The authors show that using RI yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. For instance, some forms of a Principal’s inattention can increase Agent welfare due to increased compensation, while other forms of inattentive can decrease Agent welfare by encouraging extra work effort."
SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a phase-oriented approach for generating imperceptible adversarial perturbations for ASR. Specifically, the authors leverage spectrogram consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbation to the adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram. Moreover, they propose a weighted loss function to improve the imperceptibility of PhaseFool. Experimental results demonstrate the effectiveness of the proposed method."
SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper studies the representation learning in non-contrastive self-supervised learning (nc-SSL). In particular, the authors analyze a generalized version of DirectPred (Tian et al. (2021)). They show that in a simple linear network, DirectSet(alpha) provably learns a desirable projection matrix and reduces the sample complexity on downstream tasks. Inspired by their analysis, they simplify DirectPred by removing the expensive eigen-decomposition step. "
SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a gradient-based method for learning long-term dependencies in sequential tasks. The proposed method is based on a time-discretization of a multiscale ordinary differential equations (ODEs). The authors derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem. They also prove that LEM can approximate a large class of dynamical systems to high accuracy. The empirical results on image and time-series classification, dynamical system prediction, keyword spotting and language modeling demonstrate the effectiveness of the proposed method."
SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes a geometric deep learning framework that is rotation and permutation-equivariant for small point clouds. The proposed method is composed of a set of products of terms from the geometric algebra and reductions over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. The authors demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology."
SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem in real-time, which is a combinatorial optimization problem in supply chain management. The proposed method is based on the edge-feature-embedded graph attention mechanism, which considers the high-dimensional edge features and accounts for the heterogeneous information, which are important characteristics of the studied optimization problem. The model is also size-invariant for problem instances of any scale, and it can address cases that are completely unseen during training. Experiments show that the model substantially outperforms the baseline heuristic method in optimality."
SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper proposes a framework to infer concepts out of the dialogue context (CODC) for dialogue summarization. The proposed framework consists of a CODC inference module leveraging external knowledge from WordNet and a knowledge attention module aggregating the inferred knowledge into a neural summarization model. The authors also propose a new evaluation metric based on CODCs to evaluate the inference capability of different methods. Experiments suggest that current automatic evaluation metrics of natural language generation may not be enough to understand the quality of out-of-context inference in generation results, and the proposed model can provide statistically significant improvements over both CIDEr and BLEU."
SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper considers the question of whether entity embedding models can capture logical dependencies in a less demanding sense. Specifically, the authors consider the setting where the embedding of an entity has to be learned by pooling the embeddings of its known attributes. They first show that some of the most popular embedding methods are not able to capture basic logical dependencies. However, they also find that some embedding strategies are capable, in principle, of modelling both monotonic and non-monotonic attribute dependencies."
SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of transformer-based language models on three reasoning tasks: mathematical reasoning, commonsense reasoning, and logical reasoning. The paper discusses the successes and limitations of transformers on these tasks, of both empirical and theoretical nature. In particular, the paper describes the pitfalls that all models need to handle in order to reason on natural language text, describes the theoretical limitations of transformer architecture, and shows, in Sections 4.2-4.3, that they impact natural language reasoning."
SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper introduces the compositional problem graph (CPG) as a framework for learning compositional generalization. CPG is a graph of problems with shared subproblems, where each problem can be decomposed into sub-problems and a set of partial solutions. The authors propose a framework to learn compositional algorithms for composing these partial solutions to solve a larger problem. The proposed approach is evaluated on symbolic and high-dimensional domains, where it is shown to generalize better than baselines that are not explicitly compositional."
SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IP) technique, which integrates the activation pruning with the weight pruning. Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore improves execution efficiency. The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets."
SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a feature selection method that builds on the knockoff framework by adapting the Generative Adversarial Networks framework to generate knockoffs with no assumptions on the feature distribution. The proposed method consists of 4 networks, a generator, a discriminator, a stability network, and a power network. The authors demonstrate the capability of their model to perform feature selection, showing that it performs as well as the originally proposed knockoff generation model in the Gaussian setting and outperforms the original model in non-Gaussian settings, including on a real-world dataset."
SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes a new pruning method for Winograd convolution. The authors first prune the spatial-domain weights in a structured way, in which the structures are designed to transfer the sparsity from the spatial (original) domain to the winograd domain efficiently. Then, the weights of the pruned layers will be converted to and kept in the winning domain. Finally, the authors perform pruning and retraining entirely in the wingrad domain to improve the performance. "
SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,"This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed method, Adversarially Learned Mixture Model (AMM), is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available."
SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes an unbiased estimator for Monte Carlo Monte Carlo integration that combines REINFORCE and reparameterization for variational inference and maximum likelihood estimation. The proposed estimator is unbiased, exhibits low variance, and has low computational complexity. The variance-reduction mechanism of the estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric “self-control” baseline function. The estimator achieves adaptive variance reduction by merging two expectations via common random numbers. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding VAEs and MLE estimation."
SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a modular probabilistic programming language (PPL) that allows to package a probablistic model such as Bayesian nonparametric models as fundamental building blocks. The proposed approach is based on the framework of variational inference, where the pre-specified inference methods can be transparently used for inference of the whole probablity model. The authors demonstrate the power and convenience of the proposed approach with various examples of Gaussian process models."
SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method for pruning large neural networks at initialization. The main idea is to use a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. The method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks."
SP:986b9781534ffec84619872cd269ad48d235f869,"This paper presents an empirical study of the behavior of beam search for neural sequence synthesis. The authors show that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. However, as beam width increases, such sequences are more likely to have a lower evaluation score. Based on the empirical analysis, the authors propose two methods to constrain the beam search from taking high-variance decisions early in the search. The experiments show that constrained beam search effectively eliminates the performance degradation and leads to higher evaluation scores."
SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,This paper proposes a method to improve the sample efficiency of model-free reinforcement learning (RL) by using a single demonstration to construct a curriculum for a given task. The idea is to start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. The paper analytically characterizes the types of environments where Backplay can improve training speed and demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman). Empirical results show that Backplay compares favorably to other competitive methods known to improve sample efficiency. 
SP:426c98718b2dbad640380ec4ccb2b656958389bc,"This paper proposes a multi-layer pruning method (MLPrune) that can automatically decide appropriate compression ratios for all layers. It uses efficient approximation of the Hessian as the pruning criterion, based on a Kronecker-factored Approximate Curvature method. Experiments show that it can compress AlexNet and VGG16 by 25x without loss in accuracy on ImageNet."
SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper presents an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. It first identifies a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, it investigates the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. Finally, it quantifies the causal effect of these units by measuring the ability of interventions to control objects in the output."
SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper proposes a metric to measure the distance between two neural networks in function space. The metric is based on the L2 distance between the outputs of two networks when given the same inputs. The paper also proposes a learning rule for supervised learning that constrains how much a network’s function can change in any one update. The proposed method, called HCGD, penalizes each step of SGD to reduce the magnitude of the resulting step in L2-space."
SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a deep generative Markov model (DyMoN) to model biological data. The model is trained as a neural network framework trained to predict the near-future state of a given cell given its current state. The authors claim that DyMoN is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. The proposed model can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. "
SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"This paper proposes a method for unsupervised image classification based on graph Laplacian. Unlike the widely used classification method, this architecture does not require the labels of data and the number of classes. The key idea is to introduce a approximate linear map and a spectral clustering theory on the dimension reduced spaces into generative adversarial networks (GANs). The proposed framework can classify and also generate images as the human brains do."
SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning representations of sets that are permutation-invariant, i.e. operations on sets should be permuted end-to-end. To achieve this, the authors propose a Permutation-Optimization (PO) module that learns how to permute a set in the forward pass of a neural network using pairwise comparisons. The authors demonstrate the model’s ability to learn permutations and set representations with either explicit or implicit supervision on four datasets, on which they achieve state-of-the-art results."
SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper introduces Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. In contrast to previous works, the embedding in PSN deems samples of a given class to form an affine subspace. The authors show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification. Moreover, the PSN approach has the ability of end-to-end learning."
SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"This paper proposes a meta-learning method for fast adaptation of models to new tasks. The key idea is to partition the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. The proposed method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems."
SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"This paper proposes a framework where the user controls what characteristics of the data they want to share (utility) and what they wish to keep private (secret), without necessarily asking the utility provider to change its existing machine learning algorithms. The utility providers are willing to collaborate with the sanitization process. The authors study space-preserving transformations where utility provider can use the same algorithm on original and sanitized data, a critical and novel attribute to help service providers accommodate varying privacy requirements with a single set of utility algorithms."
SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation of GANs (PAGAN) to improve the stability of the training. The main idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. The proposed method is evaluated on the MNIST, Fashion-MNIST, CIFAR10 and CELEBA datasets. The results show that the proposed method preserves the original GAN objective, does not bias the optimality of discriminator and encourages the healthy competition between the generator and discriminator."
SP:c210982ccdd134d4b293dbe144990398eefe1a86,"This paper proposes a rotation-equivariant convolutional neural network (R-CNN) to predict neural responses to natural stimuli in mouse primary visual cortex (V1) using two-photon imaging. The R-CNN is trained to predict responses of a population of 6000 neurons to natural images recorded in V1. The authors show that the proposed method outperforms a regular CNN with the same number of feature maps and reveals a number of common features, which are shared by many V1 neurons and are pooled sparsely to predict the neural activity."
SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper proposes a distributed algorithm for robust, communication-efficient deep learning, called SIGNSGD, which aggregates gradients by majority vote and sends back only the majority decision. Theoretical results show that the algorithm converges in the large and mini-batch settings, establishing convergence for a parameter regime of ADAM as a byproduct. The authors also prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The proposed algorithm uses 32x less communication per iteration than full-precision distributed SGD."
SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a conditional WGAN-based approach to generate realistic and high-fidelity stock market data. The authors model the order stream as a stochastic process with finite history dependence, and employ a conditional GAN to capture history dependence of orders in a stock market. They test their approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data."
SP:ba66503753b3c57781b435c55c47fc9f69450e65,"This paper considers the problem of learning in a noisy environment with perturbed rewards, where the observed rewards by RL agents are generated with a reward confusion matrix. The authors propose an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed reward. The framework draws upon approaches for supervised learning with noisy data and defines a set of unbiased surrogate rewards. Extensive experiments on different DRL platforms show that policies based on the estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines."
SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the effect of network width and depth on the convergence of RNNs. The authors show that the convergence rate is a function of the number of steps, the average step size, and the average angle between gradient vectors and the path that connects current weights to final wights. They also provide a simple theoretical analysis for a simplified problem of LNN and show that direct distance is expected to shrink as models get wider."
SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper proposes a framework for learning optimal algorithms for combinatorial optimization problems using reinforcement learning. The authors introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst case. They test their new ideas on the AdWords problem, the online knapsack problem, and the secretary problem. The results indicate that the models have learned behaviors that are consistent with the optimal algorithms."
SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper re-examines several domain-specific components that modify the agent’s objective and environmental interface. The authors investigated whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature. They found that performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes adaptive components performed better. They then investigated the main benefit of having fewer domain specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system."
SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for vision-and-language navigation (VLN) task. The proposed method consists of two components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images, and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. Experiments on the VLN benchmark show that the proposed method outperforms the state-of-the-art."
SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,"This paper proposes two methods to improve the performance of encoder-decoder-style neural program synthesis approaches. The first method, execution-guided synthesis, is based on the idea that executing a partial program can result in intermediate states, and thus, synthesizing the rest of the program can be conditioned on these intermediate states so that the synthesizer can take the state changes into account in the followup program generation process. The second method, synthesizer ensemble, leverages the semantic information to ensemble multiple neural program synthesizers. Experiments on the Karel dataset show that the proposed methods can boost the accuracy from around 77% to more than 90%."
SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the effect of batch normalization (BN) on the stability, convergence and acceleration properties of gradient descent (GD) in the setting of ordinary least squares (OLS) regression. In particular, the authors study the convergence and stability properties of BN on the OLS problem. Theoretically, they show that gradient descent with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, acceleration effects, as well as insensitivity to the choice of learning rates. They also demonstrate numerically that these findings are not specific to OLS and hold qualitatively for more complex supervised learning problems. "
SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,"This paper presents a new theoretical perspective of data noising in recurrent neural network language models (RNNs). The authors show that each variant of data-noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). Based on this insight, the authors propose a more principled method to apply at prediction time and propose natural extensions to data noizing under the variational framework. In particular, they propose variational smoothing with tied input and output embeddings and propose element-wise variational smooothing. The authors empirically verify their analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data noisings methods."
SP:f4a914d3df1a5a21a7365ba78279420f39210884,"This paper proposes a classifier-agnostic saliency map extraction method for object localization. The idea is to find all parts of the image that any classifier could use, not just one given in advance. The authors argue that the strong dependence on a given classifier lies at the center of the problem. To tackle this directly, the authors propose to train a saliency mapping that is not strongly coupled with any specific classifier. The proposed approach results in a neural network based saliency maps that only depends on an input image. Experiments on ImageNet show that the proposed approach outperforms the existing weakly-supervised localization techniques."
SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a method for transferring the knowledge of multiple deep nets (teachers) to a new student model (the student) that is independent of the teachers. The structure of the teacher and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces. The proposed method is evaluated on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other methods."
SP:a72072879f7c61270d952f06d9ce995e8150632c,This paper proposes to perform soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. The authors cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence constraints between the original data streams and the compact model are closely bounded. They provide theoretical guarantees concerning the convergence of the proposed learning algorithm.
SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a variational autoencoder with arbitrary conditioning (VAEAC) model for learning conditional distributions of the form p(xI |xU\I) where U is the set of all features and I is its arbitrary subset. The model is trained using stochastic gradient variational Bayes (SGD) and can be conditioned on an arbitrary subset of observed features and then sample the remaining features in “one shot”. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems shows the effectiveness of the proposed approach and diversity of the generated samples."
SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"This paper proposes a method to reduce the memory footprint of deep neural networks (DNNs) by replacing the full-precision activations in the forward pass of backpropagation with approximations. The proposed method is motivated by distributed training algorithms that succeed despite working with approximate and noisy gradients aggregated across multiple devices (Recht et al., 2011; Dean et al. 2012; Seide et al, 2014; Wen et al,. 2017). The authors propose to use 8- and 4-bit fixed-point approximation of 32-bit floating-point activations of activations instead of the full 16-bit ones."
SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"This paper proposes a method for high-resolution face completion. The authors propose a coarse-to-fine attentive module network architecture. The model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components. The proposed method can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x 1024 resolution."
SP:a300122021e93d695af85e158f2b402d21525bc8,"This paper provides a theoretical analysis of the impact of the choice of accumulation precision in deep learning training on the performance of FPUs. Specifically, the authors propose an upper bound on the reduction of complexity of multiply-accumulate units based on the accumulation precision. They show that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, and derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. They apply their analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet."
SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"This paper studies the risk convergence and asymptotic weight matrix alignment properties of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In particular, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with decreasing step sizes): (i) the risk converges to 0; (ii) the normalized ith weight matrices are aligned across layers, meaning |v>i+1ui| → 1. In the case of the logistic loss, the first right singular vector v1 of W1 is aligned with the data, meaning v1 converges  to the unique maximum margin predictor defined by the data."
SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper proposes to extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona based HRED generator (PHRED) and a conditional discriminator. The authors also explore two approaches to accomplish the conditional discriminators: (1) PHREDGANa, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) PHredGANd, a dual discriminator system that collaboratively predicts the attribute(s) that generated the input utterance. Experiments are conducted on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends."
SP:017b66d6262427cca551ef50006784498ffc741d,"This paper proposes a goal-driven collaborative task between two agents, where the goal is to reconstruct the original image of the Teller and the target image drawn by the Drawer. The two agents communicate via natural language. The authors collect the CoDraw dataset of ∼10K dialogs consisting of ∼138K messages exchanged between human agents. They define protocols and metrics to evaluate the effectiveness of learned agents on this testbed, highlighting the need for a novel crosstalk condition which pairs agents trained independently on disjoint subsets of the training data for evaluation."
SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a new approach to learning neural random fields (NRFs) by using neural networks to implement potential functions in undirected models. The approach is based on the idea of minimizing some divergence between the target random field and the auxiliary generator. The authors show that the proposed approach can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically show that it represents the best-performed random fields in these tasks."
SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper studies the problem of adversarial attacks on graph neural networks for node classification. The authors propose a meta-learning-based approach to attack the graph convolutional networks, treating the graph as a hyperparameter to optimize. They show that the perturbations created by their algorithm can misguide the graph neural network such that they perform worse than a simple baseline that ignores all relational information. The attacks do not assume any knowledge about the target classifiers."
SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper proposes a new generative model, called Cramer-wold auto-encoder (CWAE), which is a combination of sliced-Wasserstein autoencoder (SWAE) and WAE-MMD (WAE using maximum mean discrepancy based distance function). CWAE cost function is based upon a characteristic kernel, which has a simple closed-form in the case of normal prior. CWAE performance matches quantitatively and qualitatively that of SWAE and often improves upon SWAE."
SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper studies the problem of many-class few-shot (MCFS) classification in both supervised learning and meta-learning scenarios. The authors propose a memory-augmented hierarchical-classification network (MahiNet) for MCFS learning. It addresses the “many-class” problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine classes and is cheaper to obtain. MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a CNN-based attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes. The MLP extends the linear classifier, the attention module extends a KNN classifier and the convolution module is a memory module. Different training strategies are designed for supervised learning. Two new benchmark datasets “mcfsImageNet” and “McfsOmniglot” are also proposed for the MCFS problem."
SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes Structural-Jump-LSTM, a neural speed reading model that can skip and jump during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one able to exploit punctuation structure (sub-sentence separators, sentence end symbols, or end of text markers) to jump ahead after reading a word. An extensive experimental evaluation against all state-of-the-art neural reading models (Seo et al. 2018; Yu et al., 2017; Fu & Ma, 2018; Huang et al 2017; 2017), shows that the proposed model can achieve the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or improving it compared to a vanilla RNN that reads the whole text."
SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a nonlinear radial basis convolutional feature transformation by learning the Mahalanobis distance function. The proposed method maps the input features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, which prevents small adversarial perturbations from forcing a sample to cross the decision boundary. Experiments are conducted on three publicly available image classification and segmentation data-sets."
SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a novel method for on-policy temporally consistent exploration for deep RL agents. The proposed method is based on the idea of dropout, which is used as a global random variable for conditional distribution. Two factors, gradients’ alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy’s stable improvement. The experiments demonstrate that the proposed method can solve tasks with sparse reward while naive exploration and parameter noise fail."
SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes the nonlinearity coefficient (NLC) as a gradient-based metric that can be computed before training and can robustly predict the performance of the network after training is complete. The NLC combines the Frobenius norm (defined at the top of page 3) of the neural network with the input data and global variability of the outputs into a single metric that is tied to many important properties, including its simplicity, its cheapness, and its robustness to a range of confounders and architectural design choices. The authors demonstrate the properties of the NLC via an extensive empirical study covering a wide range of network architectures. The scope of the experiments exceeds that of the vast majority of related work."
SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies the phenomenon of bias amplification in binary classifiers, where a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. The authors demonstrate that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive “weak” features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification – a previously unreported form of bias that can be traced back to the features of a trained model. To mitigate bias amplification, the authors propose two new feature selection algorithms that are designed to mitigate bias. The experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy."
SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the generalization properties of neural networks with a margin-based perspective. The authors show that the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, and that increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks. In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees. "
SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"This paper proposes a GAN model that allows to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. Experiments on the Multi-MNIST, CLEVR, and the more complex MSCOCO data set show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations."
SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a representation learning and model-based reinforcement learning (RL) method, SOLAR, which jointly optimizes a latent representation and a linear-quadratic regulator (LQR) to improve the performance of a local model method. The main idea is to learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. The proposed method is evaluated on a suite of robotics tasks, including a manipulation task on a real Sawyer robotic arm directly from camera images."
SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"This paper proposes a method for counterfactual reinforcement learning, where the goal is to improve the performance of a model-based RL algorithm. The proposed method, Counterfactually-Guided Policy Search (CF-GPS), leverages structural causal models (SCMs) to model the environment with two ingredients: (1) independent random variables that cannot be influenced by the agent, and (2) deterministic transition functions (also called causal mechanisms) that take these scenarios, together with the agent’s actions, as input and produce the predicted outcome. The main idea is that, instead of running an agent on scenarios sampled de novo from a model, scenarios are inferred in hindsight from given off-policy data, and then evaluate and improve the agent on these specific scenarios using model-base predictions in inferred scenarios. As a result, this approach allows explicitly to trade-off historical data for model bias. The authors empirically show empirically in a conceptually simple setting, where unknown initial states are inferred and reused in hindsight evalute to learn new policies, that this can mitigate model mismatch."
SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,This paper studies the relationship between adversarial robustness and the geometry properties of the decision surface in input space. The authors show that the geometry property of decision surface correlates well with the robustness against adversarial attacks. They further propose a robust training method that aims to smooth the decision surfaces and enhances the adversarial generalization. 
SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The authors formulate the training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. They integrate the quantitative DNN energy estimation into the training process to assist the constrained optimization. Compared to the best prior energy-saving methods, their framework trains DNNs that provide higher accuracies under the same or lower energy budgets."
SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper proposes a Bayesian policy optimization method for continuous Bayes-Adaptive Markov Decision Process (BAMDP) problems. The proposed method builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, the authors propose a new policy network architecture that encodes the belief distribution independently from the observable state. The method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with POMDP solvers."
SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a unified framework for unsupervised representations of entities and their compositions, by viewing each entity as a histogram (or distribution) over its contexts. This enables them to take advantage of optimal transport and construct representations that effectively harness the geometry of the underlying space containing the contexts. The method captures uncertainty via modelling the entities as distributions and simultaneously provides interpretability with the optimal transport map, hence giving a novel perspective for building rich and powerful feature representations. The authors demonstrate it on tasks such as sentence similarity and word entailment detection."
SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper investigates the performance gap between model-based and model-free reinforcement learning (MBRL) methods in two MuJoCo environments. The authors propose a dynamics model that directly predicts distant states, based on current state and a long sequence of actions. This avoids the need for many recursions during long-range planning, and thus is able to yield more accurate state estimates. These predictions allow the authors to uncover the relationship between model accuracy and performance, and translate to higher task reward that matches or exceeds current state-of-the-art model free approaches."
SP:da14205470819495a3aad69d64de4033749d4d3e,"This paper proposes a method for ultra-low-precision neural network quantization. The proposed method, called precision highway, consists of an end-to-end high precision information flow that reduces the accumulated quantization error in both convolutional and recurrent neural networks. The authors also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. The experiments show that the proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantisation with a 2.45% accuracy loss in ResNet-50."
SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a general method for various image restoration problems, such as denoising, deblurring, super-resolution and inpainting. The problem is formulated as a constrained optimization problem, where the objective is to maximize a posteriori probability of latent variables, and the constraint is that the image generated by these latent variables must be the same as the degraded image. The authors use a Generative Adversarial Network (GAN) as the density estimation model."
SP:2feef921a0563d52fde1c074da754f73e6cabef8,"This paper proposes a method for knowledge distillation from few samples. The authors assume that both ""teacher"" and ""student"" have the same feature map sizes at each corresponding block, they add a 1x1 conv-layer at the end of each block in the student-net, and align the block-level outputs between ""teachers"" and students by estimating the parameters of the added layer with limited samples. Experiments verify that the proposed method is very efficient and effective to distill knowledge from teacher-net to student."
SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper proposes a transferability metric, H-score, to evaluate the performance of transferred representations from one task to another in classification problems. Inspired by a principled information theoretic approach, the proposed metric has a direct connection to the asymptotic error probability of the decision function based on the transferred feature. This formulation of transferability can further be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments using both synthetic and real image data show that the proposed transferability is meaningful in practice, but also it can generalize to inference problems beyond classification."
SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes to add a planning phase in neural machine translation (NMT) to control the global sentence structure ahead of translation. The proposed approach learns discrete structural representations to encode syntactic information of target sentences. During translation, it can either let beam search to choose the structural codes automatically or specify the codes manually. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations."
SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"This paper proposes a new loss function for generative adversarial networks (GANs) based on the Relativistic GAN (RGAN) loss function. The main idea is that the generator should increase the probability that fake data is real and decrease the probability of real data being generated by the discriminator. The authors show that the proposed loss function is a subset of the identity function of the integral probability metric (IPM) GANs. They also propose a variant of the proposed RGAN and RaGAN loss functions, where the generator is trained with a regularizer that encourages the generator to generate more realistic samples. The proposed loss functions are shown to be more stable than the standard SGAN loss function and the non-standard GAN loss functions. Empirically, the proposed method is shown to outperform WGAN-GP and SGAN."
SP:8df1599919dcb3329553e75ffb19059f192542ea,This paper proposes a method to tackle the problem of catastrophic forgetting in continual learning. The proposed method learns to build a model with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the solver to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach.
SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,This paper introduces Relational Forward Models (RFM) for multi-agent reinforcement learning. The main idea of RFM is to use graph neural networks (GN) to predict the future dynamics of multi-agents in a shared environment. The authors also propose to embed RFM modules inside agents and use them to augment the host agent’s observations with predictions of others’ behavior. Experiments show that embedding RFM results in faster learning systems compared to non-augmented baselines.
SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a method for inverse reinforcement learning (IRL) that learns a reward function from expert demonstrations. The main idea is to learn a set of tasks, along with demonstrations of the desired behaviors of those tasks, which is referred to as the meta-training set. From these tasks, the paper proposes an algorithm that enables effective learning of new reward functions by using a rich “prior” that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. They demonstrate that their method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior."
SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"This paper proposes a general framework for meta-learning approximate probabilistic inference for prediction (ML-PIP), which extends existing probablistic interpretations of meta-Learning approximate Probabilistic Inference for Prediction (PIP) to cover a broad class of methods. The proposed method, called VERSA, is an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. The method is evaluated on standard benchmarks where the method sets new state-of-the-art results, and settings where test conditions differ from training and testing."
SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper presents a novel deep learning architecture for classifying structured objects in ultrafine-grained datasets, where classes may not be clearly distinguishable by their appearance but rather by their context. The authors model sequences of images as linear-chain CRFs, and jointly learn the parameters from both local visual features and neighboring class information. The visual features are learned by convolutional layers, whereas the class-structure information is reparametrized by factorizing the CRF pairwise potential matrix. The performance of the proposed method is illustrated on a huge dataset that contains images of retail-store product displays, and shows significantly improved results compared to existing methods."
SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,This paper proposes a generative adversarial network (GAN) for audio synthesis. The authors claim that GANs are able to generate high-fidelity and locally-coherent audio by modeling log magnitude and instantaneous frequencies with sufficient frequency resolution in the spectral domain. The proposed method is evaluated on the NSynth dataset and compared with the state-of-the-art autoregressive audio synthesis methods such as WaveNet. 
SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper proposes a mixed-integer linear programming (MILP) verifier for verifying the robustness of piecewise-linear neural networks to adversarial perturbations. The proposed method is two to three orders of magnitude faster than the previous state-of-the-art verifier, Reluplex (Katz et al., 2017). The main contributions of this paper are two folds: (1) a tight formulation for nonlinearities, and (2) a novel presolve algorithm that makes full use of all information available. The authors demonstrate the effectiveness of the proposed method on the MNIST and CIFAR-10 datasets."
SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper proposes an approach to regularize learning in reinforcement learning by learning a ""default policy"" that is learned alongside the agent policy. The default policy is learned by restricting the amount of information the default policy receives, forcing it to learn reusable behaviours that help the policy learn faster. The authors formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. They present empirical results in both discrete and continuous action domains and demonstrate that for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning."
SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes a new model for conversational machine comprehension, called FLOWQA. The model is based on an alternating parallel processing structure, which can incorporate intermediate representations generated during the process of answering previous questions, such as previous question/answer pairs, document context and the current question. The proposed model is evaluated on two recently proposed conversational challenges, CoQA and QuAC, and outperforms the best models on all three domains in SCONE. "
SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,This paper tackles the problem of predicting the edits that software developers make to source code files. The authors develop several neural networks and use synthetic data to test their ability to learn challenging edit patterns that require strong generalization. They then collect and train their models on a large-scale dataset consisting of millions of fine-grained edits from thousands of Python developers. The main conclusion is that a new composition of attentional and pointer network components provides the best overall performance and scalability.
SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning strategy for multi-class classification. The proposed method optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi- class classifier as a sub-module. The authors formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. The framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings."
SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper studies the interpretability of VQA models. The authors design a set of experiments to replicate experiments from psycholinguistics where the same question was investigated for humans. The experiments show that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber’s law. Moreover, the authors identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system."
SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a probabilistic approach for link prediction in relational knowledge graphs. The authors argue that knowledge graphs should be treated within a Bayesian framework because even large knowledge graphs typically contain only few facts per entity, leading effectively to a small data problem where parameter uncertainty matters. The paper proposes to re-interpret the DistMult and ComplEx models and employ variational inference to estimate a lower bound on the marginal likelihood of the data. The main benefit of the Bayesian approach is that it allows for efficient, gradient based optimization over hyperparameters, which would lead to divergences in a non-Bayesian treatment."
SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,This paper proposes a new online learning approach for supervised dimension reduction. The proposed approach builds on top of the sliced inverse regression (SIR) algorithm and makes it implementable in an incremental manner. The authors also refine the algorithm by using an overlapping technique and develop an incremental overlapping sliced inverse regressors (IOSIR). The effectiveness and efficiency of both algorithms are verified on simulations and real data applications.
SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,This paper proposes a generative-discriminative model for multimodal learning. The proposed model is composed of two sets of independent factors (multimodal discriminative and modality-specific generative factors) that are shared across all modalities and contain joint multimodality features. The generative factor is unique for each modality and contains the information required for generating data. Experiments show that the proposed model achieves state-of-the-art or competitive performance on six multi-modal datasets.
SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper proposes a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, the authors learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The main idea is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. The authors introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the WaveNetCore fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker encoder with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-Speaker neural network to new speaker, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers, particularly when the utterances are high fidelity."
SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper provides an interesting connection between f-GANs and various depth functions through the lens of f-Learning. The authors show that these depth functions that lead to statistically optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f -Learning. This connection opens the door of computing robust estimator using tools developed for training GANs. In particular, the authors show in both theory and experiments that some appropriate structures of discriminator networks with hidden layers in GAN can lead to statistical optimal robust location estimators for both Gaussian distribution and general elliptical distributions where first moment may not exist."
SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper presents a method for inferring scene programs, representing a scene via a symbolic program for its objects, attributes, and their relations. The method is based on a hierarchical, object-based scene representation. The authors also propose a model that infers such scene programs by exploiting a hierarchical object representation. Experiments demonstrate that the model works well on synthetic data and transfers to real images with compositional structure."
SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a Gaussian-gated LSTM (g-LSTM) for reducing state updates. The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. The authors also propose a computational budget term to the training loss, which further reduces the number of computes needed for the network update by at least 10x. Finally, the authors propose a temporal curriculum learning schedule that helps speed up the convergence time of the equivalent LSTMs."
SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a plug-and-play method for out-of-distribution (OOD) detection. The proposed method is based on a simple ensembling of first and second order deep feature statistics. The statistics are obtained directly from the batch normalization layers (Ioffe & Szegedy, 2015), requiring no extra computations during training time, no changes to the network architecture and no pre-processing of the input image. The method improves the true negative rate from 39.6% to 95.3% for CIFAR-100 and TinyImageNet."
SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"This paper studies the problem of recovering the weights of a one-hidden-layer fully-connected neural network with sigmoid activations, where the training labels are generated from a Gaussian distribution and the goal is to recover the weight vectors of the neural network. The authors prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground-truth without requiring a fresh set of samples at each iteration. "
SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS) to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolution layers. It preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. Experiments show that FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs."
SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper proposes to study the training of GANs from the mixed Nash equilibrium perspective. The authors propose to use the classical prox methods to solve the infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE. They also propose a principled procedure to reduce the proposed prox method to simple sampling routines, leading to practically efficient algorithms. Finally, they provide experimental evidence that their approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality."
SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"This paper proposes a method for parameter-efficient transfer and multitask learning with deep neural networks. The basic approach is to learn a model patch a small set of parameters that will specialize to each task, instead of fine-tuning the last layer or the entire network. The approach allows both simultaneous (multi-task) and sequential transfer learning. The authors also show that re-learning existing low-parameter layers (such as depth-wise convolutions) improves transfer-learning accuracy significantly."
SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method, called mode normalization (MN), which is an extension of batch normalization to more than a single mean and variance. The proposed method first assigns samples in a mini-batch to different modes via a gating network, and then normalizes each sample with estimators for its corresponding mode. The authors demonstrate that the proposed method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets."
SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"This paper investigates the lottery ticket hypothesis, which states that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation—reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the lottery: their connections have initial weights that make training particularly effective. The authors identify winning tickets by training a winning ticket by a network and pruning its parameters. They find that a standard pruning technique automatically uncovers trainable subnetwork from fully-connected and convolutional networks."
SP:08c662296c7cf346f027e462d29184275fd6a102,"This paper proposes a method for learning a representation of the state space of a reinforcement learning agent that can be used for count-based exploration in Atari games. The proposed method is based on an attentive dynamics model (ADM) that learns to predict the actions of the agent. The ADM is trained in a self-supervised fashion, and is then used as a part of the actor-critic algorithm for exploration. The method is evaluated on three Atari games (Montezuma’s revenge, HalfCheetah, Ant, and Ant-Walker) and achieves state-of-the-art performance."
SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"This paper proposes a generative approach to generate all the parameters of a neural network. The approach is based on generative adversarial networks (GANs) where the parameters are sampled from a latent space, and the generator is trained with a classification loss. The authors apply the approach to classification tasks on MNIST and CIFAR-10, and show that it can learn to generate parameters which solve the tasks with comparable performance to fully supervised learning, while learning a rich distribution of effective parameters. They also show that HyperGAN can also provide better uncertainty than standard ensembles."
SP:230b3e008e687e03a8b914084b93fc81609051c0,This paper proposes an approach to train variational autoencoders (VAEs) with discrete valued latent variables. The authors propose a differentiable estimator for the evidence lower bound (ELBO) which is based on importance sampling. The proposed approach is evaluated on two VAEs architectures with Bernoulli and Categorically distributed latent representations on two benchmark datasets.
SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,"This paper proposes to train a feed forward neural network with increased robustness against adversarial attacks. This is achieved using a novel pre-trained building block based on a mean field description of a Boltzmann machine. On the MNIST dataset, the method achieves strong adversarial resistance without data augmentation or adversarial training. The authors also show that the increased adversarial performance is correlated with the generative performance of the underlying Boltzman machine."
SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,"This paper studies the relationship between human minimal images and the performance of deep neural networks (DNNs). Minimal images are the smallest regions of an image that remain recognizable for humans. Ullman et al. (2016) show that a slight modification of the location and size of the visible region of the minimal image produces a sharp drop in human recognition accuracy. In this paper, the authors show that such drops in accuracy are a common phenomenon between humans and existing state-of-the-art DNNs, and are much more prominent in the case of fragile recognition images (FRIs). The authors also show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in object location in DNN, and reveals a new failure mode of DNN that also affects humans to a much lesser degree."
SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper studies the problem of multi-agent reinforcement learning (MARL) in the setting where there are self-interested agents (i.e., worker agents) which have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. In this setting, the authors propose a framework called Mind-aware Multi-agent Management Reinforcement Learning (M3RL), which involves both agent modeling for estimating workers’ minds and policy learning for contract generation. The proposed approach is evaluated in two environments, Resource Collection and Crafting, to simulate the multiagent management problems with various task settings and multiple designs for the worker agents. The experimental results have validated the effectiveness of the approach in modeling worker agents’ mind online, and in achieving optimal ad-hoc teaming with good generalization and fast adaptation."
SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a new recurrent neural network (RNN) model for asynchronous time series. The authors propose a unified RNN that handles five different feature types, i.e., sparse, dense, static, time, static decay, time features, and time features at the sequential level. The sparse features are those features that are present frequently and are updated only at time steps when that feature is present. The static features are the ones that are not related to time and are combined with the encoder output. The experiments show that the proposed model achieves better performance compared to standard RNNs."
SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper presents a simple neural model that tries to answer the question whether a given logical formula has the given property, for example whether a propositional formula is always true. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a topdown manner. The results of this network are then processed by two recurrent neural networks. One of the interesting aspects of this model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct."
SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,This paper studies the problem of curriculum learning in deep neural network training. The authors propose a method to sample mini-batches with varying levels of difficulty to improve the speed of learning and generalization of the trained network. The method is based on the idea that the difficulty of a training image is defined by the transfer learning from a competitive teacher network trained on the Imagenet database. They also suggest a bootstrapping approach to evaluate the difficulty using the same network without relying on a teacher network. 
SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper proposes a general PAC-Bayesian framework to analyze the generalization properties of deterministic and uncompressed deep neural networks. The main idea is to show that if on the training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions generalize to the interaction between the matrices on the test data, thereby implying a wide test loss minimum. The authors then apply their general framework to the setting where the pre-activation values of the network are not too small. In this setup, they provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weights matrices."
SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper proposes a new training algorithm for the vector quantized variational autoencoder (VQ-VAE) based on expectation maximization (EM) and sequence level knowledge distillation. The authors show that the proposed method can outperform previous state-of-the-art neural machine translation models (Kaiser et al. 2018) on the WMT English-German translation task, achieving a BLEU score of 22.4 on English to German translation, outperforming Kaiser et al (2018) by 2.6 BU and 3.3 times faster at inference inference. "
SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes two multi-view learning methods for learning sentence representations in an unsupervised fashion. One framework uses a generative objective and the other one uses a discriminative one. In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN) and another view is a simple linear model. The authors show that, after learning, the vectors produced by these models provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view."
SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"This paper proposes an online distributed optimization method called Anytime Minibatch (AMB) to mitigate the impact of stragglers. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. The paper provides a convergence analysis and analyze the wall time performance. The numerical results show that AMB is up to 1.5 times faster in Amazon EC2 and up to 5 times faster when there is greater variability in compute node performance."
SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,This paper proposes a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. The authors argue that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. They test this approach in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.
SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper proposes a method to improve Neural Processes (NP) by adding an attention mechanism to the encoder and decoder of NP. The authors claim that the main weakness of NP is the underfitting of the context set, which causes inaccurate predictions at the inputs of the observed data they condition on. To address this issue, the authors propose to use differentiable attention to attend to the contexts relevant to the given target, while preserving the permutation invariance in the contexts. Experiments on 1D and 2D regression tasks show that the proposed method outperforms NP in terms of reconstruction performance and training speed."
SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,"This paper provides a theoretical analysis of credit assignment in gradient-based meta-RL. The authors show that the recent formulation introduced by Al-Shedivat et al and Stadie et al. (2018) leads to poor credit assignment, while the MAML formulation (Finn et al., 2017) potentially yields superior meta-policy updates. Based on this analysis, the authors propose ProMP, an efficient and stable meta-learning algorithm for RL. ProMP is based on the low variance curvature (LVC) surrogate objective which yields gradient estimates with a favorable bias-variance trade-off."
SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes a message-passing neural network (MPNN) to predict satisfiability of SAT problems. The model is trained as a classifier on a dataset of randomly generated SAT problems, and at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs. The main contribution of this paper is that the model is able to generalize to novel distributions, and that it can be run indefinitely to search for solutions to problems of varying difficulty."
SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. The main idea is to augment the imitation loss with additional losses that penalize undesirable events and encourage progress – the perturbations then provide an important signal for these losses and lead to robustness of the learned model. The authors show that the model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of the proposed changes and show that it is responding to the appropriate causal factors."
SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a method to select a subset of training data to achieve faster training with no loss in model predictive performance. The authors first train a small proxy model to estimate the utility of individual training data points, and then select the most informative ones for training the large target model. Extensive experiments show that the approach leads to a 1.6x and 1.8x speed-up on CIFAR10 and SVHN by selecting 60% and 50% subsets of the data, while maintaining the predictive performance of the model."
SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,This paper proposes a method for planning in continuous domains by treating planning as a probabilistic inference problem over future optimal trajectories. The proposed method is based on Sequential Monte Carlo (SMC) and Bayesian smoothing in the context of control as inference. The authors show that the proposed method can capture multimodal policies and can quickly learn continuous control tasks.
SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"This paper shows that adversarial robustness is sensitive to the input data distribution, unlike clean accuracy. The authors show that adversarially trained models achieve significantly different robustness accuracies. They further show that even a semantics-preserving transformations on the data distribution can cause a significantly different adversarial trained model that is both trained and evaluated on the new distribution. Their discovery is based on a study which disentangles the behaviors of clean accuracy and robust accuracy."
SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper introduces a new normalization technique that exhibits the fast convergence properties of batch normalization using a transformation of layer weights instead of layer outputs. The proposed technique keeps the contribution of positive and negative weights to the layer output in equilibrium. The authors validate the method on a set of standard benchmarks including CIFAR-10/100, SVHN, and ILSVRC 2012 ImageNet."
SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper proposes a method for robust data denoising for large-scale training of deep neural networks. The proposed method is based on the implicit regularization effect of stochastic gradient descent with large learning rates. By the loss statistics, it is able to identify mislabeled examples with remarkable success. Then, it can be discarded and continue training with the rest of the examples. This leads to ON-THE-FLY DATA DENOISING (ODD), a simple yet effective algorithm."
SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a theoretical analysis framework to link the pretraining and downstream tasks with an underlying latent variable generative model of text — the downstream classifier must recover a function of the posterior distribution over the latent variables. The authors analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. They show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker conditions, and 3) the recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory."
SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization, i.e., generalization of a trained model to a new data distribution that is not the same as the training one. The authors propose a new notion of transferability, which they define as the ability of a model to generalize to a target distribution that has the same label distribution as the source distribution. They then propose an algorithm for learning transferable features and show that it outperforms existing methods in terms of the transferability of the learned features."
SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) an ordering over trajectories. The main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. Then, the authors provide a polynomial-time algorithms that construct a reward function that can be used to optimize tasks of each of these three types and correctly determine when no such reward function exists."
SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning (RL). The authors show that the sequential structure of the RL problem necessitates new approaches to generalize beyond the well-studied techniques used in supervised learning. They show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Based on this observation, they recast the generalization problem in RL as solving the induced partially observed Markov decision process, which they call the epistemic decision process (POMDP). They suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, they demonstrate that their simple algorithm derived from epistemic POMDB achieves significant gains in generalization over current methods on the Procgen benchmark suite."
SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a unified framework for estimating higher-order derivatives of value functions in meta-reinforcement learning (meta-RL) based on the concept of off-policy evaluation. The framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. This framework also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper proposes a bidirectional compression algorithm for distributed convex optimization with a central server. The main idea is to compress the uplink communication from the local workers to the central server, while preserving the global model. The proposed algorithm, called MCM, achieves the same convergence rate as algorithms using only uplink compression. In addition, the proposed algorithm additionally combines model compression with a memory mechanism. "
SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper introduces counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. The authors also provide practical schemes for learning (approximately) counterfactually invariant predictors. In particular, the authors show that the true underlying causal structure of the data depends on whether the label causes the features or the features cause the label. They also show that different causal structures require distinct regularization schemes for different domain shift guarantees."
SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes a novel strategy called Adaptive Pseudo Augmentation (APA) to encourage healthy competition between the generator and the discriminator. APA alleviates overfitting by employing the generator itself to augment the real data distribution with generated images, which deceives the discriminators adaptively. Extensive experiments demonstrate the effectiveness of APA in improving synthesis quality in the low-data regime. "
SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a framework for causal inference between pairs of event variables in multivariate recurrent event streams by extending Rubin’s framework for the average treatment effect (ATE) and propensity scores to multivariate point processes. The setting is different from i.i.d. data in the sense that it involves multiple occurrences of various types of events over a common timeline. The authors theoretically justify their point process causal framework and show how to obtain unbiased estimates of the proposed measure. They conduct an experimental investigation using synthetic and real-world event datasets, where their proposed causal inference framework is shown to exhibit superior performance against a set of baseline pairwise causal association scores."
SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper investigates the effect of residual connections in GNN message passing on the vulnerability of GNNs to abnormal node features. The authors propose a simple, efficient, interpretable, and adaptive message passing scheme, leading to a novel GNN with Adaptive residual, named as AirGNN. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm."
SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper proposes a variational Bayesian Optimistic Sampling (VBOS) algorithm for online multi-armed bandit problems. The main idea of the paper is to extend the optimistic set of Thompson sampling (TS) to the bilinear saddle-point setting, where the objective is a convex optimization problem over the simplex and the goal is to find a policy that satisfies a particular ‘optimism’ condition. The authors show that VBOS and TS can be considered as part of the same family of algorithms since, in the bandit case, they both produce policies that are optimistic. The regret analysis for optimistic policies is also extended to zero-sum matrix games and constrained bandits as special cases. "
SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper improves the convergence analysis and rates of variance reduction under without-replacement sampling orders for composite finite-sum minimization. The authors develop a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. The rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without replacement sampling with variance-reduction. Moreover, the authors also propose a practical method to discover the optimal cyclic ordering numerically."
SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper provides theoretical guarantees on the sub-optimality of a policy learned using first-order optimization methods applied to the relative entropy policy search (REPS) objective. In particular, the authors consider the setting in which we are given access to exact gradients and demonstrate how near-optimal of the objective translates to near- optimality of the policy. The authors also consider stochastic gradients, and introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal policy. "
SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper studies the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. It proposes a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and the 3D structures. Besides, it also proposes metrics to evaluate the spatial smoothness of encoding 3d structures and the representation complexity of the DNN. The experiments expose representation problems with classic DNNs and explain the utility of the adversarial training."
SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,This paper proposes an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. The authors also introduce a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that their proposed method is competitive with current state-of-the-art Neural-network based auction designs. They validate their approach through human subject research and show that they are able to effectively capture real human preferences.
SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of model personalization in the setting of multi-task learning with user-level differential privacy. In this setting, there are n users, each of whom has a training data set drawn from their own distribution, and the goal is to learn a model that generalizes well to unseen examples from the other users' data sets. The authors propose two algorithms that exploit the non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach. They also establish a general, information-theoretic upper bound via an exponential mechanism-based algorithm. "
SP:3925fc528de17b8b2e93808f5440ea0503895b75,"This paper introduces Adversarial VQA (AdVQA), a large evaluation dataset of 46,807 examples in total, all of which fooled the MoViE+MCAN model. Human subjects interact with a state-of-the-art Visual Question Answering model, and for each image in the dataset, attempt to find a question where the model’s predicted answer is incorrect. The collected data can be used to “stress test” current models and serve as the next iteration of the V QA benchmark helping drive further progress. The authors evaluate a wide range of existing VQa models on AdVQa and find that their performance is significantly lower than on the commonly used VZA v2 dataset."
SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of heterogeneous neurons in the medial entorhinal cortex (MEC) of animals. The authors first identify the similarity transform between neural populations in different animals, and then evaluate a range of neural network models to explain the response variance of MEC neurons, treating each potential model as a potential animal and measuring how well it maps each animal to each other in terms of the inter-animal consistency of response variance. They found that task-optimized neural networks models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles — including those of grid cells — despite not being explicitly trained for this purpose. Finally, the authors introduce a new MEC model that performs reward-modulated path integration."
SP:57f9812fa5e7d0c66d412beb035301684d760746,"This paper studies the problem of online reinforcement learning with KL-regularized reinforcement learning (RL) from expert demonstrations. The authors show that KL-RL with behavioral reference policies can suffer from pathological training dynamics that can lead to slow, unstable, and suboptimal online learning. They empirically show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. Finally, the pathology can be remedied by non-parametric behavior reference policies and this allows the proposed method to outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks."
SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the relationship between the performance of deep convolutional neural networks (CNNs) and their local and translational invariance. Specifically, the authors propose a teacher-student framework for kernel regression, where the function to be learned is one of the following: fLC(x) = \sum_i\in P g_i(x), fCN(x)= \sum_{i \in P_i}(xi), where f_i is a random function and CN is a teacher kernel. The student kernel corresponds to a prior on the true function of the form described in Eq. (2), except that the filter size of the teacher kernel is smaller than that of the student kernel. In this setting, the learning curve exponent $\beta$ is the ratio between the test error $\tilde{P}-\beta$ and the number of training samples $P$, which depends on the size $d$ of the training set $P$. The authors show that in the ridgeless case, the local invariance plays a key role in determining the $\beta$, whereas the translation invariance is not. The authors also show that under a natural universality assumption, performing kernel regression with a ridge that decreases with the size of training set leads to similar learning curve exponents to those we obtain in the ridge-free case."
SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a deterministic autoencoding framework for variational autoencoders that is applicable to expressive priors and overcomes the necessity necessity for post-hoc density estimation step for deterministic training. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. Experimental results show the expressiveness and sample quality of the proposed method.
SP:6232d8738592c9728feddec4462e61903a17d131,"This paper proposes a method for self-supervised adversarial detection using autoencoders. The proposed method is based on disentangling the input images as class features and semantic features, and training an autoencoder, assisted by a discriminator network, over both correctly paired class/semantic features to reconstruct benign and counterexamples. This mimics the behavior of adversarial examples and can reduce the unnecessary generalization ability. The authors compare their method with the state-of-the-art methods under different adversarial attacks and 15 different victim models (30 attack settings), and it exhibits better performance in AUC, FPR, TPR for most attack settings."
SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper proposes a method to model the brain representations of syntactic syntactic information. The authors propose a set of features that encode information about the syntactic structure of sentences. These features and fMRI recordings of participants reading a natural text are used to predict brain activity as a function of time. The results show that syntactic features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, the authors show that regions well-predicted by syntactic feature are distributed in the language systems and are not distinguishable from those processing semantics."
SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for generating controllable images by using energy-based models (EBMs) in the latent space of a pre-trained generative model such as StyleGAN. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). The proposed method is simple, fast to train, and efficient to sample. Experimental results show that the proposed method outperforms the state-of-the-art in both conditional sampling and sequential editing."
SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper presents a federated linear contextual bandits model, where individual clients face different K-armed stochastic bandits coupled through common global parameters. A collaborative algorithm called Fed-PE is proposed to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a new model-based offline RL algorithm, COMBO, that trains a value function using both the offline dataset and data generated using rollouts under the model while additionally regularizing the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the authors show that COMBO satisfies a policy improvement guarantee in the offline setting. Empirically, it is shown that the proposed method outperforms prior offline RL methods in benchmark tasks."
SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the impact of backpropagation on the feature space of deep neural networks during training. The authors propose to model the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. The main finding is that if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, then the features of the training data become linearly separable, meaning vanishing training loss; otherwise, the features are not separable. This result provides convincing evidence for the presence of local elasticity in deep learning."
SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a framework for learning to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. Specifically, the authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embeddings space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and generalizable policies."
SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper analyzes the physics-informed neural network (PINN) approach for learning PDEs, and shows that the PINN approach can fail to learn relevant physical phenomena for even slightly more complex problems. The authors demonstrate that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where PINN’s loss term starts from a simple PDE regularization and becomes progressively more complex as the NN gets trained. The second approach is a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that these methods can achieve up to 1-2 orders of magnitude lower error as compared to regular PINN training."
SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. It cycles between a forward step and a reverse step until convergence. It introduces the Tsallis entropy as a confidence-friendly regularization to improve the quality of target pseudo-label. The paper provides theoretical analysis under realistic assumptions, and provides hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self- training fail."
SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured network pruning method, called Discriminative Masking (DAM), which is a systematic approach that jointly prunes and refines weights during training in a single stage and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. The proposed approach has remarkably good performance over a diverse range of applications, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification."
SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a modular self-attention network architecture that allows compositional reasoning and reuse of knowledge. In particular, the proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. Experiments on image classification and visual abstract reasoning on Raven Progressive Matrices show that Neural Interpreters perform on par with the vision transformer, while being transferrable to a new task in a sample efficient manner."
SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes a technique called Behavior Transfer (BT) that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. The authors show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation can lead to the emergence of complex behaviors, which can then be leveraged by BT to discover better solutions. Experiments show that combining BT with standard fine-tuning strategies results in 13 additional benefits. "
SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a differentiable surrogate loss for learning-to-rank (LTR) models. The surrogate loss is obtained via a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divide-and-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, the authors demonstrate the role of larger list sizes during training and show that the proposed method significantly improves over comparable approaches on publicly available internet-scale learning to rank benchmarks."
SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,This paper proposes a reinforcement learning approach to learn the structure of the variational quantum circuit (VQE) that can be used to estimate the ground-state energy of a given Hamiltonian Hamiltonian. The authors propose a feedback-driven curriculum learning method that autonomously adapts the complexity of learning problem to the current performance of the learning algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. The proposed approach is evaluated on the problem of estimating the ground state energy of LiH and achieves state-of-the-art results in terms of circuit depth and accuracy.
SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the effect of arbitrary class distributions within the query sets of few-shot learning tasks at inference, removing the class-balance artefact. Specifically, the authors model the marginal probabilities of the classes as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex. The authors also propose a generalization of the mutual-information loss, based on alpha-divergences, which can handle effectively class-distribution variations. Empirically, they show that the proposed method outperforms state-of-the-art methods across several data sets, models and few-shots settings."
SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a method to reduce the memory consumption of convolutional neural networks (CNNs) on tiny microcontroller units (MCUs). The authors propose a patch-by-patch scheduling scheme to perform inference on the initial memory-intensive blocks of the network. They also propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. The proposed method, called MCUNetV2, achieves state-of-the-art performance on ImageNet and Pascal VOC."
SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent, and takes actions based on the strategic agent's report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal’s utility in the face of the self-interested strategic agent. The paper gives an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. For environments with large time horizons, the paper shows that the optimal utility is hard to approximate within a certain constant factor, complementing the algorithmic result."
SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,"This paper investigates the question of how neural architecture search (NAS) is able to select the desired GNN architectures. The authors propose a Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and uses graph structure learning as a denoising process in the search procedure. Extensive experiments on real-world graph datasets demonstrate that the proposed GASSO model achieves the state-of-the-art performance compared with existing baselines."
SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fairness in clustering. In particular, the authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. They derive lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. In addition, they introduce an effective heuristic algorithm for the Leximin objectives. They also derive impossibility results for other natural fairness objectives."
SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that subject to a bounded overlap condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the effect of the value of ReLU(0) in [0,1] for a neural network on the backpropagation and training. The authors investigate the importance of the ReLU for various precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN, and ImageNet). They observe considerable variations of backpropags that occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU($0$) = 0 seems to be the most efficient."
SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. The method brings together ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. It jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck."
SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"This paper proposes a Transformer-based graph representation learning method, called Spectral Attention Network (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. The proposed method is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction."
SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper considers two-alternative elections where voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. The authors present an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium (where no coalition of voters can deviate and receive a better outcome)."
SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper studies the Hessian rank of deep linear networks. The authors develop theoretical tools to analyze the range of Hessian map, which provide us with a precise understanding of its rank deficiency and the structural reasons behind it. This yields exact formulas and tight upper bounds for the Hessians, allowing for an elegant interpretation in terms of rank deficiency. Moreover, the authors demonstrate that our bounds remain faithful as an estimate of the numerical Hessians for a larger class of models such as rectified and hyperbolic tangent networks."
SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new metric to quantify linear disentanglement (LSBD) based on the notion of linearly disentangled representations. The proposed metric, called DLSBD, can be used to evaluate the quality of the representations learned by VAE-based methods and other disentangling methods. The authors demonstrate the utility of the metric by showing that LSBD-VAE and other recent methods can learn LSBD representations, needing only limited supervision on transformations, and various desirable properties expressed by existing disenanglement metrics are also achieved by the proposed metric."
SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes a framework for training deep state-space models (DSSMs) that combines variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. The framework is based on a constrained optimisation (CO) framework that ensures a good reconstruction of the dynamical system dynamics. The proposed method is evaluated on the moving pendulum on the reacher environment, where it achieves remarkable results in identifying dynamical systems."
SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method to generate counterfactual explanations for a given query image. The proposed method is based on the idea of deep inversion to generate images from the training distribution of a deep classifier. The authors claim that the existing methods are insufficient for producing interpretable and realistic images. To this end, the authors propose DISC (Deep Inversion for Synthesizing Counterfactuals), which improves upon the existing deep-inversion methods by utilizing stronger image priors, incorporating a novel manifold consistency objective, and adopting a progressive optimization strategy. The method is evaluated on CIFAR-10 and ImageNet datasets."
SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes an algorithm for identifying regions of heterogeneity in decision-making, i.e., regions where the assignment of decision-maker has a large causal effect on the decision. The authors formalize this problem as a causal inference problem, and propose an algorithm to maximize an empirical objective to find such regions. The algorithm is evaluated on a semi-synthetic dataset and a real-world healthcare dataset. "
SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based style-based generative model (TokenGAN) for image synthesis. The key idea is to learn two types of visual tokens, i.e., the content tokens and the style tokens. The content tokens are learned as the constant input in the generator, and style tokens are projected from a learned intermediate latent space. Given a sequence of style tokens, the generator learns to control the visual token rendering by each token with related style tokens according to their semantics. Experiments on FFHQ and LSUN Church show the effectiveness of the proposed method. "
SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the robust overfitting in the presence of noiseless data. The authors show that even in the absence of noise, avoiding interpolation through ridge regularization can significantly improve generalization. They prove this phenomenon for the robust risk of both linear regression and classification, and hence provide the first theoretical result on robust over fitting. They observe this phenomenon in experiments with shallow neural networks."
SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The CPAE performs two key functions: (a) encoding an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and (b) decoding the primitive back to the original input instance shape. The primitive plays a key role to map all the unordered point clouds on the canonical surface and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the model performs favorably against state-of-the-art correspondence learning methods."
SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper studies the domain generalization (DG) problem and proposes a novel method, Stochastic Weight Averaging Densely (SWAD), to find flatter minima and suffers less from overfitting than does the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large margins of +1.6% averagely on outof-domain accuracy. The authors also compare SWAD with conventional generalization methods, such as data augmentation and consistency regularization methods to verify the remarkable performance improvements are originated from by seeking flat minima, not from better in-domain generalizability."
SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper provides a large-scale study of performance predictors for neural architecture search (NAS) by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. The authors test a number of correlation-and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. The results act as recommendations for the best predictors to use in different settings, and show that certain families of predictors can be combined to achieve even better predictive power."
SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the inherent privacy of sampling from a Dirichlet posterior distribution. In particular, the authors study the privacy guarantee of the posterior sampling in the setting of truncated concentrated differential privacy (tCDP), which is a generalization of the previous work on the differential privacy of posterior sampling from exponential families. The authors derive a simple privacy guarantee for the sampling, which allows them to analyze its utility in various settings. Specifically, they provide accuracy guarantees of the sampling in Multinomial Dirichle sampling and private normalized histogram publishing."
SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for computing random walks in the massively parallel computation model (MPC), which is a theoretical abstraction of real-world parallel computation systems such as MapReduce. The authors show that their algorithm is both memory and round efficient, and yields an efficient parallel local clustering algorithm. Theoretical analysis and experimental results show that the proposed algorithm is significantly more scalable than previous approaches. "
SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper studies the typical learning performance of L1-regularized linear regression (`1-LinR) for Ising model selection using the replica method from statistical mechanics. For typical random regular graphs in the paramagnetic phase, an accurate estimate of the typical sample complexity is obtained. Moreover, the authors provide an efficient method to accurately predict the nonasymptotic behavior of `1-LinR for moderate M,N, such as precision and recall. "
SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means problem, which is a generalization of the well-known k-mean clustering problem. The authors propose a clustering framework where the learner is allowed to interact with an oracle (domain expert), asking for the similarity between a certain set of chosen items. They prove that having a few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynemic time-complexity, where n is the number of items. "
SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method for augmenting model-based reinforcement learning (RL) by additionally encouraging a learned model and value function to be jointly self-consistent. The proposed approach differs from classic planning methods such as Dyna, which only update the value to be consistent with the model. The authors propose multiple methods to achieve this goal, and evaluate these in both tabular and function approximation settings, and find that, with appropriate choices, the proposed method can improve both policy evaluation and control."
SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper proposes a method to approximate episode sampling distributions based on the difficulty of the episodes. The proposed sampling method is algorithm agnostic and can be leveraged to improve the few-shot learning accuracies across many episodic training algorithms. The experiments demonstrate the efficacy of the proposed method across popular datasets, algorithms, network architectures, and protocols. "
SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of generalized linear bandits (GLBs) in the setting where the number of outcomes that can be selected by the user is larger than two (e.g., ‘click’, ‘show me later’ vs ‘never show again’). In this setting, the authors use multinomial logit (MNL) to model the probability of each one of K+1 2 possible outcomes. For this problem, they present MNL-UCB, an upper confidence bound (UCB)-based algorithm, that achieves regret $\tilde{O}(\frac{dK p T}{\sqrt{T})$ with small dependency on problem-dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds."
SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The proposed method aims to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-max entropy framework. Numerical results show that the proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of learning the time evolution of discrete sets of items (e.g., genetic mutations) from continuous-time Markov chains. The authors first show that the learning task is generally underspecified in the usual setting of cross-sectional data, and then propose an approximate likelihood maximization method that can scale to hundreds of items and is orders of magnitude faster than previous methods. They demonstrate the effectiveness of their approach on synthetic and real cancer data."
SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a unified pretraining framework for document understanding. It extends the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. It learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks."
SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies clustering problems with lp-norm objectives (e.g. k-center, k-median and k-means) in the context of individual fairness. The authors propose a new algorithm based on Linear Programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal. Furthermore, their theoretical fairness guarantees are comparable with MV20 in theory, and empiricalically, they obtain noticeably fairer solutions. "
SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper proposes a polynomial-time Gaussian sampling-based algorithms for MAX-K-CUT and the MAX-AGREE variant of correlation clustering on a graph G = (V,E) with n vertices. The proposed algorithm uses O(n + |E|) memory and nearly achieves the best existing approximation guarantees. For dense graphs arriving in a stream, the authors eliminate the dependence on |E^2 in the storage complexity at the cost of a slightly worse approximation ratio. "
SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a Motion-Aware Unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. The fusion module is utilized to further aggregate the augmented motion information and current appearance information (current spatial state) to the final predicted frame. Moreover, an information recalling scheme is employed into the encoders and decoders to help preserve the visual details of the predictions. Experimental results show that the MAU outperforms the state-of-the-art methods on both tasks."
SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies the sample complexity of two-layer neural networks with polynomial activation functions for reinforcement learning (RL) in the generative model and online setting. The main contributions of this paper are two folds. First, the authors propose a sample-efficient algorithm for RL with two layer neural network function approximation under completeness conditions. Second, they show that for the deterministic setting, the sample complexities scale linearly in the algebraic dimension."
SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization ability of deep metric learning (DML) models under out-of-distribution (OOD) distribution shifts. The benchmark is constructed by systematically constructing train-test splits of increasing difficulty and presents the ooDML benchmark to characterize generalization under OOD shifts in DML. Based on the new benchmark, the authors conduct a thorough empirical analysis of state-of the-art DML methods and propose few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in the proposed benchmark. "
SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a data-driven approach to unsupervised outlier model selection (UOMS) based on meta-learning. The proposed method, called METAOD, is based on the prior performances of a large collection of existing detection models on an extensive corpora of historical outlier detection benchmark datasets, and carries over this prior experience to automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. To capture task similarity within the meta learning framework, the authors introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAODE significantly outperforms no model selection."
SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. This framework gives the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. Experiments on synthetic linear programming, portfolio optimization, and resource provisioning demonstrate that the proposed method outperforms traditional two-staged methods and other decision-focused approaches."
SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper proposes DropGNNs, a new approach to overcome the limitations of standard GNN frameworks. The main idea is to execute multiple different runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, the results from these runs are aggregated to obtain the final result. Theoretical bounds are derived for the number of runs required to ensure a reliable distribution of dropouts, and they prove several properties regarding the expressive capabilities and limits of the proposed approach. Experimental results on established GNN benchmarks are also provided."
SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper presents a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, it also designs a Dual-Stream FPN to enhance contextual information for downstream dense predictions. The proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets."
SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a framework for visual reasoning with differentiable physics (VRDP) that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) based on the language, thus providing prior knowledge for the physics engine. The physics engine is implemented as an impulse-based differentiable rigid-body simulator, which performs differentiable physical simulation based on grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. "
SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The proposed method is evaluated on CIFAR-10, MNIST, and ImageNet. The results show that the proposed method outperforms existing active learning methods by as much as 5%-18% in the case of rare classes and up to 10% in out-of-distribution data."
SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"This paper proposes two identity tests for ranking data that is generated from a Mallows model in the asymptotic and non-asymptotic settings. The main contributions of this paper are the following:  1. The authors devise a Uniformly Most Powerful Unbiased (UMPU) test for testing the spread parameter of the known central ranking, which is easy to compute and sample-optimal for a wide range of parameters.  2. They also consider the case when the central ranking is unknown, and devise two algorithms to test the spread parameters. The first one is obtained by constructing a uniformly most powerful unbiased test and then converting it into a sample optimal identity test.  3. The second one is derived from an optimal learning algorithm for the unknown central ranking case.  4. They support all their findings with extensive numerical experiments. "
SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,This paper proposes a generalizable neural radiance field (NHP) method for free-viewpoint human performance rendering that generalizes across different human performers and requires only sparse camera views. The proposed method is a combination of temporal and multi-view transformers that aggregate spatio-temporal observations to robustly compute the density and color of a query point. Experiments on the ZJU-MoCap and AIST datasets show that the method significantly outperforms recent generalizable NeRF methods on unseen identities and poses.
SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes to search the search space of vision transformers by gradually evolving different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, it provides design guidelines of general vision transformer with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. The searched models, named S3 (short for Searching the Search Space), achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet."
SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this setting, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. First, the authors provide an algorithm that, given a set of bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisfied by) an unknown LTF, efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. For the case of non-monochromatic bags, the algorithm satisfies (1/2)-fractions of them. The main result provides evidence that these algorithmic bounds cannot be significantly improved, even for learning monotone ORs using LTFs."
SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper proposes a method to create surrogate neural architecture search (NAS) benchmarks that contain the full training information for each architecture, rather than just the final validation accuracy. The authors use singular value decomposition and noise modeling to create the surrogate benchmarks. They also introduce a learning curve extrapolation framework to modify single-fidelity algorithms, and show that it leads to improvements over popular single fidelity algorithms."
SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a post-hoc explanation method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space by answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example. The authors propose to reconstruct the test latent representation as a mixture of corpus latent representations. Further, they propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. They demonstrate that these decompositions are robust and accurate."
SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based visual embedding for vision-language pre-training (VLP) to better learn visual relation and further promote inter-modal alignment. Specifically, the authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., Inter-modality Flow). They also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the intermodality learning. Experiments are conducted on Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment and Visual Reasoning. "
SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the privacy dynamics of noisy gradient descent algorithms and models the dynamics of Rényi differential privacy loss throughout the training process. The analysis traces a provably tight bound on the divergence between the pair of probability distributions over parameters of models trained on neighboring datasets. The privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy loss by upper-bounding its total value over all intermediate gradient computations). For Lipschitz, smooth, and L1-smooth and L2-strongly convex losses functions, the authors prove optimal utility with a small gradient complexity for noisy GD algorithms."
SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes RLQP, a reinforcement learning approach to accelerate the optimization of operator splitting quadratic programs (QPs) by adapting the internal parameters of ADMM (alternating direction method of multipliers, ADMM) between iterations to improve the convergence of the solver. In particular, the authors propose to learn a policy that maps states in a set S to actions in set A such that the selected action maximizes the accumulated reward r. The policy can be trained either jointly across general classes of QPs or with respect to a specific class. The general policy is trained once on a broad class and can be used out-of-the-box on new problems, while the specialized version is trained on a specific set of problems that are encountered frequently in practice. In experiments, the paper shows that the proposed approach outperforms OSQP by up to 3x on QPLIB, Netlib and Maros-Mészáros problems."
SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the asymptotic properties of the over-parameterized deep linear network (DNN). It shows that the convergence rate of the DNN's parameters is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. The authors term this convergence pattern the Principal Components bias (PC-bias) and show how it streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also compare the results to the spectral bias, showing that both biases can be seen independently, and affect the learning in different ways."
SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper studies the problem of detecting backdoored data poisoning attacks in deep neural networks (DNNs) during training. The authors identify two inherent characteristics of backdoor attacks as their weaknesses: (1) the models learn backdoor data much faster than clean data, and (2) the stronger the attack the faster the model converges to the backdoor data. Based on these two weaknesses, the authors propose a general learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent backdoor attacks during training by introducing a two-stage gradient ascent mechanism for standard training to 1) help isolate backdoor examples at an early training stage, and 2) break the correlation between backdoor examples and the target class at a later training stage. Extensive experiments on multiple benchmark datasets against 10 state-of-the-art attacks show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data."
SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a novel shading-guided generative implicit model (ShadeGAN) for 3D-aware image synthesis. The key idea is that an accurate 3D shape should yield a realistic rendering under different lighting conditions, which is achieved by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, the authors devise an efficient volume rendering strategy via surface tracking, which reduces the training and inference time by 24% and 48% respectively."
SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a quasi-Bayesian approach for instrumental variable (IV) regression, building upon the recent development in kernelized IV models. The proposed approach does not require additional assumptions on the data generating process, and leads to a scalable approximate inference algorithm with time cost comparable to the corresponding point estimation methods. The algorithm can be further extended to work with neural network models. "
SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,This paper proposes a cross-lingual open-retrieval answer generation (CORA) model for multilingual open question answering (QA) that can answer questions in any target language by retrieving evidence from any language and generating answers in the target language. The authors propose a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. A multilingual autoregressive generation model is used to generate the answers directly in target language without any translation or in-language retrieval modules as used in prior work. The model is trained iteratively by automatically extending annotated data available only in high-resource languages to low-resource ones. The results show that CORA substantially outperforms the previous state-of-the-art on the XOR-TYDI QA and MKQA datasets.
SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper studies the generalization properties of ERM-based domain generalization models. The authors firstly analyze the performance of ERMs on three popular domain generalisation datasets, and find that the seminal domain adaptation theory of Ben-David et al. (2007) does not provide a tight explanation of the out-of-domain generalization observed across a large number of models trained on these three datasets. Then, the authors investigate other possible measures that could explain generalization in this setting, including the Fisher information, predictive entropy, and maximum mean discrepancy. The results show that measures relating to the Fisher Information, predictive Entropy, and Maximum Mean Discrepancy are good predictors of the Out-Of-Distribution generalization."
SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper provides a theoretical framework to study backdoor data poisoning attacks for classification problems. Theoretically, the authors show that the memorization capacity (i.e., the ability to memorize the training examples) of a classification problem is a parameter that captures the intrinsic vulnerability of a learning problem to a backdoor attack. They then show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. Finally, they show that two related problems, backdoor filtering and robust generalization, are nearly equivalent."
SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of width and depth on the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In particular, it aims to understand how width affects (standard) neural networks once they have sufficient capacity for a given modeling task. The paper provides theoretical and empirical results on Deep GP suggest that large width can be detrimental to hierarchical models. Surprisingly, the paper proves that even non-parametric Deep GP converge to Gaussian processes, effectively becoming shallower without any increase in representational power."
SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper considers the setting of federated learning (FL) where a group of clients periodically coordinate with a central server to train a statistical model. The authors develop a general algorithmic framework called FedLin to tackle some of the key challenges intrinsic to FL, namely objective heterogeneity, systems heterogeneity, and infrequent and imprecise communication. The framework is motivated by the observation that various existing FL algorithms suffer from a fundamental speed-accuracy conflict: they either guarantee linear convergence but to an incorrect point, or convergence to the global minimum but at a sub-linear rate, i.e., fast convergence comes at the expense of accuracy. In contrast, when the clients’ local loss functions are smooth and strongly convex, the authors show that FedLin guarantees linear convergence to global minimum, despite arbitrary objective and systems heterogeneity. They also establish matching upper and lower bounds on the convergence rate of FedLin that highlight the effects of infrequent, periodic communication."
SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper studies the Sliced-Wasserstein (SW) distance, which is an alternative to the Wasserstein distance in machine learning applications. The authors propose a new perspective to approximate SW by making use of the one-dimensional projections of a high-dimensional random vector are approximately Gaussian. Based on this observation, they develop a simple deterministic approximation for SW. Their method does not require sampling a number of random projections, and is therefore both accurate and easy to use compared to the usual Monte Carlo approximation. They derive nonasymptotical guarantees for their approach, and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution."
SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to map the low-dimensional structure between representations learned by neural language models, translation models, and language tagging tasks to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. This method reveals a low-dim structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embedding. The authors also show that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI."
SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. It can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. The conditional generation from new labels achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, it is two orders of magnitude faster to produce than StyleGAN2."
SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper studies self-supervised learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. It proposes a loss that performs spectral decomposition on the population augmentation graphs and can be summarized as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by the proposed objective can match or outperform several strong baselines on benchmark vision datasets."
SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. It follows up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. They further show that this result can be strengthened to a localized version of the feedback edge sets, and provide corresponding lower bounds that complement previous results to provide a complexity classification of nearly all well-studied graph parameters. Last but not least, the results can be extended to the closely related problem of Polytree Learning."
SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper proposes a novel active learning algorithm for binary classification in the streaming setting. The algorithm leverages weak labels to minimize the number of label requests, and trains a model to optimize a surrogate loss on a set of labeled and weak-labeled points. The theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate the algorithm outperforms standard baselines."
SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov growth (KG), which is used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on the bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers."
SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised image representation learning that avoids the collapse problem by using two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, and (2) another term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalisation, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks."
SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes a general active reward learning approach for learning a model of the reward function that allows standard RL algorithms to achieve high expected returns with as few expert queries as possible. The proposed approach, called Information Directed Reward Learning (IDRL), selects queries that maximize the information gain about the difference in return between plausibly optimal policies. In particular, IDRL can use arbitrary Bayesian reward models and arbitrary types of queries, making it more general than existing methods. Moreover, it achieves similar or better performance with significantly fewer queries."
SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method to predict the parameters of unseen neural network architectures by leveraging the past knowledge of training other networks. The authors introduce a large-scale dataset of diverse computational graphs of neural architectures and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, the authors propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second. The proposed model achieves surprisingly good performance on unseen and diverse networks."
SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the distortion-perception (DP) function for the MSE distortion and the Wasserstein-2 perception index. It shows that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, it further provides a closed form expression for such estimators. For general distributions, it shows how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer and a minimizer of the M SE under a perfect perceptual quality constraint."
SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a new architecture for representation learning on the textual graph. The proposed GraphFormers consists of layerwise GNN components nested alongside the transformer blocks of language models. In the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, making each node’s semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where Graphformers outperform the SOTA baselines with comparable running efficiency."
SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of learning with user-level differential privacy (DP), i.e. protecting the privacy of the entire contribution of a user to a learning task, rather than only the individual samples. The authors propose algorithms for mean estimation, empirical risk minimization (ERM), stochastic convex optimization (SCO), and learning hypothesis classes with finite metric entropy, and show that for these tasks, the privacy cost decreases as O(1/\sqrt{m}/n) as users provide more samples. In contrast, when increasing the number of users $n$, the rate is faster than $1/n$. Lower bounds on the minimax optimality of the proposed algorithms are also provided."
SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper proposes a self-consistent Gaussian Process (GP) theory for deep neural networks (DNNs) trained with noisy gradient descent on a large training set. The main contributions of this paper are:  1. The authors show that the mean predictor of a finite DNN can be obtained from GP regression on a shifted target.  2. They show that a sharp transition between a feature learning regime and a lazy learning regime can be identified in a toy model of a two-layer linear convolutional neural network (CNN) trained on CIFAR-10.  3. They demonstrate that the assumptions required for their theory hold true in more realistic settings. 
SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the phenomenon of compositionality in signaling games. The authors show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, they prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. Finally, they provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence."
SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper presents ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer 2 in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per 4 patches. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet."
SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of online multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, this paper minimizes the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e., the label of a point is given by which of k centers it is closest to in Euclidean distance – this paper shows that one can achieve a loss that is independent of the total number of queries. The paper also shows that learning general convex sets requires an almost linear loss per query."
SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a method for knowledge transfer based on a regularization term in the loss function, supervising the sequence of required reasoning operations. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. They also demonstrate the effectiveness of this approach on the GQA dataset and show its complementarity to BERT-like self-supervised pre-training."
SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the problem of simulating Turing Machines (UTM) using RNNs with bounded-precision neurons. The authors propose a growing memory module that dynamically recruits new neurons when more memories are needed, and releases them when memories become irrelevant. They prove that a 54-neuron RNN with growing memory modules can simulate a Universal Turing Machine with time complexity linear in the simulated machine’s time and independent of the memory size. The result is extendable to various stack-augmented RNN. Furthermore, the authors analyze the Turing completeness of unbounded precision and bounded precision RNN, revisiting and extending the theoretical foundations of recurrent neural networks. "
SP:3f33489b98ba6145fd4e334669493f15a63455f4,This paper studies the under-cover bias of quantile regression in the regime where the number of samples $n$ and the dimension $d$ are under-parameterized and the ratio $d/n$ is small. The main result is that the learned linear quantile function $\alpha$ has an inherent under-coverage bias in this regime. The authors show that this bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories on quantile regressions. Experiments on simulated and real-world data verify the theory and further illustrate the effect of various factors such as sample size and model capacity. 
SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for dynamic memory allocation in class-incremental learning (CIL). The main idea is to use reinforcement learning to learn an optimal memory allocation policy for each incremental phase with continuously reinforced model performance and call it “reinforced memory management” (RMM). The proposed method is evaluated on CIFAR-100, ImageNet-Subset, and ImageNet’s full dataset."
SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper considers the problem of speeding up SGD by parallelizing it across multiple workers. The authors propose a local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. Their analysis shows that this can achieve an error that scales as 1/(NT ) with a number of communications that is completely independent of T. In particular, they show that \Omega(N) communications are sufficient. Empirical evidence suggests this bound is close to tight as it further shows that $\sqrt{N} or N communications fail to achieve linear speed-up in simulations."
SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the Online Lazy Gradient Descent (GLD) algorithm for linear optimization on strongly convex domains. The authors show that GLD is universal in the sense that it achieves O(log N) expected regret against i.i.d opponents. This improves upon the more complex meta-algorithm of Huang et al. [20] that only gets O(sqrt{N}logN) regret against adversarial opponents. In addition, unlike for the simplex, the order bounds for pseudo-regret and expected regret are equivalent for strongly-convex domains for GLD."
SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper analyzes the Bures-Wasserstein (BW) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. It shows that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the Affine-Invariant (AI) geometry. The paper also shows that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also Geodesic Convex (GC) under the BW geometry. "
SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a framework to evaluate NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. The proposed framework allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness. On each task, models are ranked according to the Dynascore, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset."
SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a multi-modal text-to-speech (TTS) model for automatic video dubbing (AVD) task: synthesizing human speech synchronized with the given video from the text. The proposed model uses the lip movement in the video to control the prosody of the generated speech. An image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments on the chemistry lecture single speaker dataset and LRS2 multi speaker dataset show that the proposed model can generate speech audios on par with state-of-the-art TTS models in terms of speech quality."
SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a novel federated learning framework for collaborative learning in medical imaging. The proposed framework is based on the recently proposed Vision Transformer (ViT) architecture with straightforward decomposable configuration, which is ideally suitable for split learning without sacrificing performance. Experimental results show that the proposed framework can achieve performance comparable to data-centralized training. "
SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper presents Shape-As-Point (SAP), a differentiable point-to-mesh representation for 3D surface reconstruction. The main idea is to use a Poisson surface reconstruction (PSR) solver to bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, which allows for efficient inference of surface reconstruction metrics such as Chamfer distance. The proposed approach is evaluated on surface reconstruction from unoriented point clouds and learning-based reconstruction."
SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper introduces a new family of inverse problems for recovering representations of corrupted data. The authors propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. They achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking."
SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of structural credit assignment in neural networks, i.e., attributing credit (or blame) to nodes in the network for correct (or incorrect) predictions. The authors first formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. They show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions. They introduce an off-Policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. They conclude by showing that these networks of agents can be more robust to correlated samples when learning online."
SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"This paper investigates the properties of the ventral and dorsal pathways of the visual system of mice. The authors train a deep neural network architecture with two parallel pathways using a self-supervised predictive loss function, and show that it is able to model both the dorsal and ventral pathways. The results show that the parallel pathway architectures can account for some of the functional specialization seen in mammalian visual systems."
SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper introduces TopicNet, a deep hierarchical topic model that can incorporate prior knowledge such as knowledge graph to guide the learning of the topic hierarchy. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations."
SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes an object-level self-supervised pretraining method for object detection. The proposed method, called Selective Object COntrastive learning (SoCo), is designed for the task of object detection and achieves state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework. The main contributions of this paper are three-fold: 1) it introduces object level representations via selective search bounding boxes as object proposals; 2) the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN); 3) it is equipped with object detection properties such as object level translation invariance and scale invariance."
SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"This paper presents a learning-augmented local search framework to solve large-scale vehicle routing problems (VRPs). The method iteratively improves the solution by identifying appropriate subproblems and delegating their improvement to a black box subsolver. The proposed method accelerates state-of-the-art VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000. The results generalize to a variety of VRP distributions, variants and solvers."
SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for active forgetting in the context of Bayesian continual learning. Specifically, the authors propose to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning by dynamically expanding parameters to learn each new task and then selectively combining them, which is formally consistent with the underlying mechanism of biological active forgetting. The method is evaluated on a variety of continual learning benchmarks, including CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks, where AFEC achieves the state-of-the-art performance."
SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for the prior guided random gradient-free (PRGF) algorithms. Moreover, they present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the performance of training a pruned neural network with accelerated gradient descent (AGD) in the lottery ticket hypothesis (LTH) setting. The authors analyze the geometric structure of the objective function and the sample complexity to achieve zero generalization error. They show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to number of non-pruned weights in the hidden layer. "
SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper studies the problem of query release, where the goal is to generate a synthetic dataset that approximately preserves the answers to a large collection of statistical queries subject to differential privacy. The authors propose two new methods: generative networks with the exponential mechanism (GEM) and private entropy projection (PEP). GEM circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. PEP is an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. Experimental results show that GEM and PEP outperform existing algorithms."
SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a simple mask classification method for semantic segmentation. The proposed method predicts a set of binary masks, each associated with a single global class label prediction, to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. In particular, the proposed method is supervised with the same per-pixel binary mask loss and a single classification loss per mask. Finally, a simple inference strategy to blend MaskFormer outputs into a task-dependent prediction format is proposed to achieve superior performance for larger classes."
SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies the problem of adversarial examples in random two-layer ReLU neural networks with sub-exponential width and smooth activation functions. In particular, the authors show that adversarial perturbations of the form of sign(f(x)) can be found by a single gradient descent step on the weights of the network with high probability. The main contribution of this paper is to extend the results of Daniely and Schacham [2020] to the overcomplete case, where the number of neurons is larger than the dimension (yet also subexponential in the dimension). "
SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) that trains SGMs in a latent space, using the variational autoencoder (VAE) framework. The authors propose a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs while outperforming them in sampling time by two orders of magnitude."
SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper studies the performance of convolutional neural networks (CNNs) over kernel methods in the setting of image classification. The authors show that CNNs are able to learn to find a sparse signal in the presence of high-variance noise, while the corresponding neural tangent kernels (NTK) are unable to adapt to the signal in this setting. They further show empirically that as the background noise increases in intensity, a CNN’s performance stays relatively robust, whereas the corresponding NTK sees a notable drop in performance. "
SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper provides a tighter analysis of the gradient tracking (GT) method in the stochastic strongly convex, convex and non-convex settings. The analysis improves over all existing results that analyze the GT algorithm. Specifically, they prove a weaker dependence on the connectivity of the network (spectral gap) which is commonly incorporated into the convergence rates via standard parameter p. "
SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm for the stochastic multi-armed bandit (MAB) problem. In particular, this paper provides new results on the arm-sampling behavior of UCB, leading to several important insights. First, it is shown that arm-sampling rates under UCB are asymptotically deterministic, regardless of the problem complexity. Second, the paper also provides the first complete process-level characterization of the MAB problem underUCB in the conventional diffusion scaling. Third, the worst-case lens adopted in this paper also reveals profound distinctions between UCB and Thompson Sampling."
SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper focuses on the cross-domain cold-start recommendation (CDR) problem, i.e., how to leverage the information from a source domain, where items are ‘warm’, to improve the recommendation performance of a target domain. The authors propose DisAlign, a framework that utilizes both rating and auxiliary representations from the source domain for the CDCSR problem. Specifically, the authors first propose Stein path alignment for aligning the latent embedding distributions across domains, and then further propose its improved version, called proxy Stein path, which can reduce the operation consumption and improve efficiency. The empirical study on Douban and Amazon datasets demonstrates the effectiveness of the proposed method."
SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a simple yet computationally efficient architecture, called Global Filter Network (GFNet), that learns long-term spatial dependencies in the frequency domain with log-linear complexity. The architecture replaces the self-attention layer in vision transformers with 3 key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2d inverse Fourier transforms. The experiments on ImageNet and downstream tasks demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness."
SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of predicting trustworthiness of predictions on large-scale datasets. The authors claim that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy. To improve the generalizability of the predictors, the authors propose a novel steep slope loss to separate the correct predictions from the incorrect predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, namely, Vision Transformer and ResNet, as trustworthiness predictions on ImageNet."
SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a new approach for learning end-to-end reconstruction operators based on unpaired training data for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then initialized with the output of reconstruction network and solved iteratively till convergence. Notably, it takes significantly fewer iterations to converge as compared to variational methods."
SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a new method for vision-and-language pre-training. The authors propose a contrastive loss to align the image and text representations BEfore Fusing (ALBEF) through cross-modal attention, which enables more grounded vision and language representation learning. They also propose a momentum distillation method to improve learning from noisy web data, which learns from pseudo-targets produced by a momentum model. The proposed method achieves state-of-the-art performance on multiple downstream vision-language tasks."
SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decision making policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. They also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper considers the non-smooth convex stochastic convex optimization problems with non-sub-Gaussian (heavy-tailed) noise. The authors derive the first high-probability convergence results with logarithmic dependence on the confidence level for convex problems with heavy-tailed noise. They also propose novel stepsize rules for two methods with gradient clipping. Moreover, their analysis works for generalized smooth objectives with Hölder-continuous gradients, and for both methods, they provide an extension for strongly convex problem. "
SP:a22a893e25ce739dc757861741014764e78aa820,This paper proposes a Transformer-based long-term forecasting model for time-series forecasting. The authors propose a novel decomposition architecture with an Auto-Correlation mechanism. This design empowers Autoformer with progressive decomposition capacities for complex time series. The proposed method outperforms the state-of-the-art in both efficiency and accuracy. 
SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a new dataset of cryptic crossword clues for NLP systems that seek to process compositional language in more creative, human-like ways. The dataset is composed of two parts: a definition and a wordplay cipher. The authors also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies. "
SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper analyzes the internal representation structure of ViTs and CNNs on image classification tasks. It finds that ViTs have more uniform representations across all layers. The authors explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. They also study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods."
SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) algorithms for combinatorial multi-armed bandit (CMAB) problems under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) optimization problems. The authors provide a problem-dependent regret lower bound of order $\Omega(\log T/2)$ to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper considers the problem of federated learning, which is a hedonic game in which error-minimizing agents form federating coalitions. The authors provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players, and analyze the relationship between the stability and optimality of an arrangement. They show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal. Finally, they give the first constant-factor bound on the performance gap between stability and optimal performance, proving that the total error of the worst stable solution can be no higher than 9 times that of an optimal solution."
SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point clouds. The capsule decompositions of objects are computed through permutation-equivariant attention, and the model learns to supervise the process by training with pairs of randomly rotated objects. The proposed method outperforms the state-of-the-art in reconstruction, canonicalization, and unsupervised classification."
SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the generalization properties of invariant feature averaging in kernel ridge regression (KRR). In particular, the authors show that when the target function is invariant to the action of a compact group (i.e., the group is a set of compact groups), the effective dimension of the RKHS induced by the invariant function is the same as that of the original function space. The authors also show that the action induces an orthogonal decomposition of the reproducing kernel Hilbert space (RKHS) and its kernel. "
SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a method for composing neural networks with itself to create an ensemble-like model with uncertainty that is useful for out-of-distribution detection, adversarial example detection, and model calibration. The authors propose to train a neural network classifier jointly with a generative model whose role is to map the classifier’s activations back to the input, approximating invertibility. They further explore this compositionality by combining DecNN with pretrained models where they show promising results that neural networks can be regularized from using protected features."
SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of plug-in estimators for optimal transport (OT) maps. The main contribution is a stability estimate for barycentric projections, which proceeds under minimal smoothness assumptions, and can be used to analyze general OT estimators. The stability estimate provides rates of convergence for the natural discretediscrete and semi-discrete estimators of OT maps, and also for Sobolev type, Besov type, kernel smoothed or wavelet based estimators, under additional smoothness assumption. The authors also provide faster rates for the Wasserstein distance between two probability distributions, which complement recent results in Chizat et al. (2020)."
SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. For instance, using only 10 datapoints (0.02% of original dataset), the authors obtain over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. The authors also extend their results across many other settings for MNIST, Fashion-MNIST, CifAR-10, CIFar-100, and SVHN. "
SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an approach for semi-supervised learning (SSL) based on novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open-set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. The proposed approach achieves state-of-the-art performance on three datasets and outperforms a fully supervised model on CIFAR10."
SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes a method for unsupervised goal reaching in visual environments. The proposed method, Latent Explorer Achiever (LEXA), learns a world model from image inputs and uses it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. LEXA solves tasks specified as goal images zero-shot without any additional learning."
SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper proposes a low-rank factorized representation of matrices that underpin dense, embedding, and self-attention layers to reduce the number of trainable parameters in transformers. Specifically, the authors propose to reshape and rearrange the reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. In Transformer models, the proposed approach leads to more than tenfold reduction in the total number of parameters, including embedding and feed-forward layers, with little degradation in on-task performance."
SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes to incorporate path encoding into the attention module of Transformer. Specifically, the authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. The authors also explore the interaction between these two kinds of paths by integrating them into the unified Transformer framework. Experiments on code summarization across four different languages demonstrate the effectiveness of the proposed approach."
SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a Transformer-based generator for high-fidelity image generation. The proposed HiT achieves state-of-the-art FID scores on unconditional ImageNet 128x128 and FFHQ 256x256 with a reasonable throughput. HiT employs a hierarchical structure of Transformers and divides the generative process into low-resolution and high-resolution stages, focusing on feature decoding and pixel-level generating, respectively. The HiT has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images."
SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. Geodesic convexity is a natural generalization of Euclidean convexness and allows the definition of convex sets and half-spaces. The authors prove an upper bound which is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They also show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension.
SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method for temporal action localization (TAL) learning. The proposed method reduces the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoders and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods."
SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The main contributions are three-fold. First, it provides general formulae for the derivatives of regularized M-stimators where differentiation is taken with respect to both y and X. Second, it characterizes the distribution of the residual ri = yi-xi \beta^T in the intermediate high-dimensional regime where dimension and sample size are of the same order. Third, it proposes a novel adaptive criterion to select the tuning parameters of the regularized estimator."
SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a calibrated beam-based summarization algorithm with global awareness of the global attention distribution. Specifically, a global protocol is proposed based on the attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring mechanism is then developed to regulate beam search to generate summaries in a near-global optimal fashion. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a transformer-based attention mechanism that is equivariant to the orientation of local coordinate systems (i.e., gauge-equivariant) and rotation invariant. The proposed method employs multi-head self-attention to jointly incorporate both position-based and content-based information. A novel method to parallel transport the feature vectors in the regular field of cyclic groups is proposed to enhance expressive ability. The experimental results show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes an approach for unsupervised learning of finite mixture models that combines the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The authors put emphasis on generality of the method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations. The performance of the proposed approach is illustrated in a variety of synthetic and real-data contexts, considering deep models, such as mixtures of normalizing flows and sum-product (transform) networks."
SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes an efficient sparse training method with completely sparse forward and backward passes. It first formulate the training process as a continuous minimization problem under global sparsity constraint, and then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, it uses the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, it proposes a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. Extensive experimental results on real-world datasets demonstrate that the proposed algorithm is much more effective in accelerating training process, up to an order of magnitude faster."
SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper proposes a novel non-equilibrium importance sampling (NEOIS) and Markov chain Monte Carlo (MCMC) samplers for the problem of sampling from a complex distribution $p_i$ and approximating its normalizing constant $Z = \in \mathbb{R}^p(x) \times \sqrt{x}$. The proposed method is based on the idea of non-Equilibrium orbits, where the target distribution is assumed to be invariant to the invertible map $T$ and the proposed method combines elements from the forward and backward orbits through points sampled from a proposal distribution $P_i$. The authors show that the proposed methods are unbiased estimators of $Z$ and self-normalized IS and MCMC estimators, respectively. The authors also provide theoretical results on the mixing time of the proposed algorithms."
SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,This paper studies the problem of large-scale mini-batch set encoding. The authors introduce a new property termed Mini-Batch Consistency (MBC) and propose a scalable and efficient attention-based set encoding mechanism. The proposed method adheres to the required symmetries of permutation invariance and equivariance as well as maintaining MBC for any partition of the input set. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method.
SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes a method to train an agent to play Diplomacy, a combinatorial game with more than 10 actions per turn. The main idea is to learn a policy proposal network that simultaneously performs value iteration and action exploration. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. The proposed method is evaluated on a two-player variant of Diplomacy and on a full-scale no-press Diplomacy."
SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling. The authors firstly show that non-selective attention sharing is sub-optimal for achieving good generalization across all languages and domains. Then, the authors propose to learn shared and specialized attention heads for different languages or domains. The proposed attention sharing strategies are evaluated on various tasks including speech recognition, text-to-text and speech to text translation."
SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift on the generalization performance of random feature regression (R-SGD) models. The authors derive exact high-dimensional asymptotics of the limiting test error, bias, and variance in this setting. The results motivate a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or help) test performance. The paper also shows that overparameterized models exhibit enhanced robustness."
SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling (TS) and other Bayesian sequential decision-making algorithms to misspecified priors in the context of Bayesian meta-learning. The authors prove that the expected reward of TS with a misspecification of the prior is $\tilde{O}(\epsilon^{-1}(H^2)$, where H is the learning horizon and H^2 is the total-variation distance between the priors and the true environment. They also show that the sensitivity is independent of the cardinality or structure of the action space, and they show that it is tight up to universal constants in the worst case. Finally, they establish generic PAC guarantees for algorithms in the recently studied Bayesian Meta-learning setting."
SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample complexity of PAC-learning and Equivalence-Query-learning (EQ-learning) models. The authors prove an exponential separation for the sample/query complexity between the two models. They also discuss how their result relates to adversarial robustness and generalization. 
SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection, i.e., given some data and a source model, can you quickly predict the model’s accuracy after fine-tuning. The authors formalize this setting as “Scalable Diverse Model Selection” and propose several benchmarks for evaluating on this task. They find that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. They then introduce simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for learning low-dimensional binary codes (LLC) for both instances and classes. The proposed method learns k-bit binary codes for classes that capture semantic information through a surrogate classification task, and then uses the learnt class codes as an efficient alternative to learning instance codes in sub-linear cost (in the number of classes, L) using the Error-Correcting Output Codes (ECOC) approach. The learned codes are super-efficient while still ensuring nearly optimal classification accuracy for ResNet50 on ImageNet-1K. The method is also applied to the efficient image retrieval and OOD detection problems."
SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a method of Generalized Depthwise-Separable (GDWS) convolution – an efficient, universal, post-training approximation of a standard 2D convolution. The proposed method is able to improve the throughput (frames-per-second) on real-life hardware while simultaneously preserving robustness to adversarial perturbations. The paper also provides exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. Experiments on CIFAR-10, SVHN, and ImageNet demonstrate the effectiveness of the proposed method."
SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a new retrosynthesis prediction method for the USPTO-50k dataset, which consists of 50000 reactions across 10 reaction classes. The authors propose a graph-based approach that capitalizes on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. The proposed method achieves a top-1 accuracy of 53.7%, outperforming previous template-free and semi-template-based methods."
SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a method for downscaling low-resolution (LR) data to high-resolution data. The authors propose a Bayesian formulation of deconditioning, which naturally recovers the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1]. The authors extend this formulation to LR data and devise an efficient conditional mean embedding estimator for multiresolution data. They also show that this solution can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions."
SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,This paper proposes a method for neural architecture search (NAS) for deep sparse networks (DSNs). The authors propose a distilled search space to cover the desired architectures with fewer parameters. They also develop a progressive search algorithm for efficient search on the space and capture the order-priority property in sparse prediction tasks. Experiments on three real-world benchmark datasets show promising results of PROFIT in accuracy and efficiency.
SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model on a target task with limited labeled data. The authors propose a regularized self-labeling approach that improves the generalization and robustness properties of pre-training. Specifically, the authors propose two regularization methods: layer-wise regularization to constrain the distance traveled in each layer; and label correction and label reweighting to correct mislabeled data points and reweight less confident data points. The proposed method is evaluated on a variety of image and text classification tasks. "
SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the best arm from a finite set of arms that has the smallest CVaR, VaR, or weighted sum of CVaRs and VaRs. The main contribution is an optimal \delta-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as $\delta approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. The authors develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures. "
SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes a new Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, it has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Experiments on ImageNet and downstream tasks prove the superiority of the proposed model."
SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper studies the problem of adversarial fine-tuning of pre-trained language models. The authors claim that adversarial training, the prevalent defense technique, does not directly fit a conventional fine- tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pretrained model. To address this problem, the authors propose Robust Informative Fine-Tuning (RIFT), a novel adversarial learning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to keep the features learned from the pre-train model throughout the entire fine tuning process, whereas a conventional one only uses the pre trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference."
SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper proposes a second-order Anderson mixing (SAM) method for solving nonconvex stochastic optimization problems. The main idea is to use damped projection and adaptive regularization to accelerate the Anderson mixing. Theoretical analysis of the convergence of SAM is provided, including the almost sure convergence to stationary points and the worst-case iteration complexity. Moreover, the complexity bound can be improved when randomly choosing an iterate as the output. Experimental results on image classification and language model demonstrate the advantages of our method."
SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a novel stochastic sampling method for linear inverse problems, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed method combines ideas from Langevin dynamics and Newton’s method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. It relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors show that the algorithm can produce multiple high perceptual quality samples for the same noisy observation."
SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a framework named MetaHG to detect drug traffickers on social media (i.e., Instagram), by tackling the following two new challenges: (1) different from existing works which merely focus on analyzing post content, the proposed framework is capable of jointly modeling multimodal content and relational structured information, and (2) it addresses the issue of requiring sufficient data for model training. More specifically, in this paper, the authors first build a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on Instagram. Then, they employ a relation-based graph convolutional neural network to learn node representations over the built HG, in which they introduce graph structure refinement to compensate the sparse connection among entities in the HG for more robust node representation learning. Afterwards, they propose a meta-learning algorithm for model optimization. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model."
SP:242da1384f48260d58a0e7949438611c05079197,"This paper investigates the question of whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). The authors use techniques from mixed-integer optimization, polyhedral theory, and tropical geometry to provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. The authors also present upper bounds on the sizes of neural networks required to represent functions in the neural hypothesis classes."
SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. In particular, given a set of risk sources, minimizing the worst-case attack loss can be reformulated by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations. Extensive experiments demonstrate that the approach leads to substantial attack improvement over the existing heuristic strategies and robustness improvement over state-of-the-art defense methods trained to be robust against multiple perturbations types."
SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis (SSTM), which is a statistical primitive generalizing both sparse PCA (in its Wigner form) and tensor PCA. The goal is to recover the k-sparse unit vector x in a polynomial time. For the highly sparse regime of $k\leq \sqrt{n}$, the authors present a family of algorithms that smoothly interpolates between a simple polynomially-time algorithm and the exponential-time exhaustive search algorithm. For any $1 \leq k$, the algorithms recovers the sparse vector for signal-tonoise ratio $\lambda\geq \mathcal{O}(\sqrt{\frac{t}{\epsilon}}(k/t)$, capturing the state-of-the-art guarantees for the matrix settings. The results extend to the case of r distinct $k \sparse signals with disjoint supports, with guarantees that are independent of the number of spikes. The algorithm improves over known tensor algorithms whenever the signal vector is highly sparse. The authors also present a lower bound against low-degree polynomorphism, which extends the known lower bounds for both sparse PCs and tensors."
SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. Experiments are conducted on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes."
SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of computing the equilibrium of two-player zero-sum matrix games with entropy regularization. The authors propose a decentralized algorithm for finding the quantal response equilibrium (QRE), which is the solution of a two-party matrix game that is a constrained saddle-point optimization problem with probability simplex constraints. The main contributions of this paper are the following: 1. Theoretical analysis of the last-iterate convergence of the QRE algorithm in the unconstrained setting. 2. Theorem 3.1 shows that the QR equilibrium of the unregularized matrix game can be found at a rate of linear in the size of the state and action spaces up to logarithm factors. 3. In the case of the entropy-regularized Markov game, the authors show that by controlling the knob of the regularization, the proposed algorithms can locate an approximate Nash equilibrium at a sublinear rate without assuming the Nash equilibrium to be unique. "
SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes a method for super-resolution of low-resolution (LR) screen content images (SCIs) at arbitrary scales. The authors propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR, which is able to perform continuous SR at arbitrary ratios. The pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs."
SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning interventional distributions using sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The proposed method is motivated and illustrated by a structural causal model themed around personal health. The empirical evaluation against competing methods from both generative and causal modelling demonstrates that interventional SPNs indeed are both expressive and causally adequate. This shows that the dream of tractable causal models is not insurmountable."
SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a deep Markov factor analysis (DMFA) method for functional magnetic resonance imaging (fMRI) data, which is a generative model that employs Markov property in a chain of low dimensional temporal embeddings together with spatial inductive assumptions, all related through neural networks, to capture temporal dynamics in fMRI data and tackle their high spatial dimensionality, respectively. Augmented with a discrete latent, DMFA is able to cluster fMRI images in its temporal embedding with regard to subject and cognitive state variability, which enables validation of a variety of fMRI-driven neuroscientific hypotheses. The experiments demonstrate that DMFA uncovers meaningful clusters in these data and achieves better predictive performance for unseen data relative to the state of the art."
SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper introduces a new class of generative models, called Moser Flows (MF), which extends the work of Mathieu and Nickel (2020) on continuous normalizing flows (CNF) for generative modeling over Riemannian manifolds. The main contribution of this paper is the use of the divergence of a neural network (NN) instead of the solution of an ordinary differential equation (ODE) to the change-of-variables formula in the original CNF work. The authors prove that MFs are universal density approximators over Euclidean sub-manifolds, and derive extensions to MFs for Euclid sub-Manifolds that efficiently efficiently parameterize vector fields projected to the desired parameter space. Theoretically, the authors show that MF constitutes a universal density approximation under suitable assumptions. Empirically, they demonstrate that MF models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks."
SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of non-linear independent component analysis (ICA) and blind source separation (BSS) in the context of unsupervised representation learning. In particular, the authors consider the setting where additional, typically observed variables are included in the generative process. The authors propose an approach based on the principle of independent causal mechanisms (ICM), which is motivated by thinking of each source as independently influencing the mixing process. They provide theoretical and empirical evidence that their approach circumvents a number of nonidentifiability issues arising in nonlinear Blind source separation."
SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a differentiable variant of Annealed Importance Sampling (AIS) with Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing (UHA). The main idea is to use an AIS-like procedure with an uncorrected MCMC instead of the accept-reject steps in Hamiltonian AIS, which is motivated by the fact that Hamiltonian dynamics sometimes have high acceptance rates. The authors show that UHA leads to tight and differentiable lower bounds on the log-normalization constant of the unnormalized target distribution. They also show that tuning the parameters of UHA, including the initial distribution, the MCMC kernel, and the bridging densities, leads to better performance than other competing approaches."
SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes an efficient and trainable local Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Specifically, when computing the induced norm of a weight matrix, the authors eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global one. The authors also propose to clip activation functions with a learnable upper threshold and a sparsity loss to further tighten the bound. Experiments on MNIST, CIFAR-10 and TinyImageNet datasets demonstrate the effectiveness of the proposed method."
SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,This paper proposes a scalable method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. Conformal inference provides finite sample frequentist guarantees on predictive confidence intervals without the requirement of model fidelity. The proposed method uses ‘add-one-in’ importance sampling to construct conformal predictive intervals from any Bayesian model only given model values. The authors demonstrate the utility on a range of examples including extensions to partially exchangeable settings such as hierarchical models.
SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the convergence of denoising-by-denoising (RED) and plug-and-play priors (PnP) methods. The authors introduce image denoisers derived as the gradients of smooth scalar-valued deep neural networks, acting as potentials. They show that the proposed potentials have symmetric Jacobians, allowing for MAP and MMSE estimators interpretation. They also show that they can be integrated into RED and PnP schemes with backtracking step size, removing the need for enforcing their Lipschitz constant. Theoretically, they show that their method converges to stationary points of an underlying objective function consisting of the learned potentials and provable stability."
SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music that is in-sync with the human body movements in videos. The proposed method, called RhythmicNet, works by extracting skeleton keypoints and implementing a sequence of models translating them to rhythmic sounds. It first infers the music beat and the style pattern from each frame to produce the rhythm. Next, it implements a transformer-based model to generate the hits of drum instruments and implements a U-net based model for generating the velocity and offsets of the instruments. Finally, additional types of instruments are added to the soundtrack by further conditioning on generated drum sounds. "
SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a simple one-step policy improvement algorithm for offline reinforcement learning (RL) that uses an on-policy Q-value estimate of the behavior policy to guide policy improvement. The authors show that the proposed algorithm outperforms existing iterative RL algorithms on most of the D4RL benchmark tasks. They hypothesize that the performance is due to the high variance inherent in doing off-policy evaluation and magnified by the repeated optimization of policies against those estimates. They also provide some guidance about when iterative algorithms can perform better than the simple one step baseline.
SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper considers the problem of low-rank and nonsmooth matrix optimization, where the objective function can be written as a maximum of smooth functions. The authors prove that under a generalized strict complementarity condition, the extragradient method, when initialized with a “warm-start” point, converges to an optimal solution with rate $O(1/t)$ while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method. They support their theoretical results with empirical experiments on several nonsmoothing low rank matrix recovery tasks."
SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a method to estimate the conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). The authors propose a generalized Robinson decomposition (GRD) to isolate the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and possesses a quasi-oracle convergence guarantee under mild assumptions. In experiments with small-world and molecular graphs, the proposed method outperforms prior work in CATE estimation."
SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper studies the problem of causal effect identification, i.e., determining whether a causal effect is computable from a combination of qualitative assumptions about the underlying system (e.g., a causal graph) and distributions collected from this system. In this paper, the authors develop a new causal identification algorithm which utilizes both graphical criteria and matrix equations. Specifically, they first characterize the relationships between certain graphically-driven formulae and matrix multiplications. Then, they further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise an algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating enriched matrix-based criteria into a graphical identification approach."
SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper proposes a new algorithm for unsupervised environment design (UED) based on prioritized level replay (PLR). The authors argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods they call Dual Curriculum Design (DCD), which includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows the authors to develop a novel theory for PLR with a robustness guarantee at Nash equilibria. The theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibrium."
SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper considers the multi-armed bandit (MAB) problem in the shuffle model, where the reward distribution is unknown, and the goal is to minimize the expected regret of the algorithm. Differentially private versions of the MAB problem have been considered in various settings, including the centralized model, the local model and the distribution-dependent model. This paper provides a (\epsilon, \delta)-differentially private algorithm with a distribution-independent regret of $O(\sqrt{k^T} + k^T + k \sqrt{\frac{log 1}{\delta} log T} )$, where $k$ is the number of arms and $\delta$ is a constant. The upper bound matches the best known algorithms for the centralized and local models. "
SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration, called threshold calibration, which is a stronger condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm that takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated Forecaster. The procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, the threshold calibration improves decision loss prediction without compromising the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The method optimizes a set of centroid points to compactly approximate the argmin distribution with a simple objective function. Theoretically, the argmax centroid method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth arg max distribution and the centroids approximation under proper conditions. The authors demonstrate the applicability and effectiveness of the method on a variety of real-world multitask learning applications, including few-shot image classification, personalized dialogue systems and multi-target domain adaptation."
SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers multi-objective reinforcement learning (MORL) where the objectives are balanced using preferences. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The authors provide a model-based algorithm that achieves a nearly minimax optimal regret bound. This result partly resolves an open problem raised by Jin et al [2020]."
SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a local explanation of response generation (LERG) method for dialog response generation. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. The authors show that LERG adheres to desired properties of explanation for text generation, including unbiased approximation, consistency, and cause identification. Empirically, the method consistently improves other widely used methods on proposed automaticand humanevaluation metrics for this new task by 4.4-12.8%."
SP:965413b1726617006317bbbec55673dd5d21812a,"This paper proposes a method to accelerate gradient compression in federated learning. In particular, the authors show that error compensated gradient compression methods can be combined with acceleration, and propose a loopless Katyusha method to achieve accelerated linear convergence rate under standard assumptions. Numerical experiments show that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms. "
SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a new method for tuning the weights of a liquid state machine (LSM) that is inspired by astrocytes, a long-neglected non-neuronal brain cell that modulates synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. The authors propose a neuron-astrocyte model that integrates neuronal activity and provides global feedback to spike-timing-dependent plasticity (STDP), which self-organizes NALSM dynamics around a critical branching factor that is associated with the edge-of-chaos. Experiments on MNIST, N-MNIST, and Fashion MNIST show that the proposed method achieves comparable performance to current fully-connected multi-layer spiking neural networks trained via backpropagation."
SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of topology-imbalance node representation learning (TINL), which is the imbalance caused by the positions of labeled nodes in graph data. The authors propose a model-agnostic method ReNode to address the topology imbalance issue by re-weighting the influence of labelled nodes adaptively based on their relative positions to class boundaries. They also propose a metric called influence conflict detection–based metric Totoro to measure the degree of graph topological imbalance and propose a method to reweight the labeled nodes according to their positions in terms of the relative location. The experimental results demonstrate the effectiveness and generalizability of the proposed method in relieving the TINL issue and promoting node classification. "
SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper considers the problem of recovering the partition of the lattice induced by the constancy regions of the unknown signal, using the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k^2k\log(N)/kappa, where k is the minimal number of rectangular sub-graphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, $\kappa$ is the signal difference among contiguous elements, and N is the size of the grid. The authors further extend the theoretical guarantees to the partition estimator based on the optimal regression tree estimator (ORT) of [12] and to the one obtained through an NP-hard exhaustive search method."
SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in NLI and image captioning tasks. The authors propose to perform maximum likelihood estimation (MLE) on the interventional distribution instead of the observational distribution, which they call Counterfactual Maximum Likelihood Estimation (CMLE). They provide theoretical analysis on the underlying general Structural Causal Model (SCM) and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning models using observational data. They conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. "
SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper proposes a method for multi-task learning (MTL) that minimizes the average loss function, while leveraging the worst local improvement of individual tasks to regularize the algorithm trajectory. CAGrad balances the objectives automatically and still provably converges to a minimum over the average. It includes the regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) in the multi-objective optimization (MOO) literature as special cases. Experiments on a series of challenging MTL tasks, including supervised, semi-supervised, and reinforcement learning tasks, demonstrate the effectiveness."
SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the few-shot learning of algorithmic concepts from small witness sets. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concept from a small witness set. They explore how several GPT architectures, program induction systems and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs. This first joint analysis of language models and machine teaching can address key questions for artificial intelligence and machine learning. "
SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method to explicitly distill feature representation into the robust and non-robust features, using Information Bottleneck. Specifically, it injects noise variation to each feature unit and evaluates the information flow in the feature representation to dichotomize feature units either robust or non robust, based on the noise variation magnitude. The distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, it presents an attack mechanism intensifying the gradient of non-Robust features that is directly related to the model prediction, to break the model robustness."
SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the support vector proliferation (SVP) phenomenon, which is a connection between support vector machine (SVM) and minimum Euclidean norm least squares regression (OLS). The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for SVP in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and bound the width of this transition, and give experimental support for its universality. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM."
SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the problem of best policy identification in Markov decision processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. They also provide the first algorithm with an instance-specific sample complexity in this setting."
SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a geometry-based query embedding model, namely Cone Embeddings (ConE), that can handle all the FOL operations, including conjunction, disjunction, and negation. ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunctions operations. The geometric complement operators in the embedding space for the negation operations are also designed. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper considers the problem of optimal control of stochastic nonlinear systems with separable cost and constraints in the state and input variables. The authors propose a novel numerical scheme for implementation of the corresponding value iteration (VI) algorithm in the conjugate domain. The proposed approach reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(\X + U) by replacing the minimization operation in the primal domain with a simple addition in the dual domain. Detailed analyses of the convergence, time complexity, and error of the proposed algorithm are provided."
SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes a VideoAudio-Text Transformer (VATT) to learn multimodal representations from unlabeled data using convolution-free Transformer architectures. The authors train VATT end-to-end from scratch and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text to video retrieval. VATT’s vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on kinetics-600, 72.7% on kinesis-700, and 41.3% on moments in time. The audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training."
SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes a method to accelerate the training of Transformer-based language models by replacing the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix. The authors also conduct theoretical analysis by showing that the matrix approximation error is small in the spectral norm. Experiments on the Long Range Arena benchmark show that the proposed method is sufficient in getting comparable performance than the full self-attention while requiring fewer computation resources.
SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data-efficient learning from parametric experts. Specifically, the paper focuses on the policy cloning setting, which allows for offline queries of an expert or expert policy. The proposed method achieves a very high level of data efficiency in transferring behavior from an expert to a student policy for high-degree-of-freedom (DoF) control problems using the augmented policy cloning (APC) approach, which combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The paper shows that APC increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also shows benefits of our approach in the context of algorithms in which policy cloning is a constituent part."
SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper studies the problem of image robustness, i.e., the ability of a computer vision system to perform well on unseen or shifting data distributions. The authors propose a framework that leverages the capability—and deep networks’ unusual sensitivity to input perturbations—to design “robust objects” that are explicitly optimized to be confidently classified. The framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments."
SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the problem of data augmentation in off-policy RL algorithms and proposes a simple yet effective technique to stabilize this class of algorithms under augmentation. The authors identify two problems, both rooted in high-variance Q-targets, and propose SVEA, a method that improves stability and sample efficiency of ConvNets and ViT-based RL algorithms. Experiments are conducted on DMControl and Distracting Control Suite to demonstrate the effectiveness of the proposed method. "
SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper proposes the Stochastic Two-Sided Momentum (STEM) algorithm for Federated Learning (FL), which is the first FL algorithm that achieves near-optimal sample and communication complexities simultaneously. The authors show that when both the WN’s and the server's update directions are chosen based on a momentum-assisted stochastic momentum estimator, the algorithm requires O(\epsilon^{3/2}) samples and O(\eps^{1/\eps^1) communication rounds to compute an optimal solution. Further, they show that there is a trade-off curve between the number of local updates and the minibatch sizes, on which the above sample/communication complexities can be maintained. Finally, they also show that for the classical FedAvg (a.k.a. Local SGD, which is a momentumless special case of the STEM), a similar tradeoff curve exists, albeit with worse sample and communications complexities."
SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the generalization properties of stochastic gradient descent (SGLD) with noise. The authors propose to optimize the noise structure of the noise in SGLD such that the noise covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. Theoretically, the authors show that the optimal noise is quite close to the empirical gradient. The empirical results are also provided. "
SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,This paper proposes a video compression framework that uses one model to support all possible prediction modes. The motion compensation module applies multiple 3D motion vector fields for weighted trilinear warping in spatial-temporal space. The voxel flows convey the information of temporal reference position that helps to decouple inter prediction modes away from framework designing. The flow prediction module is applied to predict accurate motion trajectories with a unified polynomial function.
SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper introduces and analyzes MT-OMD, a multitask generalization of Online Mirror Descent (OMD) that operates by sharing updates between tasks. Theoretical results show that the regret of MT-OMD is of order $\sqrt{1 + \sigma^2(N - 1)^T)$, where $\sigma$ is the task variance according to the geometry induced by the geometry regularizer, N is the number of tasks, and T is the time horizon. This improves upon the $O(\sqrt{\frac{1}{1/\delta}^T}$ bound obtained by running independent OMDs on each task. "
SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the problem of estimating the underdamped Langevin diffusion (ULD) with strongly-convex potentials. The authors propose an efficient discretization method, which requires O(N + d 1 3N^2 / \epsilon^2 3 ) gradient evaluations to achieve $\varepsilon$-error for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $\Omega(N+d1 3N^{2} / \epsilon^2)$, which indicates that the proposed method is optimal in dependence of N, $\epsilons, and d. The method is applied to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms."
SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"This paper proposes a new method for continual learning that combines weight regularization and projected gradient descent. The proposed method, called Natural Continual Learning (NCL), uses Bayesian weight regularizer to encourage good performance on all tasks at convergence and combines this with gradient projection using the prior precision, which prevents catastrophic forgetting during optimization. NCL outperforms both standard weight-based methods and projection-based approaches when applied to continual learning problems in feedforward and recurrent networks. Finally, the trained networks evolve task-specific dynamics that are strongly preserved as new tasks are learned."
SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to train injective normalizing flows (injective flows) that can be used for density estimation and out-of-distribution detection. The main idea is to backpropagate through the volume term arising from the injective volume-change term to scale the ambient dimensions close to 3,000. The first method involves exact evaluation of this term and its gradient which incurs higher memory cost. The second method uses conjugate gradients and Hutchinson’s estimator to obtain unbiased estimations. Both methods perform end-to-end nonlinear manifold learning and density estimation for data projected onto this manifold."
SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a framework for inference and learning in neural networks with slow components. The main idea is to leverage the ability of biological neurons to phase-advance their output with respect to their membrane potential. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases. The proposed model can be interpreted as a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity."
SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph representation learning, called Nested Graph Neural Networks (NGNNs), which aims to represent a graph with rooted subgraphs instead of rooted subtrees. The key idea is to extract a local subgraph around each node and apply a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretical analysis shows that NGNN is strictly more powerful than 1-WL, and it can discriminate almost all r-regular graphs. Moreover, unlike other more powerful GNNs, NGNN only introduces a constant factor higher time complexity than standard GNN."
SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes nested variational inference (NVI), a family of methods that learns proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model, and (c) perform amortized inference in hierarchical deep generative models. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristic to guide the sampler."
SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of finding and certifying an $\epsilon$-approximate maximizer of a Lipschitz function f on a compact subset $X$ of $R$ variables, with the additional constraint that algorithms must certify the accuracy of their recommendations. Under a weak assumption on $X$, this optimal sample complexity is shown to be nearly proportional to the integral $\max(f)^{-f(x) + \varepsilon$, which was previously known in dimension $d = 1$. The authors also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes an attack on confidence estimation methods for deep neural networks (DNNs). The attack is based on perturbations to the network parameters, which are of minuscule magnitude, but can cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimations. The authors demonstrate successful attacks on three of the most popular uncertainty estimation methods: the vanilla softmax score, Deep Ensembles and MC-Dropout. Additionally, the attack is also shown to work on the selective classification architecture."
SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the fundamental statistical limits of community detection algorithms in the streaming stochastic block model (StSBM) setting, where nodes are revealed one at a time in random order. The authors introduce a simple model for networks growing over time which they refer to as streaming StSBM and show that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. "
SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost in predictor space induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. They also study the reverse problem, identifying which regularizers (e.g., group quasi-norms, the k-support-norm, elastic net) can be the regularizers induced by simple l2-regularization and designing the parameterizations that do so."
SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper presents a method for reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Given a source entity and a binary relation (eq,rq), the method first retrieves k entities in the KG that are similar to eq. Next, for each of the retrieved entities, the method finds a set of reasoning paths that connect the retrieved entity to the query entities that are connected with rq via the query relation. The method achieves new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122."
SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,This paper presents an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links. The model achieves the state-of-the-art performance on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. The authors also present promising results on more challenging settings such as end-to-end entity linking and entity linking without in-domain training data. 
SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper proposes to use ensemble Active Learning (AL) methods to perform data subset acquisition at a large scale (10k to 500k samples at a time) with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. This allows them to automatically and efficiently perform a training data subset search for large labeled datasets. They observe that their approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset. They perform an extensive experimental study of this phenomenon on three image classification benchmarks (CIFAR-10, CifAR-100 and ImageNet), analyzing the impact of initialization schemes, acquisition functions and ensemble configurations."
SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper proposes a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. The routing mechanism is designed via inverted dot-product attention, and imposes Layer Normalization as normalization; and replaces sequential iterative routing with concurrent iterative routes. The proposed method improves performance on benchmark datasets such as CIFAR-10 and CifAR-100, and it performs at-par with a powerful CNN with 4x fewer parameters."
SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a novel approach to hyperparameter optimization for deep learning models. The main idea is to use the parallel tempering technique of statistical physics to allow hyperparameters to exchange during training. The authors claim that this approach is more efficient and parallelizable than existing methods, and that it is more resilient to overfitting and achieves smaller overall errors. The proposed approach is evaluated on dropout and learning rate optimization."
SP:beba754d96cc441712a5413c41e98863c8abf605,"This paper provides a theoretical analysis of Reinforcement Learning (RL) methods for machine translation (MT). The authors show that Reinforcement learning methods for MT do not optimize the expected reward, and show that other methods take an infeasibly long time to converge. The authors also show that the performance gains observed in the literature likely stem not from making target tokens the most probable, but from unrelated effects, such as increasing the peakiness of the output distribution (i.e., the probability mass of the most likely tokens)."
SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,This paper studies the large-sample behaviors of the Q-value estimates with closed-form characterizations of the asymptotic variances. The authors propose a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. Numerical experiments show superior performances of the exploration strategy than other benchmark approaches.
SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"This paper proposes a method to improve the efficiency of top-k recommendation by representing users and items as binary codes. The proposed method, called collaborative generated hashing (CGH), is designed to learn hash functions through the Minimum Description Length (MDL) principle; thus, it can deal with various recommendation settings. In addition, CGH initiates a new marketing strategy through mining potential users by a generative step. Extensive experiments on two public datasets show the advantages for recommendations in various settings over competing baselines."
SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"This paper proposes Adversarial Inductive Transfer Learning (AITL), a method for addressing discrepancies in input and output spaces between source and target domains. AITL utilizes adversarial domain adaptation and multi-task learning to address these discrepancies. The main application is pharmacogenomics where the goal is to predict drug response in patients using their genomic information. The proposed method outperforms state-of-the-art pharmacogenomic and transfer learning baselines and may guide precision oncology more accurately."
SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes a mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. The authors propose to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward. The proposed approach is tested on a collection of benchmark tests including simulated robotic locomotion.
SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a method to detect adversarial examples by using capsule networks. Capsule networks allow a class-conditional reconstruction of the input image, and the authors propose to use the reconstruction subnetwork in a CapsNet as an attack-independent detection mechanism. Specifically, they reconstruct a given input from the pose parameters of the winning capsule and then detect the difference between the reconstruction distributions for natural and adversarial (or otherwise corrupted) images. They extend this detection mechanism to standard convolutional neural networks and show its effectiveness against black box and white box attacks on three image datasets. They show that capsule models achieve the strongest attack detection rates and accuracy on these attacks."
SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"This paper studies the effect of initialization and activation function on the Neural Tangent Kernel (NTK) with respect to the training dynamics of deep neural networks. In particular, the authors show that an initialization known as the Edge of Chaos (Yang & Schoenholz, 2017) leads to better training dynamics and that a class of smooth activation functions discussed in (Hayou et al., 2019) also improves the learning dynamics compared to ReLU-like activation functions (see also Clevert et al. 2016). The authors also provide experiments illustrating their theoretical results."
SP:dd59b897384c52c20d62be73fc33184c8c226f4b,This paper proposes a self-supervised method to learn sentence representations with an injection of linguistic knowledge. The proposed method learns sentence embeddings by contrasting multiple linguistic representations. The motivation is to benefit from linguistic structures diversity to discard noises inherent to each representation. The authors aim at encoding high-level representations by aligning the underlying shared information from multiple views.
SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"This paper proposes a transfer learning method for financial sentiment classification. The proposed method, called FinBERT, is a pre-trained language model based on BERT for financial NLP tasks. It is evaluated on two financial sentiment analysis datasets, where it achieves the state-of-the-art on FiQA sentiment scoring and Financial PhraseBank. The authors conduct experiments to investigate the effects of further pre-training on financial corpus, training strategies to prevent catastrophic forgetting and fine-tuning only a small subset of model layers for decreasing training time without a significant drop in performance."
SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a new task of singing voice synthesis, i.e., singing voice generation without pre-assigned scores and lyrics, in both training and inference time. Three tasks are considered: free singing, accompanied singing, and solo singing. The proposed tasks are either unconditioned or weakly conditioned, and the authors propose a pipeline to tackle these new tasks. This involves the development of source separation and transcription models for data preparation, adversarial networks for audio generation, and customized metrics for evaluation."
SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,This paper proposes a novel adversarial defense technique that leverages a latent high order factorization of the network. The proposed approach can be easily integrated with any arbitrary neural architecture and combined with techniques like adversarial training. The authors empirically demonstrate the effectiveness of the proposed approach on standard image classification benchmarks and audio-based classification tasks.
SP:762729b64c1c1494de0f7410ea3662da61e93b6d,"This paper studies the problem of region-based spatio-temporal forecasting in the context of traffic data. The authors propose a clustered graph transformer framework that integrates both graph attention network and transformer under an encoder-decoder architecture to address the unsmoothness issue. Specifically, the authors propose two structural components to refine the architectures of those existing deep learning models. In spatial domain, they propose a gradient-based clustering method to distribute different feature extractors to regions in different contexts. In temporal domain, multi-view position encoding is proposed to address periodicity and closeness of urban time series data. Experiments on real datasets obtained from a ride-hailing business show that the proposed method can achieve 10%-25% improvement than many state-of-the-art baselines."
SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper analyzes the deep Q-network (DQN) algorithm from both algorithmic and statistical perspectives. In particular, this paper focuses on the fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network used in DQNs. Under mild assumptions, the authors establish the rates of convergence for the action-value functions of the iterative policy sequence obtained by FQI. The statistical error characterizes the bias and variance that arise from approximating the action value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, the analysis provides justifications for the techniques of Experience Replay and Target Network, which are crucial to the empirical success of deep QN. "
SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"This paper proposes a new attention architecture called PhraseTransformer to represent the composition of words and phrases in natural language. The authors argue that the phrases play an important role in attention. Besides representing the words of the sentence, they introduce hypernodes to represent candidate phrases in attention, which is similar to the standard Transformer. The first phase is used to attend over all word/phrase pairs, and the second phase represents the inductive bias within each phrase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved."
SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper presents a modular neural network architecture for learning algorithms given a set of input-output examples. The architecture consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, MAIN uses a general domain-agnostic mechanism for selection of modules and their arguments. The authors evaluate MAIN on five algorithmic tasks and show that it can learn policies that generalizes perfectly to inputs of much longer lengths than the ones used for training."
SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"This paper proposes a Monte Carlo deep neural network arithmetic (MCDA) method to determine the sensitivity of deep neural networks to quantization in floating point arithmetic. MCDA is based on applying Monte Carlo arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The method makes no assumptions regarding the underlying parameter distributions and directly measures the effects of quantization on the problem under study, which allows us to provide insights into the precision requirements of any given inference network for any given dataset. "
SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"This paper studies the problem of objective mismatch in model-based reinforcement learning (MBRL). In particular, the authors identify a fundamental issue of the standard MBRL framework – what they call the “objective mismatch issue”, which arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, they characterize the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. The authors propose an initial method to mitigate the mismatch issue by reweighting dynamics model training."
SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper presents a new adversarial attack method that exploits both class-wise and layer-wise deep feature distributions. The proposed method achieves state-of-the-art targeted blackbox transfer-based attack results for undefended ImageNet models. The methodology affords an analysis of how adversarial attacks change the intermediate feature distributions of CNNs, as well as a measure of distributional separability/entanglement. "
SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper introduces a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. The authors show that by utilizing the dual formulation of the WD, we can learn score functions over trajectories that can be in turn used to lead policy optimization towards (or away from) (un)desired behaviors. They incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient (BGPG) and Behavioural Guided Evolution Strategies (BGES). They demonstrate that these algorithms outperform existing methods in a variety of challenging environments."
SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper proposes a stochastic gradient descent (SGD)-like algorithm for training deep neural networks. The main idea is to compute an adaptive learning rate in closed form at each iteration. The learning rate is computed for the non-stochastic gradient direction when the minimum value of the objective function is known (Polyak, 1969, Shor, 1985, Brännlund, 1995, Nedić & Bertsekas, 2001a;b). And second, one such minimum value is usually approximately known for interpolating models: for instance, it is close to zero for a model trained with the cross-entropy loss. The proposed algorithm ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune."
SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper investigates the role of bottom-up, horizontal, and top-down connections in perceptual grouping in deep neural networks. The authors evaluate the performance of neural networks with combinations of these connections on two tasks: object-based and Gestalt-based perceptual grouping. The experiments show that networks with top-up connections are able to perform better than networks that rely solely on bottom up connections on the task of object grouping. "
SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper proposes DeepHoyer regularizers, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. The proposed regularizers are inspired by the Hoyer measure (the ratio between `1 and `2 norms) used in traditional compressed sensing problems. The experiments show that enforcing the proposed regularizer can produce even sparser neural network models than previous works, under the same accuracy level. "
SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper proposes a variant of Adam (referred to as SAdam) that achieves a data-dependent $O(\log T)$ regret bound for strongly convex functions. The main idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. The proposed SAdam reduces to SC-RMSprop, which is a recently proposed variant of RMSprop. Theoretical analysis demonstrates that SAdam achieves a regret bound that follows the general $O(log T^2)$ general regret bound, which means that it converges faster than AMSgrad and AdamNC in such cases."
SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. The model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short or long-range contextual information. The authors conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%."
SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"This paper studies the effect of pruning on the generalization of deep neural networks (DNNs). Specifically, the authors define the notion of “pruning instability”, which is the size of the drop in network accuracy caused by a pruning iteration (Section 3). They then empirically analyze the instability and generalization associated with various magnitude-pruning (Han et al., 2015b) algorithms in different settings (Section 4). They find a tradeoff between the stability and potential generalization benefits, and show iterative pruning’s similarity to regularizing with noise—suggesting a mechanism unrelated to parameter counts through which pruning appears to affect generalization. Finally, they derive a batch-normalized-parameter pruning algorithm to better control pruning stability."
SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"This paper proposes a method for neural architecture search (NAS) with reinforcement learning (RL). The main idea is to search in an embedding space by using architecture encoders and decoders. The architecture simulator simulates the origin architecture space, which assists the real architecture encoder learning. The decoder realizes the relationship between origin architecture and architecture-embedding, which maps the architecture embedding to the origin network architecture. The proposed method is evaluated on CIFAR-10 classification task."
SP:e2e5bebccc76a51df3cb8b64572720da97174604,This paper proposes a homotopy training algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low-dimensional structure and tracks the solution to the high-dimensional coupled system. The authors have proved the convergence of HTA for the non-convex case and existence of the solution path for the convex case. HTA has provided a better accuracy on several examples including VGG models on CIFAR-10.
SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,"This paper introduces the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. The authors show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning. The experiments show that the simplicial agent confers an advantage over the relational agent as an inductive biases in our task."
SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,"This paper proposes a method for the task of pose estimation, i.e. estimating the orientation of an object in an image with respect to a fixed frame of reference. The proposed method is based on conditional variational autoencoders (CVAEs) with circular latent representations to estimate the corresponding 2D rotations. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing."
SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a method to scale up multi-agent reinforcement learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Specifically, it maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets with the best adaptability to the next stage. The method is evaluated on a popular MARL algorithm, MADDPG, and empirically shows that it consistently outperforms baselines by a large margin as the number of agents grows exponentially."
SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,"This paper proposes a multi-layer multiplex graph neural network architecture for abstract diagram reasoning. Specifically, the model encodes subsets of diagram panels into multiplex graphs, and combines the feature embeddings representing the relation relationships in the subset summardings to predict the correct answer. The model outperforms WReN, the state-of-the-art model, on the PGM dataset and RAVEN dataset."
SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,This paper proposes an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. The authors use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood.
SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"This paper investigates the existence of winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve a comparable accuracy to the test accuracy in a similar number of iterations. The authors discover for the first time that the winning tickets can be identified at a very early training stage, which they term as Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Furthermore, they propose a mask distance metric to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, the authors propose efficient training methods to develop EB tickets and the proposed mask distance to identify the EB tickets, which are achieved by first identifying EB tickets via low cost schemes, and then continuing to train merely the EB ticket to achieve the target accuracy."
SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"This paper proposes a method to improve the robustness of convolutional neural networks against adversarial attacks. The proposed method is based on the observation that the intrinsic dimension of image data is much smaller than its pixel space dimension and the vulnerability of neural networks grows with the input dimension, and proposes to embed high-dimensional input images into a low-dimensional space to perform classification. The authors propose to use a discriminator in the latent space that tries to separate the generated code vectors from the encoder network and the ideal code vectors sampled from a standard Gaussian distribution. Based on the optimal transport theory, the proposed ER-Classifier minimizes the discrepancy between the true label and the output distribution of the framework, thus retaining important features for classification in the embedding space. Experimental results on several benchmark datasets show that the proposed framework achieves state-of-the-art performance against strong adversarial attack methods."
SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper proposes a neural, end-to-end model for jointly extracting entities and their relations. The model is built on top of a large pre-trained language model. The authors claim that the model is fast to train because the bulk of the model’s parameters are pretrained and it eschew recurrence for self-attention. The proposed model achieves state-of-the-art performance on 5 datasets across 3 domains."
SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper studies the problem of learning a low-level feature representation for items, i.e., RGB values of images in the image context. The authors assume that no meaningful representation of the items is given. Instead, we are provided with the answers to some triplet comparisons of the following form: Is item A more similar to item B or item C? The authors provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answer to the above-mentioned triplet comparison. They demonstrate that their proposed approach is significantly faster than existing methods, and can scale to real-world large datasets."
SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"This paper proposes a meta-learning framework to learn data values jointly with the target task predictor model. The data value estimator (modeled by a deep neural network) is used to learn how likely each datum is used in training of the predictor model, and it is trained using a reinforcement signal of the reward obtained on a small validation set that reflects performance on the target target task. The proposed method is evaluated on corrupted sample discovery, domain adaptation, and robust learning."
SP:e2c3374629cfd654b7b35e88507e65646d70470e,"This paper studies the statistical properties of the gradient in random fully connected ReLU networks, through the study of the the per layer Jacobian in finite-sized networks. The authors compare three types of architectures: vanilla networks, ResNets and DenseNets. They show that while the variance of Jacobian squared norm is exponential in depth for ResNet and polynomial for DenseNet, there exists an initialization strategy for both, such that the norm is preserved through arbitrary depths, preventing exploding or decaying gradients in deep networks. They also show that the statistics of the per-layer Jacobian norm is a function of the architecture and the layer size, but surprisingly, not the layer’s depth."
SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes an approach to reduce the compilation time for neural network code optimization. The main idea is to use reinforcement learning to learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. Specifically, the authors propose an adaptive sampling algorithm that leverages reinforcement learning whose solution takes fewer steps to converge, and an adaptive clustering algorithm that utilizes clustering to adaptively reduce the number of costly hardware measurements, and a domain-knowledge inspired Sample Synthesis algorithm that focuses on the costly samples (real hardware measurements) on representative points and improves the samples itself. Experiments with real hardware shows that CHAMELEON provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of modern deep networks by 5.6%."
SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper studies the problem of certified robustness for top-k predictions, i.e., given an example x and a classifier f and a perturbation $\delta$, the classifier g(x + \delta) makes predictions for x + \epsilon as long as the $\ell_2$-norm of the adversarial perturbations is less than a threshold (called certified radius). The certified radius is the unique solution to the certified radius equation, which depends on σ, and the largest probabilities pi’s for the example x. In this paper, the authors propose to use randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. The main theoretical result is a tight certified radius bound of $\ell_{2}$ for top k predictions, which is a special case of the original certified radius for top 1 predictions. The authors also empirically evaluate the proposed method on CIFAR10 and ImageNet. "
SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the relationship between the gradient signal to noise ratio (GSNR) of parameters during training of deep neural networks (DNNs) and the generalization performance. The GSNR of a parameter is defined as the ratio between its gradient’s squared mean and variance, over the data distribution. The authors establish a quantitative relationship between model parameters’ GSNRs and generalization gap. This relationship indicates that larger GSNr during training process leads to better generalisation performance. Moreover, they show that the gradient descent optimization dynamics of DNNs naturally produces large GSNrs during training, which is probably the key to the remarkable generalization ability."
SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes a method for answering queries over large-scale knowledge graphs (KGs). The proposed method, called QUERY2BOX, is an embedding-based framework for reasoning over KGs that is capable of handling Existential Positive (FO) logical queries (i.e., queries that include any set of $\�, \�, and \tilde$ operators) in a scalable manner. Specifically, it uses a box-aligned hyper-rectangle to represent a query and embeds KG entities and queries into a low-dimensional vector space such that entities that answer the query are embedded close to the query. The authors show that queries can be embedded as boxes, where a set of points inside the box corresponds to the set of answer entities of the query, and they show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KGs entities. Experiments on three large KGs demonstrate the effectiveness of the proposed method."
SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"This paper studies the convergence of SGD with consistent gradient estimators for strongly convex, convex and non-convex objective functions. The paper builds on the work of Chen et al. (2018) who proposed to use a consistent estimator as an economic alternative to the full gradient in SGD. The authors show that consistent estimators result in the same convergence behavior as do unbiased ones. They verify the results with illustrative experiments on synthetic and real-world data. "
SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. It can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, it also proposes a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of subnetworks that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy."
SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper extends the Neural Module Networks (NMNs) to answer compositional questions against a paragraph of text. Specifically, the authors introduce modules that reason over a paragraph, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner, and propose an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, they show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning."
SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"This paper studies the effect of initialization on the connection sensitivity of pruning neural networks. The authors first show that the initial weights of the network are critical to the performance of the pruning at initialization method. Then, the authors analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. Finally, they empirically demonstrate the effectiveness of the proposed method on various datasets."
SP:d5899cba36329d863513b91c2db57675086abc49,"This paper proposes a new initialization scheme for sparse neural network initialization that allows to explore the space of very deep sparse networks. The authors evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, the authors develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on and arrive at a single topology that satisfies all of them."
SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"This paper studies the training of recurrent neural networks (RNNs) on long sequence tasks. The authors develop a mean field theory of signal propagation in LSTMs and GRUs that enables them to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, they derive a novel initialization scheme that eliminates or reduces training instabilities. They demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower."
SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"This paper proposes a method to improve the classification accuracy of deep neural networks on remote sensing scene images by leveraging the semantic similarity between neighboring scene images. The proposed method consists of a siamese sub-network, a similarity metric learning component, a convolutional network, and a decision layer. The similarity metric is used to evaluate how similar the extracted features are to the input features. The decision layer is then used to perform classification or regression. The method is evaluated on a disease density estimation task."
SP:99c10e038939aa88fc112db10fe801b42360c8dc,"This paper proposes a method for self-supervised monocular depth estimation, where geometry is the only source of supervision. The proposed method uses pixel-adaptive convolutions to learn semantic-dependent representations that can better capture the equivariance property. The authors also propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Experiments on the standard KITTI benchmark show that the proposed method outperforms the state-of-the-art methods."
SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"This paper proposes a method to build linear classifiers based on deep features that are certifiably robust against a strong variant of label-flipping attacks, where the adversary can target each test example independently. The approach leverages randomized smoothing, a technique that has previously been used to guarantee test-time robustness to adversarial manipulation of the input to a classifier. The proposed method is able to obtain the certified bounds with no additional runtime cost over standard classification."
SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper studies the use of differential privacy for anomaly detection and backdoor attack detection. The authors show that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. They first provide a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of the proposed method."
SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper studies the problem of uncertainty calibration for regression tasks. The authors propose a simple histogram-based approach inspired by reliability diagrams used in classification tasks. They show that the existing definition for calibration of a regression uncertainty (Kuleshov et al., 2018) has severe limitations in distinguishing informative from non-informative uncertainty predictions, and propose a new definition that escapes this caveat and an evaluation method using simple histograms. They also propose a scaling-based calibration that preforms well in our experimental tests."
SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a method for disentangling BERT representations into Tensor-Product Representations (TPRs), which is a collection of constituents, each of which is the binding of a filler to a structural role. The model learns to separate fillers from roles in an unsupervised fashion, trained end-to-end to perform an NLP task. Experiments on GLUE benchmark and HANS show that untangling data-specific semantics from general language structure is key for better transfer among NLP tasks."
SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"This paper proposes a method for multi-agent reinforcement learning that combines deep reinforcement learning (DRL) and hierarchical RL (HRL). Specifically, the authors propose a partial parameter sharing approach wherein the lower level of the hierarchy is shared enabling learning using decentralized methods. This drastically reduces the overall parameter space in the multiagent problem and introduces structure in the optimization problem. The authors also build on top of prior work in learning goal-conditioned policies to learn low-level physical controllers for balance and walking. These lower-level controllers are task-agnostic and can be shared by higher-level policies."
SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,This paper proposes a method to improve the performance of graph neural networks (GNNs) by using a spatial representation of the graph. The proposed method is based on a graph embedding method and a graph pooling method. The authors show that the proposed method outperforms the state-of-the-art methods on the classification task. 
SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,This paper provides a formal definition of shift-equivalent and anti-aliasing pooling when down sampling is used in convolutional neural networks (CNNs). The authors propose a method called frequency pooling which is a combination of (inverse) Discrete Fourier Transform (DFT) and low-pass filtering (low-pass filter) to achieve shift equivalent pooling. The authors prove that F-pooling is the optimal down-sampling operation from the perspective of reconstruction. Experiments on CIFAR-10 and ImageNet show that the proposed method achieves better accuracy and robustness to input shifts.
SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"This paper proposes a simple technique to improve the generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. The authors also provide an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. They demonstrate the superiority of their method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks."
SP:31772a9122ec998c7c829bc4813f6147cdc30145,"This paper proposes an explanation approach for image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification. In this task, an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match is proposed. The proposed method is evaluated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes."
SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes a flow-based generative model for raw audio, which is trained with maximum likelihood without density distillation and auxiliary losses as used in Parallel WaveNet (van den Oord et al., 2016) and ClariNet (Ping et. al., 2019). The proposed method, called WaveFlow, can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms. In particular, the small-footprint WaveFlow has 5.91M parameters and can generate 22.05kHz high-Fidelity speech 42.6x faster than real-time on a GPU."
SP:963e85369978dddcd9e3130bc11453696066bbf3,This paper proposes a graph translation-generative-adversarial-network (GT-GAN) for graph generation. It consists of a graph translator equipped with graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A new conditional graph discriminator is proposed to classify the target graphs by conditioning on input graphs while training. Extensive experiments on multiple synthetic and real-world datasets demonstrate that the proposed GT-GAN significantly outperforms other baseline methods in terms of both effectiveness and scalability.
SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a new neural sequence modeling unit, called Meta Gated Recursive Controller. The proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms of METAGROSS are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. The authors postulate that their proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks)."
SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a self-supervised learning framework for speech recognition based on a strong language model to provide learning signal given unlabeled speech. The proposed method, called local prior matching (LPM), leverages a language model, which can take advantage of vast quantities of both unpaired text and speech. LPM reduces the WER by 26% and 31% relative on a clean and noisy test set, respectively. By augmenting LPM with an additional 500 hours of noisy data, the proposed method improves the performance by 15% relative."
SP:e6af249608633f1776b608852a00946a5c09a357,"This paper considers the problem of fair and robust model training in the presence of data poisoning. The authors propose a generative adversarial network (GAN) framework that is able to perform both fair training and robust training using GANs. In particular, the authors propose two discriminators: (1) a fairness discriminator that predicts the sensitive attribute from classification results and (2) a robustness discriminator which distinguishes examples and predictions from a clean validation set. The proposed framework respects all the prominent fairness measures: disparate impact, equalized odds, and equal opportunity. Also, FR-GAN optimizes fairness without requiring the knowledge of prior statistics of the sensitive attributes. The experimental results show that the proposed framework shows almost no decrease in fairness and accuracy."
SP:6306417f5a300629ec856495781515c6af05a363,"This paper presents a physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. The learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, with moving particles. The flow velocities generated from a generalized, high-dimensional force field. The authors demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance."
SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"This paper proposes a new lens for studying gradient clipping, namely, robustness, and shows that for the common problem of label noise in classification, standard gradient clipping does not in general provide robustness. However, a simple variant of gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function. Experiments on both synthetic and real-world datasets show that partially Huberised versions of standard losses (e.g., softmax cross-entropy loss) perform well in the presence of noise."
SP:414b06d86e132357a54eb844036b78a232571301,This paper proposes a state alignment based imitation learning method to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both local and global perspectives and is combined into a reinforcement learning framework by a regularized policy update objective. The authors show the superiority of the proposed method on standard imitation learning settings and imitation learning with different dynamics models.
SP:91761d68086330ce378507c152e72218ed7b2196,This paper proposes an extension of SGD called Deep Gradient Boosting (DGB) where the back-propagated gradients inferred using the chain rule can be viewed as pseudo-residual targets of a gradient boosting problem. At each layer of a neural network the weight update is calculated by solving the corresponding boosting problem using a linear base learner. When implemented as a separate input normalization layer (INN) the new architecture shows improved performance on image recognition tasks when compared to the same architecture without normalization layers.
SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"This paper proposes a novel approach to differentiable architecture search (DARTS) by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. Specifically, the proposed approach performs operation search in a subset of channels while bypassing the held out part in a shortcut. The proposed approach can be trained with a larger batch size and enjoys both faster speed and higher training stability. The experimental results demonstrate the effectiveness of the proposed method."
SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a hybrid method that combines the best aspects of gradient-based methods and deep reinforcement learning (DRL). It builds upon the deep deterministic policy gradients (DDPG) algorithm and proposes a simple modification that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. Empirical results show that the proposed method boosts the performance of DDPG without sacrificing its robustness to local minima.
SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"This paper proposes a method to learn a task-agnostic world graph by training a binary recurrent variational autoencoder (VAE) on trajectory data and a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. The proposed method is evaluated on a suite of challenging maze tasks and shows that using world graphs significantly accelerates RL, achieving higher reward and faster learning."
SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for reverse engineering neural networks. The proposed method is based on discretizing the selection layer of a neural network, which is called the white box network. The selection layer is a discretization of the input layer of the neural network and the output layer is the output of the discretized selection layer. The authors claim that this makes the model interpretable and can be used for end-to-end reverse engineering. "
SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for self-supervised imitation learning of goal-conditioned policies by maximizing the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the state that it actually reached. The method is motivated by the observation that in the multi-task setting, trajectories that are generated by a suboptimal policy can serve as optimal examples for other tasks. The authors propose a simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. Empirical results show that it performs competitively with more complex RL methods on a range of challenging goal reaching problems."
SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper proposes a new robust asynchronous SGD method, called Zeno++, which can tolerate Byzantine failures of the workers. The key idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. The authors prove the convergence of the proposed method for non-convex problems under Byzantine failures. Experimental results show that the proposed approach outperforms existing approaches."
SP:d16ed9bd4193d99774840783347137e938955b87,This paper proposes a method to generate adversarial examples that manipulate semantically meaningful image-based visual descriptors (color and texture) in order to generate effective and photorealistic adversarial images. The proposed method can be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. The authors conduct comprehensive user studies to show that the generated semantic adversarial samples are more convincing to humans than other attacks.
SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes an RL-based approach for few-shot learning of entity recognition. The authors propose a novel architecture that uses a trainable controller to manipulate latent representations in an external memory. They use a reward signal that is proportional to the average reduction of entropy when attending the memory entries. This enables them to propose a policy that learns a policy based on interactions with the external memory using reinforcement learning. They show the generality of their solution for the few-shots learning of entities in the Stanford Task-Oriented Dialogue dataset.
SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"This paper proposes an approach to learn motor primitives across large-scale and diverse manipulation demonstrations. Motivated by human demonstrations, the authors propose to learn a set of motor programs (called ‘primitives’ in the paper) that can be used to compose a diverse set of demonstrations to perform a variety of tasks. The proposed approach is based on a hierarchical reinforcement learning setup to efficiently solve robotic manipulation tasks like reaching and pushing. The main contributions of the paper are: (1) The authors propose a method to learn the set of movement primitives from demonstrations without additional parsing or segmentation. (2) The method learns a coherent latent representation for these primitives. (3) They demonstrate that their learned primitives capture semantically meaningful aspects of a demonstration. "
SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a method for single episode transfer in reinforcement learning (RL). The authors propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then used as input to a universal control policy. This modular approach enables integration of state-of-the-art algorithms for variational inference or RL. Moreover, the approach does not require access to rewards at test time, allowing it to perform in settings where existing adaptive approaches cannot. In diverse experimental domains with a single episode test constraint, the proposed method significantly outperforms existing adaptive RL approaches."
SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"This paper studies the use of a three-head network architecture in AlphaZero, a search-based reinforcement learning algorithm for two-player zero-sum zero-star games. The paper proposes to use a three head network architecture, which is a combination of a two head network and an action-value network, to learn a third head on a fixed dataset. The proposed method is evaluated on the game of Hex, where it is shown that the proposed method outperforms the two-head architecture in terms of performance in zero-style iterative learning and Monte Carlo tree search."
SP:89d6d55107b6180109affe7522265c751640ad96,"This paper proposes a method for transfer learning in reinforcement learning. The proposed method is based on the idea of combined adaptation of behaviors and learning through experience is a primary mechanism of learning in biological creatures (Krakauer & Mazzoni, 2011; Fryling et al., 2011). The main contribution is an algorithm to transfer policies between tasks with significant differences in state transitions via a policy adaptation mechanism. The method combines supervised reference trajectory tracking and unsupervised reinforcement learning to adapt the source policy to the target domain directly. The authors show through theory and experiments that the method enjoys significantly reduced sample complexity in solving the task."
SP:626021101836a635ad2d896bd66951aff31aa846,This paper proposes a scale-equivariant convolutional networks with steerable filters. The scale-convolution and generalize other common blocks are developed to be scale equivariant. The proposed method is based on the combination of tensor expansion and 2-dimensional expansion and demonstrates the same computation time as the general CNN with a comparable filter bank. Experiments on the MNIST-scale dataset and the STL-10 dataset demonstrate the effectiveness of the proposed method.
SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper proposes an unpaired point-based 3D scan completion method that can be applied directly to real scans without the need of paired training data. The proposed method is based on a generative adversarial network (GAN) that transforms the input point clouds into a suitable latent representation such that a discriminator cannot differentiate between the transformed latent variables and the latent variables obtained from training data (i.e., complete shape models). The generator is responsible for mapping raw partial point sets into clean and complete point sets, and the process is regu-worldly by working in two different latent spaces that have separately learned manifolds of scanned and synthetic object data."
SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generating fake data for authentication and anomaly detection using generative adversarial networks (GANs). The authors cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. The analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. Based on these insights, the authors design practical learning approaches and show that they result in models that are more robust to various attacks on real-world data."
SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper studies the trade-off between robustness and accuracy in adversarial learning. The authors propose a novel framework called “sensible adversary”, which is a combination of sensible adversarial training and adversarial robustness. The proposed framework is motivated by the fact that the high dimensional distribution is poorly represented by limited data samples. Theoretically, the authors show that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under sensible adversary. The paper also proposes a novel and efficient algorithm that trains a robust model with sensible adversary and achieves state-of-the-art results against various attacks."
SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish feature map distributions of different networks. Discriminators and networks are trained concurrently in a minimax twoplayer game. Also, a novel cyclic learning scheme is proposed for training more than two networks together. The method is applied to various network architectures on the classification task and discovered a significant improvement of performance."
SP:e43fc8747f823be6497224696adb92d45150b02d,"This paper proposes a sentiment word embedding model for semantic and sentiment analysis tasks. The proposed method is based on GloVe and C&W model, which encodes sentiment information in the continuous word representations. The parameters of the proposed model are determined by using both the maximum likelihood estimation and the Bayesian estimation. Experimental results show that the proposed method significantly outperforms the baseline methods in sentiment analysis for low-frequency words and sentences. "
SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method, called Prestopping, for noise-free training under any type of label noise. In the first phase, the authors propose to early stop training a deep neural network before the noisy labels are memorized. Then, they resume training the early stopped network using a “maximal safe set,” which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Extensive experiments using four image benchmark data sets verify that the method significantly outperforms four state-of-the-art methods in test error by 0.4–8.2 percent points under existence of real-world noise."
SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a framework for action generalization in reinforcement learning. The main idea is to use an autoencoder-based representation learning approach to learn representations of actions, which are then used to train a policy that can generalize to unseen actions. The paper also proposes a regularization term that encourages the policy to overfit to the actions seen during training, which is similar to the risk minimization term in Reinforcement Learning (RL). The proposed method is evaluated on a set of Mujoco tasks, where it is shown that it is able to generalize better than the baseline RL method."
SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes two methods for storing word embedding matrix during training and inference in a highly efficient way. The authors use approaches inspired by quantum computing to propose two related methods, word2ket and word_2ketXS, for storing and accessing embedding vectors for all words in a dictionary. Their approach achieves a 100-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks."
SP:3df499068ffe6c995457c2174f987cb0ae3c2551,This paper proposes a method for imitation learning (IL) that learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The proposed method is applied to the build-order planning task in StarCraft II. The authors show that the learned policy can be effectively manipulated to express distinct behaviors. The results demonstrate that the policy can adapt its behavior in-between games to reach a performance beyond that of the traditional IL baseline approach.
SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis, which suggests that sparsified neural networks can be trained as long as the network is initialized properly. The authors claim that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. Based on these insights, the authors identify the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning."
SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes an unknown aware deep neural network (UDN) to solve the challenging problem of detecting unknowns in image classification. The key idea of UDN is to enhance existing CNNs to support a product operation that models the product relationship among the features produced by convolutional layers. UDN uses a learned ensemble of these product operations, which allows it to balance the contradictory requirements of accurately classifying known objects and correctly rejecting unknowns. To further improve the performance, UDN also proposes an information-theoretic regularization strategy that incorporates the objective to reject unknowns into the learning process. The experimental results demonstrate that UDN consistently outperforms state-of-the-art methods at rejecting knowns – up to 20 point gains in accuracy, while still preserving the classification accuracy."
SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a method for training variational distributions by starting with a coarse approximation and iteratively refining it. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. The authors demonstrate theoretically that our method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks. The method consistently outperforms recent variational inference methods in terms of log-likelihood and the ELBO."
SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,"This paper proposes a policy gradient estimator (ACMC) to stabilize the contextual generation of categorical sequences, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. The number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. The proposed methods yield lower gradient variance and consistent improvement over related baselines. The paper also proposes a binary-tree softmax model to reduce the high generation cost in large vocabulary scenarios."
SP:ab51af66e626b1b03bbf0de7a5237370e941925c,This paper proposes a stochastic goal recognition control (S-GRC) problem with deceptive opponent modeling and proactive network interdiction. The opponent’s deceptive policy is modeled as one soft multi-criteria decision policy with one tunable parameter to balance goal achievement and deception preference; and the primary objective is offline redesigning the environment by soft action interdition to control online GR. The proposed method is evaluated in two different environment representations: random generated connected graph and real road network.
SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batches for training GANs that are effectively large though actually small, inspired by the use of Coreset-selection in active learning. The method computes the Inception Datasets of Inception activations of each training image and projects them down to a smaller dimension, and then uses Coreset selection on those projected activations at training time. Experiments show that the proposed method substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it allows the method to reach a new state of the art in anomaly detection."
SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper investigates the ability of RNNs to compress the history of sensory inputs while being maximally informative about the future. In particular, the authors investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. They show that RNN is sub-optimal in the information plane, i.e. instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future, and show how constraining past information by injecting noise into the hidden state can improve the ability for both maximum likelihood and contrastive loss training."
SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes a method to mitigate the delusional bias in Q-learning by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. The authors introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class, and propose a search framework that allows multiple Q approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experiments on the Atari suite demonstrate that the proposed method can improve the performance of Q learning in a variety of Atari games, sometimes dramatically."
SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model, called SPACE, that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. SPACE also resolves the scalability problems of previous models by incorporating parallel spatial attention and thus is applicable to scenes with a large number of objects without performance degradations. The experiments on Atari and 3D-Rooms show that SPACE achieves the above properties consistently."
SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a method to compress convolutional neural networks while maintaining their accuracy on classification tasks. The proposed method is based on depthwise separable convolution, which replaces a standard convolution with a depthwise convolution and a pointwise convolutions. The method is derived by interpreting existing convolution methods using EHP (Extended Hadamard Product), which is a mathematical formulation to approximate the standard convolutions kernel. Based on the interpretation, a generalized version rank-k FALCON is proposed, which further improves the accuracy while sacrificing a bit of compression and computation reduction rates. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms existing methods."
SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper identifies four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. The contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy, recognizing and validating the powerful regularization effect of Ghost BatchNormalization for small and medium batch sizes, examining the effect of weight decay regularization on the scaling and shifting parameters, and identifying a new normalization algorithm for very small batch sizes by combining the strengths of B batch and Group Normalization. Experiments on CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet demonstrate the effectiveness of the proposed improvements."
SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"This paper studies the problem of data inspection in the federated learning setting, where the data is stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. The authors propose to use generative models trained using federated methods and with formal differential privacy guarantees to solve the data inspection problem. The proposed method is evaluated on text and image datasets."
SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the provided motion material and modifies it to become natural. The generated sequences are useful as subgoals for actual physical execution."
SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper proposes a hierarchical explanation method for NLP models based on contextual decomposition (CD) and its hierarchical extension (Singh et al., 2019). The authors propose a formal way to quantify the importance of each word and phrase, and develop effective algorithms for generating hierarchical explanations based on the new formulation. The proposed methods are evaluated on both LSTM models and BERT Transformer models on multiple datasets, and help to visualize semantic composition captured by models."
SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,This paper proposes a saliency map method for deep convolutional neural networks (CNN) that is much more efficient than popular gradient methods. The technique works by measuring information at the end of each network scale which is then combined into a single saliency maps. The method is at least 97x faster than Guided Backprop and much more accurate. The authors also visualize individual scale/layer contributions by using a layer ordered visualization of information.
SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a non-autoregressive transformer by position learning (PNAT) for text generation. The proposed method explicitly models the position of the output words as a latent variable in the text generation process. The authors propose a heuristic search process to guide the position learning, and max sampling is adopted to inference the latent model. Experimental results show that PNAT achieves top results on machine translation and paraphrase generation tasks, outperforming several strong baselines."
SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a new generative adversarial network (GAN) model, called Random Path Generative Adversarial Network (RPGAN). The main idea of the proposed model is to use random paths in the forward pass of the generator network, instead of the standard Gaussian distribution used in traditional GANs. The authors show that the proposed RPGAN model is able to provide natural interpretability and efficient model updates with new data. In addition, the authors also show that RPGAN allows the construction of generative models without nonlinearities, which can speed up the generation process for fully-connected layers."
SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes a spherical convolutional neural network based on a graph representation of the sampled sphere, which is equivariant to rotations with respect to the number of vertices and neighbors. The proposed method, called DeepSphere, is evaluated on a variety of spherical image classification and segmentation tasks. It is shown that the proposed method achieves state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation."
SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the problem of learning invariant representations for domain adaptation. In particular, the authors show that the search for invariance favors the compression of representations, which may have a bad impact on adaptability of representations. They show that weighting representations can align representation distributions without impacting their adaptability. This supports the claim that representation invariance is too strict a constraint. "
SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for inferring the pixel-level attributes of user interface components (e.g., color, border radius, shadow, shadow radius) from an input image. The authors propose a two-step process: first, they train a neural model to predict the most likely initial attribute values, and then (ii) they use imitation learning to iteratively refine the attribute values. The proposed method is evaluated on a synthetic dataset and a real-world dataset consisting of buttons found in Google Play Store applications. "
SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies robust transfer learning, in which the goal is to transfer not only performance but also robustness from a source model to a target domain. The authors first show that robust networks contain robust feature extractors, which are resistant to adversarial perturbations in different domains. Then, they consider the case of “fine-tuning” a network by re-training end-to-end in the target domain, which preserves the robustness of the source network while achieving high accuracy. They construct robust models that generalize well."
SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"This paper studies the emergence of highly compositional languages in referential games. The authors propose a neural iterated learning (NIL) algorithm that, when applied to interacting neural agents, facilitates the emergence a more structured type of language. They show that these languages provide learning speed advantages to neural agents during training, which can be incrementally amplified via NIL. They also provide a probabilistic model of NIL and an explanation of why the advantage of compositional language exist. The experiments confirm their analysis and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication."
SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The proposed method is proven convergent under certain conditions."
SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a simple reward function for goal-conditioned reinforcement learning. The reward function is a simple indicator reward function, which is used when the robot’s observation exactly matches a target goal observation. The authors propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. They show that their method can perform complex tasks in continuous state spaces such as rope manipulation."
SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper proposes a robustness verification algorithm for Transformers with complex self-attention layers. The proposed algorithm is based on the linear-relaxation framework (Weng et al., 2018; Zhang et al. 2018) and recursively propagate linear lower and upper bounds for each neuron w.r.t the input within the perturbation space S. The certified robustness bounds computed by the proposed algorithm are significantly tighter than those by naive Interval Bound Propagation (IBP). The bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis."
SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a weakly supervised pretraining objective that explicitly forces the model to incorporate knowledge about real-world entities. Specifically, it replaces the entity mentions in the original documents with names of other entities of the same type and train the models to distinguish the correct entity mention from randomly chosen ones. The proposed method is applied to four entity-related question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset (e.g., FIGER) with 5.7 accuracy gains."
SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper tackles the problem of discovering novel classes in an image collection given labelled examples of other classes. The challenge is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. The authors address this problem by combining three ideas: (1) they suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelling data; (2) they use rank statistics to transfer the model’s knowledge of the labelled classes to the problem for clustering the unlabeled images; and (3) they train the data representation by optimizing a joint objective function on the labelled/unlabelled subsets of the data, improving both the supervised classification of labelled data, and the clustering of the unllabeled data."
SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a method for visual planning, which is based on the semi-parametric topological memory (SPTM) method. The main limitations of SPTM are that it requires a suitable loss function for the connectivity classifier, which requires non-trivial manual tuning, and that it is constricted in its ability to generalize to changes in the domain, as its graph is constructed from direct observations and thus requires collecting new samples for planning. The authors propose a method called Hallucinative Topological Memory (HTM), which overcomes these shortcomings. In particular, instead of training a discriminative classifier we train an energy function using contrastive predictive coding. In addition, we learn a conditional VAE model that generates samples given a context image, and use these hallucinated samples for building the connectivity graph, allowing for zero-shot generalization to domain changes. In simulated domains, the proposed method outperforms conventional SPTM and visual foresight methods in terms of both plan quality and success in long-horizon planning."
SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes a method to reduce the spurious biases in large-scale benchmark datasets. The authors propose an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. They apply it to popular benchmarks that are practically solved and present filtered counterparts as new challenge datasets where the model performance drops considerably (e.g., from 84% to 24% for ImageNet and from 92% to 62% for SNLI)."
SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a method for few-shot classification based on prototypical networks. In particular, the authors propose to use batch normalization to enforce centering in the prototypical network. The proposed method is evaluated on the Omniglot and MiniImageNet datasets. The results show that the proposed method outperforms baselines."
SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. The proposed GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. Then, it repeatedly coarsens into much smaller graphs by merging nodes with high spectral similarities. Finally, it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. The authors have evaluated the approach on a number of popular graph datasets for both transductive and inductive tasks."
SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a method to predict pixels relatively, by predicting new pixels relative to previously generated pixels (or pixels from the conditioning context, when available). The authors show that this form of prediction fare favorably to its absolute counterpart when used independently, but their coordination under an unified probabilistic model yields optimal performance. Experiments on multiple benchmarks for unconditional image generation, image colorization, and super-resolution indicate that the presented mechanism leads to improvements in terms of likelihood compared to the absolute prediction counterparts."
SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper studies the problem of off-line estimation of the stationary distribution of a Markov chain. The authors propose a new method, called GenDICE, that is based on estimating a ratio that corrects for the discrepancy between the empirical and stationary distributions. The ratio is derived from the fundamental properties of the Markov distribution, and the authors propose to use a variational divergence minimization approach to solve the problem. Theoretical analysis is provided to show the consistency of the proposed method. Empirical results are also provided to demonstrate the effectiveness of the method. "
SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper proposes a method for training NLP models to be less sensitive to spurious patterns in natural language processing (NLP). In particular, the authors propose a human-in-the-loop system for counterfactually manipulating documents. Given documents and their initial labels, they task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactally-revised counterparts and vice versa. The authors also show that models trained on the combined datasets perform remarkably well."
SP:b720eb5b6e44473a9392cc572af89270019d4c42,"This paper studies the perceptual quality of deep convolutional neural networks (CNNs) by measuring the frequency and orientation tuning of channels in trained image classification deep CNNs (e.g., VGG-16) by applying grating stimuli of different spatial frequencies and orientations as input. They observe that the behavior of CNN channels as spatial frequency/orientation selective filters can be used to link basic human visual perception models to their characteristics. They conclude that sensitivity to spatial frequencies that have lower contrast masking thresholds and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features. They verify their hypothesis by designing an OQA experiment (Sheikh et al. (2006))."
SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to explicitly model link type correlations. The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrated that GENN is superior to many baselines without consideration of link type correlation and achieves 13.77% and 5.01% PR-AUC improvement on the two datasets, respectively."
SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a method to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The proposed method uses either a Gumbel-Softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.
SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning approach for learning-to-rank recommender systems. The main idea is to train a critic network to approximate ranking-based metrics, and then update the actor network to directly optimize against the learned metrics. The proposed method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. Experiments on three large-scale datasets demonstrate the effectiveness of the proposed method."
SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a multi-step TD-learning approach for off-policy reinforcement learning. The proposed approach is based on truncated Q-functions, which represent the return for the first n steps of a target-policy rollout w.r.t. the full action-value, and farsighted return, which represents the return after this truncated rollout. The authors prove that the combination of these short and long-term predictions is a representation of the full return, leading to the composite Q-learning algorithm. The efficacy of the approach is evaluated on three simulated robot control tasks."
SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a method for continuous reinforcement learning (RL) for dexterous manipulation. The main idea is to learn from raw sensory inputs, learn reward functions from easily available supervision, and learn in non-episodic settings without requiring human intervention to manually reset the environment. The proposed method is evaluated on a set of dexterous robotic manipulation tasks, and it is shown that it can learn without any human intervention."
SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies adversarially robust generalization of deep neural networks. The main idea is to decompose the expected robust risk into two parts: the stability term, which measures the prediction stability in the presence of adversarial perturbations, and the accuracy part, which evaluates the standard classification accuracy. As the stability part does not depend on any label information, it is possible to optimize this part using unlabeled data. The authors further show that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018), adversarial robust generalisation can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabelled data is provided."
SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper studies the effect of regulatory fit in the context of reinforcement learning (RL). The authors propose a family of advantage estimates based on the order statistics over the path ensemble, which allows one to flexibly drive the learning process in a promotion focus or prevention focus. They systematically study the impacts of different regulatory focuses. The findings reveal that regulatory focus, when chosen appropriately, can result in significant benefits."
SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks, which computes most features in a low precision and only a small proportion of important features in high precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of neural network execution with almost no accuracy loss. The experiments indicate that PG achieves excellent results on CNNs, including static compressed mobile-friendly networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4x less compute on ImageNet."
SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a new unsupervised domain adaptation (UDA) setting, where the source and target domain have noisy labeled data, and the target domain has unlabeled data. The authors show that the existing UDA methods do not handle this setting well, and propose a new method, called ""Butterfly"", to solve the problem. The main idea is to train a model on the noisy source data, then leverage this model to assign pseudo labels for the source data for the target data, so that the knowledge transferred from pseudo-labeled source data can be transferred to target data. In the experiments, the proposed method is evaluated on CIFAR-10 and ImageNet datasets."
SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,This paper proposes a model-free off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art model-based algorithms on many challenging control tasks. The authors claim that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. The proposed method is a simple autoencoder-based method that can be trained end- to-end.
SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for accelerating training and improving generalization over the original dropout. The proposed dropout creates multiple dropout samples. The loss is calculated for each sample, and then the sample losses are averaged to obtain the final loss. This technique can be implemented without implementing a new operator by duplicating a part of the network after the dropout layer while sharing the weights among the duplicated fully connected layers. Experimental results showed that the proposed method significantly accelerates training by reducing the number of iterations until convergence for image classification tasks using the ILSVRC 2012 (ImageNet), CIFAR-10, CifAR-100, and SVHN datasets."
SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"This paper proposes a label sensitive gate (LSG) structure to enable the model to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. The training strategy imposes each filter’s attention to just one or few classes, namely class-specific. Extensive experiments demonstrate the performance of the method in generating sparse and highly class-related representation of the input. Moreover, comparing to the standard training strategy, the model displays less redundancy and stronger interpretability."
SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"This paper proposes a method for learning nearly decomposable Q-functions (NDQ) in cooperative multi-agent reinforcement learning (MARL) by minimizing the entropy of messages between agents. The authors propose two information-theoretic regularizers, one for maximizing mutual information between agents’ action selection and communication messages, and the other for maximizing the expressive and succinctness of messages. The proposed method is evaluated on the StarCraft II unit micromanagement benchmark. The results show that the proposed method significantly outperforms baseline methods and allows to cut off more than 80% of communication without sacrificing performance."
SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a method for generating puzzles for training neural networks to solve programming puzzles. A programming puzzle is a short program for a Boolean function f(x) with the goal of finding an input that makes f return True. The authors propose a GAN-like algorithm called “Troublemaker” which can generate puzzles adaptively targeted at any given puzzle-solver. It generates a diverse set of puzzles that are difficult for the solver to solve. In the experiments, the authors show that the proposed method is able to generate challenging problems for a variety of state-of-the-art puzzle solving techniques."
SP:627b515cc893ff33914dff255f5d6e136441d2e2,"This paper proposes a method for hierarchical reinforcement learning (HRL) that decomposes a policy into a set of primitives and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. The authors propose to use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision, and the primitive that requests the most information about the currently state acts in the environment. They show that regularizing the primitives to use as little information as possible leads to natural competition and specialization. They also demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization."
SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,This paper proposes a model-based planning framework that learns a latent dynamics model directly from rewards. The latent representation is learned exclusively from multi-step reward prediction which is shown to be the only necessary information for successful planning. The authors demonstrate the framework in multi-pendulum and multi-cheetah environments where several pendulums or cheetahs are shown to the agent but only one of them produces rewards. They find that the proposed method can successfully learn an accurate latent reward prediction model in the presence of the irrelevant information while existing model-free and model based methods fail.
SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes two parameter reduction techniques to reduce the memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that the proposed methods lead to models that scale much better compared to the original BERT. The best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD while having fewer parameters compared to BERT-large."
SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"This paper proposes an extension of the Transformer architecture for multi-modal tasks. Specifically, the authors propose a spatio-temporal cache mechanism that enables learning spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture further enables a single model to support tasks with multiple input modalities as well as asynchronous multi-task learning. The authors demonstrate that a single instance of OmniNet can concurrently learn to perform the tasks of part-of-speech tagging, image captioning, visual question answering and video activity recognition."
SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,This paper proposes a method for quantifying the predictive uncertainty of deep learning models. The main idea is to use higher-order influence functions (HOIFs) of the trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence intervals. The proposed method can be applied in a post-hoc fashion without compromising model accuracy or interfering with model training. Experiments demonstrate that the proposed method performs competitively compared to existing Bayesian and non-Bayesian baselines.
SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a GAN-based video synthesis model that is capable of producing high-fidelity video samples of higher complexity and fidelity than previous work. The proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. The authors evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Fréchet Inception Distance for prediction for Kinetics600 and UCF-101 datasets, alongside establishing a strong baseline for synthesis."
SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper studies the problem of few-shot learning, where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. The authors propose to use a spatial attention map that allows focusing on objects and suppressing background clutter. They also show that a pretrained network can be easily adapted to novel classes."
SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes Constrained Adversarial Networks (CANs), a generalization of GANs in which the generator is encouraged (at training time) to output valid structures with high probability via forward inference. This is achieved by augmenting the standard GAN loss with a penalty term that discourages the model from producing infeasible structures. The penalty term is implemented using the semantic loss (SL) (Xu et al., 2018), which leverages knowledge compilation (Darwiche, 2011) to represent arbitrary Boolean constraints as a circuit and uses the latter to measure the mass allocated by the generator to infeasibly objects. The paper also shows how to extend the SL complex constraints that would normally lead to intractably large circuits. This enables to deal with reachability on a graph, which is beyond the reach of standard SL."
SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which gives equal expressive power to both the positive and negative halves on the input space and is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power. Finally, they show that the use of these activations can significantly increase the robustness of deep neural networks to adversarial attacks."
SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"This paper proposes a method to improve the confidence of a black-box classifier by using a Dirichlet layer as the fusion layer of a deep learning model. The main idea of the method is to use the Dirichlets of the black box classifier to estimate the uncertainty of the output of the classifier, which is then used to select the predictions that are more confident and discard the ones that are less confident. The method is evaluated on two datasets, one for natural language processing (NLP) and one for computer vision."
SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes to use blockwise adaptive stepsize to improve the generalization of Adagrad. The main idea is that blockwise adaptivity is less aggressive than adaptivity to individual coordinates, and can have a better balance between adaptivity and generalization. Theoretical analysis shows that the proposed method has comparable regret in online convex learning and convergence rate for optimizing nonconvex loss functions, but is better up to some constant. Experimental results show the proposed blockwise adaptation improves generalization performance over Nesterov’s accelerated gradient and Adam."
SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new dataset called CATER, which is a video dataset with controllable object and scene bias. The dataset is constructed using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures by being completely observable and controLLable. The authors also provide insights into some of the most recent state of the art deep video architectures."
SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes a method to improve the model compatibility of GANs by adding a boundary calibration loss to the generator of the generator to match the statistics between the posterior distributions of original data and generated data with respect to the boundaries of the pre-trained classifiers. The proposed method, Boundary-Calibration GAN (BCGAN), uses a set of pretrained classifiers to obtain multiple decision boundaries, which are then used to guide the classifier to learn the correct decision boundary. The BC-loss is provably unbiased and can be easily coupled with different GAN variants to improve their model compatibility. Experimental results demonstrate that BCGANs not only generate realistic images like original GAN but also achieves superior model compatibility than the original GANS."
SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, this paper learns an optimizer, which is parametrized as a convolutional neural network, at the same time as a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and CifAR-100 datasets demonstrate the effectiveness of the proposed method. Moreover, the L2L framework can be extended to the generative adversarial imitation learning and stabilize the training."
SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper proposes a method for inferring Markov Decision Process (MDP) constraints for IRL. The proposed approach is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent’s demonstrations given our knowledge of an MDP. The authors propose an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."
SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper introduces a deep recurrent neural network architecture that approximates known visual cortical circuits (Mély et al., 2018). They show that this architecture, which they refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces the accuracy by driving the network to prefer low-level edges over high-level object boundary contours."
SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"This paper proposes a context-aware object detection method that leverages conditional random field (CRF) for object detection. In particular, the proposed method is a combination of CNN-based module and CRF-based graphical models. The proposed method improves the AP of object detection by 2 percentage points on the COCO dataset."
SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"This paper studies the adversarial robustness of BatchNorm. The authors hypothesize that the use of different normalization statistics during training and inference is the main cause of this adversarial vulnerability in the batchNorm layer. They empirically proved this by experiments on various neural network architectures and datasets. Furthermore, they introduce RobustNorm (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherits the benefits of the original batchNorm."
SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper proposes two regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, the authors prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. The generalization analysis relies on the connection between wide neural network and neural tangent kernel (NTK). The generalisation bound is independent of the network size, and is comparable to the bound one can get when there is no label noise."
SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes to improve the performance of convolutional neural tangent kernels (CNTK) by incorporating data augmentation. Specifically, the authors propose to modify the kernel using a new operation called Local Average Pooling (LAP) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentations using pixel shifts. The authors also propose to represent the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolution layer composed of random image patches. The proposed method is evaluated on CIFAR-10 and Fashion MNIST datasets."
SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS) for reinforcement learning (RL). In particular, the authors propose a method called “Search with Amortized Value Estimates” (SAVE) which uses a combination of real experience as well as the results of past searches to improve overall performance and reduce planning cost. During training, SAVE uses MCTS to estimate the Q-values at encountered states, which are used along with real experience to fit a Q-function, thus amortizing the computation required to estimate values during search. The Q-functions are then used as a prior for subsequent searches, resulting in a symbiotic relationship between the learned prior and the model. At test time, the proposed method is able to achieve higher rewards with fewer training steps, and yields strong performance with very small search budgets."
SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a new method for the task of monocular-to-stereo view synthesis, where the goal is to generate stereo images at arbitrary camera positions given a single input image. The proposed method is based on a novel t-shaped adaptive kernel with globally and locally adaptive dilations, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB STEREO indoors dataset to prove the efficacy of the proposed method."
SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,This paper studies the variational auto-encoders (VAEs) from an information theoretic perspective. The authors propose a variational lower bound of the capacity-constrained InfoMax (CCIM) and show that the optimal generative model is the one that maximizes the capacity of the generative models. The paper also shows that the WAE and InfoVAE are actually maximising a lower bound on the mutual information between the generator map and the prior p(z|x) and the capacity p(x|z) of the network. The proposed method is called Variational InfoMax AutoEncoder (VIMAE). Experiments on MNIST and CIFAR-10 show that VIMAE is able to learn representations that are good enough to generalize to new tasks.
SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper proposes a Skip-gram style network embedding method that considers attribute distributions over local neighborhoods, both pooled (AE) and multi-scale (MUSAE), and their counterparts that attribute distinct features to each node (AE-EGO and MUSAE-USA). The authors prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that their algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets."
SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the phase transitions in the Information Bottleneck (IB) objective, which is defined on the encoding distribution p(z|x) for input X, target Y, and representation Z, where sudden jumps of dI(Y;Z) dbeta and prediction accuracy are observed with increasing beta. The authors introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phases transitions, and draw its connection with the Fisher information matrix for parameterized models. They provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, they present an algorithm for discovering phase transition points, and verify that their theory and algorithm accurately predict phase transitions."
SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a method to estimate the advantage function of a policy and a state-action pair in RL. The advantage function is estimated by subtracting an estimate of the value function V^pi(s,a) from the estimate of Q-value Q^{-q}(s,.a). The authors propose to use the independence property between current action and future states in the environment to reduce the variance of the advantage estimation. They also propose to combine it with the Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that the method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"This paper proposes a new estimator for infinite horizon off-policy policy evaluation. The proposed estimator is an extension of the work of Liu et al. (2018a) that estimates the density ratio between the state stationary distributions of the target and behavior policies. The main contribution of this paper is a bias-reduced augmentation of their method, which can take advantage of a learned value function to improve accuracy. The method is doubly robust in that the bias vanishes when either the density ratios or value function estimation is perfect. "
SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a two-stage framework for few-shot learning. First, the authors propose a novel Metric-Softmax loss, which is trained against the whole label set and learns more discriminative feature than episodic training. Second, they design a task-adaptive transformation which adapts the classifier to each few shot setting very fast within a few tuning epochs. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms the state-of-the-arts."
SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper presents a data-driven approach that learns to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data, then employs a neural network that infers a correction to move a coarse result closer to the reference data. The paper provides insights into the targeted learning problem with two learning approaches: fully supervised learning methods with a naive and an optimized data acquisition and an unsupervised learning method with a differentiable Navier-Stokes solver."
SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for learning to compress and store a representative dataset from a non-i.i.d data stream, while only observing each sample once. This problem is highly relevant for downstream online continual learning tasks, as well as standard learning methods under resource constrained data collection. To address this, the authors propose a new architecture which Stacks Quantization Modules (SQM), consisting of a series of discrete autoencoders, each equipped with their own memory. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, our approach does not require pretraining, even on challenging datasets."
SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes a bidirectional representation learning method for multi-label metric learning. The proposed method is based on deep convolutional networks that are able to operate on feature data or directly on raw image data. The model scales linearly in the number of instances and trains deep neural networks that encode both input data and output labels, then obtains a metric space for testing data. Experiments on multi-labels classification tasks demonstrate that the proposed method can achieve good performance."
SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the generalization performance of asynchronous stochastic gradient descent (A-SGD) in deep neural network training. In particular, the authors focus on the effect of the delay in the update of the parameters of the parameter server (PS) on the performance of the algorithm. They show that the degree of delay affects the learning rate and the set of minima that can be reached by the algorithm, and show that for high delay values, the learning rates should be inversely proportional to the delay. They also extend this analysis to include momentum. Finally, they show that momentum should be either turned off or modified to improve the training stability."
SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes an approach for representation learning in model-free reinforcement learning (RL) based on hindsight learning. The authors propose to learn a special value function in hindsight that receives future observations as an additional input. This learning process reveals features of future observations that are most useful for value prediction. These important features are then predicted, in advance, using only information available at test time (at the time of releasing the ball and given to the batter, the type of the batter and the spin direction). The authors show how this can help dramatically even in simple policy evaluation settings, including on 57 Atari 2600 games."
SP:6388fb91f2eaac02d9406672760a237f78735452,"This paper proposes a novel adversarial attack method for graph neural networks (GNNs) by rewiring three nodes (vfir, vsec, vthi) by adding/deleting an edge between vfir and vsec and adding one between v_i and v_thi. The attack strategy is learned using reinforcement learning. Experiments on real-world graphs demonstrate the effectiveness of the proposed framework. "
SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper studies the problem of estimating the prediction error of encoder-decoder convolutional neural networks (E-D CNNs) for denoising inverse problems. The authors propose to use Stein’s unbiased risk estimator (SURE) as an unbiased estimator of the prediction errors. The main contribution of this paper is the derivation of a close form expression for the divergence term in the SURE estimator that can be used for neural network training. The proposed method is evaluated on various inverse problems such as accelerated MRI, energy-dispersive X-ray spectroscopic imaging, deconvolution, and etc, which clearly show that the proposed method can improve the image quality compared to existing approaches."
SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"This paper proposes a meta-learning approach for few-shot classification, where the number of instances per task and classes can vary across tasks. The authors propose a Bayesian meta learning framework that learns to balance the effect of meta- and task-specific learning within each task. Specifically, the authors first obtain set-representations for each task, which are learned to convey useful statistics about the task distribution, such as mean, variance, and cardinality (the number of elements in the set), and then learn the distribution of three balancing variables a function of the set: 1) task-dependent learning rate multiplier, which decides how far away to deviate from the meta-gradient when performing task specific learning, 2) class dependent learning rate, to decide how much information to use from each class, to automatically handle class imbalance of instances, and 3) task dependent initial model initialization, which modifies the shared initial model and decides what to use and what to ignore based on its set representation. The proposed approach is evaluated on CIFAR-FS and miniImageNet datasets, where it is shown to outperform the baselines."
SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for imitation learning based on reinforcement learning. The main idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. This is achieved by giving the agent a constant reward of r = +1 for matching the demonstrated action in a state, and r = 0 for all other behavior. Theoretically, the paper shows that the proposed method can be interpreted as a regularized variant of behavioral cloning (BC) that uses a sparsity prior to encourage long-horizon imitation. Empirically, it is shown that the method outperforms BC and achieves competitive results compared to GAIL."
SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for points clouds that change over time. The authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. They combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. The method works for large, deforming point sets from different sources."
SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes an end-to-end optimizer (OT-SI) that learns the optimal transport (OT) cost function using a small amount of side information which is often available. The side information captures subset correspondence, i.e. certain subsets of points in the two data sets are known to be related. The authors develop an algorithm that differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. Experiments on image, marriage-matching and single-cell RNA-seq datasets demonstrate the effectiveness of the proposed method."
SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"um detection is an unsupervised detection of anomalies in large datasets. The paper proposes to learn the representation underlying normal data by leveraging the latest clustering technique suitable for handling high dimensional data. The key idea is to train an autoencoder from the normal data subset, and iterate between hypothesizing normal candidate subset based on clustering and representation learning. The reconstruction error serves as a scoring function to assess the normality of the data."
SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper proposes a new algorithm, Projection-Based Constrained Policy Optimization (PCPO), which is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. The authors theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence. The empirical results over several control tasks demonstrate that PCPO achieves superior performance."
SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper studies the role of the parameter $\alpha$ in word embeddings. The authors show that the word embedding can be viewed as a low-rank transformation from the word-context co-occurrence space to the embedding space, which preserves the relative distances among words. They also provide a theoretical explanation for this behavior, and derive a method to automatically find the optimal value of $\alpha$. The experiments on real datasets verify their analysis."
SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"This paper proposes a method for similarity measurement. The proposed method consists of a group of approximate Random Projection Trees (RP Trees) that are trained simultaneously to achieve high accuracy, high efficiency in terms of speed, and independence from prior knowledge. The method is evaluated on three real-world datasets, where it is shown that the proposed method achieves an efficiency of up to 3.5 times higher than the existing methods."
SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method, ergodic inference (EI), which combines Markov chain Monte Carlo (MCMC) and variational inference (VI). The main idea of EI is to tune the hyper-parameters of a finite-step MCMC chain so that its last state sampling distribution converges fast to a target distribution. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyperparameters, which offers attractive balance between approximation bias and computational efficiency. The authors show that EI produces promising results on popular benchmarks."
SP:64f2744e938bd62cd47c1066dc404a42134953da,"This paper considers the problem of causal effect estimation from observational data with missing data, i.e., missing values. Missing data greatly complicate causal inference procedures as they require an adapted unconfoundedness hypothesis, which can be difficult to justify in practice. To circumvent this issue, the authors propose to consider latent confounders whose distribution is learned through variational autoencoders adapted to missing values, and use them as a pre-processing step prior to causal inference. They also suggest to embed them in a multiple imputation strategy to take into account the variability due to missing data. The experiments demonstrate the effectiveness of the proposed methodology especially for non-linear models."
SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search algorithm to construct compact reinforcement learning policies, by combining ENAS (Efficient Neural Architecture Search) and ES (Salimans et al., 2017) in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, the authors represent compact architectures via efficient learned edge- partitionings. For several RL tasks, they manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices."
SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper proposes a generalization of the Wavelet Transform (WT) and the Continuous Wavelet transform (CWFT) by introducing the Learnable Group Transform (LGT) by generalizing the affine transformations of a mother filter leading to the wavelet filter-bank. The authors propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific loss and signal. The experiments on diverse time-series datasets demonstrate the expressivity of this framework, which competes with state-of-the-art performances."
SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCN) to (products of) constant curvature spaces. In particular, the authors introduce a unified formalism that can interpolate smoothly between all geometries of constant curvatures, leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. The authors also propose a class of models that smoothly recover the Euclidian counterparts when the curvature goes to zero from either side. Empirically, the proposed models outperform Euclideans in the tasks of node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior, according to their discrete curvature."
