paper_id,summary
SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"This paper proposes sqSGD (selective quantized stochastic gradient descent), a gradient-based learning algorithm for federated learning (FL) under local differential privacy (DP) constraints, which provides strong protection against sensitive data disclosures via obfuscating the data before leaving the client. The authors identify two major concerns in designing practical privacy-preserving FL algorithms: communication efficiency and high-dimensional compatibility, and develop a gradient based learning algorithm that addresses both concerns. The proposed algorithm is based on a novel privacy preserving quantization scheme that uses a constant number of bits per dimension per client. Then it improves the base algorithm in two ways: first, it applies a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, it utilizes randomized rotation as a preprocessing step to reduce quantization error. The practicality of the proposed framework is demonstrated on benchmark datasets."
SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a universal representation method to incorporate the prior knowledge into the Transformer-based NMT in a universal way. In particular, the proposed method can represent the prior information as a continuous space matrix to semantically generalize instead of directly combining the probability distribution of the prior translation knowledge. Experimental results verified the effectiveness of our method on the two wildly used translation tasks."
SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,"This paper proposes a Bayesian Stackelberg Markov game-theoretic model to model the dynamics of a moving target defense (MTD) system, where the attacker and the defender are playing two-player leader-follower games between the defender and a cyberadversary. The authors argue that existing models are inadequate in sequential settings when there is incomplete information about a rational adversary and yield sub-optimal movement strategies. Further, while there exists an array of work on learning defense policies for sequential settings for cyber-security, they are either unpopular due to scalability issues arising out of incomplete information or tend to ignore the strategic nature of the adversary simplifying the scenario to use single-agent reinforcement learning techniques. To address these concerns, the authors propose (1) a unifying game theoretic model, called the BSMMGs, that can model uncertainty over attacker types and the nuances of an MTD system and (2) a BSM-Q-learning approach that can, via interaction, learn the optimal movement policy for BSMGs within a reasonable time. "
SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper proposes a VAE ensemble framework to improve the disentangled representation learning. The ensemble consists of multiple VAEs. The latent variables in every pair of these models are connected through linear transformation. The authors show that the linear transformations connecting the VAEs are similar up to signed permutation transfor-37 mations, thus disentangling the representations of the individual models in the ensemble. Experiments are conducted to show the effectiveness of the proposed method. "
SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The authors train a graph neural network in which each node represents a dataset to predict the best machine learning pipeline for a new test dataset. Performance is competitive with state-of-the-art AutoML systems while reducing running time from minutes to seconds and prediction time from milliseconds.
SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"In this paper, the authors study the relationship between compositionality and gradient descent in neural network optimization. They show that gradient descent leads to non-compositional solutions, and this is caused by gradient descent trying to use all available and redundant information from input, violating the conditional independence property of compositionality. Based on this finding, they suggest that compositionality learning approaches considering only model architecture design are unlikely to achieve complete compositionality, and propose to use model structure design (manual or searching) alone."
SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper provides a theoretical analysis of the embedding-based entity alignment methods. It shows that the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function for embedding learning. However, this margin cannot guarantee to be tight enough for alignment learning. To mitigate this problem, this paper proposes a new approach that explicitly learns KG-invariant and principled entity representations, while preserving the original infrastructure of existing methods. The proposed method, called neural ontology driven entity alignment (NeoEA), is optimized jointly with a neural neural network, which determines the head-level entity distributions of this neural network. The experiments demonstrate consistent and significant improvement in performance against the existing Embedding-based Entity Alignment (EEA) methods."
SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor algorithm co-design (SACoD) framework to develop more efficient CNN-powered PhlatCam. In particular, the mask is jointly optimized in terms of both model parameters and architectures via differential neural architecture search. Extensive experiments including both simulation and physical measurement on manufactured masks show that the proposed method achieves aggressive model compression and energy savings while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art designs with six datasets on four different tasks."
SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a method for learning semantic embeddings of individuals in a social network based on the life trajectories of individuals over multiple generations in two honey bee colonies. The authors propose a new temporal matrix factorization model that jointly learns the average developmental path and structured variations in individuals in the social network over their entire lives. The proposed method yields interpretable embedding that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they lived. The method provides a quantitative framework for understanding behavioral heterogeneity in complex social systems applicable in fields such as behavioral biology, social sciences, neuroscience, and information science."
SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation strategy for image reconstruction in the context of accelerated magnetic resonance imaging, where the goal is to reconstruct an image from a few under-sampled linear measurements. Inspired by the success of Data Augmentation (DA) for classification problems, the authors propose a pipeline for data augmentations and explore its effectiveness at reducing the required training data in a variety of settings. Experiments on the fastMRI dataset show that DA can achieve comparable performance to the state of the art while using significantly fewer training data."
SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a clustering algorithm for order learning. The proposed algorithm is based on the order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. Then, it clusters object instances into clusters according to their identity features using a repulsive term. Moreover, it estimates the rank of a test instance, by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm can cluster ordered data effectively and yield excellent rank estimation performance."
SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a method for reward-based exploration in procedurally-generated environments. The main idea is to treat each episode as a whole and give an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behavior. The authors demonstrate their method on several procedurally generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks."
SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes Isometric propagation network (IPN) for zero-shot learning (ZSL), which aims to learn a mapping between the semantic space of class attributes and the visual space of images based on the seen classes and their data. The key challenge is how to align the representations in the two spaces. The proposed method learns to propagate the class representations on an auto-generated graph within each space. IPN achieves state-of-the-art performance on three popular ZSL benchmarks."
SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a Transformer-based multi-task learning method for GLUE and SuperGLUE. The proposed method is based on a task-conditioned hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. Specifically, the proposed method learns the interactions and composition between a global (task-agnostic) state and a local task-specific state. The authors conduct an extensive set of experiments on GLUE/SuperGLUE to demonstrate the effectiveness of their proposed method."
SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper studies the effect of varying-quality images on the task of “learning to steer” and proposes a systematic approach to improve the performance of the learning algorithm based on quantitative analysis. The authors first analyze how different attributes of image quality influences the performance in autonomous driving task for autonomous vehicles, as quantified by mean accuracy (MA). Then, they propose an effective and efficient training method that improves the generalization of the model for learning-based steering under multiple perturbations."
SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes a method for fast approximate solutions to optimization problems with hard constraints. Specifically, the authors propose a method, DC3, for incorporating (potentially non-convex) equality and inequality constraints into deep learning-based optimization algorithms. The proposed method includes a neural network that outputs a partial set of variables, a differentiable completion procedure that fills in remaining variables according to equality constraints, and a correction procedure that fixes inequality violations. The authors demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow."
SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper proposes two new regularization methods for neural network pruning. The first one is based on the L2 regularization, where the penalty term is uniformly raised to a large level. The second one uses the Hessian information for more accurate pruning without knowing the weights' Hessian values. The proposed methods are evaluated on CIFAR-10 and ImageNet datasets."
SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper systematically studies the role of planning and its algorithmic design choices in MuZero, a model-based reinforcement learning algorithm. The authors perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. The results suggest that (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization."
SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes an extension of Value Iteration Networks (VINs) to the case where the underlying state space is assumed to be discrete and the Markov decision process (MDP) is assumed fixed and known. The authors combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. The proposed XLVIN is able to match the performance of VINs when the underlying MDP is discrete, fixed, and known, and provide significant improvements to model-free baselines across three general MDP setups."
SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the problem of learning read-once DNFs under the uniform distribution with a one-hidden hidden layer, nonhomogeneous convex neural network and gradient descent. The authors show empirically that the learned neurons are aligned with the terms of the DNF, despite the fact that there are many zero-error networks that do not have this property. Then, the authors provide theoretical insights on the learning process and the optimization to better understand the resulting inductive bias. Finally, they derive a DNF reconstruction method and show that it works in broader settings."
SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes a method for goal-oriented reinforcement learning that leverages graph structure in historical trajectories to slowly adjust exploration directions and rapidly update value function estimation with related experiences. The proposed method constructs a dynamic graph on top of state transitions in the replay buffer and develops an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. The authors also leverage graph structure to sample related trajectories for efficient value learning. The paper provides theoretical analysis to show the efficiency and converge property of the method."
SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method for improving sample-efficiency and generalization performance of RL agents trained and evaluated on procedurally generated environments. Specifically, the authors propose a general framework for estimating the future learning potential of a level given the current state of the agent’s policy. The proposed approach is based on the hypothesis that different levels provide different learning progress for an agent at specific times during training. The authors show that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning progress when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it. The method is evaluated on ProcGen Benchmark and two MiniGrid environments."
SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper studies the problem of multi-task learning with auxiliary tasks. The authors propose to decompose auxiliary task gradients into directions which help, damage or leave the primary task loss unchanged. This allows weighting the update directions differently depending on their impact on the task of interest. They present a novel and efficient algorithm for that purpose and show its advantage in practice."
SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper presents a method for one-shot learning of objects in a 3D environment, where the goal is to learn to bind a new word to an unfamiliar object after a single exposure. The agent is trained by a combination of conventional RL and predictive (semi-supervised) learning. The authors propose a novel form of external memory, inspired by the dual-coding theory of knowledge representation (Paivio, 1969). They show that the agent exhibits an emergent propensity to integrate both fast-mapped and slowly-learned word meanings in a single episode, successfully executing instructions such as “put the dax on the bed” and “This is a dax”. They also show how the external memory is used as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later."
SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the effect of class-imbalance (CI) in few-shot learning (FSL) on the performance of several state-of-the-art FSL methods. CI is defined as the difference between the number of samples for each class in the support set and the query set. The authors consider three regimes of CI: (1) dataset vs. support set imbalance, (2) different imbalance distributions (linear, step, random), and (3) effect of rebalancing techniques (over-sampling, feature transfer, and metric-based methods). They show that meta-learning methods do not learn to balance from random-shot episodes alone, and that the effects of imbalance at the dataset level are less significant than the effects at the support-set level."
SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a new convolution layer for graph convolutional neural networks (GCNs). The proposed Polynomial Graph Convolution (PGC) layer is a layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. The authors show that the proposed PGC layer is more expressive than the most common convolution operators and their linear stacking. The proposed method is evaluated on eight commonly adopted graph classification benchmarks."
SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes Sim2SG, a scalable technique for sim-to-real transfer for scene graph generation on unlabeled real-world datasets. The authors decompose the domain gap into appearance, label, prediction and appearance discrepancies between synthetic and real domains. They propose methods to address these discrepancies and achieve significant improvements over baselines in all three environments - Clevr, DiningSim and DriveSim."
SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"This paper proposes a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), for continuous-action reinforcement learning (DRL). The authors claim that REDQ can achieve as good performance as or better than the state-of-the-art model-based algorithm for the MuJoCo benchmark. The key ingredients of REDQ are 1) a high Update-To-Data (UTD) ratio, 2) an ensemble of Q-functions, and 3) in-target minimization across a random subset of Q functions from the ensemble. The authors also provide a theoretical analysis of the proposed method."
SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes an extension of DATASET2VEC (Zaheer et al., 2017) for learning from point clouds subject to permutation invariance or equivariance. The proposed architecture, called DIDA, inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. The second contribution is to empirically and comparatively demonstrate the merits of the approach on two tasks defined at the dataset level."
SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes a graph Laplacian-based method for graph learning from data. The proposed method is based on the idea of graph-Laplacians in graphical lasso, which allows to learn ultra-sparse undirected graphs from potentially high-dimensional input data. Compared with prior state-of-the-art graph learning approaches, the proposed method GRASPEL is more scalable and allows substantially improving computing efficiency and solution quality of a variety of data mining and machine learning applications, such as manifold learning, spectral clustering (SC), and dimensionality reduction."
SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,This paper proposes a method to learn goal-conditioned policy by jointly learning an extra abstract-level policy and an intrinsic reward function. The abstract level policy is conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals for the goal-conditional policy. The learned discriminator serves as an intrinsic rewards function to imitate the trajectory induced by the abstract policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed method.
SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"This paper studies the problem of policy switching in deep reinforcement learning, i.e., the requirement that the deployed policy that interacts with the environment cannot change frequently. The authors propose a new policy switching criterion based on the feature distance between the deployed Q-network and the underlying learning Q-networks. They show that this criterion substantially decreases the switching cost while maintaining a similar sample efficiency to the case without the low-switching-cost constraint. They also provide a theoretical justification from a representation learning perspective."
SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a new stochastic method for solving non-convex large-scale optimization problems, called Homotopy-SGD, which is based on the combination of homotopy methods and SGD. Theoretical analysis of the optimality tracking properties and convergence rate are provided, under some realistic and mild assumptions. The theoretical results are confirmed by some empirical evaluations, which also show the potential in terms of performance of combining SGD with an homotopic strategy."
SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning approach for the task of shape completion, i.e., estimating the 3D shape of an object from sparse point clouds. The authors cast the task as a Bayesian meta learning problem by treating each object as a task and its sparse observations as the corresponding contextual dataset. They introduce an encoder that describes the posterior distribution of a latent representation conditioned on the sparse cloud. They demonstrate the efficacy of the proposed method on the standard ShapeNet and ICL-NUIM benchmarks."
SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper studies adversarial robustness of deep neural networks (DNNs) from a channel-wise perspective. The authors highlight two new characteristics of adversarial examples: 1) activation magnitudes are higher than that of natural examples, and 2) the channels are activated more uniformly by adversarial images than natural images. To address this issue, the authors propose Channel-wise Activation Suppressing (CAS), which learns the channel importance and leverages the learned channel importance to suppress the channel activation in the training phase. They show that, CAS can train DNNs that inherently suppress redundant channels and can be applied to existing defense methods to further improve their robustness."
SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper provides a non-asymptotic analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and over-parametrization with generalization performance. The authors exploit the fact that gradient flow preserves a certain matrix that characterizes the imbalance of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of the initialization imbalance. They also derive a novel upper-bound on the operator norm distance between the trained network and the min-norm solution, where h is the hidden layer width."
SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of kernel functions on the basis of the spherical sphere. The authors show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their shallow two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of deep architectures. "
SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper focuses on the overestimation issue in deep reinforcement learning (DRL) and proposes a novel algorithm that is able to minimize overestimation, avoid the underestimation bias and retain the policy improvement during the whole training process. Specifically, they add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which they propose a novel method to provide theoretical and experimental guarantee for future policy improvement. They evaluate their method on a set of classical control tasks and show that the proposed algorithms are more computationally efficient and stable than several existing algorithms for continuous control."
SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper proposes a generalization of the inverse reinforcement learning (IRL) problem to a stochastic inverse RL problem, where the goal is to recover the reward functions from expert demonstrations. The main idea is to use the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the probability distribution as the first solution to the SIRL problem. The solution is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem. In the experiment, the authors evaluate their approach on the objectworld."
SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervisory learning. The analysis is based on a simple but realistic “expansion assumption,” which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. The authors also assume that neighborhoods of examples in different classes have minimal overlap. They prove that under these assumptions, the minimizers of population objectives based on self training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels."
SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a hierarchical approach for long-term video prediction. The proposed method predicts future frames by first estimating a sequence of semantic structures and then translating the structures to pixels by videoto-video translation. The authors evaluate their method on three challenging datasets involving car driving and human dancing, and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (i.e., thousands frames)."
SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new graph neural network (GNN) architecture, called IGNNS, which is based on the Iterated Function System (IFS), which is an important research field in fractal geometry. The key idea is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS for iterating, and then obtain the high-level representation of graph nodes. Experiments on semi-supervised node classification tasks on citation network datasets show that the performance of the proposed method is obviously better than the related methods."
SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a geometric graph generative adversarial network (GG-GAN) which is a Wasserstein GAN that is permutation equivariant and easily scales to generate graphs of tens of thousands of nodes. The main contributions of this paper are: (1) the proposed method is able to model complex relations, (2) modeling isomorphic graphs consistently, (3) fully exploiting the latent distribution, and (4) achieves competitive or surpassing the state-of-the-art methods that are either slower or non-equivariant."
SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a novel method for exploring how neurons within a neural network interact. In particular, they consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y, where X and Y are sets of neurons in different layers of the network. They formalize the problem in terms of the Minimum Description Length principle, by which they identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that their rules give clear insight in how networks perceive the world: they identify shared, resp. class-specific traits, compositionality within the network, as well as locality in convolutional layers."
SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies the problem of policy optimization in the fixed-point setting. The authors propose a unified conceptual and mathematical framework for the study of algorithms in this regime. They show that for naı̈ve approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. "
SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a memory-efficient method to estimate the gradient of neural ODEs. The proposed method is based on the asynchronous leapfrog (ALF) solver, which has a constant memory cost w.r.t number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). The paper provides comprehensive theoretical analysis on the properties of the proposed method. Experiments on various tasks, including image recognition, continuous generative modeling, and time series modeling, demonstrate the effectiveness of the method."
SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provides an in-depth analysis that assesses the ability of each model to (1) fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) to generate images composed of unseen object combinations. The authors identify the advantages of different pipeline components, and find that encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionsings, using semantically aware losses such as scene-graph perceptual similarity helps improve some dimensions of the generation process, and enhancing the quality of generated masks."
SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper compares Graph-Augmented Multi-Layer Perceptrons (GA-MLPs) and Graph Neural Networks (GNNs) from the perspective of expressive power and learning. GA-MLP augments node features with certain multi-hop operators on the graph and then applies learnable node-wise functions. Theoretical analysis shows that there exists a separation in expressive power that grows exponentially in depth. In particular, unlike GNNs, the authors show that GA- MLPs are unable to count the number of attributed walks. The authors also demonstrate via community detection experiments that the performance of the proposed model is limited by the choice of operator family."
SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes an Actor-Learner Distillation (ALD) procedure that leverages a continual form of distillation to transfer learning progress from a large capacity learner model to a small capacity actor model. The proposed method is applied in the context of partially-observable environments, where transformer models have had large improvements over LSTMs recently, at the cost of significantly higher computational complexity. With transformer models as the learner and LSTM as the actor, the proposed method recovers the clear sample-efficiency gains of the transformer learner models while maintaining the fast inference and reduced total training time."
SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a Universal Representation Transformer (URT) layer for few-shot image classification. The URT layer is a meta-learning layer that meta-learns to leverage universal features by dynamically re-weighting and composing the most appropriate domain-specific representations. The experiments show that URT sets a new state-of-the-art result on Meta-Dataset.
SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper introduces the Unsupervised Progressive Learning (UPL) problem, where the number of object classes increases with time. The authors propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM), to learn salient representations from a non-stationary stream of unlabeled data. The proposed model learns based on a combination of clustering, novelty detection, forgetting outliers, and storing prototypical representations rather than specific examples."
SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between centralized and decentralized deep learning training. The authors identify the changing consensus distance between devices as a key parameter to explain the gap between decentralized and centralized training. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. They also highlight the intimate interplay between network topology and learning rate at the different training phases and discuss the implications for communication efficient training schemes."
SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,This paper proposes a method for learning similarity metrics between sequences using siamese recurrent neural networks. The authors draw an analogy between synchronized trajectories produced by dynamical systems and the distance between similar sequences processed by a Siamese RNN. They then propose a new neural network model that implements this coupling with a new gate integrated into the classical Gated Recurrent Unit architecture. This model is able to simultaneously learn a similarity metric and the synchronization of unaligned multi-variate sequences in a weakly supervised way. The proposed method is evaluated on an activity recognition dataset.
SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of distributed knowledge distillation (codistillation) on the performance of training neural networks in a distributed setting. The authors propose to use codistillation to encourage the models to represent the same function via an auxiliary loss, which contrasts with the more commonly used synchronous data-parallel stochastic gradient descent methods, where different model replicas average their gradients (or parameters) at every iteration and thus maintain identical parameters. They investigate the effectiveness of the proposed method in the distributed setting and show that even at moderate batch sizes, models trained with codistilliation can perform as well as those trained with synchronous methods, despite using a much weaker synchronization mechanism. "
SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. The authors claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters, the SGD iterates will converge to a heavy-tailed stationary distribution. They rigorously prove this claim in the setting of quadratic optimization: they show that even in a simple linear regression problem with independent and identically distributed Gaussian data, the iterates can be heavy-tail with infinite variance. They further characterize the behavior of the tails with respect to the stepsize, the dimension, and curvature. They translate their results into insights about the behaviour of SGDL in deep neural networks."
SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the effect of bandpass filtering on the performance of standard graph convolutional networks (GCNs) for supervised community detection. The authors empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph (e.g., images), high-frequencies are less crucial to community detection, and it is possible to obtain state-of-the-art accuracy with simple classifiers that rely only on a few low frequencies. Then, the authors also study the stability of a GCN w.r.t. spectral perturbations, and show that they are more robust to high-frequency, which is counterintuitive when compared to vanilla CNNs."
SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a method for learning the adjacency matrix of a graph neural network and a graph structure of the nodes connectivities simultaneously from data. The authors identify a “supervision starvation problem” for latent graph learning, where the edges between pairs of nodes that are far from labeled nodes receive insufficient supervision. They propose a solution to this problem by supplementing the training objective with a well-motivated self-supervised task. They show the effectiveness of their model through a comprehensive set of experiments and analyses."
SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,This paper proposes a novel method for learning in the label-agnostic incremental setting. The novelty detection method leverages network confusion caused by training incoming data as a new class. The proposed method is evaluated on a set of image classification benchmarks. The results show that the proposed method outperforms the existing methods. 
SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a method for learning to interpret natural language constraints for safe RL. The authors propose a new multi-task benchmark, called HazardWorld, that requires an agent to optimize reward while not violating constraints specified in free-form text. They then develop an agent with a modular architecture that can interpret and adhere to textual constraints while learning new tasks. Their model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. Across different domains in HAZARDWORLD, they show that their method achieves higher rewards (up to 11x) and fewer constraint violations (by 1.8x) compared to existing approaches."
SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a few-shot semantic edge detection method, which aims to localize boundaries of novel categories using only a few labeled samples. The proposed method is based on a semantic segmentation module in small-scale to compensate for the lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. The authors also propose a new regularization method based on multi-split matching."
SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method, Causal Screening, to identify the most influential edges and generate post-hoc explanations for model predictions. It incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, it can be used to generate faithful and concise explanations for any GNN model. Experiments on three graph classification datasets show that it achieves significant improvements over state-of-the-art approaches w.r.t. two quantitative metrics: predictive accuracy, contrastivity, and sanity checks."
SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper proposes a new measure of model simplicity, i.e., the smallest fraction of the network’s parameters that can be kept while pruning without adversely affecting its training loss. The authors show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10. They also show the mutual information between the predictions of their new measure and strong existing measures based on models’ margin, flatness of minima and optimization speed."
SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,"This paper proposes Discrete Object-factorized Representation Planning (DORP), which learns temporally-abstracted discrete representations from exploratory video data in an unsupervised fashion via a mutual information maximization objective. DORP plans a sequence of abstract states for a low-level model-predictive controller to follow to solve long-horizon tasks. It discovers independent representations per object and binary properties such as a key-and-door."
SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit- level sparsity. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction. The method enables the exploration of the full mixed precision space with a single gradient-based optimization process.
SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the robustness of binary neural networks (BNNs) against gradient-based adversarial attacks. The authors claim that BNNs suffer from gradient vanishing issues and show a fake sense of robustness. To mitigate this issue, the authors propose a simple temperature scaling approach to mitigate the issue while preserving the decision boundary. Experiments on CIFAR-10/100 datasets demonstrate the effectiveness of the proposed method."
SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel recurrent neural network (RNN) model, called ProtoryNet, that predicts the most similar prototype for each sentence in a text sequence. The RNN backbone captures the temporal pattern of the prototypes, to which the authors refer as prototype trajectories. The prototype trajectory enables intuitive, fine-grained interpretation of how the model reached to the final prediction, resembling the process of how humans analyze paragraphs. Experiments conducted on multiple public data sets reveal that the proposed method is more interpretable and more accurate than the current state-of-the-art prototype-based method."
SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper proposes a hidden Markov model (HMM) that is a special case of recurrent neural networks (RNNs). The authors prove that each HMRNN has the same likelihood function as a corresponding discrete-observation HMM. The authors also show that the parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. Finally, the authors demonstrate how the proposed method can be combined with other neural networks to improve parameter estimation, using an Alzheimer’s disease dataset."
SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a method for analyzing single-cell RNA-seq data in the context of neuronal phenotypes. Specifically, the authors propose a factorized linear discriminant analysis (FLDA) method, which seeks a linear transformation of gene expressions that varies highly with only one phenotypic factor and minimally with the others. The authors further leverage their approach with a sparsity-based regularization algorithm, which selects a few genes important to a specific phenotyping feature or feature combination. They applied this approach to a single- cell RNA-Seq dataset of Drosophila T4/T5 neurons. The analysis confirms results obtained by conventional methods but also points to new genes related to the phenotypes and an intriguing hierarchy in the genetic organization of these cells."
SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper proposes a method for generating saliency maps (saliency maps) that can be used to explain the behavior of an image classifier. The saliency map is assumed to be a random variable, and the posterior distribution over the saliency distribution is calculated by maximizing the ELBO. The proposed method is evaluated on the pixel perturbation benchmark, and it is shown that the proposed method outperforms existing methods."
SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a debiasing method for pretrained text encoders, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, the authors introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words, but also preserves rich semantic information of the original sentences. The FairFil is evaluated on real-world datasets, and the results show that the proposed method reduces the bias degree of pretrained language models."
SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,This paper proposes a sample-wise randomized smoothing technique to improve the l2-robustness of certified models. The main idea is to assign different noise levels to different samples during training and testing. The proposed method is evaluated on CIFAR-10 and MNIST datasets and the experimental results show that the proposed method can achieve better accuracy-robotness trade-off in the transductive setting.
SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a variational auto-encoder-based method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, the proposed method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The proposed method is evaluated in a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising with different real data sets."
SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a multi-hop attention graph neural network (MAGNA) to improve the performance of graph neural networks (GNNs). The proposed method is based on the graph attention mechanism of Velickovic et al. (2018), where the attention is computed between connected pairs of nodes and depends solely on the representation of the two nodes. In this paper, the authors propose to compute the attention between nodes that are not directly connected, which are referred to as “disconnected nodes”. The authors use a diffusion prior on the attention values to efficiently account for all paths between the pair of disconnected nodes. This helps MAGNA capture large-scale structural information in every layer, and learn more informative attention."
SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new benchmark to evaluate the performance of language models on a wide range of NLP tasks. The benchmark covers 57 tasks including math, science, history, computer science, law, and social sciences. The authors claim that the new benchmark is able to assess the breadth and depth of a model’s academic and professional understanding, and can be used to analyze models across many tasks and to identify important shortcomings. The paper also shows that most recent models have near random-chance accuracy, and the best models still need substantial improvements before they can reach expert-level accuracy."
SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,This paper proposes a pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. The authors construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). They pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in table semantic parsers into the pretrained language model. They also include masked language modeling (MLM) on several existing table-and-language datasets to regularize the pretraining process. The proposed method achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantics parsing tasks.
SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper studies the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large (p) and numerous (n) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVMs is shown to converge, as n, p\infty, to a deterministic limit involving simple (small-dimensional) statistics of the data. The paper also shows that the standard MTL LSTM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LSTMs may outperform the MTL approach, even for quite resembling tasks): the analysis provides a simple method to correct these biases, and reveals (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even with quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that the improved MTL MTL method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi- task and transfer learning techniques."
SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper introduces a new family of conditional neural processes (CNPs) that are permutation-invariant and group-equivariant. The authors provide a decomposition theorem for permutation and group equivariant maps, which leads them to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. They build architecture using Lie group convolutional layers for practical implementation. They show that EquivCNP with translation equivariance achieves comparable performance to conventional CNPs in a 1D regression task and zero-shot generalization for an image-completion task."
SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes a method for offline reinforcement learning for text generation, where the goal is to maximize quality given model-generated histories. The proposed method is based on off-policy policy gradient with importance weighting (Hastings, 1970; Hachiya et al., 2009; Parshakova et al. 2019), where training examples with higher probability under the model are weighted higher. Further, the reward functions approximate human judgment of the output quality by estimating how likely human would have generated a sequence. The method is evaluated on summarization, question generation, and machine translation tasks."
SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. The proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive experiments on CIFAR, SVHN, STL-10, ImageNet and Cityscapes validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training."
SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes a method to improve the expressive power of graph neural networks (GNNs) by designing diverse neighborhoods, i.e., rooted sub-graphs, for each target node. The proposed method is based on the idea of diverse sampling, where the representation of the target node is obtained by aggregating the representations of diverse neighborhoods obtained using any GNN model. Experiments are conducted on node-based multi-class node classification tasks and multi-label node classification task."
SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the effect of network and file corruptions on the robustness of action recognition and multi-object tracking models to such corruptions. The authors explore two types of defenses against bit-level corruptions: corruption-agnostic and corruption-aware defenses. They also propose a corruption aware baseline that exploits knowledge of bit- level corruptions to enforce model invariance to corruptions, which is called Bit-corruption Augmented Training (BAT). They show that BAT can improve performance by up to 7.1% accuracy on highly-corrupted videos while maintaining competitive performance on clean/near-clean data."
SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a new representation learning method for time-varying graphs. The proposed method is based on the skip-gram embedding approach, which can be used to perform implicit tensor factorization on different tensor representations. The authors show that the proposed method can disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. The learned representations outperform state-of-the-art methods when used to solve downstream tasks as network reconstruction."
SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper studies the question of whether self-supervised language modeling can be applied to mathematical formulas. The authors propose a skip-tree task for training language models for formal mathematics, and show that models trained on this task show surprisingly strong mathematical reasoning abilities. They also analyze the models’ ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs."
SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper identifies a new type of gradient masking effect called imbalanced gradients, where the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. To exploit this, the authors formulate a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms via a two-stage process. They derive MD variants of PGD and MT Multi-Attack (Gowal et al., 2019), and these MD attacks can be used to re-evaluate the robustness of 12 adversarial training-based defense models. For 6 out of the 12 defenses, our attack can reduce their PGD robustness by at least 9%."
SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a graph-to-sequence neural network system to generate verifiable axiomatic proofs (via rewrite rules) for equivalence for a class of symbolic programs. Evaluated on a rich language for linear algebra expressions, this system guarantees no false positives are ever produced, all true negatives are preserved, and produces correct proofs of up to 10 axioms in length in 93% of the truly equivalent cases. It achieves 93% average true positive coverage on 10,000 test cases while ensuring zero false positives."
SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a multimodal bidirectional Transformer architecture for self-supervised learning of contextualized audio-visual representation from unlabeled videos. The authors propose to decompose the Transformer into modality-specific and modality shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers."
SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes a method for online contextualized few-shot learning (OC-FSL) that aims to bridge the gap between typical human and machine-learning environments by extending the standard framework of FSL to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. The authors propose a new RoamingRooms dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. Furthermore, they convert popular FSL approaches into online versions and propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) on temporal graphs. In particular, the authors propose to train GNNs on a sliding window of different sizes over the history of the temporal graph. The sliding window size is determined by how much information is used for training or evaluated during the training process. The authors experiment with three representative GNN architectures and two scalable GNN techniques, on three new datasets. The results show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy compared to training over a full graph."
SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"This paper proposes a novel regularization technique to improve the generalization of RL agents in terms of feature representation. The authors propose a cross-state self-constraint (CSSC) that compares the representation similarity across different pairs of states. The idea is that if the agent perceives similar critical patterns across sequence of states, it would be similar if its behavior is similar while acting differently if it perceives different patterns. The proposed method is evaluated on the OpenAI ProcGen benchmark and shows significant improvement on generalization performance."
SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on graph neural networks (GNNs) in a restricted near-black-box setting, where the attacker has no access to model parameters and predictions. The authors formulate the adversarial attack as an optimization problem to maximize the mis-classification rate over the selected set of nodes, and they carry out formal analysis regarding this optimization problem. The proposed optimization problem is combinatorial and seems hard to solve in its original form, and the authors mitigate this problem by rewriting the maximization problem and connecting it with influence maximization on a special linear threshold model. Inspired by this connection, the authors derive a group of near black-box attack maximizing strategies and show that they outperform state-of-the-art attacking strategies on GNNs."
SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of causal structure learning in directed acyclic graphs (DAGs). The authors propose to exploit a low rank assumption regarding the adjacency matrix of a DAG causal model to mitigate this problem. Theoretically, the authors show that the maximum rank is highly related to hubs, suggesting that scale-free networks which are frequently encountered in real applications tend to be low rank. The authors also provide empirical evidence for the utility of their low rank adaptations."
SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML). The method performs co-optimization of neural architectures, training hyper-parameters and data augmentation policies in an end to-end fashion without the need of model retraining. Experiments show that DiffAutoML achieves superior performance compared with multi-stage AutoML algorithms with higher computational efficiency."
SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper proposes a class of uncertainty measures for regression tasks based on the Normal-Wishart distribution. In particular, the authors propose a Regression Prior Network (RPN) and Ensemble Distribution Distillation (EnD2) approaches to distill an ensemble of regression models into an RPN that retains the improved predictive performance and uncertainty estimates of the original ensemble. Two RPN training approaches are proposed. First, the reverse-KL divergence between the model and the target distribution is described, allowing the behaviour of the RPN to be explicitly controlled but requiring an OOD training dataset. Second, EnD2 is used, where an ensemble distribution distillation is used. The proposed approach is evaluated on synthetic data, selected UCI datasets, and two monocular depth estimation tasks, and shows competitive performance with ensemble approaches."
SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper proposes a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The framework incorporates MAML into a Bayes online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed framework can effectively prevent catastrophic forgetting.
SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper studies the expressive power of graph neural networks (GNNs) for counting induced subgraphs. In particular, the authors propose a new GNN architecture, called RNP-GNN, which is motivated by the idea of local neighborhood pooling (LRP) networks. The authors show that the proposed model can count subgraph of size $k$ and overcomes a known limitation of low-order GNNs. Moreover, they prove that, in several cases, the proposed method can greatly reduce the computational complexity compared to the existing higher-order k-GANs and LRP networks."
SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the volume-changing behavior of popular learning algorithms (MWU, OMWU and FTRL) in two-player bimatrix games and multi-player potential games. The authors show that MWU, its optimistic variant (OMWU) and follow-the-regularized-leader (FTRL), are Lyapunov chaotic almost everywhere in the cumulative payoff space. They also show the equivalence of MWU and OMWU in graphical games."
SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"This paper proposes a new motivation for designing the proximal function of adaptive algorithms, named as marginal regret bound minimization. The authors propose a new class of adaptive algorithm that achieves marginal optimality, but can also potentially converge much faster than any existing adaptive algorithms in the long term. Theoretical and empirical results show the superiority of the proposed algorithm."
SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes a new framework for policy gradient style reinforcement learning (RL) algorithms with mean-variance control based on the expected quadratic utility maximization (EQUM) objective. The proposed framework has several interpretations, including reward-constrained variance minimization and regularization, as well as agent utility maximisation. Compared with existing MVRL methods, the proposed EQUM framework is computationally friendly and is suitable for many real-world applications, such as finance and playing games. The experiments demonstrate the effectiveness of the proposed method."
SP:bf9d66f713b6502d274143c6273b2d071a0c045e,This paper proposes a method for learning with auxiliary tasks to improve the performance on the main task. The main idea is to learn a network that combines all losses into a single coherent objective function and learn nonlinear interactions between tasks. The proposed method is evaluated on image segmentation and learning with attributes in the low-data regime. 
SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"This paper proposes a method to detect out-of-distribution sentences in Neural Machine Translation using the Bayesian Deep Learning equivalent of Transformer models. The authors develop a new measure of uncertainty designed specifically for long sequences of discrete random variables—i.e. words in the output sentence. They use their new measure on a Transformer model trained with dropout approximate inference on the task of German-English translation using WMT13 and Europarl. They show that their measure is able to identify when Dutch source sentences, sentences which use the same word types as German, are given to the model instead of German."
SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the performance of pruning neural networks at initialization. It compares SNIP (Lee et al., 2019), GraSP (Wang et al. 2020), SynFlow (Tanaka et al 2020), and magnitude pruning (Dejia et al, 2020) and shows that random shuffling of weights or sampling new initial values preserves or improves accuracy. It also shows that the per-layer pruning decisions made by these methods can be replaced by a per- layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics."
SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper proposes Fed-learning, a federated learning algorithm that is able to defend against both a semi-honest server and Byzantine malicious clients. The authors propose to use a robust mean estimator called FilterL2 to robustly aggregate the possibly contaminated updates and secure aggregation to protect the privacy of the clients. They propose to split the clients into shards, securely aggregate each shard’s updates and run the updates from different shards. The evaluation shows that F2ED-learning consistently achieves optimal or close-to-optimal performance under three attacks among five robust FL protocols."
SP:9f89ff90b203d86a569e3d5148546942f5bf2093,This paper introduces a suite of offline model-based optimization (MBO) benchmarks for black-box optimization (BDO) problems with data-driven offline evaluation. The authors provide a comprehensive evaluation of the state-of-the-art offline MBO methods under identical assumptions and show that naıııve gradient ascent (NAI) is the best baseline method for the proposed benchmark. The paper also provides reference implementations of the proposed benchmarks. 
SP:073958946c266bf760d1ad66bd39bc28a24c8521,"This paper proposes a new multimodal ELBO formulation for self-supervised generative models. The proposed method generalizes prior works (MVAE, MMVAE) and combines their benefits. The authors also analyze the strengths and weaknesses of previous works and relate them directly to their. Experiments show the advantage of the proposed method compared to state-of-the-art models."
SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a method for Bayesian Optimization (BO) that leverages the prior knowledge of domain experts in order to improve the performance of BO. Specifically, the authors propose to use priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). These priors are then combined with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. The authors show that the proposed method is around 12x faster than state-of-the-art methods without user priors and 10,000 times faster than random search on a common suite of benchmarks."
SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper proposes to use binary weights and activations in VAE and flow models. The authors propose a new class of binary weight normalization techniques and show that they can reduce the computational cost of these models. They also provide insights for architecture designs of these binarized generative models and demonstrate that two state-of-the-art models, the ResNet VAEs and Flow++ models, can be trained using these techniques."
SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning, where the goal is to train neural networks to be robust against natural, out-of-distribution shifts in data distribution in a general context. To achieve this, the authors propose to obtain models of natural variation, which vary data over a range of natural conditions, and develop three novel model based robust training algorithms that improve the robustness of DL with respect to natural variation. Extensive experiments show that the proposed method outperforms ERM, adversarial training, and domain adaptation techniques."
SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the optimization of CNNs with ReLU activations in terms of exact finite-dimensional convex optimization problems. The authors show that two-layer CNNs can be globally optimized via an $L_2$ norm regularized convex program, and multi-layer circular CNNs are equivalent to an $\ell_1$-convex optimization problem. They also extend these results to three-layer convolutional neural networks with two ReLU layers. "
SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a framework for interpretable learning from demonstration (LfD) that allows human operators to teach a robot about different spatial, temporal and force-related aspects of a robotic manipulation task, using a set of demonstrations. The proposed method is based on a probabilistic generative model with a high-capacity neural network. The latent variables are explicitly aligned with high-level notions and concepts that are manifested in the demonstrations. They show that this alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables."
SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes to use the Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that the proposed method successfully reduces overfitting. The proposed method, VIBERT, finds the simplest sentence embedding, predictive of the target labels, while removing task-irrelevant and redundant information. Extensive experiments and analyses show the effectiveness of the proposed approach."
SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method to recover the 3D shape of an object from a single 2D image using a pre-trained 2D generative adversarial network (GAN). The main idea is to use a weak convex shape prior and explore the viewpoint and lighting variations in the GAN image manifold, and exploit these variations to refine the underlying object shape in an iterative manner. Experiments on human faces, cars, and buildings show that the method is able to recover 3D shapes with high precision."
SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a new long-tail classifier called RIDE, which reduces the model variance with multiple experts, reduces model bias with a distribution-aware diversity loss, and reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is a universal framework that is applicable to various backbone networks, long-tailed algorithms, and training mechanisms for consistent performance gains."
SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper provides an empirical analysis of the similarity and applicability of different pruning criteria for channel pruning. The authors claim that there are two blind spots: (1) Similarity: There are some strong similarities among several primary pruning criterion that are widely cited and compared. According to these criteria, the ranks of filters’ Importance Score are almost identical, resulting in similar pruned structures. (2) Applicability: The filters' importance scores are too close to distinguish the network redundancy well. In this paper, the authors analyze the above two issues with layer-wise pruning or global pruning and show that the results of `1 and `2 pruning are not always similar. "
SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for programming language that considers the inherent structure of code. Specifically, the authors use data flow in the pre-training stage, which is a semantic-level structure of codes that encodes the relation of “wherethe-value-comes-from” between variables. They develop GraphCodeBERT based on Transformer and introduce two structure-aware pretraining tasks, one is to predict code structure edges and the other is to align representations between source code and code structure. They evaluate their model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pretraining task can improve the performance."
SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method to improve the accuracy of regression models that are trained on a skewed dataset. The authors assume the existence of enough unlabeled data that follow the true distribution and assume that the distribution can be roughly estimated from domain knowledge or a few samples. They use a semi-supervised learning framework with an adversarial network to force the distribution of the regression output to resemble the assumed true distribution. They evaluate the proposed approach on four real-world datasets (pLogP, Diamond, House, Elevators)."
SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of out-of-distribution generalization, i.e. whether compositional generalization can still be learned in the test distribution. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, they propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than a 20% absolute increase in various experiments compared with baselines."
SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper proposes a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. The authors propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, which closes the gap that no poisoning method exists for policy-based RL agents. The proposed method uses a novel metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple deep RL agent and multiple environments show that the poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy."
SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper introduces Dynamic Tensor re-materialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. The authors prove that DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N})$ memory budget with only $O(N)$ tensor operations, which is within a constant factor of the optimal and matches the offline bound of the static checkpointing technique of Chen et al. (2016). The authors also incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting metadata on tensors."
SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task of hallucination detection at the token level, which aims to predict if each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. It also introduces a novel method for learning to model hallucination, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach."
SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a method to find a distinct generator architecture for each class in conditional generative adversarial networks (cGANs). The authors propose a neural architecture search (NAS) algorithm on top of reinforcement learning so that the generator architecture of each class is automatically designed. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data. The authors also propose a mixed-architecture optimization mechanism so that search and re-training processes can be performed efficiently. In a relatively small search space, the proposed Multi-Net NAS algorithm achieves FID scores of 5.85 and 12.28 on CIFAR10/100."
SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a method to estimate the average causal effect of a treatment from observational data. The proposed method is based on the idea of orthogonality, i.e. the treatment assignment should be orthogonal to the outcomes. Theoretical results show that the proposed method yields an asymptotically normal estimator, which is strictly smaller than other estimators. The method is evaluated on a variety of benchmark datasets."
SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the role and expressive power of affine parameters in BatchNorm, a feature normalization technique that normalizes activations and then applies a learned affine transform to transform features in this way. The authors investigate the performance achieved when training only these parameters and freezing all weights at their random initializations. For example, sufficiently deep ResNets reach 82% (CIFAR-10) and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. The paper claims that this performance is achieved by naturally learning to disable around a third of the random features."
SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes to adapt by test entropy minimization (tent1) to adapt the model for confidence as measured by the entropy of its predictions. Tent estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, and semantic segmentation from GTA to Cityscapes on VisDA-C benchmark."
SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate the uncertainty in RNNs via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, the proposed method offers several advantages in different settings. It can learn deterministic and probabilistic automata from data, learn well-calibrated models on real-world classification tasks, improve the performance of out-of-distribution detection, and control the explorationexploitation trade-off in reinforcement learning."
SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper studies the problem of privacy-preserving deep learning algorithms. The authors propose a stochastic differential equation principled residual perturbation for privacy preserving DL, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that the proposed method guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, it outperforms the state-of-the-art DPSGD in both membership privacy protection and maintaining the DL models' utility."
SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes to extend the PoWER-BERT (Goyal et al., 2020) model with a length-adaptive Transformer (length drop) and a drop-and-restore (drop and restore) method to reduce the computational cost of inference. The length drop is a structural variant of dropout, which is used to determine the length of a sequence at each layer. The drop and restore method is applied at intermediate layers and the last layer if necessary. The experiments on SQuAD 1.1, MNLI-m, and SST-2 show that the proposed method can achieve up to 3x speed-up over the standard transformer without sacrificing accuracy."
SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper proposes to improve the expressiveness of graph neural networks (GNNs) by exploring powerful aggregators. The authors reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficients for building more powerful aggregation functions. Based on the analysis, the authors develop two GNN layers, ExpandingConv and CombConv. Experimental results show that the proposed models significantly boost performance, especially for large and densely connected graphs."
SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"This paper proposes a disentanglement evaluation metric that measures intrinsic properties of a generative model with respect to its factors of variation. The evaluation metric circumvents the typical requirements of existing evaluation metrics, such as requiring an ad-hoc model, a particular dataset, or a canonical factorization. Experiments on several state-of-the-art models across multiple datasets show that the method ranks models similarly to existing methods."
SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make data ""unlearnable"", i.e., data that is not useful for training deep neural networks (DNNs). The idea is to add a small amount of ""error-minimizing noise"" to the training data, where the goal is to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is ""nothing to learn"" from these examples. The proposed method can be applied either sample-wise or class-wise, and is resistant to common data filtering methods. Experiments are conducted on CIFAR-10 and ImageNet datasets."
SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper proposes an extension of MuZero to stochastic, two-player, zero-sum games of perfect information. The authors formalize chance as a player in the game and incorporate the chance player into the MuZero network architecture and tree search. Experiments show that NDMZ is capable of learning effective strategies and an accurate model of the game."
SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper introduces Hindsight off-policy options (HO2), an option learning algorithm for reinforcement learning. HO2 infers option and action probabilities for trajectories in hindsight, and performs critic-weighted maximum-likelihood estimation by backpropagating through the inference procedure to infer option probabilities along trajectories and update all policy parts via an efficient dynamic programming procedure. The algorithm updates the policy via the policy updates via the critic weights, which is similar to Abdmaleki et al. (2018) and Wulfmeier et al (2020) and combines these with an actor-critic algorithm for robust and efficient option learning. The proposed method outperforms recent work in option learning on common benchmarks and is able to solve complex, simulated robot manipulation tasks from raw pixel inputs."
SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel functional form of the Bellman equation to optimize for the maximum reward achieved at any time step in an episode. The authors also introduce the corresponding evaluation and optimality operators, and prove the convergence of Q-learning with the max-Bellman formulation. The proposed method can be applied to deep reinforcement learning algorithms by demonstrating state-of-the-art results on the task of de novo drug design."
SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetworks (CHNets), a method for adapting neural networks to incorporate new output features which are both time and data-efficient. Specifically, the authors propose an auxiliary model which generates parameters for extending the base model to a new feature by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. The authors also propose to use a CHN to augment a partial variational autoencoder (P-VAE) and evaluate the performance of the proposed method."
SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method to perform Bayesian inference in deep neural networks (NNs) by performing inference over only a small subset of the model parameters while keeping all others as point estimates. This enables the authors to use expressive posterior approximations that would otherwise be intractable for the full model. In particular, the authors develop a practical and scalable Bayesian deep learning method that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The proposed method can be applied post-hoc to any pre-trained model, making it attractive for practical use."
SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a new approach for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the authors use a model of the environment to obtain embedding for state and action and present a generic architecture that uses these to learn a policy. In this way, the embedded representations obtained via the approach enable better generalization over both state and actions by capturing similarities in the embedding spaces. Evaluations on several gaming, robotic control, and recommender systems show it significantly outperforms state-of-the-art models."
SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a generative model for learning diverse and useful views for unsupervised representation learning. The proposed method is based on the idea of viewmaker networks, which are generative models that learn to produce useful views from a given input. Viewmakers are stochastic bounded adversaries: they produce views by generating and then adding an $\ell_p$-bounded perturbation to the input, and are trained adversarially with respect to the main encoder network. The learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations on CIFAR-10, despite not including transformations like cropping or color jitter. Moreover, the learned views can be combined with handcrafted views to improve robustness to common image corruptions and can increase transfer performance in cases where handcrafted data are less explored."
SP:ef7735be9423ad53059505c170e75201ca134573,"This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. The authors demonstrate how different existing detection approaches fail to detect certain types of outliers. They utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outlier. The experiments on CIFAR10, SVHN and MNIST as in-distribution data and Imagenet, LSUN and SVHNs as OOD data across different DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5 demonstrate the effectiveness of the proposed approach."
SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model that combines VAEs and autoregressive models. The authors argue that VAEs should be able to learn hierarchical representations that are at least as good as VAEs (in the worst case, VAEs learn better latent representations) and faster, better models if they exist. They also propose to scale VAEs to greater stochastic depth than previously explored and evaluate it on CIFAR-10, ImageNet, and FFHQ. The results show that the proposed VAE achieves higher likelihoods, uses fewer parameters, generates samples thousands of times faster, and is more easily applied to high-resolution images."
SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a new method for contrastive representation learning. The proposed method, called Conditional Conditional-NCE (CNCE), is a generalization of the popular noise-contrastive estimation (NCE) method (Tian et al., 2020), which is based on sampling negatives from conditional distributions. The authors show that the proposed method is a looser bound on the mutual information than NCE, and that it has a lower variance. Experiments are conducted on four image datasets (Meta-Dataset, ImageNet, CIFAR-10, Cifar-100, and ImageNet-200) and compared with IR, CMC, and MoCo."
SP:613a0e2d8cbe703f37c182553801be7537333f64,"This paper studies the problem of data leakage in federated learning (FL). The authors propose a new method to recover batch data from the shared aggregated gradients. The proposed method, called catastrophic data leakage from gradients (CAFE), is able to perform large-batch data leakage attack with high data recovery quality. Experimental results on vertical and horizontal FL settings have validated the effectiveness of CAFE in recovering private data. "
SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for inferring multi-agent interactions in a dynamic setting, where interactions change over time. The proposed model, called Dynamic multi-Agent Relational Inference (DYARI), is based on a variational auto-encoder and graph neural networks. The model is evaluated on simulated physics systems and a real-world basketball trajectory dataset. The results show that DYARI is able to recover ground-truth interactions in an unsupervised manner. "
SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes an inductive collaborative filtering framework that learns a hidden relational graph among users from the rating matrix. The key advantage of the proposed method is the capability for inductively computing user-specific representations using no feature, with good scalability and superior expressiveness compared to other feature-driven inductive models. Extensive experiments demonstrate that the proposed model achieves state-of-the-art performance for inductive learning on several matrix completion benchmarks, provides very close performance to transductive models when given many training ratings and exceeds them significantly on cold-start users."
SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage approach for learning disentangled representations of latent variables in deep generative models (DGMs). In particular, the authors propose to learn independent and correlated latent variables separately, and then combine them with a generative model that is trained to model the missing latent variables. The authors show that the proposed approach improves the reconstruction quality of VAEs, GANs, FLOWs, etc."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations learned by maximizing mutual information (MI) between random variables in the context of RL from a theoretical perspective. In particular, the authors study two popular MI-based representation learning objectives, i.e., mutual information maximization and mutual information minimization, from the perspective of sufficiency. They show that both objectives are insufficient for the general class of MDPs, in the most general case, and prove that another typical objective is sufficient. The experimental results corroborate their theoretical findings, and demonstrate that sufficiency has a substantial impact on the performance of an RL agent."
SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. In particular, the authors show that the non-convex neural networks training problem is equivalent to a finite-dimensional convex copositive program. The authors also provide the first algorithms for provably finding the global minimum of the vector output neural network learning problem, which are polynomial in the number of samples for a fixed data rank and exponential in the dimension."
SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The proposed method, Language-mediated, Object-centric Representation Learning (LORL), builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. LORL enables them to further learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. Experiments on Shop-VRBSimple and PartNet-Chairs show that language significantly contributes to learning better representations. The authors also show that concepts learned by the proposed method aid downstream tasks such as referring expression comprehension."
SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes EM-RBR (embedding and rule-based reasoning), a general framework that combines the advantages of reasoning based on rules and the state-of-the-art models of embedding for knowledge graph completion (KGC). The main idea is to utilize relational background knowledge contained in rules to conduct multi-relation reasoning link prediction. In this way, we can find the most reasonable explanation for a given triplet to obtain higher prediction accuracy. Experiments on FB15k, WN18 and a new dataset FB15K-R demonstrate the effectiveness of the proposed method."
SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a method for combining declarative and procedural knowledge in the form of object files and schemata. The object files are active modules that maintain the state of a single object and the schematas are passive modules that provide information about the current state of the object. The authors propose to use attention to determine which object files to update, the selection of schematas, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type."
SP:42a3c0453ab136537b5944a577d63412f3c22560,"This paper proposes a neural module network (NMN) approach for video-grounded language tasks, which extend the complexity of traditional visual tasks with the additional visual temporal variance. Motivated by recent NMN approaches on image-grounding tasks, the authors introduce Visio-Linguistic Neural Module Network (VilNMN), which is a pipeline of neural modules. VilNMN first decomposes all language components to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. The experiments show that the proposed approach can achieve promising performance on the video QA and TGIF-QA benchmarks."
SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variations of the Policy-Space Response Oracle (PSRO) framework, which is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). The first, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy. The second, mixed-Opponents constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies. Both of these algorithms substantially reduce the amount of simulation during training required by PSRO."
SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper presents a non-attentive variant of Tacotron, which replaces the attention mechanism with an explicit duration predictor. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. The authors also propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner. The proposed method achieves a 5-scale mean opinion score for naturalness of 4.41."
SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes an end-to-end framework for stereo bird's eye view (BEV) layout estimation from a pair of stereo images. The proposed method first generates a disparity feature volume using the features of the stereo images and then project it to the bird’s eye view coordinates. It also applies inverse perspective mapping (IPM) to map the input images and their features to the birds eye view. Experiments on KITTI (Geiger et al., 2013) and CARLA (Dosovitskiy et al, 2017) datasets show that the proposed method achieves state-of-the-art performance."
SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,This paper proposes a relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup. An ablation study shows that the proposed changes w.r.t the GAT and the GGNN are essential.
SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method for model explanation based on the Satisfiability Modulo Theory (SMT) solvers. The main idea is to use Integrated Gradient Information (IG) to find a subset of important neurons in the first layer of the network, which allows the SMT encoding of constraints to scale to larger networks for finding minimal input masks. After solving for the minimal masks, the method scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains “where a model is looking” when making a prediction."
SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a method for robust 3D pose estimation with neural networks. The main idea is to learn a generative model of neural feature activations at each vertex on a dense 3D mesh, which is then used to estimate the 3D object pose by minimizing the reconstruction error between NeMo and the feature representation of the target image. To avoid local optima in the reconstruction loss, the feature extractor is trained to maximize the distance between the individual feature representations on the mesh using contrastive learning. Experiments on PASCAL3D+, occluded-PASCAL-3D+ and ObjectNet3D show that NeMo is more robust to partial occlusion and unseen pose compared to standard deep networks, while retaining competitive performance on regular data."
SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes an approach for feature compatible learning without inheriting old classifier and training data, i.e., Non-Inherent Feature Compatible Learning. The proposed approach requires only features extracted by the old model’s backbone and new training data and makes no assumption about the overlap between old and new data. The authors propose a unified framework for FCL, and extend it to handle the case where the old models is a black-box. Specifically, they learn a simple pseudo classifier in lieu of old model, and further enhance it with a random walk algorithm. Experiments on ImageNet ILSVRC 2012 and Places365 show the efficacy of the proposed approach."
SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. The authors propose to use an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200\�20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm, as one of the hyper- parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). The results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization performance is not always consistent across phases of the training process."
SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes an autoregressive approach for entity retrieval, which generates unique entity names in an auto-regressive fashion, conditioned on the context. The proposed approach is evaluated on several entity retrieval tasks, including disambiguation, linking, and document retrieval, and shows state-of-the-art or competitive performance. The paper is well-written and easy to follow. "
SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. On each time step t, the algorithm receives a routing request and must select a valid path for each edge e in the selected path, incurs a cost ce = fe(x t e) + \eta t e, where x t e is the flow on edge e at time t, fe is the congestion function, and \eta e is a noise sample drawn from an unknown distribution. The routing requests are supplied adversarially. The algorithm observes ce, and can use this observation in future routing decisions. This paper proposes a learning model for congestion-aware routing and presents an algorithm which seeks to minimize the total driving time across all vehicles. Their algorithm has space complexity O(|E|t) and time complexity O(\|E\log t) and empirically validate their algorithm empirically on graphs from New York City road networks."
SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes a principled masking strategy based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word-masking, entity/phrase masking and random-span masking. Specifically, the authors show that uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. The authors show experimentally that PMIMasking reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training."
SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the effect of partially-conditioned amortised variational inference (i.e., variational posterior inference) on the performance of generative models. The authors show that the ELBO objective forces partially-conditional amortized variational posteriors to approximate the products of smoothing posteriors, which results in the learned generative model being compromised. They demonstrate the theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. They also provide empirical evidence that partial partial conditioning can impair maximum likelihood solutions and inference."
SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF), and obtain optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, the authors first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only O(|D|) memory and O(\|D^2$) time. Then, beyond the generalisation bounds in expectation that demonstrate the average information for multiple trails, they derive generalization bound in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance of the combination method, and validate the theoretical bounds via numerical experiments."
SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a method to improve Random-sampling Neural Architecture Search (RandomNAS) by proposing a proxy search space (PS) that is only a small subset of the global search space. Based on the observation that RandomNAS performs poorly in ranking good architectures and suffers from the lower validation loss for small architectures, the paper proposes an efficient way to Evolve the Proxy Search Space (EPS) to improve the performance of RandomNAS. The paper also proposes a simple size regularization to help RandomNAS-based algorithm jump out of the small architecture traps. In the NASBench201 experiments, EPS delivers a near-optimal solution and surpasses the existing methods."
SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes a meta-RL method that combines imitation learning and meta reinforcement learning to enable an agent to quickly adapt to new tasks at test time. The authors propose a new method to combine imitation learning with meta-reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). The proposed method is able to interpolate from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of Meta-RL benchmarks under sparse rewards. The experiments show that the proposed method outperforms the baselines."
SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the inductive bias of overparameterized convolutional neural networks (CNNs) in the setting where all the patches of an image are orthogonal and the classification task is binary. The authors show that the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set, and empirically verify it in a large number of instances. They also prove that SGD indeed satisfies this condition and prove that it is at most O(d^2 log d), where the filter dimension is d and the VC dimension is exponential in d."
SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,"This paper studies the problem of document classification in the semi-supervised setting, where the corpus is generated by a topic model, and the goal is to distinguish between positive and negative examples. The authors propose two ways to learn the representations of documents: landmark embeddings and the embedding using a function of the predictions of the prediction classifier. They show that under certain conditions, the representation of the document is a linear transformation of the topic posterior moments of the documents. They also show that the representation is close to the true representation if the topic model is a polynomial model. The proposed approach is evaluated on a synthetic dataset, and it is shown that the proposed approach outperforms the baselines."
SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive loss for multimodal variational autoencoder (VAE) models. The main idea is to train the model not just by the commonality between modalities, but by the distinction between “related” and “unrelated’ multimodality data. The proposed method is evaluated on a variety of datasets, including CIFAR-10, Fashion-MNIST, CelebA, and CelebA-HQ. The results show that the proposed method outperforms the baseline."
SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes a method to improve the generative performance of VAEs by adding a reweighting factor to the prior, which is the product of the energy-based prior and the reweighted prior. The reweighter is trained by contrastive estimation, and generalizes to hierarchical VAEs with many latent variable groups. Experiments on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 show that the proposed method improves the performance by a large margin."
SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper studies the problem of inverse reinforcement learning (IRL) in the setting of regularized MDPs, where strongly convex regularizers are applied to the learner’s policy in order to avoid the expert's behavior being rationalized by arbitrary constant rewards, also known as degenerate solutions. The authors propose tractable solutions, and practical methods to obtain them, for regularized IRL. They present theoretical backing for their proposed IRL method's applicability to both discrete and continuous controls, empirically validating their performance on a variety of tasks."
SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; 2) one-to-many translation benefits more from CLSR compared to many- to-one translation, particularly with unbalanced training data."
SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,This paper proposes a Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification of noisy labels. The proposed method normalizes uncertain measures to data-dependent Gaussian measures by imposing geometric constraints in the 2-Wasserstein space. Experimental results demonstrate that the proposed WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets.
SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a method for quantifying the uncertainty of the predictions of convolutional neural networks (CNNs) with respect to the true label with a high probability (e.g., 90%). The authors propose an algorithm that modifies any classifier to output a predictive set containing the true labels with a user-specified probability of 90%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. The method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes. The authors conduct experiments on both Imagenet and ImageNet with ResNet-152 and other classifiers and achieve coverage with sets that are often factors of 5 to 10 smaller than a stand-alone baseline."
SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper proposes a method to compute Wasserstein-2 barycenters of continuous distributions based on a novel regularized dual formulation where the convex potentials are parameterized by input convex neural networks (Amos et al., 2017). The algorithm is straightforward without introducing bias (e.g. Li et al. (2020)) or requiring minimax optimization. This is made possible by combining a new congruence regularizing term combined with cycle-consistency regularization. The paper provides theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach in low-dimensional and high-dimensional quantitative experiments."
SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the problem of learning to separate two low-dimensional sub-manifolds of the unit sphere. In particular, it shows that when the network depth L is large relative to certain geometric and statistical properties of the data, the network width n grows as a sufficiently large polynomial in L and the number of i.i.d. samples from the manifolds is polynomials in L, and the randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. The analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds. And the width enables concentration of the randomly initialized network and its gradients."
SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines, while also being able to leverage off-policy data. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code. The proposed approach, which is referred to as advantage-weighted regression (AWR), consists of two standard learning steps: one to regress onto target values for a value function, and another to regress on weighted target actions for the policy. The paper also provides a theoretical analysis of AWR, including the capability to incorporate off-Policy data with experience replay."
SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes a method for deep quantization of neural networks with heterogeneous bitwidths. The proposed method is based on the sinusoidal regularizer, which is a parametrized version of the regularizer used in Sin2. The main idea of the method is to learn the bitwidth of the layers by making the period of the Sin2 regularizer a trainable parameter and aligning its minima on the quantization levels. The method is evaluated on a variety of deep networks and achieves state-of-the-art performance."
SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,This paper proposes a data augmentation method for neural machine translation (NMT). The proposed method is simple yet effective and can be extremely useful when extra in-domain monolingual data is limited. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models. 
SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"This paper proposes a real-time contribution measurement method for federated learning. The proposed method defines the impact of each agent. The authors comprehensively consider the current round and the previous round to obtain the contribution rate of each agents. The method is sensitive to data volume and data quality, and can be used for mutual comparison between agents."
SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"This paper studies the problem of learning Bayesian networks in the presence of adversarial perturbations to the data, where a fraction of the samples are adversarially corrupted. In particular, the authors consider the setting where the underlying graph structure is known, and assume that the number of variables in the Bayesian network is bounded by a function that depends only on (the fraction of corrupted samples) and not on the total variation distance between the true data and the perturbed data. The authors propose an algorithm that is nearly-linear in the total number of samples and runs in $O(N^{2/\sqrt{N})$ time, where $N$ is the number in the data and $d$ is invertible. The main contribution of the paper is a robust mean estimation algorithm, which is a subroutine of the proposed algorithm, and it is shown that it runs in $\Omega(N^2)$ time."
SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-horizon planning in model-based reinforcement learning. The method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. The proposed method (LatCo) provides a general and effective approach to longer-horizons visual planning. Empirically, the approach significantly outperforms prior models on challenging visual control tasks with sparse rewards and long-term goals."
SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model to model the process of data-curation in Bayesian neural networks. The authors argue that standard image benchmark datasets such as CIFAR-10 are carefully curated, and that it is important to consider this curation as part of the Bayesian account of cold posteriors. They develop a simplified generative models describing dataset curation which assumes that a datapoint is included in the dataset only if there is agreement on the labels from multiple labellers. They show that toy data drawn from curation can give rise to optimal temperatures that are similar to the tempered likelihoods. They also show that the likelihood under their generative framework closely matches the tempered or cold likelihoods used in past work."
SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the speed-accuracy trade-off between autoregressive and non-autoregressive neural machine translation (NAR) models. The authors argue that the speed disadvantage of AR models has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. To address these issues, the authors propose a simple layer allocation strategy: deep encoder, shallow decoder, and empirically demonstrate that deep-shallow AR models run faster than NAR models."
SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper investigates epoch-wise double descent, i.e., the test error of a DNN also shows double descent as the number of training epoches increases. Specifically, the authors extend the bias-variance analysis to epoch-wise double descent and reveal that the variance also contributes the most to the zero-one loss. Inspired by this result, they propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) test error. Based on this metric, they further propose an approach to perform early stopping without any validation set."
SP:8d8b738c676938952e62a6b2aea42e79518ece06,"This paper studies the problem of adversarial robustness in MAML. The authors propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. They show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine tuning stage even if the latter uses a standard training protocol. Extensive experiments are provided to demonstrate the effectiveness of our approach."
SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the problem of tuning the step size for quadratic loss in the meta optimization setting. The authors show that the meta gradient can explode/vanish if the meta-objective is simply the loss of the last iteration. They also show that when the number of samples is small and the noise is large, train-by-validation approach generalizes better than train-based approach. Finally, they show that a similar phenomenon appears even for more complicated learned optimizers parametrized by neural networks."
SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a graph view-consistent learning network (GVCLN) for semi-supervised node classification, where the number of labeled samples is very small. The main idea is to use a dual-view structure to obtain different representations. Two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while a view consistency loss is applied to the two views to obtain the consistent representation and a pseudo-label loss is designed by using the common high-confidence predictions. Experiments on three citation network datasets of Cora, Citeseer, and PubMed show that the proposed method achieves state-of-the-art performance."
SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper proposes a method for learning the parameters of a Hamiltonian neural network (HNN) for predicting the dynamics of a dynamical system. The main idea is to use the cyclic coordinates of the system as the input to the HNN, which are obtained via canonical transformations. The authors show that these coordinates can be searched for automatically with appropriate loss functions which naturally arise from Hamiltonian dynamics. The proposed method is evaluated on both synthetic and real-world dynamical systems. "
SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes a novel graph neural network (GNN) architecture for learning on graphs with heterogeneous tabular node features. The authors propose a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with the heterogeneous features, while GNN accounts for the graph structure. The proposed model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN."
SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper studies the role of sample splitting in meta-learning in the asymptotic setting, where the number of tasks goes to infinity. The authors show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. They also show that if the data are generated from linear models (the realizable regime), both the splitting and non-Splitting methods converge to optimal prior. They validate their theories by experiments on both synthetic and real-world datasets."
SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard-confident examples from the noisy training data. The proposed method is built on the memorization effect of deep neural networks that they would first learn simple patterns, i.e., which are defined by these shared by multiple training examples. To extract hard confident examples that contain non-simple patterns and are entangled with the inaccurately labeled examples, the authors borrow the idea of momentum from physics. Specifically, they alternately update the confident examples and refine the classifier. The extracted confident examples in the previous round can be exploited to learn a better classifier and that the better classifiers will help identify better (and hard) examples."
SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the certified robustness of kNN and rNN against data poisoning attacks. It shows that the intrinsic majority vote mechanism in kNN provides certified accuracy guarantees against general poisoning attacks, and also provides a better lower bound of certified accuracy for rNN via jointly certifying multiple testing examples. Empirical evaluation results on MNIST and CIFAR-10 show that intrinsic certified accuracy is better than state-of-the-art certified defenses."
SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,This paper studies the problem of batch size selection for training graph neural networks (GNNs) with SGD. The authors propose a metric that combines both the variance of gradients and compute time for each mini-batch. They theoretically analyze how batch-size influence a metric and propose the formula to evaluate some rough range of optimal batch size. They also develop guidelines for the choice of batch sizes that depend on the average average degree and number of nodes of the graph.
SP:30d97322709cd292a49f936c767099f11b0e2913,"This paper proposes a method to calibrate the classifier’s inherent confidence indicators and estimate the uncertainty of the calibrated confidence scores using Gaussian Processes. The proposed method is evaluated on UCI datasets and compared with other confidence estimation methods. Experiments on a vision task with a large deep learning architecture further confirms that the method can scale up, and a case study involving out-of-distribution and adversarial samples shows potential of the proposed method to improve robustness of neural network classifiers."
SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to train an information retrieval module for downstream tasks, without using pairs of queries and documents as annotations. The approach is inspired by knowledge distillation, where the retriever module corresponds to the student model and the reader module is the teacher model. In particular, the authors use the cross-attention scores, from a sequenceto-sequence reader, to obtain synthetic targets for the retrieever. They compare different ways to aggregate the scores, as well as different training objectives, and show that iteratively training the reader and retriever leads to better performance on competitive question answering benchmarks."
SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper proposes a framework for specifying constraints in formal languages to enable the use of safety methods in reinforcement learning. Constraint states are instantiated as finite automata, which can be used to efficiently recognize constraint violations. The authors also propose a method for learning a dense cost function given a sparse cost function from joint MDP/constraint dynamics, and a method based on constraint structure to dynamically modify the set of available actions to guarantee the prevention of constraint violations in RL. The proposed framework is evaluated on the Safety Gym, MuJoCo, and Atari environments."
SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree approach to improve the comprehensibility of classifications. The main idea is to separate the notion of a decision path and an explanation path, and instead of having one monolithic decision tree, build several smaller decision subtrees and cascade them in sequence. This cascading approach is designed to specifically target explanations for positive classifications, and each subtree identifies the smallest set of features that can classify as many positive samples as possible, without misclassifying any negative samples. The authors evaluate their algorithm on standard datasets, as well as new real-world applications and find that their model shortens the explanation depth by over 40.8% for positive classification compared to the classic decision trees."
SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of the number of parameters on the performance of neural networks when the width of the network is increased. The authors compare different ways of increasing the network width while keeping the number parameters constant. They show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance. The number of weights is secondary, as long as the model achieves high training accuarcy."
SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module produces embeddings for entities in text while the language module generates context-aware initial embedding for entities and relations in the graph. Experiments on several knowledge-aware NLP tasks show that the proposed framework achieves superior performance."
SP:1db95a377f3d5ed129aa0511f840f647375e3528,This paper proposes an unsupervised learning method for inferring high-quality autoregressive orders in a data-driven way without a domain-specific prior. The proposed method is based on variational inference with the variational lower bound as a latent variable. The authors develop a practical algorithm for end-to-end optimization using policy gradients. The empirical results on sequence modeling tasks suggest that the proposed algorithm is capable of discovering various auto-regressive orders for different sequences that are competitive with or better than fixed orders.
SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction scheme that can accelerate any sampling method under the memory budget. The authors show that the induced variance can be decomposed into node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance (first-order variances) during backward propagation. They theoretically analyze the convergence of the proposed scheme and show that it enjoys an O(1/T) convergence rate. They complement their theoretical results by integrating the proposed schema in different sampling methods and applying them to different large real-world graphs.
SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a method for training conditional generators from a single training image based on thin-plate-spline augmentations. They demonstrate that the key for enabling single image training is extensive augmentation of the input image and provide a novel augmentation method. Extensive evaluations show remarkable visual performance, and the introduction of a new quantitative evaluation of manipulation."
SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes a method for maximum common subgraph (MCS) detection. The main idea is to replace the node selection heuristics used by state-of-the-art MCS solvers with a novel task-specific Deep Q-Network (DQN), allowing the search process to find larger common sub-graphs faster. To enhance the training of DQN, the authors leverage the search to provide supervision in a pre-training stage and guide the agent during an imitation learning stage. Experiments on synthetic and real-world large graph pairs demonstrate that the proposed method outperforms the state-ofthe-art methods."
SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes an end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The network takes as input an unordered set of 3D points sampled from the surface of some object, and outputs a sparse set of corner points linked by line segments. The architecture gradually builds up the model: It starts by encoding the points into feature vectors, identifies a pool of candidate vertices, then prunes those candidates to a final set of corners vertices and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe. All steps are trainable, and errors can be backpropagated through the entire sequence."
SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies the convergence rates of stochastic gradient methods in the setting where the gradient noise is non-stationary, i.e., the noise level varies with the number of iterations. This setting is motivated by empirical observations in neural network training, where the noise intensity varies with time. The authors show that SGD with a fixed step size can achieve the minimax optimal convergence rate for both convex and nonconvex optimization problems, provided the noise is uniformly bounded. They also show that under mild assumptions, one can achieve faster convergence than the fixed step SGD by a factor that is polynomial in iterations, by applying online noise estimation and using adaptive step sizes."
SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. The authors propose an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and efficient way. The method achieves BLEU improvements over a strong baseline model on English-Korean, English-to-German and English-Romanian translation tasks."
SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper proposes a benchmark for Reinforcement Learning (RL) algorithms with various dimensions of hardness that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. The authors consider and allow control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range. They define a parameterised collection of fast-to-run toy benchmarks in OpenAI Gym by varying these dimensions. Despite their toy nature and low compute requirements, these benchmarks present substantial challenges to current RL algorithms."
SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes a new method for regression calibration. The proposed method is based on the idea of quantile calibration (Kuleshov et al. 2018), which is an extension of entropy estimation to regression. The authors propose a new formulation of the quantile regularizer, which can be used as a blackbox to calibrate any probabilistic regression model. The method is trainable in an end-to-end fashion, without requiring an additional dataset. Experiments are conducted to show the effectiveness of the proposed method."
SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a deep generative probabilistic framework for learning representations of spatial environments, perceived through RGB-D and inertial sensors, such as in mobile robots, vehicles or drones. The authors propose to learn a dense 3D map and 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. This approach leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. The combination of variational inference, neural networks and a differentiable raycaster ensures that the proposed model is amenable to end-to-end gradient-based optimisation. The proposed spatial model features an expressive predictive distribution suitable for downstream control tasks, it is fully-differentiable and can be optimised with SGD."
SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for leveraging textual descriptions to improve generalization of control policies to new scenarios. The authors develop a new model, EMMA (Entity Mapper with Multi-modal Attention), which uses a multi- modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. EMMA is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision. To empirically test the model, the authors design a new framework of 1320 games and collect text manuals with free-form natural language via crowd-sourcing. They demonstrate that EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards compared to multiple baselines."
SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes a new actor-critic method for policy gradient reinforcement learning. The proposed method is based on a new state-value function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value. The authors also propose a new objective for the critic: minimizing the variance of residual errors. Theoretical consistency of the new gradient estimator is proved. Experiments show that the proposed method can improve the performance of policy gradient algorithms on a variety of continuous control tasks.
SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper studies the effect of depth on the generalization performance of deep neural networks in the over-parameterized regime. The authors introduce local and global labels as abstract but simple classification rules. They show that the locality of the relevant feature for a given classification rule plays a key role; their experimental results suggest that deeper is better for local labels, whereas shallower networks are better for global labels. They also compare the results of finite networks with those of the neural tangent kernel (NTK), which is equivalent to an infinitely wide network with proper initialization and an infinitesimal learning rate."
SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies representation learning for few-shot learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2(n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. First, they study the setting when this common representation is low-dimensional and provide a risk bound of $\tilde{O}(\delta^2 n1T + k n2)$ for the linear representation class; here d is the ambient input dimension and k(d) is the dimension of the representation. This result bypasses the i.i.d. task assumption and captures the desired property that all n1$ samples from source tasks can be pooled together for representation learning. They further extend this result to handle a general representation function class and obtain a similar result. Next, they consider the case where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, they again demonstrate the advantage of representation learning in both high dimensional linear regression and neural networks, and show that representation learning can fully utilize all $n1T$ samples of source tasks."
SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,This paper studies the robustness of deep neural networks (DNNs) to perturbations of input features. The authors propose a black-box approach to determine input features for which a network is robust or weak. They leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. They evaluate their approach with PCA features. Experimental results show that their provably-robust feature-guided neighborhoods are much larger than the standard provable robust neighborhoods.
SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task learning in reinforcement learning (RL) that automatically clusters tasks into related subsets. The proposed approach is inspired by the expectation-maximization algorithm, which finds clusters of related tasks and uses these to improve sample complexity by assigning each task to the best performing policy and then trains policies on their assigned tasks. This method is intuitive, simple to implement and orthogonal to other multi- task learning algorithms. The authors show the generality of their approach by evaluating on simple discrete and continuous control tasks, as well as complex bipedal walker tasks and Atari games."
SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper presents a self-supervised framework for learning generalizable representations for non-stationary time series. The proposed Temporal Neighborhood Coding (TNC) takes advantage of the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. The motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients’ latent states in settings where labeling data is practically impossible."
SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,This paper proposes a novel approach for learning modular networks that allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors propose to represent the entire model as a mixture of modules each of which can contribute to computation at any processing stage. The parameters of every block (or layer) of the network is computed as a linear combination of a set of “template” block parameters. This simple design can be utilized for a variety of applications from producing compact networks and training multi-task models capable of re-using individual modules to knowledge transfer and domain adaptation.
SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of posterior collapse in variational autoencoders (VAEs), which is the phenomenon that the learned latent space becomes uninformative in VAEs. The authors claim that this problem is related to local optima of the objective function that are often introduced by a fixed hyperparameter resembling the data variance. They suggest that this variance parameter regularizes the VAE and affects its smoothness, which can lead to posterior collapse. They propose AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameter, and thus avoids oversmoothing the model."
SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes an unsupervised VAE model that combines a clustering inducing mixture model prior in the local space with a global latent space with Gaussian prior and a more structured variational family. The authors show that the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). Second, the model performs domain alignment to find correlations and interpolate between different databases. Finally, the authors study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures."
SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"This paper proposes to use human interaction and attention cues to improve the performance of self-supervised representation learning. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives. The proposed method is evaluated on scene classification, action recognition, depth estimation, dynamics prediction, and walkable surface estimation. "
SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper proposes three interventions to remove the negative pretraining effect of sequential learning, where a pretrained model obtains a worse generalization performance than a model that is trained from scratch when either are trained on a target task. The proposed interventions are: (1) altering the learning rate after pretraining, (2) increasing the discretization of data distribution changes from start to target task, (3) resetting network biases to larger values, and (4) perturbing the biases of the model. The authors conduct experiments on CIFAR-10, CIFar-100, and ImageNet to demonstrate the effectiveness of the proposed interventions."
SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper proposes a new adversarial framework where the defender preemptively modifies classifier inputs. They introduce a novel optimization algorithm for finding safe spots in the vicinity of original inputs as well as a new network training method suited for enhancing preemptive robustness. The experiments show that our algorithm can find safe spots for robust classifiers on most of the correctly classified images. Further results show that they can be used to improve empirical and certified robustness on smooth classifiers.
SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to model raw point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequence. Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolutions is used to model the dynamics of the spatial regions along the time dimension. Furthermore, this paper also proposes a PST transposed convolution for point-level dense prediction tasks. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet."
SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes an adaptive text-to-speech (TTS) system for custom speech synthesis. The authors propose to model the acoustic information in both utterance and phoneme level. They use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning; in inference, they extract the utterance level vector from a reference speech and use an acoustic predictor to predict the phonemelevel vectors. To better trade off the adaptation parameters and voice quality, they introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, which fine-tune this part in addition to speaker embedding for adaptation."
SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the effect of different choices made during training of sparse neural networks (SNNs) on their performance. The authors propose a framework to compare the performance of sparse and dense neural networks in a controlled environment. They measure the gradient flow across different networks and datasets, and show that the default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based on these findings, they show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime."
SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. This paper studies the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where it analyzes how the feature of one node is spread over its neighbors; and (2) feature and label influence of how much the initial features/labels influence the final features/label of another node. Based on the theoretical analysis, the authors propose an end-to-end model that unifies GCN and LPA for node classification. In particular, edge weights are learnable, and the LPA serves as regularization to assist the GCN to learn proper edge weights that lead to improved classification performance. The model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models."
SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper considers the problem of generalization error bounds for supervised learning, where the ground truth labels are generated by a function in another function space. The authors show that the generalization gap depends on the R-Complexity of both the classifier and the generator function spaces. They also propose a joint entropy-like measure of complexity between function spaces (classifier and generator), called co-complexity, which leads to tighter bounds on the generalisation error in this setting. "
SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper studies the task of post-training quantization (PTQ), i.e., quantizing the weights of a neural network without end-to-end retraining. The authors propose a novel PTQ framework, BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. The main contributions of the paper are: (1) a comprehensive theoretical study of the second-order error, (2) a block reconstruction method, (3) a mixed precision technique, and (4) extensive experiments on various handcrafted and searched neural architectures."
SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the effect of dataset properties on the calibration of deep neural networks. The authors show that dataset properties, such as dataset size and label quality, have a significant impact on the accuracy and calibration of the network. They also show that the quality of the labels has a significant effect on the performance of the neural network, and that the network expressivity is affected by the size of the dataset and the number of parameters. The paper also shows that the calibration error increases with label noise and few samples."
SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper studies the problem of emergent communication in multi-agent settings. In particular, the authors focus on the setting of zero-shot coordination (ZS) coordination, in which agents learn to communicate via actuating their joints in a 3D environment. They show that under realistic assumptions, a non-uniform distribution of intents and a common knowledge energy cost, these agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice. They propose and evaluate initial training improvements to address these challenges."
SP:5ba686e2eef369fa49b10ba3f41f102740836859,"This paper proposes a meta-modeling framework for uncertainty quantification in sequential regression tasks. The proposed method is based on the idea of meta-learning, where the model is a combination of a base network and a meta network, and the meta network predicts the output of the base network. The meta network is trained by minimizing the KL-divergence between the predictions of the meta and base networks. The authors also propose a method for generating asymmetric and symmetric uncertainty bounds, and propose a new evaluation methodology for sequential regression. The experimental results show that the proposed method outperforms baselines on both drift and non-drift scenarios."
SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"This paper proposes two unbalanced Gromov-Wasserstein (UGW) divergences for the comparison of metric measure spaces. The first is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. The second is a distance between mm-spaces up to isometries based on the conic lifting. The authors provide a scalable, GPU-friendly algorithm to compute UGW and demonstrate its applicability in learning tasks."
SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a novel pruning method, called all-alive pruning (AAP), which aims to eliminate dead connections and make all weights in the subnetwork trainable. The main idea of the paper is that dead connections are the nodes that do not contribute to the model capacity. The authors propose a simple-yet-effective and versatile unstructured pruning algorithm, which is able to reduce the number of dead connections. The proposed method is applied to various saliency-based pruning methods and model architectures. The empirical results show that the proposed method consistently improves the accuracy of the original methods at 128x–4096x compression ratios."
SP:9eb7b946e00085b89844c485bcd94a392146d2b7,This paper proposes a latent-space-based approach for semantic image editing. The proposed approach is based on a generative adversarial network (GAN) that is trained to predict the attributes that should be manipulated in the latent space of the GAN. The paper also proposes a perceptual loss and an adversarial loss that is designed to preserve image identity and photo-realism. Experiments show that the proposed approach achieves state-of-the-art performance for targeted image manipulation.
SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"This paper proposes an adversarial training framework for discrete sequence generation, by leveraging the advance of Feature Statistics Alignment (FSA) paradigm to embed the latent feature representations in a finite feature space and force the distribution of generated samples to approach the real data by minimizing the distance between their respective feature representation centroids and the mean statistics of the fake data distribution. Experiments on synthetic and real benchmark datasets show the superior performance in quantitative evaluation and demonstrate the effectiveness of the proposed method."
SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes Spectral DQN, which decomposes the reward into frequencies that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. Experiments on two domains with extreme reward progressivity and six standard Atari games show that the proposed method is able to outperform existing value-based methods."
SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper introduces a notion of “usable information”, which measures the amount of information that can be extracted from the representation learned by a learned decoder. The authors use this notion to quantify how relevant and irrelevant information is represented across layers of the network throughout the training process, and how this is affected by the optimization algorithms and the network pretraining. They also evaluate how perturbing the initial part of training impacts the learning dynamics and the resulting representations. They find that training with SGD is critical to bias learning minimal representations in intermediate layers and that the content of the representation changes dynamically during training."
SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper studies the non-convex-strongly-concave min-max optimization problem. The authors propose a new algorithm called SREDA-Boost, which is an extension of the existing variance reduction algorithm (SREDA) proposed by Luo et al. (2020) that achieves the optimal complexity dependence on the required accuracy level. Theoretical convergence guarantees are established for the proposed algorithm, and it is shown that the convergence guarantee is better than the one of Luo (2020). The authors also propose a zeroth-order variant of the algorithm, which has a much larger step-size and less restrictive initialization requirement. The proposed algorithm is shown to run substantially faster in experiments."
SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of one-shot object detection, i.e. the task of detecting objects from a single image. The authors show that increasing the number of object categories used during training can improve the generalization from seen to unseen classes from 45% to 89% and improve the state-of-the-art on COCO by 5.4 %AP50 (from 22.0 to 27.5). The authors verify that the effect is caused by the number categories and not the amount of training samples, and that it holds for different models, backbones and datasets. "
SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a method for single-view implicit surface reconstruction from a single RGB image. The proposed method is based on the occupancy field and signed distance function (SDF) implicit neural shape functions. The authors propose a differentiable gradient sampling method to sample the spatial gradient of the implicit field and SDF from the feature map, which enables training on large-scale scenes without dense 3D supervision. The method is evaluated on ShapeNet and ScannetV2."
SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a training strategy called “Pseudo-to-real” for high-memory footprint-required large models. It is compatible with large models with architecture of sequential layers. The authors demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. They also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities."
SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper considers the problem of training energy-based generative models (EBMs), i.e. learning a Gibbs energy function $f$ that maximizes the likelihood of the target samples $x$ within a certain function class. The standard method to train EBMs is maximum likelihood estimation, where the learned energy is the one maximizing the likelihood $l_p(x)$ of a target sample $x$. This approach is challenging in generic settings where the trained energy is non-convex, due to the need to sample the Gibbs distribution associated with this energy. This paper proposes to use Fenchel duality to derive variational principles dual to maximum likelihood EBMs with shallow overparametrized neural network energies, both in the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. The authors also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies the lower bounds of DP-ERM in the unconstrained and constrained settings. In the constrained setting, the lower bound is $\tilde{O}(\sqrt{\sqrt{p}log(1/\delta) n)$, which is the well-known upper bound for approximate-DP ERM, and the upper bound is $O(L_0)$ for general convex functions. This paper improves upon this result by introducing a novel $L_2$ loss function instead of linear functions, and also introduces an auxiliary dimension to simplify the computation brought by the `2 loss. "
SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key of the proposed method is a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The proposed method covers all the classical Wasserststein gradient flows including the heat equation and the porous medium equation. The authors demonstrate the performance and scalability of the algorithm with several numerical examples."
SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper tackles the AutoML problem, which aims to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed meta-feature with the space of distributions on the hyperparameter configurations. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta features boosts the performance of state-of-the-art AutoML systems, such as AutoSkLearn and Probabilistic Matrix Factorization (Fusi et al. 2018). Furthermore, the inspection of MetaBu Meta-features gives some hints into when an ML algorithms does well."
SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a method for in-situ and on-demand model customization for heterogeneous federated learning. The proposed method splits the network into a set of base sub-networks of different sizes and robustness levels, which are later aggregated according to the inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate the effectiveness of the proposed method in adjusting model widths and adversarial robustness."
SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper proposes a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. The proposed algorithm is applicable to constrained and regularized problems and involves an adaptive stepsize allowing for potentially larger stepsizes. It also converges globally even in settings where the underlying operator exhibits limit cycles. Moreover, a variant with stochastic oracles is proposed."
SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies neural contextual bandits, a general class of contextual bandits where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a new neural contextual bandit algorithm called Neural-LinUCB, which uses the hidden layers of a deep ReLU neural network as a deep representation of the raw feature vectors and performs UCB type exploration on the last layer of the neural network. By incorporating techniques in liner contextual bandits and neural tangent kernels, the authors prove that the proposed algorithm achieves a sublinear regret when the width of the network is sufficiently large."
SP:a9a2c21110e00f19882d27bef0063c422a15e576,"This paper proposes a Shapley-inspired methodology for training action space categorization and ranking for reinforcement learning (RL). It categorizes the training action sets into dispensable and indispensable groups and ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The proposed data-driven methodology is extensible to different domains, use cases, and reinforcement learning algorithms. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study."
SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"This paper proposes an approach to construct probably approximately correct (PAC) prediction sets in the presence of covariate shift. The approach is based on importance weights that encode how the probabilities of the training examples change under the covariate shifts. The authors extend their algorithm to the setting where they are given confidence intervals for the importance weights. They demonstrate the effectiveness of their approach on two datasets, DomainNet and ImageNet. "
SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of iterative semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount of unlabelled data to progressively refine the model parameters. The authors derive an information-theoretic upper bound on the generalisation error for the binary Gaussian mixture model (bGMM) and show that when the class conditional variances are not too large, the upper bound decreases monotonically with the number of iterations, but quickly saturates. The theoretical results are corroborated by extensive experiments on several benchmark datasets as well as the MNIST and CIFAR datasets in which they notice that after several pseudo-labeling iterations, the generalizability improves after several iterations and then saturates afterwards."
SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method for model-free reinforcement learning. The main idea is to generate multi-step plans for temporally coordinated exploration towards high-value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Experiments are conducted on several benchmark environments and demonstrated its effectiveness compared with several baseline methods."
SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorizations to explore and exploit the full non-linearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix/tensor completion problems and prove that their methods have tighter generalization error bounds than conventional matrix/Tensor decomposition methods. The experiments on synthetic data and real datasets show that the proposed methods have much higher recovery accuracy than many baselines.
SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes a method to explain the behavior of structured output models, which learn mappings between an input vector to a set of output variables simultaneously. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. They introduce an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. The effectiveness of the proposed method is confirmed using a variety of simulated and real data sets."
SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a distributional policy gradient method for risk-sensitive reinforcement learning (DRL). The main idea is to use the cumulative distribution function (CDF) of the full-episode rewards as the objective function for the policy gradient. The authors propose a sampling-based method to estimate the CDF for a broad class of CDF-based objectives via sampling, and then incorporate variance reduction measures to facilitate effective on-policy learning. Experiments on six OpenAI Safety Gym environments show that the proposed method outperforms existing unconstrained and constrained methods."
SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes an active learning framework for simulating large-scale, spatiotemporal, age-structured epidemic models. The proposed method is based on the integration of neural process, deep sequence model, and active learning. In particular, the authors develop a spatio-temporal neural process model to mimic the simulator dynamics. The model automatically infers the latent process which describes the intrinsic uncertainty of the simulator. The authors design Bayesian active learning algorithms to iteratively query the simulator, gather more data, and continuously improve the model. Theoretical analysis and empirical results demonstrate the effectiveness of the proposed method."
SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"This paper proposes to improve the performance of DP-SGD for fine-tuning large pretrained language models by (1) using larger pretrained models; (2) choosing hyperparameters that suit DP optimization; and (3) fine tuning objectives aligned with the pretraining procedure. The authors also propose a memory saving technique, called ""ghost clipping"", that allows clipping to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead."
SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for optimizing the skeletal structure and joint attributes of a robotic agent. The proposed method, called Transform2Act, learns a conditional policy that first applies a sequence of transform actions to modify an agent’s skeletal structure, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods in terms of convergence speed and final performance."
SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a method to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX. The initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP."
SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method for learning object-centric representations of visual scenes without relying on annotations. The method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The proposed method is able to discover objects in a scene without annotations, and the inferred scene representations are interpretable and amenable to manipulations. Further, the scene representation is useful for visual reasoning downstream tasks such as the snitch localization task in CATER."
SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method for evaluating the subgraph explanations of GNNs. The authors argue that a distribution shift exists between the full graph and subgraphs, which causes the out-of-distribution problem. To address this problem, the authors propose Deconfounded Subgraph Evaluation (DSE) which assesses the causal effect of an explanatory subgraph on the model prediction. DSE leverages the front-door adjustment and introduces a surrogate variable of the sub-graphs. Specifically, it devise a generative model to generate plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance."
SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper studies the problem of active learning in the context of few-shot learning. In particular, the authors propose to use pretrained models to select informative examples for users to label in the presence of task ambiguity, i.e., the model is unsure of the task: is it to predict the shape or the color of the object? The authors show that pretraining enables models to disentangle and weigh various competing features, making them good active learners that can choose disambiguating examples (e.g. the blue square) that can resolve this task ambiguity."
SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper presents a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The authors propose a novel pre-training strategy for GRAPHIX, namely deleted sub-tree reconstruction, to enrich the model with implicit knowledge of program structures from unlabeled source code. The model is evaluated on the Patches in The Wild Java benchmark, using both abstract and concrete code. Experimental results show that the model significantly outperforms a wide range of baselines including CodeBERT and BART and is as competitive as state-of-the-art pretrained Transformer models despite using fewer parameters."
SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper studies the problem of Federated Adversarial Training (FAT) with Federated Learning (FL). The authors claim that the inner-maximization optimization can exacerbate the data heterogeneity among local clients, which triggers the pain points of FL. To address this problem, the authors propose a new learning framework based on a simple but effective reweighting mechanism, namely alpha-weighted weighting. The authors provide the theoretical analysis and empirical evidences to understand the proposed simple and effective method."
SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a framework for lifelong machine learning (LML) based on data programming. Specifically, the authors propose to use data programming to generate labels for the unlabeled data and then update the lifelong learner based on the labels generated by the labeler. The proposed framework, called Mako, can be applied to existing LML frameworks such as CNNL, ORDisCo and DistillMatch. The authors show that the proposed framework can achieve 97% performance of supervised learning on fully labeled data in terms of accuracy and catastrophic forgetting prevention."
SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes a method to generate adversarial examples that simultaneously satisfy two constraints: (1) misclassification and (2) non-adversarial detection. The proposed method is a modification of standard gradient descent (MMS+17) that optimizes the joint loss function f and the loss function g, instead of optimizing the joint losses f and g. This makes the proposed method both simpler and easier to analyze than prior attack approaches. The authors use their technique to evade four state-of-the-art detection defenses, reducing their accuracy to 0% while maintaining a 0% detection rate."
SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes a new energy-based treatment for cooperative games, with a theoretical justification by the maximum entropy principle. By conducting mean-field variational inference, the authors recover classical game-theoretic valuation criteria through conducting one-step fixed point iteration for maximizing the ELBO objective. This observation also verifies the rationality of existing criteria, as they are all attempting to decouple the correlations among players. The authors define the valuation with the best conceivable decoupling error as the Variational Index. "
SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty, i.e., the part of the out-of-sample prediction error that is due to the lack of knowledge of the learner. The proposed method, Direct Epistemic Uncertainty Prediction (DEUP), learns to predict generalization error and subtracts an estimate of aleatoric uncertainty (i.e. intrinsic unpredictability), which includes the effect of model bias (or misspecification) and is useful in interactive learning environments arising in active learning or reinforcement learning. Experiments on sequential model optimization and reinforcement learning tasks demonstrate its advantage against existing uncertainty estimation methods."
SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes a novel method called Givens coordinate descent (GCD) to iteratively learn a rotation matrix, in the context of end-to-end trainable PQ based embedding indexes. Based on geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n), the authors propose a family of block Given coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. Compared to the state-of-the-art SVD method, the proposed GCD algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to experimental studies. They further improve upon vanilla product quantization significantly in an end to-end training scenario."
SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices (RPMs), an abstract visual reasoning test of fluid intelligence. The framework uses (1) a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain and (2) a neural module-net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The approach is able to generalize systematically to novel target domains and exploit the process of extracting and mapping the relationship structure."
SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for learning representations of nucleotide-level genomic data. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force the framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the proposed method with self/semi-supervision settings outperforms state-of-the-art deep learning methods."
SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how our model parameters are fully steerable at inference time. They use a synthetic point set and real-world 3D skeleton data to show how the proposed spherical filter banks enable making equivariant and invariant class predictions for known point sets in unknown orientations."
SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,This paper compares the performance of 5 pretrained language models (PLMs) and 4 continual learning methods (CL methods) on 3 benchmark datasets in two typical incremental learning settings. The authors also conduct a layer-wise and task-wise analysis to dissect the performance characteristics of PLMs and CL methods. The paper is well-written and easy to follow. 
SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper proposes a defense against a directed deviation model poisoning attack in federated learning (FL). The attack is based on the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be diverted from the optima to increase the test error rate. The authors propose a stateful algorithm to allocate reputation scores to the participating clients to lower the contribution of maliciously behaving clients. They show that TESSERACT provides robustness against even a white-box version of the attack."
SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,This paper proposes a method for estimating the influence function of linear functionals of high-dimensional regression functions. The influence function is defined as the sum of two functions: the Riesz function and the regression function. The authors propose to use neural networks and random forests to perform debiasing to improve the performance of the estimator. The main contribution of the paper is that the authors propose a multi-tasking neural network debiased method. The proposed method is evaluated on two problems: average treatment effect and average marginal effect. 
SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. In this setting, the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. The authors propose a mini-batch Markovian sampled fully decentralized actor critic algorithm and analyze its finite-time convergence and sample complexity. The sample complexity matches that of the state-of-the-art single-agent actor critic algorithms for reinforcement learning."
SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper provides a theoretical understanding of how contrastive self-supervised learning (CL) works. The authors propose a new guarantee on the downstream performance without resorting to the conditional independence assumption that is widely adopted in previous work but hardly holds in practice. They show that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. They also show that their theory aligns well with existing contrastive methods."
SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. The authors also propose a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The proposed method is evaluated with additive noise, room reverberation, low-resolution, and clipping distortions. The evaluation results show that the proposed method achieves leading performance across the general GSR, speech denoising, speech de-verberation, speech declipping tasks."
SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a framework for multivariate time series forecasting based on the idea of low-rank approximation to model the variable space. In particular, the authors propose a tensor network based on low rank approximation and a series-variable encoder to improve the quality of the latent variable space representation. The authors also propose an N-order residual connection approach and couple it to the space-approximated tensor networks. Experimental results verify the effectiveness of the proposed method on four multivariate forecasting benchmark datasets."
SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"This paper proposes a variant of SCO algorithms with sparse moving averages for GNN training. By storing the moving averages in the most recent iterations, their algorithm only requires a fixed size buffer, regardless of the graph size. They show that their algorithm preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. The experiments validate their theoretical results and show that the algorithm outperforms the traditional Adam SGD."
SP:72e0cac289dce803582053614ec9ee93e783c838,This paper proposes Circulant MinHash (C-MinHash) which uses two independent random permutations in a circulant manner to approximate the Jaccard (resemblance) similarity in binary (0/1) data. Theoretical results show that the proposed method leads to uniformly smaller variance than that of the classical MinHash with K independent permutations. Experiments are conducted to show the effectiveness of the proposed methods.
SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training scheme for adversarial robustness against the union of lp-robust models. The proposed E-AT scheme is based on geometric considerations of the different Lp-balls and costs as much as normal adversarial learning against a single lp threat model. Moreover, it can be used to fine-tune with just 3 epochs to achieve multiple norm robustness and achieve state-of-the-art results on CIFAR-10 and ImageNet."
SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder, where the gate is to learn how to combine the public and private moders for the downstream public decoder. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed to allow the gate to choose either the public or the corresponding private moder. Moreover, a variant is proposed where all the gates are placed after the decoder of each task. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the uncomputable and inapproximate partition functions of energy-based sequence models (EBMs), which are parametric neural networks (NNs) with finite partition functions. In particular, the authors show that there are no good deterministic or randomized estimates of the partition functions, which makes the model selection difficult and undecidable. The authors also show that as EMs become more powerful, the partition function becomes uncomputable, and hence, model selection is generally impossible. As alternatives, they consider sequence model families whose partition functions are computable, but at the cost of reduced expressiveness."
SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), which is constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on these hypersurface would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The paper provides the condition under which the ASWD is a metric and show that this can be obtained by an injective neural network architecture. Numerical results demonstrate that the proposed ASWD significantly outperforms other Wassersteins variants for both synthetic and real-world problems."
SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a general framework for improving coordination and performance of multi-agent reinforcement learners (MARL). The framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator, that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. LIGS determines the best states to learn to add intrinsic rewards which leads to a highly efficient learning process and can be seamlessly integrated into existing MARL algorithms. Experiments show the superior performance of the proposed framework in Foraging and StarCraft II."
SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a method for multi-hop question answering (QA) that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. At each step, the model uses a query q to attend to information from a document, and combines this “retrieved” information with q to produce the next query. The proposed method is able to ""retrieve” either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. To enable this behavior, DOCHOPPER does not combine document information with text by concatenating text to the text of q, but by combining a compact neural representation of q with a compact representation of hierarchical part of document, which can potentially be quite large."
SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a new semi-supervised learning method for character representation learning for voice casting. The proposed method is based on the idea of label refining, i.e. extracting refined labels (i.e., vocal characteristics) from known initial labels (e.g. character played in a recording) and computing refined labels using a clustering algorithm to finally train a refined representation extractor. The method is validated on recordings from the MassEffect 3 video game."
SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks, allowing clients to learn multiple tasks with their private data. The key idea is to disentangle representation of local and non-local features using task-agnostic Vision Transformer and a task-specific head/tail. Each client learns a translation from its own task to a common representation, while the Transformer body learns global attention between the features embedded in the representation. The authors conduct experiments on four different image classification tasks, which shows the effectiveness of the proposed method."
SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper studies the problem of reward hacking in reinforcement learning, where RL agents exploit gaps in misspecified reward functions. The authors construct four RL environments with reward misspecifications and investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. They find that more capable agents often exploit the misspecification, achieving higher proxy reward and lower true reward than less capable agents. Moreover, they find instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. To address this, they propose an anomaly detection task for aberrant policies and offer several baseline detectors."
SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a f-divergence variational objective that generalizes the TVO by replacing the KL divergence with the arbitary differeitiable f divergence. The proposed f-TVO is derived from a deformed χ-geometry perspective, which is the deformed geodesic between the variational posterior distribution and the true posterior distribution. The authors propose a reparameterization trick and a Monte Carlo approximation to optimize the objective function. Experiments on VAE and Bayesian neural network show that the proposed f -TVO performs better than the cooresponding baseline f-VI."
SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper presents an empirical study of the interplay between different components of the deep reinforcement learning (RL) toolbox in the continuous control setting. The authors propose an ensemble deep deterministic policy gradient (ED2) algorithm for continuous control, which constructs an ensemble of streamlined versions of TD3 agents and achieves the state-of-the-art performance in OpenAI Gym MuJoCo. ED2 is conceptually straightforward, easy to code, and does not require knowledge outside of the existing RL toolbox."
SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of fair supervised learning under the Equalized Loss (EL) fairness notion, which requires the prediction error/loss to be the same across different demographic groups. By imposing EL constraint, the learning problem can be formulated as a non-convex optimization problem, and the authors introduce a number of algorithms that find the global optimal solution to this problem. Experiments on real-world data show the effectiveness of the proposed algorithms."
SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of semantic networks from the perspective of meaningful learning. The authors propose semantic linking to expose semantic relations between new and old concepts to models during training. They propose two data augmentation methods: inductive learning and deductive learning to align with the meaningful learning theory. Extensive empirical results on SCAN, GEO, and ADV illustrate that both prior knowledge and semantic linking are two essential factors in achieving one-shot generalization."
SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper presents a method for 3D shape representation learning using multi-scale wavelet decomposition. It decomposes 3D shapes into sub-bands components at multiple scales and all scales form a decomposition tree in a principled manner rooted in multi-resolution wavelet analysis. Specifically, it proposes Adaptive Wavelet Transformer Network (AWT-Net) that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-band components, using lifting scheme at multiple scale recursively and hierarchically. Then, it exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions."
SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes to combine the benefits of lightweight and full finetuning to achieve strong performance both in-distribution (ID) and out of distribution (OOD) in natural language generation tasks. The authors show that an ensemble of lightweight models and full models achieves the best of both worlds: performance matching the better of full and lightweight models, both ID and OOD. They also show that they can achieve similar improvements using a single model instead of two with their proposed cocktail finetuned, which augments full fine tuning via distillation from a lightweight model."
SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes an active learning approach to improve the performance of weakly supervised models via active learning. The proposed approach, called WARM (Active Refinement of Weakly Supervised Models), is an extension of data programming, a framework for learning from weak supervision. Data programming attempts to generate probabilistic training labels from simple yet imperfect heuristics (or labelling functions) obtained a priori from domain experts. WARM directs domain experts’ attention on a few selected data points that, when annotated, would most improve the label model’s accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling function in the weak supervision model. Experiments on multiple real-world medical classification datasets reveal that WARM can substantially improve the accuracy of probablistic labels used to train downstream classifiers, with as few as 30 queries to experts."
SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper considers the problem of training a classification model with group annotated training data. The authors propose a new and simple algorithm that explicitly encourages learning of features that are shared across various groups. Theoretically, the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. Empirically, it matches or achieves better performance compared to strong contemporary baselines including ERM and Group-DRO on standard benchmarks on both minority groups and across groups."
SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate bivariate removal-based explanation method that can capture feature interactions in black-box models, represented as a directed graph. The proposed method can identify directionally redundant features, i.e., features whose presence negates the influence of other features, and mutually redundant features. The authors also provide theoretical justification for these definitions in the context of SHAP, the Shapley value explanation map introduced by Lundberg & Lee (2017). Experiments on CIFAR10, IMDB, Census, Divorce, Drug, and gene data show the superiority of the proposed method against state-of-the-art methods."
SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a framework for interpretable policy learning that builds probabilistic tree policies determining physician actions based on patients’ observations and medical history. The proposed method is compatible with fully-offline and partially-observable clinical decision environments. The fullydifferentiable tree architectures are grown incrementally during optimization to adapt their complexity to the modelling task, and learn a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This policy learning method outperforms the state-of-the-art on real and synthetic medical datasets."
SP:5630707c9d0d9e21fce2efddef874e373bfed026,"This paper proposes Patch AutoAugment (PAA), a multi-agent reinforcement learning (MARL) approach for data augmentation. The main idea is to divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. Each agent learns an augmentation policy for each patch based on its content and semantics. The agents cooperate with each other to achieve the optimal effect of the entire image by sharing a team reward. Extensive experiments demonstrate that PAA outperforms the state-of-the-art DA methods while requiring fewer computational resources."
SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes a method for mitigating the adversarial vulnerability of deep neural networks (DNNs). The authors construct a causal graph to model the generation process of adversarial examples and define adversarial distribution to formalize the intuition of robustness to adversarial attacks. They show that the spurious correlation between labels and style variables is important for understanding and mitigating adversarial vulnerabilities. Based on this observation, the authors propose a method to align the natural and adversarial distributions by considering spurious correlations."
SP:9f09449a47464efb5458d0732df7664865558e6f,This paper proposes a method for memory-based continual learning. The authors propose to enforce a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms and then perform continual learning by simply swapping filter atoms for each task. The proposed method can be applied to a wide range of optimization schemes and CNN structures. The effectiveness of this simple scheme for continual learning is illustrated both empirically and theoretically.
SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse phenomenon of Stein Variational Gradient Descent (SVGD) in high-dimensions. The authors propose a connection between SVGD and a proposed MMD-descent algorithm, and show that SVGD suffers from the curse of dimensionality, i.e., SVGD underestimates the variance of the target distribution when the number of particles $n$ and the dimension $d$ diverge at the same rate. They also show that the bias from deterministic updates present in the “driving force” of SVGD can be removed, and empirically verify that removal of bias leads to more accurate variance estimation."
SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the interaction of adversarial training (AT) with noisy labels. In particular, the authors show that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Based on this insight, the paper proposes to adopt the number-of-PGD steps as a new criterion for sample selection to correct noisy labels and shows that AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training."
SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method allows us to provide formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The approach can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods."
SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) that learns representations from non-i.i.d sequential data from the ground up by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM, and demonstrate that HCM learns meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. The interpretability enables flexible transfer between environments that share partial representational structure."
SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems by reparametrizing the optimization variables as the output of a neural network. The authors show that to obtain the maximum speed up, the neural network architecture needs to be a specially designed graph convolutional network (GCN). The aggregation function of the GCN is constructed from the gradients of the loss function and reduces to the Hessian in early stages of the optimization. They show the utility of their method on two optimization problems: network synchronization and persistent homology optimization, and find an impressive speedup."
SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method based on a layered structure of Support Vector Machine (SVM) ensembles for non-parametric image classification. By utilizing the quick learning of SVMs compared to neural networks, the proposed method can reach higher accuracy than DCNNs when the training set is small. Experimental results show that while “conventional” DCNN architectures such as ResNet-50 outperform SVMnet when the size of the training data is large, SVMNet provides a much higher accuracy when the number of “ground truth” training samples is small, which is claimed to be the case in many real-world problems."
SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG) to reduce the communication and memory overhead. The proposed method first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation, so the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. A cascade of “exits” is attached to the model to make multiple predictions and direct further computation. To reduce total computation, and make full use of prior predictions, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. The authors evaluate their approach on semantic segmentation and human pose estimation."
SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes Neural Bootstrapping Attentive Neural Processes (NeuBANP), a bootstrapping neural process (NP) method for learning stochastic processes with neural networks. The authors propose to replace the Gaussian latent variable in the original (Attentive) Neural Process (ANP) with a set of random weights that are added to the encoder and the loss function. They also propose a generator function that produces a valid bootstrapped distribution without resampling, which can be considered as a learn-to-bootstrap method. They evaluate their models in benchmark experiments including Bayesian optimization and contextual multi-armed bandit. "
SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a multi-modal self-supervised pre-training framework for large-scale genome data. Specifically, the authors combine sequence-pre-training, region-matching, and sequence-region matching together to improve the generalizability of the model across different cell types for regulatory genome modeling. The authors pre-train their model on the ATAC-seq dataset with 17 million genome sequences. They evaluate their GeneBERT on regulatory downstream tasks, including promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction."
SP:841b12443d0274e34b78940f220b17d36798899b,This paper proposes a new method for detecting out-of-distribution (OOD) samples. The proposed method is based on the geodesic distance between the underlying data distributions and the learned features of a deep neural network (DNN). The authors propose a discriminator that is able to combine confidence scores from the logits outputs and the features of the DNN. The method is evaluated on a variety of network architectures and datasets. The results show that the proposed method outperforms the baseline methods.
SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper provides a generalization of Cover’s Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity-preserving transformations that constitute a group. The authors show that the fraction of separable dichotomyies is determined by the dimension of the space that is fixed by the group action. They show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. They also show how the capacity of induced representations, a standard tool in building GCNNs, influence capacity."
SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a method to explain the misclassification of a trained classifier in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). The method is based on two ideas: counterfactual explanations and concept activation vectors. The authors validate their approach on well-known pretrained models, showing that it explains the models’ mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample. "
SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a module named Mobile Attention Kernel Point Convolution (MAKPConv) to improve the efficiency and quality of kernel point convolution (KPconv) for 3D point cloud recognition. The proposed module employs a depthwise kernel to reduce resource consumption, and re-calibrates the contribution of kernel points towards each neighbor point via Neighbor-Kernel attention to improve representation power. In addition, the authors utilize Inverted Residual Bottleneck (IRB) to craft a design space and employ a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on the proposed MAKConv. Experiments on 3D classification and segmentation benchmarks verify the effectiveness of the proposed method."
SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper empirically shows that the problems of robust overfitting, robustness overestimation, and robustness-accuracy trade-off in adversarial training are related to the quality of the samples in the dataset. The authors propose a method to measure the data quality based on the learning behaviors of the data during adversarial learning and find that low-quality data may not be useful and even detrimental to the adversarial robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in adversarially training. They find that when low- quality data is removed, robust overfits and robusts overestimation can be largely alleviated; and robust accuracy trade-offs become less significant."
SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the number of neurons and training parameters that a neural network needs to approximate Korobov functions of bounded second mixed derivatives. The authors prove upper and lower bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. They also show that these bounds nearly match the minimal number of parameters any continuous function approximator needs for approximating the space of functions. This work contributes to understanding the practical success of neural networks theoretically."
SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the role of population size in language emergence in the Lewis Game. The authors argue that the current population-based models may actually be too simplistic as they assume homogeneous communities. They show that this simplification is a potential root cause of the experimental difference between neural observations and socio-linguistic literature. Namely, larger populations should lead to more stable and structured languages, which is not observed in recent emergent models. Yet, as soon as we add diversity within populations, this contradiction partially vanishes."
SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper proposes a polynomial graph filter-based GNN model for heterophilic graphs. It builds on the GPR-GNN model proposed by Chien et al. (2021) and proposes to learn multiple adaptive polynomials acting on different subsets of the spectrum. Theoretical and empirical results show that the proposed model learns a better filter, thereby improving classification accuracy."
SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper proposes a novel graph shortest distance embedding method called Betweenness Centrality-based Distance Resampling (BCDR) to improve the graph embedding for the shortest distance representation with two components. The first is betweenness centrality (BC)-based random walk to accommodate long-distance correlation on graphs by covering a wider range of nodes under the intrinsic graph metric. The second is a distance resampling strategy to preserve shortest distances during the mapping from graph to the embedding space via reconstructing a global distance matrix. The experimental evaluation indicates that BCDR can be integrated into graph-based learning models, which should improve their performance on graph structure recognition."
SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL), where the goal is to maintain the invariant world knowledge in the parameters of a pretrained language model (LM) as the world changes. The authors propose a new benchmark and metrics to quantify the retention of time-invariant knowledge, the update of outdated knowledge, and the acquisition of new knowledge. They also propose several baselines to create strong baselines. They show that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously."
SP:639fd88482330389019fb5be7446a909b99a8609,"This paper proposes a stochastic approach for the criterion minimization of decision trees. The splitting of a leaf node is cast as selecting the best feature and threshold that minimizes a desired criterion. Asymptotically, the proposed algorithm is faster than conventional exhaustive search by several orders of magnitude. It is also shown that the proposed approach minimizes an upper bound of the criterion."
SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper proposes a learning rate schedule for SGD on quadratic objectives with skewed Hessian spectrums, where the eigenvalue distribution of the underlying Hessian matrix is skewed. Theoretically, the paper shows that the proposed schedule can achieve minimax optimal convergence rates (up to a constant) when the Hessian distribution is skewed, which is quite common in practice. The paper also proposes two simple learning rate schedulers for practical applications that can approximate eigencurve. For some problems, the optimal shape of the proposed scheduling resembles that of cosine decay, which sheds light to the success of cosinine decay for some situations. For other situations, the proposed schedules are superior to cosine."
SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the offline model-based reinforcement learning (MBRL), where the goal is to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Offline MBRL typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. In this paper, the authors compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. They show that selecting these key hyperparameter using Bayesian optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance."
SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a method for prioritizing experiences in off-policy model-free reinforcement learning (MfRL) based on the temporal difference error (TD-error). The authors argue that the TD-error is one of the most frequently used features to compute the scores of experience replay in MfRL and critic networks often under/over-estimate Q-values, so it is often ineffective to learn to predict Q-value by sampled experiences based heavily on the TD error. The authors propose a new learnable features driven from components in model-based RL (MbRL) to calculate the scores on experiences. The proposed MaPER brings the effect of curriculum learning for predicting Q- values better by the critic network with negligible memory and computational overhead compared to the vanilla PER."
SP:0db83e057c21ac10fe91624876498d8456797492,"This paper proposes a method for learning from human-in-the-loop (HIPL) demonstrations, where a human expert can take over the control and demonstrate to the agent how to avoid probably dangerous situations or trivial behaviors. The proposed method extracts proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that HACO achieves a substantially high sample efficiency in the safe driving benchmark."
SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes a meta-learning approach for hierarchical imitation learning (HIL), where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta learning. Specifically, the authors propose to use the likelihood of state-action pairs from each sub-skill as the supervision for the high level network adaptation, and use the adapted highlevel network to determine different data set for each sub skill adaptation. The authors theoretically prove the convergence of the iterative training process of DMIL, and establish the connection between DMIL and the Expectation-Maximization algorithm. Empirically, they achieve state-of-the-art few-shot imitation learning performance on the meta-world benchmark and comparable results on the Kitchen environment."
SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a node embedding compression method to compress node embeddings for graph neural networks (GNNs). Specifically, the authors propose a bit vector representation for each node in the graph, which is based on a hashing-based coding scheme, which generates compositional codes for compactly representing nodes in graph datasets. The proposed method outperforms the prior embedding compressing method which uses a random coding scheme in almost all experiments."
SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper analyzes the adversarial domain adaptation (DAL) problem from a game-theoretical perspective. It shows that standard optimizers in DAL can violate the asymptotic guarantees of the gradient-play dynamics, requiring careful tuning and small learning rates. Based on this observation, it proposes to replace existing optimizers with higher-order ODE solvers, which are more stable and allow for higher learning rates, leading to noticeable improvements in terms of the transfer performance and the number of training iterations."
SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularizer, iFlood, which encourages the individual losses to approach a specified level. The authors claim that the design of the loss function of Flooding can lead to a discrepancy between its objective and implementation, and cause the instability issue. To resolve these issues, the authors propose a regularizer with instance-level constraints on the training loss. Experiments on both image classification and language understanding tasks confirm that models learned with iFlOOD can stably converge to solutions with better generalization ability and behave consistently at instance level."
SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a hierarchical reinforcement learning (HRL) approach for long-horizon tasks. The authors propose Value Function Spaces (VFS), a simple approach that produces a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that the approach improves long-term performance and enables better zero-shot generalization."
SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism which learns to select the most relevant points from a trainable reference set. It can replace i.i.d. generation in any VAE or GAN – it is easier to train and better captures complex dependencies in the data. The method can be used to generate any type of sets or graphs, it was built and benchmarked with molecule generation applications in mind."
SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, the authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. They establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. The authors also propose a modified version of the existing DRM and prove that PINN and the modified version can achieve minimax optimal bounds over Sobolev spaces."
SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper provides a theoretical analysis of the relationship between robustness and domain generalization. The authors propose a general theoretical framework indicating factors that can be reformed as a function class regularization process, which could lead to improvements of domain generalisation. The analysis, for the first time, shows that “robustness” is actually not the causation for domain generalizability; rather, robustness induced by adversarial training is a byproduct of function-class regularization. "
SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of memorization overfitting in gradient-based meta-learning from a causal perspective. The authors claim that the universal label space of the base learner turns to be a confounder causing a spurious correlation between the initializations learned in different steps of meta-training, which biases the meta-knowledge that should be only updated by the performance task-specific models. Based on this observation, the authors propose two simple yet effective deconfounder algorithms, i.e., sampling multiple versions of the meta knowledge via Dropout and grouping the knowledge into multiple bins. The proposed method outperforms the existing methods in four benchmark datasets."
SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a reinforcement learning approach for the problem of ad hoc teamwork, where agents need to collaborate with previously unknown teammates on the fly in an online setting. The proposed approach, called ODITS, learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, an information-based regularizer is introduced to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc collaborative tasks."
SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes an imputation method for missing data imputation. The imputation is performed via an online version of Expectation-Maximization (EM) algorithm by using a normalizing flow (NF) model which maps the data space to a latent space. The proposed EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results for high-dimensional multivariate and image datasets are presented to illustrate the superior performance of the EMFlow."
SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes deep linearly gated networks (DLGN) for DNNs with rectified linear units (ReLUs). The authors extend the recently developed dual view in which the computation is broken path-wise to show that learning in the gates is more crucial, and learning the weights given the gates are characterised analytically via the so called neural path kernel (NPK) which depends on inputs and gates. The authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. The DLGN disentangles the computations into two ‘mathematically’ interpretable linearities (i) the primal’ linearity between the input and the preactivations in the gating network and (ii) the ‘dual” linearity in the path space in the weights network characterised by the NTK."
SP:5676944f4983676b5ad843fdb190bf029ad647bb,This paper proposes a new normalization method for vision transformers. The authors claim that the ordinary layer normalization (LN) makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. The proposed Dynamic Token Normalization (DTN) is built on a unified formulation and thus can represent various existing normalization methods. Experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead.
SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper proposes two methods to measure spectral bias in modern image classification networks. The first method is based on the Fourier transform with respect to the input, and the second method uses the label noise procedure of Rahaman et al. (2019) to measure the function frequencies in multi-class classification models on high-accuracy CIFAR-10. The authors show that these networks indeed exhibit spectral bias, and that interventions that improve generalization sometimes increase and sometimes decrease the frequencies of the learned function. They also explore the connections between function frequency and image frequency and find that spectral bias is sensitive to the low frequencies prevalent in natural images."
SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper studies the question of when to switch between exploration and exploitation modes in reinforcement learning. The authors propose a set of algorithmic components to enable switching between exploration modes in RL. They also propose a method for analyzing exploration and switching at sub-episodic time-scales. Finally, they report a promising and detailed analysis on Atari. "
SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median clustering problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. The authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Theoretically, the authors show that the error of applying DP local search followed by our private HST center initialization improves previous results, and approaches the known lower bound within a small factor."
SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a new video prediction model, named FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and how it is mitigated using existing image augmentation techniques. As a result, the proposed model outperforms the current SOTA models across four different video prediction benchmarks on four different metrics."
SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the influence of data structure on the test loss dynamics of stochastic gradient descent (SGD) on the generalization performance of a machine learning algorithm such as a neural network depends in an intricate way on the structure of the data distribution. The authors study an exactly solveable model of SGD which predicts test loss when training on features with arbitrary covariance structure. They solve the theory exactly for both Gaussian features and arbitrary features and show that the simpler Gaussian model accurately predicts test error of nonlinear random-feature models and deep neural networks trained with SGD on real datasets such as MNIST and CIFAR-10. They also show that optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure, demonstrating the computational benefits of small batch sizes."
SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the non-convex non-linear optimization setting, where the loss landscape is nonlinear and the learning rate is fixed. In particular, the authors consider the setting where the noise is assumed to be Gaussian and time-independent. They show that SGD may not converge to local maxima, that it may only escape saddle points arbitrarily slowly, and that it might prefer sharp minima over flat ones. They also show that AMSGrad may converge to the local minimum. "
SP:22d01913b78ef447b064c65a646fa301b861d3f7,"This paper proposes a new method for hyperparameter optimization in meta-learning. The proposed method, HyperDistill, is based on approximating the second-order term with knowledge distillation. Specifically, the authors parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second order term. The method allows online optimization and also is scalable to the hyper parameter dimension and the horizon length. The authors demonstrate the effectiveness of the proposed method on two different meta learning methods and three benchmark datasets."
SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a Clustered TaskAware Meta-Learning (CTML) framework with task representation learned from both features and learning path. The authors first conduct rehearsed task learning from the common initialization, and collect a set of geometric quantities that adequately describes the learning path, by inputting this set of values into a meta path learner, they automatically abstract path representation optimized for the downstream clustering and modulation. To further save the computational cost incurred by the additional rehearsed learning, they devise a shortcut tunnel to directly map between the path and feature cluster assignments. Extensive experiments on two real-world application domains: few-shot image classification and cold-start recommendation demonstrate the superiority of CTML compared to state-of-the-art baselines."
SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"This paper proposes an ensemble-based approach for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near-OOD data. The proposed approach relies on regularization to promote diversity on the OOD data while preserving agreement on ID data. Extensive comparisons of the proposed approach to state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CifAR-100) and medical image datasets (Cifar-10, CIFAR100) reveal significant gains with negligible increase in computational cost."
SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,This paper proposes Latent Variable Sequential Set Transformer (AutoBots) to model time-evolution of sequential sets using discrete latent variables. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the time and social dimensions. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The model achieves top results on the global nuScenes vehicle motion prediction leaderboard and Argoverse vehicle prediction challenge.
SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,This paper presents a user study to evaluate the effectiveness of interpretable image classification models. The authors propose a synthetic dataset to measure how well users can identify the relevant set of attributes compared to the ground-truth. They also present two interpretable methods: concept-based explanations and counterfactual explanations. The results show that the baseline outperformed the two interpretability methods. 
SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper studies the problem of defending against backdoor data poisoning attacks on deep neural networks. The authors propose an iterative training procedure for removing poisoned data from the training set. They first train an ensemble of weak learners to automatically discover distinct subpopulations in the training data and then leverage a boosting framework to exclude the poisoned data and recover the clean data. The algorithm is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. Empirically, the method successfully defends against a state-of-the-art dirty label backdoor attack."
SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method for multi-label text classification (MLTC) that uses latent label representations to model label correlations implicitly. Specifically, the proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, and maps the contextual encodings of these latent labels to actual labels cooperatively. The correlations between labels, and between labels and the text are modeled indirectly through these latent-label embeddings and their correlations. The proposed method improves the state-of-the-art results on two widely used benchmark datasets by a large margin."
SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the inductive bias of deep convolutional kernels, i.e. the ability of such kernels to approximate functions that are well-suited for high-dimensional data. The authors propose to study the function spaces of these kernels by considering simple hierarchical kernels with two or three convolution and pooling layers. They show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents regularities."
SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of the Neural Tangent Kernel (NTK) in the infinite-width limit of neural networks. The NTK is defined as the outer product of the neural network Jacobians, and it is known to be computationally expensive. The authors propose two algorithms that change the exponent of the compute and memory requirements of the finite width NTK, and show that they can significantly improve the computational efficiency of the NTK. The main contributions of the paper are the following: 1. Theoretical analysis of the computation complexity of NTK in finite-width networks, and 2. Theorem 3.1 and 3.2 that shows that NTK-vector products are beneficial for networks with large outputs. 3. Theorems 4.2 and 4.3 that show that Structured derivatives can be used to reduce the time cost."
SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper considers the offline constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. The authors propose an algorithm, called COptiDICE, that optimizes the policy in the space of the stationary distribution. The main idea is to directly estimate the distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that the proposed algorithm achieves better trade-off between reward maximization and constraint satisfaction than several baselines."
SP:5a10c13eb78d26a25dac74601419deb68c53cb75,"This paper proposes a method for parallelizing the training of GRU networks. The proposed method is based on a multigrid reduction in time (MGRIT) solver, which partitions a sequence into multiple shorter sub-sequences and trains the sub-sequence on different processors in parallel. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach."
SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a manifold-regularized multi-decoder autoencoder (MRMD-AE) that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. The authors assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. They show that the learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, and improves cross-subject translation."
SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for large-scale multi-class, multi-label, and anomaly segmentation. It also introduces a new dataset of anomalous species. The authors also propose a simple detector based on the maximum logit (MaxLogit) that outperforms the maximum softmax probability (MSP) and other strong baselines. The proposed method is able to scale better than the MSP."
SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the structure of tournaments that arise out of fixed d dimensional representations. The authors show that these tournament classes have forbidden configurations which must necessarily be union of flip classes, a novel way to partition the set of all tournaments. They further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. This insight allows them to show that the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure."
SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a new regularization term for neural processes (NPs) that encourages the context embedding to be differentiated from the target dataset. The authors show that the proposed method can be used to improve the performance of NPs even under noisy data sets and restricted task distributions, where typical NPs suffer from a lack of context embeddings. They empirically show that their approach substantially outperforms conventional NPs in various domains. "
SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"This paper proposes a method to improve interpretability and explainability of transformer language models by incorporating prototype networks into the model architecture. The main idea is to use prototype networks to incorporate case-based reasoning to explain the reasoning process behind the network’s decisions. Moreover, the paper proposes an interactive prototype learning setup to incorporate human capabilities to incorporate knowledge outside of the rigid range of purely data-driven approaches. Experimental results show that the proposed method is on par with noninterpretable baselines."
SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. The authors introduce a notion of ‘trust region’ to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that it achieves significant improvement over related state-of-the-art methods."
SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a framework to connect optimization and generalization by analyzing the generalization error based on the length of optimization trajectory under the gradient flow algorithm after convergence. The authors show that, with a proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalization bound, showing that short optimization paths after convergence indicate good generalization. The framework can be applied to broad settings. For example, it can obtain generalization estimates on three distinct machine learning models: underdetermined lp linear regression, kernel regression, and overparameterized two-layer ReLU neural networks."
SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples. The authors show that adversarial attacks are neither in high-frequency nor in low-frequency components, but are simply dataset dependent. They also show that perturbations of the adversarial perturbation are dependent on the dataset, and propose and study the properties of training adversarial training using specific frequencies, which can be used to understand the accuracy-robustness trade-off."
SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f," of graph neural networks (GNNs) extend basic neural networks by using the graph structures based on the relational inductive bias (homophily assumption). In this paper, the authors first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, they propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNN. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, they find some cases of harmfulheterophily can be addressed by diversification operation. They propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophilies. They validate the ACM-augmented baselines with 10 realworld node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNN-based models."
SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,This paper proposes a deep reinforcement learning approach for solving the traveling salesman problem (TSP). The main idea is to use equivariance and local search to improve the generalizability of the RL solver. The proposed approach is evaluated on both random and realistic instances of the TSP and achieves state-of-the-art performance. 
SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a new data augmentation method to resolve the heterogeneous data distribution problem in federated learning, by introducing pretrained GANs to construct a differentially private global shared dataset. Specifically, each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct the global shared synthetic dataset. The PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold before each global aggregation. The combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation, according to the authors."
SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper studies the relationship between adversarial robustness and prediction confidence of smoothed classifiers. The authors propose to use the “accuracy under Gaussian noise” as an easy-to-compute proxy of adversarial strength for each input. They differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. The experiments show that the proposed method, despite its simplicity, exhibits improved certified robustness upon existing state-of-the-art training methods."
SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper proposes a packing approach to speed up BERT pretraining. The packing approach is based on the assumption that sequences are interchangeable and therefore packing can be performed on the histogram of sequence lengths, rather than per sample. This transformation of the problem leads to algorithms which are fast and have linear complexity in dataset size. The shortest-pack-first histogram-packing (SPFHP) algorithm determines the packing order for the Wikipedia dataset of over 16M sequences in 0.03 seconds. The non-negative least-squares histogram packing (NNLSHP) algorithms converges in 28.4 seconds but produces solutions which are more depth efficient, managing to get near optimal packing by combining a maximum of 3 sequences in one sample."
SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm (BATS) that is a deterministic variant of Monte Carlo tree search that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, our algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autorgressive models. Empirically, the authors show that BATS finds outputs with substantially better model scores compared to beam search and reranking techniques in models whose scores do not decompose additively with respect to the words in the output."
SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes an energy based model (EBM) for anomaly detection and localization that allows fast adaptation to new tasks. The EBM is an energy-based model with an adaptive sparse coding layer, of which the dictionary is directly formed by normal features of a target task. The authors adopted episodic meta-learning to extract common knowledge across tasks, which has the effect of enabling few shots adaptation. It’s worthy to note that when evaluating its performance on industrial inspection and video anomaly detection, the method is comparable and even boasts better performance than methods trained with a large amount of normal samples."
SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of question answering (QA) in the presence of both human-written and machine-generated misinformation. The authors create a large-scale dataset for this problem, CONTRAQA, which contains over 10K human- written and model-generated contradicting pairs of contexts. They show that QA models are vulnerable under contradicting contexts brought by misinformation. They also propose a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes a method for cross-domain imitation learning that uses the Gromov-Wasserstein distance (GWIL) to align and compare states between the different spaces of the agents. The paper provides theoretical analysis of the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. Experimental results show that GWIL learns optimal behaviors with a single demonstration from another domain without any proxy tasks in non-trivial continuous control settings."
SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a method to improve contrastive self-supervised learning (SSL) by distilling the information mismatched by the conventional contrastive loss. The proposed method uses a hierarchical projection head to project the raw representations of the backbone into multiple latent spaces and then compares latent features across different levels and different views. By cross-level contrastive learning, HCCL not only regulates invariant on multiple hidden levels but also crosses different levels, improving the generalization ability of the learned visual representations. The experimental results show the effectiveness of the proposed method."
SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a multi-agent deep reinforcement learning (RL) framework to learn Nash equilibria for dynamic general equilibrium models (DGE) with heterogeneous, interacting strategic agents of various agent types, such as consumers, firms, and governments. The authors show that RL can discover stable solutions that are Nash equilibrium for a meta-game over agent types in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training. The proposed approach is more flexible and does not need unrealistic assumptions, e.g., market clearing, that are commonly used for analytical tractability. The GPU implementation enables training and analyzing economies with a large number of agents within reasonable time frames."
SP:f885c992df9c685f806a653398736432ba38bd80,This paper proposes a method to prevent model stealing by requiring users to complete a proof-of-work before they can read the model’s predictions. The method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen. The key proposal is to wrap model outputs in a PoW problem to force users to expand some compute before they could read the desired output. This deters attackers by greatly increasing the computational effort needed to leverage query access for model extraction.
SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a multi-resolution approach to Continuous Normalizing Flows (MRCNF) for generative models of images. MRCNF is based on Neural Ordinary Differential Equations (ODEs) and proposes a Multi-Resolution variant of CNF by characterizing the conditional distribution over the additional information required to generate a fine image that is consistent with the coarse image. The authors introduce a transformation between resolutions that allows for no change in the log likelihood. They show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time."
SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposes a training-free method to detect noisy labels by using the neighborhood information defined by a good set of representations. The proposed method is based on the observation that good representations help define “neighbors” of each training instance, and closer instances are more likely to share the same clean label. The authors propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby representations, and the second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted, again using only representations. "
SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes a method for adversarial reinforcement learning (RL) under the worst-case adversarial perturbations on state observations (within some constraints). The key idea is to design an actor and a ""director"" that collaboratively finds the optimal state perturbation directions, and the actor learns to propose the best policy directions. The proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces."
SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper studies the problem of learning policies that can generate diverse and well-performing policies. The authors propose a novel policy seeking algorithm, Interior Policy Differentiation (IPD), which is derived from the interior point method commonly known in the constrained optimization literature. The proposed method is evaluated on a set of Mujoco environments, and it is shown to outperform existing methods."
SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper introduces a new task of audio-visual deverberation of audio, where the goal is to remove the reverberation effect from the audio signal from the visual observations. The authors propose a method that learns to remove reverberation based on both the observed sounds and visual scene. The method is evaluated on three tasks: speech enhancement, speech recognition, and speaker identification, and achieves state-of-the-art performance and substantially improves over traditional audio-only methods."
SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper studies the problem of extrapolation in transformer language models, i.e., the ability of the model to generalize to sequences that are longer than the ones it was trained on during training. The authors propose a simple and efficient position method, Attention with Linear Biases (ALiBi), which biases query-key attention scores with a penalty that is proportional to their distance. They show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences with length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory."
SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the problem of multi-objective online convex optimization in the unconstrained max-min form. The authors propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min-Norm solver. They derive regret bounds of both variants and show that the proposed algorithm enjoys a lower bound. Extensive experiments demonstrate the effectiveness of the proposed algorithms."
SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a generative replay approach for continual learning with generative models. Specifically, the authors propose to use the generated data as negative examples (or antagonists) to learn the new classes, especially when the learning experiences are small and contain examples of just one or a few classes. The proposed approach is validated on the Core50 and ImageNet-1000 datasets, which are composed of high-dimensional data and a large number of training experiences. The results show that using negative replay largely improves the classification performance w.r.t. the original data. "
SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,This paper proposes an inductive graph partitioning (IGP) framework across multiple associated graphs of a system or scenario to alleviate the NP-hard challenge. IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. The trained model is then generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines.
SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a multiresolution equivariant message passing graph generative autoencoder (MGVAE), which is a hierarchical generative model that learns to partition the graph into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. The proposed framework is end-to-end permutation equivariance with respect to node ordering. It achieves competitive results with several generative tasks including general graph generation, molecular generation, unsupervised molecular representation learning to predict molecular properties, link prediction on citation graphs and graph-based image generation."
SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper proposes a general framework for nonlinear independent component analysis (ICA), in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. The authors provide an insightful proof of the identifiability of the proposed framework. The experiments on artificial data and synthesized images indicate that the framework can disentangle interpretable features."
SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional network (GCN) based on the idea of subspace decomposition, which decomposes multi-channel signals into a collection of subspaces and shares adaptive filters to represent information in each subspace. The filters of all subspace differ in frequency response and together form a filter bank. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture."
SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor (LOO-kNN) to improve the downstream transferability of supervised pretraining methods. The authors claim that the problem of overfitting upstream tasks arise from the negligence of valuable intra-class semantic difference. To alleviate this problem, the authors propose a new method that only requires each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and preserving part of intra class difference for better transferring to downstream tasks. Extensive empirical studies on multiple downstream tasks show that look outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
SP:2b3916ba24094c286117126e11032820f8c7c50a,This paper proposes a method to generate facial details from a single image geometric facial details that are consistent with any desired target expression. The facial details are represented as a vertex displacement map and used then by a Neural Renderer to photo-realistically render novel images of any single image in any desired expression and view. The proposed method is trained using adversarial losses and weak supervision without any ground truth 3D data.
SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) to learn disentangled representations of syntactic roles (subjects, objects, etc). The model is trained on the SNLI dataset, and the authors show that it is possible to learn representations of sentences that exhibit separation in the realizations of these syntactic functions without supervision. The authors also develop an evaluation protocol to measure the disentanglement with regard to the syntactic realizations."
SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper proposes a novel multi-agent RL algorithm for achieving coordination through assessing the influence an agent has on other agents’ behaviors. The authors propose a novel framework, where at each timestep, an agent simulates counterfactual rollouts of its policy and, through a sequence of computations, assesses the gap between other agents' current behaviors and their targets. Actions that minimize the gap are considered highly influential and are rewarded. Additionally, the authors propose to learn an intrinsic reward for each agent to promote coordinated team exploration."
SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of large pre-trained neural networks. The proposed method, Model Editor Networks with Gradient Decomposition (MEND), learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre- trained model. The experiments show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters."
SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper introduces a physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in a compositional manner. Results on both one- and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper investigates the neural representations of code in the human brain. The authors analyze recordings from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They discover brain representations, in different and specific regions of the brain, that encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also map brain representations to representations of a suite of ML models that vary in their complexity. They find that the Multiple Demand system, a system of brain regions previously shown to respond to code, contains information about multiple specific code properties, as well as machine learned representations of programs."
SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper proposes Translatotron 2, a neural direct speech-to-speech translation model, which consists of a speech encoder, phoneme decoder, mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The authors also propose a new method for retaining the source speaker’s voice in the translated speech. Experiments on three different datasets, including multilingual S2ST, suggest that Translattron 2 outperforms the original Translatron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause."
SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper proposes few-shot attribute learning (FSAL) to learn new attributes that were not previously labeled in the dataset. The authors propose to use contrastive learning and self-supervised pre-training to learn the new attributes. They show that supervised learning with training attributes does not generalize well to new test attributes, whereas selfsupervised pretraining brings significant improvement. They also experiment with random splits of the attribute space and show that predictability of test attributes provides an informative estimate of a model's generalization ability."
SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle method for sampling from unnormalized distributions. The proposed method is based on the Wasserstein gradient flow of relative entropy, which is characterized by an ODE system with velocity fields depending on the density ratios of the density of evolving particles and the unnormalised target density. The authors propose a nonparametric approach to estimate the logarithmic density ratio using neural networks. Experiments on challenging multimodal 1D and 2D mixture distributions and Bayesian logistic regression on real datasets demonstrate that the REGS outperforms the state-of-the-art sampling methods."
SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a framework to classify larger, realistic images using quantum systems. The approach relies on a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. The framework is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop. The authors also propose a technique for further reducing the number of qubits needed to represent images that may result in an easier physical implementation at the expense of performance."
SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated learning (FL) approach for face recognition. The proposed approach is based on two components: (1) Differentially Private Local Clustering (DPLC) to distill sanitized clusters from local class centers; (2) Consensus-aware recognition loss to encourage global consensuses among clients, which results in more discriminative features. Experimental results show that the proposed approach can improve the performance of TAR@FAR on a large-scale dataset."
SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes a method to improve linear probing (LINEAR) for transfer learning by selecting features from all layers of the source model to train a classification head for the target-domain. The proposed method, called head-to-toe probing (HEAD2TOE), selects the most relevant features among a pretrained network’s intermediate representations. The authors show that doing so greatly improves performance over LINEAR and allows the approach to reach a performance competitive with—and in some cases superior to—finetuning. The paper also shows that for out-of-distribution transfer, it outperforms fine-tuning."
SP:d6f11fb32851f97af287f962f83220d27a8bc76a,This paper proposes an object-oriented text dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains. The OOTD model learns a graph representation for capturing object dynamics and predicts the belief of object states with independent transition layers. Experiments show that the proposed model outperforms model-free baselines in terms of sample efficiency and running scores.
SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification, where a label taxonomy has a cost associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. The authors propose a tractable method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. They employ the distributionally robust learning framework to solve the learning to abstain problems in each layer. They conduct experiments on large-scale bird dataset and on cell classification problems. The results demonstrate that LAM achieves a lower hierarchical cost sensitive loss in high accuracy regions."
SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,This paper proposes to learn a group of parameterized synperiodic filter banks to process multi-channel sound waveforms. The proposed method is based on a Transformer-like backbone with two parallel soft-stitched branches to learn semantic identity label and spatial location representation semi-independently. Experiments on both direction of arrival estimation task and physical location estimation task shows that the proposed method outperforms existing methods by a large margin.
SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of domain adaptation in the setting where the goal is to gradually shift the model towards the target distribution rather than learning domain invariant representations. The authors propose a method that creates virtual samples from intermediate distributions by interpolating representations of examples from the source and target domains. The proposed method, called Gradual Interpolation of Features toward Target (GIFT), is designed to deal with cases where there is a gap between the data distributions in the source domain and the target domain. The experiments show that in the presence of (i) access to intermediate distributions and (ii) samples being annotated with the amount of change, iterative self-training can be successfully applied on gradually shifted samples to adapt the model toward the target data distribution. In the case where either of the two assumptions does not hold, the authors propose to use the proposed method."
SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for video-text retrieval that learns representations from videos, titles and comments. The authors propose a context adapter module that is able to extract information from auxiliary input sources for learning a joint, multimodal embedding. They introduce an attention-based mechanism that allows the model to disregard text with irrelevant content. They demonstrate that by using comments, the proposed method can learn better, more contextualised, representations, while also achieving competitive results on standard benchmarks."
SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes an exploration bonus for unsupervised skill discovery, which is motivated by the fact that the discriminator may not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To address this issue, the authors propose an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. The authors demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels)."
SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a framework for generating molecules using deep neural networks (DNNs). The authors propose a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning trees and the residual edges. Based on the intermediate graph structure of the construction process, the proposed framework can constrain its generation to molecular graphs that satisfy the chemical valence rules. The authors also design a Transformer architecture with Tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed method."
SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper provides a spectral analysis of polynomial neural networks (PNNs) based on the Neural Tangent Kernel (NTK) of PNNs. The authors show that the $\�$-Net family, i.e., a recently proposed parametrization of the PNN, speeds up the learning of the higher frequencies. They also verify the theoretical bias through extensive experiments. "
SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method for training sparse neural networks (NNs) at their random initializations without changing the weight magnitudes. The authors define a class of subnetworks in randomly initialized NNs called “disguised subnetwork”, which are not only “hidden” in the random networks but also can only be “unmasked” with certain transformations on weights. They propose a two-stage algorithm that plays a Peek-a-Boo (PaB) game to identify the disguised subnetwork with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork's remaining weights. Extensive experiments on CIFAR-10/100 datasets demonstrate the competency of the proposed method over edge-popup and other counterparts."
SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a general unified framework to enhance the robustness of GNNs against adversarial attacks by jointly cleaning the perturbed graph and denoising the features of data. It extends this framework by reconstructing the graph and making convolution operations of features with intrinsic properties, and propose a robust GNN model RGUGNN. Experiments on four real-world datasets demonstrate that R-GUGNN has greatly improved the overall robustness over the state-of-the-art baselines."
SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,This paper proposes a method to learn texture mapping for a 3D surface and apply it to document image unwarping. The proposed method learns a continuous bijective mapping between 3d surface positions and 2D texture-space coordinates. The surface parameterization network can be plugged into a differentiable rendering pipeline and trained using multi-view images and rendering loss. Experiments show that the proposed method can reconstruct high-frequency textures for arbitrary document shapes in both synthetic and real scenarios.
SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes Adaptive Region Pooling (ARP), a novel downsampling algorithm for fine-grained recognition. The authors claim that ARP makes the network only focus on a smaller but more critical region, and simultaneously increase the resolution of sub-sampled feature. ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. Extensive experiments qualitatively and quantitatively validate the effectiveness and efficiency of the proposed pooling operation."
SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper studies the problem of out-of-distribution (OOD) generalization for node-level prediction on graphs and develops a new domain-invariant learning approach, named Explore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage invariant graph features for prediction. The key difference to existing invariant models is that they design multiple context explorers (specified as graph editers in the case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. This design enables the model to extrapolate from a single observed environment which is the common case. The authors prove the validity of their method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution."
SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning approach that adaptively selects optimal data augmentation strategies for contrastive representation learning for time series data. The proposed approach is based on mutual information neural estimation and cross-entropy estimation. Experiments on various datasets show that the proposed InfoTS achieves state-of-the-art performance."
SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the Lottery ticket hypothesis (LTH) for deep neural networks (DNNs), which proposes that DNNs contain sparse subnetworks that can be trained in isolation and can reach performance that is equal to, or better than, that of the full DNN in the same number of training iterations (Frankle & Carbin, 2019; Frankle et al., 2020a). These subnetwork are called winning lottery tickets. In recent years, researchers have found an intriguing corollary: winning tickets found in the context of one task can be transferred to related tasks. However, to date, there exists no principled understanding of why winning tickets can be transferable between tasks, nor does there exist any way to know, without directly performing transfer experiments, which previously studied tasks a given winning ticket can be successfully transferred to. To address these outstanding open questions, the authors make use of renormalization group theory, one of the most successful tools in theoretical physics, and show that iterative magnitude pruning, the method used for discovering winning tickets, is a Renormalization Group scheme. This opens the door to a wealth of existing numerical and theoretical tools, some of which are leveraged here to examine winning ticket universality in large scale lottery ticket experiments, and sheds new light on the success iterative magnitudes pruning has found generally in machine learning."
SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the performance gap between cross-attention (CA) and dual-encoder (DE) models for neural re-ranking. The authors show theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. The gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, the authors propose a distillation strategy that focuses on preserving the ordering among documents, and confirm its efficacy on benchmark neural reranking datasets."
SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the role of importance sampling (IS) as an active tool for variance minimization in the context of policy optimization. In particular, the authors propose a new algorithm, Policy Optimization via Optimal Policy Evaluation (POPE), that uses importance sampling as an inner loop to optimize the performance of the target policy. Theoretical results show that the proposed algorithm is able to guarantee the performance improvement for a monotonic transformation of the original objective and eventually converge, at least, to a stationary point. Finally, empirical evaluations on continuous RL benchmarks demonstrate the effectiveness of the proposed method."
SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes a graph parallelism method to scale up training of graph neural networks (GNNs) for modeling atomic simulations. The proposed method splits the input graphs across multiple GPUs, enabling to train very large GNNs with hundreds of millions or billions of parameters. The authors empirically evaluate the method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 15% on the force MAE metric on the S2EF task and 21% on IS2RS task, establishing new state of the art results."
SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes Time Control, a language model that implicitly plans via a latent stochastic process. The proposed model learns a representation which maps the dynamics of how text changes in a document to the dynamics in a Brownian process of interest. The model can generate text by first implicitly generating a document plan via the Brownian processes, and then generate text that is consistent with this latent plan. The authors show that Time Control improves performance on text infilling and discourse coherence."
SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for object-centric representation learning (OCRL) by learning to predict future scenes in the presence of moving objects. The authors treat objects as latent causes whose function to an agent is to facilitate efficient prediction of the coherent motion of their parts in visual input. The model learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. Further, the network learns a latent code space where objects with the same geometric shape and texture/color frequently group together."
SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based architecture for learning disentangled representation from real spatio-temporal data for mobility forecasting. The proposed method learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. The experimental evaluation results show that the proposed method can achieve state-of-the-art performance and is able to extract desirable spatial and temporal features.
SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"This paper proposes a variational autoencoder (VAE) model for interpolating irregularly sampled time series. The proposed model consists of an input sparsity-aware encoder, parallel deterministic and probabilistic pathways for propagating input uncertainty to the output, and a heteroscedastic output distribution to represent variable uncertainty in the output interpolations. The authors also propose an augmented training objective to combat the presence of additional local optima that arise from the use of the output structure. The experimental results show that the proposed model significantly improves uncertainty quantification and significantly improved log likelihood scores."
SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two metrics to measure the modularity of a neural network, i.e., importance and coherence. Importance is defined as how crucial sets of neurons are to network performance; coherence reflects how consistently their neurons associate with features of the inputs. To measure these proxies, the authors develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. They apply the proxies to partitionings generated by spectrally clustering a graph representation of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings reveal groups of neurons that are important and coherent. These results suggest that graph-based partitioning can reveal modularity and help us understand how deep neural networks function."
SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. They obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structure sparsity with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models, showing the extra bonus of noise robustness."
SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones entirely. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and one entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. Extensive experiments demonstrate that ZerO achieves state-of-the-art performance over various image classification datasets."
SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. The authors propose the Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Theoretical analysis and the generalizability of the robustness gained by solving minimax on clean data to unseen test data are provided.
SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the question of whether random permutations are optimal in the context of permutation-based SGD. The authors show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist permutations that offer exponentially faster convergence compared to random. They also show that there exist easy-to-construct permutations for quadratic strongly-convex functions that lead to accelerated convergence."
SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes an automated normalizing flow (NF) architecture search method. The method aims to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficient optimization of deep flow models."
SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes a self-supervised learning (SSL) method that aims to maximize the expressiveness and learnability of the learned representations. The expressiveness is measured by the Intrinsic Dimension (ID) of the dataset in representation space. The learnability is defined in terms of the learning speed of a KNN classifier trained to predict K-means cluster labels for held-out representations. This paper collects 30 state-of-the-art checkpoints and shows that ID and CL can be combined to predict downstream classification performance better than the existing techniques based on contrastive losses or pretext tasks, while having no requirements on data augmentation, model architecture or human labels. To further demonstrate the utility of the framework, the authors propose modifying DeepCluster (Caron et al., 2018) to improve the learnability."
SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box adversarial attacks to improve the imperceptibility of adversarial examples, whose perturbations are limited in salient regions. The proposed approach is based on a salient object segmentation model to produce saliency maps with no need for any information other than the input image. Experiments show that the proposed approach can achieve better imperceptible performance. Furthermore, the authors devise a new gradient-free black box Saliency attack that further enhances the imperceptionibility via refining in salient region."
SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a novel Transformer-like architecture for molecule-to-molecule (M2M) translation and retrosynthesis tasks. The proposed Graph2MILES model is based on the SMILES language model, which is used to represent the molecular structures. The authors claim that the permutation invariance of molecular graph encoders mitigates the need for input data augmentation, and can be used as a drop-in replacement for the Transformer in any task involving molecule(s)-to-Molecule(s) transformations. In the encoder, an attention-augmented directed message passing neural network (D-MPNN) captures local chemical environments, and the global attention encoder allows for long-range and intermolecular interactions, enhanced by graph-aware positional embedding. Graph2SMILES improves the top-1 accuracy of Transformer baselines."
SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical reinforcement learning approach for disease diagnosis in task-oriented dialogues setting. The high level policy consists of a master model that is responsible for triggering a low level model, while the low level policy is composed of several symptom checkers and a disease classifier. The proposed approach is evaluated on both self-constructed real-world and synthetic datasets. The results show that the proposed approach achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a self-supervised federated learning framework, named (SSFL), and a series of algorithms under this framework which work towards addressing two challenges: data heterogeneity and label deficiency at the edge. In particular, the authors propose a new personalized federated self supervised learning algorithm, called Per-SSFL, which balances personalization and consensus by carefully regulating the distance between the local and global representations of data. The authors also develop a distributed training system and related evaluation protocol for SSFL. The experimental results demonstrate that SSFL can work reliably and achieves reasonable evaluation accuracy."
SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper attempts to derive a general form of PDEs for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show that the adjustment operator for ResNet like DNN is the solution operator of the PDE. Based on this, they design a training method motivated by PDE theory to train DNN models for better robustness and less chance of overfitting."
SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the emergence of emergent languages in referential games, where agents interact and develop an emergent language to solve a task. The authors propose a hypothesis about the factors that determine the expressivity of languages, which reflects the amount of information about input spaces those languages are capable of encoding. They measure the expressiveness of languages based on their generalization performance across different games, and demonstrate that there is a trade-off between the complexity and the unpredictability of the context. They also show that using the contrastive loss proposed by Chen et al. (2020a) can alleviate the problem of message type collapse."
SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes two exploration strategies based on sample average uncertainty (SAU) to address the exploration-exploitation dilemma in reinforcement learning. The first strategy, called $\delta_2$-exploration, extends SAU from bandit problems to the general sequential RL setting. The second strategy, named \tilde_2$, is a drop-in replacement for greedy exploration in Q-learning and DQN. The proposed algorithms are simple and scalable, and can be implemented through a minimal API that is the same as for -greedy exploration."
SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes an implicit neural representation (INR)-based generative adversarial network (GAN) for video generation. In particular, the authors propose a dynamics-aware implicit generative neural network (DIGAN) that incorporates the temporal dynamics of videos into the generator and discriminator. The proposed method is evaluated on UCF-101 and CIFAR-10 datasets, and the results show that the proposed method outperforms the baselines."
SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of multi-label multi-winner voting, i.e., the task of voting on a set of k-hot binary vectors that satisfy a bounded differential privacy guarantee. The authors propose three new mechanisms: Binary, $\tilde{\phi}$ and $\phi$-voting, which operate independently per label through composition, Binary voting, which operates over the entire binary vector by viewing the possible outcomes as a power set, and Powerset voting, where each label is casted as a single candidate and the outcome is revealed as the sum of the results of all the other labels. They theoretically analyze tradeoffs showing that powerset voting requires strong correlations between labels to outperform Binary voting and Binary voting is optimal when there is a lack of correlation among the outcomes of particular candidates. They also derive tighter data-independent privacy bounds for $\phi_2$ voting by considering the situation where each voter is limited in their votes for candidates, which they call $\phi_{\tilde}$ voting. They use these mechanisms to enable privacy-preserving learning by extending the canonical single-label technique: PATE."
SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a technique of learning rate grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of optimizer hyperparameter search. Besides, the invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning."
SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper studies the problem of RL in sparse reward settings, where the reward function can only indicate whether the task is completed partially or fully. The authors propose an algorithm called Learning Online with Guidance Offline (LOGO) that combines a policy improvement step with an additional policy guidance step by using the offline demonstration data. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimal behavior policy, while being able to learn beyond and approach optimality. Theoretical analysis of the proposed algorithm is provided, and a lower bound on the performance improvement in each learning episode is also provided. Empirical results show that the proposed method achieves superior performance over state-of-the-art approaches on a number of benchmark environments."
SP:cf857736e3dc01325948488c791cbafc24b1c0fe,This paper proposes a two-stage method to extract the Pareto optimal solution set for multi-task learning (MTL) problems with non-convex functions and constraints. The main idea is to use a weak and a low-cost Paredto filter to extract an optimal set from the weak set. The proposed method is based on the Fritz-John conditions (FJC) as the discriminator and the diffusive manifold is used to bound the error between the true and the Stage-1 extracted weak set and Stage-2 extracted strong set. Numerical experiments demonstrate the accuracy and efficiency of the proposed method.
SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper studies the problem of learning a consolidated image feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks. This differs from traditional knowledge distillation in which a student model is trained to emulate the input/output functionality of a teacher. The authors show that traditional distillation can result in a representation suboptimal for downstream task transfer learning, because it only focuses on preserving the end-to-end input-output mapping of the old task. They propose representation consolidation with the generalist model as an additional teacher. Their method preserves the wide-range transferability of the strong ImageNet baseline and improve the performance for both related and unrelated downstream tasks over traditionally distilled networks."
SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. The authors provide theoretical guarantees for its coverage and efficiency in certain situations, and develop a gradient-based practical implementation which performs well empirically on several large-scale tasks."
SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. The proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of the proposed method."
SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes QuadTree Attention, a transformer-based attention method for image classification, feature matching, stereo, and object detection. The proposed method builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patch. Experiments show that the proposed method achieves state-of-the-art performance in various vision tasks, e.g. with 2.7% improvement in feature matching on ScanNet."
SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper considers the problem of learning reusable temporally extended actions, or options, in reinforcement learning. Motivated by the recent success of mutual information (MI) based skill learning, the authors propose a method for learning termination conditions of options by maximizing MI between options and corresponding state transitions. They derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards, combined with intrinsic rewards."
SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a semantic topology approach for open-world object detection. The proposed approach assigns all object instances from the same category to their corresponding pre-defined nodes in the semantic space, including the ‘unknown’ category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that the proposed approach outperforms the current state-of-the-art open world object detectors."
SP:97f618558f4add834e5930fd177f012a753247dc,"This paper studies the problem of selecting a subset of the unlabeled dataset with a small labeled (seed) dataset and an annotation budget to select the subset that, when annotated, will achieve the best performance. The authors propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. They show that the proposed algorithm can identify informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset."
SP:e0432ff922708c6c6e59124d27c1386605930346,This paper proposes an adaptive inference strategy for semantic segmentation that adjusts the model to the test sample before producing the final prediction. They achieve this by using Instance-adaptive Batch Normalization (IaBN) to modify normalization layers by combining the feature statistics acquired at training time with those of the test samples. They also introduce a test-time training (Seg-TTT) approach that adapts the model parameters using a self-supervised loss. Experimental results show that these techniques consistently and significantly outperform the baseline and attain a new state of the art.
SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper considers the task of finding out-of-distribution samples in tabular data, where little can be assumed on the structure of the data. The proposed method learns neural mappings that maximize the mutual information between each sample and the part that is masked out. The method assumes that it is possible to identify missing features based on the rest and employs a contrastive loss for learning without any other auxiliary loss. In an extensive set of experiments, the method presents a significant advantage over existing anomaly detection methods, and is stable with respect to its hyperparameters."
SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a method to learn low-dimensional embeddings of neural correlates that preserve the diagnostic attributes of neuropsychiatric disorders. The authors propose a conditional variational auto-encoder (VAE) that incorporates dual utilisation of diagnostic information. The proposed method is evaluated on two empirical datasets and discovers a reliable and consistent nosological relation among ASD, MDD, and SCZ."
SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper introduces a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture is composed of a parametric tensor-train network (TTN) for feature extraction and a tensor product encoding (TPE) for generating quantum embeddings. The authors theoretically characterize the QTN by analyzing its representation power of input features and show that QTN enables an end-to-end parametric model pipeline, namely QTN-VQCs, from the generation of quantum embedings to the output measurement. The experiments on the MNIST dataset demonstrate the advantages of QTN for quantum embeddedding over other quantum learning approaches."
SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method for constructing low-dimensional manifolds of the space of neural networks, where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The proposed method, called DYNAMO, takes as input a collection of pre-trained neural networks and outputs a meta-model that emulates the dynamics of the hidden states as well as the outputs of any model in the collection. The specific model to be emulated is determined by a model embedding vector that is taken as input. The authors apply the proposed method to both RNNs and CNNs, and find that the resulting model embeddings enable novel applications: clustering of neural network on the basis of their high- level computational processes in a manner that is less sensitive to reparameterization; model averaging of several neural networks trained on the same task to arrive at a new, operable neural network with similar task performance; and semi-supervised learning via optimization on the model space."
SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a framework for constraint-based learned simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The authors implement their method using a graph neural network as the constraint function and gradient descent as a constraint solver. They test the model on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids."
SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes EDO-CS, a new method for finding a set of high-quality policies with both high quality and diversity in reinforcement learning. The proposed method is based on clustering-based selection, where the policies are divided into several clusters based on their behaviors, and a high quality policy is selected from each cluster for reproduction. Experiments on various continuous control tasks show the superior performance of the proposed method over previous methods."
SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper considers a distributed linear stochastic approximation algorithm with Markovian noise and general consensus-type interaction. The interconnection structure among the agents is described by a time-varying directed graph. The paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. The equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication. "
SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes Graph Mechanics Network (GMN) that is equivariant, constraint-aware, and constraint-satisfied. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. The authors also develop a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed equivariance formulation is proved to be universally expressive under certain conditions."
SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposes two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. The authors give convergence analyses of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalization with a small fraction of personalized parameters, and the alternating update algorithm often outperforms the simultaneous update algorithm."
SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a framework for unsupervised learning of object representations, called Contrastive Learning Through Time (CLTT), which simulates viewing sequences as they might be experienced by an infant while interacting with objects and avoids arbitrary augmentation operations. Instead, positive pairs are formed by successive views in unsegmented viewing sequences. This allows perfect control over the temporal structure of the input and allows us to ask the following two questions. First, can CLTT approach the performance of fully supervised learning? Second, if so, what are the required conditions on the temporal structures of input? To answer these questions, they develop a new data set using a near-photorealistic training environment based on ThreeDWorld (TDW). They consider several state-of-the-art contrastive learning methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. They also consider the effect of one object being seen systematically before or after another object, and show that this leads to increased representational similarity between these objects, reminiscent of classic neurobiological findings."
SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes a method for goal-directed planning, which extends AlphaZero with Hindsight Experience Replay (HER) to tackle the sparse reward problem. The method is based on the Monte Carlo Tree Search (MCTS) algorithm, which is an extension of the AlphaZero algorithm that uses function approximators to bias the search of the tree. This paper proposes to use the policy and value networks of AlphaZero to improve the performance of MCTS. The proposed method is evaluated on several domains, including a quantum-computation domain, where it outperforms AlphaZero."
SP:e2d33c7331db7f52b84ad1018152564d91a9f126,This paper proposes Recursive Gradient Optimization (RGO) for continual learning. RGO is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer (FEL) that represents different network structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines.
SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper studies the relationship between StyleGAN2 and aligned models, where the parent and child models share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. The authors empirically analyze aligned models and provide answers to important questions regarding their nature. They find that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Then, equipped with this better understanding, they leverage aligned models to solve a diverse set of tasks such as image-to-image translation, cross-domain image morphing, zero-shot classification and regression."
SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a new Gromov-Wasserstein (GW) divergence between two graphs based on Optimal Transport (OT). The main idea is to relax the conservation of mass constraint, which imposes a coupling between all the nodes from the two considered graphs, and relax it by proposing a new semi-relaxed GW (SRGW). Theoretical properties of SRGW are discussed, and it is shown that it can lead to an efficient graph dictionary learning algorithm. Experiments on graph partitioning, clustering and completion are conducted to demonstrate the effectiveness of the proposed method."
SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a new reward-conditioned human model to model systematic suboptimality, i.e. systematic deviation from optimal behavior. The authors introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models with far less data."
SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free post-training quantization (DFQ) method called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. The authors decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, they propose a novel optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short), which does not need any dataset and is even not aware of network architecture. They also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver."
SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a method for time series segmentation, i.e., splitting time series into segments that correspond to given categories. The authors propose a bi-pass architecture that combines LSTM and very deep CNN-based 1D-encoder-decoder, several multi-scale structures, depthwise separable and atrous convolution, and a stepwise segmentation module. The proposed method is evaluated on two time-series datasets with fast changing and slow changing labels. The results show that SegTime can outperform the baselines."
SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a method to explain GNNs by decomposing the information generation and aggregation mechanisms. The authors propose concrete decomposition schemes for commonly used layers and operations in GNN. They also propose an algorithm to provide subgraph-level explanation via agglomeration, which efficiently employs the topological information in graphs. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes DiffStride, a new method for learning the stride of pooling and striding layers in convolutional neural networks. The proposed method is based on the idea of learning the size of a cropping mask in the Fourier domain, which can be used as a differentiable parameter for the stride. The authors also propose a regularization term to control the computational complexity of the architecture. Experiments on audio and image classification tasks show the effectiveness of the proposed method."
SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local models, where each prediction is associated with a small receptive field. The proposed method is based on localized randomized smoothing, i.e. randomly smoothing different outputs using different non-i.i.d. smoothing distributions matching the model’s locality. Experiments on both image segmentation and node classification tasks demonstrate that localized smoothing can offer a better robustness-accuracy trade-off than existing random smoothing techniques."
SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain knowledge into normalizing flows. The method is based on the idea of embedding a probabilistic model into a bijective transformation, which is then converted into a normalizing flow. The paper also introduces gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. Experiments show that the proposed method can induce desirable properties such as multimodality, hierarchical coupling, and continuity."
SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the extent to which large pre-trained language models (LMs) can be taught to map previously learned word forms onto conceptual world representations. Specifically, the authors test whether the LMs can learn to map an entire conceptual domain (e.g., direction or color) onto a grounded world representation given only a small number of examples. They evaluate a range of generative language models of varying sizes and show that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalize to several instances of unseen concepts as well."
SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the effect of shaped rewards on emergent language in the context of reinforcement learning. In particular, the authors show that shaped rewards can change the distribution of an emergent property of the language (i.e., entropy) and mask the emergent effects of other environmental variables. The authors use a simple sender-receiver navigation game to demonstrate how shaped rewards are able to explicitly bias the semantics of the learned language, significantly change the entropy of the learnt language, and can mask the potential effects of environmental variables of interest."
SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. IWDA is based on the observation that representation invariance of the feature representations strongly correlates with transfer performance, and distributional shift in class priors between data in the source and target languages negatively affects performance. Experiment results demonstrate its superiority under large prior shifts. The method delivers further performance gains when combined with existing semi-supervised learning techniques."
SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a meta-learning algorithm that first bootstraps a target from the metalearner, then optimizes the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. The authors establish conditions that guarantee performance improvements and show that the metric can control meta-optimization. They achieve a new state-of-the-art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta learning. Finally, they explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an $\epsilon$-greedy Q-learning agent."
SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of model-based RL agents in comparison to their model-free counterparts. Specifically, the authors evaluate the performance of MuZero on both procedural and task generalization. They identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, they achieve state-of-the-art generalization performance and data efficiency on Procgen. However, they find that these factors do not always provide the same benefits for task generalisation in Meta-World, indicating that transfer remains a challenge. Overall, they suggest that building generalizable agents requires moving beyond the single-task, single-model-free paradigm and towards self supervised models."
SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph convolutional neural network (GCN) based approach for inferring the structure of latent graphs from data. The proposed approach is based on the idea of convolution between the observed and latent graphs, and formulate the graph learning task as a network inverse (deconvolution) problem. In particular, the proposed method unrolls and truncates proximal gradient iterations to arrive at a parameterized neural network architecture that the authors call Graph deconvolution network (GDN). GDNs can learn a distribution of graphs in a supervised fashion, and perform link prediction or edge-weight regression tasks by adapting the loss function. The authors claim that GDNs are inherently inductive and generalize to larger-sized graphs after training."
SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,This paper proposes a framework for reward shaping in reinforcement learning. The authors propose a two-player Markov decision process (MGMDP) that involves two independent learners with distinct objectives: a reward-shaping agent (Shaper) and a controller (controller). The controller learns the optimal policy while the Shaper learns to select which states to add shaping rewards and their optimal values. Theoretical results show that the proposed framework is guaranteed to preserve the underlying learning task for the controller while guiding Controller to higher performance policies. Empirical results demonstrate the effectiveness of the proposed approach.
SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper proposes a novel approach to improve the robustness of vertical federated learning (VFL) against backdoor attacks and adversarial attacks. In particular, the authors propose a novel robust feature subspace recovery (RVFR) framework to recover the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. The authors conduct extensive experiments on NUS-WIDE and CIFAR-10 datasets and show that RVFR outperforms different baselines in terms of robustness against diverse types of attacks."
SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"This paper investigates the use of contrastive learning to train unsupervised dense retrievers for document retrieval. The authors propose to use the inverse Cloze task (ICT) as a pretext task to pretrain a dense retriever, and then fine-tune it on the MS MARCO dataset before fine-tuning on the BEIR dataset. They show that the proposed method outperforms BM25 on 8 out of 14 datasets. They also show that when a few thousands examples are available, the proposed approach is able to outperform BM25."
SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the trade-off between fine-tuning and linear probing for out-of-distribution (OOD) transfer. The authors theoretically analyze the tradeoff between the two methods and empirically show that linear probing performs better than fine tuning in ID and OOD settings, especially when the pretrained features are good and distribution shift is large. Further, the authors show that the simple two-step strategy of linear probing then full finetuning combines the benefits of both methods and achieves better ID/OOD accuracy than fine-tuneing. "
SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning to discover novel classes (L2DNC), where a clustering model is trained on labeled data from seen classes and unlabeled data from unseen classes, and then we train clustering models for the unseen classes. However, the implicit assumptions behind the problem are still unclear, and this paper aims to demystify the assumptions behind L2DN. The authors claim that high-level semantic features should be shared among the seen and unseen classes and prove that the problem is theoretically solvable under certain assumptions and can be naturally linked to meta-learning that has exactly the same assumption as L2NDC. Then, this paper proposes to solve the L2DNCL problem by meta learning algorithms with slight modifications. The proposed meta learning-based methodology significantly reduces the amount of unlabeling data needed for training and makes it more practical, as demonstrated in experiments."
SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper studies the problem of combining offline and online data in the Partially-Observable Markov Decision Process (POMDP) setting. In this setting, the learning agent has the ability to collect online experiences through direct interactions with the environment (interventional data) but also has access to a large collection of offline experiences obtained from observing another agent (observational data). The authors propose a general yet simple methodology for safely leveraging offline data during learning. In a nutshell, their method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard POMDP transition model via deconfounding using the recovered latent variable. They prove their method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and they assess its effectiveness empirically on a series of synthetic toy problems."
SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper proposes a new approach for open-ended text generation tasks, where a retriever is used to retrieve relevant passages from a textual knowledge corpus (e.g., Wikipedia) and provide these passages as additional context to the generator. The authors propose to use an additional guide retriever that is allowed to use the target output and “in hindsight” retrieve the relevant passages during training. The guide guide is trained jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over the posterior distribution Q of passages given the input and the output. The approach is evaluated on the Wizard of Wikipedia dataset, and it is shown that the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passages, and the end-to-end system produces better overall output."
SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper proposes a graph neural network (GNN) based approach for influence estimation and influence maximization. The main idea is to use a GNN to parameterize an upper bound of influence estimation, and train it on small simulated graphs and then apply it to real-world graph optimization problems. The authors show that the proposed approach is able to outperform the state-of-the-art in terms of accuracy and time complexity. "
SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper studies the problem of active learning for general loss functions under domain shift. The authors propose to use the discrepancy distance between source and target distributions to restrict the maximization over the hypothesis class to a localized class of functions which are performing accurate labeling on the source domain. They derive generalization error bounds for active learning strategies in terms of Rademacher average and localized discrepancy for general losses which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds. Numerical experiments show that the proposed algorithm is competitive against other state-of-the-art active learning techniques in the context of domain adaptation.
SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks by leveraging insights from singular learning theory. Namely, for large n, the posterior distribution over neural network weights is not Gaussian but rather can be put into a mixture of standard forms. From this, the generalized gamma mean-field family, following desingularization, can in theory achieve the leading order term of the log normalized evidence."
SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper studies the problem of domain generalization (DG), i.e., generalizing a model trained on multiple known data distributions to generalize well on unseen data distributions. The authors present a learning-theoretic generalization bound for DG that bounds novel DG performance in terms of the model’s Rademacher complexity. They conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. The analysis suggests that domain generalisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective."
SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper introduces the maximum n-times coverage problem, which is a generalization of the multi-set multi-cover problem and is NP-complete, and is not submodular. The authors also introduce the min-cost n- times coverage problem where the objective is to select the minimum set of overlays that the sum of the weights of the elements that are covered at least n times is at least $\tau$. The authors introduce two new practical solutions for the max-n-times problem, NTIMES-ILP and MARGINALGREEDY, which are based on integer linear programming (ILP) and sequential greedy optimization (SGO). The authors show that the proposed methods can be used to solve the max n-time coverage problem on both synthetic data and real vaccine design, and show that it produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage."
SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"This paper proposes a weight initialization method for spiking neural networks (SNNs) that is consistent with the response of spiking neurons in initial training. The authors first derive an asymptotic formula for their response curve, which approximates the actual neuron response distribution. Then, they propose an initialization method based on the slant-asymptote to overcome gradient vanishing. Experiments with different coding schemes in classification tasks on the MNIST and CIFAR-10 dataset show that the proposed method can improve the training speed and the model accuracy compared with traditional deep learning initialization methods and existing SNN initialization methods."
SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes FedTEM, a federated expectation-maximization algorithm for dealing with the distribution shift problem in FL. The authors propose to use a mixture of distributions model to model the shift in the client data distribution, which gradually changes between daytime and nighttime modes, and find this intuitive model to better match the observations in practical federated learning systems. They propose a Federated Expectation-Maximization (FEM) algorithm enhanced by Temporal priors of the shifting distribution (FedTEM), which jointly learns a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments for image classification on EMNIST and CIFAR datasets and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of the distribution shifts and significantly improve the final model performance."
SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a method for pruning deep neural networks (DNs) based on the spline interpretation of DNs. The authors show that a DN’s spline mappings exhibit an early-bird (EB) phenomenon, whereby the splines’ partition converges at early training stages, bridging the recently developed DN spline theory and lottery ticket hypothesis. They leverage this insight to develop a principled and efficient pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate that their new spline-based DN pruning approach reduces training FLOPs by up to 3.5x while achieving similar or better accuracy than current state-of-the-art methods."
SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper considers the problem of fair representation learning, where the goal is to ensure that the optimal predictors on top of the data representation are invariant with respect to different subgroups. The authors formulate the problem as a bi-level optimization, where representation is learned in the outer-level and invariant optimal group predictors are updated in the inner-level. They propose an implicit path alignment algorithm, which only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. Moreover, the proposed method is demonstrated to fulfill the sufficiency rule, which is desirable in various practical scenarios but was not commonly studied in the literature."
SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper provides a detailed empirical study of reinforcement learning via supervised learning (RvS) methods, which solve RL problems via conditional imitation learning (CBI). The authors show that RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data. The most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). The experiments find that more complex design choices, including the large sequence models and value-based weighting schemes used in prior work, are often not necessary."
SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a computational model that predicts human behavior by leveraging Bayesian inference about the structure of unobserved space (i.e., map induction). The authors hypothesize that humans explore new environments by inferring the structure in unobserved spaces re-using spatial information collected from previously explored spaces. They model this cognitive process computationally through “Map Induction”, which involves the compositional formation of proposed maps of complex spaces based on already-seen spaces through program induction in a Hierarchical Bayesian framework. The model thus explicitly reasons about unseen spaces through a distribution of strong spatial priors. They introduce a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models as well as our Map induction framework. "
SP:cccdcc95c4177b5531bad23b662060fdd0d88849,This paper proposes a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. The authors replace each crafted factor with a neural network enabling the factors to be learned using an efficient optimization routine from labeled data. The method learns to maintain a set of marginal posterior samples using end-to-end training. The experiments demonstrate the effectiveness of using learned factors for tracking and suggest the practical advantage over hand-crafted approaches.
SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based generative model for molecule generation. The proposed model is able to support scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. The experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from."
SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper proposes a new method for imitation learning, which is based on the reduction of imitation learning to reinforcement learning with a stationary reward. The authors prove that for deterministic experts, imitation learning can be performed with a single invocation of an RL solver. Theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments on a family of continuous control tasks show that the proposed method achieves competitive performance while being simple to implement."
SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper provides a theoretical analysis of the effect of reweighting algorithms on the worst-group performance of overparameterized neural networks. Specifically, the authors prove that for almost all reweighted algorithms, the model always converges to the same ERM interpolator that fits all training samples, and consequently its worst group test performance will drop to the level of ERM in the long run. Then, they analyze whether adding regularization helps fix the issue, and show that for regularization to work, it must be large enough to prevent the model from achieving small training error."
SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes a framework for multi-agent reinforcement learning (MARL) based on the Rational Inattention (RI) model, which models the cost of cognitive information processing using mutual information. The proposed RIRL framework generalizes and is more flexible than prior work by allowing multi-timestep dynamics and information channels with heterogeneous processing costs. The authors evaluate the proposed method in two Principal-Agent (specifically manager-employee relations) problem settings of varying complexity where RI models information asymmetry (e.g. it may be costly for the manager to observe certain information about the employees). The authors show that using RirL yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions."
SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a phase-oriented audio adversarial attack that exploits the influence of phase spectrogram. The authors leverage spectrogram consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbations to adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram, and propose a weighted loss function to improve the imperceptibility of the attack. Experiments on the LibriSpeech dataset show that the proposed method achieves a 6.64x generation speed-up over current state-of-the-art imperceptible counterparts."
SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper studies the representation learning in non-contrastive self-supervised learning (SSL). In particular, the authors analyze a generalized version of the DirectPred algorithm proposed by Tian et al. (2021) that sets the predictor directly. They show that in a simple linear network, DirectSet(alpha) provably learns a desirable projection matrix and also reduces the sample complexity on downstream tasks. Their analysis suggests that weight decay acts as an implicit threshold that discard the features with high variance under augmentation, and keep the feature with low variance. Inspired by their analysis, they simplify DirectPred by removing the expensive eigen-decomposition step. They also propose a simpler and more efficient algorithm DirectCopy that achieves comparable or better performance than the original DirectPred."
SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a gradient-based method for learning long-term dependencies in sequential data. The proposed method is based on a system of multiscale ordinary differential equations and a suitable time-discretization of this system. The authors prove that the proposed method mitigates the exploding and vanishing gradients problem, which is a well-known challenge for gradient based recurrent sequential learning methods. They also prove that LEM can approximate a large class of dynamical systems to high accuracy."
SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes geometric deep learning models that are rotation and permutation-equivariant. The proposed models are composed of a set of products of terms from the geometric algebra and reductions over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. The authors demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology."
SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem, which is a combinatorial optimization problem for real-time decisions in online retailing. The proposed method is based on the edge-feature-embedded graph attention, which considers the high-dimensional edge features and accounts for the heterogeneous information, which are important characteristics of the studied optimization problem. The model is also size-invariant for problem instances of any scale. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality."
SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper proposes a framework for dialogue summarization based on the concept of concepts out of the dialogue context (CODC) in the dialog summarization task. The proposed framework consists of a CODC inference module leveraging external knowledge from WordNet and a knowledge attention module aggregating the inferred knowledge into a neural summarization model. The authors also propose a new evaluation metric based on COCO to evaluate the inference capability of different methods. Experiments suggest that current automatic evaluation metrics of natural language generation may not be enough to understand the quality of out-of-context inference in generation results, and the proposed model can provide statistically significant improvements over both CIDEr and traditional automatic metrics. Human evaluation of the model’s inference ability demonstrates the superiority of the proposed models."
SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the question of what kinds of semantic dependencies can be modelled in entity embeddings. Specifically, the authors focus on settings where the embedding of an entity is obtained by pooling the embeddeddings of its known attributes. They first show a number of negative results, revealing that some of the most popular embedding models are not able to capture even basic Horn rules. However, they also find that some embedding strategies are capable, in principle, of modelling both monotonic and non-monotonic attribute dependencies."
SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of Transformer-based language models on various natural language processing tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning. The authors find that when the models are explicitly given all the information required to perform deductive reasoning, they can easily learn logical reasoning, but when this information is stated only implicitly in the text or in the supervision, the models struggle. They also find that the strength of transformer-based models comes from simple patterns in the training data, combined with background knowledge from the pretraining."
SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper proposes a framework for learning compositional problem graphs, which are a set of problems with shared subproblems that can be decomposed into sub-problems of different complexity. The authors propose an evaluation scheme to measure how readily old knowledge can be reused and hence built upon. They also propose a curriculum that encourages the learner to build off old solutions to solve new problems by re-representing the new problem into one it knows how to solve, rather than learning from scratch. They show on a symbolic and a high-dimensional domain that their compositional approach can generalize to more complex problems than the learners has previously encountered."
SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IP), which combines activation pruning and weight pruning to improve the execution efficiency of deep neural networks. The idea is to learn the importance of different neuron responses and connections, which balances the sparsity between activations and weights and improves execution efficiency. Experiments are conducted on MNIST, CIFAR-10 and ImageNet datasets to demonstrate the effectiveness of the proposed method."
SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a new method for feature selection based on generative adversarial networks. The proposed method is built on top of the KnockoffGAN framework, which is the seminal paper on feature selection. In the original paper, the relationship between the original label and the features is constrained to be of a very specific form; in this paper, they remove this constraint and instead provide a theoretical analysis that shifts the burden of knowledge onto the underlying feature distribution. They adapt the Generative Adversarial Networks framework to allow them to generate knockoffs that are conditionally independent of the label given the real features. They demonstrate the capability of their model to perform feature selection, showing that it performs as well as the originally proposed knockoff generation model in the Gaussian setting and that it outperforms the original model in non-Gaussian settings, including on a real-world dataset."
SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes a new Winograd-based pruning method, spatial-Winograd pruning, to improve the sparsity of convolutional networks. The method consists of two steps: spatial structured pruning and direct pruning. In the first step, the spatial-domain weights are pruned in a structured way, which can efficiently transfer the spatial domain sparsity into the winograd domain. The second step is to use an importance factor matrix to adjust the weight importance and weight gradients in the retraining step, which makes it possible to effectively retrain the pruned network without changing the network structure. The proposed method is evaluated on CIFAR10/CIFAR100 and ImageNet."
SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,"This paper proposes a generative model for unsupervised and semi-supervised clustering. The proposed method, Adversarially Learned Mixture Model (AMM), is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available."
SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes an estimator for the gradient of the parameters of the Bernoulli distribution of a discrete latent variable model. The estimator is based on the augment-reinforcement-merge (ARM) technique, which combines the REINFORCE and reparameterization techniques to reduce the variance of the estimator. The authors show that the gradient is unbiased, exhibits low variance, and has low computational complexity. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric “self-control” baseline function together with the augmented space estimator in that augmented space."
SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a new probabilistic programming language (PPL) that allows for the use of more sophisticated models such as Bayesian nonparametric models as fundamental building blocks in the PPLs. The proposed approach is based on the framework of variational inference, which allows the pre-specified inference methods to be transparently used for inference of the whole model. The authors demonstrate the power and convenience of the proposed approach with various examples of Gaussian process models on real data."
SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method to prune a neural network once at initialization prior to training. The proposed method is based on a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. The method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet."
SP:986b9781534ffec84619872cd269ad48d235f869,This paper provides an empirical study of the degradation of the performance of beam search for decoding neural sequence models. The authors show that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. They propose two methods to constrain the search and show that constrained beam search effectively eliminates the problem of beam-search degradation.
SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes a method to improve the sample efficiency of model-free reinforcement learning (RL) by constructing a curriculum around a demonstration. The proposed method, called Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment’s fixed initial state, Backplay starts the agent near the end of the demonstration and moves the starting point backwards during the course of training until we reach the initial state. The authors empirically show that Backplay compares favorably to other competitive methods known to improve sample efficiency, including reward shaping, behavioral cloning, and reverse curriculum generation."
SP:426c98718b2dbad640380ec4ccb2b656958389bc,"This paper proposes an automated multi-layer pruning method to compress deep neural networks. The proposed approach exploits an efficient approximation of the Hessian based on the K-FAC method as the pruning criterion. It has much fewer hyper-parameters and can be used in an automated manner without costly test and trial processes. The authors have demonstrated the effectiveness of their method on several datasets and architectures, outperforming previous state-of-the-art by a large margin."
SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper presents an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. The authors first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method, and then measure the ability of interventions to control objects in the output. Finally, they show the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. They show several practical applications enabled by their framework, from comparing internal representations across different layers, models, and datasets, to improving GAN models by locating and removing artifact-causing units, to interactively manipulating objects in a scene."
SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper proposes a metric to measure the distance between two networks in function space. The metric is based on the distance in L2-space between the outputs of two functions when given the same inputs. The paper shows that the function space metric can be used to measure and regularize function distances directly, without relying on parameters or local approximations like loss curvature. In particular, the paper shows how the L distance could be applied directly to optimization and how it could be used in multitask learning. "
SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a framework for learning a deep generative Markov model of biological dynamics from biological data. The model is trained as a neural network, where the next state is a probability distribution based on the current state. The authors show that the model is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. They show the advantage of learning deep models over shallow models such as Kalman filters and hidden Markov models that do not learn representations of the data, both in terms of learning embeddings of data and also in terms training efficiency, accuracy and ability to multitask."
SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"This paper proposes a generative adversarial network-based unsupervised learning method for image classification. The proposed method is based on graph Laplacian, where the number of classes is assumed to be unknown. The authors show that there exists a linear map between two separated neural networks of Generative Adversarial Networks (GANs). Then, the authors derive a framework to extract k vectors representing each class from the latent space. "
SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning permutation-invariant representations of sets. The authors propose a Permutation-Optimisation module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a permutation invariant representation of that set, avoiding a bottleneck in traditional set models. They demonstrate the model’s ability to learn permutations and set representations with either explicit or implicit supervision on four datasets."
SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper proposes Projective Subspace Networks (PSN), a method for few-shot learning that learns non-linear embeddings from limited supervision. In contrast to previous works, the embedding in PSN deems samples of a given class to form an affine subspace. The authors show that modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-Shot classification. Moreover, the PSN approach has the ability of end-to-end learning."
SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"This paper proposes CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), CAML can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems."
SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"This paper proposes a framework where the user controls what characteristics of the data they want to share (utility) and what they wish to keep private (secret), without necessarily asking the utility provider to change its existing machine learning algorithms. The authors first analyze the space of privacy-preserving representations and derive natural information-theoretic bounds on the utility-privacy trade-off when disclosing a sanitized version of data X. They present explicit learning architectures to learn privacy preserving representations that approach this bound in a data-driven fashion. The utility providers are willing to collaborate with the sanitization process."
SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation technique to improve the training stability of GANs. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. The proposed method is evaluated on MNIST, Fashion-MNIST, CIFAR10 and CELEBA."
SP:c210982ccdd134d4b293dbe144990398eefe1a86,"This paper proposes a rotation-equivariant convolutional neural network to predict neural responses in primary visual cortex (V1) from images recorded from a population of 6000 neurons in V1. The network is trained to be rotation equivariant and is able to predict the responses of V1 neurons to natural images using two-photon imaging. The results show that the network can outperform a regular CNN with the same number of feature maps and reveals a number of common features, which are shared by many neurons and are pooled sparsely to predict Neural activity."
SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper proposes a simple algorithm called SIGNSGD for distributed training of deep neural networks. The algorithm is based on the idea of aggregating gradients by majority vote, where each worker sends the sign of their gradient to the parameter server, which aggregates the signs and sends back the majority decision. Theoretical analysis shows that the algorithm converges in the large and mini-batch settings, establishing convergence for a parameter regime of ADAM as a byproduct. The authors also prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially."
SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a method to generate realistic and high-fidelity stock market data based on generative adversarial networks. They model the order stream as a stochastic process with finite history dependence, and employ a conditional Wasserstein GAN to capture history dependence of orders in a stock market. They test their approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data."
SP:ba66503753b3c57781b435c55c47fc9f69450e65,"This paper studies the problem of reinforcement learning in the presence of adversarial perturbations to the rewards. Specifically, the authors consider the setting where the observed rewards are generated with a reward confusion matrix. The authors propose an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed rewards. The framework draws upon approaches for supervised learning with noisy data and defines a set of unbiased surrogate rewards. Extensive experiments on different DRL platforms show that policies based on the estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines."
SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the effect of network structure (depth and width) on halting time and shows that larger models--wider models in particular--take fewer training steps to converge. The authors design simple experiments to quantitatively characterize the effects of overparametrization on weight space traversal. Results show that halting time improves when growing model’s width for three different applications, and the improvement comes from each factor: the distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows, and gradient vectors become more aligned with each other."
SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper investigates the question of whether reinforcement learning can find theoretically optimal algorithms for online optimization problems, and introduces a novel learning framework in this setting. Specifically, the authors introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst case. They test their new ideas on the AdWords problem, the online knapsack problem, and the secretary problem. The results indicate that the models have learned behaviors that are consistent with the optimal algorithms derived using the online primal-dual framework."
SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper studies the effect of removing domain-specific components (i.e., domain knowledge and hyper-parameters) from deep reinforcement learning (RL) algorithms. The authors propose to replace these fixed components with adaptive solutions from the literature. They show that the performance sometimes decreases with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive solutions perform better. They then investigate the main benefit of having fewer domain specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system."
SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for the Vision-and-Language Navigation (VLN) task. The proposed approach consists of two complementary modules: visual-textual co-grounding module and progress monitor. The visual-language module locates the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images. The progress monitor regularizes and ensures the grounded instruction correctly reflects the progress towards the goal by explicitly estimating the completeness of instruction-following. The approach sets a new state-of-the-art performance on the standard Room-to-Room dataset."
SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,"This paper proposes two methods to improve the performance of neural program synthesis approaches. The first method is called execution-guided synthesis, which views the program execution as a sequence of manipulations to transform each input state into the corresponding output state. The second one is called synthesizer ensemble, which leverages the semantic information to ensemble neural program synthesizers. The authors evaluate the proposed methods on the Karel dataset, which is the largest publicly available benchmark for input-output program synthesis, and show that it can improve the accuracy from around 77% to more than 90%."
SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the effect of batch normalization (BN) on the stability, convergence and acceleration properties of gradient descent (GD) with ordinary least squares (OLS) on a simplified supervised learning problem. Theoretically, the authors show that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, acceleration effects, as well as insensitivity to the choice of learning rates. The authors also demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems."
SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,"This paper provides a theoretical analysis of data noising in recurrent neural network language models. The authors show that each variant of data-noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus as the unigram distribution). Based on this insight, the authors propose a more principled method to apply at prediction time and propose natural extensions to data noizing under the variational framework. In particular, they propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoothhing method. Experiments on Penn Treebank (Marcus et al. 1994) and Wikitext-2 (Merity et al 2017) datasets demonstrate the effectiveness of the proposed method."
SP:f4a914d3df1a5a21a7365ba78279420f39210884,"This paper proposes a method for classifier-agnostic saliency map extraction, i.e. finding a saliency mapping that works for all possible classifiers weighted by their posterior probabilities. The authors propose a practical algorithm that amounts to simultaneously training a classifier and a salience mapping using stochastic gradient descent. They qualitatively show that the proposed approach extracts saliency maps that cover all the relevant pixels in an image and that the masked-out images cannot be easily recovered by inpainting, unlike for classifer-dependent approaches."
SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a method to train a neural network from a set of teacher networks. The idea is to use the weights of the teacher network as the weights for the student network. The student network is trained to be independent of the teachers at the end of the training process. The method is evaluated on a variety of tasks, including reinforcement learning and supervised learning."
SP:a72072879f7c61270d952f06d9ce995e8150632c,This paper proposes a method to learn a compact dynamical model for complex systems. The authors propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamic model while still ensuring the original objectives of causal inference and accurate predictions. The model construction is cast as a maximization of the compression of the state variables that the predictive ability and causal interdependence constraints between the original data streams and the compact model are closely bounded. They provide theoretical guarantees concerning the convergence of the proposed learning algorithm. 
SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a variational autoencoder (VAE) model for learning conditional distributions of the form $p(xI |xU\I)$, where $U$ is the set of features and $I$ is an arbitrary subset of features. The proposed model is a VAE with a Gaussian prior conditioned on a set of latent Gaussian variables. The model is trained using stochastic gradient variational Bayes (SGVAB). The authors propose several tricks to improve optimization and give recommendations about hyperparameters choice. The experimental results show that the proposed method is competitive with state of the art methods for both missing features imputation and image inpainting problems."
SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"This paper proposes a method to reduce the memory footprint of neural network training by approximating the activations in the forward pass with lower-precision approximations. The proposed approach is motivated by distributed training algorithms that work with approximate gradients and aggregated aggregated gradients across multiple devices (Recht et al. 2014; Seide et al., 2014; Wen et al, 2017; Gomez et al 2017). The authors propose to use uniform quantization to approximate the floating-point activations during the forward and backward passes of the network. The authors conduct experiments on CIFAR-10 and ImageNet to show the effectiveness of the proposed approach."
SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"This paper proposes a progressive GAN with frequency-oriented attentive modules (FAM) for high-resolution and fast face completion, which learns face structures from coarse to fine guided by the FAM. By consolidating information across all scales, the proposed model not only outperforms state-of-the-art methods by generating sharper images in low resolution, but is also able to synthesize faces in higher resolutions than existing techniques. A conditional version of our model allows users to control the properties of generated images explicitly with attribute vectors and landmarks."
SP:a300122021e93d695af85e158f2b402d21525bc8,"This paper presents a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. It shows that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, and derives a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. The authors apply their analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet, and show that with accumulation precision set in accordance with their proposed equations, the networks successfully converge to the single precision floating-point baseline."
SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"This paper studies the risk convergence and asymptotic weight matrix alignment of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. The authors show that the risk converges to 0, the normalized weight matrices are aligned across layers, and the linear function induced by the network converges in the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon."
SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper extends the Seq2Seq model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. Experiments on two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends."
SP:017b66d6262427cca551ef50006784498ffc741d,"This paper proposes a goal-driven collaborative task that contains language, vision, and action in a virtual environment as its core components. Specifically, the authors develop a Collaborative image-Drawing game between two agents, called CoDraw. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas. The two players communicate via two-way communication using natural language. The authors collect the CoDraw dataset of ∼10K dialogs consisting of ∼138K messages exchanged between human agents. They define protocols and metrics to evaluate the effectiveness of learned agents on this testbed, highlighting the need for a novel crosstalk condition which pairs agents trained independently on disjoint subsets of the training data for evaluation."
SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a new approach to learning neural random fields (NRFs) with inclusive-divergence minimized auxiliary generator for continuous data (e.g. images), with solid theoretical examination on exploiting gradient information in model sampling. The authors show that the proposed approach can be flexibly used in unsupervised/supervised image generation and semi-supervised classification, and empirically to the best of our knowledge, represent the best-performed random fields in these tasks. Extensive empirical evaluations show that inclusive-NRF approach can obtain state-of-the-art sample generation quality and achieve strong classification results on the widely benchmarked datasets."
SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper proposes an algorithm for training-time adversarial attacks on (attributed) graphs, focusing on the task of node classification. They use meta-gradients to solve the bilevel optimization problem underlying the challenging class of poisoning adversarial attack. They propose an algorithm based on the principle of meta-learning, which has traditionally been used for hyperparameter optimization (Bengio, 2000), or, more recently, few-shot learning (Finn et al., 2017). In essence, they turn the gradient-based optimization procedure of deep learning models upside down and treat the graph at hand – the input data – as a hyper-parameter to learn. Their experiments show that attacks created using our meta-gradient approach consistently lead to a strong decrease in classification performance of graph convolutional models and even transfer to unsupervised embeddings."
SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper proposes a new generative model, called Cramer-wold autoencoder (CWAE), which is based on the sliced-Wasserstein autoencoders (SWAE) and WAE-MMD (WAE using maximum mean discrepancy based distance function). The main contribution of the paper is the introduction of the CramerWold distance between distributions, which has a closed-form for the distance of a sample from standard multivariate normal distribution. CWAE cost function is based upon a characteristic kernel and has a simple closed form in the case of normal prior. Experiments on MNIST and CIFAR-10 datasets show that the proposed model is able to match the performance of WAE and sometimes improves upon it."
SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper proposes MahiNet, a memory-augmented hierarchical classification network (MahiNet) for the many-class few-shot (MCFS) problem. The proposed method is based on the idea of exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain. The authors also propose two novel benchmark datasets for the MCFS problem."
SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes Structural-Jump-LSTM, a recurrent neural network for speed reading. The proposed method is inspired by human speed reading, and can skip irrelevant words in important sections, while also jumping past unimportant parts of a text. It uses the dynamically spaced punctuation structure of text to determine whether to jump to the next word, the next sub-sentence separator (,;), next end of sentence (.!?), or to the end of text. In addition, it allows skipping a word after observing it without updating the state of the RNN."
SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a nonlinear radial basis convolutional feature transformation by learning the Mahalanobis distance function. The proposed method is evaluated on three publicly available image classification and segmentation data-sets namely, MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14. The authors evaluate the robustness of the proposed method to different gradient (targeted and untargeted) and nongradient based attacks and compare it to several non-gradient masking defense strategies. The results demonstrate that the method can boost the performance of deep convolution neural networks against adversarial perturbations without accuracy drop on clean data."
SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a method for learning a stochastic policy model that is inherently consistent through a period of time, especially for tasks with either sparse rewards or long term information. The proposed method, called NADPEx, is based on the idea of learning a global random variable for conditional distribution, which is modelled as a dropout network. The dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients’ alignment with the objective and KL constraint in policy space, are discussed to guarantee the policy’s stable improvement. The experiments demonstrate the effectiveness of the proposed method."
SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes the nonlinearity coefficient (NLC) as a new metric for predicting the performance of neural networks. The NLC is computed in the network’s randomly initialized state and is a powerful predictor of test error. The authors show that attaining a right-sized NLC can predict an optimal test error, at least in fully-connected feedforward networks."
SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies the phenomenon of bias amplification in binary classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. The authors demonstrate that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive “weak” features if insufficient training data is available. The overestimation gives rise to feature-wise bias amplification – a previously unreported form of bias that can be traced back to the features of a trained model. The paper proposes two feature selection algorithms that are designed to mitigate bias amplification. The experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy."
SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the generalization properties of neural networks with relu activation functions. The authors show that the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, and that increasing the over-parameterization improves the normalized margin and generalization error bounds for deep networks. In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees."
SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"This paper proposes an object pathway for fine-grained control of the location of objects within an image by adding an additional object pathway to both the generator and the discriminator. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. Experiments on the Multi-MNIST, CLEVR, and the more complex MSCOCO data set show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations."
SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a representation learning and model-based reinforcement learning algorithm, SOLAR, which is able to learn policies in a data-efficient manner directly from raw high-dimensional observations. The key insights in SOLAR involve learning latent representations where simple models are more accurate and utilizing PGM structure to infer dynamics from data conditioned on entire real-world trajectories. The experimental results demonstrate that SOLAR is competitive in sample efficiency, while exhibiting superior final policy performance compared to other model-free RL methods."
SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"This paper proposes a method for counterfactual policy evaluation and policy search in reinforcement learning. The proposed method, Counterfactually-Guided Policy Search (CF-GPS), leverages structural causal models (SCMs) to model the environment with two ingredients: 1) independent random variables, called scenarios, that summarize all aspects of the environment that cannot be influenced by the agent; 2) Deterministic transition functions (also called causal mechanisms) take these scenarios, together with the agent’s actions, as input and produce the predicted outcome. The authors show that the proposed method can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weights data, the proposed approach leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data."
SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"This paper studies the relationship between adversarial robustness and the geometric properties of the decision surface of the loss surface of a neural network in both parameter space and input space. The authors show that the geometry property of decision surface in the input space correlates well with the robustness against adversarial attacks. Based on this observation, the authors propose a robustness indicator, which can evaluate a network’s intrinsic robustness property without testing its accuracy under adversarial attack. The paper also proposes a robust training method, which aims to smooth the decision surfaces and enhances the adversarial training process."
SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. The authors integrate the quantitative DNN energy estimation into the training process to assist the constrained optimization. Compared to the best prior energy-saving methods, the framework trains DNNs that provide higher accuracies under the same or lower energy budgets."
SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper studies the problem of Bayesian Adaptive Markov Decision Processes (BAMDPs), where the agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this distribution. The authors propose a new policy network architecture that encodes the belief distribution independently from the observable state. The proposed method, Bayesian Policy Optimization (BPO), builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. The method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions."
SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a unified framework for unsupervised representations of entities and their compositions, by viewing each entity as a histogram (or distribution) over its contexts. This enables them to take advantage of optimal transport and construct representations that effectively harness the geometry of the underlying space containing the contexts. The method captures uncertainty via modelling the entities as distributions and simultaneously provides interpretability with the optimal transport map, hence giving a novel perspective for building rich and powerful feature representations. Empirical results show strong advantages gained through the proposed framework. This approach can potentially be used for any unsupervisory or supervised problem (on text or other modalities) with a co-occurrence structure."
SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper investigates the relationship between model-based reinforcement learning (MBRL) and the performance of model-free RL methods in two MuJoCo environments. The authors propose a dynamics model that directly predicts distant states, based on current state and a long sequence of actions, and claims that it avoids the need for many recursions during long-range planning, and thus is able to yield more accurate state estimates. Experiments show that the optimal planing horizon can be over 100 steps – much longer than typically considered in many MBRL approaches. "
SP:da14205470819495a3aad69d64de4033749d4d3e,"This paper proposes a method to reduce the accumulated quantization error in neural network quantization, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. The proposed method, called precision highway, forms an end-to-end high-precision information flow while performing the ultralow precision computation. The paper provides the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. The experiments show that the proposed method outperforms the best existing quantization methods while offering 3- bit weight/activation quantization with no accuracy loss and 2-bit quantization."
SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a generative adversarial network-based method for image denoising, deblurring, super-resolution and inpainting. The proposed method is formulated as a constrained optimization problem, where the objective is to maximize a posteriori probability of latent variables, and its constraint is that the image generated by these latent variables must be the same as the degraded image. The authors use a GAN-based density estimation model as the density estimator, which is trained on the MNIST dataset."
SP:2feef921a0563d52fde1c074da754f73e6cabef8,"This paper proposes a novel method for knowledge distillation from few samples (FSKD) for transferring knowledge from a large ""teacher"" network to a compact yet efficient ""student"" network by matching certain statistics such as softmax outputs and feature responses. The authors propose to add a 1x1 conv-layer at the end of each block in the student-net, and align the block-level outputs between ""Teacher"" and ""Student"" by estimating the parameters of the added layer with limited samples. The method works for student-nets constructed in various ways, including compression from teachernets and fully redesigned networks with random initialization on various datasets. Experiments demonstrate that the proposed method is very efficient and effective to distill knowledge from teacher-net to student networks constructing in different ways."
SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper proposes a metric, H-score, to quantify the transferability of feature representations across tasks via an approach grounded in statistics and information theory. The transferability metric is defined as the ratio between the best achievable error exponent of the transferred representation and the minium error exponents of the target task. This formulation of transferability can further be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments using both synthetic and real-world image data show that the proposed transferability is meaningful in practice, and can generalize to inference problems beyond classification."
SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes to add a planning phase to NMT to control the global sentence structure ahead of translation. The proposed approach learns discrete structural representations to encode syntactic information of target sentences. During translation, the model first generates the codes, then output the words conditioned on the structural constraint imposed by the codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations."
SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"This paper proposes a new loss function for generative adversarial networks (GANs) based on the notion of ""relativism"", i.e. the probability that the given real data is more realistic than a randomly sampled set of fake data. The proposed loss function is a weighted average of the gradients of the discriminator and the generator, where the generator is trained to maximize the likelihood of the real data and minimize the probability of the fake ones. The authors show that the proposed loss can be used to improve the stability and quality of GANs. "
SP:8df1599919dcb3329553e75ffb19059f192542ea,This paper proposes a method to address the catastrophic forgetting problem in continual learning. The proposed method learns to build a model with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the solver to suit each test example in order to classify it. Experimental results show that the proposed approach outperforms the existing baseline methods markedly.
SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"This paper proposes Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents’ future behavior in multiagent environments. The authors build on recent advances in neural networks that effectively perform relational reasoning with graph networks (Battaglia et al., 2018) to construct a graph network to construct the forward dynamics of multiagent systems. They show that RFM can capture the rich social dynamics of the environment, that its intermediate representations contain valuable interpretable information, and that providing this information to learning agents results in faster learning system."
SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a meta-IRL approach for inverse reinforcement learning (IRL) that learns a reward function from expert demonstrations. The main idea is to learn a reward parameterization that can be used for few-shot learning of new tasks. The reward function is learned from demonstrations of the desired behaviors for those tasks, which are referred to as the meta-training set. The paper shows that the proposed approach can recover rewards from images for novel tasks and provides intuition as to how the approach is analogous to learning a prior. "
SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"This paper proposes a general framework for meta-learning approximate probabilistic inference for prediction (ML-PIP) that unifies a broad class of meta learning methods, including gradient-based meta learning (Finn et al. 2017), metric based meta learning, amortized MAP inference (Qiao et al 2017), conditional probability modelling (Garnelo et al 2018a;b), and conditional probability modeling (Gang et al., 2018). The proposed method, called VERSA, is an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. The authors evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, and for classification, arbitrary number of classes at train and test time (see Section 3)."
SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper proposes a method for fine-grained structured object classification. The proposed method is based on linear chain CRF, where the CRF is parametrized as a pairwise potential matrix, and features are learned by a convolutional network. The paper also proposes a surrogate likelihood, which is a local likelihood approximation of the original CRF with integrated batch-normalization. Experiments are conducted on a toy dataset and a real-world dataset, and the proposed method outperforms the existing methods."
SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a generative adversarial network (GAN) for generating high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. The authors demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts. This work also opens up possible avenues for domain transfer and other exciting applications of adversarial losses to audio."
SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper proposes a mixed-integer linear programming (MILP) verifier for verifying the properties of piecewise-linear feed-forward neural networks. The main contributions of the paper are: (1) a new formulation of the MILP for non-linearities, and (2) a novel presolve algorithm that makes full use of all information available. The proposed method is shown to be two to three orders of magnitude faster than the state-of-the-art. (3) it is able to verify properties on convolutional and residual networks with over 100,000 ReLUs. (4) it can determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l_\infty norm = 0.1."
SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper proposes to learn a default policy in the KL-regularized RL objective. The main idea is to restrict the amount of information the default policy receives, forcing it to learn reusable behaviours that help the policy learn faster. The authors formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. They present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default behavior can significantly speed up and improve learning."
SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes a novel FLOW component for conversational machine comprehension. The FLOW module is a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to approaches that concatenate previous questions/answers as input, FLOW integrates the latent semantics of the conversation history more deeply. The proposed FLOWQA shows superior performance on two recently proposed conversational challenges."
SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,This paper tackles the problem of predicting the edits that software developers make to source code files. The authors develop several neural networks and use synthetic data to test their ability to learn challenging edit patterns that require strong generalization. They then collect and train their models on a large-scale dataset consisting of millions of fine-grained edits from thousands of Python developers. The main conclusion is that a new composition of attentional and pointer network components provides the best overall performance and scalability.
SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning strategy for multi-class classification. The proposed method optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi class classifier as a sub-module. The authors formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. The same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings."
SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper investigates the question of how deep learning models for visual question answering (VQA) interpret statements involving the quantifier “most” in the context of a visual scene. The authors design experiments and data to shed light on the question whether the state-of-the-art FiLM VQA model shows preference for one strategy over the other. The experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber’s law. Moreover, the authors identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system."
SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a probabilistic extension of DistMult and ComplEx models for link prediction in relational knowledge graphs. The authors argue that knowledge graphs should be treated within a Bayesian framework because even large knowledge graphs typically contain only few facts per entity, leading effectively to a small data problem where parameter uncertainty matters. They introduce a variational inference algorithm to estimate a lower bound on the marginal likelihood of the data and optimize the many hyperparameters of these models by gradient descent (variational EM); an approach that would not be possible when point estimating model parameters. The proposed approach outperforms the state-of-the-art by a significant margin on several benchmarks."
SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,This paper proposes a new online learning approach for supervised dimension reduction. The proposed approach builds on top of the sliced inverse regression (SIR) algorithm and makes it implementable in an incremental manner. The authors also refine the algorithm by using an overlapping technique and develop an incremental overlapping sliced in regression (IOSIR). The effectiveness and efficiency of both algorithms are verified on synthetic and real data applications.
SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,This paper proposes a multimodal factorization model (MFM) that decomposes multi-modal representations into two sets of independent factors. The discriminative and modality-specific factors are shared across all modalities and contain joint multimodality features. The generative factors are unique for each modality and contain the information required for generating data. The proposed MFM achieves state-of-the-art or competitive performance on six multimodals datasets.
SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper proposes a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, the authors learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. The authors introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the wave-net core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the embedding with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-speech neural network to new speaker, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers, and outperforms other recent works in matching the new speaker’s voice."
SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper studies the problem of computing robust estimators using tools developed for training GANs. In particular, the authors establish an intriguing connection between f-GANs and various depth functions through the lens of f-Learning. They show that these depth functions that lead to statistically optimal estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f -Learning. This connection opens the door of computing estimators with appropriate structures of discriminator networks with hidden layers in GAN. "
SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper proposes to represent scenes as scene programs, representing a scene via a symbolic program for its objects, attributes, and their relations. The authors also propose a model that infers scene programs by exploiting a hierarchical, object-based scene representation. Experiments demonstrate that the proposed model works well on synthetic data and transfers to real images with compositional structure."
SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a Gaussian-gated LSTM (g-LSTM) to reduce RNN state updates, thereby allowing the network to preserve memory over long temporal intervals. The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. Because the time gate limits the updates of the neuron state, the number of computes needed for the network update is also reduced. The authors also propose a temporal curriculum learning schedule to speed up the convergence time of the equivalent LSTMs on long sequences."
SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a simple yet efficient method to detect out-of-distribution samples. The proposed method is based on computing averages of low-order statistics at the batch normalization layers of the network and then use them as features in a linear classifier. This procedure is much simpler and efficient than current stateof-the-art methods, and outperforms them by a large margin in the traditional ID/OOD fitting task (as proposed in previous works)."
SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"This paper studies the problem of learning a one-hidden-layer neural network with sigmoid activations, where the training labels are generated from a one hidden-layer fully-connected neural network and the goal is to recover the weight vectors of the neural network. The authors prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground-truth without requiring a fresh set of samples at each iteration."
SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. The authors compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification."
SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the training objective of GANs from the mixed Nash equilibrium perspective. The authors propose a novel algorithmic framework via an infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE. They then propose a principled procedure to reduce their novel prox methods to simple sampling routines, leading to practically efficient algorithms. They provide experimental evidence that their approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality."
SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"This paper proposes a method that enables parameter-efficient transfer and multitask learning with deep neural networks. The basic approach is to learn a model patch a small set of parameters that will specialize to each task, instead of fine-tuning the last layer or the entire network. The approach allows both simultaneous (multi-task) and sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-only fine tuning, the proposed method is able to match singletask performance."
SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method, called mode normalization (MN), that allows the network to jointly normalize its features within multiple modes. The proposed method first assigns samples in a mini-batch to different modes via a gating network, and then normalizes each sample with estimators for its corresponding mode (Figure 1). The authors further show that MN can be incorporated into other normalization techniques such as group normalization by learning which filters should be grouped together."
SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"This paper investigates the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation—reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the lottery lottery: their connections have initial weights that make training particularly effective. The authors only consider vision-centric classification tasks on smaller datasets (MNIST, CIFAR10)."
SP:08c662296c7cf346f027e462d29184275fd6a102,This paper proposes a method for learning representations of the state space that can be used in the context of reinforcement learning. The authors propose to use an attentive dynamics model (ADM) that is trained in a self-supervised fashion to predict the actions taken by the agent. The ADM is then used as a part of the representation for the actor-critic algorithm that is used for count-based exploration. The method is evaluated on a set of Atari games with sparse rewards.
SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"This paper proposes HyperGAN, a generative adversarial network that learns to generate all the parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. The authors apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data and adversarial examples."
SP:230b3e008e687e03a8b914084b93fc81609051c0,"This paper proposes a differentiable estimator for the ELBO of the variational auto-encoder (VAE) based on importance sampling. Theoretical and empirical results show that the estimator can be used to train VAEs with binary or categorically valued latent representations. The proposed estimator is based on an importance sampling approach, which is differentiable and does not rely on reparametrized sampling. Experiments are conducted to verify the approach."
SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes to train a feed forward neural network with a pre-trained building block based on a mean field description of a Boltzmann machine. The proposed method is evaluated on the MNIST dataset. The authors show that the adversarial robustness is correlated with the generative performance of the underlying Boltzman machine.
SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,"This paper studies the effect of changes in the size and location of the minimal region of an image on the recognition performance of deep neural networks (DNNs) and humans. The authors introduce the concept of “fragile recognition images” (FRIs), which they claim are more general than the minimal images for humans and are more prominent in DNNs. They show that a slight adjustment of a one-pixel shift or two-pixel shrink of the visible. image region produces a drop in the DNN recognition accuracy in many image regions. They also show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in object location."
SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper studies the problem of multi-agent reinforcement learning (MARL) with self-interested agents (i.e., worker agents) who have their own minds (preferences, intentions, skills, etc.) and can not be dictated to perform tasks they do not want to do. To achieve optimal coordination among these agents, the authors propose Mind-aware Multi-agent Management Reinforcement Learning (MRL), which consists of agent modeling and policy learning. For agent modeling, they infer workers’ identities by their performance history, track with a mind tracker by imitation learning, and track with an internal state trained with a tracker by RL. For policy learning, they combine imitation learning and reinforcement learning for a joint training of agent modelling and management policy optimization. The authors have evaluated their approach in two environments, Resource Collection and Crafting, and show that it learns effectively, generalizes well, and has a fast and continuous adaptation."
SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a new recurrent neural network (RNN) architecture for asynchronous time series. The authors introduce a unified RNN that handles five different feature types, i.e., sparse, dense, time, static, time at the sequential level, time between events, and static at the whole sequence level. The proposed RNN can handle sparse features and time features in a different manner. The experiments show that the proposed model is able to achieve better performance compared to standard RNNs."
SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper presents a top-down approach to represent formulae by neural networks. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a topdown manner. The results of this network are then processed by two recurrent neural networks and the propositional atoms are treated. The model is insensitive to their names, it only matters whether they are the same or distinct."
SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper studies the problem of curriculum learning for training deep neural networks. The authors propose a method to sample mini-batches with a gradually increasing level of difficulty. The difficulty of a training image is defined using transfer learning from some competitive “teacher” network trained on the Imagenet database. They then suggest a bootstrap alternative to evaluate the difficulty of points using the same network without relying on a ”teacher network”. They compare this approach to a related version of Self-Paced Learning, showing that our method benefits learning while SPL impairs it."
SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper proposes a general PAC-Bayesian framework to provide generalization bounds for deterministic and uncompressed neural networks. The authors show that if on the training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions generalize to the interaction between the matrices on test data, thereby implying a wide test loss minimum. They then apply their general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the train data). In this setup, they provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weights matrices."
SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper builds on the work of Kaiser et al. (2018) who proposed the Latent Transformer, a non-autoregressive machine translation model, which outperforms the previous state-of-the-art on the WMT English-German translation task (Kaiser et al., 2018). The main contribution of this paper is an extension of the VQ-VAE (van den Oord et al, 2017) to the case of discrete latent variables, which is inspired by its connection to the Expectation Maximization (EM) algorithm (Dempster & Fleet, 2016) and the sequence level knowledge distillation (Hinton & Kim, 2015; Kim & Rush, 2016). The authors show that the EM algorithm can be used to improve the performance of the proposed model, while being 3.3 times faster at inference. The authors also show that tuning the size of the encoder and decoder can significantly improve the model performance."
SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes a framework for unsupervised sentence representation learning based on distributional hypothesis and multi-view learning. In particular, the authors propose a generative objective and a discriminative objective, which is a combination of the two. The generative model is an RNN-based encoder-decoder model, while the discriminator is a linear model. The authors show that the proposed framework outperforms the baselines on a variety of downstream tasks. "
SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"This paper proposes a distributed optimization method called Anytime Minibatch (AMB) to mitigate the impact of stragglers. In AMB, all nodes are given a fixed time to compute the gradients of as many data samples as possible. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. The paper provides a convergence analysis and analyze the wall time performance. Numerical results show that AMB is up to 1.5 times faster in Amazon EC2 and up to 5 times faster when there is greater variability in compute node performance."
SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,This paper proposes a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. The authors train a neural architecture to predict a driver’s peripheral blood flow modulation based on the first-person video from the vehicle. This architecture acted as the reward in our reinforcement learning step. A major advantage of training a reward on a signal correlated with the sympathetic nervous system response is that the rewards are non-sparse - the rewards start to show up much before the car collides. This leads to efficiency in training and with proper design can lead to policies that are also aligned with the desired mission.
SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper proposes an extension of Neural Processes (NP) by adding attention to the encoder and decoder. Specifically, the attention is applied to the context set of the observed input-output pairs, and the decoder is trained with a permutation invariant function that maps the input and target to a fixed-length latent summary. The authors show that the proposed approach is able to improve the accuracy of the proposed model, and that it is faster to train."
SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,"This paper provides a theoretical analysis of credit assignment in gradient-based meta-reinforcement learning (Meta-RL) and proposes Proximal Meta-Policy Search (ProMP), a meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies, the proposed algorithm endows efficient and stable meta learning. Experimental results show that ProMP consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance."
SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. The proposed method is not competitive with state-of-the-art SAT solvers, but it can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. It generalizes to novel SAT problems, including graph coloring, clique detection, dominating set, and vertex cover problems."
SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. The authors leverage a perception system for preprocessing the input and a controller for executing the output on the car. They propose exposing the learner to synthesized data in the form of perturbations to the expert’s driving, which creates interesting situations such as collisions and/or going off the road. They augment the imitation loss with additional losses that penalize undesirable events and encourage progress. They show that the model can handle complex situations in simulation, and show how the model successfully drives a car in the real world and is able to negotiate turns, stop signs, and traffic lights."
SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a method called Select Via Proxy (SVP) to efficiently select a subset of training data to achieve faster training with no loss in model predictive performance. SVP first trains a small proxy model to estimate the utility of individual training data points, and then select the most informative ones for training the large target model. Extensive experiments show that SVP leads to a 1.6x and 1.8x speed-up on CIFAR10 and SVHN by selecting 60% and 50% subsets of the data."
SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"This paper proposes Sequential Monte Carlo Planning (SMC), a planning algorithm that views planning as a probabilistic inference problem over future optimal trajectories. The authors propose to use sampling methods, and thus, tackle planning in continuous domains using a fixed computational budget. They also propose a way to combine model-free and model-based reinforcement learning for planning based on the SMC perspective. They empirically demonstrate that their method achieves state of the art results on Mujoco."
SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"This paper shows that adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Theoretical analysis shows that the robustness of adversarial trained models are sensitive to semantically-preserving transformations on data. Experiments on MNIST and CIFAR-10 show that adversarially trained models achieve significantly different robustness accuracies. This is counter-intuitive."
SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper introduces a new normalization technique that exhibits the fast convergence properties of batch normalization using a transformation of layer weights instead of layer outputs. The proposed technique keeps the contribution of positive and negative weights to the layer output in equilibrium. The authors validate the method on a set of standard benchmarks including CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet."
SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper proposes a data denoising method for training with noisy examples based on the implicit regularization effect of stochastic gradient descent with large learning rates. The proposed method identifies examples whose losses exceed a certain threshold. Then, it removes these examples from the dataset and continues training with the rest of the examples until convergence. Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples."
SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a theoretical analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text — the downstream classifier must recover a function of the posterior distribution over the latent variables. The authors analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. They show that, under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, and that prompt tuning obtains downstream guarantees with weaker conditions. They also show that the recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM."
SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of domain generalization (DG), i.e., generalization of a trained model to out-of-distribution (OOD) data, even if the target domain is not exactly the same as the training domain. The authors propose a new notion of transferability, which they define as the difference in transferability between source and target domains. They then show that the transferability can be estimated with enough samples and give a new upper bound for the target error based on our transferability. Finally, they propose an algorithm for learning transferable features and test it over various benchmark datasets."
SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a part ordering over trajectories. Their main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. Then, they provide a polynomial-time algorithms that construct a reward function that can be used to optimize tasks of each of these three types, and correctly determine when no reward function exists. An empirical study corroborates and illustrates the theoretical findings."
SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning (RL). The authors show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully observed MDPs into POMDPs. Based on this observation, the authors recast the generalization problem in RL as solving the induced partially observed Markov decision process (POMDP), which they call the epistemic POMD, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, they demonstrate that their simple algorithm derived from POMP achieves significant gains in generalization over current methods on the Procgen benchmark suite."
SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper provides a unified framework for estimating higher-order derivatives of value functions for model-agnostic meta-reinforcement learning. This framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. It also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper considers distributed convex optimization with a convex convex cost function. The authors propose a new algorithm that performs bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. The main idea is that the downlink compression only impacts local models, while the global model is preserved. The proposed algorithm is based on the idea of randomization, which allows to reduce the variance associated with the down-link compression."
SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper introduces counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn’t change the model predictions. The authors also provide practical schemes for learning (approximately) counterfactually invariant predictors. The paper shows that both the means and implications depend on the true underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label. Distinct causal structures require distinct regularization schemes to induce counterfactuality."
SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes an adaptive pseudo-augmentation method for training GANs with limited data. The authors claim that the overfitting of the discriminator is the critical reason that impedes effective GAN training on limited data, rendering severe instability of training dynamics. To address this problem, the authors propose a simple yet effective way to regularize a discriminator without introducing any external augmentations or regularization terms. They call their method Adaptive Pseudo Augmentation (APA). In contrast to previous standard data augmentations, they exploit the exploit the generator in a GAN itself to provide the augmentation, a more natural yet effective regularization method. They conduct extensive experiments to demonstrate the effectiveness of APA."
SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a framework for causal inference between pairs of event variables in multivariate recurrent event streams by extending Rubin’s framework for the average treatment effect (ATE) and propensity scores to multivariate point processes. The setting is similar to i.i.d. data, but with irregularly spaced occurrences of various types of events over a common timeline. The authors theoretically justify their point process causal framework and show how to obtain unbiased estimates of the proposed measure. They conduct an experimental investigation using synthetic and real-world event datasets, where their proposed causal inference framework is shown to exhibit superior performance against a set of baseline pairwise causal association scores."
SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper proposes a novel GNN with Adaptive residual, called AirGNN, to improve the resilience of GNNs against abnormal node features. The authors analyze possible reasons to understand this phenomenon and propose and derive a simple, efficient, interpretable, and adaptive message passing scheme. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm. "
SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper proposes a variational Bayesian optimistic sampling (VBOS) algorithm that produces policies by solving an optimization problem in each round of a sequential decision problem. The optimization problem that must be solved is always convex, no matter how complicated the posteriors are. The authors showed that VBOS enjoys low regret for several problem families, including the well-known multi-armed stochastic bandit problem as well as more exotic problems like constrained bandits where TS can fail. "
SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper improves the convergence analysis and rates of variance reduction under without-replacement sampling orders for composite finite-sum minimization. The authors develop a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. These rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without replacement sampling with variance-reduction. In addition, the authors also propose a practical method to discover the optimal cyclic ordering numerically."
SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper provides convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the relative entropy policy search (REPS) objective. The authors consider the setting in which they are given access to exact gradients and demonstrate how near-optimalality of the objective translates to near-optimability of the policy. Then, they propose a stochastic gradient method that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy."
SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper studies the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. The authors propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. They also propose metrics to evaluate the spatial smoothness of encoding 3d structures, and the representation complexity of the DNN. The experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training."
SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,This paper proposes an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. They introduce a new metric to evaluate an auction allocations’ adherence to socially desirable constraints and demonstrate that their proposed method is competitive with current state-of-the-art neural-networks based auction designs. They validate their approach through human subject research and show that they are able to effectively capture real human preferences.
SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of personalized supervised learning with user-level differential privacy. In this setting, there are n users, each of whom has a training data set drawn from their own distribution Pi. Assuming some shared structure among the problems Pi, can users collectively learn the shared structure—and solve their tasks better than they could individually—while preserving the privacy of their data? The authors formulate this question using joint, user level differential privacy—that is, we control what is leaked about each user’s entire data set. They provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach. They also establish a general, information-theoretic upper bound via an exponential-based algorithm."
SP:3925fc528de17b8b2e93808f5440ea0503895b75,"This paper introduces AdVQA (Adversarial Visual Question Answering), a new benchmark for visual question answering. The dataset is built using a dynamic human-adversarial approach, where each image in the dataset is annotated with a question where the model’s predicted answer is incorrect. Human subjects interact with a state-of-the-art VQA model on the dataset, and for each image, they attempt to find a question on which the model predicts the answer incorrectly. The authors also conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions."
SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper studies the role of heterogeneous neurons in the medial entorhinal cortex (MEC) of animals. The authors first identify the similarity transform between neural populations in different animals, and then evaluate the ability of multiple neural network models to explain the response variance of MEC neurons, treating each candidate model as a potential ""source"" animal and measuring how well it maps to each potential ""target"" animal. They then evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells. Finally, they introduce a new MEC model that performs reward-modulated path integration, and find that it matches neural recordings across all variable-reward conditions."
SP:57f9812fa5e7d0c66d412beb035301684d760746,This paper studies the problem of learning from expert demonstrations in reinforcement learning with KL-regularized reinforcement learning (RL). The authors identify a previously unrecognized pathology in the learning dynamics of RL with behavioral reference policies and propose to use non-parametric behavior reference policies to remedy the pathology. The proposed method is evaluated on a variety of locomotion and dexterous hand manipulation tasks. The results show that the proposed method can significantly improve the sample efficiency of online learning and yield online policies that outperform current state-of-the-art methods.
SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the relationship between the compositionality of the data and learning curves in the teacher-student framework for kernel regression, where the function to be learned is expressed as a sum of constituent functions each depending on a smaller number of variables. The learning curve exponent is independent of d and governed by s if s>t, optimal for s=t and null if s<t. The authors show empirically that performing kernel regression with a ridge that decreases with the size of the training set leads to similar learning curve exponents to those obtained in the ridgeless case."
SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a variational autoencoder (VAE) model with a deterministic prior. The authors propose to regularize the latent space of the VAE to be similar to the prior of a uni-modal Gaussian distribution. They also propose to replace the Wasserstein distance between the encoder and the decoder with a KL-divergence between the latent vectors of the prior and the latent vector of the decoders. The proposed model is evaluated on both continuous and discrete data sets. 
SP:6232d8738592c9728feddec4462e61903a17d131,"This paper proposes an autoencoder-based self-supervised adversarial detection method to detect adversarial examples by disentangling input images as class features and semantic features. The proposed method trains an auto-encoder, assisted by a discriminator network, over both correctly paired class/semantic features and in correctly paired classes/semantics features to reconstruct benign and counterexamples. This mimics the behavior of adversarial images and can reduce the unnecessary generalization ability of autoencoders. The authors compare their method with the state-of-the-art methods under different adversarial attacks and different victim models (30 attack settings), and it exhibits better performance in most attack settings."
SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper proposes a method to model the brain representations of syntactic syntactic information in the context of fMRI. The main idea is to embed the syntactic structure of a sentence into a vector representation, which is then used to predict the time series of brain activity as a function of the time. The authors propose a novel multi-dimensional embedding space that can be used to model syntactic representations and fMRI recordings of participants reading a natural text. They show that syntactic features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, they show that regions well-predicted by syntactic feature are distributed in the language systems and not distinguishable from those processing semantics."
SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation using energy-based models (EBMs) to handle compositional generation over a set of attributes. The authors propose a novel EBM formulation representing the joint distribution of data and attributes together, and show how sampling from it is formulated as solving an ordinary differential equation (ODE). Sampling with ODEs is done efficiently in the latent space and is robust to hyperparameters. Experimental results show that the proposed method outperforms the state-of-the-art in both conditional sampling and sequential editing."
SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper proposes a federated linear contextual bandits (FL) model, where each client faces K-armed stochastic bandits coupled through common global parameters. By leveraging the geometric structure of the linear rewards, a collaborative algorithm called Fed-PE is proposed to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. The proposed algorithm relies on a novel multi-client G-optimal design, and achieves near optimal regrets for both disjoint and shared parameter cases with logarithmic communication costs. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a conservative offline model-based RL algorithm, COMBO, which trains a value function using both the offline dataset and data generated using rollouts under the model, and additionally additionally regularizes the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the authors show that COMBO achieves less conservative Q values compared to prior model-free offline RL methods and guarantees a safe policy improvement. Empirically, it outperforms prior offline RL algorithms on widely studied offline RL benchmarks, including image-based tasks."
SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the effect of backpropagation on the feature space of deep neural networks during training. The authors propose to model the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. The main finding is that if the SDEs are locally elastic, then the features of the training data become linearly separable, meaning vanishing training loss; otherwise, the features are not separable. The results are corroborated with experiments on a synthetic dataset and CIFAR-10."
SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a framework to learn to synthesize a program that describes the procedure to solve a task in a flexible and expressive manner, solely from reward signals. The authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embeddings space to yield a program maximizes the return for a given task. The proposed method is evaluated on the Karel domain, where it outperforms DRL and program synthesis baselines."
SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper studies the physics-informed neural networks (PINNs), which incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. The authors demonstrate that existing PINN methodologies can learn good models for relatively trivial problems, but they can easily fail to learn relevant physical phenomena for even slightly more complex problems. They provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They also show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN’s setup makes the loss landscape very hard to optimize. "
SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, CST generates target pseudo-label with a source-trained classifier, and in the reverse step, it trains a target classifier using target pseud-label, and then updates the shared representations to make the target model perform well on the source data. The authors introduce the Tsallis entropy as a confidence-friendly regularization to improve the quality of target pseudo labels. "
SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured pruning method, called Discriminative Masking (DAM), to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. Theoretical analysis shows that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. Experiments are conducted on a variety of tasks, including dimensionality reduction, recommendation system, graph representation learning, and structured training for image classification."
SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a self-attention-based neural network architecture, called Neural Interpreters, that allows for compositional reasoning and reuse of knowledge. In particular, the proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. The proposed architecture is evaluated on image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, the authors show that the proposed model can perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner, and in the latter, it is competitive with respect to the state of the art in terms of systematic generalization."
SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes a technique called behavior transfer (BT) that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. The authors show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors. They also show that combining BT with standard fine-tuning strategies results in additional benefits."
SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a new class of differentiable surrogates for ranking, which employ a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divide-and-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, the authors demonstrate the role of larger list sizes during training and show that the proposed PiRank significantly improves over comparable approaches on publicly available learning-to-rank benchmarks."
SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,This paper proposes a reinforcement learning approach to improve the performance of variational quantum circuits (VQEs) by optimizing the structure of the variational ansatz. The main idea is to use reinforcement learning (RL) to explore the space of possible VQE structures. The RL algorithm uses a feedback-driven curriculum learning method that adapts the complexity of the learning problem to the current performance of the RL algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. The proposed approach is evaluated on the problem of estimating the ground-state energy of LiH and achieves state-of-the-art results.
SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the effect of arbitrary class distributions within the query sets of few-shot tasks at inference, removing the class-balance artefact. Specifically, they model the marginal probabilities of the classes as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex. They evaluate the state-of-the-art transductive methods over 3 widely used data sets, and observe, surprisingly, substantial performance drops, even below inductive methods in some cases. Furthermore, they propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations."
SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a patch-by-patch inference scheduling method to reduce the memory consumption of convolutional neural networks on tiny microcontroller units (MCUs). The authors claim that the memory bottleneck is due to the imbalanced memory distribution in CNNs, where the first several blocks have an order-of-magnitude larger memory usage than the rest of the network. To alleviate this issue, the authors propose a generic patch-based inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. The authors also propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead."
SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies Bayesian automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent, and takes actions based on the strategic agent's report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal’s utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. For environments with large time horizons, the authors show that the optimal utility is hard to approximate within a certain constant factor, complementing the algorithmic result."
SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,"This paper investigates the question of how neural architecture search (NAS) is able to select the desired graph neural networks (GNNs) for different tasks. The authors propose a Graph Differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and is capable of discovering graph neural architectures with better performance through employing graph structure learning as a denoising process in the search procedure. Extensive experiments on real-world graph datasets demonstrate that the joint optimization of neural architecture and graph structure not only benefits in graph learning but also enhances the denoing ability."
SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fairness in clustering. In particular, the authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, and also a generalization of the group leximin objective. They show that these objectives are NP-hard in general, and derive lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. They further derive impossibility results for other natural fairness objectives. They conclude with experimental results on real-world datasets. "
SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. The authors prove that subject to a bounded overlap condition, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. They complement their negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics."
SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the effect of the choice of ReLU(0) for the activation function in [0,1] for deep neural networks. The authors study the impact of the ReLU activation function at various precision levels (16, 32, and 64 bits) on backpropagation and training. They show that the effect disappears with double precision, while it is systematic at 16 bits. They also show that for vanilla SGD training, the choice ReLU($0) = 0 seems to be the most efficient."
SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. This method brings together ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. The method jointly optimizes a latent-space model and policy to be self-consistent, that is, the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck. As a result of this compression, the policies learned by our method are robust and generalize well to new tasks."
SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"This paper proposes Spectral Attention Network (SAN), a Transformer-based graph neural network (GNN) architecture that uses a learned positional encoding (LPE) to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, the proposed model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. In addition, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. The proposed model achieves state-of-the-art performance on several benchmark datasets."
SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper considers the setting of two-alternative elections where voters’ preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. In this setting, even if every voter is a contingent voter, agents voting according to their private information need not result in the adoption of the universally preferred alternative, because the signals can be systematically biased. The authors present an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters, and outputs the alternative that is favored by the majority. Theoretical properties of the proposed mechanism still hold to a certain extent."
SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper studies the rank of the Hessian of deep neural networks. The authors derive exact formulas and tight upper bounds for the Hessians of deep linear networks, and show that the rank is proportional to the layer width. They also show that for a larger class of models such as rectified and hyperbolic tangent networks, the bounds remain faithful as an estimate of the numerical Hessian rank. Finally, they investigate the implications of model architecture on the rank deficiency."
SP:24cdcb12fca34680d8b34bc61c51b9003368228a,This paper proposes a new metric to quantify linear disentanglement based representation learning (LSBD). The proposed metric is based on the notion of linear symmetry-based disentangling (LSD). The authors also propose a VAE-based representation learning method based on this metric. The authors show that the proposed metric can be used to evaluate the performance of existing disentangled representation learning methods.
SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes a general framework for learning deep state-space models (DSSMs) by extending the variational inference framework of variational autoencoders (VAEs) to DSSMs. In particular, the authors propose a general Lagrangian formulation of the sequential evidence lower bound (ELBO) and extend the empirical Bayes prior (VHP) and the associated optimisation algorithm introduced in the context of VAEs to the case where the prior is a Gaussian. The authors also propose an extension of the Kalman VAE (EKVAE), which combines amortized variational inferences with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based models. The proposed method is evaluated on a variety of tasks, including system identification, model-based reinforcement learning, and disentanglement of representations."
SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations (counterfactuals) for black-box models without access to the training data. The proposed method, called DISC (Deep Inversion for Synthesizing Counterfactual) is based on the idea of deep model inversion (DIN), which is a method to generate images from the latent space of a trained deep classifier (i.e., the latent representation of the classifier) that is consistent with the underlying data distribution. The authors propose three modifications to DIN: (1) stronger image priors, (2) incorporating a novel manifold consistency objective, and (3) adopting a progressive optimization strategy. Experiments on CIFAR-10 and ImageNet datasets show that the proposed method is able to generate high-quality images, and is robust to corruptions."
SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes an algorithm for identifying regions of heterogeneity (i.e., regions where the assignment of decision-maker has a large causal effect on the decision) by maximizing an empirical objective. The authors formalize the problem as a causal inference problem, and propose an algorithm to find a region by maximizing the empirical objective, and provide a generalization bound for its performance. In a semi-synthetic experiment, the authors show that their algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, they apply their algorithm to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."
SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based style-based generative model for image synthesis. The key idea is to model the image synthesis as a visual token generation problem, i.e., to predict the visual tokens for synthesizing an image. Specifically, it takes as input a sequence of latent tokens to predict visual tokens and outputs two semantically different visual tokens, namely, the learned constant content tokens and the style tokens from the latent space. The style tokens are assigned to the content tokens by an attention mechanism with a Transformer. The authors conduct extensive experiments and show that the proposed TokenGAN has achieved state-of-the-art results on several widelyused image synthesis benchmarks, including FFHQ and LSUN CHURCH with different resolutions."
SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the robust overfitting of overparametrized linear regression and classification models. The authors show that the robust risk can be upper bounded by the risk of the ridge regularizer, and show that avoiding ridge regularization can improve generalization in the presence of no noise. They also show that early stopping can improve the robust generalization. "
SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. CPAE encodes an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and decodes the primitive back to the original input instance shape. The primitive plays a key role to map all the unordered point clouds on the canonical surface and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. The proposed method does not require any form of annotation or self-supervised part segmentation network and can handle unaligned input point clouds within a certain rotation range. Experimental results on 3D semantic keypoint transfer and part segmentations transfer show that the model performs favorably against state-of-the-art correspondence learning methods."
SP:8f28988012f8dca74c90316f7feeda15d49af2c5,This paper proposes a new method for domain generalization (DG) called Stochastic Weight Averaging Densely (SWAD) to find flatter minima and suffers less from overfitting than vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. The authors theoretically and empirically demonstrate that seeking flat minima can achieve better generalizability to both in-domain and out-of-domain. The extensive experiments on five DG benchmarks show superior performances of SWAD compared with existing DG methods.
SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper provides a large-scale study of performance predictors for neural architecture search (NAS) by analyzing 31 NAS techniques ranging from learning curve extrapolation, weight-sharing, to supervised learning, to zero-cost proxies. The authors test a number of correlation-and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. Their results act as recommendations for the best predictors to use in different settings, and they show that certain families of predictors can be combined to achieve even better predictive power."
SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the inherent privacy of releasing a single sample from a Dirichlet posterior distribution. In particular, the authors study the accuracy guarantees of the posterior sampling in Multinomial Dirichlett sampling and private normalized normalized histogram publishing. Specifically, they provide accuracy guarantees for the posterior distribution in multinomial-Dirichlet sampling and privacy guarantees for private normalized histograms publishing. "
SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for computing random walks in the massively parallel computation model (MPC) model, which is a theoretical abstraction of real-world MapReduce systems such as Mapreduce, MapVZ11, GS11, BKSVZ13, and DryadIBY+07. The authors show that their algorithm is both memory and round efficient, and in particular yields an efficient parallel local clustering algorithm. They also provide theoretical analysis and experimental results showing that the algorithm is significantly more scalable than previous approaches. "
SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper provides a unified statistical mechanics framework for the analysis of typical learning performance of L1-regularized M-estimators for Ising model selection on typical paramagnetic RR graphs. It is revealed that, perhaps surprisingly, the misspecified `1-LinR estimator is model selection consistent using M = O (logN) samples, which is of the same order of sample complexity as the L1 regularized logistic regression (`1-LogR) estimator. Moreover, it provides an efficient method to accurately predict the nonasymptotic behavior of the estimator for moderate M,N."
SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the problem of fuzzy k-means, which is a generalization of the well-known k-mean clustering problem. In this setting, the learner is allowed to interact with an oracle (domain expert) and asks for the similarity between a set of chosen items. The authors prove that having a few of similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. They provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynomially-complexity, where n is the number of items. "
SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method to improve model-based reinforcement learning by encouraging the learned model and value function to be jointly self-consistent. The authors propose multiple methods to achieve this goal, including a semi-gradient temporal difference objective and a value-equivalent method. The proposed method is evaluated on a number of Mujoco environments, and the results show that the proposed method outperforms the baselines."
SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper presents a careful study of sampling in the context of few-shot learning, with an eye on episodes and their difficulty. The authors propose an importance sampling-based method to compare different episode sampling schemes. Their experiments suggest that sampling uniformly over episode difficulty performs best across datasets, training algorithms, network architectures and few shot protocols."
SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of generalized linear bandits in the setting where the number of outcomes that can be selected by the user is larger than two (e.g., ‘show me later’ vs ‘never show again’, ‘click vs no-click’). In this setting, the authors use multinomial logit (MNL) to model the probability of each one of K+1 2 possible outcomes (+1 stands for the ‘not click’ outcome) and propose an upper confidence bound (UCB)-based algorithm that achieves regret $\tilde{O}(\sqrt{dK p T})$ with small dependency on problem-dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. The authors also provide numerical simulations that corroborate their theoretical results."
SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The proposed MME framework aims to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed. Numerical results show that the proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of learning the time evolution of discrete sets of items (e.g., genetic mutations) in continuous-time Markov chains, and shows that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. The authors propose an approximate likelihood maximization method that can scale to hundreds of items and is orders of magnitude faster than the existing methods. They demonstrate the effectiveness of their approach on synthetic and real-world cancer data."
SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper presents a unified pretraining framework for document understanding. It extends the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. An important feature of UDoc is that it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks."
SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in clustering problems with lp-norm objectives (e.g. k-center, k-median and k-means) in the context of data clustering. In particular, the authors consider the setting where the dataset consists of $n$ points and the goal is to find $k$ centers that satisfy the individual fairness constraint that every point $v$ has a center within a distance at most $r(v)$, where $r$ is v’s distance to its (n/k)th nearest point. The main contribution is to use Linear Programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. They prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal. Furthermore, their theoretical fairness guarantees are comparable with MV20 in theory, and empiricalically, they obtain noticeably fairer solutions."
SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper proposes a polynomial-time Gaussian sampling-based algorithms for two graph partitioning problems, namely MAX-K-CUT and MAX-AGREE. The main contributions of this paper are the following: 1. The authors propose a Gaussian-sampling-based algorithm that uses O(n + |E|) memory and nearly achieves the best existing approximation guarantees. 2. For dense graphs arriving in a stream, the authors propose to eliminate the dependence on|E| in the storage complexity at the cost of a slightly worse approximation ratio by combining our approach with sparsification."
SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a motion-aware unit (MAU) for video prediction and beyond. The proposed model can take advantage in the broadened temporal receptive field, where more temporal states can be simultaneously perceived. In particular, the proposed model are constructed based on the attention mechanism, which consists of two modules, the attention module and the fusion module. Experimental results show that the proposed MAU can outperform other state-of-the-art methods on both tasks."
SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies the sample complexity of two-layer neural network value function approximations of Q-functions in deep reinforcement learning (RL). The authors consider the setting where the function class consists of both ReLU and polynomial activation functions, and consider a generative model setting with either policy complete or Bellman complete. In this setting, the authors prove two main results: (1) a sample complexity result for generative models that scales linearly in the algebraic dimension, and (2) sample complexity results for the online RL setting with deterministic dynamics and stochastic dynamics. In both settings, sample complexity improves upon what can be achieved with linear function approximation methods."
SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper studies the generalization of deep metric learning (DML) models to out-of-distribution (OOD) data. In particular, the authors construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under OOD generalization in DML. The authors also propose few-shot DML as an efficient way to consistently improve generalization performance in response to unknown test shifts. The paper provides a thorough empirical analysis of the existing DML methods."
SP:bacff3685476855a32549d03095375649fd89df2,"This paper tackles the unsupervised outlier model selection (UOMS) problem, i.e., how to automatically select a good outlier detection algorithm and its hyperparameter(s) (collectively called a model) for a new dataset. The authors propose METAOD, a principled, data-driven approach to UOMS based on meta-learning. It leverages the performances of a large body of detection models on historical outlier discovery benchmark datasets, and carries over this prior experience to automatically selects an effective model to be employed on a new datasets without any labels, model evaluations or model comparisons. To capture task similarity within the meta learning framework, the authors introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by MetaOD significantly outperforms no model selection."
SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a novel surrogate objective framework for solving linear and quadratic programming problems with soft linear and non-negative hard constraints. This framework gives the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. The authors evaluate their method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning, demonstrating that their method outperforms traditional two-staged methods and other decision-focused approaches."
SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper proposes DropGNN, a new approach that aims to overcome the limitations of standard GNN frameworks. The idea is to execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs, and then combine the results from these runs to obtain the final result. Theoretical results show that the proposed method can distinguish various graph neighborhoods that cannot be separated by message passing GNNs, and derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and prove several properties regarding the expressive capabilities and limits of the proposed methods."
SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper presents a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, it also proposes a Dual-Stream FPN to further enhance contextual information for downstream dense predictions. It achieves state-of-the-art performance over other Vision Transformers and ResNets."
SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a framework for dynamic visual reasoning that combines visual perception, concept learning, and differentiable physics. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) based on the language, thus providing prior knowledge for the physics engine. The physics model is implemented as an impulse-based differentiable rigid-body simulator, which performs differentiable physical simulation based on grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. The proposed framework is evaluated on CLEVRER and Real-Billiards."
SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The proposed method is evaluated on CIFAR-10, MNIST, and ImageNet datasets. The results show that the proposed method outperforms existing active learning algorithms by as much as 18% in the case of rare classes and 5%-10% for out-of-distribution data."
SP:c141dc29b487ebfaa20ee50786886b0383d938bc,This paper studies the problem of identity testing for ranking data that is generated from Mallows models in the asymptotic and non-asymptotic settings. The authors propose two identity tests for Mallows model. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test and then converting it into a sample-optimal identity test. The second one is derived from an optimal learning algorithm and is easy to compute and is sample optimal for a wide range of parameters. 
SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a method for synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. The proposed method is based on a parametric 3D body model and consists of a temporal transformer that aggregates trackable visual features based on the input skeletal motion across time, a temporal transformers that aggregate spatio-temporal observations, and a multiview transformer that performs cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that the method significantly outperforms recent generalizable NeRF methods on unseen identities and poses."
SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes to search the search space of vision transformers by gradually evolving different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, it provides design guidelines of general vision transformer with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. The searched models, named S3 (short for Searching the Search Space), achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering."
SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this setting, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. The authors provide an algorithm that efficiently produces an LTF that satisfies at least (2/5)-fraction of the bags. If all the bags are non-monochromatic (i.e., bags of size two with differently labeled feature-values), the algorithm satisfies (1/2)-fractions of them. For the special case of OR over the d-dimensional boolean vectors, the authors give an algorithm which achieves an additional $\�(1/d)$ in accuracy for the two cases."
SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper presents a method to combine singular value decomposition and noise modeling to create surrogate benchmarks for neural architecture search (NAS) that output the full training information for each architecture, rather than just the final validation accuracy. The authors also introduce a learning curve extrapolation framework to modify single-fidelity algorithms, and show that it leads to improvements over popular single fidelity algorithms which claimed to be state-of-the-art upon release. "
SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a post-hoc explanation method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. The authors use the corpus to improve the user’s understanding of the latent space with two questions: (1) which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example. SimplEx provides an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, the authors propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Experiments on tasks ranging from mortality prediction to image classification demonstrate the effectiveness of the proposed method."
SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based visual embedding for vision-language pre-training (VLP) to better learn visual relation and further promote inter-modal alignment. Specifically, the authors propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., Inter-modality). They also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the intermodality learning. Experiments are conducted on Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment and Visual Reasoning. The results show that the proposed method outperforms the state-of-the-art VLP models."
SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the dynamics of the differential privacy of noisy gradient descent (GD) algorithms with smooth and strongly convex loss functions. The main result is a provably tight bound on the Rényi divergence between the pair of probability distributions over parameters of models trained on neighboring datasets, which is a significant improvement over composition theorems (which over-estimate the privacy loss by upper-bounding its total value over all intermediate gradient computations). The paper also proves optimal utility with a small gradient complexity for noisy GD algorithms."
SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes a reinforcement learning approach to speed up the convergence of the OSQP solver for solving quadratic programs (QPs). The approach uses reinforcement learning (RL) to learn a policy to adapt the internal parameters of the solver to allow for fewer iterations and faster convergence. The proposed approach is evaluated on the QPLIB, Netlib LP, and Maros-Mészáros problems. The results show that the proposed approach outperforms the state-of-the-art QP solvers."
SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the phenomenon that convolutional neural networks of different architectures learn to classify images in the same order. The authors study the over-parameterized deep linear network model and show that the convergence rate of this model’s parameters is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. They show how the PC-bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They then compare their results to the spectral bias, showing that both biases can be seen independently, and affect the order-learning in different ways. Finally, they discuss the benefits of early stopping and its connection to PCA."
SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes an approach to defend against backdoor attacks during training. The authors identify two inherent characteristics of backdoor attacks as their weaknesses: 1) backdoor examples are easier and faster to learn than clean examples, and 2) backdoor learning establishes a strong correlation between backdoor examples and the target label. Based on these two findings, the authors propose a framework - Anti-Backdoor Learning (ABL) - which consists of two stages of learning utilizing local gradient ascent (LGA) and global gradients ascent (GGA), respectively. Empirical results demonstrate that ABL is resilient to various experimental settings and can effectively defend against 10 state-of-the-art backdoor attacks."
SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a new generative implicit model for shape-aware 3D-aware image synthesis. The proposed method, called ShadeGAN, learns more accurate 3D shapes by explicitly modeling the interaction of illumination and shape, i.e., the lighting and shape. To reduce the computational cost, the authors propose a light-weighted surface tracking network, which enables an efficient volume rendering technique, achieving significant acceleration on both training and inference speed. Experiments on multiple datasets show that the proposed approach achieves photorealistic 3D aware image synthesis while capturing accurate underlying 3D shape."
SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a scalable quasi-Bayesian procedure for instrumental variable (IV) regression, building upon the recently developed kernelized IV models. The proposed approach does not require additional assumptions on the data generating process, and leads to a scalable approximate inference algorithm with time cost comparable to the corresponding point estimation methods. Empirical evaluations show that the proposed method scales to large and high dimensional datasets, and can be particularly advantageous when the instrument strength is weak."
SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a multilingual open-retrieval answer generation (CORA) model for multi-lingual open question answering (QA), which is the task of answering questions from a large collection of multilingual documents in many languages without language-specific annotated data or knowledge sources. The authors propose a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. They also propose an iterative training method that automatically extends annotated training data available only in high-resource languages to low-resource ones. The results show that CORA substantially outperforms the previous state-of-the-art on multilingual question answering benchmarks across 26 languages."
SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper investigates the role of domain adaptation theory in explaining the performance of empirical risk minimization (ERM) on domain generalization. The authors first show that adapting theory from unsupervised domain adaptation was of limited use in predicting how well models generalize out-of-distribution. Then, the authors investigate other possible measures which could explain generalization in this setting. They find that measures relating to the Fisher information, predictive entropy, and maximum mean discrepancy are good predictors of the out of distribution generalization of ERM models."
SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper provides a theoretical framework to study backdoor data poisoning attacks for classification problems. The authors identify a parameter they call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows them to argue about the robustness of several natural learning problems to backdoor attacks. They show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. They then show that backdoor filtering and robust generalization are nearly equivalent."
SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the effect of depth and width on the performance of deep neural networks. The authors propose a generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsumes neural nets. They aim to understand how width affects (standard) neural networks once they have sufficient capacity for a given modeling task. They show that depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian."
SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper proposes a general algorithmic framework called FedLin to tackle some of the key challenges intrinsic to FL, namely objective heterogeneity, systems heterogeneity, and infrequent and imprecise communication. The authors show that FedLin guarantees linear convergence to the global minimum when the clients’ local loss functions are smooth and strongly convex. They also establish matching upper and lower bounds on the convergence rate of FedLin that highlight the effects of infrequent, periodic communication. Finally, the authors show FedLin preserves linear convergence rates under aggressive gradient sparsification."
SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper proposes a method to approximate the Sliced-wasserstein distance (SW) using deterministic operations, which is computationally efficient even on high-dimensional settings and does not require any hyperparameters. The authors derive nonasymptotical guarantees for their approach, and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. They validate their theoretical findings on synthetic datasets, and illustrate the proposed approximation on a generative modeling problem."
SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to measure how related are the representations learned by neural language models, translation models, and language tagging tasks? The method is based on adapting an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. This method reveals a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embedding. The authors also show that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI."
SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. It can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional image manipulation, it is two orders of magnitude faster to produce than StyleGAN2 and preferred by 50%-60% of the human evaluators."
SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper presents a theoretical framework for self-supervised learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. The authors propose a loss function that performs spectral decomposition on the population augmented graph and can be written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by our objective can match or outperform several strong baselines on benchmark vision datasets."
SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. It follows up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. The authors show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. Then, they proceed by showing that this result can be strengthened to a localized version of the feedback edges set, and provide corresponding lower bounds that complement previous results to provide a complexity classification of the problem. Last but not least, they show how their results can be extended to the closely related problem of Polytree Learning."
SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper considers the problem of active learning in binary classification, i.e., given a training set with labels $y$ and a set of pseudo-labels $y$, the goal is to minimize the number of label requests. The authors propose an active learning algorithm called ALPS, which is a generalization of DHM (Dasgupta et al., 2008) for general surrogate losses. Theoretical analysis shows that ALPS is guaranteed to converge to the best-in-class prediction model at the same rate as passive learning, while achieving favorable label complexity guarantees under a mild assumption on the class of requester functions. Empirical results on 18 real-world datasets demonstrate that the proposed algorithm outperforms standard baselines."
SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity, Kolmogorov growth (KG), which is used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on these bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization (N2N), leads to clear improvements in the generalization ability of classifiers."
SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised learning of embeddings for image representation learning. The authors propose a method that explicitly avoids the collapse problem with two regularizations terms applied to both embedding separately: (1) a term that maintains the variance of each embedding dimension above a threshold, and (2) another term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalisation, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks."
SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper studies the problem of active reward learning, where the goal is to learn a reward model that allows standard RL algorithms to achieve high expected returns with as few expert queries as possible. To this end, the authors propose Information Directed Reward Learning (IDRL), which uses a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. The authors show that IDRL achieves similar or better performance with significantly fewer queries by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model."
SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method to predict the parameters of unseen neural networks by leveraging the past knowledge of training other networks. The authors introduce a large-scale dataset of diverse computational graphs of neural architectures and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, the authors propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second. The proposed model achieves surprisingly good performance on unseen and diverse networks. "
SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper provides a closed form expression for the distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. It shows that the DP function is quadratic, regardless of the underlying distribution. In the Gaussian setting, they further provide a closed-form expression for estimators. For general distributions, they show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer and a minimizer of the MSE under a perfect perceptual quality constraint."
SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a novel model architecture GraphFormers for textual graph representation. By having GNNs nested alongside each transformer layer of the pretrained language model, the underlying semantic of each textual node can be precisely captured and effectively integrated for high-quality textual graph representations. The authors also introduce a two-stage progressive training strategy to further strengthen GraphFormer’s representation quality; and simplify the model with the unidirectional graph aggregation, which eliminates the unnecessary computation cost. The experimental studies on three large-scale textual graph datasets verify the effectiveness of the proposed methods, where the proposed method notably outperform the existing cascaded Transformer-GNNs methods with comparable running efficiency and scalability."
SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of learning with user-level differential privacy (DP), i.e. protecting the privacy of a user’s entire contribution (m < 1 samples) rather than individual samples, which is a more stringent but realistic protection against information leaks. The authors propose and analyze algorithms to solve a range of learning tasks under user level DP. They show that for high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy, the privacy cost decreases as O(1/\sqrt{m}) as users provide more samples. In contrast, when increasing the number of users n, the cost decreases at a faster rate of O(\1/n) rate. They complement these results with lower bounds showing the minimax optimality of their algorithms for mean estimation and SCO. "
SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"This paper studies the connection between Gaussian Processes (GPs) and deep neural networks (DNNs) in the infinite width/channel limit. The authors derive a self-consistent Gaussian process theory that captures both the finite-DNN and feature learning effects. They further identify a sharp transition between a feature learning regime and a lazy learning regime in this model, which is similar to the transition associated with the Wish recovery of a low-rank matrix from the noisy matrix taken from a noisy matrix from Wish recovery matrix. They also show how their framework can be used to study statistical properties of weights in hidden layers in the CNN in the toy model."
SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the question of whether inductive biases on both the training framework and the data are needed to develop a compositional communication in emergent communication games. The authors theoretically show that inductive bias is needed for the compositionality to emerge in signaling games. They also show that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. Finally, they empirically confirm that a range of noise levels, which depends on the model and data, indeed promotes compositionality."
SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper proposes a multi-layer perceptron architecture for image classification. The proposed ResMLP consists of a linear layer, two-layer feed-forward network, and a feed-back network. The network is trained with a modern training strategy using heavy data augmentation and self-supervised training. The model is also adapted to machine translation and achieves surprisingly good results."
SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, this paper minimizes the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e., the label of a point is given by which of k centers it is closest to in Euclidean distance – the authors show that one can achieve a loss that is independent of the total number of queries. The authors also show that learning general convex sets requires an almost linear loss per query."
SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a method for knowledge transfer based on a regularization term in the loss function, supervising the sequence of required reasoning operations. The authors also provide a theoretical analysis based on PAC-learning, showing that program prediction can lead to decreased sample complexity under mild hypotheses. Experiments on the GQA dataset and the BERT-like self-supervised pre-training dataset show its effectiveness."
SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the problem of constructing RNNs that are Turing-completeness, i.e. that they can simulate a universal Turing machine (UTM) with a fixed number of neurons with unbounded precision. The authors propose a way to construct such an RNN that is Turing-complete by adding a growing memory module, which is a stack of fixed-precision neurons with pushing and popping operations. They show that such a RNN can simulate the UTM with time complexity linear in the simulated machine's time and independent of the memory size. They also prove the Turing completeness of the proposed RNN. "
SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper provides a theoretical analysis of the under-covering bias of quantile regression. The main result is to show that even in the well-specified linear setting of learning a linear quantile function, when the true data follows a Gaussian distribution, the learned quantiles are not asymptotically correct as the sample size goes to infinity or approximately correct in finite samples under specific modeling assumptions. The paper further identifies the high-dimensional estimation error in the regression coefficient as the main source of this under-coverage bias, which holds more generally on a broad class of data distributions. Experiments on simulated and real data verify the theory and illustrate the effect of various factors such as sample size and model capacity."
SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for memory allocation in the context of class-incremental learning (CIL). In CIL, in each incremental phase, the classifier is trained on new data, and the preserved data are used as exemplars for replaying during the next incremental phase. The authors propose a dynamic memory allocation strategy that is optimized for the incremental phases and different object classes. They propose two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2 allocates memory for each specific class. Experiments on CIFAR-100 and ImageNet show that the proposed method outperforms the baselines."
SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper studies the communication complexity of local SGD, which is an algorithm for speeding up stochastic gradient descent (SGD) by parallelizing it across multiple workers. In this paper, the authors propose a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. Their analysis shows that this can achieve an error that scales as 1/(NT ) with a number of communications that is completely independent of T. In particular, they show that \Omega(N) communications are sufficient. Empirical evidence suggests this bound is close to tight as they further show that $\sqrt{N} or N communications fail to achieve linear speed-up in simulations."
SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the Online Lazy Gradient Descent for linear optimization on strongly convex domains. The authors show that the algorithm is universal in the sense that it achieves $O(\log N)$ expected regret against i.i.d. opponents. This improves upon the more complex meta-algorithm of Huang et al. [20] that only gets $O(\sqrt{N})$ regret against adversarial opponents. Moreover, the authors also show that, unlike for the simplex, order bounds for pseudo-regret and expected regret are equivalent for strongly-convex domains, and for various types of noise on unit balls."
SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper compares the Bures-Wasserstein (BW) geometry with the Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. It shows that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. The paper also shows that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesics convex in the BW geometry. Extensive experiments on various applications support the findings."
SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper introduces Dynaboard, an evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. The platform evaluates NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences."
SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a multi-modal text-to-speech (TTS) model for automatic video dubbing (AVD) task: synthesizing human speech synchronized with the given video from the text. The proposed model uses the lip movement in the video to control the prosody of the generated speech. An image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments on the chemistry lecture single speaker dataset and LRS2 multi speaker dataset show that the proposed model can generate speech audios on par with state-of-the-art TTS models in terms of speech quality."
SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a new federated learning (FL) framework based on the recently proposed Vision Transformer (ViT) architecture for split learning. The ViT is a Transformer-based deep learning architecture with straightforward decomposable configuration, which is ideally suitable for FL without sacrificing performance. The authors propose a novel Federated Split Task-Agnostic (FESTA) framework suitable to leverage the formidable benefit of ViT to simultaneously process multiple CXR tasks including the diagnosis of COVID-19. The results affirm the suitability of Transformer for collaborative learning in medical imaging."
SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes Shape-As-Point (SAP), a differentiable point-to-mesh representation for 3D surface reconstruction. The main idea is to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. The paper also introduces a Poisson solver which performs fast GPU-accelerated Differentiable Poisson Surface Reconstruction (DPSR) and solves for an indicator function from an oriented point cloud in a few milliseconds. The method is evaluated on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction."
SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a method for training image representation networks that are robust to various distortions on the input data. Specifically, the authors propose a method to train an encoder S(A(x)) that is trained to approximate the behavior of the pre-trained CLIP encoder R(x) on clean images, and the student encoder is trained on corrupted images. The proposed method is evaluated on a subset of ImageNet and achieves a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions."
SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper investigates the use of reinforcement learning for structural credit assignment in neural networks. The authors formalize training a neural network as a finite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. They show that the standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions. They introduce an off-Policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents."
SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"This paper investigates whether a single model with a single loss function can capture the properties of both the ventral and the dorsal pathways of the visual system of mice. The authors propose a self-supervised predictive loss function (contrastive predictive coding) to induce representations that match mouse visual cortex. They show that when we train a deep neural network architecture with two parallel pathways using a contrastive predictive loss, we can outperform other models in fitting mouse visual cortices. Moreover, they can model both the dorsal and ventral pathways. These results demonstrate that a self supervised predictive learning approach applied to parallel pathway architectures can account for some of the functional specialization seen in mammalian visual systems."
SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper introduces TopicNet, a deep hierarchical topic model that can inject prior structural knowledge as an inductive bias to influence the learning of the topic hierarchy. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. The model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document mining."
SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes an object-level self-supervised pretraining method named Selective Object COntrastive learning (SoCo), which aims to align pretraining to object detection. Different from prior image-level contrastive learning methods which treat the whole image as an instance, SoCo treats each object proposal generated by the selective search algorithm as an independent instance. The proposed method achieves state-of-the-art performance on COCO detection using a Mask R-CNN framework."
SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"This paper presents a learning-augmented local search framework to solve large-scale VRP. The method iteratively improves the solution by identifying appropriate subproblems and delegating their improvement to a black box subsolver. The proposed method accelerates competitive VRP solvers on problems of sizes up to 3000, requiring an order of magnitude less computation time."
SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for Bayesian continual learning based on active forgetting inspired by biological active forgetting. Specifically, the authors propose Active Forgetting with Synaptic Expansion-Convergence (AFEC), which dynamically expands parameters to learn each new task and then selectively combines them, which is formally consistent with the underlying mechanism of biological neural networks. The method achieves the SOTA performance on a variety of continual learning benchmarks through effectively improving the learning of new tasks and boosts representative continual learning strategies."
SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for the prior guided random gradient-free (PRGF) algorithms. Moreover, they present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks and adversarial attacks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper provides a theoretical analysis of learning one-hidden-layer pruned neural networks, which offers formal justification of the improved generalization of winning tickets observed from empirical findings in the Lottery Ticket Hypothesis (LTH). The authors characterize the impact of the number of remaining weights in a pruned network on the required number of samples for training, the convergence rate of the learning algorithm, and the accuracy of the learned model. They also provide extensive numerical validations of their theoretical findings."
SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper studies the problem of differentially private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries. The authors propose two new methods. The first method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy."
SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a unified approach for semantic segmentation and instance-level segmentation. The authors propose a simple mask classification model that predicts a set of binary masks, each associated with a single global class label prediction, to solve both semantic and instance level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. The proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation task and shows excellent empirical results."
SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies the problem of finding adversarial examples in random two-layer ReLU neural networks with smooth and ReLU activation functions. The authors extend the result of Daniely and Schacham [2020] to the overcomplete case, where the number of neurons is larger than the dimension (yet also sub-exponential in the dimension). In this case, they prove that a single gradient step of gradient descent suffices. They also show this result for any subexponential width random neural network with smooth activation function."
SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) that trains SGMs in the latent space of a variational autoencoder (VAE). The authors claim that this allows them to train more expressive generative models, apply SGMs to non-continuous data, and reduce the number of network evaluations and sampling time. To enable training LSGMs end-to-end in a scalable and stable manner, they introduce a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset."
SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper proposes a new explanation for the performance gap between neural networks and kernel methods in the image classification setting, based on finding a sparse signal in the presence of noise. Specifically, they prove that for a simple data distribution with sparse signal amidst high-variance noise, a simple convolutional neural network trained using SGD simultaneously learns to threshold out the noise and find the signal. On the other hand, the corresponding neural tangent kernel, with a fixed set of predetermined features, is unable to adapt to the signal in this manner. They supplement their theoretical results by demonstrating this phenomenon empirically: in CIFAR-10 and MNIST images with various backgrounds, as the background noise increases in intensity, a CNN’s performance stays relatively robust, whereas the corresponding Neural Tangent Kernel (NTK) sees a notable drop in performance. They therefore propose the local signal adaptivity (LSA) phenomenon as one explanation why neural networks outperform kernel methods."
SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies the convergence of the gradient tracking (GT) algorithms for decentralized stochastic gradient descent (D-SGD, GT and D2) in the strongly convex, convex and non-convex settings. The authors provide a tighter analysis of the GT method in the convex setting, and improve the dependency on the mixing parameter p (related to the spectral gap of the connectivity matrix) from $p-2$ to $p^{-1/\epsilon}$ in the noiseless case and $p_{-3/2}$, $p_1/2$ in general. This improvement was possible due to a new proof technique which could be of independent interest. "
SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm for the stochastic multi-armed bandit problem. The authors show that the arm-sampling rates under UCB are asymptotically deterministic, regardless of the problem complexity. The paper also provides the first complete process-level characterization of the MAB problem with UCB in the conventional diffusion scaling. "
SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper focuses on the cross-domain cold-start recommendation (CDR) problem, where two domains share the same user set but different items, and both domains have auxiliary representations such as item profiles or descriptions. The key to the CDCSR problem is to reduce the discrepancy between the latent embedding distributions across domains. To address this issue, the authors propose a framework called DisAlign, which utilizes both rating and auxiliary representations from the source domain to improve the recommendation performance of the target domain. Specifically, they first propose Stein path alignment for aligning the latents across domains, and then further propose its improved version, i.e., proxy Stein path, which can reduce the operation consumption and improve efficiency. The empirical study on Douban and Amazon datasets demonstrates that the proposed model significantly outperforms the state-of-the-art models."
SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a simple yet computationally efficient architecture that learns long-term spatial dependencies in the frequency domain with log-linear complexity. The architecture replaces the self-attention layer in vision transformers with 3 key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and 2D inverse Fourier transforms. Experiments on both ImageNet and downstream tasks demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency."
SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of predicting trustworthiness on a large-scale dataset. The authors observe that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy due to overfitting. To improve the generalizability of the predictors, the authors propose a novel steep slope loss to separate the features w.r.t. correct predictions from the ones w.rt. incorrect predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, ie., Vision Transformer and ResNet, as trustworthiness predictions."
SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a new approach for learning end-to-end reconstruction operators based on unpaired training data for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then solved iteratively till convergence. Notably, it takes significantly fewer iterations to converge as compared to variational methods, thanks to the excellent initialization obtained via the unrolling operator."
SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes ALBEF, a new framework for vision-language representation learning. The proposed method first aligns the unimodal image representation and text representation before fusing them with a multimodal encoder. To improve learning from noisy web data, the authors propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. Theoretical analysis shows that different training tasks can be interpreted as different ways to generate views for an image-text pair. Experiments on VQA and NLVR2 demonstrate the effectiveness of the proposed method."
SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper considers the problem of offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decision making policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. They also establish the nonparametric consistency of the estimators under quite mild assumptions."
SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper considers the non-smooth convex stochastic convex optimization problems with heavy-tailed (heavy-tailed) noise and non-sub-Gaussian (sub-gaussian) noise distribution. The authors provide the first high-probability convergence results with logarithmic dependence on the confidence level for such problems. They propose novel stepsize rules for two methods with gradient clipping. Moreover, their analysis works for generalized smooth objectives with Hölder-continuous gradients and for strongly convex problems."
SP:a22a893e25ce739dc757861741014764e78aa820,"This paper studies the long-term forecasting problem of time series, which is a pressing demand for real-world applications. The authors propose the Autoformer as a decomposition architecture by embedding the series decomposition block as an inner operator, which can progressively aggregate the long term trend part from intermediate prediction. Besides, the authors design an efficient Auto-Correlation mechanism to conduct dependencies discovery and information aggregation at the series level, which contrasts clearly from the previous self-attention family. The proposed model achieves state-of-the-art performance on six benchmarks, covering five practical applications."
SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper introduces a new dataset of cryptic crossword clues, which is composed of two parts: a definition and a wordplay cipher. The authors also propose a curriculum approach, in which the model is first fine-tuned on related tasks as unscrambling words. They also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies."
SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper investigates the similarities and differences between the internal representations of ViTs and CNNs on image classification tasks. They find that ViTs have more uniform representations across all layers. They also find that skip connections in ViTs are even more influential than in ResNets, having strong effects on performance and representation similarity. Finally, they study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as MLP-Mixer."
SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) in combinatorial multi-armed bandit (CMAB) problems under the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) optimization problems. It provides a problem-dependent regret lower bound of order $\Omega(\log T/2)$ and an almost matching regret upper bound. These are the first theoretical results for TS to solve CMAB with a common approximation oracle and break the misconception that TS cannot work with approximation oracles."
SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning, i.e., the distributed learning setting where each of the agents has access to local data and the goal is to jointly learn a global model. The authors propose a notion of optimality given by the average error rates among federating agents (players). They provide and prove the correctness of an efficient algorithm to calculate an optimal (error minimizing) arrangement of players. They analyze the relationship between the stability and optimality of an arrangement. "
SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction, canonicalization, and unsupervised classification. The main idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a capsule decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. The results show that the proposed method outperforms the state-of-the-art."
SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper studies the generalization properties of feature averaging in kernel ridge regression when the target is invariant to the action of a compact group. The paper builds on the work of Elesedy and Zaidi [8] to derive a strictly non-zero generalization benefit of incorporating invariance in the feature averaging method. The authors show that generalization is governed by a notion of effective dimension that arises from the interplay between the kernel and the group, and show that the action induced by the group induces an orthogonal decomposition of the reproducing kernel Hilbert space and its kernel."
SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a method to combine neural networks with a generative model to constrain neural network activations to “decode” back to inputs. This design enables a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. The authors demonstrate applications of this uncertainty to out-of-distribution detection, adversarial example detection, and calibration — while matching standard neural networks in accuracy. They further explore this compositionality by combining this design with pretrained models, where they show promising results that neural networks can be regularized from using protected features."
SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the rates of convergence of plug-in estimators of the optimal transport map (OT) between two probability distributions (mu and y) on the Euclidean space. The main contribution is a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug in estimators. The authors show that when \mu and \� are chosen as the empirical distributions supported by the standard empirical distributions on m and n atoms, i.e., \mu \in \mathbb{R}^m and \nabla \in x_n \in y_n, the rate of convergence is faster than for natural discretediscrete/semi-discrete estimators and for kernel smoothed or wavelet based estimators, respectively. "
SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. The authors show that using only 10 datapoints (0.02% of original dataset), they obtain over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best best test accuracy of 40%. The authors also perform some preliminary analyses of the distilled datasets to shed light on how they differ from naturally occurring data."
SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes an approach for semi-supervised learning with unlabeled data that can contain outliers, i.e., samples of novel categories unseen in the labeled training set. The authors propose a novel approach called OpenMatch, which unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open-set soft-consistency regularization loss, which enhances the smoothness of the OVA classifier with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets."
SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes Latent Explorer Achiever (LEXA), an agent for unsupervised RL that explores its environment, learns to achieve the discovered goals, and solves image-based tasks in a zero-shot way. LEXA learns a world model from image inputs and uses it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. The proposed method is able to solve challenging downstream tasks specified as images without any supervision as rewards or demonstrations."
SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,This paper proposes a method for reducing the number of parameters in transformer-based language models. The authors propose to use a low-rank factorized representation of a reshaped and rearranged original matrix to achieve space efficient and expressive linear layers. Theoretical results are provided to show the effectiveness of the proposed method. Experiments are conducted on Transformer-based models on a range of machine translation tasks.
SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,"This paper proposes to incorporate path encoding into the attention module of Transformer. Specifically, the authors propose to encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. The authors also explore the interaction between these two kinds of paths by integrating them into the unified Transformer framework. The detailed empirical study for path encoding methods also leads to the novel state-of-the-art representation model TPTrans, which finally outperforms strong baselines."
SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a Transformer-based generator for high-resolution image generation based on GANs. To address the quadratic scaling problem of Transformers, the authors structure the low-resolution stages of HiT following the design of Nested Transformer and enhance it by the proposed multi-axis blocked self-attention. In order to handle extremely long inputs in the high resolution stages, HiT also reduces the model into implicit functions. The experiments demonstrate that HiT achieves state-of-the-art FID scores on unconditional ImageNet 128x128 and FFHQ 256x256."
SP:41a6753bc56eb16040600666a859294ae36cfa9c,"This paper studies the query complexity of active learning of geodesically convex halfspaces on graphs. Geodesic convexity is a generalization of Euclidean convexness and allows the definition of convex sets and halfspace on graphs, which is a natural generalization to convex interval spaces. The authors prove an upper bound on the query complexities which is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They also show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. "
SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method to improve the performance of temporal action localization (TAL) models. The key idea is to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoders and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Experiments show that the proposed LoFi optimization approach can significantly enhance performance of existing TAL methods."
SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The main contributions are three-fold. First, it provides general formulae for the derivatives of regularized M-stimators where differentiation is taken with respect to both y and X; this reveals a simple differentiability structure shared by all convex regularized estimators. Second, it characterizes the distribution of the residual ri = yi^{i-xi\beta} in the intermediate high-dimensional regime where dimension and sample size are of the same order. Third, it proposes a novel adaptive criterion to select tuning parameters of regularised estimators, which can be used as a proxy for minimizing the out-of-sample error."
SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a calibrated beam-based algorithm with awareness of the global attention distribution for neural abstractive summarization, aiming to improve the local optimality problem of the original beam search in a rigorous way. Specifically, a global protocol is proposed based on the attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring mechanism is then developed to regulate beam search to generate summaries in a near-global optimal fashion. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyperparameters."
SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a transformer-based attention mechanism that is equivariant to the orientation of local coordinate systems (i.e., gauge-equivariant) and rotation-invariant. The proposed method employs multi-head self-attention to jointly incorporate both position-based and content-based information. To enhance expressive ability, the authors adopt regular field of cyclic groups as feature fields in intermediate layers, and propose a novel method to parallel transport the feature vectors in these fields with any rotation angles. Extensive experiments show that the proposed method achieves state-of-the-art performance on two common recognition tasks."
SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models by combining the expectation maximization and the Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The authors put emphasis on generality of the method, equipping it with the ability to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations. The performance of the proposed method is illustrated in a variety of synthetic and real-data contexts, such as mixtures of normalizing flows and sum-product (transform) networks."
SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for training sparse neural networks. The authors formulate the training process as a continuous minimization problem under global sparsity constraint, and separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, the authors use the conventional chain rule, which can be sparse via exploiting the sparse structure, and for the latter step, instead of using the chain rule based gradient estimators as in existing methods, they propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, the proposed method is much more effective in accelerating the training processes."
SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper proposes a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) based on the non-equilibrium orbits (NEOs) for normalizing constant estimation and sampling from complex distributions, respectively. The authors also propose a novel sampling-importance resampling mechanism to sample from the target distribution. Theoretical analysis of the proposed methods is provided."
SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,This paper studies the problem of large-scale mini-batch set encoding. The authors propose a new property termed Mini-Batch Consistency (MBC) that is required for large scale mini- batch set encoding and propose a scalable and efficient attention-based set encoding mechanism. The proposed method adheres to the required symmetries of permutation invariance and equivariance as well as maintaining MBC for any partition of the input set. The experiments show that the proposed method is computationally efficient and results in rich set encoding representations for set-structured data.
SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes an algorithm for action exploration and equilibrium approximation in games with combinatorial action spaces. The main idea is to train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. Additionally, the authors extend their methods to full-scale no-press Diplomacy. They show that the proposed method is compatible with human-data bootstrapped agents."
SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling, where the key challenge is to maximize positive transfer and mitigate negative interference across languages and domains. The authors claim that non-selective attention sharing is sub-optimal for achieving good generalization across all languages/domains, and propose attention sharing strategies to facilitate parameter sharing and specialization. The proposed method is evaluated on various tasks including speech recognition, text-to-text and speech to text translation."
SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift in the setting of random feature regression, i.e., when the distribution of the training and test sets differ while the conditional label distributions remain the same. The authors provide exact asymptotic analysis of the limiting test error, bias, and variance in this setting. They also show that overparameterized models exhibit enhanced robustness to covariate shifts, and that a linear relationship exists between the generalization error on shifted and unshifted data."
SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the sensitivity of Thompson Sampling (TS) and other Bayesian sequential decision-making algorithms to prior misspecification. The authors prove that the expected reward of TS with a misspecified prior is $\tilde{O}(\epsilon^{-1}(H^2)$ (where H is the learning horizon) worse than that of TS when the prior is well-specified. They also show that the sensitivity is independent of the cardinality or structure of the action space and tight up to universal constants in the worst case. Finally, they establish generic PAC guarantees for algorithms in the Bayesian meta-learning setting and derive corollaries for various families of priors."
SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"This paper proves an exponential separation between PAC-learning and Equivalence-Query-learning (EQ-learning) models, where the learner is given i.i.d. samples from an unknown distribution and is tasked with finding a hypothesis with low generalization error. In the PAC model, all samples are provided at the beginning of the learning process, while in the EQ-learning model the samples are acquired through an interaction between a teacher and a learner. The authors prove that in order to achieve an error $\� exponentially (in $\�) fewer samples suffice than what the PAC bound requires. The paper also shows that adversarial training with on-manifold adversarial examples aids generalization compared to standard training."
SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper proposes Scalable Diverse Model Selection (PARC), a scalable model selection method that can query a large set of pre-trained models and select a small subset of them to perform fine-tuning on a set of target data. The authors propose several benchmarks for evaluating on this task. They find that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. They introduce simple techniques to improve the performance and speed of these algorithms. They iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes Learning Low-dimensional Binary Codes (LLC), a method for learning binary embeddings for both instances and classes. The proposed method learns both class and instance codes via the standard classification task and its setup without any side-information. In the first phase, LLC learns low-dimensional (k-bit) binary codes for classes that capture semantic information through a surrogate task, and in the second phase, it uses these learnt class codes as an efficient alternative to learning the instance codes. The method is evaluated on the efficient image retrieval and out-of-distribution (OOD) detection problems."
SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. GDWS is able to improve the throughput (frames-per-second) on real-life hardware while simultaneously preserving robustness to adversarial perturbations. The paper also provides exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. Experiments on CIFAR-10, SVHN, and ImageNet datasets demonstrate the effectiveness of GDWS."
SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a semi-template-based retrosynthesis model for single-step organic synthesis, where the goal is to identify precursor molecules that can be used to synthesize a target molecule. The authors propose a graph-based approach that capitalizes on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and also amenable to manual correction. The proposed model achieves a top-1 accuracy of 53.7% on the USPTO-50 benchmark dataset."
SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a Bayesian formulation of statistical downscaling, i.e., the problem of combining low-resolution (LR) data with high-resolution information, often known as statistical down-scaling. In this paper, the authors propose a conditional mean embedding estimator for multiresolution data. By treating conditional expectations as inter-domain features of the underlying field, a posterior for the latent field can be established as a solution to the deconditioning problem. The authors show that this solution can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions. "
SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,This paper proposes a neural architecture search method for deep sparse networks (DSNs). The authors propose a distilled search space to cover the desired architectures with fewer parameters and develop a progressive search algorithm for efficient search on the space. Experiments on three real-world benchmark datasets show promising results of the proposed method.
SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper considers the problem of fine-tuning a pre-trained model on a target task with a small amount of labeled data, where the capacity of the model is much larger than the size of the target data set. The authors propose a regularized self-labeling approach that improves the generalization and robustness properties of finetuning. Specifically, they propose to use layer-wise regularization to constrain the distance traveled in each layer; and self label-correction and label-reweighting to correct mislabeled data points and reweight less confident data points. They validate their approach on an extensive collection of image and text data sets."
SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the distribution (or arms) with the smallest CVaR, VaR, or weighted sum of CVaRs and VaRs in a stochastic multi-armed bandit (MAB) setting. The main contribution is an optimal $\delta$-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as \delta approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, which requires delicate analysis. The authors develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures."
SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes a new Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE, which is composed of two spatial spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, it has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed method."
SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tune process, whereas a conventional one only uses the pretrained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks."
SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper proposes a Stochastic Anderson Mixing (SAM) scheme to solve nonconvex stochastic optimization problems. The authors prove the convergence of SAM, including the almost sure convergence to stationary points and the worst-case iteration complexity. Moreover, the complexity bound can be improved when randomly choosing an iterate as the output. To further accelerate the convergence, the authors incorporate a variance reduction technique into the proposed SAM. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability."
SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a novel stochastic algorithm for solving general linear inverse problems, where the observation is assumed to be contaminated by additive white Gaussian noise. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. The authors demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing."
SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a framework named MetaHG to detect illicit drug traffickers on social media (i.e., Instagram) by jointly modeling multi-modal content and relational structured information. The authors first build a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on Instagram. Then, they employ a relation-based graph convolutional neural network to learn node representations over the built HG, introduce graph structure refinement to compensate the sparse connection among entities in the HG, and propose a meta-learning algorithm for model optimization. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments based on the real-world data collected from Instagram demonstrate that the proposed model outperforms state-of-the-art methods."
SP:242da1384f48260d58a0e7949438611c05079197,"This paper investigates the question of whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). The authors use techniques from mixed-integer optimization, polyhedral theory, and tropical geometry to provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. The authors also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes."
SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. In particular, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as a min-min problem by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations. Extensive experiments demonstrate that the approach leads to substantial attack improvement over existing heuristic strategies and robustness improvement over state-of-the-art defense methods trained to be robust against multiple perturbations types."
SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis for the spiked Wigner tensor model, which is a generalization of both sparse PCA and tensor PCA. The goal is to recover the $k$-sparse unit vector $x(q,p)$ for a given $Y = W + \lambdax(p)$, where $W$ is a Gaussian noise tensor with i.i.d. Gaussian entries. The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm for the highly sparse regime of $k \leq \sqrt{n}$. For any $1\leq k$, the algorithms recovers the sparse vector for signal-tonoise ratio $\lambda$ in time $\tilde O(k/t)$ capturing the state-of-the-art guarantees for the matrix settings (in both the polynomially-time and sub-exponential time regimes). The results naturally extend to the case of r distinct $k\sparse$ signals with disjoint supports, with guarantees that are independent of the number of spikes. The algorithm improves over known tensor tensor algorithms whenever the signal vector is highly sparse."
SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. Experiments are conducted on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and geometric task of mesh transfer between 3D shapes."
SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of finding the quantal response equilibrium (QRE) in two-player zero-sum matrix games and Markov games with entropy regularization. In particular, the authors propose a decentralized algorithm (PU and OMWU) that is guaranteed to converge linearly to the QRE at a linear rate. The authors also propose an algorithm (OMWU) for solving entropy-regularized Markov Games with linear rates. The convergence rates are independent of the size of the state and action spaces up to logarithm factors."
SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,This paper proposes an implicit transformer super-resolution network (ITSRN) for SCISR. The proposed method is based on the idea of point-to-point function to represent images with continuous coordinates and directly maps the coordinates to image implicit function. The authors also propose an implicit position encoding scheme to aggregate similar neighboring pixel values to the query one. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs.
SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method to combine sum-product networks (SPNs) and probabilistic models (BCNs) for learning interventional distributions using sumproduct neural networks (NNs) that are over-parameterized by gate functions, e.g., neural networks. The proposed method is motivated and illustrated by a structural causal model themed around personal health. The empirical evaluation against competing methods from both generative and causal modelling demonstrates that interventional SPNs indeed are both expressive and causally adequate."
SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative Markov model for functional magnetic resonance imaging (fMRI) data. The proposed model is based on a chain of low dimensional temporal embeddings connected by neural networks and conditioned on a discrete latent as a state-space embedding for temporal weights to account for nonlinear dynamics, enable data clustering in a low dimensional space, and provide informative visualizations about data. Experimental results on both synthetic and real fMRI data demonstrate the capacity of DMFA in revealing interpretable clusters and capturing nonlinear temporal dependencies in these high dimensional imaging data."
SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper introduces a new class of generative models within the family of continuous normalizing flows (CNF) called Moser Flows (MF), which is a generalization of FFJORD (Grathwohl et al., 2018) and Riemannian CNFs (Mathieu and Nickel, 2020) that operate in the Euclidean submanifolds of the manifold space. The authors show that MF is a universal density approximator under suitable assumptions. Theoretically, they prove that MFs are universal generative model over Euclid manifolds. Empirically, the authors demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNF models."
SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of identifiability in the context of unsupervised representation learning. In particular, the authors consider the nonlinear ICA setting, where the mixing function is assumed to be non-linear. The authors propose an approach based on the principle of independent causal mechanisms (ICM), which is motivated by thinking of each source as independently influencing the mixing process. They provide theoretical and empirical evidence that their approach circumvents a number of nonidentifiability issues arising in nonlinear blind source separation. "
SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a differentiable variant of Annealed Importance Sampling (AIS) with Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing (UHA). UHA is a variant of Hamiltonian AIS that uses uncorrected MCMC kernels instead of accept-reject steps. The authors show that UHA leads to tight and differentiable lower bounds on the log-likelihood of the unnormalized target distribution $p(z) = p(z)/Z$. They also show that maximizing the ELBO is equivalent to minimizing the KL-divergence from $q$ to $p$. They show that tuning the parameters of UHA can be done using reparameterization gradients."
SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes an efficient and trainable local Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Specifically, when computing the induced norm of a weight matrix, the authors eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global Lipchitz constant of the neural network. The authors also propose to clip activation functions with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local upper bound. Experiments on MNIST, CIFAR-10 and TinyImageNet show the effectiveness of the proposed method."
SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper proposes a scalable method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. Conformal inference provides finite sample frequentist guarantees on predictive confidence intervals without the requirement of model fidelity. Using ‘add-one-in’ importance sampling, the authors show that conformal predictive intervals are efficiently obtained from re-weighted posterior samples of model parameters. The approach contrasts with existing conformal methods that require expensive refitting of models or data-splitting to achieve computational efficiency. The authors demonstrate the utility on a range of examples including extensions to partially exchangeable settings and hierarchical models."
SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper introduces potential-driven denoisers, i.e., gradients of smooth scalar-valued deep neural networks, acting as potentials, for solving general inverse problems. The authors show that the proposed potentials exhibit symmetric Jacobians, allowing for MAP and MMSE estimators interpretation; and can be integrated into regularization-by-denoising (RED) and plug-and-play priors (PnP) schemes with backtracking step size, removing the need for enforcing the Lipschitz constant of the potentials. The paper also provides a simple inversion method that utilizes the proposed models, and theoretically establishes its convergence to stationary points of an underlying objective function consisting of the learned potentials; and empirically shows improved performance compared to standard RED and PnP."
SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a generative model for generating music that is in-sync with human body movements. The proposed method, called RhythmicNet, is based on the idea of music improvisation in which a rhythm of movements is established and is translated to drumming music with potentially additional accompanying instruments. The main contribution of the paper is the use of skeleton keypoints for generating the music beat and the style pattern from body keypoints per each frame to produce the rhythm. The paper also implements a transformer-based model to generate the hits of drum instruments and implements a U-net-based models to generate velocity and the offsets of the instruments. "
SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,"This paper proposes a simple one-step algorithm for offline reinforcement learning. The algorithm is based on the behavior Q function, which estimates the Q-values of the behavior policy. The authors show that the proposed algorithm outperforms the existing iterative approaches on the D4RL benchmark. They hypothesize that the strong performance is due to a combination of favorable structure in the environment and behavior policy, as well as favorable distribution shift and dynamic programming."
SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper considers the problem of low-rank and nonsmooth matrix optimization, where the objective function is convex but non-smooth. The authors prove that the extragradient method, when initialized with a “warm-start-point”, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which we need to initialize the method. Finally, they support their theoretical results with empirical experiments on several non-convex matrix recovery tasks."
SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a method to estimate the conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). The authors propose a generalized Robinson decomposition (GRD) to isolate the causal estimand (reducing regularization bias), allows one to plug in arbitrary models for learning, and possesses a quasi-oracle convergence guarantee under mild assumptions. In experiments with small-world and molecular graphs, the authors demonstrate that their approach outperforms prior work in CATE estimation."
SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper studies the problem of causal effect identification, i.e., determining whether a causal effect is computable from a combination of qualitative assumptions about the underlying system (e.g., a causal graph) and distributions collected from this system. The authors develop a new causal identification algorithm which utilizes both graphical criteria and matrix equations. Specifically, they characterize the relationships between certain graphically-driven formulae and matrix multiplications. They further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a new algorithm, which accepts as input a collection of marginal, conditional, and interventional distributions, integrating enriched matrix-based criteria into a graphical identification approach."
SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper proposes a dual curriculum learning approach for unsupervised environment design (UED) based on prior work, Prioritized Level Replay (PLR). The authors argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods they call Dual Curriculum Design (DCD), which includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows them to develop a version of PLR with a robustness guarantee at Nash equilibria. Furthermore, their theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibrium. "
SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper considers the multi-armed bandit problem in the shuffle model, where the arms are randomly sampled from a set of arms and the goal is to maximize the cumulative reward. The authors propose two algorithms, SDP-AE and VB-AE, which are batched variants of the non-differentially private (AE) algorithm by Dworkin et al. (2018) that use a private binary summation mechanism for the shuffle models as a building block. The main contributions of the paper are the following: (1) The authors provide a distribution-dependent regret upper bound of $O(\epsilon^2 + \sqrt{k^1/\delta^2})$, where $\delta$ is the suboptimality gap of the arms, $T$ is number of rounds, and $k$ is total number of arms. The upper bound matches the upper bound for the best known algorithms for the centralized model. (2) The algorithms have distribution-independent regret of $\sqrt{\log T} + \log T + k^1\log 1/\varepsilon^2$ and distribution-agnostic regret of $\tilde{O}(\log T) + k\log T^1)$. (3) The algorithm is a batched variant of the AE algorithm, which uses a differentially private binary summing mechanism. (4) The first algorithm has an additive factor of $k \log 1\log^1 \delta \epsilons^2$. (5) The second algorithm uses exponentially growing batches to improve the additive factor. "
SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated Forecaster. The procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, the proposed method improves decision loss prediction without compromising the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a general method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The proposed method optimizes a set of centroids to compactly approximate the arg max distribution with a simple objective function. Theoretically, the argmax centroid method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth argmin distribution and the centroid approximation under proper conditions. The authors demonstrate the applicability and effectiveness of the proposed method on a variety of real-world multitask learning applications, including few-shot image classification, personalized dialogue systems and multi-target domain adaptation."
SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper studies the problem of multi-objective reinforcement learning (MORL) in the online and reward-free settings. In the online setting, the preferences are adversarially presented and the agent receives a preference every episode and proposes policies to interact with the environment. The authors provide a model-based algorithm that achieves a nearly minimax optimal regret bound. The paper also provides an information-theoretic lower bound to justify the near-tightness of the proposed algorithms."
SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a local explanation method for dialogue response generation (LERG) that aims to explain the reasoning process of a generation model by the mutual interaction of segments in input and output sentences. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. The authors show that the method adheres to desired properties of explanation for text generation, including unbiased approximation, consistency, and cause identification. The method consistently improves other widely used methods on proposed automatic evaluation metrics for this new task by 4.4-12.8%."
SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the problem of gradient compression in distributed optimization. In particular, the authors focus on developing efficient distributed methods that can work for any compressor satisfying a certain contraction property, which includes both unbiased (after appropriate scaling) and biased compressors such as RandK and TopK. The authors propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. They show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes an astrocyte-modulated liquid state machine (NALSM) that combines critical branching dynamics and spike-timing-dependent plasticity (STDP) to improve the performance of the LSM. The authors show that the astroCyte model integrates neuronal activity and provides global feedback to spike-time dependent plasticity, which self-organizes NALSM dynamics around a critical branching factor that is associated with the edge-of-chaos. They demonstrate that the proposed method achieves state of the art accuracy on MNIST, N-MNIST and Fashion MNIST."
SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper studies the problem of topology-imbalance node representation learning (TINL), i.e., the imbalance caused by the asymmetric topological properties of the labeled nodes in graph data. The authors propose a label propagation algorithm to jointly analyze the quantity and topology imbalance issues by considering the node influence shift phenomenon with the Label Propagation algorithm. They devise an influence conflict detection–based metric Totoro to measure the degree of graph topological imbalance and propose a model-agnostic method ReNode to reweight the influence of labeled nodes adaptively based on their relative positions to class boundaries. Extensive experiments demonstrate the effectiveness and generalizability of the proposed method in relieving topology balance issue and promoting semi-supervised node classification."
SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery of piece-wise constant signals corrupted by additive Gaussian noise over a d-dimensional lattice. The authors propose to use the DCART-based procedure to estimate the partition of the lattice induced by the constancy regions of the unknown signal using the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. They prove that, under appropriate regularity conditions on the shape of the partition elements, the proposed method consistently estimates the underlying partition at a rate of order $\sigma^2k^3 \log(N)/\kappa$, where k is the minimal number of rectangular sub-graphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, $\kappa$ is the smallest magnitude of the signal difference among contiguous contiguous elements, N is the size of lattice, and $d \in \mathbb{D}$. The authors further extend to the partition estimator based on the optimal regression tree estimator (ORT) and to the one obtained through an NP-hard exhaustive search method. "
SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method for counterfactual maximum likelihood estimation (CMLE) based on the Structural Causal Model (SCM) to reduce the spurious correlations caused by observed confounders. The authors provide theoretical analysis on the underlying general SCM and propose to perform MLE on the interventional distribution instead of the observational distribution. They derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning models using observational data. They conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. They show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations, while maintaining comparable performance on the regular evaluations."
SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper proposes a multi-objective optimization method for multi-task learning (MTL) that optimizes the minimum decrease rate of any specific task’s loss while still provably converging to the optimum of the average loss. The main idea is to leverage the worst local improvement of individual tasks to regularize the algorithm trajectory. The proposed method generalizes the gradient descent and multiple gradient descent algorithm, and demonstrates improved performance compared to the state-of-the-art methods."
SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of language models and machine teaching. The authors frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concepts from small witness sets. In particular, they explore how several GPT architectures, program induction systems and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs. This first joint analysis of machine teaching can address key questions for artificial intelligence and machine learning, such as whether some strong prior, and Occam's razor in particular, can be distilled from data, making learning from a few examples possible."
SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method to identify robust and non-robust features in the feature space of deep neural networks (DNNs) that can be a primary cause of the adversarial examples. Specifically, the authors inject noise variation to each feature unit and evaluate the information flow in feature representation to dichotomize feature units either robust or not robust, based on the noise variation magnitude. They demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism that intensifies the gradient of robust features that is directly related to the model prediction."
SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the connection between the support vector machine (SVM) and the ordinary least squares (OLS) in the high-dimensional regime. The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. They also identify a sharp phase transition in Gaussian feature models and bound the width of this transition. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM, and present a new geometric characterization of the problem that may elucidate this phenomenon."
SP:99f226a63902863c429cb7baefab09626d13921e,This paper studies the problem of finding the best policy in Markov Decision Processes (MDPs). The authors propose a lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. They also provide the first algorithm with an instance-specific sample complexity in this setting. This algorithm addresses the general case of communicating MDPs; they also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption.
SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes Cone Embeddings (ConE), a geometry-based query embedding method for multi-hop reasoning over knowledge graphs. ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunction operations, and geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,"This paper considers the problem of optimal control of stochastic nonlinear systems with separable cost and constraints in the state and input variables. The authors propose a novel numerical scheme for implementation of the corresponding value iteration (VI) algorithm in the conjugate domain. The proposed approach reduces the time complexity of each iteration in the VI algorithm from O(XU) to O(\X + U) by replacing the minimization operation in the primal domain with a simple addition in the dual domain. Detailed analyses of the convergence, time complexity, and error are provided."
SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper presents a self-supervised multimodal representation learning framework based on Transformers. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multimmodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to video retrieval. The results show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream task."
SP:97f533426dce73d27768dd7afc2ddf035cf21e61,This paper proposes to replace the softmax structure in self-attention with a Gaussian kernel to stabilize the model training and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. The authors also conduct theoretical analysis by showing that the matrix approximation error of the proposed method is small in the spectral norm. Experiments on the Long Range Arena benchmark show that it is sufficient in getting comparable or better performance than the full self attention while requiring fewer computation resources.
SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data efficient learning from parametric experts. The main idea is to combine conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. The authors show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also shows benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes an approach to improve the robustness of computer vision models in the face of data corruptions or distribution shifts. The authors propose a framework that leverages the input-sensitivity of deep networks to design robust objects, i.e., objects that are explicitly optimized to be confidently classified. The proposed approach yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments. "
SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper studies the problem of data augmentation in off-policy reinforcement learning (RL) from visual observations. The authors identify two main causes of instability in previous work’s application of data augmentations: indiscriminate application of augmentation resulting in high-variance Q-targets and over-regularization. Based on the findings, the authors propose a simple yet effective technique to stabilize this class of algorithms under augmentation. The proposed method consists of three components: 1) by only applying augmentation to Q-value estimation of the current state, without augmenting Q-targeted Q-values used for bootstrapping, SVEA circumvents erroneous bootstrapped caused by data augumentation; 2) a modified Q-objective that optimizes Q value estimation jointly over both augmented and unaugmented copies of the observations; 3) an actor-critic algorithm that learns a generalizable policy indirectly through parameter-sharing. The method is evaluated on the DMControl Benchmark Distracting Control Suite and the robotic manipulation tasks."
SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies Federated Learning (FL) with stochastic algorithms where the WNs perform a few local updates before communication. The authors propose the Stochastic Two-Sided Momentum (Stochastic STEM) algorithm that utilizes certain momentum-assisted gradient directions for both the WN and the SN updates. They show that the algorithm requires $\tilde{O}(\epsilon^{-3/2})$ samples and $\epsilone$-1/2) communication rounds to compute an optimal solution. Further, they show that there is a trade-off curve between the number of local updates and the minibatch sizes, on which the above sample and communication complexities can be maintained."
SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the optimal noise covariance of SGLD in terms of its generalization ability. Specifically, the authors propose to optimize the structure of the noise to ensure that the generalization bound is minimized while a low empirical risk is guaranteed. Theoretically, they show that with constraint to guarantee low risk, the optimal covariance is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The authors also provide empirical evidence to support their theoretical results."
SP:19fbd1a381598538662417a4a1885ba4ac04f5f8," video compression methods have demonstrated great promise in catching up with traditional video codecs in their rate-distortion (R-D) performance. How2 ever, existing learned video compression schemes are limited by the binding of the prediction mode and the fixed network framework. They are unable to support various inter prediction modes and thus inapplicable for various scenarios. In this paper, to break this limitation, the authors propose a versatile video compression framework (VLVC) framework that uses one model to support all possible prediction modes. Specifically, to realize versatile compression, the motion compensation module applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. In case of multiple-reference-frame predictions, a flow prediction module is applied to predict accurate motion trajectories with a unified polynomial function. Experimental results demonstrate that the proposed VLVC supports versatile compression in various settings but also achieves comparable R-D performance with the latest Versatile Video Coding (VVC) standard in terms of MS-SSIM."
SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper introduces and analyzes a multitask generalization of Online Mirror Descent (OMD) which operates by sharing updates between tasks. Theoretical results show that the regret of MT-OMD is of order $\sqrt{\frac{1 + \sigma^2(N-1)}{\sqrt{T}}$, where $\sigma$ is the task variance according to the geometry induced by the regularizer, N is the number of tasks, and T is the time horizon. This improves upon the \sqrt[1/T] bound obtained by running independent OMDs on each task."
SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the problem of approximating the underdamped Langevin diffusion (ULD) with strongly-convex potential. The authors propose a class of MCMC methods for finite sum-decomposable potential functions consisting of finite summation of N smooth components, and propose an efficient discretization method, which requires O(N + d) gradient evaluations to achieve $\epsilon$-error for approximating d-dimensional ULD. Moreover, the authors prove a lower bound of gradient complexity as $\Omega(N+d 1 3 N^2 3 /\epsilons^2^3)$, which indicates that the proposed method is optimal in dependence of N, \eps, and d. In particular, they apply their method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms."
SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"This paper proposes a method for continual learning that combines weight regularization and projected gradient descent. In particular, the proposed method uses Bayesian weight regularizer to encourage good performance on all tasks at convergence and combines this with gradient projection using the prior precision, which prevents catastrophic forgetting during optimization. The proposed method outperforms existing methods in both feedforward and recurrent networks. Finally, the trained networks evolve task-specific dynamics that are strongly preserved as new tasks are learned."
SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to train normalizing flows (NFs) on low-dimensional manifolds that are supported on high-dimensional spaces. The main idea is to use injective flows instead of bijective flows, which have the advantage that the volume-change term can be computed end-to-end, thus enabling maximum likelihood to be used to train NFs. The authors propose two methods that allow back-propagation through the volume arising from the injective flow term. The first method involves exact evaluation of the volume change term and its gradient, which incurs a higher memory cost; the second method uses conjugate gradients to obtain unbiased stochastic gradient estimates. "
SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a new framework for inference and learning in neural networks with slow components. The response time of physical computational elements is finite, and neurons are no exception. In hierarchical models of cortical networks each layer thus introduces a response lag. This inherent property of physical dynamical systems results in delayed processing of stimuli and causes a timing mismatch between network output and instructive signals, thus afflicting not only inference but also learning. The authors propose Latent Equilibrium, a framework that harnesses the ability of biological neurons to phase-advance their output with respect to their membrane potential. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases. The resulting model can be interpreted as a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity."
SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes Nested Graph Neural Networks (NGNNs), a general framework for improving GNN’s representation power. NGNN learns node representations encoding rooted subgraphs instead of rooted subtrees. It extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. Theoretically, the authors prove NGNN can discriminate almost all r-regular graphs where 1-WL always fails."
SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper proposes nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn heuristic approximations that approximate the likelihood of future observations in a hidden Markov model, and (c) perform amortized inference in hierarchical deep generative models. The experiments show that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size."
SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of certified zeroth-order Lipschitz optimization, i.e., black-box optimization on a compact subset X of R^d, with the additional constraint that algorithms must certify the accuracy of their recommendations. The authors characterize the optimal number of evaluations of any Lipchitz function f to find and certify an approximate maximizer of f at accuracy $\varepsilon$. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral $\int(X\delta)/max(f)^{-f(x)^2)$, which was previously only known in dimension d = 1. They also show that a certified version of the DOO algorithm matches these packing and integral bounds."
SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes an adversarial attack on uncertainty estimation of deep neural networks (DNNs). The attack is based on perturbing the weights of the DNN with a small number of perturbations, such that the network is more confident of its incorrect predictions than about its correct ones. The attack can be used in both black-box and white-box settings, where the attacker can only query the attacked model for predicted labels and has no knowledge of the target network. The authors demonstrate the effectiveness of the attack on three uncertainty estimation methods: vanilla softmax score, deep ensemble, and MC-dropout. They also show an attack on the selective classification architecture."
SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the community detection problem in the streaming stochastic block model (StSBM) setting, where nodes are revealed one at a time in random order. In this setting, the authors prove that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes. The authors validate their theoretical findings on synthetic and real data."
SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost induced by different parameterizations (mappings from parameters to predictors) induced by l2 regularization on the parameters (weights). The authors focus on linear neural networks as parameterizations of linear predictors. They identify the representation cost of certain sparse linear ConvNets and residual networks. They also study the reverse problem, identifying which regularizers on linear predictor (e.g., lp quasi-norms, group quasi-Norms, the k-support-norm, elastic net) can be the representation costs induced by simple l2-regularization, and designing the parameterizations that do so."
SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper proposes a simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Consider the task of finding a target entity given a source entity and a binary relation. The proposed method derives crisp logical rules for each query by finding multiple graph path patterns that connect similar source entities through the given relation. It achieves new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122. "
SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,This paper presents an entity linking model which combines a Transformer architecture with large scale pretraining from Wikipedia links. The model achieves the state-of-the-art on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. The authors also present promising results on more challenging settings: end-to-end entity linking and entity linking without in-domain training data.
SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper proposes an approach to build data subsets for deep neural networks by using ensemble Active Learning to estimate the uncertainty of each sample in a dataset, and then chooses only the highest-uncertainty samples for training. The method is simple yet effective. The authors demonstrate that this improves the performance of a DNN compared to training with the entire dataset on three different image classification benchmarks. Moreover, the authors also propose a simple technique to scale up ensembles leading to additional accuracy gains."
SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper proposes a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent’s state and the child's vote. The routing is done via inverted dot-product attention, where the higher-level (parent) units compete for the lower-level attention of the child (child) units, which is commonly used in the other way around. The authors also propose to use Layer Normalization as normalization and replace sequential iterative routing with concurrent iterative routes. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets, and it performs at-par with ResNet-18 with 4x fewer parameters."
SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a method for hyperparameter optimization of deep neural networks. The proposed method is based on the idea that hyperparameters can be interpreted as the level of correlated noise in training, which can be mapped to an effective temperature. The main idea is to use the parallel tempering technique of statistical physics to exchange hyper-parameters between two independent instances of the model. Each simulation corresponds to a unique path, or history, in the joint hyper parameter/model-parameter space. "
SP:beba754d96cc441712a5413c41e98863c8abf605,"This paper analyzes the training dynamics of GANs and REINFORCE for machine translation (MT). The authors show that the performance gains obtained by these methods are not necessarily due to the training signal, but rather to changes in the shape of the distribution curve. They also show that GAN is infeasibly slow to converge to the correct translation. The authors conclude that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the right translation."
SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"This paper studies the asymptotic behavior of the Q-value and optimal value functions in the context of reinforcement learning. The main contributions of this paper are two-fold: (1) the authors provide a closed-form characterization of the variances of Q-values and the optimal value function, and (2) they propose a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. The authors show that the proposed exploration strategy outperforms the existing ones in terms of the probability in selecting the best policy and generating the tightest confidence bounds for value estimates. "
SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"This paper proposes a method to improve the efficiency of the Top-k recommendation by representing users and items as binary codes. The proposed method is based on the minimum description length (MDL) principle, which is used to learn hash functions of users and users. The authors also propose a new marketing strategy through mining potential users by a generative step. Extensive experiments on two public datasets show the advantages for recommendations in various settings over competing baselines."
SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"This paper proposes Adversarial Inductive Transfer Learning (AITL), a method for addressing discrepancies in input and output spaces between source and target domains. AITL utilizes adversarial domain adaptation and multi-task learning to address these discrepancies. The motivating application is pharmacogenomics where the goal is to predict drug response in patients using their genomic information. The proposed method is evaluated on four different drugs and compared against state-of-the-art baselines."
SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes a mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. The authors propose to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward. The proposed approach is tested on a collection of benchmark tests including simulated robotic locomotion.
SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a class-conditional reconstruction-based adversarial detection method that does not rely on a specific predefined adversarial attack. By reconstructing the input from the internal class conditional representation, the system is able to accurately detect black-box and white-box FGSM, BIM, PGD, and CW attacks. Then, the authors propose a new attack to beat the defense - the Reconstructive Attack, in which the adversary optimizes not only the classification loss but also minimizes the reconstruction loss. The authors showed that this attack was able to fool the detection mechanism but with a much smaller success rate than a standard attack."
SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,This paper studies the impact of the initialization and the activation function on the Neural Tangent Kernel (NTK) for fully-connected feed-forward neural networks (FFNN). The authors prove that only an initialization on the Edge of Chaos (EOC) leads to an invertible NTK for deep neural networks. They also show that the smoothness of the activation functions plays a major role in the behavior of NTK.
SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method to learn sentence representations with an injection of linguistic knowledge. Multiple linguistic frameworks propose diverse sentence structures from which semantic meaning might be expressed out of compositional words operations. The authors aim to take advantage of this linguist diversity and learn to represent sentences by contrasting these diverse views. By contrasting different linguistic views, the authors aim at building embeddings which better capture semantic and are less sensitive to the sentence outward form."
SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"This paper introduces FinBERT, a language model based on BERT for financial sentiment classification, which improves the state-of-the-art performance by 14 percentage points for the FinancialPhraseBank dataset. This work is the first application of BERT to finance to the best of the knowledge and one of the few that experimented with further pre-training on a domain-specific corpus. On both of the datasets we used, we achieved state of the art results by a significant margin."
SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a new task of singing voice generation without pre-assigned scores and lyrics, in both training and inference time. The authors propose three singing schemes with different input conditions: free singer, accompanied singer, and solo singer. They also propose a BEGAN based architecture that uses GRUs and grouped dilated convolutions to learn to generate singing voices in an adversarial way. The evaluation shows that the audio quality of the generated voices still leave much room for improvement."
SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,"This paper proposes a novel adversarial defense technique that leverages a latent high order factorization of the network. Randomization is applied in the latent subspace, therefore resulting in dense reconstructed weights, without the sparsity or perturbations typically induced by the randomization. The approach can be easily integrated with any arbitrary neural architecture and combined with techniques like adversarial training. The authors empirically demonstrate the effectiveness of the approach on standard image classification benchmarks."
SP:762729b64c1c1494de0f7410ea3662da61e93b6d,This paper proposes a clustering-based clustering method for spatio-temporal forecasting based on graph attention network and transformer. The proposed method is evaluated on real datasets obtained from a ride-hailing business and achieves 10%-25% improvement than many state-of-the-art baselines. The authors also propose to use multi-view position encoding for temporal domain.
SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper provides a theoretical analysis of the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In particular, the authors study the fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network. Under mild assumptions, the rates of convergence for the action-value functions of the iterative policy sequence obtained by FQI are established. The statistical error characterizes the bias and variance that arise from approximating the action value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, the analysis provides justifications for the techniques such as experience replay, target network and batch size reduction, which are crucial to the empirical success of the proposed algorithm."
SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"This paper proposes to represent the semantics of a sentence as an atom in the attention mechanism of the Transformer. Specifically, the authors argue that the phrase plays an important role in attention and propose a new attention architecture called PhraseTransformer. Besides representing the words of the sentence, the paper introduces hypernodes to represent candidate phrases in attention. The experimental results show the effectiveness of the proposed method on the translation task."
SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper presents a modular neural network architecture MAIN (short for Modular Algorithm Induction Network) for neural algorithm induction. The architecture consists of a neural controller that interacts with a variable-length read/write tape where inputs, outputs, and intermediate values are stored. Each module is a small computational procedure that reads from and writes to a small fixed number of tape cells. At each time step, the controller chooses which module to use together with the corresponding tape locations for module arguments and for writing the module output back to the tape. This architecture is trained end-to-end using reinforcement learning and it can learn several algorithms successfully that generalize perfectly to inputs of much longer length (100) than the ones used for training (up to 10)."
SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,"This paper proposes Monte Carlo Deep Neural Network Arithmetic (MCDA), a technique for determining the sensitivity of deep neural networks to quantization in floating point arithmetic. MCDA is based on applying Monte Carlo arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The authors evaluate their method on pre-trained image classification models on the CIFAR-10 and ImageNet datasets and demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials."
SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"This paper studies the problem of objective mismatch in model-based reinforcement learning (MBRL). In particular, the authors focus on the case where one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In this case, they characterize the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. They propose an initial method to mitigate the mismatch issue by reweighting dynamics model training."
SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper proposes a new targeted blackbox transfer-based adversarial attack method for adversarial attacks of CNN classifiers. The proposed method is based on the modeling and exploitation of class-wise and layer-wise deep feature distributions, and achieves state-of-the-art targeted black-box transfer based attack results for undefended ImageNet models. The authors also place a priority on explainability and interpretability of the attacking process. The methodology affords an analysis of how adversarial perturbations change the intermediate feature distributions of CNNs, as well as a measure of feature distributional separability/entanglement."
SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper introduces a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. The authors propose a new paradigm for on-policy learning in RL, where policies are embedded into expressive latent behavioral spaces and the optimization is conducted by utilizing the repelling/attraction signals in the corresponding probabilistic distribution spaces. They incorporate these regularizers into two novel algorithms, Behavior-Guided Policy Gradient (BG) and Behavior Guided Evolution Strategies (BGES), which outperform existing methods in a variety of challenging environments."
SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper proposes a stochastic gradient descent (SGD)-like algorithm for training deep neural networks with interpolating data, where the empirical loss can be driven to near zero on all samples at the same time. The proposed algorithm, called ALI-G (Adaptive Learning-rates for Interpolation with Gradients), uses a single hyper-parameter for its learning rate and does not require a decaying schedule, which makes it considerably easier to tune. The authors provide convergence guarantees for the proposed algorithm in the convex setting, and show that it converges up to some tolerance."
SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper investigates the role of bottom-up, horizontal, and top-down connections in perceptual grouping in the brain. The authors systematically evaluate neural network architectures featuring combinations of the three connectivity types on two synthetic visual tasks, which stress low-level “Gestalt” vs. high-level object cues for perceptual grouping. They show that increasing the difficulty of either task strains learning for networks that rely solely on bottom up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top down connections are used to rescue learning on task with high level object cues by modifying coarse predictions about the target object."
SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper proposes DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Inspired by the Hoyer measure (the ratio between `1 and `2 norms) used in traditional compressed sensing problems, the authors propose a set  of regularizers to encourage weight sparsity during training. The proposed regularizers have similar range and minima structure as the `0 norm, so it can effectively measure and regularize the sparsity of the weight matrices of DNN models. Meanwhile, the differentiable property enables the proposed regularizer to be simply optimized with standard gradient-based methods, in the same way as the \ell_1 regularizer is."
SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper proposes a variant of Adam, called SAdam, which maintains a faster decaying yet under controlled step size for exploiting strong convexity. Theoretical analysis shows that SAdam achieves a data-dependent $O(\log T)$ regret bound for strongly convex functions, which means that it converges much faster than Adam, AdamNC, and AMSgrad in cases, and can enjoy a huge gain in the face of sparse gradients. In addition, under a special configuration of hyperparameters, the proposed SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop."
SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a method to reduce the memory consumption and training time of BERT-based models. The authors identify the memory bottlenecks in BERT as dot-product self-attention, which consumes quadratic memory with respect to the sequence length, and propose to sparsify the attention matrices to be sparse block matrices. The proposed BlockBERT achieves time and memory saving without significant loss of performance."
SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"This paper studies the effect of pruning on the generalization of neural networks. The authors introduce the notion of “pruning instability”, which is defined as the size of the drop in network accuracy caused by a pruning iteration. They then empirically analyze the instability and generalization associated with various magnitude-pruning algorithms in different settings. They find a tradeoff between the stability and potential generalization benefits, and show iterative pruning’s similarity to regularizing with noise suggests a mechanism for pruning-based generalization improvements compatible with the strong generalization recently observed in over-parameterized networks."
SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,This paper proposes a method for neural architecture search (NAS) with reinforcement learning. The main idea is to use architecture encoders and decoders to search in an embedding space by using architecture-embedding searching and pre-training controller. The proposed method is evaluated on CIFAR-10 and shows comparable performance to other NAS methods.
SP:e2e5bebccc76a51df3cb8b64572720da97174604,This paper proposes a homotopy training algorithm (HTA) to solve non-convex optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupling systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homo-opy path guided by the HTA. The authors prove the convergence of HTA for the non-Convex case and existence of the solution path for the convex case. The proposed HTA has provided a better accuracy on several examples including VGG models on CIFAR-10.
SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,"This paper introduces the 2-simplicial transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention and uses this attention to update entity representations with tensor products of value vectors. The authors show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning. The experiments show that the simplicial agent confers an advantage over the relational agent in the task of logical reasoning."
SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,"This paper proposes a method for semi-supervised object pose estimation with limited labeled data. The proposed method is based on Conditional Variational Autoencoders (CVAEs) with circular latent representations to estimate the corresponding 2D rotations of an object. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing. As expected, model’s performance increases with the amount of supervision provided to the network."
SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a method to scale up multi-agent reinforcement learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. The proposed method maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets, and promotes the sets with the best adaptability to the next stage. The authors implement EPC on a popular MARL algorithm, MADDPG, and empirically show that the approach consistently outperforms baselines by a large margin as the number of agents grows exponentially."
SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,"This paper presents a new graph-based approach to diagrammatic reasoning problems in the style of Raven Progressive Matrices (RPM). It combines three powerful ideas, namely, object-level representation, graph neural networks and multiplex graphs, to capture relations present in the reasoning task. The experiments show that the proposed model performs better than previous models on two RPM datasets."
SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,This paper proposes an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. The authors use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood.
SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"This paper proposes a method to identify winning tickets (small but critical subnetworks) at the very early training stage, i.e., Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. The authors also propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, the authors leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low cost schemes, and then continuing to train merely the EB tickets towards the target accuracy."
SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"This paper proposes a method to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. The authors propose to project the input images to a low-dimensional space to remove adversarial perturbation and stabilize the model through minimizing the discrepancy between the true label distribution and the framework output distribution. To constrain the mapping function, the authors employ distribution regularization in the latent embedding space leveraging optimal transport theory. Experimental results on several benchmark datasets show that, the proposed framework achieves state-of-the-art performance against strong adversarial attack methods."
SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper proposes a neural, end-to-end model for jointly extracting entities and their relations, which does not rely on external NLP tools and integrates a large, pre-trained language model. The model is fast to train, converging in approximately 1 hour or less on a single GPU for all datasets used in this study. The proposed model matches or exceeds state-of-the-art performance, sometimes by a large margin."
SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper studies the problem of learning a data representation in the setting where no meaningful input representation or explicit similarity information exists. The authors assume that no meaningful representation of the items is given. Instead, we are provided with the answers to some triplet comparisons of the following form: Is item A more similar to item B or item C? The authors provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answer to the above-mentioned triplet comparison. This problem has been studied in a sub-community of machine learning by the name “Ordinal Embedding”. Previous approaches to the problem are painfully slow and cannot scale to larger datasets."
SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"This paper proposes a meta-learning framework to learn the value of each datum in a dataset. The authors propose a data value estimator (i.e., a deep neural network) to learn how likely each data is to be used in training of the predictor model. They train the estimator using a reinforcement signal of the reward obtained on a small validation set that reflects performance on the target task. They demonstrate that the proposed method yields superior data value estimates compared to alternative methods across different types of datasets and in a diverse set of application scenarios. "
SP:e2c3374629cfd654b7b35e88507e65646d70470e,"This paper studies the statistical properties of the per-layer Jacobian of random fully-connected ReLU networks at the point of initialization. The authors compare three types of architectures: vanilla networks, ResNets, and DenseNets. They show that while the variance of Jacobian squared norm is exponential in depth for ResNet, and polynomial for DenseNet, there exists an initialization strategy for both, that that the norm is preserved through arbitrary depths, preventing exploding or decaying gradients in deep networks. "
SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes a reinforcement learning-based approach for optimizing neural network code. The main idea is to use reinforcement learning to learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. The authors also propose an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experiments with real hardware shows that CHAMELEON provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of modern deep networks by 5.6%."
SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper studies the problem of certified robustness for top-k predictions, i.e., the robustness of a classifier against adversarial perturbations. The authors propose to use randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example, and derive a tight robustness in the $\ell_2$ norm for topk predictions. The main contribution of this paper is to derive the first certified radius under $\ell_{2}$-norm for top k predictions. Moreover, the authors also prove that the certified radius is tight for randomized smoothed with Gaussian noise."
SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the relationship between the gradient signal to noise ratio (GSNR) of parameters and the generalization ability of deep neural networks (DNNs). The GSNR of a parameter is defined as the ratio between its gradient’s squared mean and variance, over the data distribution. The authors show that the gradient descent optimization dynamics of DNNs naturally produces large GSNRs during training, which is probably the key to DNN's remarkable generalisation ability. They then show how the training dynamics of gradient descent leads to large G SNRs of model parameters and how this leads to better generalization."
SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper presents a method for answering complex logical queries on large-scale incomplete knowledge graphs (KGs). The authors propose to embed KG entities as well as the query into a low-dimensional vector space that entities that answer the query are embedded close to the query in the vector space. In particular, the queries can be embedded as boxes (i.e., hyper-rectangles), where the set of points inside the box corresponds to a set of answer entities of the query. The authors show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KGs entities. Experimental results on standard KGs demonstrate that the proposed method significantly outperforms existing work in answering diverse logical queries."
SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"This paper considers the problem of computing the gradient of the loss function in SGD for strongly convex, convex and non-convex optimization problems. The authors propose a gradient estimator that is biased but consistent. Theoretical results show that the gradient with respect to the parameter matrix B(v) is the same as that of the unbiased estimator. Experiments on both synthetic and real-world data are provided to support the theoretical results."
SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. They can quickly get a specialized sub-network by selecting from the OFA network without additional training. They also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of subnetworks that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently."
SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper extends the Neural Module Networks (NMNs) to answer non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. The authors extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning ( arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. The proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset."
SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"This paper studies the problem of pruning neural networks at initialization from a signal propagation perspective. In particular, the authors study the effect of initialization on connection sensitivity and find that initial weights have a critical impact on pruning and therefore, therefore, pruning results can be improved. Theoretical analysis based on dynamical isometry and a mean field theory, and formally characterize a sufficient condition to ensure faithful signal propagation in a given network. Moreover, the analysis on compressed neural networks revealed that signal propagation characteristics of a sparse network highly correlates with its trainability, and also that pruning can break dynamicalisometry ensured on a network at initialization, resulting in degradation of trainability of the compressed network. The authors then introduce a simple, yet effective data-free method to recover the orthogonality and enhance trainability."
SP:d5899cba36329d863513b91c2db57675086abc49,"This paper studies the problem of training a priori sparse neural networks. The authors propose a new sparse initialization scheme that allows them to explore the space of very deep sparse networks. They evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. They then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them."
SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"This paper studies the problem of training recurrent neural networks (RNNs) on long sequence tasks. The authors develop a mean field theory of signal propagation in LSTMs and GRUs that enables them to calculate the time scales for signal propagation and the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, they derive a novel initialization scheme that eliminates or reduces training instabilities. They demonstrate the efficacy of their initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. They also observe a beneficial effect on generalization performance using this new initialization."
SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,This paper proposes a siamese network to improve the discriminative power of convolutional neural networks on a pair of neighboring scene images. It exploits semantic coherence between this pair to enrich the feature vector of the image for which we want to predict a label. The proposed method is evaluated on a disease density estimation task and shows comparable performance with results from existing post-classification methods.
SP:99c10e038939aa88fc112db10fe801b42360c8dc,"This paper proposes a self-supervised monocular depth estimation method that leverages semantic information from a fixed pretrained network to guide the generation of multi-level depth features via pixel-adaptive convolutions. The monodepth network learns semantic-aware geometric representations that can disambiguate photometric ambiguities in the motion context. Furthermore, the authors introduce a two-stage training process that resamples training data to overcome a common bias on dynamic objects resulting in predicting them at infinite depths. The experiments on challenging real-world data shows that the proposed architecture consistently improves the performance of different monodeepth architectures."
SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"This paper considers label-flipping attacks, a type of data poisoning attacks where an adversary relabels a small number of examples in a training set in order to degrade the performance of the resulting classifier. The authors propose a strategy to build linear classifiers based on deep features that are certifiably robust against a strong variant of label- flipping attacks, where the adversary can target each test example independently. The approach leverages randomized smoothing, a technique that has previously been used to guarantee test-time robustness to adversarial manipulation of the input to a classifier, and the authors obtain these certified bounds with no additional runtime cost over standard classification. On the Dogfish binary classification task from ImageNet, the baseline undefended classifier achieves no more than 29.3% accuracy; the proposed approach achieves 64.2% certified accuracy against the same adversary."
SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper proposes to use differential privacy to improve anomaly detection and defense against backdoor attacks. Differential privacy has been proposed to avoid leaking any individual’s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset or to intermediate results of the aggregation mechanism. In this paper, the authors demonstrate that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks, which could be regarded as “outliers” that are intentionally added by attackers."
SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper studies the calibration of uncertainty prediction for regression tasks. The authors show that the existing definition for calibration of a regression uncertainty (Kuleshov et al., 2018) has severe limitations in distinguishing informative from non-informative uncertainty predictions. They propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach inspired by reliability diagrams used in classification tasks. They also propose a simple scaling-based calibration that preforms well in our experimental tests."
SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a modification of the BERT language model to improve the transferability of knowledge across NLP tasks. Specifically, the authors propose to add a Tensor-Product Representation (TPR) layer on top of BERT to disentangle the semantic content and the structural role in BERT representations. The TPR is a set of constituents, each of which is the binding of a filler to a structural role. The proposed HUBERT model is evaluated on the GLUE benchmark and HANS dataset, where it is shown to outperform BERT+ in terms of transferability."
SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"This paper proposes a multi-agent reinforcement learning (MARL) and hierarchical RL (HRL) approach to train humanoid agents that can both navigate and control in dynamic simulation. The authors propose a partial parameter sharing approach where the lower level of the hierarchy is shared enabling learning using decentralized methods. This drastically reduces the overall parameter space in the MARL problem and introduces structure in the optimization problem. The results show that with this combination of methods, RL techniques can be used for finding strong policies."
SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,"This paper proposes a method to improve the performance of graph neural networks (GNNs) by introducing a spatial convolution layer. The proposed approach leverages a spatial representation of the graph which makes the neural network aware of the differences between the nodes and also their locations in the graph. The spatial representation is obtained by a graph embedding method. Moreover, the proposed approach is utilized to simplify the graph down-sampling problem and a novel graph pooling method is proposed."
SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,"This paper proposes frequency pooling (F-pooling) to reduce the shift-equivalent and anti-aliasing effects of convolutional neural networks (CNNs). The proposed method is based on the Discrete Fourier Transform (DFT) to transform a signal or image into a low-frequency domain and then only retaining its low frequencies, i.e. the frequencies which are smaller than Nyquist samplingrate. Finally, it converts the low frequencies back into the time domain via inverse DFT. Experiments on image classification show that the proposed method improves accuracy and robustness w.r.t. shifts of CNNs."
SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"This paper proposes a method to improve the generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Experiments on CoinRun, DeepMind Lab exploration and 3D robotics control tasks demonstrate the superiority of the proposed method."
SP:31772a9122ec998c7c829bc4813f6147cdc30145,"This paper proposes an explanation approach for image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification. In this task, an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match is proposed. Experiments on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2, demonstrate the effectiveness of the proposed method."
SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes a generative model for time-domain waveforms, which is trained with maximum likelihood without density distillation and auxiliary losses as used in Parallel WaveNet. It provides a unified view of flow-based models for raw audio, including autoregressive flow (e.g., WaveNet and WaveGlow) as special cases. The authors systematically study these likelihood-based generative models in terms of test likelihood and speech fidelity. They demonstrate that WaveFlow can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms."
SP:963e85369978dddcd9e3130bc11453696066bbf3,This paper proposes a graph translation-generative-adversarial-networks (GT-GAN) method that transforms the input graphs into their target output graphs. GT-GAN consists of a graph translator equipped with innovative graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A new conditional graph discriminator is proposed to classify the target graphs by conditioning on input graphs while training. Extensive experiments on multiple synthetic and real-world datasets demonstrate the proposed method significantly outperforms other baseline methods in terms of effectiveness and scalability.
SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a new neural sequence modeling unit, called METAGROSS (Meta Gated Recursive Controller), which is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. The authors postulate that their proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks)."
SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a self-supervised learning framework for speech recognition by leveraging a strong language model to provide learning signal given unlabeled speech. The proposed method is motivated by how humans learn to recognize speech, and the proposed method can be applied to other sequence transduction tasks such as machine translation and text summarization. The authors also conduct extensive ablative studies to show the importance of various configurations of the proposed approach."
SP:e6af249608633f1776b608852a00946a5c09a357,"This paper considers the problem of fair and robust model training in the presence of data poisoning. The authors propose FR-GAN, which is a framework that enables both unfairness mitigation and robust training. It combines two approaches: (1) employing a fairness discriminator that distinguishes predictions w.r.t. one sensitive group from others, and (2) using a robust discriminator to distinguish training data with predictions from a clean yet small yet small validation set. The proposed framework is robust to the poisoning and can be adjusted to maintain reasonable accuracy and fairness even if the validation set is too small or unavailable."
SP:6306417f5a300629ec856495781515c6af05a363,"This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. The learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, with moving particles. The authors demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance."
SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"This paper proposes a new lens for studying gradient clipping, namely, robustness: informally, one expects clipping to mitigate the effects of noise, since one does not overly trust any single sample. The authors prove that for the common problem of label noise in classification, standard gradient clipping does not in general provide label noise robustness. On the other hand, a simple variant of gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function. Experiments confirm that our composite loss-based gradient clipping performs well on datasets corrupted with label noise."
SP:414b06d86e132357a54eb844036b78a232571301,"This paper proposes SAIL, a state-based imitation learning method that uses state alignment from both local and global perspectives to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both a local and a global perspective, and is combined into a reinforcement learning framework by a regularized policy update objective. The proposed SAIL is evaluated on MuJoCo environments, where the action dynamics are different from the demonstrations."
SP:91761d68086330ce378507c152e72218ed7b2196,"This paper proposes an extension of SGD called Deep Gradient Boosting (DGB), which is an extension to SGD that combines backpropagation with gradient boosting. The key idea of DGB is that back-propagated gradients inferred using the chain rule can be viewed as pseudo-residual targets of a gradient boosting problem. At each layer of a neural network the weight update is calculated by solving the corresponding boosting problem using a linear base learner. The resulting weight update formula can also be seen as a normalization procedure of the data that arrives at each layer during the forward pass. When implemented as a separate input normalization layer (INN) the new architecture shows improved performance on image recognition tasks."
SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,This paper proposes a new algorithm for differentiable neural architecture search (NAS). The main idea is to perform operation search in a subset of channels while bypassing the held out part in a shortcut. The proposed algorithm can be trained with a larger batch size and enjoys both faster speed and higher training stability. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a hybrid method that combines the best aspects of gradient-based methods and deep reinforcement learning (DRL). It builds upon the deep deterministic policy gradients (DDPG) algorithm and proposes a simple modification that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. Empirical results show that the proposed method boosts the performance of DDPG without sacrificing its robustness to local minima.
SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"This paper proposes a method to learn a task-agnostic world graph for reinforcement learning (RL) agents. The world graph is an abstraction that enables agents to focus exploration on a subspace of the environment. The nodes of a world graph are important waypoint states and edges represent feasible traversals between them. The proposed method has two learning phases: 1) identifying world graph nodes and edges by training a binary recurrent variational autoencoder (VAE) on trajectory data and 2) a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. The method is evaluated on a suite of challenging maze tasks and shows that using world graphs significantly accelerates RL, achieving higher reward and faster learning."
SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for reverse engineering neural networks. The proposed method is based on discretizing the layers of a neural network so that it can be used to construct a target function to reveal its design. The paper also proposes an end-to-end PathNet structure through this discretization by considering the function blocks as neural networks, which obtains the exact order and correct inputs of function blocks to compose them for constructing target functions. It is shown that the proposed method can reveal LLDs in devices, e.g., logic devices."
SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for self-supervised imitation learning of goal-conditioned policies by maximizing the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the state that it actually reached. The authors show that this algorithm can be used to learn goal-reaching policies entirely from scratch, without the need for complex RL algorithms. They also provide a theoretical result linking the proposed method to the self supervised imitation learning and reinforcement learning, and empirical results show that it performs competitively with more complex RL methods on a range of challenging goal reaching problems."
SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper proposes a new robust asynchronous SGD algorithm, called Zeno++, which is designed to tolerate Byzantine failures of the workers. The main idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. Theoretical and empirical results show that the proposed algorithm outperforms existing approaches."
SP:d16ed9bd4193d99774840783347137e938955b87,This paper proposes two methods to generate adversarial perturbations that manipulate semantically meaningful image-based visual descriptors - color and texture - in order to generate effective and photorealistic adversarial examples. The proposed methods can be applied to both image classification and image captioning tasks on ImageNet and MSCOCO. The authors conduct comprehensive user studies to show that the generated adversarial images are more convincing to humans than other attacks.
SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a reinforcement learning approach for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset. The authors propose a novel architecture that uses a trainale controller to manipulate latent representations in an external memory. They introduce a sparse structure of associative arrays to locally update a small set of memory entries and use a trainable controller that manages the transition of the sparse memory to a state of low entropy. This enables a model to incrementally learn in the presence of a few observations per class. They experimentally show that their system obtains accurate results.
SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"This paper proposes an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. Motivated by Schmidt’s notion of ‘generalized’ motor programs (Schmidt, 1975) that can allow abstracting a class of movement patterns instead of a singular one, this paper proposes a method to discover the shared space of (generalized) motor programs underlying a variety of manipulation tasks and show that elements from this space can be composed to accomplish diverse tasks. Not only does this allow understanding the commonalities and shared structure across diverse skills, the discovered space of motor programs can provide a high-level abstraction using which new skills can be acquired by simply learning the desired motor programs to compose. This allows us to compose these primitives in a hierarchical reinforcement learning setup to efficiently solve robotic manipulation tasks like reaching and pushing."
SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a method for single-episode transfer in reinforcement learning, where the goal is to perform near-optimal in a single attempt at test time, possibly without access to dense rewards. To achieve this goal, the authors propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then used as input to a universal control policy. This modular approach enables integration of state-of-the-art algorithms for variational inference or RL. Moreover, the approach does not require access to rewards at test-time, allowing it to perform in settings where existing adaptive approaches cannot. In diverse experimental domains with a single episode test constraint, the proposed method achieves favorable performance against baselines."
SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"This paper investigates the use of a three-head network architecture for two-player reinforcement learning in the context of AlphaZero and AlphaZero Zero. The main contribution of this paper is the introduction of a two-head neural network architecture that outputs two estimates — policy and value — for one input game state. The authors show that the proposed architecture is compatible with the zero-style closed loop learning paradigm, consistent to the conjecture made in (Gao et al., 2018b); moreover, in the game of Hex, the new architecture also enables faster search and yields better performance."
SP:89d6d55107b6180109affe7522265c751640ad96,This paper proposes a method for transfer learning in reinforcement learning. The main idea is to adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. The proposed method combines supervised reference trajectory tracking and unsupervised reinforcement learning to learn the target policy. Theoretical analysis shows that the proposed method can reduce the sample complexity of transferring the policies between the tasks.
SP:626021101836a635ad2d896bd66951aff31aa846,"This paper proposes a method for scale-equivariant convolutional neural networks. The proposed method is based on the idea of steerable filter parametrization, which allows for scaling without the need for tensor resizing. The authors also propose a generalization of convolution and other common convolution blocks to be scale equivariant. Experiments on MNIST-scale and STL-10 datasets show that the proposed method outperforms the existing methods. "
SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper presents a point-based unpaired shape completion framework that can be applied directly on raw partial scans to obtain clean and complete point clouds. At the core of the algorithm is an adaptation network acting as a generator that transforms latent code encodings of the raw point scans, and maps them to latent codes of clean and completed object scans. The two latent spaces are learned separately and reglarized by working in different latent spaces that have been learned separately. The authors extensively evaluated the method on real scans and virtual scans, demonstrating that the approach consistently leads to plausible completions and perform superior to other methods."
SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generating fake data using generative adversarial networks (GANs). The authors cast the problem as a two-player maximin game between an attacker and an authenticator, where the attacker has access to a finite set of observations and the authenticator is given access to finite sets of observations. The authors show that the problem is a Nash equilibrium and that the optimal strategies can be found in closed form for the case of Gaussian source distributions. Theoretical analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. Based on these insights, the authors design practical learning approaches and show that they result in models that are more robust to various attacks."
SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper proposes a sensible adversarial learning approach to improve the robustness of deep neural networks against adversarial perturbations. The approach is motivated by the fact that the high dimensional distribution is poorly represented by limited data samples. The authors define a sensible adversary which is useful for learning a defense model and keeping a high natural accuracy simultaneously. They theoretically establish that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under the framework of the proposed approach. They propose a novel and efficient algorithm that trains a robust model with the proposed adversarial examples, without a significant drop in natural accuracy. The proposed approach yields state-of-the-art results on CIFAR10 and MNIST."
SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish feature map distributions of different networks. By training a network to fool the corresponding discriminator, it can learn the other network’s feature map distribution. Discriminators and networks are trained concurrently in a minimax twoplayer game."
SP:e43fc8747f823be6497224696adb92d45150b02d,"This paper proposes a sentiment word embedding model for semantic and sentiment analysis. The proposed method is based on the idea of group homomorphism, which is used to construct the objective function, and the maximum likelihood estimator and the Bayesian estimator are used to determine the parameters of the model. Experiments are conducted on various tasks to demonstrate the effectiveness of the proposed method. "
SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training strategy for the noisy training data. The first phase retrieves an initial set of true-labeled samples as many as possible and the second phase, “learning from a maximal safe set,” completes the rest training process only using the true labels with high precision. Experiments on four image benchmark data sets verify the effectiveness of the proposed method."
SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes an approach for action generalization, i.e., the ability of an agent to generalize to unseen actions in the presence of a novel set of actions. Specifically, the authors propose to learn action representations using an unsupervised approach, and then use a reinforcement learning approach to use these representations in the downstream task of reinforcement learning. The proposed approach is based on a hierarchical variational autoencoder (HVAE) approach, where the action representations are learned from a collection of data samples reflecting the diverse properties of that action. The authors also propose regularization metrics to enforce the generalization of the policy to the unseen actions. The approach is evaluated on four Mujoco environments, where it is shown that the proposed approach outperforms the baselines."
SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes a method for storing and accessing vector word embeddings, such as word2vec or GloVe, to represent words in NLP. The authors use approaches inspired by quantum computing to propose two related methods, word2ket and word_2ketXS, for storing word embedding matrix during training and inference in a highly efficient way. The proposed method achieves a 100-fold or more reduction in the space required to store the embedding vectors with almost no relative drop in accuracy in practical natural language processing tasks."
SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning based on imitation learning. The main idea is to learn a set of behaviors from a dataset of demonstrations by augmenting the state-action pairs with behavioral descriptions. The authors propose a method called Behavioral Repertoire Imitation Learning (BRIL) that learns a single neural network policy conditioned on a behavior description that can be precisely modulated. They apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors."
SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis, which suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. This paper investigates the structure of the winning ticket, which is the size of the initial weights of the network. The authors show that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. Based on these insights, they discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning."
SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes an unknown aware deep neural network (UDN) to detect and reject unknowns in image classification. The idea is to replace the softmax layer in traditional CNNs with a novel tree ensemble that takes the product of feature values, which allows it to balance the contradictory requirements of accurately classifying known objects and correctly rejecting unknowns. To further improve the performance of UDN at detecting unknowns, the authors propose an information-theoretic regularization strategy that incorporates the objective of rejecting knowns into the learning process. The experimental results show that UDN consistently outperforms state-of-the-art methods."
SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a method for variational inference (VI) for deep neural networks. The main idea is to use a coarse mean-field approximation of the posterior over the parameters of the neural network to approximate the true posterior, and then iteratively refine the approximation by sampling the values of auxiliary variables according to the current variational approximation and conditioning on the newly sampled value q(w) = p(w|x, y, a1:k) (Figure 1 right illustrates the process). The refinement iterations have to be repeated for each posterior sample. The authors show theoretically that the method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks."
SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,"This paper proposes a method for contextual generation of categorical sequences using reinforcement learning. The proposed method is based on the hierarchical softmax approach of Morin & Bengio (2005) and the binary tree approach of Grave et al. (2017). The authors propose a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. The number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. Experiments on both neural program synthesis and image captioning show that the proposed methods yield lower gradient variance and consistent improvement."
SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"This paper proposes a stochastic goal recognition control (S-GRC) problem with two main stages: (1) deceptive opponent modeling based on maximum entropy regularized Markov decision processes (MDPs) and (2) Goal recognition control under proactively static interdiction. The main idea is to use the worst case distinctiveness (wcd) as a measure of the nondistinctive path without revealing the true goals, and the task is to interdict a set of actions that improve or reduce the wcd. The authors empirically demonstrate that their proposed approach control the goal recognition process based on opponent’s deceptive behavior."
SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batches for training GANs that are effectively large though actually small. The proposed method is inspired by the use of Coreset-selection in active learning. In particular, it compresses a large batch of samples from the prior and then compresses that batch using Coreset selection. The method can be applied to nearly all GAN variants. The authors conduct experiments on CIFAR and LSUN datasets showing that the method substantially reduces training time and memory usage, that it reduces the fraction of dropped modes in a synthetic dataset, and that it allows the method to reach a new state of the art in anomaly detection."
SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper studies the performance of RNNs trained with maximum likelihood on the information plane, i.e., how well they are able to compress the history of sensory inputs while being maximally informative about the future. In particular, the authors measure the predictive information (MI) between a finite set of observations (the past of a sequence) and an infinite number of additional draws from the same process (the future of the sequence). The predictive information is a measure of how much observing the past can help in predicting the future, which is a symmetric measure of the co-dependence of two random variables. In this paper, the MI is measured as the number of bits we can predict the future given our observations of the past."
SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes to mitigate the delusional bias by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. The authors introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class, and propose a search framework that allows multiple Q approximators to be generated and tracked. Experimental results demonstrate that these methods can improve the performance of Q-learning in a variety of Atari games, sometimes dramatically."
SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial- attentions and thus is applicable to scenes with a large number of objects without performance degradations."
SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a depthwise separable convolution (FALCON) method for compressing convolutional neural networks. FALCON is derived by interpreting existing convolution methods based on depthwise convolution using EHP (Extended Hadamard Product), which is a mathematical formulation to correlate the standard convolution kernel with the depthwise and pointwise convolutions. The authors also propose a generalized version of FALCon, which further improves the accuracy while sacrificing a bit of compression and computation reduction rates. Experiments show that the proposed method outperforms existing methods by up to 8x compression and 8x computation reduction."
SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper identifies four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. The contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy, recognizing and validating the powerful regularization effect of Ghost BN for small and medium batch sizes, examining the effect of weight decay regularization on the scaling and shifting parameters, and identifying a new normalization algorithm for very small batch sizes by combining the strengths of BN and Group Normalization. The authors validate their results empirically on six datasets."
SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"This paper proposes a method to detect bugs in the training of generative models (word-LMs) and character language models (CLMs) using differentially private federated RNNs and CLMs. Specifically, the proposed method is based on the DP-FedAvg algorithm of McMahan et al. (2018), which is used to train the RNN and the CLMs in a federated fashion. The paper also proposes a new algorithm for differentially privacy-preserving federated GANs. The proposed approach is evaluated on text and image datasets. "
SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the provided motion material and modifies it to become natural. On skeleton datasets with diverse motion, the proposed method outperforms existing parametric and non- parametric baselines."
SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper studies hierarchical explanation of neural network predictions. The authors identify two desirable properties for informative hierarchical explanations of predictions, namely the non-additivity and context-independence. They propose a formulation to quantify context independent importance of words and phrases that satisfies the properties above. They revisit the prior line of works on contextual decomposition algorithms, and propose Sampling and Contextual Decomposition (SCD) algorithm and Sampling & Occlusion (SOC) algorithm. Experiments on multiple datasets and models show that the explanation algorithms generate informative hierarchical explanation, help to extract classification rules from models and enhance human trust of models."
SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"This paper proposes a saliency map method for deep convolutional neural networks (CNN) that is much more efficient than popular gradient methods and quantitatively similar and better in accuracy. The technique works by measuring information at the end of each network scale which is then combined into a single saliency maps. The saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. Finally, the authors visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information."
SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation by explicitly modeling the positions of generated words. Specifically, the position is modeled as latent variables, and training with heuristic searched positions with MC algorithms. The proposed method is evaluated on machine translation and paraphrase generation tasks, outperforming several strong baselines."
SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a new generative adversarial network (GAN) architecture, called Random Path Generative Adversarial Network (RPGAN), which is an alternative to traditional GANs. The main idea of the paper is to use random paths as the source of stochasticity in the forward pass of the generator network. The paper shows that the proposed RPGAN can be used to understand factors of variation, captured by different generator layers, providing their natural interpretability. Besides, the paper also shows that RPGAN provides competitive generation quality and allows efficient incremental learning on new data."
SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes DeepSphere, a spherical convolutional neural network for spherical data. The proposed method is based on a graph representation of the sampled sphere with respect to the number of vertices and neighbors. A single parameter controls the tradeoff between cost and equivariance (which is linked to performance). As computational cost and memory consumption scales linearly with number of pixels, DeepSphere scales to spherical maps made of millions of pixels. Experiments show state-of-the-art performance."
SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the problem of learning invariant representations for domain adaptation. The authors show that the search for invariance favors the compression of representations, which may have a bad impact on the adaptability of representations expressed as a minimal combined domain error. They also show that weighting representations can align representation distributions without impacting their adaptability. This supports the claim that representation invariance is too strict a constraint."
SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper presents a method for learning to infer user interface attributes from images. The authors propose a two-step process which predicts the attribute values from an input image – (i) first, we train a neural model to predict the most likely initial attribute values, and then (ii) we use imitation learning to iteratively refine the predicted attribute values to achieve pixel-level accuracy. The proposed approach is applied to the task of inferring the implementation of the Android Button attributes and achieves 92.5% accuracy on a dataset consisting of real-world Google Play Store applications."
SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning, in which a network is trained on one task and then re-purposed on another. The authors propose a method based on the observation that robust networks contain robust feature extractors, which can be used to produce new models that inherit the robustness of their parent networks. They also consider the case of “fine-tuning” a network by re-training end-to-end in the target domain. The proposed method is based on lifelong learning strategies, which are applied to semi-supervised transfer learning."
SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"This paper proposes an iterated learning (NIL) algorithm to encourage the emergence of high compositional language in a multi-agent communication game. The authors show that their procedure, consisting in resetting neural agents playing a referential game and pre-training them on data generated by their predecessors, can incrementally advantage emergent languages with high topological similarity. They demonstrate its interest by obtaining large performance improvements in a validation setting, linking compositionality and ability to generalize to new examples."
SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The proposed method is proven convergent under certain conditions. Empirical results show that it can deal with a broad family of MRFs in a fully black-box manner and outperforms both the standard contrastive divergence method and the black box NVIL algorithm."
SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a simple reward function for goal-conditioned reinforcement learning. The proposed reward function is a simple indicator reward function, which is used to reward the agent when the robot’s observation exactly matches a target goal observation. The authors also propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. The experiments show that the proposed method can learn complex skills for manipulating deformable objects, including a real-world experiment."
SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper considers the robustness verification problem for Transformers with complex self-attention layers. In particular, the authors consider cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. The proposed method computes certified lower bounds that are significantly tighter than those by IBP. Quantitative and qualitative analyses show that the bounds are meaningful and can reflect the importance of different words in sentiment analysis."
SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a weakly supervised pretraining method to encourage pretrained language models to learn entity-level knowledge. The proposed method uses minimal entity information during pretraining and does not introduce additional computation, memory or architectural overhead for downstream task fine-tuning. The trained model demonstrates strong performance on a probing fact completion task and two entity-related NLP tasks. The results show the potential of directly learning entity-centric knowledge from unstructured natural language."
SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper tackles the problem of discovering novel classes in an image collection given labelled examples of other classes. The authors address this problem by combining three ideas: (1) they suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) they use rank statistics to transfer the model’s knowledge of the labelled classes to the task of clustering the unlabeled images; and (3) they train the data representation by optimizing a joint objective function on the labelled-unlabeled subsets of the data. They evaluate their approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin."
SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a method for visual planning based on semi-parametric topological memory (SPTM) and hallucination-based generative models. The main contributions of this paper are: (1) the authors propose a generative generative model that hallucinates possible states of the domain on unseen data without actual exploration; (2) they train a contrastive predictive coding (CPC) energy function using contrastive learning; and (3) they learn a conditional VAE model that generates hallucinated samples for building the connectivity graph, allowing for zero-shot generalization to domain changes. The proposed method is evaluated on a variety of visual planning tasks, and it is shown that it outperforms SPTM and visual foresight methods."
SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper investigates the question of whether the high performance on existing benchmarks are inflated due to spurious biases in them, and if so, how we can effectively revise these benchmarks to better simulate more realistic problem distributions in the real world. To address this challenge, the authors present a novel framework of Adversarial Filters to investigate model-based reduction of dataset biases. They discuss that the optimum bias reduction via AFOPTIMUM is intractable, thus propose AFLITE, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic task distributions and considerably less spurious biases. The authors apply it to popular benchmarks that are practically solved — ImageNet and Natural Language Inference (SNLI, MNLI, QNLI) — and present filtered counterparts as new challenge datasets where the model performance drops considerably."
SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a new way of training prototypical few-shot models for just a single class. The authors propose a “null class” centered around zero, and enforce centering with batch normalization. They also propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples’ distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available."
SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,This paper proposes a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. The proposed GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. Then it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. Experiments are conducted on a number of popular graph datasets for both transductive and inductive tasks.
SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a method to predict pixels relatively, by predicting new pixels relative to previously generated pixels (or pixels from the conditioning context, when available). They show that this form of prediction fare favorably to its absolute counterpart when used independently, but their coordination under an unified probabilistic model yields optimal performance. Experiments on multiple benchmarks for unconditional image generation, image colorization, and super-resolution indicate that the presented mechanism leads to improvements in terms of likelihood compared to the absolute prediction counterparts."
SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper studies the problem of estimating the stationary distribution of the Markov chain in the off-line setting, i.e., when access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. The authors show that consistent estimation remains possible in this challenging scenario, and that effective estimation can still be achieved in important applications. Their approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective."
SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,This paper proposes a method for training NLP models that are less sensitive to spurious patterns. The authors propose to use counterfactually-revised data with human feedback to disentangle the spurious and non-spurious associations. The proposed method is evaluated on sentiment analysis and natural language inference tasks. The results show that the proposed method outperforms baselines trained on original data and counterfactual data.
SP:b720eb5b6e44473a9392cc572af89270019d4c42,"This paper proposes a method to analyze the role of spatial frequency and orientation selective filters in CNN features as perceptual quality features for image classification and super-resolution. The proposed method is based on the Contrast Sensitivity Function (CSF) of Deia et al. (1998) and the contrastive function in contrastive neural networks (DNNs) (Bosse et al., 2017). The CSF is used to measure the contrast between the input image and the output image. The authors show that the CSF can be used as a measure of the perceptual quality of the input images. They also show that spatial frequencies that have lower contrasting thresholds in human visual perception and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality."
SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to explicitly model link type correlations. The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrated that GENN is superior to many baselines without consideration of link-type correlations on the two datasets."
SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.
SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic framework for collaborative filtering on implicit data. The proposed method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. The experimental results on three large-scale datasets demonstrate the effectiveness of the proposed method."
SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a multi-step off-policy reinforcement learning method that combines truncated Q-functions for the first n steps of a target-policy rollout and shifted Q-values for the remainder of the rollout. The authors prove that the combination of these short and long-term predictions is a representation of the full return, leading to the proposed Composite Q-learning algorithm. The proposed method is evaluated on three simulated robot tasks and compared with TD3, Model-based Value Expansion and TD3(\infty)."
SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a method for continuous reinforcement learning for dexterous manipulation tasks. The proposed method is based on three components: (1) raw sensory inputs, (2) assign rewards to their own trials without hand-designed perception systems or instrumentation, and (3) learn continuously in non-episodic settings without requiring human intervention to manually reset the environment. The paper provides an in-depth analysis of the challenges associated with this learning paradigm, and proposes simple and scalable solutions that together produce a complete robotic learning system. Experiments on a set of dexterous robotic manipulation tasks show that the proposed method can learn dexterous tasks in the real world, outperforming baselines."
SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the problem of adversarial robustness of deep neural networks. The authors propose a risk decomposition theorem, which shows that the expected robust risk can be decomposed into two parts: the stability part which measures the prediction stability in the presence of perturbations, and the accuracy part which evaluates the standard classification accuracy. The stability part does not depend on any label information, so the authors propose to optimize this part using unlabeled data. They further show that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018), adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabelled data is provided."
SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a family of advantage estimates based on the order statistics over the path ensemble, which allows one to flexibly drive the learning process in a promotion focus or prevention focus. The authors systematically study the impacts of different regulatory focuses. The findings reveal that regulatory focus, when chosen appropriately, can result in significant benefits. In particular, for the environments with sparse rewards, promotion focus would lead to more efficient exploration of the policy space; while for those where individual actions can have critical impacts, prevention focus is preferable."
SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks, which computes most features in a low precision and only a small proportion of important features in high precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of the DNN execution with almost no accuracy loss. Experiments indicate that PG achieves excellent results on CNNs, including static compressed mobile-friendly networks such as ShuffleNet."
SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a new problem called wildly unsupervised domain adaptation (WUDA), which is a more realistic and challenging problem setting where classifiers have to be trained with noisy labeled data from SD and unlabeled data from the target domain. The authors show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, they propose a Butterfly framework, a powerful and efficient solution to the problem. The proposed method maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled to unlabeled, and SD to TD distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving the problem of wildly UDA."
SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"This paper proposes an autoencoder-based off-policy model-free reinforcement learning algorithm for image-based RL. The authors argue that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image based RL. Based on these findings, the authors devise an off-Policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-based and model free algorithms on many challenging control tasks."
SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for accelerating training and improving generalization over the original dropout. The key idea is to create multiple dropout samples at the dropout layer. The loss is calculated for each dropout sample, and then the sample losses are averaged to obtain the final loss. This technique can be easily implemented without implementing a new operator by duplicating a part of the network after the drop-out layer while sharing the weights among the duplicated fully connected layers. Experiments using the ILSVRC 2012 (ImageNet), CIFAR-10, CifAR-100, and SVHN datasets show that networks trained using multi- sample dropout achieve lower error rates and losses for both the training set and validation set."
SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"This paper proposes a label-sensitive gate (LSG) structure to disentangle the filters in CNNs. The key idea is to impose each filter’s attention to just one or few classes, namely class-specific. The proposed method is simple but effective. Extensive experiments demonstrate the effectiveness of the proposed method. "
SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"This paper proposes a method for learning nearly decomposable Q-functions (NDQ) in cooperative multi-agent reinforcement learning (MARL). NDQ learns a decentralized Q function for each agent and uses a mixing network to combine these local Q values into a global action value. The authors propose two information-theoretic regularizers. The first one is maximizing mutual information between agents’ action selection and communication messages while minimizing the entropy of messages between agents. The second one is minimizing the communication entropy between agents in order to encourage communication to be both expressive and succinct (i.e., only sending useful and necessary information). To optimize these regularizers, the authors derive a variational lower bound objective, which is easily integrated with existing value function factorization methods such as QMIX."
SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a new type of problem called Programming Puzzles and a framework for generating a large set of hard problems that can both expose the weaknesses of existing solvers and which can be used in a GAN-like setup to train better solvers. A programming puzzle is a short program for a Boolean function f(x) with the goal of finding an input that makes f return True. The authors propose an algorithm called “Troublemaker” that can generate puzzles adaptively targeted at any given puzzle-solver. Rather than generating a single dataset of puzzles at random, it generates a diverse set of puzzles that are difficult for the solver. In the experiments, it learns to generate challenging problems for a variety of state-of-the-art puzzle solving techniques."
SP:627b515cc893ff33914dff255f5d6e136441d2e2,This paper proposes a hierarchical reinforcement learning approach that decomposes a policy into lower-level primitives and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. The paper proposes to use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information is the one that acts in the environment. Experiments show that the proposed approach improves over both flat and hierarchical policies in terms of generalization.
SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,This paper proposes a method for learning a latent dynamics model for planning in high-dimensional state spaces. The proposed method learns a latent reward prediction model and then uses it to plan in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which is shown to be the only necessary information for successful planning. The method is evaluated on the multi-pendulum and multi-cheetah environments and shows good performance and high sample efficiency.
SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes two parameter reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). The first technique is factorized embedding parameterization, which separates the size of the hidden layers into two small matrices. The second technique is cross-layer parameter sharing, which prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 7x faster."
SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"This paper proposes a spatio-temporal encoder-decoder architecture for multi-modal NLP tasks. The proposed architecture, called OmniNet, can be used to learn representations for multiple modalities (image, text, video, audio) and can be trained together with a single model. The main idea of the proposed architecture is to use a spatial encoder and a temporal encoder to capture the spatial and temporal information of the input data, and then use a generalized encode() function to store the representations for each modality. This allows the model to be used for multiple tasks (e.g. part-of-speech tagging, image captioning, visual question answering, video activity recognition) at the same time. Experiments show that the proposed model is able to achieve better performance than the baselines. "
SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"This paper proposes a new method for quantifying the predictive uncertainty of deep learning models. The proposed method, called discriminative jackknife (DJ), is a formal inference procedure that constructs predictive confidence intervals for a wide range of regression models, is easy to implement, and provides rigorous theoretical guarantees on (1) and (2). The method uses higher-order influence functions (HOIFs) of the trained model parameters to construct a jack-knife (leave-one-out) estimator of predictive confidence interval. Experiments demonstrate that DJ performs competitively compared to existing Bayesian and non-Bayesian baselines."
SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a GAN-based method for generating high-fidelity videos. The proposed method, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. The authors evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Fréchet Inception Distance for prediction for Kinetics600 and UCF-101 datasets, and establish a strong baseline for synthesis on Kinetics-600."
SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper studies the problem of few-shot learning, where the representation is learned from a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. The authors propose a spatial attention mechanism that allows focusing on objects and suppressing background clutter. They also show that a pre-trained network can be easily adapted to novel classes, without meta-learning."
SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes Constrained Adversarial Networks (CANs), a generalization of GANs in which the generator is encouraged during training to output valid structures. CANs make use of the semantic loss (Xu et al., 2018) to measure the mass allocated by the generator to invalid structures and penalize the latter accordingly. The authors show that CANs improve the validity of the generated structures (on average) while keeping the computational cost of training under control and substantially reducing inference runtimes."
SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power. Experiments on both black-box and open-box adversarial attacks show that commonly-used architectures, namely Lenet, Network-in-Network, and ResNet-18 can be up to 51% more resistant to adversarial fooling by only using the proposed activation functions instead of ReLUs."
SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"This paper proposes a deep learning wrapper technique that can endow any black-box model with uncertainty features. The wrapper uses a reparameterization trick on the Dirichlet distribution, and it can capture the distribution on the multinomial parameters of the output of the black box classifier. The authors show that the predicted uncertainty can be used to fuel a rejection method and show how this helps in assessing the fitness of a model to a new domain or data set. They show results in NLP and CIFAR-10 datasets."
SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes to use blockwise adaptive stepsize instead of coordinate-wise stepsize in Adagrad to improve the generalization of SGD. Theoretical analysis shows that blockwise adaptivity is less aggressive than adaptivity to individual coordinates, and can have a better balance between adaptivity and generalization. Experiments on synthetic datasets, image classification and language modeling confirm the theoretical results. "
SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new dataset, called CATER, for spatiotemporal video understanding. CATER is a synthetic dataset with 3D objects, and is designed to test the ability to recognize compositions of object movements that require long-term reasoning. In addition, CATER also provides a plethora of diagnostic tools to analyze modern spatio-temporal video architectures by being completely observable and controllable. It provides insights into some of the most recent state-of-the-art deep video architectures."
SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes a method to improve GANs with regards to model compatibility. Specifically, the authors introduce an auxiliary Boundary-Calibration loss (BC-loss) into the generator of GAN to match the statistics between the posterior distributions of original data and generated data with respect to the boundaries of the pre-trained classifiers. The BC-loss is provably unbiased and can be easily coupled with different GAN variants to improve their model compatible. Experimental results demonstrate that BCGANs not only generate realistic images but also achieves superior model compatibility than the original GAN."
SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, the authors learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and CifAR-100 datasets demonstrate that the L2L outperforms existing adversarial defense methods in both classification accuracy and computational efficiency. Moreover, the framework can be extended to the generative adversarial imitation learning and stabilize the training."
SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper proposes a method for constraint learning in inverse reinforcement learning (IRL), where the goal is to learn to maximize cumulative rewards subject to a set of hard constraints. The proposed method is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent’s demonstrations given our knowledge of an MDP. The authors propose an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and they evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."
SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper introduces a deep recurrent neural network architecture that approximates known visual cortical circuits (Mély et al., 2018). The authors show that this architecture, which they refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. The authors claim that this phenomenon is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour discovery and that incorporating circuits in artificial neural networks can improve computer vision."
SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"This paper proposes a context-aware CNN (conCNN) for object detection. ConCNN is inspired by the conditional random field (CRF) model in CNNs, and leverages the semantic context to enforce the semantics context constraints in the CNN-based object detector by leveraging the popular CRF model. In particular, the proposed conCNN features a context aware module that naturally models the mean-field inference method for CRF using a stack of common CNN operations. Experiments on COCO datasets showcase that conCNN improves the average precision (AP) of object detection by 2 percentage points, while only introducing negligible extra training overheads."
SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"This paper investigates the adversarial vulnerability of the BatchNorm layer. The authors hypothesize that the use of different normalization statistics during training and inference (i.e. mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerabilities in the BN layer. They empirically proved this by experiments on various neural network architectures and datasets. Furthermore, they introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherit the benefits of BN."
SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper proposes and analyzes two simple and intuitive regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, they prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels. Their generalization analysis relies on the connection between wide neural network and neural tangent kernel (NTK). The generalization bound is independent of the network size, and is comparable to the bound one can get when there is no label noise."
SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes a method to improve the performance of convolutional neural tangent kernels (CNTK) and CNN-GP (CNN-GP) by incorporating data augmentation and global average pooling. Specifically, the authors propose a new operation called Local Average Pooling (LAP) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. The authors also propose a pre-processing technique called Coates et al. (2011) to represent the input image using a single convolution layer composed of random image patches. On CIFAR-10, the proposed method achieves 89% accuracy, matching the best previous classifier that is not a trained neural network AlexNet."
SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS) for reinforcement learning (RL). In particular, the authors propose a method called “Search with Amortized Value Estimates” (SAVE) which uses a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-actions values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTs, resulting in a cooperative relationship between model free learning and model based search. The authors demonstrate the effectiveness of SAVE by incorporating it into agents that perform challenging physical reasoning tasks and Atari."
SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or “Deep 3D Pan”, with “t-shaped” adaptive kernels equipped with globally and locally adaptive dilations. The proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with global and local adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB STEREO indoors dataset to prove the efficacy of the proposed method."
SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the variational auto-encoders (VAEs) from an information theoretic perspective. The authors propose the capacity-constrained information-maximization (CCIM) objective, which aims to learn the maximal informative generative model while maintaining bounded the network capacity. In particular, the authors show that the optimal generative models are the ones that maximize the maximum capacity of the network and minimize the minimization of the CCIM objective. Theoretical results are also provided to support the proposed approach."
SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper proposes two approaches to learn node embeddings based on attribute-neighborhood relationships, i.e., pooled (AE) and multi-scale (MUSAE) approaches. The authors prove that these algorithms implicitly factorize probability matrices of features appearing in the neighbourhood of nodes. Experiments show that the proposed approaches are robust, computationally efficient, and outperform existing approaches."
SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the phase transitions in the Information Bottleneck (IB) objective, i.e., how do the terms I(X;Z) and I(Z) behave, and what’s their relationship with the dataset and the learned representation? The authors introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phases transitions, and draw its connection with the Fisher information matrix for parameterized models. They provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representations, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, they present an algorithm for discovering phase transition points."
SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a method to estimate the advantage function of policy gradient methods in reinforcement learning. The main idea is to use the independence property between current action and future states in the environment to reduce the variance of the advantage estimation. In particular, the authors propose an importance sampling advantage estimator with close-to-zero variance even when the Monte-Carlo return signal yields a large variance. To further remove the risk of the high variance introduced by the new estimator, they combine it with existing Monte-carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that the method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,This paper proposes a doubly robust estimator for infinite horizon off-policy policy evaluation. The proposed estimator is based on the infinite horizon density ratio and off policy value estimation. The bias vanishes when either the density ratio or value function estimation is perfect. Both theoretical and empirical results show that the proposed method yields significant advantages over previous methods.
SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a two-stage framework for few-shot learning. In the first stage, it learns task-agnostic feature on base data with a novel Metric-Softmax loss, which is trained against the whole label set and learns more discriminative feature than episodic training. The second stage proposes a task-adaptive transformation which adapts the classifier to each few-shots setting very fast within a few tuning epochs. Experiments on mini-ImageNet and CUB-200-2011 benchmarks show that the proposed method outperforms the existing methods."
SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper presents a data-driven approach that learns to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data, then employs a neural network that infers a correction to move a coarse result closer to the reference data. The authors provide insights into the targeted learning problem with two learning approaches: fully supervised learning methods with a naive and an optimized data acquisition and an unsupervised learning method with a differentiable Navier-Stokes solver. In all cases, the trained models yield improved simulation accuracy."
SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for learning to compress and store a representative dataset from a non-i.i.d data stream, while only observing each sample once. This problem is highly relevant for downstream online continual learning tasks, as well as standard learning methods under resource constrained data collection. The authors propose a new architecture which Stacks Quantization Modules (SQM), consisting of a series of discrete autoencoders, each equipped with their own memory. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, our approach does not require pretraining, even on challenging datasets."
SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes a bidirectional representation learning method for multi-label metric learning. The proposed method is based on deep neural networks, which is able to operate on feature data or directly on raw image data. The model scales linearly in the number of instances and trains deep convolutional networks that encode both input data and output labels, and obtains a metric space for testing data. Experiments on a number of data sets demonstrate the effectiveness of the proposed method."
SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the generalization performance of asynchronous stochastic gradient descent (SGD) with a parameter server (PS) that updates the parameters of the neural network. The authors analyze the stability of A-SGD from the perspective of dynamical stability and show that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous SGD algorithm. They derive closed-form rules on how the learning rates could be changed, while keeping the accessible set the same. Specifically, for high delay values, they find that for high learning rate and high delay, it is necessary to keep inversely proportional to the delay. They also extend this analysis to include momentum. They find momentum should be either turned off, or modified to improve the training stability. "
SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes a method for learning representations of future observations that can be used for value prediction in reinforcement learning. The authors propose to learn a special value function that takes future observations as an additional input and outputs the features of the future observations. These features are then used to train a forward model to predict the features that are most useful for the prediction of the value function. The proposed method is evaluated on a variety of domains, including the Atari 2600 and Mujoco environments."
SP:6388fb91f2eaac02d9406672760a237f78735452,"This paper proposes a new adversarial attack method for graph neural networks. The main idea is to modify the graph structure in a less noticeable way than adding/deleting edges. The rewiring operation preserves some basic graph properties such as number of nodes and number of edges, while adding or deleting an edge can make remarkable changes on the eigenvalues/eigenvectors of the graph Laplacian (Ghosh & Boyd, 2006). The authors propose a new attack operation based on graph re-wiring, where they remove the existing edge between two nodes and add the edge between vth and vsec, and add and delete edges between vsec and vth. Note that vth is a constraint to be the 2-hop neighbor of vfir. It is obvious that in our setting in this paper, the proposed attack operation is not obvious. The authors then use reinforcement learning to learn the attack strategy. Experiments on real world graphs demonstrate the effectiveness of the proposed framework."
SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper studies the problem of estimating the prediction error of encoder-decoder convolutional neural networks (CNNs) for inverse problems. The authors propose to use Stein’s unbiased risk estimator (SURE) as an unbiased estimator for prediction error. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. Inspired by the finding that an encoderdecoder CNN can be expressed as a piecewise linear representation, the authors provide a close form expression of the unbiased estimators for the prediction errors. The close form representation leads to a novel bootstrap and aggregation scheme to prevent a CNN from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems, including accelerated MRI and EDX."
SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"This paper proposes a meta-learning approach for few-shot classification, where the number of instances per task and class is not fixed. The authors propose a Bayesian task-adaptive meta learning (Bayesian TAML) approach, which learns to balance the effect of meta learning and task adaptive learning, under a more realistic task distribution where each task and classes can have varying number of training instances. The proposed approach is validated on multiple realistic task- and class-imbalanced datasets, on which it outperforms existing meta learning approaches."
SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for imitation learning that does not require learning a reward function. The main idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. The proposed method, called soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, the authors show that SQIL can be interpreted as a regularized variant of behavioral cloning (BC) that uses a sparsity prior to encourage long-horizon imitation. Empirically, they show that the proposed method outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks."
SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for points clouds that change over time. The authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. They combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. They show that their method works for large, deforming point sets from different sources to demonstrate the flexibility of our approach."
SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes an end-to-end optimizer (OT-SI) that learns the optimal transport cost function using a small amount of side information which is often available. The side information captures subset correspondence, i.e. certain subsets of points in the two data sets are known to be related. The authors develop an OT-SI that differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. Experiments on image, marriage-matching and single-cell RNA-seq datasets demonstrate the effectiveness of the proposed method."
SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"This paper presents an end-to-end method for anomaly detection under a fully unsupervised setting. The key insight of the paper is to model normal data. The authors leverage distribution clustering technique to make an educated guess on the normal data subset. By incorporating clustering to provide supervisory signals, the authors iteratively iterate between hypothesizing normal candidate subset and representation learning. This framework iteratively distills out anomalous data and improves the learned representation of normal data, and the reconstruction error serves as a scoring function to assess the normality of the data."
SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper considers the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. The authors propose a new algorithm, Projection-Based Constrained Policy Optimization (PCPO), which is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. They theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence."
SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper analyzes the role of the parameter $\alpha$ in word embeddings. The authors show that the word embedding can be viewed as a low-rank transformation from the word-context co-occurrence space to the embedding space, which preserves the relative distances among words. They also provide a theoretical explanation for this behavior, and derive a method to automatically find its optimal value. The experiments on real datasets verify their analysis."
SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,"This paper proposes a new clustering algorithm, X-Forest, consisting of a group of approximate Random Projection Trees (RP Trees) that can be used to measure the similarity between two data points in the space (e.g., Euclidean space). The proposed method is based on the idea of similarity measurement, which is a central role in data mining and machine learning and has practical applications in other fields, such as biochemistry, biology, botany, etc. The authors propose three techniques to improve the accuracy and efficiency of the proposed method. First, they introduce RP Trees into similarity measurement that accuracy is improved. Second, they enforce certain layers in each tree to share identical projection vectors, which improves efficiency. Third, the authors introduce randomness into partition to eliminate the reliance on prior knowledge. Experiments on three real-world datasets demonstrate that the proposed model achieves an efficiency of up to 3.5 times higher than RP Trees."
SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method, Ergodic Inference (EI), which combines Markov chain Monte Carlo (MCMC) and variational inference (VI). The main idea is to tune the hyper-parameters of a flexible finite-step MCMC chain so that its last state sampling distribution converges fast to a target distribution. This is done by optimizing an objective function that directly quantifies the bias of the resulting samples. The proposed method is evaluated on synthetic and deep generative models."
SP:64f2744e938bd62cd47c1066dc404a42134953da,This paper proposes a method for treatment effect estimation from observational data with missing covariates. The authors propose to use latent confounders whose distribution is learned through variational autoencoders adapted to missing values. They also propose a multiple imputation strategy that allows to fully exploit the posterior distribution of the latent variables. Numerical experiments demonstrate the effectiveness of the proposed methodology.
SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search algorithm to construct compact reinforcement learning (RL) policies, by combining ENAS (Efficient Neural Architecture Search) and ES (Salimans et al., 2017) in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, the authors represent compact architectures via efficient learned edge partitions. For several RL tasks, they manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies."
SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper proposes a generalization of the Wavelet Transform (WT) method to the Group Transform (GT) method, which allows for non-linear transformations in the space of strictly increasing and continuous functions. The authors also propose a tractable way to learn to sample these transformations. The proposed method is evaluated on a variety of time-series tasks, including bird detection and haptic classification. The results show that the proposed method outperforms the baseline methods."
SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCN) to non-Euclidean spaces, e.g. hyperbolic or spherical manifolds. The authors introduce a unified formalism that can interpolate smoothly between all geometries of constant curvature, leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. The proposed method is evaluated on node classification and distortion minimization tasks."
