paper_id,summary
SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,"This paper proposes sqSGD (selective quantized stochastic gradient descent), a gradient-based learning algorithm for federated learning under local differential privacy constraints. The authors propose a novel privacy-preserving quantization scheme that uses a constant number of bits per dimension per client. Then they improve the base algorithm in two ways: first, they apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, they utilize randomized rotation as a preprocessing step to reduce quantization error. The practicality of the proposed framework is demonstrated on benchmark datasets."
SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a universal representation method to incorporate prior knowledge into the Transformer-based NMT in a universal way. The proposed method can represent the prior knowledge as a continuous space matrix to semantically generalize instead of directly combining the probability distribution of the prior translation knowledge, thereby enhancing the representation of the input sentence. Experimental results verified the effectiveness of our method on the two widely used translation tasks."
SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,This paper proposes a Bayesian Stackelberg Markov game-theoretic model to model the uncertainty over attacker types and the nuances of an MTD system. The proposed model is a unifying model that can model uncertainty over the attacker type and the dynamics of the MTD systems. A Bayesian Q-learning (BSS-Q) approach is proposed to learn the optimal movement policy for BSMGs within a reasonable time. The authors show that their learning approach converges to an SSE of a BSMG and then highlight that the learned movement policy improves the state-of-the-art in MTD for web-application security.
SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper proposes a VAE-based framework for unsupervised disentangled representation learning. The framework is based on the assumption that the latent representations of each VAE are unique in their own ways, and disentanglement representations are “alike” (similar up to a signed permutation transformation). The authors propose an ensemble framework consisting of multiple VAEs, where each model maintains its original objective, but also encodes to and decodes from other models through pair-wise linear transformations between the latent reprere sentations. Theoretical analysis shows that such success is mainly due to the VAE im-plementation choices that encourage a PCA-like behavior locally on data sampler. Experiments are conducted to compare the proposed method with the state-of-the-art disenanglement methods."
SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper presents a zero-shot approach to automated machine learning (AutoML) that predicts a high-quality model for a supervised learning task and dataset in real-time without fitting a single model. The method uses a transformer-based language embedding to represent datasets and algorithms using their free-text descriptions and a meta-feature extractor to represent the data. The authors train a graph neural network in which each node represents a dataset to predict the best machine learning pipeline for a new test dataset. Performance is competitive with other state-of-the-art AutoML systems while reducing running time from minutes to seconds and prediction time from hours to milliseconds.
SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the relationship between gradient descent and compositional generalization in neural networks. The authors argue that gradient descent is one of the reasons that make compositionality learning hard during neural network optimization. They find that the optimization process imposes a bias toward non-compositional solutions. This is caused by gradient descent, trying to use all available and redundant information from input, violating the conditional independence property of compositionality. Based on this finding, the authors suggest that compositionality approaches considering only model architecture design are unlikely to achieve complete compositionality and suggest that if only model structure design is considered in compositionality, it is hard to achieve good compositionality in practice."
SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,"This paper studies the representation learning for entity alignment in Knowledge Graph (KG) representation learning. The authors propose a new approach that explicitly learns KG-invariant and principled entity representations, while preserving the original infrastructure of existing methods. The proposed approach is based on a typical paradigm abstracted from the existing methods, and analyzes how the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function for embedding learning. However, such a margin cannot guarantee to be tight enough for alignment learning. To mitigate this problem, the authors propose to learn KG invariant entity representations and aligns the neural ontologies of two KGs to eliminate the discrepancy in feature distribution and underlying ontology knowledge."
SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor algorithm co-design (SACoD) framework to develop more efficient CNN-powered PhlatCam. Specifically, the mask coded in the sensor and the backend CNN model are jointly optimized in terms of both model parameters and architectures via differential neural architecture search. Extensive experiments including both simulation and physical measurement on manufactured masks show that the proposed SACOD framework achieves aggressive model compression and energy savings while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art (SOTA) designs with six datasets on four different tasks."
SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a temporal matrix factorization model (NMF) for learning the average developmental path and structured variations of individuals in the social network over their entire lives. The authors use a unique dataset containing lifetime trajectories of all individuals over multiple generations in two honey bee colonies. The method yields inherently interpretable embeddings that are biologically plausible and consistent over time, which allows comparing individuals regardless of when or in which colony they lived."
SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation (DA) pipeline for image reconstruction tasks arising in medical imaging and explores its effectiveness at reducing the required training data in a variety of settings. They focus on accelerated magnetic resonance imaging, where the goal is to reconstruct an image from a few under-sampled linear measurements. They demonstrate the effectiveness of the proposed DA pipeline by showing that for some problem regimes, DA can achieve comparable performance to the state of the art on the fastMRI dataset while using significantly fewer training data. Specifically, for 8-fold acceleration, they achieve performance comparable to state-of-the-art with only 10% of the training data for multi-coil reconstruction and with only 33% of trained data for single-coill reconstruction."
SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a clustering method for order learning based on the deep repulsive clustering (DRC) algorithm. The proposed method uses the order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. Then, it group object instances into clusters according to their identity features using a repulsive term. Moreover, it estimates the rank of a test instance, by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show the effectiveness of the proposed method."
SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behavior. The authors demonstrate the method on several procedurally generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks. The results show that the proposed method outperforms the state-of-the-art intrinsic reward strategies in terms of sample efficiency and final performance."
SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes Isometric Propagation Network (IPN) for zero-shot learning (ZSL), which learns to strengthen the relation between classes within each space and align the class dependency in the two spaces. Specifically, IPN learns to propagate the class representations on an auto-generated graph within each spaces. In contrast to only aligning the resulted static representation, it also regularizes the two dynamic propagation procedures to be isometric in terms of the two graphs’ edge weights per step by minimizing a consistency loss between them. IPN achieves state-of-the-art performance on three popular ZSL benchmarks."
SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a new Transformer architecture that leverages task-conditioned hyper networks for controlling its feed-forward layers. Specifically, they propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. They conduct an extensive set of experiments on GLUE/SuperGLUE. On the SuperGLUE test set, they match the performance of the state-of-the-art while being 16 times more parameter efficient."
SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper studies the effect of image quality on the performance of the task of “learning to steer”, i.e. how the quality of the input image can affect the performance in autonomous driving. The authors propose a sensitivity analysis of the learning algorithm with respect to varying quality in the image input for autonomous driving, and propose an algorithm to improve the overall performance. The results show that the proposed algorithm is able to enhance the learning outcomes up to 48%."
SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes a method for fast approximate solutions to optimization problems with hard constraints. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. The authors demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid."
SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper proposes a new regularization scheme for deep neural network pruning, where the penalty is uniformly raised to a large level. The growing penalty scheme also provides an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. The authors also propose an L2 regularization variant with rising penalty factors and show it can bring significant accuracy gains compared with its one-shot counterpart, even when the same weights are removed."
SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper systematically investigates the role of planning in MuZero, a model-based reinforcement learning algorithm. The authors compare the performance of MuZero with a number of other methods and show that planning alone is not sufficient to drive good generalization. They also show that shallow trees with simple Monte-Carlo rollouts are as performant as more complex methods, except in the most difficult reasoning tasks."
SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes a method to improve the performance of Value Iteration Networks (VINs) by combining contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning. The proposed method, called eXecuted Latent Value iteration networks (XLVINs), is able to generalize VINs to irregular, continuous, or unknown MDPs. The authors show that XLVIN can match or outperform appropriate baselines, often at low-data or out-of-distribution regimes. The learned executors are robust and transferable across environments."
SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,"This paper studies the problem of learning read-once DNFs using neural networks. In particular, the authors focus on the setting where the functions are learned by a convex neural network and gradient descent. They first observe empirically that the learned neurons are aligned with the terms of the DNF, which suggests that the learning process has a clear inductive bias towards such logical formulas. Then, they show that this risk can be minimized by multiple networks: from ones that memorize data to ones that compactly represent the functions. They then set out to understand why gradient descent “chooses the compact representation” by proving that GD is biased towards unique global minima that recover the terms. Finally, they derive a DNF reconstruction method that works in broader settings that include standard two-layer networks and tabular datasets."
SP:6e600bedbf995375fd41cc0b517ddefb918318af,"This paper proposes a method for learning and planning on a map of states and transitions built using historical trajectories. The proposed method constructs a dynamic graph on top of state transitions in the replay buffer and develops an attention strategy on the map to select an appropriate goal direction, which decomposes the task of reaching a distant goal state into a sequence of easier tasks. The authors also leverage graph structure to sample related trajectories for efficient value learning. Theoretical analysis shows the efficiency and converge property of the proposed method."
SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method for improving sample-efficiency and generalization performance of RL agents trained and evaluated on procedurally generated environments. Specifically, the authors propose a general framework for estimating the future learning potential of a level given the current state of the agent’s policy. They find that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's learning potential when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it. The proposed method is evaluated on ProcGen Benchmark and two challenging MiniGrid environments."
SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a method to decompose auxiliary task gradients into directions that help, damage, or leave the primary task loss unchanged. The proposed method is model-agnostic and can be applied to both pre-training and multi-task learning. The authors propose a novel and efficient algorithm for that purpose and show its advantage in practice. The method leverages efficient automatic differentiation procedures and randomized singular value decomposition for scalability."
SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,"This paper proposes a method for one-shot word-object binding, where the agent is placed in a simulated 3D world and given a single introduction to a novel object via visual perception and language (“This is a dax”), the agent can manipulate the object as instructed. The agent is given short-term, within-episode knowledge of the object and long-term lexical and motor knowledge. The authors show that, under certain training conditions and with a particular memory writing mechanism, the agent’s learning generalizes to novel exemplars within the same ShapeNet category and is effective in settings with unfamiliar numbers of objects. They also show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later."
SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of class-imbalance in few-shot learning. The authors study the effect of dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. They extensively compare over 10 state-of-the-art methods using backbones of different depths on multiple datasets. Their analysis reveals that compared to the balanced task, the performances of their class imbalance counterparts always drop, by up to 18.0% for optimization-based methods. They also show that strategies used to mitigate imbalance in supervised learning can be adapted to the FSL case resulting in better performances."
SP:2a9cbbe3661d2f02f71472d0111f22a739412226,This paper proposes a new method for graph convolutional neural networks (GCNs). The main idea is to decouple the representation of each node in the graph at different topological distances. The authors show that the proposed method is more expressive than the most common convolution operators and their linear stacking. The proposed method achieves state-of-the-art performance on several graph classification benchmarks. 
SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes a method for scene graph generation from synthetic data to real-world data. The main idea is to decompose the domain gap into appearance, label and prediction discrepancies between the two domains by introducing pseudo statistic based self-learning and adversarial techniques. The proposed method is evaluated on toy simulators (Clevr, Dining-Sim and Drive-Sim) and realistic simulators."
SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,"This paper proposes a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), for continuous-action deep reinforcement learning (DRL) with a high Update-to-Data (UTD) ratio. The proposed algorithm has three components: (i) a UTD ratio 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q function from the ensemble. The authors also provide a detailed analysis of REDQ and related model-based algorithms."
SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a neural network architecture that is invariant under permutation of the features and robust to Lipschitz-bounded transformations of the input distribution. The proposed architecture, called DIDA, inherits the NN properties of universal approximation and robustness, and its robustness to permutation is established. Experiments on two tasks, patch identification and performance model learning, demonstrate the merits of the approach."
SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes GRASPEL, a graph-Laplacian-like method to learn sparse undirected graphs from high-dimensional data. The proposed method is based on the idea of graph Laplacians, which is a graph approximation of the Laplace matrix in graphical Lasso. The authors claim that the proposed method can be applied to a variety of data mining and machine learning tasks, such as manifold learning, data representation and analysis, dimensionality reduction, data clustering, and visualization. "
SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,This paper proposes a method to learn an abstract-level policy and a goal-conditioned policy in an unsupervised manner. The abstract level policy is conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals for the goal-conditional policy. The learned discriminator serves as an intrinsic reward function for the policy to imitate the trajectory induced by the abstract- level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of the proposed method.
SP:bdf293bf2118a927cbec6b96be03bfcad0243640,"This paper studies the problem of policy switching in deep reinforcement learning, where the goal is to find a set of policies that have low switching cost, i.e., a small number of policy switches during training. The authors propose an adaptive approach based on the feature distance between the deployed Q-network and the underlying learning Q-networks. The proposed method is evaluated on a medical treatment environment and a collection of the Atari games and shows that it substantially decreases the switching cost while maintaining a similar sample efficiency to the case without the switching-cost constraint. "
SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper proposes a new method for solving large-scale non-convex optimization problems by combining homotopy methods and SGD. In particular, the proposed method is based on approximately and sequentially solving a sequence of optimization problems where the source problem is gradually morphed via a homotope map into the target one. The authors conduct a theoretical analysis of the optimality tracking properties and convergence rate of H-SGD under some realistic and mild assumptions. Theoretical results are confirmed by some empirical evaluations, which also show the great potential in terms of performance of combining SGD with an homotopic strategy."
SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning approach for the task of shape completion using implicit representations of 3D surfaces. The authors propose an encoder that describes the posterior distribution of a latent representation conditioned on the sparse cloud. The proposed encoder mechanism allows the learning of object-agnostic properties separately from object-specific properties, thus succeeding in training a model that can consistently produce smooth predictions from highly sparse observations, achieving state-of-the-art results."
SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,"This paper studies adversarial robustness and adversarial training from a channel-wise perspective. The authors highlight two new characteristics of adversarial examples: 1) activation magnitudes are higher than that of natural examples, and 2) channels are activated more uniformly than natural examples. Based on these observations, the authors propose to suppress redundant activation from being activated by adversarial perturbations via a Channel-wise Activation Suppressing (CAS) strategy. They show that CAS can train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness."
SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper provides a non-asymptotic analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and over-parametrization with generalization performance. The authors show that gradient flow preserves a certain matrix that characterizes the imbalance of the network weights, and show that the squared loss converges exponentially at a rate that depends on the level of imbalance of initialization. They also show that large hidden layer width and (properly scaled) random initialization implicitly constrains the dynamics of network parameters to be close to a low-dimensional manifold. In turn, minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case."
SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of deep neural networks with ReLU activations. In particular, the authors show that the kernel functions of deep networks have essentially the same approximation properties as their shallow two-layer counterparts, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. "
SP:3dd495394b880cf2fa055ee3fe218477625d2605,"This paper studies the overestimation problem in continuous control through deep reinforcement learning (DRL) and proposes a novel algorithm that can minimize overestimation, avoid the underestimation bias and retain the policy improvement during the whole training process. Specifically, the authors add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which the authors propose a novel method to provide theoretical and experimental guarantee for future policy improvement. The authors evaluate their method on a set of classical control tasks, and the results show that the proposed algorithms are more computationally efficient and stable than several existing algorithms."
SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper proposes a generalization of the inverse reinforcement learning (IRL) problem to a well-posed expectation optimization problem stochastic inverse RL (SIRL), where the goal is to recover the probability distribution over reward functions from expert demonstrations. The authors adopt the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the distribution and propose a solution that is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem. In the experiment, the authors evaluate their approach on the objectworld and the experimental results confirm the effectiveness of their approach."
SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervision learning. The core of the analysis is a simple but realistic “expansion assumption,” which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. The authors also assume that neighborhoods of examples in different classes have minimal overlap. They prove that under these assumptions, the minimizers of population objectives based on self- training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. They convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness."
SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method for long-term video prediction. The method predicts future frames by first estimating a sequence of semantic structures and then translating the structures to pixels by videoto-video translation. The proposed method is evaluated on three challenging datasets involving car driving and human dancing, and demonstrates that it can generate complicated scene structures and motions over a very long time horizon (i.e., thousands frames) with orders of magnitude longer prediction time than existing approaches."
SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a new framework of graph neural networks (GNNs) based on the Iterated Function System (IFS). The key idea of IGNNS is to use a pair of affine transformations to characterize the process of message passing between graph nodes and assign an adjoint probability vector to them to form an IFS layer with probability. After embedding in the latent space, the node features are sent to IFS layers for iterating, and then obtain the high-level representation of graph nodes. The paper also analyzes the geometric properties of the proposed method from the perspective of dynamical system."
SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a new method for graph generation based on geometric interpretation. The main contribution is dubbed as the geometric graph (GG) generative adversarial network (GAN), which is a Wasserstein GAN that addresses the above challenges. GG-GAN is permutation equivariant and easily scales to generate graphs of tens of thousands of nodes. It also strikes a good trade-off between novelty and modeling the distribution statistics, being competitive or surpassing the state-of-the-art methods."
SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper proposes a novel method for exploring how neurons within a neural network interact. In particular, the authors consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y, where X and Y are sets of neurons in different layers. To ensure we obtain a small and non-redundant set of high quality rules, they formalize the problem in terms of the Minimum Description Length principle, by which we identify the best set of rules as the one that best compresses the activation data. To discover good rule sets, they propose the unsupervised EXPLAINN algorithm. Extensive evaluation shows that our rules give clear insight in how networks perceive the world: they identify shared, resp. class-specific traits, compositionality within the network, and locality in convolutional layers."
SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,"This paper studies the problem of fixed-dataset policy optimization, where the goal is to find an optimal policy that maximizes the expected return of the agent in the worst-case, i.e. the worst possible world. The paper provides a unified conceptual and mathematical framework for the study of algorithms in this regime. The analysis reveals that for naı̈ve approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in theworst possible world, and derive families of algorithms which follow this principle. The theoretical findings are validated by experiments on a tabular gridworld and deep learning experiments on four MinAtar environments."
SP:363661edd15a06a800b51abc1541a3191311ee0e,"This paper proposes a memory-efficient ALF Integrator (MALI) to estimate the gradient for Neural ODEs. MALI is based on the asynchronous leapfrog (ALF) solver, which has a constant memory cost w.r.t number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). The proposed method is evaluated on various tasks, including image recognition, continuous generative modeling, and time-series modeling."
SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a methodology to compare complex scene conditional generation models, and provides an in-depth analysis that assesses the ability of each model to (1) fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) to generative scenes composed of unseen object combinations. The authors identify the advantages of different pipeline components, and find that encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionsings, using semantically aware losses such as the scene-graph perceptual similarity helps improve some dimensions of the generation process, and enhancing the quality of generated masks and image quality of individual objects are crucial steps to improve robustness."
SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,"This paper compares Graph Augmented Multi-Layer Perceptrons (GA-MLPs) and Graph Neural Networks (GNNs) from the perspective of expressive power and learning. The authors show that GA-MLP can distinguish almost all non-isomorphic graphs, while GNNs have higher expressive power in terms of equivalence classes induced by their node-level functions. In addition, the authors show via community detection experiments that the expressive power of the proposed method is limited by the choice of operator family, whereas GNN has higher flexibility in learning."
SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes a continual distillation method to improve the sample efficiency of RL agents in the context of learning in partially-observable environments. The proposed method is based on the idea of distillation, i.e. learning progress of a large capacity learner model is transferred to a small capacity actor model. The main contribution of this paper is that it proposes a method to perform distillation between the actor and the learner models. The method is evaluated in the case of partially observable environments, where it is shown that the proposed method can recover the sample-efficiency gains of the transformer learner and LSTM actor models."
SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a Universal Representation Transformer (URT) layer for few-shot image classification. The URT meta-learns to leverage universal features by dynamically re-weighting and composing the most appropriate domain-specific representations. URT sets a new state-of-the-art result on Meta-Dataset and achieves top-performance on the highest number of data sources compared to competing methods.
SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes a method for unsupervised progressive learning (UPL) based on self-taught associative memory (STAM) to learn representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time. The authors propose an architecture that involves an online clustering module, called Self-Taught Associative Memory, to solve the UPL problem. Layered hierarchies of STAM modules learn based on a combination of clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples."
SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between centralized and decentralized training of deep learning models. The authors identify the changing consensus distance between devices as a key parameter to explain the gap between decentralized and centralized training. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. They highlight the intimate interplay between network topology and learning rate at the different training phases and discuss the implications for communication efficient training schemes."
SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,This paper proposes a new framework for sequence metric learning based on dynamical system synchronization theory. The authors draw an analogy between synchronized trajectories produced by dynamical systems and distance between similar sequences processed by a siamese recurrent neural network. They propose a new neural network model that implements this coupling with a new gate integrated into the classical Gated Recurrent Unit architecture. This model is able to simultaneously learn a similarity metric and the synchronization of unaligned multi-variate sequences in a weakly supervised way. The experiments show that introducing such a coupling improves the performance of the Siamese RNN architecture on an activity recognition dataset.
SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation (codistillation) on the performance of distributed SGD in a distributed setting. In particular, the authors propose to use a weakly-synchronous version of SGD, where the gradients of the two models are averaged at each iteration. The authors show that this weak SGD is still able to achieve better performance than synchronous SGD with a much weaker synchronization mechanism. Moreover, they show that even with moderate batch sizes, models trained with the weaker SGD perform as well as synchronous data-parallel methods. "
SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of SGD in deep learning. In particular, the authors show that SGD converges to a heavy-tailed stationary distribution with respect to the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters $\eta$ and $\beta$ in the setting of quadratic optimization. They show that even in a simple linear regression problem with independent and identically distributed Gaussian data, the iterates can be heavy-tail with infinite variance. The authors further characterize the behavior of the tails with the help of algorithm parameters, the dimension, and curvature. They finally support their theory with experiments conducted on both synthetic data and fully connected neural networks."
SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the effect of bandpass filtering on the performance of GCNs on the supervised community detection task. The authors empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to Euclidean graph, high-frequencies are less crucial to community detection. Then, the authors are able to design MLPs that rely simply on a few eigenvectors of the graph Laplacian that are competitive with deep supervised graph approaches. The stability of a GCN w.r.t. spectral perturbations, and show that they are more robust to high-frequency, which is counterintuitive when compared to vanilla CNNs."
SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a method for learning graph structure and graph neural networks (GNNs) with self-supervision. The main idea is to learn the parameters of a graph neural network and a graph structure of the nodes connectivities simultaneously from data. The authors identify a supervision starvation problem that emerges for graph structure learning, especially when training data is scarce. They propose a solution to this problem by supplementing the training objective with a well-motivated self supervised task. They show the effectiveness of their model through a comprehensive set of experiments and analyses."
SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,This paper proposes a novel detection method for continual learning. The novelty detection method is based on the network confusion caused by training incoming data as a new class. The proposed method is evaluated on a series of image classification benchmarks. The results show that the proposed method outperforms the existing methods. 
SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a framework for learning to interpret natural language constraints for safe RL. The framework consists of a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. The paper also introduces a new multi-task benchmark, called HAZARDWORLD, that requires an agent to optimize reward while not violating constraints specified in free-form text. The proposed framework achieves higher rewards (up to 11x) and fewer constraint violations (by 1.8x) than existing approaches."
SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper tackles the problem of few-shot semantic edge detection, which aims to localize boundaries of novel categories using only a few labeled samples. The proposed method is based on meta-learning strategy and employs a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. The authors also propose a new regularization method based on multi-split matching."
SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a method, Causal Screening, to identify the most influential edges and generate post-hoc explanations for model predictions. It incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, it can be used to generate faithful and concise explanations for any GNN model. The authors conduct extensive experiments on three graph classification datasets and show that the proposed method achieves significant improvements over state-of-the-art approaches."
SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper introduces a new measure of network simplicity, called prunability, which is the smallest fraction of parameters that can be kept while pruning without hurting the model’s training loss. The authors show that this measure is highly predictive of the generalization performance across a large set of convolutional networks trained on CIFAR-10. They also show the mutual information between the predictions of their new measure and strong existing measures based on models’ margin, flatness of minima and optimization speed."
SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,"This paper proposes Discrete Object-factorized Representation Planning (DORP), which learns temporally-abstracted discrete representations from exploratory video data in an unsupervised fashion via a mutual information maximization objective. DORP plans a sequence of abstract states for a low-level model-predictive controller to follow. It discovers independent representations per object and binary properties such as a key-and-door. Experiments on long-horizon tasks show the effectiveness of the proposed method."
SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit- level sparsity. The authors consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed precision quantization scheme. The method enables the exploration of the full mixed precision space with a single gradient-based optimization process."
SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the robustness of quantized neural networks against gradient-based adversarial attacks. The authors claim that these quantized models suffer from gradient vanishing issues and show a fake sense of robustness. To mitigate this issue, the authors introduce a simple temperature scaling approach to mitigate the gradient vanishing issue while preserving the decision boundary. Experiments on CIFAR-10/100 datasets with multiple network architectures demonstrate the effectiveness of the proposed method."
SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes a novel recurrent neural network (RNN) model, called ProtoryNet, which is motivated by the prototype theory in modern linguistics. The proposed method makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each of the sentences to the prototypes. The prototype trajectories enable intuitive, fine-grained interpretation of how the model reaches to the final prediction, resembling the process of how humans analyze paragraphs. Experiments conducted on multiple public data sets reveal that the proposed method is more interpretable and more accurate than the current state-of-the-art."
SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,"This paper proposes a hidden Markov model (HMM) that is a special case of recurrent neural networks (RNNs). The authors prove that each HMRNN has the same likelihood function as a corresponding discrete-observation HMM. The authors also show that the parameter estimates are numerically close to those obtained from via the Baum-Welch algorithm, validating their theoretical equivalence. Finally, the authors demonstrate how the proposed method can be combined with other neural networks to improve parameter estimation, using an Alzheimer’s disease dataset."
SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a method to analyze single-cell RNA-Seq data in the context of neuronal phenotypes. The authors propose a supervised learning approach that factorizes the gene expression data into components corresponding to individual phenotypic characteristics and their interactions. This method, which they call factorized linear discriminant analysis (FLDA), seeks a linear transformation of gene expressions that vary highly with only one phenotypical factor and minimally with the others. They further leverage their approach with a sparsity-based regularization algorithm, which selects a few genes important to a specific feature or feature combination. They applied this approach to a single- cell RNA-seq dataset of Drosophila T4/T5 neurons. The analysis confirms results obtained by conventional methods but also points to new genes related to the phenotypes and an intriguing hierarchy in the genetic organization of these cells."
SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper proposes a new approach to interpretability methods for saliency maps, where the saliency map is assumed to be a random variable. The authors propose a variational approximation to the likelihood function and a prior distribution to model the prior distribution. The prior distribution is modeled using a soft-TV Gaussian distribution, and the posterior distribution is optimized by maximizing the ELBO. The proposed approach is evaluated on the pixel perturbation benchmark, and it is shown that the proposed approach outperforms the existing methods."
SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a debiasing method for large-scale pretrained text encoder neural networks. The authors propose a fair filter (FairFil) network, which takes the original sentence embeddings as input and outputs the debiased sentence embedding. To train the fair filter, the authors constructed a multi-view contrast learning framework, which maximizes the mutual information between each sentence and its augmentation. The augmented sentence is generated by replacing sensitive words in the original sentences with words in a similar semantic but different bias directions. This post hoc method does not require access to the training corpora or any retraining process."
SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a method to improve the robustness of neural networks against adversarial perturbations. The proposed method is based on randomized smoothing, which assigns different noise levels to different samples during training and testing. Specifically, the authors propose a pretrain-to-finetune framework that first pretrains a model and then adjusts the noise levels for higher performance based on the model’s outputs. The authors carefully allocate specific robust regions for each test sample. Experiments on CIFAR-10 and MNIST datasets demonstrate that the proposed method can achieve better accuracy-robustness trade-off in the transductive setting."
SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,"This paper proposes a variational inference method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, the proposed method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. The authors derive a reduced entropy condition approximate inference method that results in rich posteriors. The proposed method outperforms existing variational methods for imputation and de-noising with different real data sets."
SP:4b7d050f57507166992034e5e264cccab3cb874f,"This paper proposes a method to incorporate multi-hop context information into the self-attention mechanism in graph neural networks (GNNs) to improve the performance on node classification and knowledge graph completion tasks. The proposed method, called MAGNA, combines the benefits of graph attention and diffusion techniques in a single layer through attention diffusion, layer normalization, and deep aggregation. MAGNA enables context-dependent attention between any pair of nodes in the graph, enhances large-scale structural information, and learns more informative attention distribution. Experimental results show that MAGNA achieves state-of-the-art results."
SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper proposes a new multi-task multitask test to evaluate the ability of text models to learn and apply knowledge encountered during pretraining. The proposed test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. The authors find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy."
SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,This paper proposes a pre-training approach for table semantic parsing. The authors construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). They pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in table semantic parsers into the pre-trained language model. They also include masked language modeling (MLM) on several existing table-and-language datasets to regularize their pre- training process. The proposed method is much data-efficient. It achieves new state-of-the-art results on four popular fully-supervised and weakly supervised tasks.
SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,"This paper analyzes the performance of the least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large (p) and numerous (n) data. It shows that the standard MTL MTL algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): the analysis provides a simple method to correct these biases, and that the sufficient statistics at play in the method can be efficiently estimated, which can be exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that the proposed method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi- task and transfer learning techniques."
SP:2be727b1333122fef3abfd2f7c576d2fc467893f,"This paper proposes a group equivariant conditional neural process (EquivCNP) that is permutation-invariant and group-equivariant in data space and also has transformation equivariance in the data space. The authors prove a decomposition theorem for permutation invariant maps, which leads them to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. They build architecture using Lie group convolutional layers for practical implementation. They show that the proposed method achieves comparable performance to conventional CNPs in a 1D regression task and zero-shot generalization for an image-completion task."
SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper proposes a text generation method based on off-policy learning from expert demonstrations. The main idea is to train a generative model on the reference data (expert demonstrations) instead of the training data (model-generated history). The model is trained to maximize the quality of the generated history given the likelihood of the current sample. The proposed method is evaluated on summarization, question generation, and machine translation tasks. "
SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. The authors show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, the authors propose InfoPro, which is the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive experiments validate that InfoPro significantly reduces the GPU memory footprint during training without sacrificing accuracy."
SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,"This paper proposes to improve the expressive power of graph neural networks (GNNs) by designing diverse neighborhoods, i.e., rooted sub-graphs, for each node in the graph. The representation of the target node is obtained by aggregating the representation of diverse neighborhoods obtained using any GNN model. Experiments are conducted at multi-class node classification task on three benchmark datasets and node-based multi-label node classification tasks."
SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper investigates the effect of network and file corruptions on video model robustness. The authors propose a corruption-aware defense baseline, Bit-corruption Augmented Training (BAT), which exploits knowledge of bit-level corruptions to enforce model invariance to such corruptions. BAT outperforms corruption-agnostic defenses, recovering up to 7.1% accuracy on highly-corrupted videos while maintaining performance on clean/near-clean data."
SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a new representation learning method for time-varying graphs based on skip-gram embedding and negative sampling. The proposed method is able to disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. Experiments on time-resolved face-to-face proximity data show that the learned representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction. "
SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a new self-supervised learning task for training language models for formal mathematics, where the goal is to infer the types of theorems. The authors propose to use a skip-tree task to train the language models, and show that models trained on this task show surprisingly strong mathematical reasoning abilities. They also propose several evaluation tasks to measure the logical reasoning abilities of the models, including inferring types, suggesting missing assumptions, and completing equalities."
SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper studies the problem of adversarial robustness in the presence of imbalanced gradients, i.e. the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. The authors formulate a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. They examine 12 state-of-the-art defense models and find that models exploiting label smoothing easily cause imbalanced gradient, and on which our MD attacks can decrease their PGD robustness (evaluated by PGD attack) by over 23%. For 6 out of the 12 defenses, our attack can reduce their PGd robustness by at least 9%."
SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a graph-to-sequence deep learning system to generate verifiable axiomatic proofs of semantic equivalence between two programs. The system is evaluated on a rich multi-type symbolic language for linear algebra. It achieves 93% average true positive coverage on 10,000 test cases while ensuring zero false positives by design. The paper is well-written and easy to follow."
SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a multimodal bidirectional Transformer architecture for self-supervised learning of contextualized audio-visual representation from unlabeled videos. The authors propose to decompose the Transformer into modality specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. They also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers."
SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,"This paper proposes a framework for online contextualized few-shot learning (OC-FSL), which extends the standard framework of few-shots learning to an online, continual setting, where episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. The authors propose a new RoamingRooms dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. Furthermore, the authors convert several existing approaches into online versions and propose a contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past."
SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of online learning of graph neural networks (GNNs) in the context of distribution shift and changing training data, when temporal graphs evolve over time. The authors systematically analyze these issues by incrementally training and evaluating GNNs in a sliding window over temporal graphs. They experiment with three representative GNN architectures and two scalable GNN techniques, on three new datasets. The results show that no more than 50% of the GNN’s receptive field is necessary to retain at least 95% accuracy compared to training over a full graph."
SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,This paper proposes a cross-state self-constraint (CSSC) to regularize the representation feature space by comparing representation similarity across different pairs of state. The motivation is that the behavior of a rational agent would have certain relationship with general cross-states features or patterns. The proposed CSSC is tested on the OpenAI ProcGen benchmark and shows significant improvement on generalization performance. 
SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on GNNs in a restricted near-black-box setting, by perturbing the features of a small set of nodes, with no access to model parameters and model predictions. The authors draw a connection between this type of attacks and an influence maximization problem on the graph, which allows them to propose a group of efficient and effective attack strategies. The experiments verify that the proposed strategies significantly degrade the performance of three popular GNN models and outperform baseline adversarial attack strategies on multiple types of GNN."
SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of learning causal structures represented by directed acyclic graphs (DAGs). The authors propose to exploit a low-rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to mitigate this problem. They demonstrate how to adapt existing methods for causal structure learning to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low rank assumption. They also provide empirical evidence for the utility of their low rank adaptations, especially on relatively large and dense graphs."
SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization solution for efficient end-to-end AutoML (DiffAutoML). The method performs co-optimization of the neural architectures, training hyper-parameters and data augmentation policies in an end- to-end fashion without the need of model retraining. Experiments show that DiffAutoML achieves state-of-the-art results on ImageNet."
SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper extends prior networks (PN) and ensemble distribution distillation (EnD) to regression tasks by considering the Normal-Wishart distribution. A Regression Prior Network (RPN) is proposed to predict the parameters of the target Normal Wishart distribution, enabling it to efficiently represent ensembles of regression models, allowing interpretable measures of uncertainty to be obtained at a low computational cost. Two RPN training approaches are proposed. First, the reverse-KL divergence between the model and the target distribution is described, allowing the behaviour of an RPN to be explicitly controlled but requiring an OOD training dataset. Second, Ensemble Distribution Distillation (enD2) is used, where an ensemble of regression model is distilled into RPN such that it retains the improved predictive performance and uncertainty estimates of the original ensemble. The properties of RPNs are demonstrated on synthetic data, selected UCI datasets, and two monocular depth estimation tasks."
SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper introduces a Bayesian online meta-learning framework to tackle the catastrophic forgetting and the sequential few-shot tasks problems. The framework incorporates MAML into a bayesian online learning algorithm with Laplace approximation or variational inference. The experimental evaluations demonstrate that the proposed framework can effectively prevent catastrophic forgetting. BOMLA and BOMVI are able to perform well in various settings.
SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper introduces a new pooling technique of local neighborhoods that allows different tradeoffs of computational cost and expressive power. First, the authors show that this model can count subgraphs of size k, and thereby overcomes a known limitation of low-order GNNs. Second, they prove that, in several cases, the proposed RNP-GNNs can greatly reduce computational complexity compared to the existing higher-order k-GANs and Local Relational Pooling (LRP) networks."
SP:c43f5deb340555d78599a3496318514a826b1aae,"This paper studies the volume-changing behavior of MWU, OMWU, and FTRL in bimatrix games and potential games. The authors extend the volume expansion argument of Cheung & Piliouras (2019) by decomposing the game into zero-sum and coordination components. They show that the two components induce opposite volume-change behaviors, so the overall behavior can be analyzed by comparing the strengths of the components against each other. For multi-player games, they present a local equivalence of volume change between general games and graphical games."
SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,"This paper proposes a new motivation for designing the proximal function of adaptive algorithms, named as marginal regret bound minimization. Based on this motivation, the authors propose a new class of algorithms that achieves marginal optimality, but can also potentially converge much faster than any existing adaptive algorithms in the long term. The authors show the superiority of the proposed algorithms both theoretically and empirically using experiments in deep learning."
SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper proposes expected quadratic utility maximization (EQUM) as a new framework for policy gradient style reinforcement learning (RL) algorithms with mean-variance control. The proposed method has several interpretations, such as reward-constrained variance minimization and regularization, as well as agent utility maximisation. In experiments, the authors demonstrate the effectiveness of the proposed method in benchmark setting of RL and financial data."
SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes a framework for multi-task learning based on implicit differentiation. The main idea is to learn a network that combines all losses into a single coherent objective function. This network can learn nonlinear interactions between tasks. When no useful auxiliary task is known, this paper describes how to learn the network that generates a novel auxiliary task. The proposed method is evaluated on image segmentation and learning with attributes in the low data regime."
SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"This paper proposes a method to detect out-of-distribution sentences in Neural Machine Translation using the Bayesian Deep Learning equivalent of Transformer models. The authors develop a new measure of uncertainty designed specifically for long sequences of discrete random variables—i.e. words in the output sentence. They use their new measure on a Transformer model trained with dropout approximate inference. On the task of German-English translation using WMT13 and Europarl, they show that their measure is able to identify when Dutch source sentences, sentences which use the same word types as German, are given to the model instead of German."
SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the performance of pruning neural networks at initialization. The authors compare SNIP (Lee et al. 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al, 2020), and magnitude pruning (Verdenius et al 2020) and show that random shuffling or sampling new initial values preserves or improves accuracy. They also show that the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune."
SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,"This paper presents FedLearning, a federated learning framework that can defend against both a semi-honest server and Byzantine malicious clients. The authors propose to use a robust mean estimator called FilterL2 to robustly aggregate the possibly contaminated updates and secure aggregation to protect the privacy of the clients. They reconcile the contradictory components with sharding. The evaluation results show that F2ED-LEARNING consistently achieves the optimal or close-to-optimal performance among five robust FL protocols."
SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper proposes a benchmark for offline model-based optimization (MBO) problems. The benchmark consists of a suite of offline MBO tasks with a unified evaluation protocol and reference implementations of recent methods. The proposed benchmark includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics. The benchmarks, together with the reference implementations, are available at sites. Google.com/view/design-bench."
SP:073958946c266bf760d1ad66bd39bc28a24c8521,"This paper proposes a new ELBO formulation for multi-modal generative models. The proposed method generalizes prior works (MVAE, MMVAE) and combines their benefits. The authors also analyze the strengths and weaknesses of previous works and relate them directly to the proposed method. Extensive experiments demonstrate the advantage of the proposed MoPoE-VAE over the existing methods."
SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a prior-guided Bayesian Optimization (PrBO) method that allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions (which are much less intuitive for users). PrBO combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. The authors show that PrBO is around 12x faster than state-of-the-art methods without user priors and 10,000 times faster than random search on a common suite of benchmarks."
SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,This paper proposes to use binary neural networks to reduce the computational cost of deep generative models. The authors propose a new class of binary weight normalization techniques and show that they can be used to train binary models that are 90%-94% smaller in size and also allow significant speed-ups in execution time. They also provide insights for architecture designs of these binarized models. 
SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper proposes a paradigm shift from perturbation-based adversarial robustness to model-based robust deep learning. The key idea is to obtain models of natural variation, which vary data over a range of natural conditions, and then develop three novel model based robust training algorithms that improve the robustness of DL with respect to natural variation. The authors conduct extensive experiments on 12 datasets and show that the proposed methods outperform ERM, adversarial training, and domain adaptation techniques."
SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the optimization of CNNs with ReLU activations and introduces exact convex optimization formulations with polynomial complexity with respect to the number of data samples, number of neurons, and data dimension. The authors first show that two-layer CNNs can be globally optimized via an `2 norm regularized convex program. They then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an `1 norm regularization. They also extend these results to three-layer convolutional neural networks with two ReLU layers."
SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a probabilistic generative model for interpretable learning from demonstration (LfD). The model is based on a high-capacity neural network, and the latent variables are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. The authors show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary. The approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot."
SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that the proposed method successfully reduces overfitting. The authors also show that their VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Extensive experiments and analyses show that our method substantially improves transfer performance in low- resource scenarios."
SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method to recover the 3D shapes of objects from images using an off-the-shelf 2D generative model trained on RGB images only. The method is based on an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, yet it successfully recovers 3D shape with high precision for human faces, cars, buildings, etc. The proposed method can be applied to relighting and object rotation."
SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a multi-expert model for long-tailed recognition. The authors propose a new method to reduce the model variance with multiple experts, reduces the model bias with a distribution-aware diversity loss, and reduces the computational cost with a dynamic expert routing module. The proposed method outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks."
SP:f4d0e821de6830722a3458fd40d8d6793a107827,"This paper provides an empirical analysis of different pruning criteria for layer-wise and global pruning of convolutional neural networks. The authors show that there are two blind spots: (1) Similarity: There are some strong similarities among several primary pruning metrics that are widely cited and compared. According to these criteria, the ranks of filters’ Importance Score are almost identical, resulting in similar pruned structures. (2) Applicability: The filters' importance scores are too close to distinguish the network redundancy well. "
SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained transformer-based model for learning code representations. The authors propose to use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of “wherethe-value-comes-from” between variables. The proposed model is evaluated on four tasks, including code search, clone detection, code translation, and code refinement. "
SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method to improve the accuracy of regression models that are trained using a skewed dataset. The method forces the regression outputs to follow the true distribution; the forcing algorithm regularizes the regression results while keeping the information of the training data. The authors assume the existence of enough unlabeled data that follow the assumed true distribution and that it can be roughly estimated from domain knowledge or a few samples. They evaluated the proposed approach on four real-world datasets (pLogP, Diamond, House, Elevators). The proposed approach reduced the root mean squared error of the regression by around 55 percent to 75 percent compared to regression models without adjustment of the distribution."
SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper studies the problem of out-of-distribution generalization in deep neural networks. The authors argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, the authors propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than a 20% absolute increase in various experiments compared with baselines."
SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper proposes a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. The authors propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. Experiments on multiple RL agents and multiple environments show that the poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy with a limited attacking budget."
SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper introduces Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. The authors prove that DTR can train an N-layer linear feedforward network on an $\Omega(\sqrt{N}$ memory budget with only $O(N)$ tensor operations. In simulation, it enables training for a range of static and dynamic models under various restricted memory budgets and closely matches the performance of optimal static checkpointing."
SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collects new manually annotated evaluation sets for this task. The authors also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of the proposed approach."
SP:3d0d026888cf87073df5bd74edd986f15351ff5a,"This paper proposes a new method for conditional generative adversarial networks (cGAN). The main idea is to use NAS to find a distinct generator architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data for each generator. The proposed method follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes."
SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a regularization framework for estimating average causal effects. It formalizes unconfoundedness as an orthogonality constraint that is used to learn outcomes that are orthogonal to the treatment assignment. The authors prove theoretical guarantees that this yields an asymptotically normal estimator for the average causal effect. Based on this framework, the authors develop a neural network model called DONUT, which leverages the predictive capabilities of neural networks to estimate average causal outcomes. The experimental results show that DONUT outperforms the state-of-the-art substantially."
SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper investigates the role and expressive power of affine parameters in BatchNorm, a popular feature normalization technique that normalizes activations and then applies a learned affine transform. The authors investigate the performance achieved when training only these parameters and freezing all weights at their random initializations leads to surprisingly high performance considering the significant limitations that this style of training imposes. For example, sufficiently deep ResNets reach 82% (CIFAR-10) and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. "
SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a method for fully test-time adaptation, where the model has only the test data and its own parameters. The method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on VisDA-C benchmark."
SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method to estimate uncertainty in RNNs via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. The proposed method can (1) learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the explorationexploitation trade-off in reinforcement learning."
SP:a38c523196f68a90b5db45671f9dbd87981a024c,"This paper studies the problem of privacy-preserving deep learning. The authors propose a stochastic differential equation principled residual perturbation for privacy preserving DL, which injects Gaussian noise into each residual mapping of ResNets. Theoretically, the authors prove that the proposed method guarantees differential privacy (DP) and reduces the generalization gap for DL. Empirically, it outperforms the state-of-the-art DPSGD in both membership privacy protection and maintaining the DL models' utility."
SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes an extension of the PoWER-BERT framework, which allows to train a large-scale transformer once and use it for various inference scenarios without re-training it. To do so, the authors train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines the length of a sequence at each layer. They then use a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the computational complexity under any given computational budget. The experiments on SQuAD 1.1, MNLI-m, and SST-2 show that the proposed method can achieve up to 3x speed-up over the standard transformer without sacrificing accuracy."
SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressive power of graph neural networks (GNNs) in the context of the Weisfeiler-Lehman (WL) graph isomorphism test. The authors propose to improve the expressiveness of GNNs by exploring powerful aggregators. They reformulate aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements of the aggregation coefficients for building more powerful aggregation coefficients and injective aggregation coefficients. Based on the theoretical analysis, they develop two GNN layers, ExpandingConv and CombConv. Experimental results show that the proposed models significantly boost performance, especially for large and densely connected graphs."
SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,"This paper proposes a disentanglement evaluation metric that measures intrinsic properties of a generative model with respect to its factors of variation. The evaluation metric circumvents the typical requirements of existing evaluation metrics, such as requiring an ad-hoc model, a particular dataset, or a canonical factorization. The authors empirically evaluate several state-of-the-art models across multiple datasets and find that the method ranks models similarly to existing methods."
SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper proposes a method to make training examples ""unlearnable"", i.e., the model will not be able to learn from them. The idea is to minimize the error of one or more of the training examples close to zero, which can trick the model into thinking there is ""nothing to learn"" from these example(s). The proposed method is based on the idea of error-minimizing noise, where the noise is intentionally generated to reduce the model's error. The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. The authors empirically verify the effectiveness of the method in both sample-wise and class-wise forms."
SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,"This paper presents Nondeterministic MuZero (NDMZ), an extension of MuZero for nondeterministic, two-player, zero-sum games of perfect information. It formalizes chance as a player in the game and incorporates the chance player into the MuZero network architecture and tree search. Experiments show that NDMZ is capable of learning effective strategies and an accurate model of the game."
SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper introduces Hindsight off-policy option learning (HO2), an efficient option learning algorithm, which is based on the Hindsight Off-Policy Options (HOTO) algorithm. HO2 infers option and action probabilities for trajectories in hindsight, and performs critic-weighted maximum-likelihood estimation by backpropagating through the inference procedure. The ability to infer option choices given a trajectory, allows us to train from off-Policy trajectories, including those from different tasks and makes it possible to impose hard constraints on the termination frequency without introducing additional weighted objectives. The approach outperforms existing option learning methods and is able to solve complex, simulated robot manipulation tasks from raw pixel inputs more reliably than competitive baselines."
SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper studies the problem of maximizing the expected cumulative return in reinforcement learning. The authors formulate an objective function to maximize the expected maximum reward along a trajectory, propose a novel functional form of the Bellman equation, introduce the corresponding Bellman operators, and provide a proof of convergence. They achieve state-of-the-art results on the task of synthesizable molecule generation that mimics a real-world drug discovery pipeline."
SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model which generates parameters for extending the base model to a new feature by utilizing both existing data as well as any observations and/or metadata associated with the new feature. At prediction time, the CHN requires only a single forward pass through a neural network, yielding a significant speed-up when compared to re-training and fine-tuning approaches. The experiments demonstrate that CHNs outperform a range of baselines in terms of predictive performance across a range the datasets, in both regression and classification settings."
SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a method to perform probabilistic inference over a subset of the weights of a deep neural network. The authors propose to perform inference over only a small subset of model parameters while keeping all others as point estimates. This enables them to use expressive posterior approximations that would otherwise be intractable for the full model. In particular, the authors develop a practical and scalable Bayesian deep learning method that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. The proposed method can be applied post-hoc to any pre-trained model."
SP:be361952fe9de545f68b8a060f790d54c6755998,"This paper proposes a new approach for jointly learning embeddings for states and actions that combines aspects of model-free and model-based reinforcement learning, which can be applied in both discrete and continuous domains. Specifically, the authors use a model of the environment to obtain embedding for state and action and present a generic architecture that uses these to learn a policy. In this way, the embedded representations obtained via the approach enable better generalization over both state and actions by capturing similarities in the embedding spaces. Evaluations of the approach on several gaming, robotic control, and recommender systems show it significantly outperforms state-of-the-art models."
SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper proposes a method for learning views for unsupervised representation learning. The authors propose a generative model, called viewmaker networks, that learns to produce useful views from a given input. Viewmaker networks are stochastic bounded adversaries: they produce views by generating and then adding an $\ell_p$-bounded perturbation to the input, and are trained adversarially with respect to the main encoder network. The learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations on CIFAR-10, and outperform the baseline augmentation on speech recordings and wearable sensor data."
SP:ef7735be9423ad53059505c170e75201ca134573,"This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. The authors then utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. The results include experiments on CIFAR10, SVHN and MNIST as in-distribution data and Imagenet, LSUN, SVhN, KMNIST, and F-MNIST as OOD data across different DNN architectures such as ResNet, WideResNet, DenseNet, and LeNet5."
SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model with hierarchical latent variable models. The authors claim that VAEs can be used to represent autoregressive models, and demonstrate that the proposed model achieves higher likelihoods, uses fewer parameters, generates samples thousands of times faster, and is more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations."
SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a new method for contrastive representation learning. The authors propose to sample negative samples conditionally, i.e., in a ""ring"" around each positive, and show that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. They also show that choosing semi-hard negatives can yield stronger contrastive representations. Experiments show that the proposed method improves accuracy by 2-5% absolute points on four standard image benchmarks."
SP:613a0e2d8cbe703f37c182553801be7537333f64,This paper studies the problem of data leakage in federated learning (FL). The authors propose a data leakage attack to efficiently recover batch data from the shared aggregated gradients. The proposed method is named as catastrophic data leakage (CAFE) and it demonstrates the ability to perform large-batch data leakage attacks with high data recovery quality. Experimental results on vertical and horizontal FL settings have validated the effectiveness of CAFE in recovering private data. 
SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for dynamic multi-agent relational inference (DYARI) for unsupervised learning of dynamic relations in the context of physics. The proposed model is built on top of the existing neural relational inference model (NRI) for static relations and extends it to dynamic relations, which is a more general setting where interactions change over time. The authors conduct extensive experiments using a simulated physics system to study the performance of DYARI in handling various dynamic relations. They perform ablative study to understand the effect of dynamic and inference period, training scheme, and model design choice."
SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes an inductive collaborative filtering framework that learns a hidden relational graph among users from the rating matrix. The authors first consider a base matrix factorization model trained on one group of users’ ratings and devise a relation inference model that estimates their underlying relations (as dense weighted graphs) to other users with respect to historical rating patterns. The relational graphs enable attentive message passing from users to users in the latent space and are updated in end-to-end manner. Extensive experiments demonstrate that the model achieves state-of-the-art performance for inductive learning on several matrix completion benchmarks, provides very close performance to transductive models when given many training ratings and exceeds them significantly on cold-start users."
SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a multi-stage model for disentangled representation learning, where the disentanglement factors are first learned using a preexisting representation learning method (beta-TCVAE) and then, the low-quality reconstruction is improved with another deep generative model that is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangling factors. Taken together, the proposed model results in a coherent probabilistic model that can be realized with a variety of model classes including likelihood-based models, variational autoencoders, implicit models such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. The authors demonstrate that this model dramatically improves the reconstruction of existing disentanged representation learning methods."
SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"This paper studies the sufficiency of state representations for learning and representing the optimal policy in the context of mutual information maximization (MI) based RL methods. The authors consider the problem of learning representations that are sufficient for RL from a theoretical perspective, and study several popular MI based objectives through this lens. They find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. They corroborate their theoretical results with empirical experiments on a simulated game environment with visual observations."
SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. In particular, the authors show how neural networks implicitly attempt to solve copositive programs via semi-nonnegative matrix factorization, and draw key insights from this formulation. They describe the first algorithms for provably finding the global minimum of the vector output neural networks training problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. In the case of convolutional architectures, the computational complexity is exponential in only the filter size."
SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a method for learning disentangled, object-centric scene representations from vision and language. The proposed method builds upon recent advances in unsupervised object segmentation algorithms, notably MONet and Slot Attention. It enables them to further learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. LORL can be integrated with these algorithms that are language-agnostic. Experiments on Shop-VRBSimple and PartNet-Chairs show that language contributes to learning better representations."
SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a method for knowledge graph completion, i.e., predicting the possibility that a certain triple belongs to the knowledge graph (KG). The authors propose a general framework, named EM-RBR(embedding and rule-based reasoning), capable of combining the advantages of reasoning based on rules and the state-of-the-art models of embedding. In particular, the authors propose to utilize relational background knowledge contained in rules to conduct multi-relation reasoning link prediction. In this way, they can find the most reasonable explanation for a given triplet to obtain higher prediction accuracy. In experiments, the proposed method achieves better performance compared with previous models."
SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a method for combining declarative and procedural knowledge in the form of object files and schemata. The main idea is to use attention to determine which object files to update, the selection of objects files, and the propagation of information between object files. The proposed method is a drop-in replacement for LSTM, GRU, and other recurrent neural networks, and achieves substantially better generalization on environments that have multiple object tokens of the same type."
SP:42a3c0453ab136537b5944a577d63412f3c22560,This paper introduces a neural module network (NMN) approach for video-grounded language tasks. The main idea is to decompose the language components into entity and action components and use them as parameters to instantiate neural module networks and extract visual cues from the video. The proposed approach is evaluated on two tasks: video QA and video dialogues. The results show that the proposed approach can achieve competitive performance.
SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper proposes two variations of the Policy-Space Response Oracle (PSRO) algorithm, which is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (Deep RL). The first, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL, requiring training only against the opponent’s newest policy. The second, mixed-opponents constructs a pure-strategy opponent by mixing existing strategy’S action-value estimates, instead of their policies. Both algorithms substantially reduce the amount of simulation during training required by PSRO while producing equivalent or better solutions to the game."
SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,"This paper presents a non-attentive Tacotron, which replaces the attention mechanism in TacOTron 2 with an explicit duration predictor and Gaussian upsampling. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. The authors also propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training."
SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method to estimate the bird’s eye view layout (BEV) from a pair of stereo images. The proposed method is an end-to-end approach, which first generates a disparity feature volume using the features of the stereo images and then project it to the bird's eye view coordinates. Inverse perspective mapping (IPM) is used to map the input images and their features to the birds' eye view. The method is evaluated on KITTI (Geiger et al. 2013) and CARLA (Dosovitskiy et al., 2017) and shows state-of-the-art performance."
SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,This paper proposes a relation-aware GNN architecture based on the Graph Attention Network that uses gated skip connections to improve long-range modeling between nodes and uses a more scalable vector-based approach for parameterizing relations. The proposed method significantly outperforms several commonly used GNN variants when used in deeper configurations and stays competitive to existing architectures in a shallow setup. Experiments on synthetic and real-world datasets show that the proposed method is at least competitive with previous work.
SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a method for identifying minimal regions in an input that are most relevant for a neural network’s prediction. The method is based on the Satisfiability Modulo Theory (SMT) solvers, which is able to scale to large networks. It uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows the method to be scalable to larger networks. After solving for the minimal masks, the method scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains “where a model is looking” when making a prediction."
SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper studies the problem of robust 3D pose estimation with neural networks. The authors propose to integrate deep neural networks with 3D generative representations of objects into a unified neural architecture that learns a generative model of neural feature activations at each vertex on a dense 3D mesh. Using differentiable rendering, they estimate the 3D object pose by minimizing the reconstruction error between NeMo and the feature representation of the target image. To avoid local optima in the reconstruction loss, they train the feature extractor to maximize the distance between the individual feature representations on the mesh using contrastive learning. Experiments on occluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to partial occlusion and unseen pose compared to standard deep networks."
SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes an approach for feature compatible learning without inheriting old classifier and training data, i.e., Non-Inherent Feature Compatible Learning (NIPL). The approach requires only features extracted by the old model’s backbone and new training data and makes no assumption about the overlap between old and new data. The authors propose a unified framework for NIPL and extend it to handle the case where the old models is a black-box. Experiments on ImageNet ILSVRC 2012 and Places365 show the efficacy of the proposed approach."
SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. The authors propose to use an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200∼20,000 times faster). The empirical studies clearly find that the use of approximated gradient norm, as one of the hyper- parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). The results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalisation error is not always consistent across phases of the training process."
SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes a method for entity retrieval that generates entity names, left to right, token-by-token, in an autoregressive fashion and conditioned on the context. The authors claim that this approach allows them to capture relations between context and entity name, effectively cross-encoding both entity and context information. They also claim that the memory footprint is greatly reduced because the parameters of the encoder-decoder architecture scale with vocabulary size, not entity count, and the exact softmax loss can be efficiently computed without the need to subsample negative data. They show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, entity linking, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprints."
SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper considers the problem of routing users through a network with unknown congestion functions over an infinite time horizon. On each time step t, the algorithm receives a routing request and must select a valid path for each edge e in the selected path, incurs a cost ce = fe(x t e) + η t e, where x t e is the flow on edge e at time t, fe is the congestion function, and η e is a noise sample drawn from an unknown distribution. The algorithm observes ce, and can use this observation in future routing decisions. The routing requests are supplied adversarially. The authors present an algorithm with cumulative regret Õ(|E|t), where the regret on each timestep is defined as the difference between the total cost incurred by our chosen path and the minimum cost among all valid paths. "
SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,"This paper proposes a new masking strategy based on Pointwise mutual information (PMI) to improve the pretraining of Masked Language Models (MLMs) such as BERT. The authors show that uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. PMI-masking is an alternative to random-token masking, which is a principled approach, based on a nuanced extension of binary PMI to the n-ary case, and leads to better downstream performance, for example it surpasses RoBERTa (which uses vanilla random token masking) on the challenging reading comprehension RACE test with 6x less training over a smaller corpus."
SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,"This paper studies the effect of partially-conditioned variational inference in generative models, where the posterior is approximated by a mixture of smoothed variational posterior. The authors show that the ELBO objective forces the posterior to approximate the product of the smoothed posterior and the true posterior. They also show that this is the case in the case of traffic flow, handwritten digits, and aerial vehicle dynamics. Finally, they show that using a fully conditioned posterior improves the performance."
SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF), and obtain optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, the authors first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only O(D^2) memory and O(|D|) time. Then, beyond the generalizability bounds in expectation that demonstrate the average information for multiple trails, they derive generalisation bounds in probability to capture the learning performance for a single trail. Finally, they propose an effective communication strategy to further improve the performance of the proposed method, and validate the theoretical bounds via numerical experiments."
SP:129872706a12d89f0886c2ad0fd4083d0632343c,"This paper proposes a method to improve Random-sampling Neural Architecture Search (RandomNAS) by using a proxy search space (PS) that is only a small subset of the global search space. Specifically, the paper proposes an efficient way to Evolve the Proxy Search Space (EPS) and a simple size regularization to help RandomNAS-based algorithm jump out of the small architecture traps. In the NASBench-201 experiments, the authors show that the proposed method can achieve near-optimal NAS performance and surpass all existing state-of-the-art."
SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,"This paper proposes a method to combine imitation learning and meta-reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. The proposed method is called PERIL, which is a combination of Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL) and Meta-RL. PERIL is able to interpolate from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards."
SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the inductive bias of SGD on overparametrized convolutional neural networks with gradient-based optimization. In particular, the authors consider the setting where the input is a set of orthogonal patches and the goal is to learn a classifier that can distinguish between the patches. In this setting, they show that the dot-product between the learned pattern detectors and their detected patterns are governed by the pattern statistics in the training set. They call this phenomenon Pattern Statistics Inductive Bias (PSI) and empirically verify it in a large number of instances. They prove that if a learning algorithm satisfies PSI then its sample complexity is O(d log(d)) where d is the filter dimension. In contrast, they also show a VC dimension lower bound which is exponential in d."
SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,"This paper proposes a method for learning a representation of documents that reveals their underlying topic posterior information to linear models. The method is based on contrastive learning, i.e. finding a set of similar and dissimilar pairs of data points to find useful embeddings of data. The authors apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples."
SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive loss for multimodal generative models. The main idea is to train the model not just by the commonality between modalities, but by the distinction between “related” and “unrelated’ multimodals data. The authors propose to use the contrastive-style objective for training the generative model by exploiting the difference between ""related"" and ""unrelated"" multi-modal pairs. Experiments show that the proposed method enables data-efficient multimodality learning on challenging datasets for VAE models."
SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper proposes an energy-based prior for variational autoencoders (VAEs) that is the product of a base prior distribution and a reweighting factor, which is designed to bring the prior closer to the aggregate posterior. The reweighted prior is trained by noise contrastive estimation, and generalize it to hierarchical VAEs with many latent variable groups. Experiments on MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 show that the proposed method improves the generative performance of state-of-the-art VAEs by a large margin."
SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper considers the inverse reinforcement learning (IRL) problem in the setting of regularized MDPs, where the policy is assumed to satisfy a class of strongly convex regularizers. Theoretical results are provided to show that learning with these rewards is equivalent to a specific instance of imitation learning, i.e., one that minimizes the Bregman divergence associated with policy regularizers, and the authors propose a sample-based IRL algorithm, called RAIRL, to solve the IRL problem in this setting. Experiments on both discrete and continuous control domains demonstrate the effectiveness of the proposed algorithm."
SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes a method for scheduling language-specific (LS) paths in multilingual neural machine translation (MNMT) models. CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, CLSR can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many to one translation."
SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,"This paper proposes a Wasserstein distributional normalization (WDN) algorithm to handle noisy labels for accurate classification. The proposed method normalizes uncertain measures to data-dependent Gaussian measures by imposing geometric constraints in the 2-Wasserstein space. The authors simulated discrete SDE using the Euler-Maruyama scheme, which makes the method fast, computationally efficient, and non-parametric. In theoretical analysis, the authors derived the explicit upper-bound of the proposed WDN and experimentally demonstrated that the proposed method significantly outperforms other state-of-the-art methods."
SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a method for quantifying the uncertainty of conformal prediction, i.e. the probability that the predicted set contains the true label with a user-specified probability, such as 90%. The method is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. The method modifies an existing conformed prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scale. The results show that the proposed method outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt baseline."
SP:bf93641cbeaaa147ad0307de694e20adc23c290a,"This paper considers the problem of computing Wasserstein-2 barycenters, which is a geometric notion of weighted average of probability measures based on optimal transport. In this paper, the authors propose a scalable algorithm to compute the barycenter given sample access to the input measures, which are not restricted to being discrete. They employ input convex neural networks and cycle-consistency regularization to avoid introducing bias. They provide theoretical analysis on error bounds and empirical evidence of the effectiveness of the proposed approach in low-dimensional qualitative scenarios and high-dimensional quantitative experiments."
SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the problem of learning to separate two low-dimensional sub-manifolds of the unit sphere using deep neural networks. The authors prove that when the network width grows as a sufficiently large polynomial in L and the number of i.i.d. samples from the manifolds is polynomials in L, then gradient descent rapidly learns to classify the two manifolds perfectly with high probability. The analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width enables concentration of the randomly initialized network and its gradients."
SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,"This paper proposes a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines, while also being able to leverage off-policy data. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of existing RL algorithms. The authors provide a theoretical motivation for AWR and analyze its properties when incorporating off-Policy data from experience replay. They evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms when learning from purely static datasets with no additional environmental interactions."
SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes a method for learning the quantized weights and the bitwidth of neural networks. The method is based on the sinusoidal regularizer, which is parametrized by a parametric sinusoid. The authors propose a gradient-based method to learn the parameters of the regularizer and also learn the bit width of the network. The proposed method is evaluated on a variety of deep neural network architectures and shows competitive performance."
SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,"This paper presents a data augmentation method for neural machine translation (NMT) by using only the original training data without extra data. More accurately, it randomly replaces words or mixup with their aligned alternatives in another language when training NMT models. Experiments on both small and large scale datasets show that the proposed method significantly outperforms the baseline models."
SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"This paper proposes a real-time contribution measurement method for federated learning. The proposed method defines the impact of each agent. Furthermore, it comprehensively considers the current round and the previous round to obtain the contribution rate of each agents. To verify effectiveness of the proposed method, the work conducts pseudo-distributed training and an experiment on the Penn Treebank dataset. Comparing the Shapley Value in game theory, the proposed methods is more sensitive to both data quantity and data quality."
SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,"This paper studies the problem of learning Bayesian networks with adversarial perturbations to the input data, where a fraction of the samples are adversarially corrupted. The authors propose a nearly-linear time algorithm for this problem with a dimension-independent error guarantee. The algorithm and analysis are considerably simpler than those in previous work. "
SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-term trajectory optimization for visual model-based reinforcement learning. The method is based on the idea of collocation-based planning and adapts it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. LatCo provides a general and effective approach to longer-horizon visual planning. Empirically, the approach significantly outperforms prior model based approaches on challenging visual control tasks with sparse rewards and reward shaping."
SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper studies the effect of data curation on the performance of Bayesian neural networks (BNNs). In particular, the authors propose a generative model to model the process of data-curation, and show that the likelihood under this model closely matches the tempered likelihoods used in past work. The authors then argue that in fact, BNNs for image classification use the wrong likelihood, and that standard image datasets such as CIFAR-10 are carefully curated."
SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the speed-accuracy trade-off between autoregressive and non-autoregressive neural machine translation models. It argues that the speed disadvantage of non-auto-regressive models has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. The authors propose a simple layer allocation strategy: deep encoder, shallow decoder. Theoretical and empirical results show that the proposed strategy achieves substantial improvement in translation quality with comparable inference speed."
SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper investigates epoch-wise double descent, i.e., the test error of a DNN also shows double descent as the number of training epoches increases. Specifically, the authors extend the bias-variance analysis to epoch-wise double descent and show that the variance also contributes the most to the zero-one loss. Inspired by this result, they propose a novel metric, optimization variance (OV), to measure the diversity of model updates caused by the stochastic gradients of random training batches drawn in the same iteration. OV can be estimated using samples from the training set only but correlates well with the (unknown) test error. Based on this metric, they further propose an approach to perform early stopping without any validation set."
SP:8d8b738c676938952e62a6b2aea42e79518ece06,This paper studies the adversarial robustness of model-agnostic meta-learning (MAML). The authors propose a robustness-promoting regularization to improve the generalization of MAML. They show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. They also show that unlabeled data augmentation and contrastive representation learning can help improve the robustness.
SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,"This paper studies the generalization properties of the learning-to-learn (LTL) approach for tuning the step size for quadratic loss. In particular, the authors show that the meta-gradient can explode/vanish in the setting where the objective function is simply the loss of the last iteration. The authors also show that when the number of samples is small and the noise is large, the LTL approach generalizes better than train-by-train."
SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a graph view-consistent learning network (GVCLN) for semi-supervised node classification in the case of low label rate. GVCLN adopts a dual-view structure to learn the representation between two views, where the two views have different viewing angles, but their observation objects are the same, so their observation representations need to be consistent. Two loss functions are designed besides a supervised loss. The supervised loss uses the known labeled set, while a view consistency loss is applied to two views to obtain the consistent representation and a pseudo-label loss is designed by using the common high-confidence predictions. Experiments are conducted on three citation network datasets of Cora, Citeseer, and PubMed."
SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper studies the problem of designing the optimal physics bias for neural networks predicting the dynamics of systems when the underlying dynamics shall be inferred from the data directly. In particular, the description of physical systems is greatly simplified if the underlying symmetries of the system are taken into account. In classical systems described via Hamiltonian dynamics this is achieved by using appropriate coordinates, so-called cyclic coordinates, which reveal conserved quantities directly. The authors show that such coordinates can be searched for automatically with appropriate loss functions which naturally arise from Hamiltonian Dynamics. They test their method on standard classical physics systems using synthetic and experimental data where our network identifies the conserved quantity in an unsupervised way and find improved performance on predicting the system compared to networks biasing just to the Hamiltonian."
SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper proposes a method to combine gradient boosted decision trees (GBDT) and graph neural networks (GNNs) for heterogeneous tabular node features. The idea is to train GBDT and GNN jointly to get the best of both worlds: GBDT deals with heterogeneous features, while GNN accounts for the graph structure. The proposed method is end-to-end and can be incorporated with any message-passing neural network and gradient boosting method. Extensive experiments demonstrate the proposed architecture is superior to strong existing competitors in terms of accuracy of predictions and training time."
SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,"This paper studies the role of train-validation split in meta-learning. In particular, the authors consider the setting where the number of tasks goes to infinity. They show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general without structural assumptions on the data. They also show that data splitting may not always be preferable, especially when the data is realizable by the model. Finally, they validate their theories by experimentally showing that non-Splitting method can indeed outperform splitting method."
SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes a method to extract hard examples, i.e., examples that are close to the decision boundary, are essential to shaping accurate classifiers. The authors propose a method called Me-Momentum, which alternately updates the confident examples and refine the classifier. The extracted confident examples in the previous round can be exploited to learn a better classifier and help identify better (and hard) confident examples. Empirical results on benchmark-simulated and real-world label-noise data show the effectiveness of the proposed method."
SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,This paper studies the robustness of kNN and rNN against data poisoning attacks. The authors show that the intrinsic majority vote mechanisms in kNN provide certified robustness guarantees against general data poisoning attack. They also derive a better lower bound of certified accuracy for rNN via jointly certifying multiple testing examples. The empirical evaluation results on MNIST and CIFAR-10 show that intrinsic certified guarantees are better than state-of-the-art certified defenses.
SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for training graph neural network (GNN) with SGD method. To reduce the training time while keeping a decent model performance, the authors propose a metric that combines both the variance of gradients and compute time for each mini-batch. They theoretically analyze how batch-size influence such a metric and propose the formula to evaluate some rough range of optimal batch size. They complement their theoretical results with extensive empirical experiments for ClusterGCN, FastGCN and GraphSAINT on 4 datasets: Ogbn-products, Ogbnarxiv, Reddit and Pubmed."
SP:30d97322709cd292a49f936c767099f11b0e2913,"This paper proposes a method to calibrate the classifier’s inherent confidence indicators and estimates uncertainty of the calibrated confidence scores using Gaussian Processes. The proposed method, RED, is based on a GP model, and the uncertainty is estimated using the mean and variance of the predicted confidence scores. The method is evaluated on UCI datasets and vision tasks. The results show that the proposed method outperforms other confidence estimation methods."
SP:131b3da98f56d3af273171f496b217b90754a0a7,"This paper proposes a method to train an information retrieval module for downstream tasks, without using pairs of queries and documents as annotations. The approach is inspired by knowledge distillation, where the retriever module corresponds to the student model and the reader module corresponding to the teacher model. In particular, the authors use the cross-attention scores, from a sequenceto-sequence reader, to obtain synthetic targets for the retrieval module. They compare different ways to aggregate the scores, and show that iteratively training the reader and the retrieval leads to better performance on competitive question answering benchmarks."
SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper studies the problem of designing a safe controller for reinforcement learning. In particular, the authors consider the setting where an agent is constrained from acting freely in order to satisfy safety conditions. In this setting, the constraints are represented as finite automata, which can be used to efficiently recognize constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function. The authors empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo and Atari environments."
SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model to improve the comprehensibility of classifications. The key insight is to separate the notion of a decision path and an explanation path. Instead of having one monolithic decision tree, the authors build several smaller decision subtrees and cascade them in sequence. The cascading decisions subtrees are designed to specifically target explanations for positive classifications and each subtree identifies the smallest set of features that can classify as many positive samples as possible, without misclassifying any negative samples. The authors evaluate their algorithm on standard datasets, as well as new real-world applications and find that their model shortens the explanation depth by over 40.8% for positive classification."
SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the effect of the number of parameters and the width of the network on the performance of deep neural networks. The authors propose to use the Gaussian process kernel as a proxy for the network width, and show that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of the model performance. They also show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while the size of the weights is secondary."
SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,"This paper proposes a joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module produces embeddings for entities in text while the language module generates context-aware initial embedding for entities and relations in the graph. Experiments on several knowledge-aware NLP tasks show that the proposed framework achieves superior performance by effectively leveraging knowledge in language understanding."
SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes an unsupervised learning method for learning autoregressive orderings in a data-driven way without a domain-specific prior. The proposed method is based on variational inference with the space of orderings as a latent variable. The authors propose a practical algorithm for end-to-end optimization using policy gradients to optimize the encoder of the variational objective, and propose an encoder architecture that conditions on training examples to output orderings. Empirical results on sequence modeling tasks suggest that the algorithm is capable of discovering various orderings for different sequences that are competitive with or even better than fixed and predefined orders."
SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a general doubly variance reduction scheme for sampling-based sampling methods in graph convolutional neural networks. The authors show that the induced variance can be decomposed into node embedding approximation variance (zeroth-order variance) during forward propagation and layerwise-gradient variance (first-order variances) during backward propagation. They theoretically analyze the convergence of the proposed scheme and show that it enjoys an O(1/T) convergence rate. They also show that integrating the proposed schema in different sampling methods and applying them to different large real-world graphs.
SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a method for training a conditional adversarial generator on a single image to perform complex image manipulation tasks. The proposed method is based on thin-plate-spline augmentation, which augments the input image by modifying the primitive representation of the image (e.g. edges and segmentation) to the image itself. At manipulation time, the generator can be used to make general image changes. Experiments are conducted to evaluate the proposed method and show its effectiveness."
SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,"This paper proposes a method for maximum common subgraph (MCS) detection, which is an NP-hard MCS problem. The proposed method is based on Graph Neural Network (GNN) and uses a branch and bound algorithm as the backbone search algorithm to extract subgraphs by selecting one node pair at a time. In order to make better node selection decision at each step, the authors replace the node selection heuristics with a novel task-specific Deep Q-Network (DQN), which allows the search process to find larger common sub-graphs faster. Experiments on synthetic and real-world large graph pairs demonstrate that the proposed method outperforms state-of-the-art MCS solvers and neural graph matching network models."
SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes an end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The architecture gradually builds up the model: It starts by encoding the points into feature vectors, identifies a pool of candidate vertices, then prunes those candidates to a final set of corner vertices and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe. All steps are trainable, and errors can be backpropagated through the entire sequence. The method achieves very promising results for man-made, polyhedral objects, going one step further than low-level edge detectors, while at the same time outperforming them on the isolated vertex and edge detection tasks."
SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies the convergence of stochastic gradient methods in the setting where the second moment of the oracle is unknown and the noise level is changing over iterations. The authors show that under mild assumptions, one can achieve faster convergence than the fixed step SGD by applying online noise estimation and using adaptive step sizes. The analysis provides one explanation for the recent success of adaptive methods in neural network training."
SP:71c4e6ab911962d730461eda0f2d72d810fc017c,"This paper proposes a method to improve the performance of neural machine translation (NMT) models by incorporating prior word alignment information into the training process. Specifically, the paper proposes an enhancement learning model, which can learn how to replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and efficient way. The method achieves BLEU improvements over a strong baseline model on English-Korean, English-to-German and English-Romanian translation tasks."
SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper proposes a benchmark for evaluating RL algorithms in environments with varying levels of hardness (i.e. different levels of reward, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action range) in a low-cost way. The authors define a collection of toy benchmarks in OpenAI Gym by varying these dimensions. They show that these benchmarks present substantial challenges to current RL algorithms. They also evaluate the kinds of transfer for these dimensions that may be expected from these benchmarks to more complex benchmarks."
SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes a method for regression calibration. The method is based on the idea of quantile calibration (Kuleshov et al., 2018), recast it as entropy estimation, and leverage the new formulation to construct a novel quantile regularizer, which can be used as a blackbox to calibrate any probabilistic regression model. Unlike most of the existing approaches for calibrating regression models, which are based on post hoc processing of the model’s output and require an additional dataset, the proposed method is trainable in an end-to-end fashion, without requiring additional dataset. The authors conduct experiments to show effectiveness of their proposed method."
SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,"This paper proposes a deep generative probabilistic framework for dense 3D reconstruction and 6-DoF localisation. The proposed method is based on variational inference, neural networks and a differentiable raycaster. The method is evaluated on realistic drone flight data and achieves comparable performance to state-of-the-art visual-inertial odometry systems. "
SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for leveraging textual descriptions to improve the generalization of control policies to new scenarios. The proposed method, EMMA (entity mapper with multi-modal attention), is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision. The authors design a framework of 1320 games and collect text manuals with free-form natural language via crowd-sourcing. They demonstrate that EMMA achieves successful zeroshot generalization to unseen games with new dynamics, obtaining significantly higher rewards. The grounding acquired by EMMA is also robust to noisy descriptions and linguistic variation."
SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,This paper proposes a new policy gradient method for training the critic in the actor-critic framework. The critic is trained using a new state-value function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value. The authors prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. 
SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper investigates the effect of increasing the depth within an overparameterized regime. The authors introduce local and global labels as abstract but simple classification rules to gain an insight into the advantage of depth. It turns out that the locality of the relevant feature for a given classification rule plays a key role; the experimental results suggest that deeper is better for local labels, whereas shallower is good for global labels."
SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies few-shot learning via representation learning, where one uses T source tasks with n1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n2(n1) data. Specifically, the authors focus on the setting where there exists a good common representation between source and target, and their goal is to understand how much a sample size reduction is possible. They provide a risk bound of $\tilde{O}(\sqrt{d}(d^k n1T + k n2^2)$ for the linear representation class, where d is the ambient input dimension and k(d) is the dimension of the representation. This result bypasses the i.i.d. task assumption and can capture the desired property that all n1-T samples from source tasks can be pooled together for representation learning. They further extend this result to handle a general representation function class and obtain a similar result. Finally, they consider the case where the common representation may be high-dimensional but is capacity-constrained (say in norm), and show that representation learning can fully utilize all n2-N samples from the source tasks."
SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,This paper proposes a method to study the robustness of neural networks to perturbations of the input features. The authors propose a black-box approach to determine the features for which a network is robust or weak. They leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. They evaluate their approach with PCA features.
SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task learning in reinforcement learning (RL) that automatically clusters tasks into related subsets. The method is inspired by the expectation-maximization algorithm, which finds clusters of related tasks and uses these to improve sample complexity. The proposed method is intuitive, simple to implement and orthogonal to other multi- task learning algorithms. The authors show the generality of their approach by evaluating on simple discrete and continuous control tasks, as well as complex bipedal walker tasks and Atari games."
SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning framework for learning generalizable representations for non-stationary time series. The proposed method, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal’s generative process to define neighborhoods in time with stationary properties. The framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distributions of non-neighboring signals. The motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients’ latent states in settings where labeling data is practically impossible. Experiments on clustering and classification tasks for multiple datasets demonstrate superior performance of TNC."
SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,"This paper proposes a method for learning modular networks for multi-task learning. The proposed method is based on the idea of conditional computation and modular networks, which allows soft weight sharing between tasks with only a small increase in the number of parameters. The authors show that their method leads to interpretable self-organization of modules in case of multi- task learning, transfer learning and domain adaptation while achieving competitive results."
SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the problem of posterior collapse in variational autoencoders (VAEs), which is the phenomenon that the learned latent space becomes uninformative due to oversmoothing. The authors claim that the posterior collapse problem is related to local optima of the objective function that are often introduced by a fixed hyperparameter resembling the data variance. They suggest that this variance parameter regularizes the VAE and affects its smoothness, which can cause oversmoothness and leads to posterior collapse. They propose AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower BOund). It controls the strength of regularization by adapting the variance parameter, and thus avoids over-smoothhing the model."
SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes a VAE-based generative model that is able to capture both local data features and global features among batches of data samples in an unsupervised fashion. The proposed method is based on a mixture model in the local or data-dependent space and a global Gaussian latent variable, which leads to the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in beta-VAE and its generalizations). Second, the model performs domain alignment to find correlations and interpolate between different databases. Finally, the authors study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures."
SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"This paper proposes to use human interaction and attention cues to improve the performance of self-supervised representation learning. The authors collect a dataset of human interactions capturing body part movements and gaze in their daily lives and use these cues to learn a visual embedding. The experiments show that the learned representation outperforms a visual-only state-of-the-art method MoCo (He et al., 2020) on a variety of target tasks: scene classification, action recognition, depth estimation, dynamics prediction, and walkable surface estimation."
SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the problem of negative pretraining, where a pretrained model obtains a worse generalization performance than a model that is trained from scratch when either are trained on a target task. The authors propose three interventions to remove and fix it. First, they show that increasing the learning rate after pretraining can yield better results than training directly on the target task, on the learning task-level, and on the discretization of data distribution changes from start to target task instead of “jumping” to a target. Finally, they also show that resetting the network biases to larger values can also remove negative preraining effects."
SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,"This paper studies the problem of adversarial robustness, i.e., the robustness of a classifier to adversarial perturbations. The authors propose a bi-level optimization algorithm that can find safe spots on over 90% of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets. The experiments also show that they can be used to improve both the empirical and certified robustness on smoothed classifiers. Furthermore, by exploiting a novel safe spot inducing model training scheme and a safe spot generation method, they propose a new out-of-distribution detection algorithm."
SP:1350ab543b6a5cf579827835fb27011751cc047f,"This paper proposes a point spatio-temporal (PST) convolution to disentangle space and time in point cloud sequences. A spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution are used to model the dynamics of the spatial regions along the time dimension. Furthermore, the authors incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of the proposed method."
SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices for text to speech (TTS) service in commercial speech platforms. The authors propose several techniques to address the two challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions which could be very different from source speech data, and 2) the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. They pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets with few adaptation data, e.g., 20 sentences, about 1 minute speech. The audio samples are available at https://speechresearch.io/adaspeech/."
SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,"This paper studies the gradient flow of training sparse neural networks. The authors propose a tool to measure the impact of architecture and optimization choices on sparse networks. They show that weight decay and data augmentation can hurt optimization, when adaptive optimization methods are used and this usually corresponds to a much higher EGF. They also show that batch normalization is critical to training sparse networks, more so than dense networks."
SP:d9f17344cd266b16a70c37d891b2c64a6d454908,"Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. This paper studies the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature of one node is spread over its neighbors; and (2) feature and label influence of how much the initial feature/labels influence the final features/label of another node. Based on the theoretical analysis, this paper proposes an end-to-end model that unifies GCN and LPA for node classification. In this unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN to learn proper edge weights that lead to improved classification performance. Experiments on five datasets demonstrate that the proposed model outperforms state-of-the-art baselines."
SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of CNNs in the setting where the ground truth labels are generated by a function within another function space, which the authors call the generator space. The authors show that the R-Complexity of both the classifier and the generator function spaces determines the generalizability of the training error. They also propose a joint entropy-like measure of complexity between function spaces (classifier and generator) called co-complexity, which leads to tighter bounds on the generalisation error in this setting. "
SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a novel post-training quantization (PTQ) framework, BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. It leverages the basic building blocks in neural networks and reconstructs them one-by-one. The mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Extensive experiments are conducted for both image classification and object detection tasks."
SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the effect of label noise and class imbalance on the calibration of deep neural networks. The authors show that label noise significantly increases the calibration error, and that poor calibration can come from small dataset sizes. They suggest that for sensitive applications, one should evaluate the calibration when collecting datasets. "
SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper studies the problem of emergent communication between agents that learn to communicate via actuating their joints in a 3D environment. The agents are assumed to have different intents and a non-uniform distribution of intents, and a common knowledge energy cost. The authors show that under realistic assumptions, the agents can find protocols that generalize to novel partners. They also explore and analyze specific difficulties associated with finding these solutions in practice."
SP:5ba686e2eef369fa49b10ba3f41f102740836859,"This paper proposes a meta-modeling framework for uncertainty quantification in sequential regression. The proposed method is able to generate symmetric and asymmetric uncertainty estimates, makes no assumptions about stationarity, and outperforms competitive baselines on both drift and non-divergence scenarios. This work helps make sequential regression more effective and practical for use in real-world applications."
SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"This paper proposes two unbalanced Gromov-Wasserstein (UGW) divergences for the comparison of metric measure spaces endowed with a probability distribution. The first is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. The second is a distance between mm-spaces up to isometries using a conic lifting. The authors provide a scalable, GPU-friendly algorithm to compute UGW and demonstrate its applicability in learning tasks."
SP:47dcefd5515e772f29e03219c01713e2403643ce,"This paper proposes a novel pruning method, called all-alive pruning (AAP), which aims to eliminate dead connections and make all weights in the subnetwork trainable during the pruning process. The main idea is that dead connections do not contribute to the model capacity, and the proposed method aims to make the pruned networks with only trainable weights. The proposed method is applicable to various saliency-based pruning methods and model architectures, and is shown to improve the accuracy of existing methods at 128x–4096x compression ratios."
SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for controllable semantic image editing. The proposed method learns a latent space and suitable latent-space transformations for each attribute. The paper proposes to learn multiple attribute transformations simultaneously, integrate attribute regression into the training of transformation functions, and apply a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. The method is evaluated on both natural and synthetic images and achieves state-of-the-art performance."
SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,"This paper proposes an adversarial training framework for discrete sequence generation, by leveraging the advance of Feature Statistics Alignment (FSA) and Gumbel-Softmax relaxation. FSA forces the mean statistics of the fake data distribution to approach that of real data as close as possible in a finite-dimensional feature space. Experiments on synthetic and real-world benchmark datasets show the superior performance in quantitative evaluation and demonstrate the effectiveness of the proposed approach."
SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper studies the problem of reward progressivity, i.e., the property that the rewards tend to increase in magnitude over time. The authors hypothesize that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, the authors propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressiveness, the proposed method is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, the method remains more than competitive."
SP:bff215c695b302ce31311f2dd105dace06307cfc,"This paper introduces a notion of sufficient information contained in the representation learned by a deep network, and uses it to study how optimal representations for the task emerge during training. They show that the implicit regularization coming from training with Stochastic Gradient Descent with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations. In particular, they find that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded. They also evaluate how perturbing the initial part of training impacts the learning dynamics and the resulting representations."
SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper studies the problem of nonconvex-strongly-concave min-max optimization, where the objective function is convex but strongly concave. The authors propose an improved variance reduction algorithm (SREDA) based on the recent work of Luo et al. (2020) that achieves the optimal complexity dependence on the required accuracy level. However, the convergence guarantee of SREDA requires stringent initialization accuracy and an accuracy-dependent stepsize for controlling the per-iteration progress, so that SREDa can run very slowly in practice. This paper develops a novel analytical framework that guarantees the SRED a’s optimal complexity performance for a much enhanced algorithm SRED-Boost, which has less restrictive initialization requirement and accuracy-independent (and much bigger) stepsize. In addition, the authors propose a zeroth-order algorithm named ZO-SREDa-Boost for the scenario that has access only to the information about function values not gradients, and show that it outperforms the best known complexity dependence dependence on."
SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies the problem of few-shot object detection. The authors show that increasing the number of object categories during training can improve the generalization from seen to unseen classes from 45% to 89% and improve the state-of-the-art on COCO by 5.4 %AP50 (from 22.0 to 27.5). The authors also verify that the effect is caused by the number categories and not the number training samples, and that it holds for different models, backbones and datasets. This result suggests that the key to strong few shot detection models may not lie in sophisticated metric learning approaches, but instead simply in scaling the number category."
SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper presents a method for single-view implicit surface reconstruction from a single image. The authors propose to use the spatial gradient of the implicit neural shape function (SDF) as a source of supervision for single view reconstruction. The main contribution of this paper is a novel closed-form Differentiable Gradient Sampling (DGS) solution that enables backpropagation of the loss on spatial gradients to the feature maps, thus allowing training on large-scale scenes without dense 3D supervision. Experiments on ShapeNet and ScannetV2 demonstrate the effectiveness of the method."
SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper proposes a simple training strategy called “Pseudo-to-real” for high-memory footprint-required large models. It is compatible with large models with architecture of sequential layers. The authors demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. They also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities."
SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper studies the duality of energy-based generative models (EBMs). In particular, the authors consider EBMs with shallow overparametrized neural network energies, both in the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. The authors also consider a variant of this algorithm where the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training."
SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies the lower bounds of differentially private convex functions in the unconstrained and constrained settings. The lower bounds are improved over the existing upper bounds by a logarithmic factor. The main contribution of this paper is the introduction of a novel loss function for the constrained setting, which improves the upper bound of Bassily et al. (2014) by a factor of log(1/\delta) and log(p/n) for the pure case. "
SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The key of the method is a variational formulation of the objective function, which makes it possible to realize the JKO proximal map through a primal-dual optimization. The framework covers all the classical Wasserststein gradient flows including the heat equation and the porous medium equation. The authors demonstrate the performance and scalability of the algorithm with several numerical examples."
SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed meta-feature with the space of distributions on the hyper- parameter configurations. The authors claim that MetaBu induces a topology on the set of datasets that is exploited to define a distribution of promising hyperparameter configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta features boosts the performance of state-of-the-art AutoML systems, AutoSkLearn (Feurer et al. 2015) and Probabilistic Matrix Factorization (Fusi et al 2018). Furthermore, the inspection of MetaBu Meta-features gives some hints into when an ML algorithms does well."
SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a federated learning (FL) strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, it learns a set of base sub-networks that are aggregated on-demand according to inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate the effectiveness of the principle in adjusting model widths and model robustness when much fewer parameters are used."
SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper introduces a new algorithm for solving nonconvex-nonconcave minimax problems, which is a generalization of variational inequalities in the weak MVI setting. The main idea of the paper is to use the weak variational inequality (weak MVI) as a condition for the convergence of an extragradient-type algorithm. The algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. The proposed algorithm also converges globally even in settings where the underlying operator exhibits limit cycles. Moreover, a variant with stochastic oracles is proposed—making it directly relevant for training of generative adversarial networks."
SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies neural contextual bandits, a general class of contextual bandits where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. The authors propose a novel learning algorithm that transforms the raw feature vectors using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). They prove that under standard assumptions, the proposed algorithm achieves $\tilde{O}(\sqrt{T})$ regret, where T is the learning time horizon."
SP:a9a2c21110e00f19882d27bef0063c422a15e576,This paper proposes a Shapley-inspired methodology for training action space selection for reinforcement learning (RL). It categorizes the training action sets into dispensable and indispensable groups. It also ranks different training actions to facilitate high-performance yet cost-efficient RL model design. The effectiveness of the methodology is illustrated using a cloud infrastructure resource tuning case study.
SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"This paper studies the problem of constructing probably approximately correct (PAC) prediction sets in the presence of covariate shift. The authors propose a novel algorithm for building PAC prediction sets under covariate shifts, which leverages rejection sampling and the Clopper-Pearson interval, and accounts for uncertain IWs. They demonstrate the efficacy of their approach on natural, synthetic, and adversarial data sets. "
SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper studies the generalization error of iterative semi-supervised learning (SSL) algorithms that iteratively generate pseudo-labels for a large amount of unlabelled data to progressively refine the model parameters. In particular, they seek to understand the behaviour of the generalisation error of the iterative SSL algorithms using information-theoretic principles. The authors first show that when the class conditional variances are not too large, the upper bound on the generalizability error decreases monotonically with the number of iterations, but quickly saturates. The theoretical results are corroborated by extensive experiments on several benchmark datasets such as the MNIST and CIFAR datasets in which they notice that the generalizing error improves after several pseudo-labeling iterations but saturates afterwards."
SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method for model-free reinforcement learning. The key idea is to generate multi-step plans, which can be used to guide the agent towards high-value regions of the environment. The proposed method is evaluated on a number of continuous control tasks and compared with several baselines. The results show that the proposed method outperforms the baselines in terms of exploration efficiency and exploration success."
SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper presents a framework of multi-mode deep matrix and tensor factorizations to explore and exploit the full nonlinearity of the data in matrices and tensors. The authors use the factorization methods to solve matrix/tensor decomposition problems and prove that their methods have tighter generalization error bounds than conventional matrix/te tensor decomposition methods. The experiments on synthetic data and real datasets show that the proposed methods have much higher recovery accuracy than many baselines.
SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"This paper proposes a method to explain the behavior of structured output models. The authors focus on one of the outputs as the target and try to find the most important features utilized by the structured model to decide on the target in each locality of the input space. They introduce an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. The effectiveness of the proposed method is confirmed using a variety of simulated and real data sets."
SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a risk-sensitive deep reinforcement learning (DRL) approach for distributional DRL, where the distribution of full-episode outcomes is optimized to maximize a chosen function of its cumulative distribution function (CDF). This technique allows for outcomes to be weighed based on relative quality, does not require modification of the reward function to modulate agent behavior, and may be used for both continuous and discrete action spaces. The authors show how to achieve an asymptotically consistent estimate of the policy gradient for a broad class of CDF-based objectives via sampling, subsequently incorporating variance reduction measures to facilitate effective on-policy learning. They use the resulting algorithm to train agents with different “risk profiles” in penalty-based formulations of six OpenAI Safety Gym environments, observing that moderate emphasis on improvement in training scenarios where the agent performs poorly both increases the accumulation of positive rewards and decreases the frequency of incurred penalties."
SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning framework to proactively learn a deep learning surrogate model and accelerate simulation of large-scale, spatiotemporal, age-structured epidemic models. The framework is based on the novel integration of neural process, deep sequence model, and active learning. In particular, the authors develop a novel spatio-temporal neural process model to mimic the simulator dynamics. The model automatically infers the latent process which describes the intrinsic uncertainty of the simulator. This also gives rise to a new acquisition function based on latent information gain. The authors perform theoretical analysis and demonstrate that their approach reduces sample complexity compared with random sampling in high dimension."
SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"This paper proposes to use DP-SGD for training large pretrained models for NLP tasks. The authors show that the performance drop can be mitigated with (1) the use of large pre-trained models, (2) hyperparameters that suit DP optimization, and (3) fine-tuning objectives aligned with the pretraining procedure. The paper also proposes a memory saving technique that allows clipping to run without instantiating per-example gradients for any linear layer in the model. The technique enables training Transformers with almost the same memory cost as non-private training at a modest run-time overhead."
SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for optimizing the skeletal structure and joint attributes of a robotic agent. The main idea is to learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent’s skeletal structure, and then applies control actions under the new design. To handle a variable number of joints across designs, the authors use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Experiments show that the proposed method outperforms prior methods in both convergence speed and final performance by an order of magnitude."
SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a method to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX, which splits the initial layers to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP."
SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method to infer object-centric representations of visual scenes without relying on annotations. The method learns to decompose a scene into multiple objects, with each object having a structured representation that disentangles its shape, appearance and 3D pose. Each object representation defines a localized neural radiance field that is used to generate 2D views of the scene through a differentiable rendering process. The model is subsequently trained by minimizing a reconstruction loss between inputs and corresponding rendered scenes. The authors empirically show that INFERNO discovers objects in a scene without supervision. They also validate the interpretability of the learned representations."
SP:05c61145f3fc9486728aca19c4543065fe04e99c,"This paper proposes a method for evaluating the importance of subgraphs in the explanation of a graph neural network (GNN). The authors argue that a distribution shift exists between the full graph and the subgraph, which causes the out-of-distribution problem. The authors propose to use a generative model to generate plausible surrogates of these surrogate variables, which can be used to evaluate the causal effect of an explanatory subgraph on the model prediction. Empirical results demonstrate the effectiveness of the proposed method."
SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper investigates whether pretrained models are better active learners, capable of asking for example labels that disambiguate between the possible tasks a user may be trying to specify. The authors show that finetuning pre-trained models with data acquired through simple uncertainty sampling achieves the same accuracy with up to 6x fewer labels compared to random sampling. The examples chosen by these models are preferentially minority classes or informative examples where the spurious feature and class label are decorrelated. Notably, gains from active learning are not seen in unpretrained models, which do not select such examples, suggesting that the ability to actively learn is an emergent property of the pretraining process."
SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. The proposed model is based on the deleted sub-tree reconstruction, which is used to enrich the model with implicit knowledge of program structures from unlabeled source code. The pre-training objective is made consistent with the bug fixing task to facilitate the downstream learning. Experimental results show that GRAPHIX significantly outperforms a wide range of baselines including CodeBERT and BART."
SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper proposes a new method to improve the performance of Federated Adversarial Training (FAT) by relaxing the inner-maximization of adversarial training into a lower bound friendly to Federated Learning (FFL). Specifically, the authors propose to apply an $\alpha$-weighted relaxation of the adversarial loss into the standard FAT loss to relax the inner optimization. The authors provide the theoretical analysis and empirical evidences to understand the proposed simple but effective method."
SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a data programming-based framework for continual learning (LML) that can be used on top of existing continual learning frameworks such as CNNL, ORDisCo and DistillMatch. The proposed method, called Mako, is able to leverage a limited number of labeled data and an unlimited number of unlabeled data at the task level. It achieves similar performance, in terms of per-task accuracy and resistance to catastrophic forgetting, as compared to fully labeled data. The paper is well-written and easy to follow."
SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes a method to generate adversarial examples that are both misclassified by the model and detected as non-adversarial at the same time. This is achieved by orthogonalizing the gradients when running standard gradient-based attacks. The authors show that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. They use their technique to evade four state-of-the-art detection defenses, reducing their accuracy to 0% while maintaining a 0% detection rate."
SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes an energy-based approach to solve the problem of value-based learning in cooperative games. The proposed approach is based on variational inference of the energy based model, which recovers classical game-theoretic valuation criteria through conducting one-step fixed point iteration for maximizing the ELBO objective. The paper also provides a trajectory of the variational valuations, among which the one with the best conceivable decoupling error is defined as the Variational Index. Empirical results are provided to demonstrate the effectiveness of the proposed approach."
SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty, i.e., the part of the out-of-sample prediction error due to the lack of knowledge of the learner. The method is based on directly estimating the generalization error and subtracting an estimate of aleatoric uncertainty, which is a measure of intrinsic uncertainty. The authors argue that this estimator is useful in interactive learning environments arising in active learning or reinforcement learning. Experiments on sequential model optimization and reinforcement learning tasks demonstrate the effectiveness of the proposed method."
SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes a method called Givens coordinate descent (GCD) to iteratively learn a rotation matrix, in the context of end-to-end trainable PQ-based embedding indexes. The proposed method is based on geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n). The authors propose a family of block Given coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. Compared to the state-of-the-art SVD method, the proposed GCD algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to experimental studies. They further improve upon vanilla product quantization significantly in an end to end training scenario."
SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework to learn visual analogies from Raven’s Progressive Matrices (RPM), an abstract visual reasoning test of fluid intelligence. The framework uses (1) a multi-task visual relationship encoder to extract constituent concepts from raw visual input in the source domain and (2) a neural module net-based analogy inference engine to reason compositionally about the inferred relation in the target domain. The approach is able to generalize systematically to novel target domains, compensate for the lack of a prior in candidate selection, and successfully exploit the process of extracting and mapping the relationship structure."
SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a contrastive self-supervised method for nucleotide genome representation learning. The proposed method learns and parameterizes the latent space by leveraging the reverse-complement of genomic sequences. During the training procedure, the authors force the framework to capture semantic representations with a novel context network on top of features extracted by an encoder network. The network is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that the proposed method with self/semi-supervision settings outperforms state-of-the-art deep learning methods."
SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"This paper proposes a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. The authors derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, the authors show how our model parameters are fully steerable at inference time. The proposed spherical filter banks enable making equivariant and, after online optimization, invariant class predictions for known point sets in unknown orientations."
SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,This paper compares the performance of pretrained language models (PLMs) and continual learning methods (CL methods) on 3 benchmarks in 2 typical incremental settings. The authors also conduct a layer-wise and task-wise analysis to dissect the performance characteristics of the pretrained PLMs. The results reveal interesting performance differences across PLMs and across CL methods. The insights gained from this study open up new research questions.
SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,This paper proposes a defense against directed deviation attacks in federated learning. The proposed defense is based on the intuition that certain patterns of gradient flips are indicative of an attack. It assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. The authors show that TESSERACT provides robustness against even a white-box version of the attack.
SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper proposes a debiasing method for regression functions defined by linear functionals of high-dimensional or non-parametric regression functions. The proposed method is based on the Riesz representation of the linear functional using Neural Nets and Random Forests. The authors propose a multi-tasking Neural Net debiased method with stochastic gradient descent minimization of a combined Riesze representer and regression loss, while sharing representation layers for the two functions. They also propose a Random Forest method which learns a locally linear representation of RiesZ function. "
SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. In this problem, a set of N agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network. This paper considers a practical MARL setting, where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. The authors propose a mini-batch Markovian sampled fully decentralized actor critic algorithm and analyze its finite-time convergence and sample complexity. The sample complexity bound matches that of the state-of-the-art single-agent actor critic algorithms."
SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper studies the role of data augmentation in contrastive self-supervised learning (SSL) and proposes a theoretical understanding of how it works. In particular, it shows that different samples from the same class could be bridged together with aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. The paper also shows that the proposed theory aligns well with existing contrastive methods on both synthetic and real-world datasets."
SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a generative framework for general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. The proposed method is evaluated with additive noise, room reverberation, low-resolution, clipping, and additive noise and achieves a 0.499 higher mean opinion score (MOS) than the speech denoising SSR model."
SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a framework for multivariate time series forecasting based on the idea of low-rank approximation to model the variable space. Theoretical analysis of the latent variable space is provided, and a tensor network based on low rank approximation is proposed. The tensor components are shared to ensure the translation invariance of the network. An N-order residual connection approach is proposed to improve the ability to model long-term sequences, and the series-variable encoder is used for improving the quality of the model. Experiments are conducted on four time series datasets."
SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"This paper proposes a variant of SCO algorithms with sparse moving averages for GNN training. By storing the moving averages in the most recent iterations, the proposed algorithm only requires a fixed size buffer, regardless of the graph size, and preserves the convergence rate of the original SCO algorithm when the buffer size satisfies certain conditions. The experiments validate the theoretical results and show that the proposed method outperforms the traditional Adam SGD with a small memory overhead."
SP:72e0cac289dce803582053614ec9ee93e783c838,"This paper proposes Circulant MinHash (C-MinHash) to approximate the Jaccard (resemblance) similarity in massive binary (0/1) data. The basic theory of MinHash requires applying hundreds or thousands of independent random permutations to each data vector in the dataset, in order to obtain reliable results for (e.g.,) building large-scale learning models or approximate near neighbor search in massive data. This paper provides the surprising theoretical results that using only two independent permutations in a circulant manner leads to uniformly smaller Jaccord estimation variance than that of the classical MinHash with K independent permutation. Experiments are conducted to show the effectiveness of the proposed method."
SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a simple and efficient training scheme to achieve adversarial robustness against the union of lp-robust models. The proposed E-AT scheme is based on geometric considerations of the different Lp-balls and costs as much as normal adversarial training against a single-lp-threat model. Moreover, it can be used to fine-tune with just 3 epochs any lp robust model (for p \in {1,2,\infty}) and achieve multiple norm adversarial attacks. In this way it can boost the state-of-the-art for multiple-norm robustness to more than 51% on CIFAR-10 and report up to the first ImageNet models with multiple norm robustness."
SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper proposes a safe multi-task learning (SMTL) model, which consists of a public encoder shared by all the tasks, private encoders, gates, and private decoders. Specifically, each task has a private encoder, a gate, and a private decoder, where the gate is to learn how to combine the public and private components for the downstream task. To reduce the storage cost during the inference stage, a lite version of SMTL is proposed, which allows the gate to choose either the public or the corresponding private component. Moreover, a variant is proposed to place all the gates after the decoder of all the task. Experiments on several benchmark datasets demonstrate the effectiveness of the proposed methods."
SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the problem of partition functions of energy-based sequence models backed by expressive parametric families. The authors show that there are no good deterministic or randomized estimates of the partition functions, which makes model selection, and therefore learning model parameters, undecidable. As alternatives, the authors consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness."
SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on these hypersurfaced would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. The authors show the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. The ASWD adaptively updates the hypersurface used to slice compared distributions by learning from data. Numerical results demonstrate that the proposed ASWD significantly outperforms other Wassertein variants for both synthetic and real-world problems."
SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a framework for improving coordination and performance of multi-agent reinforcement learners (MARL). The framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS), introduces an adaptive learner, Generator, that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. LIGS determines the best states to learn to add intrinsic rewards using a novel combination of reinforcement learning (RL) and switching controls, which leads to a highly efficient learning process. The authors demonstrate the superior performance of the proposed method in Foraging and StarCraft II tasks."
SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a new multi-hop QA model, named DOCHOPPER, that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. The model is able to “retrieve” either short passages or long sections of the document, thus emulating a multi-step process of “navigating” through a long document to answer a question. To enable this behavior, the model combines a compact neural representation of the query q with a compact representation of a hierarchical part of the documents, which can potentially be quite large. The proposed model achieves state-of-the-art results on four different QA tasks."
SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper proposes a new semi-supervised learning method for extracting refined labels (e.g. vocal characteristics) from known initial labels. The proposed method first suggests using a representation extractor based on the initial labels, then computing refined labels using a clustering algorithm to finally train a refined representation extractionor. The method is validated by applying Label Refining on recordings from the MassEffect 3 video game."
SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper proposes a new distributed learning framework for image processing tasks, allowing clients to learn multiple tasks with their private data. The key idea arises from a disentangled representation of local and non-local features using a task-agnostic Vision Transformer and a task specific head/tail. Each client learns a translation from its own task to a common representation, while the Transformer body learns global attention between the features embedded in the representation. To enable decomposition between the task-specific and common representations, the authors propose an alternating training strategy in which task specific learning for the heads and tails is run on the clients by fixing the ViT on the server, which alternates with task agnostic learning on the client side."
SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper investigates the problem of reward hacking in reinforcement learning (RL). The authors construct four RL environments with misspecified rewards and investigate how reward hacking arises as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. The authors also propose an anomaly detection task for aberrant policies and offer several baseline detectors."
SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes a generalization of TVO by replacing KL divergence with arbitary differeitiable f-divergence. The proposed f-TVO is derived from a deformed χ-geometry perspective, which is the deformed geodesic between variational posterior distribution and true posterior distribution. The authors propose a reparameterization trick and a Monte Carlo approximation scheme to optimize the objective function. Experiments on VAE and Bayesian neural network show that the proposed f -TVO performs better than cooresponding baselines."
SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper provides an empirical study of the interplay between different RL tools and methods in the continuous control setting. The authors show that the performance of policies trained using existing methods varies significantly across training runs, epochs of training, and evaluation runs. They also propose a new method, Ensemble Deep Deterministic Policy Gradients (ED2), which combines the ensemble-based actor-critic exploration and weighted Bellman backup to achieve state-of-the-art performance on the Gym MuJoCo continuous control tasks."
SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of fair supervised learning under the Equalized Loss (EL) fairness notion, which requires the prediction error/loss to be equalized across different demographic groups. The authors propose a number of algorithms that find the global optimal solution to this non-convex optimization problem. They also introduce a simple algorithm that is computationally more efficient and finds a sub-optimal EL fair predictor using unconstrained convex programming tools."
SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper revisits systematic generalization from the perspective of meaningful learning, an exceptional capability of humans to learn new concepts by connecting them with other previously known knowledge. The authors propose to reassess models’ compositional skills conditioned on the semantic connections between new and old concepts. In experiments, following the meaningful learning principle, they augment a training dataset in either an inductive or deductive manner to expose such semantic links to models. Their observations on SCAN, as well as two real-world datasets on semantic parsing suggest that modern sequenceto-sequence models, including RNNs, CNNs, and Transformers, can successfully one-shot generalize to novel concepts and compositions through semantic linking. They further demonstrate that both prior knowledge and semantic linking play a key role in achieving systematic generalisation and that inductive learning generally works better than deductive learning."
SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper presents a method for 3D shape representation learning using multi-scale wavelet decomposition. It decomposes 3D shapes into sub-bands components at multiple scales and all scales form a decomposition tree in a principled manner rooted in multi-resolution wavelet analysis. Specifically, it proposes Adaptive Wavelet Transformer Network (AWT-Net) that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-band components, using lifting scheme at multiple scale recursively and hierarchically. Then, it exploits Transformers that regard the features from different but complementary components as two holistic representations, and fuse them with the original shape features with different attentions. Experimental results show that the proposed method achieves state-of-the-art or comparable performances."
SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes two methods to combine the benefits of full and lightweight finetuning in natural language generation (NLG) tasks. In particular, the authors show that an ensemble of lightweight and full fine-tuning models achieves the best of both worlds: performance matching the better of both in-distribution (ID) and out of distribution (OOD) performance. They also show that one can achieve similar improvements using a single model instead of two with their proposed cocktail finetuned, which augments full finetune via distillation from a lightweight model. Finally, they provide some explanatory theory in a multiclass logistic regression setting with a large number of classes, describing how distillation on ID data can transfer the OOD behavior of one model to another."
SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes a method to improve the performance of weakly supervised models via active learning. In particular, the authors propose a method called Active Refinement of Weakly Supervised Models (WARM) to guide the attention of domain experts on a few data points that, when annotated, would most improve the label model’s probabilistic accuracy. Gradient updates are then backpropagated to iteratively update the parameters of the individual expert labelling functions in the weak supervision model. Experiments on multiple real-world medical classification datasets show that WARM can substantially improve the accuracy of probablistic labels used to train downstream classifiers, with as few as 30 queries to experts."
SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper considers the problem of training a classification model with group annotated training data. The authors propose a new and simple algorithm that explicitly encourages learning of features that are shared across various groups. Theoretically, the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions. Empirically, it matches or achieves better performance compared to strong contemporary baselines including ERM and Group-DRO on standard benchmarks on both minority groups and across all groups."
SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a bivariate explanation method to explain black-box models. The proposed bivariate method is based on a directed graph representation of the features. The authors define two concepts of redundancy between features, namely mutual redundancy and directional redundancy, and propose a systematic approach for detecting such redundancies through finding the cliques and sources in graph H. Experiments show the superiority of the proposed method over the existing methods."
SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a framework for interpretable policy learning, compatible with fully-offline and partially-observable clinical decision environments, and builds probabilistic tree policies determining physician actions based on patients’ observations and medical history. The proposed method, Policy Extraction through decision trees (POETREE), learns a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This policy learning method outperforms the state-of-the-art on real and synthetic medical datasets."
SP:5630707c9d0d9e21fce2efddef874e373bfed026,This paper proposes a method for automatically searching for data augmentation policies for fine-grained image classification tasks. The proposed method is based on multi-agent reinforcement learning (MARL) where each agent learns an augmentation policy for each patch based on its content and the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. Extensive experiments demonstrate that PAA improves the target network performance with low computational cost in various tasks.
SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper proposes a method to mitigate the adversarial vulnerability of adversarial examples. The authors construct a causal graph to model the generation process and define the natural and adversarial distribution to formalize the intuition of the attack. They show that the spurious correlation between labels and style variables is important for understanding and mitigating adversarial vulnerabilities. Based on this observation, they propose a method called Adversarial Distribution Alignment Method (ADAM) to improve adversarial robustness. Experiments show the efficacy of the proposed method."
SP:9f09449a47464efb5458d0732df7664865558e6f,"This paper proposes to enforce a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms. Then, it performs continual learning with filter atom swapping. In other words, it learns for each task a new filter sub space for each convolution layer, i.e., hundreds of parameters as filter atoms, but keeps subspace coefficients shared across tasks. The effectiveness of this simple scheme for continual learning is illustrated both empirically and theoretically."
SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse problem of Stein variational gradient descent (SVGD) in high-dimensional settings. The authors propose a connection between SVGD and gradient descent on the maximum mean discrepancy (MMD) objective, and show that SVGD suffers from the curse of dimensionality. They also propose a modification of SVGD that removes the bias from deterministic updates present in the “driving force”, and empirically verify that removal of such bias leads to more accurate variance estimation."
SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies adversarial training (AT) with noisy labels and adversarial examples. The authors show that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. They also show that if a point is too close to its noisy-class boundary (e.g., one step is enough to attack it), this point is likely to be mislabeled. They suggest to adopt the number the number PGD steps as a new criterion for sample selection to correct NL."
SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes a statistical method, called Robustness Measurement and Assessment (RoMA), which can measure the expected robustness of a neural network model. Specifically, RoMA determines the probability that a random input perturbation might cause misclassification. The method allows us to provide formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The approach can be applied to large-scale, black-box neural networks, which is a significant advantage compared to recently proposed verification methods."
SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) to learn representations from non-i.i.d sequential data by first discovering the minimal atomic sequential units as chunks. As learning progresses, a hierarchy of chunk representation is acquired by chunking previously learned representations into more complex representations guided by sequential dependence. The authors provide learning guarantees on an idealized version of HCM and demonstrate that HCM learns meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data. The interpretability of the learned chunks enables flexible transfer between environments that share partial representational structure."
SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of accelerating non-linear non-convex optimization problems by reparametrizing the optimization variables as the output of a neural network. In particular, the authors propose to use a specially designed graph convolutional network (GCN) which aggregates the gradients of the loss function and reduces to the Hessian in early stages of the optimization. They show the utility of their method on two optimization problems: network synchronization and persistent homology optimization, and find an impressive speedup, with the proposed method being 4 to 80x faster."
SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a method based on a layered structure of Support Vector Machine (SVM) ensembles for non-parametric image classification. By utilizing the quick learning of SVMs compared to neural networks, the proposed method can reach higher accuracy than neural networks when the training set is small. Experimental results show that while conventional DCNN architectures such as ResNet-50 outperform SVMnet when the size of the training data is large, SVMNet provides a much higher accuracy when the number of “ground truth” training samples is relatively small."
SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG) to reduce the communication and memory overhead. Each local machine trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, the authors propose to apply Global Server Corrections on the server to refine the locally learned models. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance."
SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a unified and end-to-end model approach for anytime pixel-level recognition. A cascade of “exits” is attached to the model to make multiple predictions and direct further computation. The authors redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, the authors develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. The full model with redesigned exit architecture and spatial adaptivity enables anytime inference, achieves the same level of final accuracy and significantly reduces total computation."
SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a new bootstrapping method for neural processes (NP) based on neural networks. The proposed method is inspired by the Bootstrapping Neural Processes (B(A)NP) method, which proposes a bootstrap method to capture the functional uncertainty in the stochastic processes. The main difference is that the proposed NeuBANP learns to generate the bootstrap distribution of random functions by injecting multiple random weights into the encoder and the loss function. Experiments are conducted on Bayesian optimization and contextual multi-armed bandit tasks. Results show the proposed method achieves the best performance among NP methods."
SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes a multi-modal and self-supervised approach for pre-training genome data for regulatory genome modeling. Specifically, the authors propose to simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input, where three pretraining tasks are proposed to improve the robustness and generalizability of our model. The authors pre-train our model on the ATAC-seq dataset with 17 million genome sequences. They evaluate our GeneBERT on regulatory downstream tasks across different cell types, including promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction. Extensive experiments demonstrate the effectiveness of the proposed method."
SP:841b12443d0274e34b78940f220b17d36798899b,This paper proposes a method for detecting out-of-distribution (OOD) samples. The proposed method is based on the geodesic distance between the underlying data distributions and the latent representations of a deep neural network (DNN). The main idea is to use the Fisher-Rao distance of the probabilistic manifold of the learned latent representations as an effective measure for OOD detection. The method is evaluated on a variety of network architectures and datasets.
SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper provides a generalization of Cover’s Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. The authors show that the fraction of separable dichotomy is determined by the dimension of the space that is fixed by the group action. They show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. Finally, they test their theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement."
SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper proposes a method to identify spurious correlations in the training data that cause the model to misclassify a test sample. The method is based on two ideas: counterfactual explanations and concept activation vectors. The paper shows that the proposed method can identify the spurious correlations and identify the cause of the model's misclassification from a single misclassified test sample, and validate the method on two medical datasets. "
SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"This paper proposes a module named Mobile Attention Kernel Point Convolution (MAKPConv) to improve the efficiency and quality of KPConv. The proposed module employs depthwise kernel to reduce resource consumption, and re-calibrates the contribution of kernel points towards each neighbor point via Neighbor-Kernel attention to improve representation power. In addition, the authors utilize Inverted Residual Bottleneck (IRB) to craft a design space and employ a predictor-based Neural Architecture Search (NAS) approach to automate the design of efficient 3D networks based on MAKPconv. Experiments and ablation study over 3D classification and 3D semantic segmentation benchmarks verify the effectiveness of the proposed module."
SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper studies adversarial training from a data quality perspective. The authors propose a strategy to measure the data quality based on the learning behaviors of the data and find that low-quality data may not be useful and even detrimental to the adversarial robustness. They then design controlled experiments to investigate the interconnections between data quality and problems in adversarial learning. They find that when low- quality data is removed, robust overfitting and robustness overestimation can be largely alleviated and robust-accuracy trade-off becomes less significant."
SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper analyzes the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives — Korobov functions. The authors prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. They also show that these bounds nearly match the minimal number of parameters any continuous function approximator needs to approximate the optimal set of functions in the Sobolev space. "
SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper investigates the effect of population size on the emergent properties of the speaker-listener Lewis Game. The authors argue that the current population-based models may actually be too simplistic as they assume homogeneous communities. They show that this simplification is a potential root cause of the experimental difference between neural observations and socio-linguistic literature. Namely, larger populations should lead to more stable and structured languages, which is not observed in recent emergent models."
SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper studies the problem of learning polynomial graph filters to improve the performance of GNNs on heterophilic graphs. The authors propose to combine GPR-GNN with additional polynomials that adapt specifically to low and high-frequency components. Theoretically and empirically, the authors show that such an approach can learn filter functions that improve performance on the node classification task."
SP:903545b1b340ec5c13070e0f25f550c444de4124,This paper proposes a method for embedding-based distance prediction for shortest distance queries (SDQs) in graphs. The proposed method is based on betweenness centrality (BC)-based random walk and distance resampling (DR). Theoretical results show that the proposed method can cover a wider range of distances than existing methods. The method is evaluated on a variety of real-world graph datasets and shows better performance than the baselines.
SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper proposes a new continual learning (CL) problem called Continual Knowledge Learning (CKL), where the goal is to maintain invariant world knowledge in large language models (LMs) as the world knowledge stored in the LMs can quickly become outdated. To avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge, the authors formulate a new benchmark and metric to quantify the retention of time-invariant world-knowledge, the update of outdated knowledge, and the acquisition of new knowledge. The authors adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, they find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously."
SP:639fd88482330389019fb5be7446a909b99a8609,This paper proposes a stochastic approach for the criterion minimization of decision trees. The proposed algorithm is faster than conventional exhaustive search by several orders of magnitude. It is also shown that the proposed approach minimizes an upper bound of the criterion. Experiments on the MNIST dataset demonstrate the effectiveness of the proposed algorithm.
SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper proposes a new learning rate schedule for SGD on quadratic functions. The proposed schedule is based on the idea that the eigenvalue distribution of the underlying Hessian matrix is skewed. Theoretically, the paper shows that the proposed schedule has the same convergence rate as the standard step-decay schedule. The paper also proposes two simple learning rate schedulers for practical applications that can approximate the proposed eigencurve."
SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the offline model-based reinforcement learning (MBRL), where the goal is to learn a probabilistic model of the environment using a learned dynamics model. The authors compare a number of existing uncertainty heuristics for offline MBRL, including pessimistic, optimistic, and stochastic MDPs. They also investigate the effect of various hyperparameters, such as the number of models, the rollout horizon, and the choice of hyperparameterization. They show that Bayesian Optimization (BO) provides superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods and result in drastically stronger performance."
SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a method for prioritized experience replay (PER) for off-policy model-free reinforcement learning (MfRL). In particular, the authors propose to use a model-augmented critic network (MaCN) to estimate the Q-values and the TD-errors of the critic network to improve the performance of PER. The proposed method is evaluated on a variety of MfRL algorithms and shows improved performance over the baselines."
SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a method to train a reinforcement learning agent in a safe driving environment with human intervention. Human intervention is done by a human expert who can take over the control and demonstrate to the agent how to avoid probably dangerous situations or trivial behaviors. The agent learns from the data collected both from the trial-and-error exploration and human’s partial demonstration to learn a high-performing agent. The proposed HACO extracts proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. The experiments show that the proposed method achieves a substantially high sample efficiency in the safe driving benchmark.
SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning. DMIL uses the likelihood of state-action pairs from each sub-skill as the supervision for the adaptation, and use the adapted high level network to determine different data set for each sub skill adaptation. The authors theoretically prove the convergence of the iterative training process of DMIL, and establish the connection between DMIL and the Expectation-Maximization algorithm. Empirically, the authors achieve state-of-the-art few-shot imitation learning performance on the meta-world benchmark and comparable results on the Kitchen environment."
SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a method to compress node embeddings for graph neural networks (GNNs). The idea is that the number of parameters associated with the embedding grows linearly with the node number, and it is impractical to train the input node embedding together with GNNs in an end-to-end fashion when dealing with industrial scale graph data. The authors propose a hashing-based coding scheme which generates compositional codes for compactly representing nodes in graph datasets. The proposed coding scheme outperforms the prior embedding compressing method which uses a random coding scheme in almost all experiments."
SP:15c243829ed3b2505ed1e122bd499089f8a862da,"This paper studies the problem of learning invariant representations using domain adversarial training (DAL) from a game theoretical perspective. The authors show that standard optimizers in DAL can violate the asymptotic guarantees of the gradient-play dynamics, and propose to replace existing optimizers with higher-order ODE solvers, which are more stable and allow for higher learning rates, leading to noticeable improvements in terms of the transfer performance and the number of training iterations. The experiments show that in conjunction with state-of-the-art domain-adversarial methods, the proposed method achieves up to 3.5% improvement with less than half of training iteration."
SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularizer, called individual Flooding (iFlood), to prevent overfitting of machine learning models. Specifically, the proposed iFlood encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. The authors theoretically show that the design of the loss function can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. The experimental results on both image classification and language understanding tasks confirm that models learned with iFlOOD can stably converge to solutions with better generalization ability and behave consistently at instance-level."
SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes a hierarchical reinforcement learning (HRL) approach for long-horizon tasks that require planning over and composing lower-level skills. The authors propose a simple approach that produces such a representation by using the value functions corresponding to each lower level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that the approach improves the performance of the proposed method."
SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for one-shot probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. This architecture is designed to generate exchangeable distributions (all permutations of an output are equally likely) but it is hard to train due to the stochasticity of i.i.d. generation. In this work, the authors propose a new definition of equivariance and show that exchangeability is in fact unnecessary in VAEs and GANs. The proposed method can be incorporated in any GAN or VAE architecture and replace favorably other set creation methods."
SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, the authors focus on a prototype elliptic PDE: the Schrödinger equation on a hypercube with zero Dirichlet boundary condition, which is applied in quantummechanical systems. The authors establish upper and lower bounds for both methods, which improve upon concurrently developed upper bounds for this problem via a fast rate generalization bound. They also propose a modified version of the DeepRitz Method, which can achieve minimax optimal bounds over Sobolev spaces. Empirically, following recent work which has shown that the deep model accuracy will improve with growing training sets according to a power law, they show similar-behavior of dimension dependent power law for deep PDE solvers."
SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper provides a theoretical analysis of the relationship between robustness and generalization in the context of domain generalization. It shows that robustness induced by adversarial training is a byproduct of such function class regularization. The authors also discuss in details about different properties of DA and prove that under certain conditions, DA can be viewed as regularization and therefore improve generalization, and show extensive experiments to verify their theoretical findings and show several counterexamples."
SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper presents a causal perspective of meta-learning from the lens of causality. The authors identify the root cause of the memorization problem as a spurious correlation in the label space and propose a novel causal intervention principle to debias the spurious correlation. The proposed method is based on the causal inference principle of front-door adjustment and proposes two algorithms, i.e., sampling multiple versions of the meta-knowledge via Dropout and grouping the meta knowledge into multiple bins. Two implementations of the proposed principle have demonstrated their effectiveness and compatibility in four benchmark datasets."
SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a reinforcement learning framework for adapting to the presence of new teammates in an online fashion. The proposed method, called ODITS, learns latent variables of teammates’ behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, the paper introduces an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc teamwork tasks."
SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a method for missing data imputation. The method is based on the Expectation-Maximization (EM) algorithm and normalizing flow (NF) model. The proposed method is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results for high-dimensional multivariate and image datasets are presented to illustrate the superior performance of the proposed method."
SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a deep linearly gated networks (DLGN) that disentangles the computations in a DNN with ReLUs into two mathematically interpretable linearities, the primal linearity from the input to the pre-activations that trigger the gates, and the ‘dual’ linearity in the path space in the weights network characterised by the neural path kernel (NPK), which depends on inputs and gates. The authors show that convolution with global pooling and skip connection provide respectively rotational invariance and ensemble structure to the NPK. The DLGN is not an alternative architecture per se, but a disentanglement and an interpretable re-arrangement of the computational computations. The experiments show that the DLGN recovers more than 83.5% of the performance of state-of-the-art DNNs."
SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes a new normalization method for vision transformers, called dynamic token normalization (DTN). The idea is to normalize tokens in both intra-token (intra-token) and inter-token manners. The proposed DTN is built on a unified formulation and thus can represent various existing normalization methods. Experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead."
SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper proposes two methods to measure the spectral bias in modern image classification neural networks, and apply these methods towards the central question: what kinds of function frequencies are needed for modern neural networks to generalize? Specifically, the authors applied these methods to examine the impact of a variety of training choices on the learned frequencies. Among training choices that improve validation accuracy, using a larger model, training longer, and using RandAugment or AutoAugment tend to increase the frequency of the learned function. However, regularizing with modest weight decay, training with more examples, using Mixup augmentation (Zhang et al., 2018), and performing self-distillation all improve validation performance while decreasing the frequency. The experimental results offer a valuable window into precisely what kind of function complexity we should strive for."
SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper presents an initial study of mode-switching, nonmonolithic exploration for RL. The authors investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. They also propose practical algorithmic components that make the switching mechanism adaptive and robust. Finally, they report a promising and detailed analysis on Atari, using two-mode exploration and switching at sub-episodic time-scales."
SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper proposes a new initialization scheme for the k-median problem in the general metric space. The proposed scheme is based on the construction of metric embedding tree structure of the data. The authors propose a novel and efficient search algorithm for good initial centers that can be used subsequently for the local search algorithm. The method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Theoretically, the authors show that the initial centers from the proposed scheme can achieve lower error than those from another popular initialization method, k-Median++, in the non-DP setting."
SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a variational video prediction model, named FitVid, which is capable of fitting the common benchmarks so well that it begins to suffer from overfitting – while having similar parameter count as the current state-of-the-art models. The authors analyze the consequences of overfitting, illustrating how it can produce unexpected outcomes such as generating high quality output by repeating the training data, and how the overfitting can be mitigated using existing image augmentation techniques. As a result, the proposed model is able to outperform the current SOTA models across four different video prediction benchmarks."
SP:6eb5ce1d85928a3af759d75016089c535941d0b0,This paper studies the influence of data structure on the generalization performance of SGD on the test loss dynamics. The authors propose an exactly solveable model of stochastic gradient descent (SGD) which predicts test loss when training on features with arbitrary covariance structure. They solve the theory exactly for both Gaussian features and arbitrary features and show that the simpler Gaussian model accurately predicts test error of nonlinear random-feature models and deep neural networks on real datasets such as MNIST and CIFAR-10. They also show that optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure.
SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the behavior of SGD in the non-linear, non-convex optimization problem of training deep neural networks. The authors construct example optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange behaviors that are undesirable. The constructions show that, with specific choices of learning rate and specific model (1) SGD may converge to local maxima, (2) SGd might only escape saddle points arbitrarily slowly, (3) SGiD may prefer sharp minima over flat ones, and (4) AMSGrad may not converge to flat minima. "
SP:22d01913b78ef447b064c65a646fa301b861d3f7,"This paper proposes a method for hyperparameter optimization in meta-learning. The proposed method is based on approximating the second-order term with knowledge distillation. Specifically, the authors parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second order term. The method allows online optimization and also is scalable to the hyper parameter dimension and the horizon length. The authors demonstrate the effectiveness of the proposed method on two different meta learning methods and three benchmark datasets."
SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a clustered task aware meta-learning (CTML) framework with task representation learned from both features and learning path. CTML first conduct rehearsed task learning from the common initialization, and collect a set of geometric quantities that adequately describes this learning path, which is then inputted into a meta path learner, which automatically abstracts the learning path representation optimized for the downstream clustering and modulation. To further save the computational cost incurred by the additional rehearsed learning, the authors devise a shortcut tunnel to directly map between the path and feature cluster assignments. Extensive experiments on two real-world application domains: few-shot image classification and cold-start recommendation demonstrate the superiority of CTML compared to state-of-the-art baselines."
SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,This paper proposes an ensemble-based procedure for semi-supervised novelty detection (SSND) that only utilizes a mixture of unlabeled ID and OOD samples to achieve good detection performance on near OOD data. It crucially relies on regularization to promote diversity on the near-OOD data while preserving agreement on ID data. Extensive comparisons of the proposed approach to state-of-the-art SSND methods on standard image data sets (SVHN/Cifar-10/CIFAR-100) and medical image datasets reveal significant gains with negligible increase in computational cost.
SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,This paper proposes Latent Variable Variable Sequential Set Transformers (AutoBots) for predicting multi-agent trajectories. The encoder is a stack of interleaved temporal and social multi-head self-attention modules which alternately perform equivariant processing across the temporal dimension and social dimension. The decoder can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. The model achieves top results on the global nuScenes vehicle motion prediction leaderboard and produces strong results on Argoverse vehicle prediction challenge.
SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,This paper presents a user study to evaluate the effectiveness of different methods to explain image classification models. The authors propose a simple baseline explanation technique that relies on the model’s outputs only. They also introduce a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. The results show that the baseline outperformed concept-based explanations.
SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes an approach to defend against backdoor data poisoning attacks on deep neural networks. The authors propose an iterative training procedure for removing poisoned data from the training set. They first train an ensemble of weak learners to automatically discover distinct subpopulations in the training data and then leverage a boosting framework to exclude the poisoned data and recover the clean data. The algorithm is based on a novel bootstrapped measure of generalization, which provably separates the clean from the dirty data under mild assumptions. Empirically, the method successfully defends against a state-of-the-art dirty label backdoor attack."
SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a method to model label correlations in multi-label text classification (MLTC). The proposed method concatenates a set of latent labels (instead of actual labels) to the text tokens, inputs them to BERT, then maps the contextual encodings of these latent labels to actual labels cooperatively. The correlations between labels, and between labels and the text are modeled indirectly through these latent-label embeddings and their correlations. The method is conceptually simple but quite effective. It improves the state-of-the-art results on two widely used benchmark datasets."
SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the approximation and generalization properties of convolutional kernels. The authors show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. They also provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities."
SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of the Neural Tangent Kernel (NTK) in the finite-width limit of neural networks. The authors provide an analysis of the compute and memory requirements for NTK computation in finite width networks, and then propose two algorithms that change the exponent of the computed and memory requirement of the finite width NTK, improving efficiency. The proposed algorithms are general-purpose JAX function transformations that can be applied to any differentiable computation (convolutions, attention, recurrence, etc.) and introduce no new hyper-parameters."
SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper considers the offline constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. The paper proposes an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. The proposed algorithm, COptiDICE, directly estimates the stationary distributions corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that the proposed algorithm achieves better trade-off between reward maximization and constraint satisfaction than several baselines, across domains and conditions."
SP:5a10c13eb78d26a25dac74601419deb68c53cb75,"This paper proposes a novel parallel training scheme (called parallel-in-time) for Gated Recurrent Unit (GRU) networks based on a multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into multiple shorter sub-sequences and trains them on different processors in parallel. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset, where each video is an image sequence, demonstrate that the proposed method achieves up to 6.5x speedup over a serial approach."
SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method to learn a meaningful embedding of the data that denoises, and reveals its intrinsic structure. Specifically, they assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. They propose a neural network called MRMD-AE (manifold-regularized multiple decoder, autoencoder), that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. They show that their learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, and improves cross-subject translation of fMRI signal."
SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper introduces a new benchmark for multi-class anomaly detection and anomaly segmentation. The authors also introduce a new dataset of anomalous species to evaluate multi-label anomaly detectors. The main contribution of this paper is the introduction of the MaxLogit detector, which is a simple yet effective detector for out-of-distribution detection. The paper is well-written and easy to follow. "
SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the structure of the class of tournaments that arise out of fixed d-dimensional representations. The authors show that the tournament classes have forbidden configurations which must necessarily be union of flip classes, a novel way to partition the set of all tournaments. They further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. This insight allows them to show the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure."
SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a method to improve the performance of neural processes (NPs) by introducing a regularization term that encourages the representations to be aware of both the target and context data. Specifically, the authors propose to use a weighted average of the features of the target data and the context data as a regularizer to force the NPs to pay more attention to the context. The proposed method is evaluated on three tasks: 1D regression, predator-prey model, and image completion."
SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,"This paper proposes a method to improve interpretability and explainability of transformer language models (LMs). The main idea is to use prototype networks (prototype networks) to incorporate case-based reasoning to explain the LM’s decisions. In particular, the authors propose an interactive prototype learning setup to overcome these challenges and improve the network's capabilities by incorporating human knowledge with the consideration of knowledge certainty. "
SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"This paper proposes a method for continual learning based on trust region gradient projection (TRGP) to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. The authors introduce a notion of ‘trust region’ to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that the approach achieves significant improvement over related state-of-the-art methods."
SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a framework to connect optimization and generalization by analyzing the generalization error based on the length of optimization trajectory under the gradient flow algorithm after convergence. It shows that, with proper initialization, gradient flow converges following a short path with an explicit length estimate. Such an estimate induces a length-based generalization bound, showing that short optimization paths after convergence indicate good generalization. The framework can be applied to broad settings. For example, it can be used to obtain generalization estimates on three distinct machine learning models: underdetermined lp linear regression, kernel regression, and overparameterized two-layer ReLU neural networks."
SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper presents a frequency-based understanding of adversarial examples, which is motivated by the common misconception that adversarial attacks are high-frequency noise. The authors analyze adversarial robustness through the perspective of spatial frequencies and show that adversary examples are not just a high frequency phenomenon, but are dependent on the dataset. Then they propose and study the properties of the adversarial training using specific frequencies, which can be used to understand the accuracy-robustness trade-off."
SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper proposes a new graph similarity metric for graph neural networks (GNNs) that considers the influence of both graph structure and input features on the performance of GNNs. The authors show that not all cases of heterophily are harmful for GNN with aggregation operation and propose new metrics based on a similarity matrix. They also propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmfulheterophily. They validate the ACM-augmented baselines with 10 real-world node classification tasks."
SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,This paper proposes a deep reinforcement learning (RL) approach for solving the traveling salesman problem (TSP). The main idea is to leverage the equivariance property of the TSP solver to improve the generalizability of the RL solver. The proposed approach is evaluated on both synthetic and real-world TSP problems.
SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a framework for federated learning (FL) to address the non-IID (independent and identically distributed) data among clients. Specifically, the authors propose a novel framework, namely Synthetic Data Aided Federated Learning (SDA-FL), to share differentially private synthetic data. Each client pretrains a local generative adversarial network (GAN) to generate synthetic data, which are uploaded to the parameter server (PS) to construct a global shared synthetic dataset. The PS is responsible for generating and updating high-quality labels for the global dataset via pseudo labeling with a confident threshold before each global aggregation. A combination of the local private dataset and labeled synthetic dataset leads to nearly identical data distributions among clients, which improves the consistency among local models and benefits the global aggregation, according to the authors."
SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,This paper studies the relationship between adversarial robustness and prediction confidence of smoothed classifiers. The authors propose to use the “accuracy under Gaussian noise” as an easy-to-compute proxy of robustness for each input. They differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. The experiments show that the proposed method exhibits improved certified robustness compared to existing state-of-the-art training methods.
SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper studies the problem of packing and un-padding the Wikipedia dataset for pretraining BERT (Bidirectional Encoder Representations from Transformers). The authors show that at sequence length 512 padding tokens represent in excess of 50% of the Wikipedia data, the packing algorithm can achieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic of the dataset, the authors develop and contrast two packing algorithms. The shortest-pack-first histogram-packing (SPFHP) algorithm determines the packing order for the entire Wikipedia dataset of over 16M sequences in 0.03 seconds. The non-negative least-squares histogram packing (NNLSHP) algorithms converges in 28.4 seconds but produces solutions which are more depth efficient, managing to get near optimal packing by combining a maximum of 3 sequences in one sample. Finally, BERT-Large can be pretrained using the packed dataset, demonstrating no loss of convergence and the desired 2x speeds-up."
SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper proposes an adaptive tree search algorithm, which is a deterministic variant of Monte Carlo tree search, that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, this algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autorgressive models. The authors also characterise the correlation of several translation model objectives with respect to BLEU. They find that while some standard models are poorly calibrated and benefit from the beam search bias, other often more robust models benefit from increasing amounts of search using our proposed decoder."
SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper proposes an energy based model (EBM) for the task of anomaly detection and localization. The EBM is based on the energy-based model (EBM) framework, which learns to associate low energies to correct values and higher energies to incorrect values. The authors also propose an adaptive sparse coding layer to avoid training an anomaly detector for every task, and a plug-and-play feature that can be used to quickly update what is normal during inference time. The proposed method is evaluated on industrial inspection and video anomaly detection."
SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of question answering (QA) systems being robust to misinformation. The authors create the first large-scale dataset for this problem, CONTRAQA, which contains over 10K human-written and model-generated contradicting pairs of contexts. They show that QA models are vulnerable under contradicting contexts brought by misinformation. They also build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion."
SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"This paper proposes Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation learning (CIL) that uses the GOM to align and compare states between the different spaces of the agents. The authors provide a theoretical analysis of the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. They demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformations of the state-action space."
SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes a method for self-supervised learning based on cross-level contrastive learning. The main idea is to explore multiple outputs from hidden layers instead of the output of the last one, and to compare latent features on different levels and views. Experiments on classification, detection, segmentation, and few-shot learning tasks show the effectiveness of the proposed method."
SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a method for learning the Nash equilibria of dynamic general equilibrium models (DGE) in a meta-game setting, where there are multiple agents (firms, consumers, and governments) and the goal is to find the optimal policy for each of them. The authors propose to learn the equilibrium of the DGE model in a multi-agent RL setting, and show that their method is able to find a set of optimal policies for each agent. They also show that the learned policies can be used to predict the Nash values of the optimal policies of the other agents."
SP:f885c992df9c685f806a653398736432ba38bd80,This paper proposes a defense against model stealing attacks by requiring users to complete a proof-of-work (PoW) before they can read the model’s predictions. The key idea is to calibrate the effort required to complete the PoW to each query. The calibration is achieved by using tools from differential privacy to measure the information revealed by a query. This deters attackers by greatly increasing the computational effort needed to leverage query access for model extraction.
SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a multi-resolution variant of continuous normalizing flows (CNFs) for generative models of images. In particular, the authors propose a transformation between resolutions that allows for no change in the log likelihood. They show that this approach yields comparable likelihood values for various image datasets, using orders of magnitude fewer parameters than the prior methods, in significantly less training time, using only one GPU. "
SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"Label noise is pervasive in real-world datasets, which encodes wrong correlation patterns and impairs the generalization of deep neural networks (DNNs). It is critical to find efficient ways to detect the corrupted patterns. In this paper, given good representations, the authors propose a universally applicable and training-free solution to detect noisy labels. Intuitively, good representations help define “neighbors” of each training instance, and closer instances are more likely to share the same clean label. Based on the neighborhood information, the first one uses “local voting” via checking the noisy label consensuses of nearby representations. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted."
SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"This paper proposes an adversarial reinforcement learning (AD) algorithm, PA-AD, which is based on the actor-critic framework proposed by Zhang et al. (2021) and PPO(LSTM)+SA Reg (Zhang et al., 2021). The main idea of the paper is to train an RL-based adversarial learner to find the optimal policy perturbation directions, and then use the learned policy to train the adversarial RL learner. The proposed algorithm is shown to be theoretically optimal and significantly more efficient than the existing methods. Empirical results show that the proposed method outperforms the state-of-the-art attacking methods in various Atari and MuJoCo environments."
SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper proposes a novel policy seeking algorithm, Interior Policy Differentiation (IPD), which is derived from the interior point method commonly known in the constrained optimization literature. The authors propose a new metric to evaluate the difference between policies, based on which they define the novelty of a policy as the distance between policies. Then, they propose to solve the novelty-seeking problem through the lens of constrained optimization, to address the dilemma between the task performance and the behavioral novelty in existing multi-objective optimization approaches. Experimental results on benchmark environments show that IPD can achieve a substantial improvement over previous novelty seeking methods in terms of both novelty of generated policies and their performances in the primal task."
SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper proposes an end-to-end approach to remove reverberation from audio-visual observations to improve the quality of speech enhancement and speech recognition. The main idea is that the visual environment surrounding a human speaker reveals important cues about the room geometry, materials, and speaker location, which influence the precise reverberation effects in the audio stream. The proposed VIDA approach learns to remove reverb by attending to both the audio and visual streams, recovering valuable signals about room geometry and materials from visual encodings of the environment. In support of this task, the authors develop a large-scale dataset providing realistic, spatially registered observations of speech and 3D environments."
SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a simple and efficient position representation method, Attention with Linear Biases (ALiBi), which does not add positional embedding to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. ALiBi is simple to implement and does not slow down runtime or require extra parameters (but does occasionally require a negligible amount of extra memory). Using our method, we sped up the training of a 1.3 billion parameter model evaluated on the same input sequence length as GPT-3 (2048)."
SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the problem of multi-objective online convex optimization, where the goal is to optimize the logistic loss and the squared L2-norm of the model parameters. In particular, the authors propose the Multi-Objective Online Convex Optimization (MOO) framework, which encompasses a novel multiobjective dynamic regret in the unconstrained max-min form. The authors show that it is equivalent to the regret commonly used in the zero-order bandit setting and overcomes the problem that the latter is hard to optimize via first-order gradient-based methods. Then, they propose the Online Mirror Multiple Descent algorithm with two variants, which computes the composite gradient using either the vanilla min-norm solver or a newly designed L1-regularized min-normalized solver. They further derive regret bounds of both variants and show that the L1 regularized variant enjoys a lower bound."
SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper studies the problem of continual learning with generative replay. The authors propose to use the generated data as negative examples (or antagonists) to learn the new classes, especially when the learning experiences are small and contain examples of just one or few classes. The proposed approach is validated on two datasets (CORe50 and ImageNet-1000) composed of high-dimensional data and a large number of training experiences. The results show that using negative replay largely improves classification performance w.r.t. the original data."
SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework to solve the NP-hard combinatorial optimization problems regarding graph partition (GP), e.g., modularity maximization and NCut minimization. In contrast to conventional transductive GP methods applied to a single graph, the proposed IGP framework is applied to multiple associated graphs of a system or scenario. IGP first conducts the offline training of a dual graph neural network on historical graph snapshots to capture properties of the system. The trained model is then generalized to newly generated graphs for fast high-quality online GP without additional optimization. Experiments on a set of benchmarks demonstrate that IGP achieves competitive quality and efficiency over various state-of-the-art baselines."
SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGVAE employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. The proposed framework is end-to-end permutation-equivariant with respect to node ordering. It achieves state-of-the-art results on several generative tasks."
SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"This paper proposes a general framework for nonlinear ICA, in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. The authors provide an insightful proof of the identifiability of the proposed framework. They implement the framework by volume preserving flow-based models, and verify the theory by experiments on artificial data and synthesized images."
SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a novel graph convolutional operator, termed BankGCN, which extends the capabilities of most MPGCNs beyond single ‘low-pass’ features and simplifies spectral methods with a carefully designed sharing scheme between filters. It decomposes multi-channel signals on arbitrary graphs into subspaces and shares adaptive filters to represent information in each subspace. The filter bank and the signal decomposition permit to adaptively capture diverse spectral characteristics of graph data for target applications with a compact architecture. Experiments are conducted on node classification and graph classification tasks."
SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a new supervised pre-training method based on leave-one-out k-nearest-neighbor (LOO-kNN) classifier for better downstream transferring. The authors argue that the worse transferability of existing supervised pretraining methods arise from the negligence of valuable intra-class semantic difference. To alleviate this problem, the authors propose a new method called Lookout, which relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and preserving part of intra class difference for better transferring to downstream tasks. Extensive empirical studies on multiple downstream tasks show that Lookout outperforms other state-of-the-art methods for supervised and self-supervised pretraining."
SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes FaceDet3D, a method that generates from a single image geometric facial details that are consistent with any desired target expression. The facial details are represented as a vertex displacement map and used then by a Neural Renderer to photo-realistically render novel images of any single image in any desired expression and view. The proposed method hallucinates facial geometric details consistent with the target expression and then adds them to a smooth proxy geometry extracted using FDS. The detailed geometry is then rendered using Neural Rendering to give the final image."
SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) for disentanglement of syntactic roles. The model is based on a deep probabilistic generative generative model with attention, and it is shown that it is possible to obtain representations of sentences where different syntactic role correspond to clearly identified different latent variables. Experiments on the SNLI dataset show that the model is able to learn representations that exhibit separation in the realizations of these syntactic functions without supervision."
SP:57ace99a05a76b7d7427619cb6881fc87d74160f,This paper proposes a novel multi-agent reinforcement learning (MARL) algorithm that encourages agents to explore a joint action space that grows exponentially with the number of agents. The authors propose a framework where agents simulate counterfactual rollouts at each timestep and evaluate the influence of their actions on other agents' policies. They also propose an intrinsic reward for each agent to promote coordinated team exploration. The proposed method is evaluated on a set of challenging tasks with sparse rewards and partial observability.
SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc editing of large pre-trained neural network models. The proposed method, Model Editor Networks with Gradient Decomposition (MEND), is a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre- trained model’s behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. The method can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-training model."
SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper introduces a compositional physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. The authors propose a new way of combining the learning abilities of artificial neural networks with physical and structural knowledge from numerical simulation by modeling the constituents of partial differential equations (PDEs) in compositional manner. Results on both one-dimensional and two-dimensional PDEs demonstrate the superior modeling accuracy and excellent out-of-distribution generalization ability beyond initial and boundary conditions.
SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper presents a study of how representations of programs generated by unsupervised machine learning (ML) models and representations of computer programs in the human brain are related. The authors analyze recordings—brain representations—from functional magnetic resonance imaging (fMRI) studies of people comprehending Python code. They discover brain representations, in different and specific regions of the brain, that encode static and dynamic properties of code such as abstract syntax tree (AST)-related information and runtime information. They also map brain representations to representations of a suite of ML models that vary in their complexity."
SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper presents Translatotron 2, a neural-based speech-to-speech translation model, which consists of a speech encoder, phoneme decoder, mel-spectrogram synthesizer, and an attention module that connects all the previous three components. The authors also propose a new method for retaining the source speaker’s voice in the translated speech. Experiments are conducted on three different datasets, including multilingual S2ST, and the proposed method outperforms the original Translatron by a large margin in terms of translation quality."
SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper studies the problem of few-shot attribute learning, i.e., the task of learning a representation of a set of attributes (e.g., attributes of a class) that are not known at training time. The authors propose to use contrastive learning and self-supervised pre-training to improve the performance of the representation learning algorithm. They also show that random splits of the attribute space can provide an informative estimate of the generalization ability of the model."
SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle method for sampling from unnormalized distributions. The proposed method is based on the Wasserstein gradient flow of relative entropy, which interpolates between the reference distribution and the target distribution. The method is characterized by an ODE system with velocity fields depending on the density ratios of the density of evolving particles and the unnormalised target density. To sample with REGS, the authors propose a nonparametric approach to estimate the logarithmic density ratio using neural networks."
SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a framework to classify larger, realistic images using quantum systems. The approach relies on a novel encoding mechanism that embeds images in quantum states while necessitating fewer qubits than prior work. The framework is able to classify images that are larger than previously possible, up to 16x16 for the MNIST dataset on a personal laptop and obtains accuracy comparable to classical neural networks with the same number of learnable parameters. "
SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This paper proposes a federated learning approach for face recognition. The proposed approach is based on two components: (1) sanitized local clustering (DPLC) to distill sanitized clusters from local class centers, and (2) a consensus-aware recognition loss to encourage global consensuses among clients, which results in more discriminative features. Experiments on a large-scale dataset demonstrate the effectiveness of the proposed approach."
SP:408d9e1299ee05b89855df9742b608626692b40d,"This paper proposes an extension of the linear probing method for transfer learning. The proposed method, Head-to-Toe probing (HEAD2TOE), selects features from all layers of the source model to train a classification head for the target-domain. The experiments on the Visual Task Adaptation Benchmark (VTAB) show that the proposed method can outperform fine-tuning on average and for out-of-distribution transfer. "
SP:d6f11fb32851f97af287f962f83220d27a8bc76a,This paper proposes an object-oriented text dynamics (OOTD) model for text-based game planning. The OOTD model learns a graph representation for capturing object dynamics and predicts the belief of object states with independent transition layers. The authors develop variational objectives under the object-supervised and self supervised settings to model the stochasticity of predicted dynamics. Empirical results show that the proposed method outperforms model-free baselines in terms of proposed efficiency and running time.
SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"This paper studies the problem of cost-sensitive hierarchical classification, where a label taxonomy has a loss associated with it, which represents the cost of (wrong) predictions at different levels of the hierarchy. The authors propose a method that breaks the hierarchical learning problem into layer-by-layer learning-to-abstain sub-problems. They prove that there is a bijective mapping between the original hierarchical loss and the set of layer-wise abstaining losses under symmetry assumptions. They employ the distributionally robust learning framework to solve the learning to abstaining problems in each layer. They conduct experiments on large-scale bird dataset as well as on cell classification problems."
SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,This paper proposes to learn a group of parameterized synperiodic filter banks to process sound waveforms that have limited time-frequency resolution capability. The authors propose a Transformer-like backbone with two parallel soft-stitched branches to learn semantic identity label and spatial location representation semi-independently. Experiments on both direction of arrival estimation task and the physical location estimation task show that the proposed method outperforms existing methods by a large margin.
SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper studies the problem of gradual domain adaptation, where the goal is shifting the model towards the target distribution rather than learning domain invariant representations. The authors hypothesize that under the following two assumptions: (i) access to samples from intermediate distributions, and (ii) samples being annotated with the amount of change from the source distribution, self-training can be successfully applied on gradually shifted samples to adapt the model toward the target distributions. The proposed method, GIFT (Gradual Interpolation of Features toward Target), creates virtual samples from the intermediate distributions by interpolating representations of examples from source and target domains. The experiments show that in the presence of (i), iterative selftraining naturally forms a curriculum of samples which helps the model to adapt better to the target domain."
SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper presents a method for video-text retrieval. The authors propose a context adapter module, which is able to extract information from auxiliary input sources for learning a joint, multimodal embedding. In particular, the authors introduce an attention-based mechanism that allows the model to disregard text with irrelevant content. The experiments show that the proposed method improves the rep-resentation quality of video embedding learning when adapting the representation with user comments."
SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes a new intrinsic reward for unsupervised skill learning that encourages the discriminator to disagree with the policy on whether a given skill is distinguishable. This is motivated by the fact that the discriminators may not have seen enough training data to be confident in their predictions, leading to low intrinsic rewards for the policy and effective penalization of the sort of exploration needed to maximize the objective. The paper proposes to use an ensemble of discriminators and reward the policy for their disagreement. The ensemble consists of a policy, a discriminator, and a classifier. The discriminators are trained to disagree on whether the classifier is confident in its predictions. The classifier and the policy are jointly trained to minimize the mutual information between their predictions and the posterior distributions of the latent states. The posterior distributions are estimated using the epistemic uncertainty, which is used as an intrinsic reward."
SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a molecular graph generation framework based on the construction of a spanning tree and the residual edges. The authors also design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework."
SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper provides a spectral analysis of the Neural Tangent Kernel (NTK) of Polynomial Neural Networks (PNNs). The authors show that the $\�-Net$ family, a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies, and verify the theoretical bias through extensive experiments. They also provide some insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials."
SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a two-stage approach to identify sparse subnetworks in randomly initialized neural networks. The first step is to search for a sparse subnetwork at random initialization, and the second step is the unmasking process, which is a combination of two operations: (1) searching efficiently for a subnetwork; (2) unmasking the disguise by learning to transform the resulting subnetwork’s remaining weights. The authors show that the unmaskings process can be efficiently implemented without referring to any latent weights or scores, so that the whole training algorithm is computationally light. Extensive experiments on several large-scale models demonstrate the competency of PaB over edge-popup and other counterparts."
SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a general unified framework to enhance the robustness of GNNs against adversarial attacks by jointly cleaning the perturbed graph and denoising the features of data. The authors extend this framework by reconstructing the graph and making convolution operations of features with intrinsic properties, and propose a robust GNN model RGUGNN. Experiments on four real-world datasets demonstrate that R-GUGNN has greatly improved the overall robustness over the state-of-the-art baselines."
SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,"This paper proposes a method to learn implicit surface parametrization for document texture extraction. The method is based on the recent work on differentiable rendering techniques for implicit surfaces, which have shown high-quality 3D scene reconstruction and view synthesis results. However, these methods typically learn the appearance color as a function of the surface points and lack explicit surface parameterization. Thus they do not allow texture map extraction or texture editing. This paper proposes to learn surface parametric mapping by learning a continuous bijective mapping between 3D surface positions and 2D texture-space coordinates. The proposed method can be trained using multi-view images and rendering loss. The authors demonstrate that their approach can reconstruct high-frequency textures for arbitrary document shapes."
SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes Adaptive Region Pooling (ARP), a novel downsampling algorithm for fine-grained recognition. The idea is to make the network only focus on a smaller but more critical region, and simultaneously increase the resolution of sub-sampled feature. ARP owns a trade-off mechanism that allows users to actively balance the scale of receptive field and the granularity of feature. The experiments show the effectiveness and efficiency of the proposed model in both the tasks of image classification and image retrieval."
SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"This paper studies the problem of out-of-distribution (OOD) generalization for graph-structured data. The authors formulate the OOD problem for node-level prediction on graphs and develop a new domain-invariant learning approach, named Explore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage invariant graph features for prediction. The key difference to existing invariant models is that the authors design multiple context explorers (specified as graph editers in the case of this paper) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment. Theoretical guarantees are also provided to prove the validity of the proposed method."
SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning approach that adaptively selects optimal data augmentation for contrastive representation learning. InfoTS is based on the information theory that good augmentation should preserve high variety and high fidelity. The authors approximate the criteria with mutual information neural estimation and cross-entropy estimation. Experiments on various datasets show that the proposed InfoTS outperforms the existing methods."
SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper investigates the universality of winning tickets in the Lottery Ticket Hypothesis. The authors show that iterative magnitude pruning, the method used for discovering winning tickets, is a renormalization group scheme. They show that winning tickets can be found in the context of one task and can be transferred to similar tasks, possibly even across different architectures. They also show that the success of iterative pruning has found generally in machine learning."
SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper studies the performance gap between cross-attention (CA) and dual-encoder (DE) models in the context of neural re-ranking. The authors show theoretically that with a sufficiently large encoder size, DE models can capture a broad class of scores without cross attention. They also show empirically that on real-world problems, the gap between CA and DE models may be due to the latter overfitting to the training set. To mitigate this behaviour, the authors propose a distillation strategy that focuses on preserving the ordering amongst documents, and confirm its efficacy on benchmark NeuralReranking datasets."
SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper studies the role of importance sampling (IS) in the context of off-policy evaluation and policy optimization. In particular, the authors show the connection between importance sampling and variance minimization in Monte Carlo simulation. They then propose a policy optimization algorithm (PO2PE) that uses importance sampling as an inner loop to improve the performance of policy optimization algorithms. Experiments are conducted on several continuous control tasks. "
SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,"This paper proposes a graph parallelism method to scale graph neural networks (GNNs) for modeling atomic simulations. Specifically, the authors propose a method to distribute input graphs across multiple GPUs, enabling them to train very large GNNs with hundreds of millions or billions of parameters. The authors empirically evaluate their method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 15% and 21% on the force MAE metric on the S2EF task and IS2RS task, respectively."
SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes Time Control (TC), a language model that learns to map the dynamics of how text changes in a document to dynamics of a stochastic process of interest. The authors propose to use Brownian bridge processes as a desirable latent space and learn to map coherent text to smooth Brownian Bridge trajectories. The proposed method is evaluated on two tasks: text infilling and discourse coherence, and on long text generation settings."
SP:56a74403d4471cd95641dc669f5eac89a2c93144,This paper proposes a method for learning object-centric representation from single 2D images by learning to predict future scenes in the presence of moving objects. It treats objects as latent causes whose function to an agent is to facilitate efficient prediction of the coherent motion of their parts in visual input. The model learns to explicitly infer objects’ locations in 3D environment in addition to segmenting objects. The network learns a latent code space where objects with the same geometric shape and texture/color frequently group together.
SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based architecture for learning the disentangled representation from real spatio-temporal data for mobility forecasting. The proposed method learns a latent representation that separates the temporal dynamics of the data from the spatially varying component and generates effective reconstructions. The experimental evaluation results show that the proposed method can achieve state-of-the-art performance and is able to extract desirable spatial/temporal features.
SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"This paper proposes a new deep learning framework for probabilistic interpolation of irregularly sampled time series that the authors call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode information about input observation sparsity, a temporal VAE architecture to propagate uncertainty due to input sparsity and a heteroskedastic output layer to enable variable uncertainty in output interpolations. The authors show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models, as well as recently proposed deep latent variable models."
SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes two metrics to measure the modularity of a neural network, i.e., importance and coherence. The authors propose to measure these metrics by clustering a graph representation of the network’s neurons with edges determined either by network weights or correlations of activations. They show that these partitionings, even ones based only on weights, reveal groups of neurons that are important and coherent."
SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates the possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and (3) compatible with structured sparsity. The authors conducted extensive experiments on CTC, RNN-Transducer, and Transformer models, and verified the existence of highly sparse “winning tickets” that can match the full model performance across those backbones. They obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structured sparse with no performance loss, and transfer exceptionally from large source datasets to various target datasets."
SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes to replace the widely used random weight initialization with a fully deterministic initialization scheme ZerO, which initializes residual networks with only zeros and ones. By augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms, ZerO allows us to start the training from zero and ones entirely. This has many benefits such as improving reproducibility (by reducing the variance over different experimental runs) and allowing network training without batch normalization. Extensive experiments demonstrate that ZerO achieves state-of-the-art performance over various image classification datasets, including ImageNet, which suggests random weights may be unnecessary for modern network initialization."
SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper proposes a minimax formulation for removing backdoors from a poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. The authors propose the Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Theoretical analysis and the generalizability of the robustness gained by solving minimax on clean data to unseen test data are provided.
SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the question of whether random permutations are optimal in the context of permutation-based SGD. The authors show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist permutations that offer exponentially faster convergence compared to random. However, for general strongly-concave functions, the authors argue that for quadratic functions, there are easy-to-construct permutations which lead to accelerated convergence. "
SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"This paper proposes an automated normalizing flow (NF) architecture search method. The method aims to find the optimal sequence of transformation layers from a given set of unique transformations with three folds. First, a mixed distribution is formulated to enable efficient architecture optimization originally on the discrete space without violating the invertibility of the resulting NF architecture. Second, the mixture NF is optimized with an approximate upper bound which has a more preferable global minimum. Third, a block-wise alternating optimization algorithm is proposed to ensure efficiency of deep flow models."
SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes to use the Intrinsic Dimension (ID) and Cluster Learnability (CL) to measure the expressiveness and learnability of representations learned by self-supervised learning (SSL) methods. The authors propose to use ID and CL to predict downstream classification performance better than the existing techniques based on contrastive losses or pretext tasks, while having no requirements on data augmentation, model architecture or human labels. They also propose modifying DeepCluster (Caron et al., 2018) to improve the learnability."
SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes to use segmentation priors for black-box adversarial attacks such that the perturbations are limited in the salient region. The authors also propose the Saliency Attack, a new gradient-free black box attack that can further improve the imperceptibility of adversarial examples. Experiments are conducted to show the effectiveness of the proposed method."
SP:779821ed85084f8bf1b29d8822b312989b186ee9,This paper proposes a novel Transformer-based architecture for molecule-to-molecule (M2M) reaction outcome prediction and retrosynthesis. The proposed Graph2SMILES model combines the power of Transformer models for text generation with the permutation invariance of molecular graph encoders that mitigates the need for input data augmentation. The authors show that the proposed method improves the top-1 accuracy of the Transformer baselines by 1.7% and 1.9% for the USPTO_480k and USPto_STEREO datasets respectively and by 9.8% for one-step retroSynthesis on the USpTO_50k dataset.
SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical approach for disease diagnosis with reinforcement learning (RL) methods in task-oriented dialogues setting. The authors formulate disease diagnosis as a hierarchical policy learning problem, where symptom acquisition and disease diagnosis are assigned to different kinds of workers in the lower level of the hierarchy. A master model is designed in the higher level that is responsible for triggering models in the low level. Experimental results on both self-constructed real-world and synthetic datasets demonstrate that the hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis compared with existing systems."
SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a federated learning (FL) framework called Self-supervised Federated Federated Learning (SSFL) to address two challenges: data heterogeneity and label deficiency at the edge. The authors propose a series of algorithms to address these challenges, including per-FedAvg, Ditto, and local fine-tuning, and propose a novel personalized federated self-supervision algorithm, Per-SSFL, which balances personalization and consensus by carefully regulating the distance between the local and global representations of data. They also develop a distributed training system and related evaluation protocol for SSFL. The experimental results demonstrate that SSFL can work reliably and achieves reasonable evaluation accuracy."
SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper attempts to derive a general form of PDEs for the design of ResNet-like DNNs. The authors first formulate DNN as an adjustment operator applied on the base classifier. Then based on several reasonable assumptions, they show the adjustment operator for DNN is the solution operator of the PDE. They show that several effective networks can be interpreted by the general form and design a training method motivated by PDE theory to train DNN models for better robustness and less chance of overfitting."
SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the expressivity of emergent languages in referential games. The authors propose a hypothesis about the factors that influence the emergent language's expressivity based on the generalization performance across a set of games. They then validate the three predictions from their hypothesis, which further indicates that expressivity is indeed a trade-off between the complexity and the unpredictability of the context. They also show that using the contrastive loss proposed by Chen et al. (2020a) can alleviate the problem of message type collapse."
SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes two exploration strategies that extend SAU from bandits to the general sequential Reinforcement learning scenario. The first strategy, called $\delta$-exploration, is an exploration strategy that extends SAU (Sample Average Uncertainty) from bandit problems to general sequential reinforcement learning. The second strategy, named \delta2 exploration, is a drop-in replacement for -greedy action selection in Q-learning and DQN. Both strategies can be implemented with minimal change on existing RL code bases and can be easily deployed to mitigate the exploration-exploitation dilemma."
SP:2f6e266b03939c96434834579999707d3268c5d6,"This paper proposes an implicit generative adversarial network (GAN) for video generation. In particular, the authors propose to use implicit neural representations (INRs) to model the dynamics of videos, and then use a GAN to generate videos. The authors also propose a motion discriminator to identify the unnatural motions in the generated videos. Experimental results show that the proposed method outperforms the state-of-the-art methods on UCF-101."
SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the multi-label multi-winner voting, i.e. the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee. The authors propose three new privacy-preserving mechanisms: Binary, \tau, and Powerset voting. Binary voting operates independently per label through composition. The $\tau$ voting bounds votes optimally in their $\ell_2$ norm. Powerset Voting operates over the entire binary vector by viewing the possible outcomes as a power set. The paper theoretically analyzes tradeoffs showing that powerset voting requires strong correlations between labels to outperform Binary voting. "
SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper proposes a technique of learning rate grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of optimizer hyperparameter search. Besides providing a resource-saving tool, the invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning."
SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper studies the problem of online reinforcement learning (RL) in the presence of sub-optimal behavior policy and offline demonstration data. The authors propose an algorithm called Learning Online with Guidance Offline (LOGO) that combines a policy improvement step with an additional policy guidance step by using the offline data. They provide a theoretical analysis of their algorithm, and provide a lower bound on the performance improvement in each learning episode. They also extend their algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation. They demonstrate the superior performance over state-of-the-art approaches on a number of benchmark environments with sparse rewards and censored state."
SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This paper proposes a two-stage method to extract the Pareto optimal solution set for multi-task learning (MTL) problems. The proposed method is based on a weak and a low-cost neural network. The weak neural network is used as the discriminator, while the low cost one is used to extract a subset of the weak set. The high-cost one is then used to compute the approximation error between the true and the extracted set. Experiments are conducted to show the effectiveness of the proposed method."
SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper studies the problem of knowledge distillation, i.e. learning a consolidated feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks. The authors propose a multi-head, multi-task distillation method using an unlabeled proxy dataset and adding a generalist teacher is sufficient to consolidate representations from the teacher(s) and improve downstream performance, outperforming the teacher (or best of all teachers) and the strong baseline of ImageNet pre-trained features. The proposed method preserves the wide-range transferability of the strong ImageNet baseline and improves the performance for both related and unrelated downstream tasks."
SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. Conformal prediction is a powerful technique for learning prediction sets with valid coverage, yet by default its conformalization step only learns a single parameter, and does not optimize the efficiency over more expressive function classes. In this paper, the authors propose a meta-algorithm generalizes existing conformal Prediction algorithms, and show that it achieves approximate valid population coverage and near-optimal efficiency within class, whenever the function class is low-capacity in a certain sense."
SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a reinforcement learning-based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. It learns a transferable reward signal formulated using the exemplary set by ordinal metric learning. The proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. Experiments on corrupted MNIST, the CU-Birds, and the COCO datasets demonstrate the effectiveness of the approach."
SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper introduces QuadTree Attention, a method to reduce the computational complexity of vision transformers from quadratic to linear. The proposed method builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patch. The authors demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks including feature matching, stereo, image classification, and object detection."
SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes a method for learning termination conditions of options by maximizing mutual information between options and corresponding state transitions. The authors derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. The experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards, combined with intrinsic rewards. Moreover, the authors also show that options learned by IMTC help an agent to quickly adapt to a specific reward function by transferring learned options."
SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper proposes a method for open-world object detection, i.e. a method that can identify objects of known categories and also discover unknown objects, and incrementally learn to categorize them when their annotations progressively arrive. The proposed method is based on semantic topology, which assigns all objects of the same category to their corresponding pre-defined centroids, including the ‘unknown’ category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."
SP:97f618558f4add834e5930fd177f012a753247dc,"This paper studies the problem of identifying informative and diverse subsets of data that lead to deep learning models with similar performance as the ones trained with the original dataset. The authors propose a novel formulation of these constraints using matroids, an algebraic structure that generalizes linear independence in vector spaces, and present an efficient greedy algorithm with constant approximation guarantees. They outperform competing baselines on standard classification datasets such as CIFAR-10, CifAR-100, ImageNet, as well as long-tailed datasets."
SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper proposes an adaptive inference strategy for semantic segmentation that adjusts the model to the test sample before producing the final prediction. The authors propose Instance-adaptive Batch Normalization (IaBN) to modify normalization layers by combining the feature statistics acquired at training time with those of the test samples. Seg-TTT (test-time training) is also proposed, which adapts the model parameters using a self-supervised loss. Experimental results show that these techniques consistently and significantly outperform the baseline and attain a new state of the art."
SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a one-class classification method for tabular data. The method assumes that it is possible to identify missing features based on the rest and employs a contrastive loss for learning without any other auxiliary loss. In an extensive set of experiments, the method presents a significant advantage over existing anomaly detection methods. "
SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a neural correlate based approach to deriving a low-dimensional embedding space for abnormal resting-state functional connectivity. The authors propose a novel type of conditional variational auto-encoder that incorporates dual utilisation of diagnostic information. The proposed method is evaluated on two empirical neuropsychiatric neuroimaging datasets and discovers a reliable nosological relation among autism spectrum disorder, major depressive disorder, and schizophrenia."
SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes an end-to-end learning framework named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit. The architecture of QTN is composed of a parametric tensor-train network for feature extraction and a tensor product encoding (TPE) framework for generating quantum embeddings. The authors theoretically characterize the QTN by analyzing its representation power of input features; (2) QTN enables an end to-end parametric model pipeline, which is the generation of quantum embedings to the output measurement. The experiments on the MNIST dataset demonstrate the advantages of QTNN for quantumembedding over other quantum embeding approaches."
SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes a method for constructing low-dimensional manifolds of neural network models, where each point corresponds to a neural network model, and two points are nearby if the corresponding neural networks enact similar high-level computational processes. The proposed method, called DYNAMO, takes as input a collection of pre-trained neural networks and outputs a meta-model that emulates the dynamics of the hidden states as well as the outputs of any model in the collection. The specific model to be emulated is determined by a model embedding vector that the meta-models takes as inputs, and the embedding vectors constitute a manifold corresponding to the given population of models. The authors apply the proposed method to both RNNs and CNNs, and find that the resulting model embeddings spaces enable novel applications: clustering of neural networks on the basis of their high level computational processes in a manner that is less sensitive to reparameterization; model averaging of several neural networks trained on the same task to arrive at a new, operable neural network with similar task performance; and semi-supervised learning via optimization on the model embeding space."
SP:29a42fdae15b9da955513f71e3100ebd0146a28a,"This paper proposes a framework for constraint-based learned simulation, where a scalar constraint function is implemented as a trainable function approximator, and future predictions are computed as the solutions to a constraint satisfaction problem. The authors implement their method using a graph neural network as the constraint function and gradient descent as a constraint solver. They test the model on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids."
SP:db07c2c0afdf27692dc504c9c54387c20211d469,"This paper proposes EDO-CS, a new evolutionary diversity optimization algorithm with clustering-based selection. In each iteration, the policies are divided into several clusters based on their behaviors, and a high-quality policy is selected from each cluster for reproduction. The authors also adaptively balance the importance between quality and diversity in the reproduction process. Experiments on various (i.e., deceptive and multi-modal) continuous control tasks show the superior performance of the proposed method."
SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper considers a distributed linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction. The interconnection structure among the agents is described by a time-varying directed graph. The paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. The equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication. "
SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes Graph Mechanics Network (GMN), a graph neural network (GNN) that is equivariant and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward Kinematics. The authors also develop a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed GMN formulation is proved to be universally expressive under certain conditions. Extensive experiments show that GMN outperforms the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency on simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture."
SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposes two federated optimization algorithms for training partially personalized models, where the shared and personal parameters are updated either simultaneously or alternately on each device, but only the shared parameters are communicated and aggregated at the server. The authors give convergence analyses of both algorithms for minimizing smooth nonconvex functions, providing theoretical support of them for training deep learning models. The experiments on real-world image and text datasets demonstrate that the proposed algorithms can obtain most of the benefit of full model personalization with a small fraction of personalized parameters, and the alternating update algorithm often outperforms the simultaneous update algorithm."
SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a framework for unsupervised learning of object representations called Contrastive Learning Through Time (CLTT), which simulates viewing sequences as they might be experienced by an infant while interacting with objects and avoids arbitrary augmentation operations. Instead, positive pairs are formed by successive views in such unsegmented viewing sequences. The authors develop a new data set using a near-photorealistic training environment based on ThreeDWorld (TDW). They consider several state-of-the-art contrastive learning methods and demonstrate that CLTT allows linear classification performance that approaches that of the fully supervised setting if subsequent views are sufficiently likely to stem from the same object. They also consider the effect of one object being seen systematically before or after another object and show that this leads to increased representational similarity between these objects, reminiscent of classic neurobiological findings."
SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes a method for goal-directed planning under a deterministic transition model. The authors extend AlphaZero with Hindsight Experience Replay (HER) method to tackle complex goal directed planning tasks. They provide a straightforward procedure that does not involve high computational costs by sampling other goals from the visited states and addressing the intrinsic on-policy nature of AlphaZero. They demonstrate the effectiveness of the proposed approach through an extensive empirical evaluation in several simulated domains, including a novel application to a quantum compiling domain."
SP:e2d33c7331db7f52b84ad1018152564d91a9f126,This paper proposes Recursive Gradient Optimization (RGO) for continual learning. RGO is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer (FEL) that represents different network structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines and achieves new state-of-the-art performance.
SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper investigates the properties and applications of aligned generative models, where two models are aligned if they share the same architecture and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. The authors empirically analyze aligned models and provide answers to important questions regarding their nature. They find that the child model’s latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Then, equipped with this better understanding, they leverage aligned models to solve a diverse set of tasks, including image-to-image translation, cross-domain image morphing, zero-shot classification and regression."
SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper proposes a new Gromov-Wasserstein (GW) distance between two graphs based on Optimal Transport (OT). The main idea is to relax the mass constraint, which imposes a coupling between all the nodes from the two considered graphs, and relax it by proposing a new semi-relaxed GW (SRGW). Theoretical analysis shows that the proposed SRGW can be used to learn a unique structure to describe a dataset of graphs, which can be seen as a Dictionary Learning approach where graphs are embedded as a subgraph of a single atom. Experiments on graph partitioning and unsupervised representation learning are conducted through clustering and completion of graphs."
SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper proposes a method to model systematic suboptimality of human behavior, i.e. when human behavior is systematically suboptimal over time. The authors introduce the Boltzmann policy distribution (BPD) as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but the authors leverage tools from generative and sequence models to enable efficient sampling and inference. They show that the BPD enables prediction of human behaviour and human-AI collaboration equally as well as imitation learning-based human models while using far less data."
SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes an on-the-fly data-free quantization (DFQ) framework with sub-second quantization time, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. The authors decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise and output channel-wise. Then, they propose a novel optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short), which does not need any dataset and is even not aware of network architecture. They also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver."
SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes a method for time series segmentation. The proposed method is based on a bi-pass architecture that combines LSTM and very deep CNN-based 1D-encoder-decoder, with several multi-scale structures, depthwise separable and atrous convolution, and a stepwise segmentation module. The experiments show that the proposed method outperforms the baselines on datasets with fast changing and slow changing labels."
SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"This paper proposes a decomposition-based explanation method for explaining GNNs. The main idea is to decompose the information generation and aggregation mechanism of GNN into feedforward propagation process and subgraph level interpretation. The authors propose concrete decomposition schemes for commonly used layers in GNN and also design an algorithm to provide subgraph-level explanation via agglomeration, which efficiently employs the topological information in graphs. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method."
SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper introduces DiffStride, a new downsampling layer for convolutional neural networks with learnable strides. The idea is to learn the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of the solution. The authors show that the proposed method can be used as a drop-in replacement to strided convolutions and outperform them."
SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a new collective robustness certificate for softly local multi-output classifiers. It is based on localized randomized smoothing, i.e. randomly smoothing different outputs using different non-i.i.d. smoothing distributions matching the model’s locality. The proposed method yields strong collective guarantees while maintaining high prediction accuracy on image segmentation and node classification tasks."
SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper proposes a method for embedding domain-specific inductive biases into normalizing flows. The method is based on the idea of normalizing flow layers, which are constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. The authors also introduce gated structured layers which allow bypassing the parts of the models that fail to capture the statistics of the data. They demonstrate that EMFs can be used to induce desirable properties such as multimodality, hierarchical coupling and continuity, and enable a high performance form of variational inference."
SP:825a254c0725008143b260ead840ae35f9f096d1,"This paper investigates the extent to which large pre-trained language models (LMs) can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. The authors test a range of generative language models of varying sizes (including GPT-2 and GPT3) and show that although smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalize to several instances of unseen concepts as well. The results suggest an alternative means of building grounded language models: rather than learning grounded representations “from scratch”, it is possible that large text-only models learn a sufficiently rich conceptual structure that could allow them to be grounded in a data-efficient way."
SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies the problem of emergent language in the presence of shaped rewards, which are designed to make the task easier to learn by introducing biases in the learning process. The authors show that the inductive bias introduced by shaped rewards biases the object of study, i.e., the language. They also show that shaped rewards can change the distribution of an emergent property of the language, and mask the emergent effects of other environmental variables. "
SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"This paper proposes an unsupervised cross-lingual learning method, called importance-weighted domain alignment (IWDA), that performs representation alignment, prior shift estimation, and correction. The authors show that IWDA is effective at the estimation and correction of class prior shift from unlabeled data, and robust to large prior shifts. In addition, the method delivers further performance gains when combined with existing semi-supervised learning techniques."
SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper proposes a bootstrapping meta-learning algorithm to improve the efficiency and performance of the meta-learner. The algorithm first bootstraps a target from the metalearner, then optimizes the meta learner by minimising the distance to that target under a chosen (pseudo-)metric. The authors show that the proposed algorithm can achieve a new state-of-the-art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta learning. Finally, the authors explore how bootstrap can meta-learn efficient exploration in an $\epsilon$-greedy Q-learning agent."
SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of model-based RL agents in the context of procedural and task generalization. Specifically, the authors compare the performance of MuZero, a state-of-the-art model-free RL agent, on Procgen and Meta-World. They identify three factors of procedural generalization: planning, self-supervised representation learning, and procedural data diversity, and show that combining these techniques results in improved performance and data efficiency. They also find that these factors do not always provide the same benefits for task generalisation in Meta-world, indicating that transfer remains a challenge. Overall, they suggest that building generalizable agents requires moving beyond the single-task single-model-free paradigm and towards self supervised models-based agents."
SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a graph convolutional neural network (GCN) architecture for graph learning. In particular, the authors formulate the graph learning task as a network inverse (deconvolution) problem, and propose a graph deconvolution network (GDN) to solve this problem. The proposed GCN consists of a set of layers that directly operate on, combine, and refine graph objects (instead of node features), and can generalize to larger-sized graphs after training. Experiments on synthetic and real-world datasets demonstrate the effectiveness of the proposed method. "
SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes a method for reward shaping in reinforcement learning. The authors propose a framework in which the shaping-reward function is constructed in a Markov game between two agents. The primary agent (Shaper) determines which states to add shaping rewards and their optimal values, while the other agent (controller) learns the optimal policy for the task using these shaped rewards. Theoretical results show that the proposed method is able to learn to construct a shaping reward function that is tailored to the task thus ensuring efficient convergence to high performance policies. Empirical results demonstrate the effectiveness of the proposed approach."
SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,This paper proposes a method to improve the robustness of vertical federated learning (VFL) by recovering the underlying uncorrupted features with provable guarantees and thus sanitizes the model against a vast range of backdoor attacks. The authors also defend against inference-phase adversarial and missing feature attacks. They conduct extensive experiments on NUS-WIDE and CIFAR-10 datasets and show that RVFR outperforms different baselines in terms of robustness against diverse types of attacks.
SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,This paper investigates the use of contrastive learning for unsupervised dense information retrieval. The authors propose to use a contrastive loss to improve the performance of the dense retrieval model. They show that the proposed method outperforms the existing dense retrieval methods on the BEIR and MARCO datasets. They also show that fine-tuning the model on the MARCO dataset improves the performance. 
SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper analyzes the trade-off between fine-tuning and linear probing when transferring a pre-trained model to a downstream task. The authors show that fine tuning can lead to worse accuracy than linear probing in-distribution (ID) and OOD, especially when the pretrained features are good and distribution shift is large. They also theoretically analyze the tradeoffs arising in fine tuning over-parameterized two-layer linear networks. The analysis suggests that the simple two-step strategy of linear probing (updating the last linear layer) and full fine tuning combines the benefits of both fine tuning (i.e. linear probing then full finetuning) and achieves better ID/OOD accuracy than fine tuning."
SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper studies the problem of learning to discover novel classes (L2DNC). In this problem, we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. In this paper, the authors demystify assumptions behind L2DNc and find that high-level semantic features should be shared among the seen and unseen classes and can be naturally linked to meta-learning that has exactly the same assumption as L2Nc. Then, they can empirically solve the L2DNCL problem by meta learning algorithms after slight modifications. The proposed meta discovery method significantly reduces the amount of unlabeling data needed for training and makes it more practical."
SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper considers the problem of learning a causal model of the environment in POMDPs, where the learning agent has the ability to collect online experiences through direct interactions with the environment (interventional data), but also has access to a large collection of offline experiences obtained by observing another agent (observational data). A key ingredient, which makes this situation non-trivial, is that we allow the observed agent to act based on privileged information, hidden from the learner. The authors propose a general yet simple methodology for safely leveraging offline data during learning. In a nutshell, their method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard POMD transition model via deconfounding using the recovered latent variable. They prove their method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and they assess its effectiveness empirically on a series of synthetic toy problems."
SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper studies the problem of using a retriever to retrieve passages from a textual knowledge corpus (e.g., Wikipedia) and providing these passages as additional context to the generator for open-ended generation tasks. The authors propose using an additional guide retriever that is allowed to use the target output and “in hindsight” retrieve relevant passages during training. They model the retriever after the posterior distribution Q of passages given the input and the target text output and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q. For informative conversations from the Wizard of Wikipedia dataset, with posterior-guided training, the retrieever finds passages with higher relevance in the top-10 (23% relative improvement), the generator’s responses are more grounded in the retrieved passage (19%) and the end-to-end system produces better overall output (6.4%) relative improvement."
SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper proposes a Graph Neural Network (GNN) for influence estimation and influence maximization. The main idea is to use a GNN to predict the influence of each node in a graph, and then use a submodular function based on the predictions to rank the nodes in the graph. The proposed method is evaluated on simulated and real-world graphs, and is shown to outperform existing methods. "
SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,This paper proposes a novel active learning approach for domain adaptation based on a localized discrepancy between the labeled and unlabeled distributions. The authors provide both theoretical guarantees of this approach and an active learning algorithm scaling to large data sets. Several experiments show that the proposed approach is competitive against other state-of-the-art active learning techniques in the context of domain adaptation.
SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper proposes a variational approximation for Bayesian neural networks by leveraging insights from singular learning theory. Namely, for large n, the posterior distribution over neural network weights is not Gaussian but rather can be put into a mixture of standard forms. The generalized gamma mean-field family, following desingularization, can in theory achieve the leading order term of the log normalized evidence."
SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper provides a generalization bound for domain generalization (DG) based on the Rademacher complexity of the model. The authors conjecture that existing methods’ efficacy or lack thereof is a variant of the standard empirical risk-predictor complexity trade-off, and demonstrate that their performance variability can be explained in these terms. The analysis suggests that domain generalisation should be achieved by simply performing regularised ERM with a leave-one-domain-out cross-validation objective."
SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper introduces the maximum n-times coverage problem, which is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. The authors propose a novel ILP based method and a beam search algorithm for solving the problem, and use them to produce a peptide vaccine design for COVID-19. The pan-strain vaccine delivers MHC class I and class II optimized epitopes with a single mRNA-LNP construct."
SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"This paper proposes an initialization method for weight initialization for spiking neural networks (SNNs) that is consistent with the response of spiking neurons in initial training. The authors derive an asymptotic formula for their response curve, approximating the actual neuron response distribution. Then, they propose a weight initialization method based on the slant-asymptote to overcome gradient vanishing. Experiments on the MNIST and CIFAR-10 datasets show that the proposed method can improve the training speed and the model accuracy compared with other SNN initialization methods."
SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a federated expectation-maximization (FEM) algorithm for dealing with distribution shift in federated learning. The authors propose to model the distribution shift with a mixture of distributions that gradually changes between daytime and nighttime modes, and find this intuitive model to better match the observations in practical FL systems. They propose a Federated Expectation-Maximization algorithm enhanced by Temporal priors of the shifting distribution (FedTEM), which jointly learns a mixture model to infer the mode of each client, while training a network with multiple light-weight branches specializing at different modes. Experiments for image classification on EMNIST and CIFAR datasets and next word prediction on the Stack Overflow dataset show that the proposed algorithm can effectively mitigate the impact of distribution shift and significantly improve the final model performance."
SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,This paper proposes a method for pruning deep neural networks (DNs) based on a recently developed spline interpretation of DNs. The authors show that a DN’s spline mappings exhibit an early-bird (EB) phenomenon whereby the spline's partition converges at early training stages. They leverage this insight to develop a principled and efficient pruning strategy that focuses on a tiny fraction of DN nodes whose corresponding spline partition regions actually contribute to the final decision boundary. Extensive experiments on four networks and three datasets validate that our new spline-based DN pruning approach reduces training FLOPs by up to 3.5x while achieving similar or better accuracy than current state-of-the-art methods.
SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper considers the problem of fair representation learning, i.e. ensuring that the optimal predictors, on top of the data representation, are ensured to be invariant with respect to different subgroups. Specifically, the authors formulate the problem as a bi-level optimization, where the representation is learned in the outer-level, and invariant optimal group predictors are updated in the inner-level. To avoid the high computational and memory cost of differentiating, the paper proposes the implicit path alignment algorithm, which only relies on the solution of inner optimization and the implicit differentiation rather than the exact optimization path. The proposed method is evaluated in both classification and regression settings."
SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper presents an empirical study of reinforcement learning via supervised learning (RvS) methods, which solve RL problems via conditional imitation learning (CIL). RvS methods have been shown to reduce the offline RL problem to weighted, filtered, or conditional behavioral cloning problems. The main idea behind these methods is that data that is suboptimal for one task may nonetheless be useful for learning to solve another task. However, it remains unclear which ingredients are essential for such methods to work well, and when these methods do or do not work outperform value-based approximate dynamic programming algorithms. This paper studies the importance of these design decisions. The most important design decisions boil down to carefully choosing model capacity and choosing which information to condition on (e.g., goals or rewards). The experiments show that more complex design choices, such as the large sequence models and value based weighting schemes used in prior work, are often not necessary."
SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a Hierarchical Bayesian framework to model the exploration process of humans. The proposed method is based on the idea that humans explore new environments by inferring the structure of unobserved spaces re-using spatial information collected from previously explored spaces. The authors model this cognitive process computationally through “Map Induction”, which involves the compositional formation of proposed maps of complex spaces based on already-seen spaces through program induction in a hierarchical framework. They introduce a new behavioral Map Induction Task, and compare human performance with that of state-of-the-art existing models as well as our MapInduction framework. "
SP:cccdcc95c4177b5531bad23b662060fdd0d88849,This paper proposes a differentiable approach to learn the probabilistic factors used for inference by a nonparametric belief propagation algorithm. The method learns to maintain a set of marginal posterior samples using end-to-end training. The authors evaluate the proposed method on a series of articulated pose tracking tasks and compare performance with learned baselines. Results demonstrate the effectiveness of using learned factors for tracking and suggest the practical advantage over hand-crafted approaches.
SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,"This paper proposes a graph-based generative model for molecular optimization. The authors propose to use the initial seed of the generative procedure as the scaffold for the generation process, which is not conditioned on the generation history. The proposed method is evaluated on unconstrained optimization and scaffold-based optimization tasks, and outperforms the state-of-the-art methods."
SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper considers the problem of imitation learning for deterministic deterministic experts. The authors propose to reduce the imitation learning problem to a reinforcement learning problem, where the reward function is assumed to be a reward function of the policy. They prove a lower bound on the total variation distance between the expert and the imitation learner, and show that this lower bound is upper bounded by an RL solver. In addition, they show that if the expert policy is deterministic, then the imitation policy can be learned by a single invocation of an RL algorithm. Finally, they evaluate the proposed method on a family of continuous control tasks, showing that it achieves competitive performance while being relatively simple to implement."
SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper provides a theoretical analysis of the effect of reweighting algorithms on the worst-case performance of fairness-aware algorithms. The authors show that under certain assumptions, the overparameterized model always converges to the same ERM interpolator that fits all training samples, and consequently its worst-group test performance will drop to the level of ERM in the long run. Then, the authors analyze whether adding regularization helps fix the issue, and prove that for regularization to work, it must be large enough to prevent the model from achieving small training error."
SP:318ace9202e42d1d278eb79fe1853138e1d00a06,"This paper proposes a framework, Rational Inattention (RI) for multi-agent reinforcement learning (MARL), which models the cost of cognitive information processing using mutual information. The framework generalizes and is more flexible than prior work by allowing multi-timestep dynamics and information channels with heterogeneous processing costs. The authors evaluate RIRL in two Principal-Agent (specifically manager-employee relations) problem settings of varying complexity where RI models information asymmetry (e.g., it may be costly for the manager to observe certain information about the employees). The authors show that using RI yields a rich spectrum of new equilibrium behaviors that differ from those found under rational assumptions. For instance, some forms of a Principal’s inattention can increase Agent welfare due to increased compensation, while other forms can decrease Agent welfare by encouraging extra work effort."
SP:100c91da177504d89f1819f4fdce72ebcf848902,"This paper proposes a phase-oriented adversarial attack for audio adversarial attacks. The authors leverage spectrogram consistency of short-time Fourier transform (STFT) to adversarially transfer phase perturbations to the adjacent frames of magnitude spectrogram and dissipate the energy patterns in the spectrogram. Moreover, the authors propose a weighted loss function to improve the imperceptibility of the adversarial examples. The proposed method achieves a 6.64x generation speed-up over current state-of-the-art imperceptible counterparts."
SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper studies the representation learning in contrastive self-supervised learning (SSL). The authors propose a generalized version of DirectPred (Tian et al., 2021) which they call DirectSet(alpha) and show that it provably learns a desirable projection matrix and reduces the sample complexity on downstream tasks. They also show that weight decay acts as an implicit threshold that discards the features with high variance under augmentation and keeps the feature with low variance. Inspired by the theory, they simplify DirectPred by removing the expensive eigen-decomposition step. On CIFAR-10, Cifar-100, STL-10 and ImageNet, DirectCopy achieves comparable or better performance than the original DirectPred."
SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. The authors derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, and prove that LEM can approximate a large class of dynamical systems to high accuracy. The empirical results on image and time-series classification, dynamical system prediction, keyword spotting and language modeling demonstrate the effectiveness of LEM."
SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper proposes geometric deep learning models that are rotation- and permutation-equivariant on small point clouds, composed of a set of products of terms from the geometric algebra and reductions over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. They demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology to motivate the development of equivariant models."
SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a method to solve the order fulfillment problem, which is a combinatorial optimization problem for real-time online retailing. The proposed method is based on the edge-feature-embedded graph attention, which considers the high-dimensional edge features and accounts for the heterogeneous information. The model is size-invariant for problem instances of any scale, and it can address cases that are completely unseen during training. Experiments show that the proposed method substantially outperforms the baseline heuristic method in optimality."
SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,"This paper addresses the problem of inferring concepts out of the dialogue context (CODC) in the dialogue summarization task. The authors propose a novel framework comprised of a CODC inference module leveraging external knowledge from WordNet and a knowledge attention module aggregating the inferred knowledge into a neural summarization model. They also propose a new evaluation metric based on the inference capability of different methods. Experiments suggest that current automatic evaluation metrics of natural language generation may not be enough to understand the quality of out-of-context inference in generation results, and the proposed summarisation model can provide statistically significant improvements on both CIDC inference and traditional automatic metrics."
SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the question of what kinds of semantic dependencies can be modelled in entity embedding models. Specifically, the authors focus on settings where the embedding of an entity is obtained by pooling the embeddings of its known attributes. They first show a number of negative results, revealing that some of the most popular embedding approaches are not able to capture even basic Horn rules. However, they also find that some embedding strategies are capable, in principle, of modelling both monotonic and non-monotonic attribute dependencies."
SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper studies the performance of transformer-based models on different reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning. It shows that when explicitly given all the information required to perform deductive reasoning, such as facts and rules, the models can easily learn logical reasoning, but when this information is stated only implicitly in the text or in the supervision the models struggle. It also shows that the strength of transformer models comes from simple patterns in the training data, combined with background knowledge from the pretraining."
SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper introduces the compositional problem graph (CPG), which is a framework for learning algorithms for composing representation transformations in order to generalize to new tasks. The authors propose a new algorithm called Compositional Recursive Learner (CRL), which learns to predict which sub-problems to solve by making analogies to previously seen problems. The proposed method is evaluated on a symbolic and a high-dimensional domain, where it is shown that it outperforms baselines that are not explicitly compositional."
SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IP), which combines the activation pruning and weight pruning techniques to improve the execution efficiency of neural networks. The idea is to prune the activations of the network in order to balance the sparsity between activations and weights. The proposed method is evaluated on the MNIST, CIFAR-10 and ImageNet datasets. The results show that the proposed method can reduce the model size by 2.3x-5.8x and 2.5x-10x."
SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a method for feature selection based on generative adversarial networks (GANs). Specifically, the authors propose to use GANs to generate knockoffs of the original features, and then use the generator, discriminator, stability network, and power network to select the features to be used in the knockoff generation process. The proposed method is evaluated on synthetic and real-world datasets, and it is shown that the proposed method outperforms the original knockoff framework."
SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes a method to improve the Winograd-domain sparsity of convolutional neural networks. The method consists of two steps: spatial structured pruning and direct pruning. In the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the sparsity from the spatial domain into the winograd domain. The second step is to perform both pruning in the spatial domains and in the general domain. An importance factor matrix is proposed to adjust the weight gradients in the final step, which makes it possible to effectively retrain the pruned network without changing the network structure. Experiments are conducted on CIFAR-10, Cifar-100, and ImageNet."
SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,"This paper proposes Adversarially Learned Mixture Model (AMM), a generative model for unsupervised or semi-supervised data clustering. The AMM is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available. The paper also shows that the use of additional losses or discriminators are redundant additions to frameworks, as AMM yields similar or better results than these methods."
SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes the augment-REINFORCE-merge (ARM) estimator to backpropagate the gradients through stochastic binary layers, which is unbiased, exhibits low variance, and has low computational complexity. The estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric “self-control” baseline function together with the REinFORCE estimator in that augmented space. Experimental results show that the proposed estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation for discrete latent variable models."
SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a modular probabilistic programming language (MXFusion) that includes a new type of re-usable building blocks (probabilistic modules) that can be used as fundamental building blocks in the ML framework. The authors propose to use the framework of variational inference (VI) for the inference of the modules, so that the pre-specified inference methods can be transparently used for inferring the whole probablistic model. Experiments on real data demonstrate the effectiveness of the proposed approach. "
SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method for pruning large neural networks at initialization. The proposed method is based on a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. The method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to various architectures including convolutional, residual, and recurrent networks."
SP:986b9781534ffec84619872cd269ad48d235f869,"This paper provides an empirical study of the behavior of beam search algorithm across three sequence synthesis tasks. It finds that increasing the beam width leads to sequences that are disproportionately based on early and highly non-greedy decisions. These sequences typically include a very low probability token that is followed by a sequence of tokens with higher (conditional) probability leading to an overall higher probability sequence. As beam width increases, such sequences are more likely to have a lower evaluation score. Based on the empirical analysis, the authors propose two methods to constrain the beam search from taking non-rewarding decisions early in the search. Experiments show that constrained beam search effectively eliminates the performance degradation and leads to higher evaluation scores."
SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes Backplay, a method to improve the sample efficiency of model-free reinforcement learning by constructing a curriculum around a demonstration. The idea is to start each training episode in the environment’s fixed initial state and move the starting point backwards during the course of training until we reach the initial state. The authors analyze the types of environments where Backplay can improve training speed and demonstrate the effectiveness of Backplay both in large grid worlds and a complex four-player zero-sum game (Pommerman)."
SP:426c98718b2dbad640380ec4ccb2b656958389bc,"This paper proposes a multi-layer pruning method (MLPrune) to automatically decide appropriate compression ratios for all layers. The authors use an efficient approximation of the Hessian as the pruning criterion, based on a Kroneckerfactored Approximate Curvature method. They demonstrate the effectiveness of their method on several datasets and architectures, outperforming previous state-of-the-art by a large margin."
SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper presents an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. It identifies a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, it measures the ability of interventions to control objects in the output by inserting the discovered object concepts into new images. Finally, it provides open source interpretation tools to help researchers and practitioners better understand their GAN models."
SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper studies the relationship between the distance between the parameters of a neural network and the function that maps inputs to outputs in the space of functions in L-Hilbert space. In particular, the authors show that the function L distance is related to the parameter L distance in terms of the ratio of the L/` ratio. The authors then propose to use the function distance as a regularizer in two applications: multitask learning and optimization. "
SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a framework for learning deep generative Markovian dynamics models from biological data. The main idea is to train a generative model that predicts the probability distribution of the next state based on the current state. The generative process can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. The authors show the advantage of learning deep models over shallow models such as Kalman filters and hidden Markov models that do not learn representations."
SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,"This paper proposes a graph Laplacian-based unsupervised generative adversarial networks (GANs) for spectral clustering. The main idea is to use a linear map between the discriminator (D) and generator (G) to estimate the number of classes, which is then used to train a generative model. The proposed method can also be used to classify the images. "
SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,This paper proposes a method for learning permutation-invariant representations of sets. The proposed method is based on the idea that sets should be permutation invariant (i.e. set operations should be set-specific). The authors propose a module that learns how to permute a set end-to-end. The permuted set can be further processed to learn a set representation. The authors demonstrate the model’s ability to learn permutations and set representations with either explicit or implicit supervision on four datasets.
SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper proposes Projective Subspace Networks (PSNets) for few-shot learning, which learns non-linear embeddings from limited supervision. The embedding in PSN deems samples of a given class to form an affine subspace. The authors show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised learning. Moreover, the PSN approach has the ability of end to end learning."
SP:d7544bc4a0ae3237daa207e789a522363fb5170d,"This paper proposes a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), this method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems."
SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"This paper proposes a framework where the user controls what characteristics of the data (utility) and what they want to keep private (secret), without necessarily asking the utility provider to change its existing machine learning algorithms. They analyze the space of privacy-preserving representations and derive natural information-theoretic bounds on the utility-privacy trade-off when disclosing a sanitized version of the original data X. They present explicit learning architectures to learn privacy preserving representations that approach this bound in a data-driven fashion. They describe important use-case scenarios where the utility providers are willing to collaborate with the sanitization process. The utility provider can use the same algorithm on original and sanitized data, a critical and novel attribute to help service providers accommodate varying privacy requirements with a single set of utility algorithms."
SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation (PAGAN) method to improve the stability of GAN training. The main idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. The authors show that the proposed PAGAN preserves the original GAN objective, does not bias the optimality of discriminator and encourages the healthy competition between the generator and discriminator, leading to a better-performing generator. They experimentally demonstrate the effectiveness of the proposed approach on multiple benchmarks for the image generation task."
SP:c210982ccdd134d4b293dbe144990398eefe1a86,"This paper proposes a rotation-equivariant convolutional neural network to predict neural activity in primary visual cortex (V1) using two-photon imaging (2P imaging). The network is trained to be rotation equivariant and is able to predict the responses of a population of 6000 neurons to natural images recorded in V1 using 2P imaging. The results show that the network outperforms a regular CNN with the same number of feature maps and reveals a number of common features, which are shared by many V1 neurons and are pooled sparsely."
SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper proposes a distributed stochastic gradient descent (SIGNSGD) algorithm for robust, communication-efficient neural network training. The main idea of the algorithm is to use a majority vote to decide the overall update of the gradients. Theoretically, the authors prove that the algorithm converges to a parameter regime of the ADAM as a byproduct. The authors also prove that unlike distributed SGD, majority vote is robust when up to 50% of workers behave adversarially, which is a special case of invert or random gradient estimation."
SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a GAN-based approach to generate realistic stock market data based on generative adversarial networks. The authors model the order stream as a stochastic process with finite history dependence, and employ a conditional Wasserstein GAN to capture history dependence of orders in a stock market. They test their approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data."
SP:ba66503753b3c57781b435c55c47fc9f69450e65,This paper studies the problem of reinforcement learning in the setting where the observed rewards are generated with a reward confusion matrix. The authors call such observed rewards as perturbed rewards. They develop an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed reward. The framework draws upon approaches for supervised learning with noisy data. The core ideas of the solution include estimating a reward matrix and defining a set of unbiased surrogate rewards. Extensive experiments on different DRL platforms show that policies based on the estimated surrogate reward can achieve higher expected rewards and converge faster than existing baselines.
SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the effect of network structure (depth and width) on halting time and shows that larger models--wider models in particular--takes fewer training steps to converge. The authors design simple experiments to quantitatively characterize the effect on weight space traversal. Results show that halting time improves when growing model’s width for three different applications, and the improvement comes from each factor: the distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a Power-Law- like relationship, and gradient vectors become more aligned with each other during traversal (Table 1)."
SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper studies the problem of learning optimal algorithms for online combinatorial optimization problems. The authors propose a primal-dual framework for online optimization problems, in which the optimal algorithms can be found using RL. Specifically, they introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to find algorithms that work well in the worst-case. They test their new ideas on the AdWords problem, the online knapsack problem, and the secretary problem. Their results indicate that the models have learned behaviors that are consistent with the optimal policies for these problems derived using the online primal-Dual framework."
SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper re-examines several domain-specific components that modify the agent’s objective and environmental interface. The authors investigate whether the performance deteriorates when all these fixed components are replaced with adaptive solutions from the literature. They show that performance sometimes decreases with the adaptive components, as one might expect when comparing to components crafted for the domain. They then investigate the main benefit of having fewer domain- specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system. The system with adaptive components performed better on many of the tasks."
SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for the Vision-and-Language Navigation (VLN) task with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images, and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. The proposed method sets a new state-of-the-art performance on the standard Room-to-Room dataset on both seen and unseen environments."
SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,"This paper proposes two methods to improve the performance of neural program synthesis (NP). The first method is execution-guided synthesis (EG), which uses a neural network to predict the output of an encoder-decoder architecture. The second method is synthesizer ensemble (SE), which is an ensemble of neural networks that predict the outputs of a set of synthesizers. The authors show that using these two methods improves the performance on the Karel dataset from 77% to 90%."
SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the stability, convergence and acceleration properties of batch normalization (BN) in the context of ordinary least squares (OLS). In particular, it shows that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, acceleration effects, as well as insensitivity to the choice of learning rates. The authors also demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems."
SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,"This paper presents a new perspective of data noising in recurrent neural network language models (RNNs). The authors show that RNNs are Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). Based on this insight, the authors propose a more principled method to apply at prediction time and propose natural extensions to data noisings under the variational framework. In particular, they propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoother method. The authors empirically verify their analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data-noising methods."
SP:f4a914d3df1a5a21a7365ba78279420f39210884,"This paper proposes a classifier-agnostic saliency map extraction method for object localization. The proposed method aims at finding a saliency mapping that works for all possible classifiers weighted by their posterior probabilities. The authors designed a practical algorithm that amounts to simultaneously training classifier and saliency maps using stochastic gradient descent. Qualitatively, the proposed approach extracts saliency masks that cover all the relevant pixels in an image and that the masked-out images cannot be easily recovered by inpainting, unlike for classifier dependent approaches. "
SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a method for transferring knowledge from a large number of deep neural networks (DNNs) to a new DNN model, which is called a student. The student model is trained independent of the teachers and can be trained on different tasks with different output spaces. The proposed method is evaluated on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other methods."
SP:a72072879f7c61270d952f06d9ce995e8150632c,This paper proposes a method for learning a compact dynamical model of complex systems. The authors propose to perform a soft-clustering of the data and learn the dynamics of the system to produce a compact model while still ensuring the original objectives of causal inference and accurate predictions. They cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. They provide theoretical guarantees concerning the convergence of the proposed learning algorithm.
SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in “one shot”. The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of generated samples."
SP:f46f0cb43274fb20cba91ef7318305f668bc6928,"This paper proposes an approximation strategy to reduce the memory footprint of neural networks during training. During the forward pass, the authors replace activations with lower-precision approximations immediately after they have been used by subsequent layers, thus freeing up memory. The approximate activations are then used during the backward pass. Experiments on CIFAR and ImageNet show that using the proposed approach has only a minor effect on training and validation performance while affording significant savings in memory usage."
SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,"This paper proposes a progressive GAN with frequency-oriented attentive modules (FAM) for high-resolution and fast face completion, which learns face structures from coarse to fine guided by the FAM. The proposed method can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x 1024 resolution. A conditional version of the model allows users to control the properties of generated images explicitly with attribute vectors and landmarks."
SP:a300122021e93d695af85e158f2b402d21525bc8,"This paper presents a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. They show that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, and derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. They apply their analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet, and show that with accumulation precision set in accordance with their proposed equations, the networks successfully converge to the single precision floating-point baseline."
SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,"This paper studies the risk convergence and asymptotic convergence of gradient flow and gradient descent on deep linear networks on linearly separable data. In particular, this paper shows that, for gradient flow applied to strictly decreasing loss functions, the risk converges to 0, normalized weight matrices are aligned across layers, and the linear function induced by the network converges in the same direction as the maximum margin solution. This paper also shows that for gradient descent applied to the same loss functions with decreasing step sizes."
SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper extends the hredGAN (Li et al., 2016) to a multi-turn dialogue scenario by modifying the state-of-the-art sequence-to-sequence neural network (Seq2Seq) neural network conversation model to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN, has a persona-based HRED generator (PHRED) and a conditional discriminator. The authors also explore two approaches to accomplish the conditional discriminators: (1) PHredGANa, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) PHREDGANd, a dual discriminator system which collaboratively predicts the attribute(s) that generated the input utterance."
SP:017b66d6262427cca551ef50006784498ffc741d,"This paper proposes a goal-driven collaborative task between two agents, called CoDraw, which is a collaborative game between a Teller and a Drawer, where the Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, and the Drawer tries to reconstruct the scene on an empty canvas. The two players communicate via two-way communication using natural language. The authors collect the CoDraw dataset of 10K dialogs consisting of 138K messages exchanged between human agents. They also define protocols and metrics to evaluate the effectiveness of learned agents on this testbed, highlighting the need for a novel crosstalk condition which pairs agents trained independently on disjoint subsets of the training data for evaluation. They present models for our task, including simple but effective nearest-neighbor techniques and neural network approaches trained using a combination of imitation learning and goal- driven training."
SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a new approach to learn neural random fields (NRFs) with an inclusive-divergence minimized auxiliary generator. The approach is based on the idea that the generator and the random field can be trained jointly in a black-box manner, i.e., the generator can be used for both unsupervised/supervised image generation and semi-supervised classification. The proposed approach is evaluated on the widely-used datasets MNIST, SVHN and CIFAR-10. "
SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper studies adversarial attacks on graph neural networks for node classification that perturb the discrete graph structure. The authors propose to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. The experiments show that small perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings."
SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper proposes a new generative model based on sliced Wasserstein autoencoders (SWAE) and WAE-MMD (WAE using maximum mean discrepancy based distance function). The proposed model is called Cramer-Wold autoencoder (CWAE). CWAE cost function is based upon a characteristic kernel (CramerWold kernel) and has a simple closed-form in the case of normal prior. As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of SWAE."
SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper studies the problem of few-class few-shot (MCFS) classification in both supervised learning and meta-learning scenarios. The authors propose a memory-augmented hierarchical-classification network (MahiNet) for MCFS learning. It addresses the “many-class” problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine classes and is cheaper to obtain. MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory augmentation module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes. The MLP extends the linear classifier and the attention module extends a KNN classifier, both together targeting the ""few-shot"" problem."
SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes Structural-Jump-LSTM, a recurrent neural network for speed reading, which can skip irrelevant words in important sections, while also jumping past unimportant parts of a text. It uses the dynamically spaced punctuation structure of text to determine whether to jump to the next word, the next sub-sentence separator (,;), next end of sentence, or to the end of the text. In addition, it allows skipping a word after observing it without updating the state of the RNN. The proposed method is evaluated against all five state-of-the-art neural reading models and achieves the best overall FLOP reduction."
SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a nonlinear radial basis convolutional feature transformation by learning the Mahalanobis distance function that maps the input convolutionsal features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, which prevents small adversarial perturbations from forcing a sample to cross the decision boundary. The proposed method is evaluated on three publicly available image classification and segmentation data-sets namely, MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14."
SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a novel method for temporally consistent exploration in reinforcement learning. The proposed method, called NADPEx, uses dropout at the beginning of episodes to sample plausible subnetworks from the same complete network and uses them to explore the environment with diverse and consistent behavior patterns and updated through simultaneous gradient back-propagation. Two factors, gradients’ alignment with the objective and KL constraint in policy space, are discussed to guarantee the stable improvement. The experiments demonstrate the effectiveness of the proposed method on the Mujoco benchmark."
SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper introduces the nonlinearity coefficient (NLC) as a new metric for predicting the test error of neural networks. The NLC is computed in the network’s randomly initialized state and is shown to be a powerful predictor of test error. The authors show that attaining a right-sized NLC (i.e., a non-zero value) is essential for attaining an optimal test error, at least in fully-connected feedforward networks. They also show that avoiding excessive output bias and using skip connections play important independent roles in performance."
SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. The authors demonstrate that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive “weak” features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification, which is a previously unreported form of bias that can be traced back to the features of a trained model. The paper shows that while some bias cannot be mitigated without sacrificing accuracy, the authors propose two feature selection algorithms for mitigating bias amplification. The experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy, in some cases eliminating predictive bias altogether while providing modest gains in accuracy."
SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the generalization properties of neural networks from a margin-based perspective. The authors show that the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, and that increasing the over-parameterization improves the normalized margin and generalization error bounds for deep networks. In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees."
SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"This paper proposes an approach to control the location of objects within an image by adding an object pathway to both the generator and the discriminator. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. Experiments on the Multi-MNIST, CLEVR, and the more complex MSCOCO data set show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations."
SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a model-based reinforcement learning algorithm, SOLAR, that learns representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local model methods to be used for policy learning in complex systems. The key insights in SOLAR involve learning latent representations where simple models are more accurate and utilizing PGM structure to infer dynamics from data conditioned on entire real-world trajectories. The experimental results demonstrate that SOLAR is competitive in sample efficiency, while exhibiting superior final policy performance."
SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,This paper proposes a method for counterfactually guided policy search (CF-GPS) that leverages structural causal models for evaluation of arbitrary policies on individual off-policy episodes. The proposed method is based on the Guided Policy Search (GP) algorithm. The authors show that the proposed method can improve the performance of GP by making use of available logged data to de-bias model predictions. The method is evaluated on a grid-world task. 
SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,"This paper studies the relationship between adversarial robustness and the geometry of the decision surface in the input space of a neural network. The authors show that the geometry property of decision surface correlates well with the robustness of the network to adversarial attacks. Based on this, the authors propose a robust training method that does not require adversarial training. Experiments are conducted to verify the effectiveness of the proposed method."
SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. The authors integrate the quantitative DNN energy estimation into the training process to assist the constrained optimization. They prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving methods, this framework trains DNNs that provide higher accuracies under the same or lower energy budgets."
SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper proposes a method for learning a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. The authors formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this distribution. To address challenges from discretizing the continuous latent parameter space, the authors propose a new policy network architecture that encodes the belief distribution independently from the observable state. The proposed method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art POMDP solvers."
SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a unified framework for building unsupervised representations of entities and their compositions by viewing each entity as a histogram (or distribution) over its contexts. This enables us to take advantage of optimal transport and construct representations that effectively harness the geometry of the underlying space containing the contexts. The method captures uncertainty via modelling the entities as distributions and simultaneously provides interpretability with the optimal transport map, hence giving a novel perspective for building rich and powerful feature representations. The key tools at the core of this framework are Wasserstein distances and wasserstein barycenters, hence raising the question from our title, how to represent entities by representing entities by a distributional estimate on top of any given co-occurrence structure. For each entity, we jointly consider the histogram information (with its contexts) as well as the point embeddings of the context. The framework results in an efficient, interpretable and compositional metric to represent and compare entities and groups thereof."
SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper investigates the relationship between the performance of model-based reinforcement learning (ML) methods and the asymptotic performance of a model-free RL method (MLP). The authors propose to use a dynamics model to predict the next state given the current state and a long sequence of actions, and then use a planner to estimate the future state. They show that the dynamics model is able to predict more accurate state estimates than the planner. They also show the importance of long planning horizons, beyond those typically used."
SP:da14205470819495a3aad69d64de4033749d4d3e,This paper proposes an end-to-end high-precision information flow to reduce the accumulated quantization error in neural network quantization. The proposed precision highway reduces quantization errors by keeping high precision activation from the input to output of the network with small computation costs. The authors also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. The experimental results show that the proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization and 2-bit quantization with no accuracy loss.
SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a generative adversarial network-based method for image restoration. The proposed method is based on the density estimation of latent variables, which is formulated as a constrained optimization problem, where the objective is to maximize a posteriori probability of the latent variables and the constraint is that the image generated by these latent variables must be the same as the degraded image. The method is evaluated on the MNIST dataset, and the results show that the proposed method outperforms the baselines."
SP:2feef921a0563d52fde1c074da754f73e6cabef8,"This paper proposes a method for knowledge distillation from few samples (FSKD) by adding a 1x1 conv-layer at the end of each block in the student-net, and aligning the block-level outputs between ""teacher"" and ""student"" by estimating the parameters of the added layer with limited samples. The method works for student-nets constructed in various ways, including compression from teachernets and fully redesigned networks with random initialization on various datasets. Experiments verify that the proposed method is very efficient and effective to distill knowledge from teacher-net to student networks in different ways."
SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper proposes a transferability metric, H-score, to measure the transferability of features from one classification task to another classification task. The transferability is defined as the ratio between the best achievable error exponent of the transferred representation and the minium error exponent for the target task. This metric can be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments on both synthetic and real-world data show that the proposed transferability score is meaningful in practice, and can generalize to inference problems beyond classification."
SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes to add a planning phase in neural machine translation (NMT) to control the global sentence structure ahead of translation. The proposed approach learns discrete structural representations to encode syntactic information of target sentences. During translation, the model first generates the codes, then output the words conditioned on the structural constraint imposed by the codes. Experiments show that the proposed method can avoid degrading the translation performance."
SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"This paper proposes a new loss function for generative adversarial networks (GANs) based on the notion of “relativism”, i.e., the probability that the given real data is more realistic than a randomly sampled fake data, on average. The authors also propose a variant of this loss function, which they call “Relativistic average GANs” (RGANs), which is a generalization of the original RGAN. They show that the proposed loss function can be used to replace the identity function in the standard GAN, and that it can also be used in the non-standard GAN loss functions. They also show that this loss can be combined with other loss functions such as the spectral norm, gradient penalty, etc. to improve the performance of the GAN."
SP:8df1599919dcb3329553e75ffb19059f192542ea,"This paper proposes a method to tackle the problem of catastrophic forgetting in continual learning. The proposed method learns to build a model, called the solver, with two sets of parameters. The first set is shared by all tasks learned so far and the second set is dynamically generated to adapt the model to suit each test example in order to classify it. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed approach."
SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"This paper introduces Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents’ future behavior in multi agent environments. The authors show that RFM can capture the rich social dynamics of multi agent systems, that its intermediate representations contain valuable interpretable information, and that providing this information to learning agents results in faster learning system. The analysis tools introduced allow researchers to answer new questions, such as what entities, relations and social interactions drive agents' behaviors and what environment events or behavior patterns mediate these social and non-social influence signals."
SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,This paper proposes a meta-IRL approach for inverse reinforcement learning (IRL) that learns to encode common structure across tasks. The idea is to learn a “prior” that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. The authors demonstrate that their method can efficiently recover rewards from images for novel tasks and provide intuition as to how our method is analogous to learning a prior in this work.
SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,"This paper proposes a meta-learning framework called ML-PIP, which extends existing probabilistic interpretations of Meta-Learning approximate Probabilistic Inference for Prediction (PIP) to cover a broad class of methods. The proposed method, called VERSA, is an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. The method is evaluated on benchmark datasets where the method sets new state-of-the-art results."
SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper presents a novel deep learning architecture for classifying structured objects in ultrafine-grained datasets, where classes may not be clearly distinguishable by their appearance but rather by their context. The authors model sequences of images as linear-chain CRFs, and jointly learn the parameters from both local visual features and neighboring class information. The visual features are learned by convolutional layers, whereas class-structure information is reparametrized by factorizing the CRF pairwise potential matrix. This form a context-based semantic similarity space, learned alongside the visual similarities, and dramatically increases the learning capacity of contextual information."
SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a generative adversarial network (GAN) for generating high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. By carefully controlling the audio representation used for generative modeling, the authors have demonstrated high-quality audio generation with GANs on the NSynth dataset, exceeding the fidelity of a strong WaveNet baseline while generating samples tens of thousands of times faster. The paper is well-written and easy to follow."
SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper studies the problem of verifying the robustness of neural networks to adversarial perturbations. The authors formulate verification of piecewise-linear neural networks as a mixed integer program, and propose a method that is two to three orders of magnitude faster than the state-of-the-art. They achieve this speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows them to verify properties on convolutional and residual networks with over 100,000 ReLUs — several orders of magnitudes more than networks previously verified by any complete verifier. In particular, they determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbation with bounded l_\infty norm = 0.1: for this classifier, they find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded robustness for the remainder."
SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper studies the effect of learning the default policy in the KL-regularized RL objective. Specifically, the authors propose to enforce information asymmetry between the default and the main policy in order to force the default to learn task-agnostic behaviour. The authors show that this approach improves the performance of the RL algorithms in both discrete and continuous action domains. The main contribution of this paper is the introduction of the idea of enforcing information asymmetries between the main and the default policies. "
SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,This paper presents a new approach for conversational machine comprehension. The authors propose to use an alternating parallel processing structure to incorporate intermediate representations generated during the process of answering previous questions. The proposed approach is evaluated on two recently proposed conversational challenge datasets and three domains of a sequential instruction understanding task (through reduction) and outperforms existing models. 
SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,This paper tackles the problem of generating generative models of source code by modeling the edits that software developers make to source code files. The authors develop several neural networks and use synthetic data to test their ability to learn challenging edit patterns that require strong generalization. They then collect and train their models on a large-scale dataset consisting of millions of fine-grained edits from thousands of Python developers. The main conclusion is that a new composition of attentional and pointer networks provides the best overall performance and scalability.
SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning approach for multi-class classification, where the goal is to learn a binary classifier for pairwise similarity prediction and then use it as a sub-classifier for learning multi-classes. The authors formulate the problem setting via a probabilistic graphical model and derive a simple likelihood objective that can be effectively optimized via neural networks. They show how this same framework can be used for three learning paradigms: supervised learning, unsupervised cross-task transfer learning, and semi-supervised learning. Results show comparable or improved results over state-of-the-art."
SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper studies the question of how visual question answering (VQA) models learn the correct interpretation of quantifier statements in the context of a visual scene. Specifically, the authors propose two strategies of algorithmically interpreting “most” in a visual context, with different implications on cognitive concepts. They design experiments and data to shed light on the question whether the state-of-the-art FiLM VQA model shows preference for one strategy over the other. Performance on various specifically designed instances indicates that a form of approximate number system is learned, which generalizes to more difficult scenes as predicted by Weber’s law. "
SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a probabilistic extension of DistMult and ComplEx for link prediction in relational knowledge graphs. The authors argue that knowledge graphs should be treated within a Bayesian framework because even large knowledge graphs typically contain only few facts per entity, leading effectively to a small data problem where parameter uncertainty matters. The main benefit of the proposed approach is that it allows for efficient, gradient based optimization over hyperparameters, which would lead to divergences in a non-Bayesian treatment. The proposed approach outperforms the state-of-the-art by a significant margin."
SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,This paper proposes a new algorithm for online dimension reduction. The proposed algorithm builds on top of sliced inverse regression (SIR) and makes it implementable in an incremental manner. The authors also refine the algorithm by using an overlapping technique and develop an incremental overlapping sliced in regression (IOSIR) algorithm. The effectiveness and efficiency of both algorithms are verified by simulations and real data applications.
SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,This paper proposes a generative-discriminative model for learning multimodal representations. The proposed model is based on the idea of factorizing multimodality into two sets of independent factors: multimodale discriminative factors and modality-specific generative factors. The multimo-modal discriminatively factors are shared across all modalities and contain joint multimodals features required for the task of sentiment prediction. The modality specific factors are unique for each modality and contain the information required for generating data. Experimental results show that the proposed model achieves state-of-the-art or competitive performance on six multimodel datasets.
SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper proposes a meta-learning approach for adaptive text-to-speech (TTS) with few data. The authors introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the WaveNet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting speaker embeddings with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio from new speakers."
SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper provides an interesting connection between f-GANs and various depth functions through the lens of f-learning. Similar to the derivation of f GANs, the depth functions that lead to statistically optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f -Learning. This connection opens the door of computing robust estimation methods based on tools developed for training fGANs. In particular, it shows that some appropriate structures of discriminator networks with hidden layers in GAN leads to statistical optimal robust location estimators for both Gaussian distribution and general elliptical distributions where first moment may not exist."
SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper proposes a method to represent a scene via a symbolic program for its objects, attributes, and their relations. The authors also propose a model that infers such scene programs by exploiting a hierarchical, object-based scene representation. Experiments demonstrate that the proposed method works well on synthetic data and transfers to real images with such compositional structure."
SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a timing-gated LSTM RNN model, called the g-LSTM, for reducing state updates. The time gate controls when a neuron can be updated during training, enabling longer memory persistence and better error-gradient flow. This model captures long temporal dependencies better than an LSTMs on very long sequence tasks and the time gate parameters can be learned even from a non-optimal initialization. The authors also propose a temporal curriculum learning schedule for the model that helps speed up the convergence time of the equivalent L STM on long sequences."
SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a simple and efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model. The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute. Notably, the method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out of distribution dataset is TinyImageNet resize."
SP:827f95cdefae78e38a9c4b5718fcf294606a1989,"This paper considers the problem of recovering the weights of a one-hidden-layer fully-connected neural network with sigmoid activations. The authors show that the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the true ground truth. "
SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolution layers and preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. Experiments show that FBS can respectively provide 5x and 2x savings in compute on VGG-16 and ResNet-18."
SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the training objective of GANs from the mixed Nash equilibrium perspective. The authors propose a proximal method for GAN training based on the infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE. They then propose a procedure to reduce their novel prox methods to simple sampling routines, leading to practically efficient algorithms. Finally, they provide experimental evidence that their approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality."
SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,"This paper proposes a method for parameter-efficient transfer and multitask learning with deep neural networks. The basic approach is to learn a model patch a small set of parameters that will specialize to each task, instead of fine-tuning the last layer or the entire network. The approach allows both simultaneous (multi-task) as well as sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-only fine tuning, the proposed method can match singletask performance."
SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes an extension of batch normalization (BN) to allow the network to jointly normalize its features within multiple modes. In particular, the authors propose a method to detect modes of data on-the-fly, jointly normalizing samples that share common features. They demonstrate that their method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets."
SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,"This paper investigates the lottery ticket hypothesis, which is the hypothesis that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that can reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets are found to be sparse pruned networks that are not optimized for modern libraries or hardware. The authors claim that the winning tickets have won the lottery: their connections have initial weights that make them training particularly effective."
SP:08c662296c7cf346f027e462d29184275fd6a102,This paper proposes a method to learn an attentive dynamics model (ADM) to predict the actions of an RL agent. The ADM is trained in a self-supervised fashion and is used as a part of the state representation for exploration purposes. The method is evaluated on a set of challenging Atari games and achieves state-of-the-art results.
SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,"This paper proposes HyperGAN, a generative adversarial network that learns to generate all the weight parameters of deep neural networks. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. The authors apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. This is evaluated by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples."
SP:230b3e008e687e03a8b914084b93fc81609051c0,"This paper proposes a method to train VAEs with binary or categorically valued latent representations. The authors use a differentiable estimator for the ELBO which is based on importance sampling. Theoretical analysis shows that the variance of the estimator approaches zero near the optimal parameter configuration, which is a desirable property for training."
SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes a method to improve the adversarial robustness of generative models against adversarial attacks. The proposed method is based on a pre-trained building block based on the mean field description of a Boltzmann machine. The method is evaluated on the MNIST dataset without data augmentation or adversarial training. The authors show that the method achieves strong adversarial resistance without data augmentations.
SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,This paper studies the relationship between human and deep neural networks (DNNs) when the minimal region of an image is modified in such a way that the human recognition performance is affected. The authors show that both humans and DNNs are affected by slight changes of the visible region of the minimal image. They also show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in the object location in the minimal images. The results thus reveal a new failure mode of DNN that also affects humans to a much lesser degree and expose how fragile DNN recognition is for natural images even without adversarial patterns being introduced.
SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper proposes a multi-agent reinforcement learning approach to solve the collaboration problems among self-interested workers with different skills and preferences. They train a super agent (i.e., the manager) to simultaneously infer workers’ minds and optimally assign contracts to workers for maximizing the overall productivity. They combine imitation learning and reinforcement learning for a joint training of agent modeling and management policy optimization. They also improve the model performance by learning high-level successor representation, agent-wise greedy exploration, and agent identification based on performance history."
SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a new recurrent neural network (RNN) architecture for learning representations of time series. The authors propose a unified RNN that handles five different feature types, each in a different manner. The sequential features are split into two groups dependent on their frequency, sparse and dense features, which affect cell updates differently, and also incorporate time features at the sequential level that relate to the time between specified events in the sequence and are used to modify the cell’s memory state. Two types of static (whole sequence level) features, one related to time and one not, are combined with the encoder output. The experiments show that the proposed modeling framework does increase performance compared to standard cells."
SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper presents a top-down approach to represent formulae by neural networks. The main idea is to use a feedforward neural network recursively built for the given formula to capture the structure of the formula in a topdown manner. The results of this network are then processed by two recurrent neural networks (RNNs). One of the interesting aspects of the proposed model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct."
SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper studies the problem of curriculum learning for training neural networks. In particular, the authors propose a method to sample mini-batches with gradually increasing levels of difficulty. The method is evaluated on CIFAR-10/100 datasets. The authors also propose a bootstrap method to evaluate the difficulty of points using the same network without relying on a “teacher” network."
SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper proposes a general PAC-Bayesian framework to study the generalization properties of deterministic and uncompressed neural networks. In particular, the authors show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, then these conditions generalize to the interaction between the matrices on test data, thereby implying a wide test loss minimum. The authors then apply their general framework in a setup where the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, they provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weights matrices."
SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper proposes an alternate training technique for vector quantized autoencoders (VQ-VAE) inspired by the Expectation Maximization (EM) algorithm. In particular, the paper proposes to train the discrete autoencoder with EM and then combine it with sequence level knowledge distillation to improve the performance of the model. Experiments on CIFAR-10 demonstrate the effectiveness of the proposed approach."
SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes a multi-view learning framework for learning sentence representations in an unsupervised fashion. The proposed framework combines an RNN-based encoder and an average-on-word-vectors linear encoder. The authors show that, after learning, the vectors produced by the multi-views frameworks provide improved representations over their single-view learnt counterparts. The combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks."
SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,"This paper proposes an online distributed optimization method called Anytime Minibatch to mitigate the impact of stragglers. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. The paper also provides a convergence analysis and analyze the wall time performance."
SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,This paper proposes a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. The authors train a neural architecture to predict a driver’s peripheral blood flow modulation based on the first-person video from the vehicle. This architecture acts as the reward in the reinforcement learning step. Experiments show that it can increase the speed of learning and reduce the number of collisions during the learning stage.
SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper proposes an extension of Neural Processes (NPs) to the regression setting, where the goal is to learn a predictive distribution conditioned on a context set of observed input-output pairs to a distribution over regression functions, conditioned on the context. The authors show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. They address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. They show that this improves the accuracy of predictions, results in noticeably faster training and expands the range of functions that can be modelled."
SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,"This paper provides a theoretical analysis of credit assignment in gradient-based meta-reinforcement learning (Meta-RL) and proposes a proximal meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. The proposed algorithm, ProMP, optimizes for the pre-update sampling distribution leading to effective task identification. Experimental results demonstrate that ProMP outperforms previous Meta-RL algorithms in a diverse set of continuous control tasks."
SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes NeuroSAT, a neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. The main contribution of this paper is that the network is able to solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, the network generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT tasks such as graph coloring, clique detection, dominating set, and vertex cover problems."
SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. The authors propose exposing the learner to synthesized data in the form of perturbations to the expert’s driving, which creates interesting situations such as collisions and/or going off the road. They augment the imitation loss with additional losses that penalize undesirable events and encourage progress. They show that the model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of their proposed changes."
SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a method to select a subset of training data to achieve faster training of deep learning models with no loss in predictive performance. The proposed method first trains a small proxy model to estimate the utility of individual training data points, and then select the most informative ones for training the large target model. Extensive experiments show that the proposed method leads to a 1.6x and 1.8x speed-up on CIFAR10 and SVHN by selecting 60% and 50% subsets of the data."
SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"This paper proposes Sequential Monte Carlo Planning (SMCP), an approach to planning that views planning as a probabilistic inference problem over future optimal trajectories. The authors propose a particle-based sampling method to approximate the posterior of the planning process. They also propose a way to combine model-free and model-based reinforcement learning for planning based on the SMC perspective. They empirically demonstrate that their method achieves state-of-the-art results on Mujoco."
SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,"This paper shows that adversarial training is sensitive to the input data distribution, unlike clean accuracy. Even a semantics-preserving transformations on the data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. The paper provides theoretical analyses to show the significance of input data distributions in adversarial robustness, which further motivated the systematic experiments on MNIST and CIFAR10 variants. This paper also discusses the practical implications of the existence of such sensitivity questions the reliability in evaluating robust learning algorithms on particular datasets."
SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper proposes a new normalization technique for batch normalization, which is based on a transformation of layer weights instead of layer outputs. The proposed technique keeps the contribution of positive and negative weights to the layer output in equilibrium. The authors validate their method on a set of standard benchmarks including CIFAR-10/100, SVHN, and ILSVRC 2012 ImageNet."
SP:8188f15c8521099305aa8664e05f102ee6cea402,This paper proposes a method to identify mislabeled examples on the fly and discard them during training. The method is based on the implicit regularization effect of stochastic gradient descent with large learning rates. The proposed method is evaluated on both synthetic and real-world datasets.
SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a theoretical analysis framework to link pretraining and downstream tasks with an underlying latent variable generative model of text — the downstream classifier must recover a function of the posterior distribution over the latent variables. The authors analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. They show that under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task. They also show that prompt tuning obtains downstream guarantees with weaker conditions. Experiments on synthetically generated data from HMMs back their theoretical findings."
SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of out-of-distribution generalization (OoD). The authors propose a new notion of transferability, which they define as the difference and connection with common discrepancy measures between domains, such as total variation and Wasserstein distance. They then prove that our transferability can be estimated with enough samples and give a new upper bound for the target error based on the transferability. They also propose an algorithm for learning transferable features and test it over various benchmark datasets, including RotatedMNIST, PACS, Office-Home and WILDS-FMoW."
SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the expressivity of reward as a way to capture tasks that we would want an agent to perform. The authors frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a part ordering over trajectories. They show that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. They then provide a polynomial-time algorithms that construct a reward function that can capture these tasks. They conclude with an empirical study that corroborates and illustrates their theoretical findings."
SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"This paper studies the problem of generalization in reinforcement learning (RL) from the point of view of epistemic uncertainty. The authors show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into epistemic POMDPs. Based on this observation, the authors recast the RL problem as solving the induced partially observed Markov decision process (POMDP) and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, the proposed algorithm achieves significant gains in generalization over current methods on the Procgen benchmark suite."
SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper provides a unified framework for estimating higher-order derivatives of value functions, based on off-policy evaluation. The framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. This framework also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice."
SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper studies the problem of federated learning, i.e. distributed learning with a central server and local workers. The authors propose a new algorithm that performs bidirectional compression and achieves the same convergence rate as algorithms using only uplink (from the local workers to the central server) compression. The main contribution of this paper is to design an algorithm such that the downlink compression only impacts local models, while the global model is preserved. In addition, the authors also propose the concept of randomization which allows to reduce the variance associated with the uplink compression."
SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper introduces counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. The authors also connect it to out-of-domain model performance, and provide practical schemes for learning (approximately) counterfactually invariant predictors. It turns out that both the means and implications depend fundamentally on the true underlying underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label."
SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes an adaptive pseudo-augmentation method for training GANs with limited training data. The authors claim that the main cause of the poor performance of GAN training is the overfitting of the discriminator. They propose to use the generator to augment the real data distribution with generated images, which deceives the discriminators adaptively. They provide theoretical analysis to show the convergence and rationality of their new training strategy."
SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a framework for causal inference between pairs of event variables in multivariate recurrent event streams by extending Rubin’s framework for the average treatment effect (ATE) and propensity scores to multivariate point processes. Theoretically, the authors theoretically justify the point process causal framework and show how to obtain unbiased estimates of the proposed measure. They conduct an experimental investigation using synthetic and real-world event datasets, where their proposed causal inference framework is shown to exhibit superior performance against a set of baseline pairwise causal association scores."
SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper investigates the relationship between feature aggregation and residual connections in the message passing scheme of GNNs. The authors propose a simple, efficient, interpretable, and adaptive message-passing scheme, AirGNN, which is a GNN with Adaptive Residual Connections (ARC). The authors show that the residual connections amplify the vulnerability of the GNN to abnormal node features. They further propose an adaptive transition mechanism, which can be seen as a graph Laplacian smoothing approach. Experiments are conducted to demonstrate the effectiveness of the proposed method."
SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper considers online sequential decision problems where an agent must balance exploration and exploitation. The authors derive a set of Bayesian ‘optimistic’ policies which, in the stochastic multi-armed bandit case, includes the Thompson sampling policy. They provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $\tilde{O}(\sqrt{AT})$ Bayesian regret for a problem with A actions after T rounds. They extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case they show that Thompson sampling can produce policies outside of the optimistic sets and suffer linear regret in some instances."
SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the convergence analysis and rates of variance reduction under without-replacement sampling orders for composite finite-sum minimization. The authors develop a damped variant of Finito called Prox-DFinito and establish its convergence rates with random reshuffling, cyclic sampling, and shuffling-once, under both convex and strongly convex scenarios. These rates match full-batch gradient descent and are state-of-the-art compared to the existing results for without replacement sampling with variance-reduction. In the highly data-heterogeneous scenario, the authors also propose a practical method to discover the optimal cyclic ordering numerically."
SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,This paper provides convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the relative entropy policy search (REPS) objective in reinforcement learning. The authors consider the setting in which they are given access to exact gradients and demonstrate how near-optimalality of the objective translates to near-optimability of the policy. They then introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal policy.
SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method to evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. The authors propose to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. They also propose metrics to evaluate spatial smoothness and representation complexity of the DNN. The experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training."
SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,This paper proposes an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. They introduce a new metric to evaluate an auction allocations’ adherence to such socially desirable constraints and demonstrate that their proposed method is competitive with current state-of-the-art neural network based auction designs. They validate their approach through human subject research and show that they are able to effectively capture real human preferences.
SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"This paper studies the problem of personalization of supervised learning with user-level differential privacy. In this setting, each of the users has a training data set drawn from their own distribution Pi, and the goal is to learn a shared linear representation of the data. The paper provides algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user level privacy guarantees for their general approach. The error bounds and sample complexity bounds are nearly optimal in key parameters and are in fact, comparable to the best known bounds available for a much simpler linear regression problem."
SP:3925fc528de17b8b2e93808f5440ea0503895b75,"This paper introduces Adversarial VQA (Adversarial Question Answering), a new benchmark for evaluating state-of-the-art visual question answering (VQA) models against human-adversarial questions and answers. Human subjects interact with a model and for each image in the dataset, attempt to find a question where the model’s predicted answer is incorrect. The authors conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. They find that a wide range of models perform poorly on these examples."
SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper investigates the role of heterogeneous cell types in the neural activity of the medial entorhinal cortex (MEC). The authors first show that heterogeneous MEC cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role. Then, they evaluate a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous cells. They find that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles, including those of grid cells themselves. Finally, they introduce a new MEC model that performs reward-modulated path integration, which matches neural recordings across all variable-reward conditions."
SP:57f9812fa5e7d0c66d412beb035301684d760746,"This paper studies the problem of pathological training dynamics in KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations. The authors identify a previously unrecognized pathology in the training dynamics of the RL algorithm, which they call “pathological training dynamics”. They show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. Finally, they propose a non-parametric behavioral reference policy to remedy the pathology and show that it can significantly accelerate and improve online learning and yield online policies that outperform current state-of-the-art methods on challenging continuous control tasks."
SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the learning curve of kernel regression with neural tangent kernels (NTK) in the teacher-student setting, where the teacher has a smaller filter size than the student and the student has a larger filter size. The authors show that in the ridgeless case, the local and translational invariance of the data are important factors in determining the exponent of the regression error, and that if the filter size of the teacher is smaller than that of the student, then the exponent is a function of s only and does not depend on the input dimension. They also show that performing kernel regression on a ridge that decreases with the size of training set leads to similar learning curve exponents to those obtained in the ridge-free case."
SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a variational autoencoder (VAE) with a deterministic prior (GMM) that is trained end-to-end. The main idea is to use the latent space of the model during training and utilize the capacity of expressive multi-modal latent distributions. The proposed training procedure provides evidence if the latent distribution adequately captures complex aspects of the encoded data. The experiments show the expressiveness and sample quality of the proposed method.
SP:6232d8738592c9728feddec4462e61903a17d131,"This paper proposes a method for self-supervised adversarial detection based on disentanglement of class and semantic features. Specifically, the authors train an autoencoder, assisted by a discriminator network, over both correctly paired class/semantic features and in correctly paired classes/semantics features to reconstruct benign and adversarial examples. The proposed method is evaluated on CIFAR-10 and ImageNet under various adversarial attacks."
SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,"This paper proposes a method to model the brain's representation of syntactic information in order to identify syntactic processing areas that are correlated with semantic processing load. In particular, the authors propose a set of features that encode information about the syntactic structure of sentences. These features and fMRI recordings of participants reading a natural text are used to model brain representations of syntax. The authors find that syntactic features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load, and show that regions well-predicted by syntactic feature are distributed in the language systems and are not distinguishable from those processing semantics."
SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation based on energy based models (EBMs). The authors introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. They propose a novel EBM formulation representing the joint distribution of data and attributes together, and they show how sampling from it is formulated as solving an ordinary differential equation (ODE). The method is simple, fast to train, and efficient to sample. Experimental results show that the proposed method outperforms the state-of-the-art in both conditional sampling and sequential editing."
SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper proposes a federated linear contextual bandits model, where each client faces different K-armed stochastic bandits coupled through common global parameters. By leveraging the geometric structure of the linear rewards, a collaborative algorithm called Fed-PE is proposed to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. The proposed algorithm relies on a novel multi-client G-optimal design, and achieves near optimal regrets for both disjoint and shared parameter cases with logarithmic communication costs. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets."
SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a new offline model-based RL algorithm, called COMBO, which trains a value function using both the offline dataset and data generated using rollouts under the model, and additionally regularizes the value function on out-of-support state-action tuples generated via model rollouts. Theoretically, the authors show that COMBO achieves less conservative Q values compared to prior model-free offline RL methods and guarantees a safe policy improvement. Empirically, COMBO attains greater performance than prior offline RL on problems that demand generalization to related but previously unseen tasks, and consistently matches or outperforms prior offlineRL methods on widely studied offline RL benchmarks."
SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the role of local elasticity in the training dynamics of deep neural networks (DNNs) during training. Specifically, the authors model the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample and each SDE contains a drift term that reflects the impact of backpropagation at an input on the features of all samples. Their main finding uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, then the features become linearly separable, meaning vanishing training loss. Otherwise, the features are not separable regardless of how long the training time is. Moreover, the emergence of a simple geometric structure called the neural collapse of the features is shown."
SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a framework that learns to synthesize a programmatic policy, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, the authors propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embeddings space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies."
SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper studies the limitations of physics-informed neural networks (PINNs), a popular approach to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. The authors show that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. They also show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN’s setup makes the loss landscape very hard to optimize. They then describe two promising solutions to address these failure modes."
SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, CST generates target pseudo-label with a source-trained classifier, and then updates the shared representations to make the target classifier perform well on the source data. The authors also introduce the Tsallis entropy as a confidence-friendly regularization to improve the quality of target pseud-label. They analyze CST theoretically under realistic assumptions, and provide hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self training fail."
SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured pruning method termed Discriminative Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. They show that the proposed approach has remarkably good performance over a diverse range of applications in representation learning and structured network pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured prune for image classification. They also theoretically show the learning objective is directly related to minimizing the L0 norm of the masking layer."
SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a self-attention-based architecture, called Neural Interpreters, that is capable of generalizing to data drawn from unseen but related distributions. The main idea is to decompose inference into a system of modules, which the authors call functions, and the input to the model is routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. Experiments are conducted on two tasks: image classification and visual abstract reasoning on Raven Progressive Matrices."
SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes a technique called Behavior Transfer (BT) that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. The authors show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation can lead to the emergence of complex behaviors, which can then be leveraged by BT to discover better solutions than without pretraining, and combining BT with standard fine-tuning strategies results in additional benefits. The largest gains are generally observed in domains requiring structured exploration."
SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes PiRank, a new class of surrogates for ranking, which employ a continuous, temperature-controlled relaxation to the sorting operator based on NeuralSort. The authors show that PiRank exactly recovers the desired metrics in the limit of zero temperature and further propose a divide-and-conquer extension that scales favorably to large list sizes, both in theory and practice. Empirically, the authors demonstrate the role of larger list sizes during training and show that piRank significantly improves over comparable approaches on publicly available-scale learning-to-learn datasets."
SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,This paper proposes a reinforcement learning algorithm for variational quantum entanglement solvers (VQEs). The main idea is to use reinforcement learning (RL) to explore the space of possible economic circuits for VQEs. The proposed algorithm uses a feedback-driven curriculum learning method that autonomously adapts the complexity of the learning problem to the current performance of the RL algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. The authors demonstrate the performance of their algorithm on the problem of estimating the ground-state energy of LiH.
SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the effect of arbitrary class distributions within the query sets of few-shot tasks at inference, removing the class-balance artefact. Specifically, the marginal probabilities of the classes are modeled as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex. The authors also propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations. Empirically, the authors show that the proposed method outperforms state-of-the-art methods across several data sets, models and settings."
SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. The authors also propose receptive field redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. The proposed patch-based inference effectively reduces the memory usage of existing networks by 4-8x. "
SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of automated mechanism design in unstructured dynamic environments, where a principal repeatedly interacts with an agent and takes actions based on the strategic agent's report of the current state of the world. The goal is to compute an optimal mechanism which maximizes the principal’s utility in the face of the self-interested strategic agent. The authors give an efficient algorithm for computing optimal mechanisms, with or without payments, under different individual-rationality constraints, when the time horizon is constant. The algorithm is based on a sophisticated linear program formulation, which can be customized in various ways to accommodate richer constraints. For environments with large time horizons, the authors show that the optimal mechanism is hard to approximate within a certain constant factor, complementing the algorithmic result."
SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,"This paper investigates the question of how Neural Architecture Search (NAS) is able to select the desired GNN architectures. The authors conduct theoretical analysis and measurement study with experiments to discover that gradient based NAS methods tend to select proper architectures based on the usefulness of different types of information with respect to the target task. Based on their findings, the authors propose a Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and is able  to discover graph neural architectures with better performance through employing graph structure learning as a denoising process in the search procedure. Extensive experiments on real-world graph datasets demonstrate that the proposed GASSO model is capable of achieving state-of-the-art performance compared with existing baselines."
SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper considers the problem of fair clustering, i.e. the clustering problem where a dataset is partitioned into clusters that consist of nearby points in a metric space. The authors consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective. They derive lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. They further derive impossibility results for other natural fairness objectives. "
SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies the limitations of edge independent random graph models, in which each edge is added to the graph independently with some probability. Such models include both the classic Erdös-Rényi and stochastic block models, as well as modern generative models such as NetGAN, variational graph autoencoders, and CELL. They prove that subject to a bounded overlap condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. Notably, such high densities are known to appear in real-world social networks and other graphs."
SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the effect of the choice of ReLU(0) in [0,1] for deep neural networks on the accuracy of backpropagation and training. The authors compare the performance of neural networks trained with and without the ReLU activation function at various precision levels (16, 32, 64 bits) and on various networks and datasets (MNIST, CIFAR-10, SVHN, and ImageNet). They observe considerable variations of back-propagated outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU($0) = 0 seems to be the most efficient."
SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method (RPC) for learning simple policies. This method brings together ideas from information bottlenecks, model-based RL, and bits-back coding into a simple and theoretically-justified algorithm. The method jointly optimizes a latent-space model and policy to be self-consistent, such that the policy avoids states where the model is inaccurate. The authors demonstrate that their method achieves much tighter compression than prior methods, yielding up to 5x higher reward than a standard information bottleneck."
SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"This paper proposes a Spectral Attention Network (SAN) for graph neural networks (GNN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplace spectrum, the proposed model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. The model was shown to perform on par or better than the SOTA on multiple benchmarks and outperforms other Attention-based models."
SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper considers the problem of voting in a two-alternative election, where the voters' preferences depend on a state variable that is not directly observable. Each voter receives a private signal that is correlated to the state variable. The authors present an easy-to-deploy mechanism that elicits and aggregates the private signals from the voters and outputs the alternative that is favored by the majority. In particular, voters truthfully reporting their signals forms a strong Bayes Nash equilibrium (where no coalition of voters can deviate and receive a better outcome)."
SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper provides exact formulas and tight upper bounds for the Hessian rank of deep linear networks. The authors show that the rank is proportional to the width of the layers. The analysis also reveals several striking properties of the Hessians, such as surprisingly small overlap in the column spaces of Hessian Ho and Hf, and independence of the layer-wise column blocks in Hf."
SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new disentanglement metric, called DLSBD, which quantifies the distance between the representations of a linear disentangled representation (LSBD) and the representation of the true representation. The authors also propose a new method, LSBD-VAE, which is a VAE-based representation learning method that is semi-supervised in the sense that it is able to learn the representations in a way that does not require explicit supervision. The paper also shows that the proposed metric can be used to evaluate the performance of existing disentangling methods, and that the representation learned by the proposed method is better than the representations learned by existing methods."
SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes a constrained optimisation framework for training deep state-space models (DSSMs) to learn the underlying dynamics of observed sequence data. The authors propose a general Lagrangian formulation of the sequential ELBO on the basis of distortion and rate, and extend the empirical Bayes prior (VHP) and the associated optimisation algorithm introduced in the context of VAEs to DSSMs. Building upon the CO framework, the authors introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSM. The proposed method outperforms previous models w.r.t. prediction accuracy and achieves remarkable results in identifying dynamical systems."
SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The method is based on the idea of deep inversion, i.e. synthesizing images from the training distribution to generate images with desired changes in the model prediction. The authors show that existing methods are insufficient for producing meaningful explanations. They propose to improve upon the existing methods by using stronger image priors, incorporating a novel manifold consistency objective, and adopting a progressive optimization strategy. They also show that the generated explanations are effective at learning classifier decision boundaries and robust to unknown test-time corruptions."
SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes a causal inference algorithm for identifying regions of decision-maker disagreement in the decision-making process. The algorithm is based on the idea of causal inference, i.e. finding the region where the assignment of the two decision-makers has a large causal effect on the decision. The paper provides a generalization bound for the performance of the algorithm and shows that it recovers the correct region of disagreement accurately compared to baselines. Finally, the algorithm is applied to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge. "
SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based generative model for the task of image synthesis. The key idea is to treat the task as a visual token generation problem, i.e., it takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. Specifically, the TokenGAN inputs two semantically different visual tokens: the learned constant content tokens and the style tokens from the latent space. The tokenGAN is able to control the image synthesis by assigning the styles to the content tokens by attention mechanism with a Transformer. The experimental results show that the proposed TokenGAN has achieved state-of-the-art results on FFHQ and LSUN CHURCH with different resolutions."
SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the robust overfitting of overparameterized linear models. In particular, the authors show that the robust risk of linear models can overfit with respect to the standard risk, even when there is no noise in the training data. They also show that avoiding ridge regularization through avoiding interpolation can significantly improve generalization. The authors also provide a theoretical result that shows the double descent phenomenon for both linear regression and classification. "
SP:09f080f47db81b513af26add851822c5c32bb94e,"This paper proposes a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. CPAE encodes an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and decodes the primitive back to the original input instance shape. The primitive plays a key role to map all the unordered point clouds on the canonical surface and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that the model performs favorably against state-of-the-art correspondence learning methods."
SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper proposes a simple yet effective method, named Stochastic Weight Averaging Densely (SWAD), to find flatter minima and suffers less from overfitting than does the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. The authors theoretically and empirically demonstrate that seeking flat minima can achieve better generalizability to both in-domain and out-of-domain tasks. The extensive experiments on five DG benchmarks show superior performances of SWAD compared with existing DG methods."
SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"This paper provides a large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. They test a number of correlation and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. Their results act as recommendations for the best predictors to use in different settings, and they show that certain families of predictors can be combined to achieve even better predictive power."
SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the inherent privacy of releasing a single sample from a Dirichlet posterior distribution. Specifically, the authors study the accuracy guarantees of the posterior sampling in two settings: (1) private normalized histogram publishing and (2) private truncated concentrated differential privacy (tCDP). The authors provide a simple privacy guarantee of the Dirichle posterior sampling. "
SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for computing random walks in the context of local clustering. In particular, the authors propose an algorithm that builds random walks efficiently and locally at the same time. The proposed algorithm is both memory and round efficient, and yields an efficient parallel clustering algorithm. Experimental results show that the proposed method is significantly more scalable than previous approaches."
SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper provides a unified statistical mechanics framework for the analysis of typical learning performances of `1-regularized linear regression (`1-LinR) for Ising model selection on typical paramagnetic RR graphs. The high-dimensional estimator is decoupled into a pair of scalar estimators, by which the authors obtain an accurate estimate of the typical sample complexity. It is revealed that, perhaps surprisingly, the misspecified $\ell_1_LinR$ is model selection consistent using M = O (logN) samples, which is of the same order of sample complexity as the logistic regression (LogR) estimator. Moreover, with a slight modification, the authors further obtain sharp predictions of the non-asymptotic behavior of $\ell_{1}_1$ for moderate M,N."
SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the fuzzy k-means problem, which extends the clustering capability of the well-known kmeans clustering problem to datasets that are uncertain, vague and hard to cluster. The authors propose a semisupervised active clustering framework, where the learner is allowed to interact with an oracle (domain expert), asking for the similarity between a certain set of chosen items. They prove that having a few of such similarity queries enables one to get a polynomial-time approximation algorithm to an otherwise conjecturally NP-hard problem. In particular, they provide algorithms for fuzzy clustering in this setting that ask O(poly(k) log n) similarity queries and run with polynomially-complexity, where n is the number of items."
SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a method to improve model-based reinforcement learning by encouraging the learned model and value function to be jointly self-consistent. The authors propose multiple methods to achieve this goal, and evaluate these in both tabular and function approximation settings. They find that, with appropriate choices, the proposed method can improve the performance of both policy evaluation and control."
SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"This paper studies the sampling strategy for few-shot episodic training. The authors propose a method to approximate the episode sampling distributions based on the difficulty of the episodes. They also propose an importance sampling-based method to compare different episode sampling schemes. Their experiments suggest that sampling uniformly over episode difficulty performs best across datasets, training algorithms, network architectures, and few shot protocols."
SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of logistic bandits, where the number of outcomes that can be selected by the user is larger than two (e.g., ‘click’ vs ‘no click’). The authors use multinomial logit (MNL) to model the probability of each one of K+1 2 possible outcomes (+1 stands for the “not click” outcome), and assume that for a learner’s action, the user selects one of the outcomes, say outcome i, with a MNL probabilistic model with corresponding unknown parameter. Each outcome i is also associated with a revenue parameter ⇢i and the goal is to maximize the expected revenue. For this problem, the authors present MNL-UCB, an upper confidence bound (UCB)-based algorithm, that achieves regret O(dK p T ) with small dependency on problem-dependent constants that can otherwise be arbitrarily large and lead to loose regret bounds. Numerical simulations corroborate the theoretical results."
SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a max-min entropy framework for reinforcement learning (RL) to overcome the limitation of the soft actor-critic (SAC) algorithm implementing the maximum entropy RL in model-free sample-based learning. The proposed MME framework aims to learn to visit states with low entropy and maximize the entropy of these low-entropy states to promote better exploration. For general Markov decision processes (MDPs), an efficient algorithm is constructed under the proposed max-max entropy framework. Numerical results show that the proposed algorithm yields drastic performance improvement over the current state-of-the-art RL algorithms."
SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper studies the problem of learning the time evolution of discrete sets of items (e.g., genetic mutations) in the setting of continuous-time Markov chains, and shows that the resulting learning task is generally underspecified in the usual setting of cross-sectional data. The authors then propose an approximate likelihood maximization method that can scale to hundreds of items and is orders of magnitude faster than previous methods. They demonstrate the effectiveness of their approach on synthetic and real-world cancer data."
SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper presents a unified pretraining framework for document understanding. It extends the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. It learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks."
SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of clustering problems with lp-norm objectives (e.g. k-median and k-means) in the context of individual fairness, where the goal is to find k centers such that (a) the objective is minimized, while (b) respecting the individual fairness constraint that every point v has a center within a distance at most r(v), where r is v’s distance to its (n/k)th nearest point. The main contribution is to use linear programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. Theoretically, the authors prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal. "
SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper studies the problem of graph partitioning and correlation clustering. The authors propose polynomial-time Gaussian sampling-based algorithms for these two problems that use O(n + |E|) memory and nearly achieve the best existing approximation guarantees. For dense graphs arriving in a stream, the authors eliminate the dependence on |E^2$ in the storage complexity at the cost of a slightly worse approximation ratio."
SP:cfd6cf88a823729c281059e179788248238a6ed7,"This paper proposes a motion-aware unit (MAU) to capture reliable inter-frame motion information by broadening the temporal receptive field of the predictive units. The MAU consists of two modules, the attention module and the fusion module. The attention module aims to learn an attention map based on the correlations between the current spatial state and the historical spatial states. The fusion module is utilized to further aggregate the augmented motion information (AMI) and current appearance information (current spatial state) to the final predicted frame. An information recalling scheme is employed into the encoders and decoders to help preserve the visual details of the predictions. Experimental results show that the MAU outperforms the state-of-the-art methods on both tasks."
SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies function approximation with two-layer neural networks, considering both ReLU and polynomial activation functions. The main result is a computationally and statistically efficient algorithm in the generative model setting under completeness. The second result considers this setting but under only realizability of the neural net function class. Under deterministic dynamics, the sample complexity scales linearly in the algebraic dimension."
SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for evaluating the generalization performance of deep metric learning (DML) models under out-of-distribution (OOD) distribution shifts. Specifically, the authors systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under unknown test distribution shifts in DML. Based on the new benchmark, they conduct a thorough empirical analysis of current DML methods and find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, they propose few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in ooOddML."
SP:bacff3685476855a32549d03095375649fd89df2,"This paper tackles the problem of unsupervised outlier model selection (UOMS) in the context of outlier detection. The authors propose a data-driven approach to UOMS based on meta-learning to automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. To capture task similarity, the authors introduce specialized metafeatures that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD significantly outperforms no model selection and other meta learning techniques."
SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for solving linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. This framework gives the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. The authors evaluate their method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning."
SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper introduces Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNN, the authors execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, they combine the results from these runs to obtain the final result. Theoretical results are also provided to show the expressive power of the proposed method."
SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Specifically, it proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, it also design a dual-stream FPN (D-FPN) to enhance contextual information for downstream dense predictions. The proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets."
SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a framework for dynamic visual reasoning that combines visual perception, concept learning, and differentiable physics. The visual perception module parses the video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object representations based on the language, thus providing prior knowledge for the physics engine. The physics engine is implemented as an impulse-based differentiable rigid-body simulator and performs differentiable physical simulation to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations."
SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. The proposed method is evaluated on CIFAR-10, MNIST, and ImageNet. The authors show that the proposed method outperforms existing active learning algorithms by as much as 5%-18% in the case of rare classes and 5%+10% for out-of-distribution data on several image classification tasks."
SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"This paper studies identity tests for ranking data that is generated from Mallows model both in the asymptotic and non-asymptotic settings. The authors consider the case when the central ranking is known, and devise two algorithms for testing the spread parameter of the Mallows models. The first one is obtained by constructing a Uniformly Most Powerful Unbiased (UMPU) test and then converting it into a sample-optimal identity test. The second one is derived from an optimal learning algorithm and is easy to compute and is sample optimal for a wide range of parameters."
SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a method for synthesizing a video of a human performance from sparse multi-view cameras. The proposed method is based on a parametric human body model for robust performance capture. Specifically, the authors propose a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multiview transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that the proposed method significantly outperforms recent generalizable NeRF methods on unseen identities and poses."
SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes to search the search space of vision transformers by gradually evolving different search dimensions guided by their E-T Error computed using a weight-sharing supernet. The authors also provide design guidelines of general vision transformer with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. The searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering."
SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this setting, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. The authors provide an algorithm that can efficiently produce an LTF that satisfies at least (2/5)-fraction of the bags. In addition, if all the bags are non-monochromatic (i.e., bags of size two with differently labeled features), the algorithm can also satisfy (1/2)-fractions of them. For the special case of OR over the d-dimensional boolean vectors, the authors also give an algorithm which can achieve an additional $\Omega(1/d)$ accuracy for the two cases."
SP:2eb193c76355aac08003c9b377895202fd3bd297,"This paper presents a method to combine singular value decomposition and noise modeling to create surrogate benchmarks for neural architecture search (NAS) that output the full training information for each architecture, rather than just the final validation accuracy. The authors also introduce a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single fidelity algorithms. The paper is well-written and easy to follow."
SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a method that provides example-based explanations with reference to a freely selected set of examples, called the corpus, to improve the user’s understanding of the latent space with post-hoc explanations answering two questions: (1) which corpus examples explain the prediction issued for a given test example; (2) what features of these corpus examples are relevant for the model to relate them to the test example? SimplEx provides an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, the authors propose a novel approach, the Integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Experiments on tasks ranging from mortality prediction to image classification demonstrate that these decompositions are robust and accurate."
SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based approach for vision-language pre-training (VLP). The authors claim that CNNs have limitations in visual relation learning due to local receptive field’s weakness in modeling long-range dependencies. To tackle this challenge, the authors propose a fully Transformer visual embedding for VLP to better learn visual relation and further promote inter-modal alignment. Specifically, they propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., Inter-modality). They also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the intermodality learning. They verify their method on a wide range of vision- language tasks, including Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment and Visual Reasoning."
SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the dynamics of the privacy loss of noisy gradient descent (GD) algorithms in the setting where the internal state of the algorithm is kept private. The main result is a tight privacy guarantee for Noisy GD on smooth and strongly convex loss functions. The analysis traces a provably tight bound on the Rényi divergence between the pair of probability distributions over parameters of models trained on neighboring datasets. The privacy loss converges exponentially fast, which is a significant improvement over composition theorems. "
SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper proposes a reinforcement learning approach to improve the convergence of first order methods for quadratic optimization (QP) solvers. In particular, the authors propose to use reinforcement learning (RL) to learn a policy to adapt the internal parameters of the solver to allow for fewer iterations and faster convergence. The proposed approach is evaluated on the QPLIB, Netlib LP and Maros-Mészáros problems. The results show that the proposed approach outperforms existing methods by up to 3x."
SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"This paper studies the over-parameterized deep linear network, and shows that the convergence rate of the parameters is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. The authors show how the PC-bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. They also compare the results to the spectral bias, showing that both biases can be seen independently, and affect the learning in different ways. Finally, they discuss how the results may explain some benefits of early stopping and its connection to PCA."
SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper studies the problem of detecting and erasing backdoors in deep neural networks. The authors identify two inherent characteristics of backdoor attacks as their weaknesses: 1) backdoor examples are easier and faster to learn than clean examples, and 2) backdoor learning establishes a strong correlation between backdoor examples and the target label. Based on these two findings, the authors propose a framework - Anti-Backdoor Learning (ABL) - which consists of two stages of learning utilizing local gradient ascent (LGA) and global gradients ascent (GGA), respectively. At the early learning stage, LGA intentionally maximize the training loss gap between clean and backdoor examples to isolate out the backdoored data via the low loss value, and GGA is used to unlearn the backdoor-poisoned model with the isolated backdoor data at the last learning stage."
SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a generative implicit model for shape-accurate 3D-aware image synthesis. The key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions, which is achieved by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To reduce the additional computational burden of calculating surface normals, the authors devise an efficient volume rendering strategy via surface tracking, which reduces the training and inference time by 24% and 48% respectively."
SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a scalable quasi-Bayesian procedure for instrumental variable regression, building upon the recently developed kernelized IV models. Theoretical properties of the proposed quasi-posterior are analyzed, and a scalable algorithm for approximate inference is derived. Empirical evaluations show that the proposed method scales to large and high-dimensional datasets, and can be particularly advantageous when the instrument strength is weak."
SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a method for cross-lingual open-retrieval answer generation (CORA) for multilingual open-QA. The authors propose a dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. They also propose a multilingual autoregressive generation model to generate answers directly in the target language without any translation or in-language retrieval modules as used in prior work. Finally, they propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. The results show that CORA substantially outperforms the previous state-of-the-art on multilingual question answering benchmarks across 26 languages."
SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper investigates the role of domain adaptation theory in predicting when ERM-trained deep neural networks will generalize out-of-distribution. In particular, the authors study the performance of ERM on three popular domain generalization datasets. They find that adapting theory from unsupervised domain adaptation is of limited use in predicting how well models generalize. The authors also investigate other possible measures, that lack theory, which could explain generalization in this setting. "
SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This paper provides a theoretical framework to study backdoor data poisoning attacks for classification problems. The authors identify a parameter they call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows them to argue about the robustness of several natural learning problems to backdoor attacks. They also show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set."
SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper analyzes the effect of width and depth on the performance of deep hierarchical models (GP) trained with L2 regularization. In particular, the authors study the generalization of neural networks to Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsumes neural nets. They show that depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian. They also show that there is a “sweet spot” that maximizes test performance before the limiting GP behavior prevents adaptability, occurring at width = 1 or width = 2."
SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper considers the setting of federated learning (FL), where a group of clients periodically coordinate with a central server to train a statistical model. The authors develop a general algorithmic framework called FedLin to tackle some of the key challenges intrinsic to FL, namely objective heterogeneity, systems heterogeneity, and infrequent and imprecise communication. They show that when the clients’ local loss functions are smooth and strongly convex, FedLin guarantees linear convergence to the global minimum, and establish matching upper and lower bounds on the convergence rate of FedLin that highlight the effects of infrequent, periodic communication. Finally, they show that FedLin preserves linear convergence rates under aggressive gradient sparsification."
SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper proposes a method to approximate the Sliced-Wasserstein distance (SW) by using the concentration of measure phenomenon, which shows that one-dimensional projections of a high-dimensional random vector are approximately Gaussian under mild assumptions. Based on this observation, the authors develop a simple deterministic approximation for SW, which is both accurate and easy to use compared to the usual Monte Carlo approximation. The method does not require sampling a number of random projections and is computationally efficient. The authors prove non-asymptotical guarantees for their approach, and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution."
SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method to measure the similarity between representations learned by language models, translation models, and language tagging tasks. The authors use an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. They find that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI. The method reveals a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embedding. This suggests that the embedding captures some part of the brain’s natural language representation structure."
SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"This paper proposes Diffusion-Decoding models with Contrastive representations (D2C) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. It can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional image manipulation, it achieves superior performance over state-of-the-art VAEs and diffusion models."
SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"This paper studies the problem of self-supervised learning from the perspective of contrastive learning, i.e., learning representations by pushing positive examples from the same class closer together while keeping negative pairs far apart. The authors propose a novel loss function that performs spectral decomposition on the population augmentation graph and can be written as a contrastive loss objective on neural representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. Empirically, the features learned by the proposed objective can match or outperform several strong baselines on benchmark vision datasets."
SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"This paper studies the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. The authors follow up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. They show that a different kind of parameterization—notably by the size of a feedback edge set—yields fixed-parameter tractability. Then they proceed by showing that this result can be strengthened to a localized version of the feedback edge sets, and provide corresponding lower bounds that complement previous results to provide a complexity classification of the problem. Finally, they show how their results can be extended to the closely related problem of Polytree Learning."
SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper considers the problem of active learning in the streaming setting for binary classification tasks. The authors propose an active learning algorithm, called ALPS, for general loss functions that learns to leverage pseudo-labels by using requester functions in order to train a model over a joint set of pseudo-labeled and labeled points. Theoretical analysis shows that the algorithm attains favorable generalization and label complexity bounds, while the empirical study on 18 real-world datasets demonstrate that the proposed algorithm outperforms standard baselines, including the Margin Algorithm."
SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a new measure of complexity called Kolmogorov growth (KG) which is used to derive new generalization error bounds that only depend on the final choice of the classification function. Based on these bounds, the authors propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classifiers."
SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a method for self-supervised learning of embeddings for image representation learning. The main idea is to learn representations that are invariant to different views of the same image, avoiding the collapse of the representations with a variance preservation term, and maximizing the information content of the representation with a covariance regularization term. The proposed method achieves results on par with the state of the art on several downstream tasks."
SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"This paper proposes Information Directed Reward Learning (IDRL), an active reward learning algorithm that learns a reward function using a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. The algorithm is designed to work with different types of feedback, and it is shown that it achieves similar or better performance with significantly fewer queries by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model."
SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,This paper proposes a method to predict the parameters of a neural network in a single forward pass. The proposed method is based on graph neural networks (GNNs) and is able to predict parameters for very diverse and large-scale architectures. The method is evaluated on CIFAR-10 and ImageNet and achieves good performance.
SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper provides a closed form expression for the distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. It shows that the DP function is always quadratic, regardless of the underlying distribution. In the Gaussian setting, it further provides closed form expressions for such estimators. For general distributions, it shows how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer and a minimizer of the MSE under a perfect perceptual quality constraint."
SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a novel model architecture for the representation learning on the textual graph. The proposed GraphFormers consists of layerwise GNN components nested alongside the transformer blocks of the pretrained language model. The text encoding and the graph aggregation are fused into an iterative workflow, making each node’s semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets."
SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of learning under user-level differential privacy constraints, i.e., the privacy of a user’s entire contribution (m>1 samples) is protected against information leaks. The authors propose and analyze algorithms for high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy, and show that the privacy cost decreases as O(1/\sqrt{m}^2) as users provide more samples. In contrast, when increasing the number of users n, the privacy costs decrease at a faster rate O(\1/n) rate. Lower bounds are also provided."
SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"This paper provides a self-consistent Gaussian process (GPs) theory for deep neural networks (DNNs) trained with noisy gradient descent on a large training set. The authors derive a GPs theory for DNNs with infinite width/channel limit and derive a transition between a feature learning regime and a lazy learning regime in this model. They further identify, both analytically and numerically, a sharp transition between the feature learning regimes and the lazy learning regimes in the model. "
SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the role of inductive bias in the emergence of compositional communication in signaling games. The authors theoretically show that inductive biases on both the training framework and the data are needed for the compositionality to emerge in the signaling game. They also empirically confirm that a range of noise levels, which depends on the model and data, indeed promotes compositionality."
SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper proposes a new network architecture, called ResMLP, for image classification. The architecture consists of a linear layer, a two-layer feed-forward network, and a residual layer. The residual layer consists of two linear layers, one for each channel, and the other for each image patch. The network is trained with data augmentation and self-supervised training. The proposed method achieves good performance on ImageNet. The paper also shows that the proposed method can be used for machine translation."
SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"This paper considers the problem of multi-class classification, where a stream of adversarially chosen queries arrive and must be assigned a label online. Unlike traditional bounds which seek to minimize the misclassification rate, this paper minimizes the total distance from each query to the region corresponding to its correct label. When the true labels are determined via a nearest neighbor partition – i.e., the label of a point is given by which of k centers it is closest to in Euclidean distance – this paper shows that one can achieve a loss that is independent of the total number of queries. This paper also shows that learning general convex sets requires an almost linear loss per query."
SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a method for knowledge transfer for Visual Question Anwering (VQA) based on a regularization term in the loss function, supervising the sequence of required reasoning operations. The proposed method relies on the availability of reasoning program annotations, which are costly to annotate, especially when dealing with human generated questions. The authors provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. The effectiveness of this approach is demonstrated on the GQA dataset and show its complementarity to self-supervised training."
SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the problem of constructing RNNs that are Turing-completeness, i.e. those that can run any given algorithms. The authors propose a method to construct such an RNN by adding a memory module to the RNN. The memory module consists of a fixed number of neurons of fixed precision and a growing memory module, which consists of neurons that are added to the network when new memories are needed and removed when they are no longer needed. They prove that such a RNN can simulate a Universal Turing Machine with time complexity linear in the simulated machine’s time and independent of the memory size. "
SP:3f33489b98ba6145fd4e334669493f15a63455f4,"This paper provides a theoretical analysis of the under-covering bias of quantile regression. The authors show that under the assumption that the quantile function is a realizable linear function and the number of training samples is small d/n, the coverage of the learned quantile is $\alpha$-1/2, where d is the input dimension and n is the number training data. The main contribution of this paper is to show that this bias is due to a high-dimensional estimation error in the regression coefficient, which is the main source of this under-coverage bias. Experiments on simulated and real data verify the theory."
SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based method for memory allocation in the context of class-incremental learning (CIL). The main idea is to use reinforcement learning (RL) to guide the allocation of memory in two levels: level-1 determines how to split the memory between old and new classes, and level-2 allocates memory for each specific class. The proposed method is evaluated on two CIL benchmarks (UCIL and POD+AANets) and shows improved performance over the baselines."
SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper considers the problem of speeding up stochastic gradient descent (SGD) by parallelizing it across multiple workers. The local SGD method, proposed and analyzed in the earlier literature, suggests machines should make many local steps between such communications. In this paper, the authors suggest a Local SGD scheme that communicates less overall by communicating less frequently as the number of iterations grows. Their analysis shows that this can achieve an error that scales as 1/(NT ) with a number of communications that is completely independent of T. In particular, they show that \Omega(N) communications are sufficient."
SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies the Online Lazy Gradient Descent for optimisation on a strongly convex domain. The algorithm is known to achieve $O(\sqrt{N}(\log N)$ regret against adversarial opponents; here the authors show that it also achieves $O(logN)$ expected regret against i.i.d opponents. This improves upon the more complex metaalgorithm of Huang et al [20] that only gets $\sqrt{\frac{N}{\log N}\log N}$ and $\log N$ bounds. The authors also show that, unlike for the simplex, order bounds for pseudo-regret and expected regret are equivalent for strongly-convex domains."
SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper compares the Bures-Wasserstein (BW) geometry and Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. It shows that the BW geometry has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI geometry. It also shows that several popular cost functions, which are known to be geodesic convex under the AI geometries, are also geodesics convex in the  BW geometry. Extensive experiments on various applications support the findings."
SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper introduces Dynaboard, an evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. The platform evaluates NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. Under this paradigm, models are submitted to be evaluated in the cloud, circumventing the issues of reproducibility, accessibility, and backwards compatibility that often hinder benchmarking in NLP. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness, which – despite their importance to practitioners – have traditionally been absent from leaderboards. On each task, the models are ranked according to the utility-based aggregation of these statistics, which users can customize to better reflect their preferences."
SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a method for automatic video dubbing (AVD) by synthesizing human speech synchronized with the given video from the text. The proposed method is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. An image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments on the chemistry lecture dataset and LRS2 dataset show that the proposed method can generate speech audios on par with state-of-the-art TTS models in terms of speech quality."
SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a method for federated learning in medical imaging. The proposed method is based on the recently proposed Vision Transformer (ViT) architecture with straightforward decomposable configuration, which is ideally suitable for split learning without sacrificing performance. The authors show that the proposed method outperforms the existing methods for distributed learning and achieves the performance comparable to data-centralized learning with their framework, even under the non-independent and identically distributed data distribution which emulates a real collaboration between hospitals using CXR datasets from multiple sources."
SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This paper proposes a differentiable point-to-mesh representation for 3D surface reconstruction based on oriented point clouds. The proposed method is based on the Poisson Surface Reconstruction (PSR) formulation of the indicator function given an oriented point cloud. The differentiable PSR layer allows to efficiently and differentiably bridge the explicit 3D point representation with the implicit 3D mesh via the implicit indicator field, enabling end to end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows to represent shapes as oriented points clouds, which are explicit, lightweight and expressive."
SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a method for recovering the representation of a pre-trained representation learning network (R(x) that operates on clean images, like CLIP) from a corrupted version A(x). The authors propose a supervised inversion method that uses a contrastive objective to obtain excellent representations for highly corrupted images. They achieve a higher accuracy than end-to-end supervised baselines when classifying images with various types of distortions, including blurring, additive noise, and random pixel masking."
SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper studies the problem of structural credit assignment in neural networks, where each node is treated as an agent and the goal is to train a neural network to assign the same amount of credit to each of the nodes in the network. The authors formalize the problem as a finite-horizon MDP, and show that the local policy gradient updates for each node (coagent) provide an unbiased estimate of the joint policy gradient for all nodes. They show that standard on-policy REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions, and introduce an off-policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents."
SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,This paper proposes a method to learn representations for both the ventral (what) and dorsal (where) pathways of the visual cortex of mice. The authors propose to use a self-supervised predictive loss function to learn the representations for the two pathways. They show that the proposed method outperforms other methods in fitting the mouse visual cortex. 
SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper introduces TopicNet, a deep hierarchical topic model that can inject prior structural knowledge as an inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and better document representations."
SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This paper proposes an object-level self-supervised pretraining method named Selective Object COntrastive learning (SoCo), which aims to align pretraining to object detection. Different from prior image-level contrastive learning methods which treat the whole image as an instance, SoCo treats each object proposal generated by the selective search algorithm as an independent instance, enabling SoCo to learn object- level visual representations. SoCo achieves state-of-the-art transfer performance on COCO detection using a Mask R-CNN framework. Experiments on two-stage and single-stage detectors demonstrate the generality and extensibility of SoCo."
SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"This paper presents a learning-augmented local search framework to solve large-scale VRP. The method iteratively improves the solution by identifying appropriate subproblems and delegating their improvement to a black box subsolver. The proposed method accelerates competitive VRP solvers on problems of sizes up to 3000, requiring an order of magnitude less computation time."
SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes Active Forgetting with Synaptic Expansion-Convergence (AFEC) to improve the performance of Bayesian continual learning by actively forgetting the old knowledge that limits the learning of new tasks to benefit continual learning. The method dynamically expands parameters to learn each new task and then selectively combines them, which is formally consistent with the underlying mechanism of biological active forgetting. Experimental results on CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks demonstrate the effectiveness of the proposed method."
SP:a5945ec13e2f362df03b42511d44827ef081f4c3,"This paper studies the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. The authors provide a convergence guarantee for the prior guided random gradient-free (PRGF) algorithms. Moreover, they present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. The theoretical results are confirmed by experiments on several numerical benchmarks and adversarial attacks."
SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper provides a theoretical analysis of the lottery ticket hypothesis (LTH) which states that learning on a properly pruned network (the winning ticket) improves test accuracy over the original unpruned network. This paper characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. It shows that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, it shows that when the algorithm is specified as an (accelerated) stochastic gradient descent algorithm, the number of samples required for achieving zero generalisation error is proportional to the total number of non-pruned weights in the hidden layer."
SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes two new methods for private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset subject to differential privacy that approximately preserves the answers to a large collection of statistical queries. The first method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. The authors demonstrate that GEM and PEP empirically outperform existing algorithms."
SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a simple mask classification model, which predicts a set of binary masks, each associated with a single global class label prediction, for instance-level semantic and panoptic segmentation tasks. The proposed mask classification-based method simplifies the landscape of effective approaches for semantic segmentation and shows excellent empirical results. In particular, the proposed method outperforms per-pixel classification baselines when the number of classes is large."
SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper extends the work of Daniely and Schacham (2020) who showed that gradient descent can find adversarial examples on random undercomplete two-layer ReLU neural networks. In particular, the authors extend their result to the overcomplete case, where the number of neurons is larger than the dimension (yet also sub-exponential in the dimension). They also show this result for any subexponential width random neural network with smooth activation function."
SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (SGMs) that trains SGMs in a latent space, instead of directly in the data space. The authors claim that this allows them to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGM in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, the authors introduce a new score-matching objective suitable to the LSGM setting, propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and analytically derive multiple techniques for variance reduction of the training objective. Experimental results show that latent SGMs outperform recent pixel-space SGMs."
SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper proposes a new explanation for the performance gap between neural networks and neural tangent kernels (NTKs) in image classification. Specifically, the authors show that, for a simple data distribution with sparse signal amidst high-variance noise, a convolutional neural network trained using SGD simultaneously learns to threshold out the noise and find the signal. On the other hand, the corresponding NTK, with a fixed set of predetermined features, is unable to adapt to the signal in this manner. The authors further show empirically that, as the background noise increases in intensity, a CNN’s performance stays relatively robust, whereas the corresponding neural NTK sees a notable drop in performance."
SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies the convergence rate of gradient tracking (GT) algorithms for decentralized machine learning over a network, where the training data is distributed across n agents, each of which can compute stochastic model updates on their local data. The agent’s common goal is to find a model that minimizes the average of all local loss functions. While gradient tracking algorithms can overcome a key challenge, namely accounting for differences between workers’ local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter p (related to the spectral gap of the connectivity matrix). This paper provides a tighter analysis of the GT method in the convex, convex and non-convex settings. The authors improve the dependency on p from O(p-2) to O(\p-1/c-1) in the noiseless case and from O-(p-3/2)-O(p^{1/2}) in the general case. This improvement was possible due to a new proof technique which could be of independent interest."
SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies the upper confidence bound (UCB) algorithm for the stochastic multi-armed bandit problem. The authors show that the arm-sampling rates under UCB are asymptotically deterministic, regardless of the problem complexity. The paper also provides the first complete process-level characterization of the MAB problem in UCB in the conventional diffusion scaling. "
SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper focuses on the cross-domain cold-start recommendation (CDCSR) problem, which is how to leverage the information from a source domain, where items are ‘warm’, to improve the recommendation performance of a target domain. The authors propose DisAlign, which utilizes both rating and auxiliary representations from the source domain (e.g., rating prediction module and embedding distribution alignment module) to improve recommendation performance in the target domain, and propose two methods to align the latent embedding distributions across domains: Stein path alignment and proxy path alignment. The proposed method is evaluated on the Douban and Amazon datasets."
SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes to replace the self-attention layer in vision transformers with 3 operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and an inverse Fourier transformation. The proposed method is simple yet computationally efficient. The experimental results demonstrate that the proposed method can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness."
SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper studies the problem of predicting trustworthiness on large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and a large number of samples. The authors observe that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and TCP confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy due to overfitting. To improve the generalizability of such predictors, the authors propose a novel steep slope loss to separate the features w.r.t. correct predictions from the ones that are incorrect by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, Vision Transformer and ResNet."
SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper studies adversarial robustness from the perspective of linear components, and finds that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, the authors propose a novel understanding of robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of the proposed clustering strategy."
SP:590b67b1278267e966cf0b31456d981441e61bb1,"This paper proposes a new approach for learning end-to-end reconstruction operators based on unpaired training data for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then solved iteratively till convergence. Notably, it takes significantly fewer iterations to converge as compared to variational methods, thanks to the excellent initialization obtained via the unrolling operator."
SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a framework for vision-language representation learning. It first aligns the unimodal image representation and text representation before fusing them with a multimodal encoder. To improve learning from noisy web data, it proposes momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. It provides a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair."
SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decisionmaking policies based on static datasets. The authors study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. They obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, they also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions."
SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,"This paper considers the problem of non-smooth convex stochastic convex optimization problems with non-sub-Gaussian (heavy-tailed) noise. In particular, the authors consider the setting where the objective function is a convex function of a set of convex functions, and they study the convergence of two methods, one accelerated and one non-accelerated. They show that the first method converges to the optimal solution with high probability, and the second one is optimal in the same regime. They also provide an extension to the strongly convex setting."
SP:a22a893e25ce739dc757861741014764e78aa820,"This paper studies the long-term forecasting problem of time series. The authors propose a novel decomposition architecture with an Auto-Correlation mechanism. They break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series, inspired by the stochastic process theory, they design the Auto-correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. The proposed method outperforms self-attention in both efficiency and accuracy."
SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a dataset of cryptic crossword clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. It also introduces a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies. Finally, it proposes a curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words."
SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper analyzes the internal representation structure of ViTs and CNNs on image classification benchmarks, and finds striking differences between the two architectures, such as ViT having more uniform representations across all layers. The authors explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. They also study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, they study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer."
SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) in combinatorial multi-armed bandit (CMAB) problems with the greedy oracle, which is a common (approximation) oracle with theoretical guarantees to solve many (offline) optimization problems. The authors provide a problem-dependent regret lower bound of order $\Omega(\log T/2)$ to quantify the hardness of TS to solve CMAB problems with greedy oracles, where T is the time horizon and is some reward gap. They also provide an almost matching regret upper bound. "
SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies federated learning from a game-theoretic point of view. In particular, the authors propose a notion of optimality given by the average error rates among federating agents (players). They also provide an efficient algorithm to calculate an optimal (error minimizing) arrangement of players. Finally, they analyze the relationship between the stability and optimality of an arrangement. "
SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for learning capsule decompositions of 3D point clouds. Capsule decomposition is computed by permutation-equivariant attention, and the proposed method learns capsule decomposition by training with pairs of randomly rotated objects. The key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This enables the training of a semantically consistent decomposition, and also allows us to learn a canonicalization operation that enables object-centric reasoning."
SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method to compute prediction intervals for nonparametric regression that can automatically adapt to skewed data. It leverages black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, and translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives."
SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,"This paper builds on the work of Elesedy and Zaidi [8] to show that enforcing invariance improves generalization in kernel ridge regression when the target is invariant to the action of a compact group. The authors study invariance enforced by feature averaging and find that generalization is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In particular, they show that action of the group induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right."
SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model to constrain neural network activations to “decode” back to inputs. This design enables a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. In the experiments, the authors demonstrate applications of this uncertainty to out-of-distribution detection, adversarial example detection, and calibration — while matching standard neural networks in accuracy. They further explore this compositionality by combining this design with pretrained models, where they show promising results that neural networks can be regularized from using protected features."
SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the rates of convergence of plug-in estimators of optimal transport (OT) maps between two probability distributions $\mu$ and $\tilde{x}$ on R. The main contribution is a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general OT estimators. The authors show that the stability estimate is useful for the rate of convergence for the natural discretediscrete and semi-discrete estimators, Sobolev type, Besov type, kernel smoothed or wavelet type, and kernel smoothing. They also show that under additional smoothness conditions, the stability of the estimator can speed up convergence. "
SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. The proposed method achieves over 65% test accuracy on CIFAR10 image classification task, a dramatic improvement over the previous best best test accuracy of 40%. The authors also perform some preliminary analyses of the distilled datasets to shed light on how they differ from naturally occurring data."
SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes a novel semi-supervised learning method for detecting novelty in unlabeled data. The proposed method is based on the one-vs-all (OVA) classifier, which predicts the confidence score of a sample being an inlier. The paper also proposes an open-set soft-consistency regularization loss, which enhances the smoothness of the OVA-classifier with respect to input transformations and improves outlier detection. Experiments are conducted on CIFAR-10, Cifar-100, and ImageNet."
SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper presents Latent Explorer Achiever (LEXA), an agent for unsupervised RL that explores its environment, learns to achieve the discovered goals, and solves image-based tasks in a zero-shot way. The agent learns a world model from image inputs and uses it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. LEXA substantially outperforms previous approaches, both on prior benchmarks and on a new challenging benchmark with 40 test tasks spanning across four robotic manipulation and locomotion domains."
SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper proposes a method to reduce the number of parameters in Transformer-based language models. The authors propose to use low-rank factorized representations of matrices that underpin dense, embedding, and self-attention layers, and replace each parameter matrix with its compact equivalent while maintaining the architecture of the network. Theoretical and experimental results show that stacked low rank Kronecker-product-based representations are more expressive, yet smaller than standard low rank factorized matrix representations. "
SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,This paper proposes a Transformer-based code summarization model that leverages the syntax trees and paths in the source code. The authors propose to encode pairwise paths between tokens and paths from the leaf node to the root of the tree for each token in the syntax tree. They also propose a relative path encoding method that encodes the path between the leaf nodes and the root nodes for each tokens in the tree. The proposed approach is evaluated on four different languages and compared with several baselines. 
SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a Transformer-based generator for high-resolution image generation based on Generative Adversarial Networks (GANs). To address the quadratic scaling problem of Transformers, the proposed HiT structure the low-resolution stages of HiT following the design of Nested Transformer and enhance it by the proposed multi-axis blocked self-attention. In order to handle extremely long inputs in the high resolution stages, HiT also reduces the model into implicit functions and improves the model performance. The experiments demonstrate that HiT achieves state-of-the-art FID scores of 30.83 and 2.95 on unconditional ImageNet 128x128 and FFHQ 256x256."
SP:41a6753bc56eb16040600666a859294ae36cfa9c,"This paper studies the query complexity of learning geodesically convex halfspaces on graphs. Geodesic convexity is a natural generalization of Euclidean convexness and allows the definition of convex sets and halfspace on graphs, and the authors prove an upper bound on query complexity which is linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. They also show tight lower bounds along well-established separation axioms and identify the Radon number as a central parameter of query complexity and the VC dimension. The authors also provide evidence that ground-truth communities in real-world graphs are often convex."
SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,"This paper proposes a low-fidelity (LoFi) video encoder optimization method to improve the performance of temporal action localization (TAL) methods. In particular, the authors propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoders and TAL heads becomes operable under the same memory conditions of a mid-range hardware budget. Experiments show that the proposed LoFi optimization approach can significantly enhance existing TAL methods."
SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies M-estimators with gradient-Lipschitz loss function regularized with convex penalty in linear models with Gaussian design matrix and arbitrary noise distribution. The authors provide general formulae for the derivatives of regularized $beta^*(y,X)$ where differentiation is taken with respect to both y and X. They characterize the distribution of the residual ri = yi−xi $\tilde{beta}^*$ in the intermediate high-dimensional regime where dimension and sample size are of the same order. They also propose a novel adaptive criterion to select the tuning parameters of regularised $B^*$."
SP:be53bc4c064402489b644332ad9c17743502d73c,"This paper proposes a calibrated beam-based algorithm with awareness of the global attention distribution for neural abstractive summarization, aiming to improve the local optimality problem of the original beam search in a rigorous way. Specifically, a novel global protocol is proposed based on the attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring mechanism is then developed to regulate beam search to generate summaries in a near-global optimal fashion. Extensive experiments on nine datasets show that the global (attention)-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters."
SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a self-attention mechanism that is equivariant to the orientation of local coordinate systems (i.e., Gauge Equivariant Transformer). It employs multi-head selfattention to jointly incorporate both position-based and content-based information. To enhance expressive ability, the authors adopt regular field of cyclic groups as feature fields in intermediate layers and propose a novel method to parallel transport the feature vectors in these fields, and project the position vector of each point onto its local coordinate system to disentangle its orientation in ambient space. The proposed method can be efficiently implemented on triangle meshes."
SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The proposed method combines the expectation maximization and Metropolis-Hastings algorithm to evaluate only a small number of, stochastically sampled, components, thus substantially reducing the computational cost. The method is able to train both shallow and deep mixture models which involve complex, and possibly nonlinear, transformations."
SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for sparse training of deep neural networks. The authors formulate the training process as a continuous minimization problem under global sparsity constraint and separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, the authors use the conventional chain rule, which can be sparse via exploiting the sparse structure. The latter step, instead of using the chain rule based gradient estimators as in existing methods, they propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training."
SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper proposes a novel importance sampling method for non-equilibrium orbits (NEO) and Markov chain Monte Carlo (MCMC) samplers for normalizing constant estimation (Z) based on the invertible map T. The proposed method, named as NEO-IS, provides unbiased estimators of Z and self-normalized IS estimators for expectation under π, while the proposed method is able to explore highly multimodal targets. The authors also provide detailed theoretical results for both methods. "
SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a scalable and efficient attention-based set encoding mechanism that is amenable to mini-batch processing of sets, and capable of updating set representations as data arrives in a stream. The proposed method adheres to the required symmetries of permutation invariance and equivariance as well as maintaining MBC for any partition of the input set. The authors perform extensive experiments and show that the method is computationally efficient and results in rich set encoding representations for set-structured data."
SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper proposes an algorithm for action exploration and equilibrium approximation in games with combinatorial action spaces. This algorithm simultaneously performs value iteration while learning a policy proposal network. A double oracle step is used to explore additional actions to add to the policy proposals. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. Using this algorithm, the authors train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. The authors also extend their methods to full-scale no-press Diplomacy."
SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,"This paper studies the problem of multi-head attention in multilingual and multi-domain sequence modeling, where the key challenge is to maximize positive transfer and mitigate negative interference across languages and domains. It finds that non-selective attention sharing is sub-optimal for achieving good generalization across all languages, and proposes attention sharing strategies to facilitate parameter sharing and specialization for multilingual sequence modeling. The proposed methods are evaluated in various tasks including speech recognition, text-to-text and speech to text translation. The results show that the proposed methods outperform the baselines."
SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the asymptotic properties of random feature regression under covariate shift, i.e., when the input distributions of the training and test sets differ while the conditional label distributions remain the same. The authors provide a precise characterization of the limiting test error, bias, and variance in this setting. They motivate a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. They find that overparameterized models exhibit enhanced robustness to covariate distribution shift, providing one of the first theoretical explanations for this phenomenon."
SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies Thompson Sampling and other Bayesian sequential decision-making algorithms with misspecified prior. The authors prove that the expected reward accrued by such algorithms with a misspecification of the prior differs by at most $\tilde{O}(\epsilon^{-\varepsilon}(H^2)$ from TS with a well-specified prior, where $H$ is the total-variation distance between priors and H is the learning horizon. The results generalize to Bayesian POMDPs and Bayesian Knowledge Gradient. "
SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,"This paper proves an exponential separation between PAC and Equivalence-query-learning (EQ) learning models. In the PAC model all samples are provided at the beginning of the learning process, and in the EQ model the samples are acquired through an interaction between a teacher and a learner, where the teacher provides counterexamples to hypotheses given by the learner. The authors prove that in order to achieve an error $\epsilon$-error, fewer samples suffice than what the PAC bound requires. They also discuss how their result relates to adversarial robustness."
SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of scalable model selection for accessible transfer learning. The authors formalize this setting as “Scalable Diverse Model Selection” and propose several benchmarks for evaluating on this task. They find that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. They then introduce simple techniques to improve the performance and speed of these algorithms. Finally, they iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection."
SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for learning low-dimensional binary representations of instances and classes. The proposed method is based on the idea of learning a codebook of binary embeddings of an instance and a class. The codebook is then used to train a neural network to predict the binary representation of the instance and the class. This codebook can then be used to perform classification, retrieval and OOD detection tasks."
SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a method of Generalized Depthwise-Separable (GDWS) convolution, which is an efficient, universal, post-training approximation of a standard 2D convolution. GDWS improves the throughput of a pre-trained network on real-life hardware while preserving its robustness to adversarial perturbations. It is scalable to large problem sizes and doesn’t require any additional training. Experiments are conducted on CIFAR-10, SVHN, and ImageNet."
SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a graph-based semi-template-based retrosynthesis model for single-step organic synthesis. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and also amenable to manual correction."
SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a Bayesian formulation of the deconditioning problem, which naturally recovers the initial reproducing kernel Hilbert space formulation from Hsu and Ramos [1]. The authors extend the proposed method to a downscaling setup and devise an efficient conditional mean embedding estimator for multiresolution data. They show that this solution can be viewed as a two-staged vector-valued kernel ridge regressor and show that it has a minimax optimal convergence rate under mild assumptions."
SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method for neural architecture search for deep sparse networks (DSNs) by searching the critical component in DSNs, the feature-interaction layer. The authors propose a distilled search space to cover the desired architectures with fewer parameters. They then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world benchmark datasets show promising results of PROFIT."
SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the generalization properties of fine-tuning for transfer learning, where a pre-trained model is finetuned on a target task with a small amount of labeled data. The authors provide a PAC-Bayes generalization bound that depends on the distance traveled in each layer and the noise stability of the model. Based on the analysis, the authors propose regularized self-labeling, including (i) layer-wise regularization, (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and (iii) reweighting less confident data points. They validate their approach on an extensive collection of image and text data sets."
SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper considers the problem of identifying the arm with the smallest conditional CVaR, VaR, or weighted sum of the mean and mean. The authors propose an optimal $\delta$-correct algorithm that matches the lower bound on the expected number of samples needed, asymptotically (as \delta approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. "
SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes a new Vision Transformer Advanced by Exploring Intrinsic Inductive Bias (ViTAE) to incorporate two types of intrinsic inductive bias (IB) into transformers, i.e., locality and scale-invariance, by using multiple convolutions with different dilation rates. The proposed ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context, which is able to learn robust feature representation for objects at various scales. The convolution block in each transformer layer is parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and can learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of the proposed model."
SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine- tuning process, whereas a conventional one only uses the pretrained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks."
SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,This paper proposes a Stochastic Anderson Mixing (SAM) scheme to solve nonconvex stochastic optimization problems. The authors introduce damped projection and adaptive regularization to the classical Anderson mixing (AM) scheme. They also propose a preconditioned mixing strategy for SAM which can empirically achieve faster convergence or better generalization ability. Experimental results on image classification and language model demonstrate the advantages of the proposed SAM.
SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a novel stochastic algorithm for solving general noisy linear inverse problems. The algorithm is based on annealed Langevin dynamics and Newton’s method, and relies on the availability of a pre-trained Gaussian MMSE denoiser. SNIPS produces a random variety of high quality samples from the posterior distribution of the unknown given the measurements, while guaranteeing their validity with respect to the given data. The authors demonstrate SNIPS’ success on image deblurring, super-resolution, and compressive sensing."
SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,"This paper proposes a framework to detect drug traffickers on Instagram. The proposed framework builds a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media. Then, it employs a relation-based graph convolutional neural network to learn node representations over the built HG, introduces graph structure refinement to compensate the sparse connection among entities in the HG, and proposes a meta-learning algorithm for model optimization. A self-supervised module and a knowledge distillation module are further designed to exploit the unlabeled data for improving the model."
SP:242da1384f48260d58a0e7949438611c05079197,"This paper investigates the question of whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size) in a neural network with ReLU activations and a given architecture. This question has potential impact on algorithmic and statistical aspects because of the insight it provides into the set of functions represented by neural hypothesis classes. To the best of my knowledge, this question has not been investigated in the neural network literature. The authors also present upper bounds on the sizes of neural networks required to represent functions in these neural hypotheses classes."
SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper proposes a general framework of min-max optimization over multiple domains that can be leveraged to advance the design of different types of adversarial attacks. In particular, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as a min-min problem by introducing domain weights that are maximized over the probability simplex of the domain set. The authors showcase this unified framework in three attack generation problems – attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations. Extensive experiments demonstrate that the proposed approach leads to substantial attack improvement over the existing heuristic strategies as well as robustness improvement over state-of-the-art defense methods trained to be robust against multiple perturbations types."
SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of sparse tensor principal component analysis, where the goal is to recover the k-sparse unit vector x in R^d with i.i.d. Gaussian entries. The authors propose a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. The algorithms recovers the sparse vector for signal-tonoise ratio, capturing the state-of-the-art guarantees for the matrix settings (in both the polynomially-time and sub-exponential time regimes). The results extend to the case of r distinct k sparse signals with disjoint supports, with guarantees independent of the number of spikes."
SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper presents a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. Experiments are conducted on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and geometric task of mesh transfer between 3D shapes."
SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle-point optimization problem with probability simplex constraints. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, the authors develop provably efficient extragradient methods to find the quantal response equilibrium (QRE) for zero-sum two-player matrix games and Markov games at a linear rate. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly. The authors also provide a sublinear rate for finding an approximate Nash equilibrium of the unregularized matrix game."
SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,"This paper proposes a method for super-resolution (SR) of low resolution (LR) and high resolution (HR) screen content images (SCI) at arbitrary scales. The authors propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at arbitrary ratios, pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. The proposed method achieves the best results on four datasets at various magnification ratios."
SP:3751625929b707ced417c3eb10064e4917866048,"This paper proposes a method for learning interventional sumproduct networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. The authors propose to use sum-product networks parameterized by universal function approximators in the form of neural networks to predict the parameters of the SPN. The resulting interventional SPNs are motivated and illustrated by a structural causal model themed around personal health. The empirical evaluation shows that the proposed method is able to precisely estimate the conditioned variables and outperform generative baselines."
SP:c857ff674ca05c1d949337cb885f056b82d981d6,"This paper proposes a generative model for functional magnetic resonance imaging (fMRI) data, which is based on Markov property in a chain of low dimensional temporal embeddings together with spatial inductive assumptions, all related through neural networks, to capture temporal dynamics in fMRI data, and tackle their high spatial dimensionality, respectively. Augmented with a discrete latent, DMFA enables validation of a variety of fMRI-driven neuroscientific hypotheses. Experimental results on both synthetic and real-data demonstrate the capacity of DMFA in revealing interpretable clusters and capturing nonlinear temporal dependencies in these high-dimensional imaging data."
SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper introduces a new class of generative models within the family of continuous normalizing flows (CNF) called Moser flows (MF), which is a solution to the change-of-variable formula. Different from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds, and does not require invoking or backpropagating through an ODE solver during training. Theoretically, the authors prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, they demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences."
SP:545554de09d17df77d6169a5cc8f36022ecb355c,"This paper studies the problem of unsupervised representation learning with non-identifiability in the setting where additional, typically observed variables are included in the generative process. The authors propose to use the principle of independent causal mechanisms, which is motivated by thinking of each source as independently influencing the mixing process. This gives rise to a framework which the authors term independent mechanism analysis. They provide theoretical and empirical evidence that their approach circumvents a number of non identifiability issues arising in nonlinear blind source separation."
SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a method for approximate Hamiltonian Hamiltonian MCMC-based importance sampling, where the goal is to obtain samples from an unnormalized target distribution and a tight lower bound on its (log) normalization constant logZ. The authors propose to use an AIS-like procedure with Uncorrected HamiltonianMCMC (UHMC) and show that it leads to tight and differentiable lower bounds on logZ, and empirically show that their method yields better performances than other competing approaches. "
SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes an efficient and trainable local Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Specifically, when computing the induced norm of a weight matrix, the authors eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global neural network neural network. The authors also propose to clip activation functions with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local Lipchitz bound. Experiments are conducted on MNIST, CIFAR-10 and TinyImageNet datasets."
SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,This paper proposes a method for producing conformal Bayesian predictive intervals with finite sample calibration guarantees. Conformal inference provides finite sample frequentist guarantees on predictive confidence intervals without the requirement of model fidelity. The authors propose to use ‘add-one-in’ importance sampling to obtain conformal predictive intervals that are efficiently obtained from re-weighted posterior samples of model parameters. They demonstrate the utility on a range of examples including extensions to partially exchangeable settings.
SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper introduces potential-driven denoisers, i.e., gradients of smooth scalar-valued deep neural networks, acting as potentials, for solving general inverse problems. The authors show that the gradients are symmetric Jacobians, allowing for maximum a posteriori (MAP) or minimum mean square error (MMSE) estimators interpretation, and that they can be integrated into RED and PnP schemes with backtracking step size, removing the need for enforcing their Lipschitz constant. They also provide a simple inversion method to show its convergence to stationary points of an underlying objective function consisting of learned potentials."
SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a generative model for generating music that is in-sync with the human body movements captured in a video. The proposed method, called RhythmicNet, takes as an input a video with human movements and generates a soundtrack for it, by extracting skeleton keypoints and implementing a sequence of models translating them to rhythmic sounds. The method first infers the music beat and the style pattern from body keypoints per each frame to produce the rhythm. Next, it implements a transformer-based model to generate the hits of drum instruments and implements a U-net based model to translate the velocity and the offsets of the instruments. Additional types of instruments are added to the soundtrack by further conditioning on generated drum sounds."
SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes a simple one-step offline reinforcement learning algorithm that uses an on-policy Q-value estimate of the behavior policy to improve the performance of the actor-critic algorithm. The proposed algorithm is shown to outperform the existing iterative offline RL algorithms on the D4RL benchmark. The authors hypothesize that the performance is due to the high variance inherent in doing off-policy evaluation and magnified by the repeated optimization of policies against the Q estimates.
SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper considers the problem of low-rank matrix recovery for nonsmooth matrix optimization, where the objective function can be written as a maximum of smooth functions. The authors show that the extragradient method, when initialized with a “warm-start-point”, converges to an optimal solution with rate O(1/t) while requiring only two low rank SVDs per iteration. They also give a precise trade-off between the rank of the SVD and the radius of the ball in which they need to initialize the method. They support their theoretical results with empirical experiments on several nonsmoothing low rank matrix recovery tasks."
SP:d39f1d77d9919f897ccf82958b71be8798523923,"This paper proposes a generalized Robinson decomposition (GRD) method for conditional average treatment effects (CATEs) estimation for structured treatments (e.g., graphs, images, texts). The main contributions of the paper are: 1) GRD decomposes the causal estimand (reducing regularization bias), 2) allows one to plug in arbitrary models for learning, and 3) provides a quasi-oracle convergence guarantee under mild assumptions. The proposed method is evaluated on small-world and molecular graphs. "
SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,"This paper studies the problem of causal effect identification, i.e., determining whether a causal effect is computable from a combination of qualitative assumptions about the underlying system (e.g., a causal graph) and distributions collected from this system. The authors first characterize the relationships between certain graphically-driven formulae and matrix multiplications. Then, they extend the spectrum of proxy variable based identification conditions and propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, they devise a new algorithm which accepts as input a collection of marginal, conditional, and interventional distributions, integrating enriched matrix-based criteria into a graphical identification approach."
SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper proposes a new approach to unsupervised environment design (UED) based on prioritized level replay (PLR). The authors show that PLR is a special case of UED, and propose a new algorithm called Dual Curriculum Design (DCD) that combines the benefits of both PLR and PAIRED. They also provide a theoretical analysis of PLR, and show that it converges to Nash equilibria. Finally, they propose an improvement to PLR by stopping the agent from updating its policy on uncurated levels (training on less data), which leads to improved convergence."
SP:9ed528da4b67f22678303cfd975aafe678db6411,This paper studies the multi-armed bandit problem in the shuffle model. The authors provide an algorithm with a distribution-dependent regret of $O(\frac{k}{\sqrt{T} + \log T} + k \log 1/\delta^1 + k\log T)$ where $\log T$ is the number of arms and $\delta$ is a measure of the suboptimality gap of the arms. The main contribution of this paper is to provide a differentially private algorithm that matches the regret of the best known algorithms for the centralized model and significantly outperforms the existing algorithms in the local model. 
SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper proposes a new notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. The authors provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated Forecaster. The procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, the proposed method improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."
SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method to construct centroid approximation for the distribution of maximum points of a random function (a.k.a. argmax distribution), which finds broad applications in machine learning. The method optimizes a set of centroid points to compactly approximate the arg max distribution with a simple objective function, without explicitly drawing exact samples from the arg min distribution. Theoretically, the argmax centroid method can be shown to minimize a surrogate of Wasserstein distance between the ground-truth arg max and the centroids approximation under proper conditions. The authors demonstrate the applicability and effectiveness of the proposed method on a variety of real-world multitask learning applications, including few-shot image classification, personalized dialogue systems and multi-target domain adaptation."
SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper considers the multi-objective reinforcement learning (MORL) setting where the objectives are balanced using preferences. In this setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. The authors provide a model-based algorithm that achieves a nearly minimax optimal regret bound, where the reward function is the inner product of a preference vector with pre-specified reward functions. The proposed algorithm is provably efficient with a nearly optimal trajectory complexity. "
SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a new method, local explanation of response generation (LERG), that aims to explain dialogue response generation models through the mutual interaction of segments in input and output sentences. LERG views the sequence prediction as uncertainty estimation of a human response and then creates explanations by perturbing the input and calculating the certainty change over the human response. Experiments show that LER can find explanations that can both recover a model’s prediction and be interpreted by humans."
SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence rate of error-compressed gradient compression algorithms for distributed optimization. The authors propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. They show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms."
SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,This paper proposes an astrocyte-inspired liquid state machine (LSM) model that combines critical branching dynamics and spike-timing-dependent plasticity (STDP) into a single liquid to improve the performance of the LSM. The authors claim that the proposed model achieves state-of-the-art performance on MNIST and N-MNIST datasets. They also show that the model is able to achieve comparable performance to a fully-connected neural network trained via backpropagation.
SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,This paper studies the problem of topology imbalance in semi-supervised node classification. The authors propose a new metric called influence conflict detection–based metric Totoro to measure the degree of graph topological imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. Extensive experiments demonstrate the effectiveness and generalizability of the proposed method.
SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper considers the problem of recovering the partition of the lattice induced by the constancy regions of the unknown signal, using the computationally-efficient dyadic classification and regression tree (DCART) methodology proposed by [14]. The authors prove that, under appropriate regularity conditions on the shape of the partition elements, a DCART-based procedure consistently estimates the underlying partition at a rate of order σ2k∗ log(N)/kappa, where k is the minimal number of rectangular sub-graphs obtained using recursive dyadic partitions supporting the signal partition, σ is the noise variance, $\kappa$ is the largest magnitude of the signal difference among contiguous elements, and N is the size of lattice. The authors also show that under stronger assumptions, the method attains a sharper estimation error of order \tilde{O}(\sigma^2k\log(N/kappa)) independent of k, which they show to be minimax rate optimal."
SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a method to reduce the spurious correlations caused by observed confounders in deep learning models. The authors propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, which they call Counterfactual maximum likelihood estimation (CMLE). They then derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning model using observational data. They conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations, while maintaining comparable performance on the regular evaluations."
SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper studies the multi-objective optimization (MOO) problem, where the objective is to minimize the average loss across all tasks. The authors propose a new algorithm, called CAGrad, which optimizes the minimum decrease rate of any specific task’s loss while still provably converging to the optimum of the average. The proposed algorithm is a generalization of the regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) in the MOO literature as special cases. On a series of challenging multi-task supervised learning and reinforcement learning tasks, the proposed algorithm achieves improved performance over prior state-of-the-art multi-Objective gradient manipulation methods."
SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of language models. The authors consider the setting of a teacher-teacher setup, where the teacher is given a small set of examples and the learner is asked to identify a set of algorithmic concepts from small witness sets. The paper considers the setting where the language model is a GPT-like model, and the task is to identify the set of concepts that the teacher can identify from a small number of examples. In particular, the paper focuses on the question of whether language models can identify simple algorithmic concept from small sets of examples, and whether they are able to capture patterns in the data."
SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes a method to identify robust and non-robust features in the feature space of deep neural networks (DNNs) based on Information Bottleneck. Specifically, the authors inject noise variation to each feature unit and evaluate the information flow in feature representation to dichotomize feature units based on the noise variation magnitude. They demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, they present an attack mechanism intensifying the gradient of the robust features directly related to the model prediction and validate its effectiveness of breaking model robustness."
SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,"This paper studies the support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL). The authors prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. They further identify a sharp phase transition in Gaussian feature models and bound the width of this transition. Finally, they hypothesize that this phase transition occurs only in much higher-dimensional settings in the l1 variant of the SVM and present a new geometric characterization of the problem that may elucidate this phenomenon for the general lp case."
SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the problem of best policy identification (BPI) in Markov Decision Processes (MDPs), where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. The authors propose a problem-dependent lower bound on the average number of steps required before a correct answer can be given with probability at least 1-\delta. They also provide the first algorithm with instance-specific sample complexity in this setting."
SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes Cone Embeddings (ConE), a geometry-based query embedding model that can handle all the FOL operations, including conjunction, disjunction, and negation. ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunctions operations. The authors also design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets."
SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,This paper considers the problem of optimal control of stochastic nonlinear systems with separable cost and constraints in the state and input variables. The authors propose a novel numerical scheme for implementation of the corresponding value iteration (VI) algorithm in the conjugate domain. The proposed approach reduces the time complexity of each iteration in the VI algorithm from $O(XU)$ to $O(\X^2 + U)$ by replacing the minimization operation in the primal domain with a simple addition.
SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper presents a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, the authors propose a VideoAudio-Text Transformer (VATT) that takes raw signals as inputs and extracts multimodals representations that are rich enough to benefit a variety of downstream tasks. The authors train VATT end-to-end from scratch using multimodality contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text- to-video retrieval. They also study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities."
SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"This paper proposes a kernelized version of transformers, which replaces the softmax structure in self-attention with a Gaussian kernel and adapts the Nyström method to a non-positive semidefinite matrix to accelerate the computation. Theoretical analysis shows that the matrix approximation error of the proposed method is small in the spectral norm. Experiments on the Long Range Arena benchmark show that it is sufficient in getting comparable or better performance than the full self-Attention while requiring fewer computation resources."
SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,"This paper proposes a data-augmentation technique to enable data efficient learning from parametric experts. Specifically, the authors focus on what they refer to as the policy cloning setting, which allows for offline queries of an expert or expert policy. They achieve a very high level of data efficiency in transferring behavior from an expert to a student policy for high-degrees of freedom (DoF) control problems using their augmented policy cloning (APC) approach, which combines conventional image-based data augmentation to build invariance to image perturbations with an expert-aware offline data augmentation approach that induces appropriate feedback-sensitivity in a region around expert trajectories. They show that their method increases data-efficiency of policy cloning, enabling transfer of complex high-DoF behaviors from just a few trajectories, and also show benefits of their approach in the context of algorithms in which policy cloning is a constituent part."
SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,"This paper proposes an approach to improve the robustness of deep neural networks (DNNs) to perturbations in the input data. In particular, the authors propose a framework that leverages the ability of DNNs to be sensitive to input perturbation to design ""robust objects"", i.e., objects that are explicitly optimized to be confidently classified. The framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments. "
SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,"This paper investigates the effect of data augmentation on the stability and sample efficiency of convolutional neural networks (CNNs) and vision transformers (ViTs) in the context of off-policy reinforcement learning (RL). The authors identify two problems, both rooted in high-variance Q-targets, and propose a simple yet effective technique to stabilize CNNs and ViTs under augmentation. Experiments are conducted on the DeepMind Control Suite (DMC) benchmark and on a set of robotic manipulation tasks. The results show that the proposed method improves the sample efficiency and stability of CNN and ViT algorithms, and is competitive with state-of-the-art methods for image-based RL in environments with unseen visuals."
SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies federated learning (FL) in the setting where multiple worker nodes (WNs) build a joint model by using local data. The authors consider a class of stochastic algorithms where the WNs perform a few local updates before communicating with the server. They show that when both the WN’s and the server's update directions are chosen based on a certain momentum estimator, the algorithm requires O(3/2) samples and O(1) communication rounds to compute an $\epsilon$-stationary solution. They also show that there is a trade-off curve between the number of local updates and the minibatch sizes."
SP:bd3eecb81a17af010f2d3555434990855c1810f2,"This paper studies the optimal noise covariance of SGD with isotropic noise in the context of the information-theoretical generalization bound for large models trained by SGD. In particular, the authors show that optimal noise is the square root of the expected gradient covariance if both the prior and the posterior are jointly optimized. The authors also show that the optimal covariance is quite close to the empirical covariance. "
SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,"This paper proposes a video compression framework that can support multiple prediction modes. The proposed method is based on a motion compensation module that applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. The paper also proposes a flow prediction module to predict accurate motion trajectories with a unified polynomial function. Experimental results demonstrate that the proposed method can achieve comparable R-D performance with the latest Versatile Video Coding (VVC) standard in terms of MS-SSIM."
SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper introduces and analyzes MT-OMD, a multitask generalization of Online Mirror 1 Descent (OMD) which operates by sharing updates between tasks. Theoretical results show that the regret is of order $\tilde{O}(\sqrt{1+\sigma^2}(N-1)^T)$, where $\sigma$ is the task variance according to the geometry induced by the regularizer, N is the number of tasks, and T is the time horizon. This improves upon the $O(1)$-NT bound obtained by running independent OMDs on each task. Experiments on several real-world datasets support the theoretical findings."
SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,"This paper studies the underdamped Langevin diffusion (ULD) with strongly-convex potential consisting of finite summation of N smooth components, and proposes an efficient discretization method, which requires O(N + d 1 3N 2 /ε 2 3 ) gradient evaluations to achieve the $\epsilon$-error for approximating d-dimensional ULD. The authors also prove a lower bound of gradient complexity as $\Omega(\frac{N}{1/3}N^{2/3})$, which indicates that the method is optimal in dependence of N, ε, and d. Experimental results on both synthetic and real-world data show that the proposed method consistently outperforms the existing ULD approaches."
SP:22822f378c3fbc15b77eb736194b1ce7f0585072,"This paper proposes a new method for continual learning that combines weight regularization and projected gradient descent. The proposed method, called Natural Continual Learning (NCL), uses Bayesian weight regularisation to encourage good performance on all tasks at convergence and combines this with gradient projection using the prior precision, which prevents catastrophic forgetting during optimization. NCL outperforms existing methods in both feedforward and recurrent neural networks."
SP:26de056be14962312c759be5d284ef235d660f9c,"This paper proposes two methods to train rectangular normalizing flows, i.e. flows that are invertible neural networks with tractable change-of-volume terms, by learning distributions on manifolds. Injective flows aim to map data from low-dimensional space to high-dimensional spaces, but the resulting volume-change term becomes more challenging to evaluate. The authors propose two approaches to calculate the gradient of this term with respect to the parameters of the model, relying on careful use of automatic differentiation and techniques from numerical linear algebra. Both approaches perform end-to-end nonlinear manifold learning and density estimation for data projected onto this manifold."
SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a framework for inference and learning in networks of slow components by harnessing the ability of biological neurons to phase-advance their output with respect to their membrane potential. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases. The authors jointly derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network’s generalized position and momentum. The resulting model can be interpreted as a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity."
SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a general framework for improving GNN’s representation power. The key idea is to extract a local subgraph around each node and apply a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is obtained by pooling these subgraph representations. Theoretically, the authors prove that NGNN can discriminate almost all r-regular graphs where 1-WL always fails."
SP:7b8284aa82022ce73802bfc57238b0d82031b226,"This paper introduces nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. The authors apply NVI to (a) sample from a multimodal distribution using a learned annealing path, (b) learn to approximate the likelihood of future observations in a hidden Markov model, and (c) perform amortized inference in hierarchical deep generative models. The experiments show that NVI improves sample quality in terms of log average weight and effective sample size."
SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the problem of certified zeroth-order black-box optimization of a Lipschitz function f defined on a compact subset X of R, with the additional constraint that algorithms must certify the accuracy of their recommendations. The authors characterize the optimal number of evaluations of f to find and certify an approximate maximizer of f at accuracy $\varepsilon$. Under a weak assumption on X, this optimal sample complexity is shown to be nearly proportional to the integral $\infty^{X\delta}/(max(f)− f(x) + \epsilon)$. The authors also show that a certified version of the computationally tractable DOO algorithm matches these packing and integral bounds."
SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper proposes an adversarial attack on uncertainty estimation methods. The attack is based on perturbing the weights of the neural network with a small number of perturbations. The authors show that the perturbation can be of minuscule magnitude and still have a significant impact on the prediction accuracy of the model. They also show that their attack can be used to attack the vanilla softmax score, deep ensemble, and MC-dropout methods."
SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,"This paper studies the problem of community detection in dynamic networks. The authors propose a simple model for networks growing over time which they refer to as streaming stochastic block model (StSBM). Within this model, they show that voting algorithms have fundamental limitations. They also develop a streaming belief-propagation (STREAMBP) approach, for which they prove optimality in certain regimes."
SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the regularization cost induced by l2 regularization on the parameters of linear predictors. The authors identify the representation cost of certain sparse linear convNets and residual networks. They also study the reverse problem, identifying which regularizers (e.g., group quasi-norms, the k-support-norm, elastic net) can be represented by simple l2-regularization and designing the parameterizations that do so."
SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper proposes a non-parametric approach to reasoning in knowledge graphs (KGs) that is reminiscent of case-based reasoning in classical artificial intelligence (AI). The proposed method is based on finding multiple graph path patterns that connect similar source entities through the given relation. It achieves new state-of-the-art performance on the NELL-995 and FB122 datasets, outperforming all previous models, on both high-data and low-data settings. "
SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,"This paper presents a transformer-based entity linking model that combines a Transformer architecture with large scale pretraining from Wikipedia links. The model achieves the state-of-the-art on two commonly used entity linking datasets: 96.7% on CoNLL and 94.9% on TAC-KBP. The authors analyze the model design choices, including choices of negative entity candidates, transformer architecture, and input perturbations."
SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper proposes an approach to build data subsets for deep neural networks. The method uses ensemble Active Learning to estimate the uncertainty of each sample in a dataset, and then chooses only the highest uncertainty samples for training. The authors demonstrate that this improves the performance of a DNN compared to training with the entire dataset on three different image classification benchmarks. Moreover, they propose a simple technique to scale up ensembles leading to additional accuracy gains."
SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper introduces a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent’s state and the child's vote. The new mechanism designs routing via inverted dot-product attention, imposes Layer Normalization as normalization, and replaces sequential iterative routing with concurrent iteration. The proposed method improves performance on CIFAR-10/100 and ImageNet, and it performs at-par with ResNet-18 with 4x fewer parameters."
SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a method for hyperparameter optimization of deep neural network models. The main idea is to use a replica exchange technique to exchange hyperparameters between different instances of the model. The idea is that the hyper-parameters can be interpreted as controlling the level of correlated noise in training, which can be mapped to an effective temperature. Each simulation corresponds to a unique path, or history, in the joint hyper-model-parameter space. The method is applied to dropout, learning rate and batch normalization optimization."
SP:beba754d96cc441712a5413c41e98863c8abf605,"This paper studies the role of reinforcement learning (RL) in the context of machine translation (MT) tasks. In particular, the authors show that one of the most common RL methods for MT, REINFORCE, does not optimize the expected reward, and show that other methods take an infeasibly long time to converge. They also show that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation, and suggest that observed gains may be due to effects unrelated to the training signal."
SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"This paper studies the inference of the Q-value and optimal value functions in the context of reinforcement learning. In particular, the authors propose a closed-form characterization of the asymptotic variances of Q-values and optimal values, which allows them to efficiently construct confidence regions for Q and optimal functions, and to develop policies to minimize their estimation errors. This leads to a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. Numerical experiments show superior performances of the proposed exploration strategy than other benchmark approaches."
SP:d922459581c3295ff315fda6e59b9f7e9147f22d,"This paper presents a collaborative generated hashing (CGH) to improve the efficiency by denoting users and items as binary codes. Specifically, CGH is designed to learn hash functions of user and items through the Minimum Description Length (MDL) principle; thus, it can deal with various recommendation settings. CGH initiates a new marketing strategy through mining potential users by a generative step. Extensive experiments on two public datasets show the advantages for recommendations in various settings over competing baselines and analyze the feasibility of the application in marketing."
SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,"This paper proposes Adversarial Inductive Transfer Learning (AITL), a method for addressing discrepancies in input and output spaces between source and target domains. AITL utilizes adversarial domain adaptation and multi-task learning to address these discrepancies. The motivating application is pharmacogenomics where the goal is to predict drug response in patients using their genomic information. The proposed method is evaluated on the TCGA dataset consisting of gene expression data of more than 12,000 patients (without drug response outcome) and compared against state-of-the-art baselines."
SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes a mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. The authors propose to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward. The approach is tested on a collection of benchmark tests including simulated robotic locomotion.
SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a class-conditional reconstruction-based adversarial detection method that does not rely on a specific predefined adversarial attack. By reconstructing the input, the proposed method is able to accurately detect black-box and white-box FGSM, BIM, PGD, and CW attacks. Then, the authors propose a new attack to beat the defense - the Reconstructive Attack - in which the adversary optimizes not only the classification loss but also minimizes the reconstruction loss. The authors show that this attack was able to fool the detection mechanism but with a much smaller success rate than a standard attack."
SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"This paper studies the effect of the initialization and the activation function on the Neural Tangent Kernel (NTK) of deep neural networks. The authors show that only an initialization on the Edge of Chaos (EOC) leads to an invertible NTK, while all other initializations will lead to a trivial NTK. They also show that the smoothness of the activation functions plays a major role in the behaviour of NTK and provide experiments to illustrate their theoretical results."
SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method to learn sentence representations with an injection of linguistic knowledge. The authors propose to take advantage of this linguist diversity and learn to represent sentences by contrasting these diverse views. Formally, multiple views of the same sentence are mapped to close representations. On the contrary, views from other sentences are mapped further. By contrasting different linguistic views, the authors aim at building embeddings which better capture semantic and are less sensitive to the sentence outward form."
SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,"This paper investigates the effectiveness of NLP transfer learning in financial sentiment classification. The authors introduce FinBERT, a language model based on BERT, which improves the state-of-the-art performance by 14 percentage points for a financial sentiment classifier task in the FinancialPhrasebank dataset. This work is the first application of BERT for finance to the best of our knowledge and one of the few that experimented with further pre-training on a domain-specific corpus. For the classification task, the authors show that finBERT outperforms ELMo and ULMFit."
SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper introduces a new task of singing voice generation without pre-assigned scores and lyrics, in both training and inference time. The authors propose three singing schemes with different input conditions: free singer, accompanied singer, and solo singer. They also propose a BEGAN based architecture that uses GRUs and grouped dilated convolutions to learn to generate singing voices in an adversarial way. The evaluation shows that the audio quality of the generated voices still leave much room for improvement, but in terms of humanness and emotion expression the models work fine."
SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,"This paper proposes a novel adversarial defense technique that leverages a latent high-order factorization of the network. The proposed method is applied in the latent subspace, therefore resulting in dense reconstructed weights, without the sparsity or perturbations typically induced by the randomization. The method can be easily integrated with any arbitrary neural architecture and combined with techniques like adversarial training. The authors empirically demonstrate the effectiveness of the proposed method on standard image classification benchmarks and audio classification tasks."
SP:762729b64c1c1494de0f7410ea3662da61e93b6d,"This paper proposes a clustered graph transformer framework that integrates both graph attention network and transformer under an encoder-decoder architecture to address the unsmoothness issue. In spatial domain, the authors propose a gradient-based clustering method to distribute different feature extractors to regions in different contexts. In temporal domain, they propose to use multi-view position encoding to address periodicity and closeness of urban time series data. Experiments on real datasets obtained from a ride-hailing business show that the proposed method can achieve 10%-25% improvement than many state-of-the-art baselines."
SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper analyzes the Fitted Q iteration (FQI) algorithm with deep neural networks (DQN) from both algorithmic and statistical perspectives. FQI is a slightly simplified version of DQN, which captures the tricks of experience replay and target network. Under mild assumptions, the authors establish the algorithmic rates of convergence for the action-value functions of the iterative policy sequence obtained by the fitted Q iteration. In particular, the statistical error characterizes the bias and variance that arise from approximating the action value function using deep neural network, while the algorithm error converges to zero at a geometric rate. As a byproduct, the analysis provides justifications for the techniques of experience Replay and target networks, which are crucial to the empirical success of deep Q-learning."
SP:a558ffa1706ef78893528c8c23e2295a79824d2f,"This paper proposes a new Transformer-based attention mechanism for translating sentences. The authors argue that the phrases play an important role in attention and propose to represent them as hypernodes in the attention module. The proposed method has two phases: the first phase is used to attend over all word/phrase pairs, and the second phase represents the inductive bias within each phrase. The experiments show the effectiveness of the proposed method on machine translation task."
SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper presents a modular neural network architecture MAIN that learns algorithms given a set of input-output examples. MAIN consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, MAIN uses a general input tape layout together with a parallel history tape to indicate most recently used locations. Finally, it uses a memoryless controller with a length-invariant self-attention based input tape encoding to allow for random access to tape locations. The MAIN architecture is trained end-to-end using reinforcement learning. It is evaluated on five algorithmic tasks and shows that it can learn policies that generalize perfectly to inputs of much longer lengths than the ones used for training."
SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,This paper proposes a Monte Carlo Deep Neural Network arithmetic (MCDA) technique to quantify the sensitivity of neural networks to quantization in floating point arithmetic. The authors apply Monte Carlo arithmetic to the inference computation and analyze the relative standard deviation of the neural network loss. They evaluate their method on pre-trained image classification models on the CIFAR-10 and ImageNet datasets. They demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials.
SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,"This paper studies the problem of objective mismatch in model-based reinforcement learning (MBRL). Specifically, the authors study the mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. The authors propose an initial method to mitigate the mismatch issue by reweighting dynamics model training."
SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper presents a new targeted blackbox transfer-based adversarial attack method for adversarial attacks on undefended ImageNet classifiers. The proposed method leverages both class-wise and layer-wise deep feature distributions to improve the transferability of adversarial examples. In particular, the authors show that the optimal attack transfer layers have feature distributions that are class-specific and highly-separable, but are not overly-correlated with the white-box model output. They also show that highly transferable attacks induce large disruptions in the intermediate feature space of the blackbox models."
SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper introduces a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. The authors show that by utilizing the dual formulation of the WD, they can learn score functions over trajectories that can be in turn used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, the dual formulations allows them to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. They incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient (BPG) and BPG-EHSA, which can outperform existing methods in a variety of challenging environments."
SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper proposes an adaptive learning rate for interpolation with gradients (ALI-G) algorithm for deep learning. The main idea is to use the interpolation property of deep neural networks to compute the adaptive learning-rate in closed form at each iteration, which results in the Adaptive Learning-rates for Interpolation with Gradients algorithm. The learning rate is computed using a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. The authors also provide convergence guarantees of ALi-G in the stochastic convex setting. The experiments on a variety of architectures and tasks show the effectiveness of the proposed algorithm."
SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper investigates the role of bottom-up, horizontal, and top-down connections in the learning of perceptual grouping. The authors systematically evaluate neural network architectures with combinations of bottom up, horizontal and top down connections on two synthetic visual tasks, which stress low-level “Gestalt” vs. high-level object cues. They show that increasing the difficulty of either task strains learning for networks that rely solely on bottom up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-up connections rescue learning for tasks with high- level object cues by modifying coarse predictions about the position of the target object."
SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper proposes DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Inspired by the Hoyer measure (the ratio between `1 and `2 norms) used in traditional compressed sensing problems, this paper proposes a new regularizer that is almost everywhere differentiable and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. The authors also show that the proposed regularizers can be applied to both element-wise and structural pruning."
SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper proposes a variant of Adam for strongly convex functions. The main idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. Theoretical analysis shows that SAdam achieves a data-dependent O(d log T ) regret bound for strongly-convex functions, which means that it converges much faster than Adam, AdamNC, and AMSgrad in such cases, and can enjoy a huge gain in the face of sparse gradients. In addition, under a special configuration of hyperparameters, the proposed SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly concave functions, for which the authors provide the first data dependent logarithmic regret bound."
SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a lightweight and efficient BERT model, BlockBERT, which is designed to better model long-distance dependencies. The proposed model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short or long-range contextual information. The authors conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that the proposed model uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%."
SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,"This paper studies the effect of pruning on the generalization of deep neural networks. The authors show that the instability of the pruned network is more important than the final size of the network. They also show that even the pruning of unimportant parameters can lead to instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements that is compatible with the strong generalization recently observed in over-parameterized networks."
SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,This paper proposes a method for neural architecture search (NAS) with reinforcement learning (RL) to search in an embedding space by using architecture encoders and decoders. The main idea is to search over a discrete and high-dimensional architecture space and then use RL to search on the embeddings space. The proposed method is evaluated on CIFAR-10 for image classification. 
SP:e2e5bebccc76a51df3cb8b64572720da97174604,This paper proposes a homotopy training algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. It can be connected to the original system via a continuous homotope path guided by the HTA. The authors have proved the convergence of HTA for the non-convex case and existence of the homotopic solution path for the convex case. HTA has provided a better accuracy on several examples including VGG models on CIFAR-10.
SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,"This paper introduces the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention and uses this attention to update entity representations with tensor products of value vectors. The authors show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning."
SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,"This paper proposes a method for semi-supervised pose estimation with limited labeled and unlabelled data. The method is based on Conditional Variational Autoencoders (CVAEs) with circular latent representations to estimate the corresponding 2D rotations of an object. It is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing. Moreover, when just a small percentage of the training labels are missing the model can achieve comparable performance to that of the fully supervised method."
SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a method to scale up multi-agent reinforcement learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. The proposed method maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets with the best adaptability to the next stage. The authors implement EPC on a popular MARL algorithm, MADDPG, and empirically show that the approach consistently outperforms baselines by a large margin as the number of agents grows exponentially."
SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,"This paper presents a new graph-based approach to diagrammatic reasoning problems in the style of Raven Progressive Matrices (RPM). It combines three powerful ideas, namely, object-level representation, graph neural networks and multiplex graphs, to capture relations present in the reasoning task. It also shows that the proposed method has better generalisation performance."
SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,This paper proposes an adaptive noise MCMC (ATMCMC) algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. The authors use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood.
SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,"This paper studies the problem of identifying winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve a comparable accuracy to the latter in a similar number of iterations. The authors discover for the first time that the winning tickets can be identified at a very early training stage, which they term as Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Furthermore, they propose a mask distance metric to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, they leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low cost schemes, and then continuing to train merely the EB tickets towards the target accuracy."
SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"This paper proposes a method to improve the robustness of convolutional neural networks against adversarial attacks. The proposed method, called Embedding Regularized Classifier (ER-Classifier), projects the input images to a low-dimensional space to remove adversarial perturbation and stabilize the model through minimizing the discrepancy between the true label distribution and the framework output distribution. Experimental results on several benchmark datasets show that, the proposed framework achieves state-of-the-art performance against strong adversarial attack methods."
SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper proposes a neural, end-to-end model for jointly extracting entities and relations in entity recognition (NER) and relation extraction (RE) tasks. The proposed model does not rely on any hand-crafted features or external NLP tools and integrates a large, pre-trained language model. The model is fast to train, converging in approximately 1 hour or less on a single GPU for all datasets used in this study. It achieves state-of-the-art performance on 5 datasets across 3 domains."
SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper studies the problem of representation learning from a new perspective. In this paper, the authors assume that no meaningful representation of the items is given. Instead, we are provided with the answers to some triplet comparisons of the following form: Is item A more similar to item B or item C? The authors provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answers of the triplet comparison. This problem has been studied in a sub-community of machine learning by the name “Ordinal Embedding”. Previous approaches to the problem are painfully slow and cannot scale to larger datasets. "
SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,"This paper proposes a meta-learning framework to learn data values jointly with the target task predictor model. Specifically, a data value estimator (modeled by a deep neural network) is used to learn how likely each datum is used in training of the predictor model, which is trained using a reinforcement signal of the reward obtained on a small validation set. Experiments on corrupted sample discovery, domain adaptation, and robust learning demonstrate the effectiveness of the proposed method."
SP:e2c3374629cfd654b7b35e88507e65646d70470e,"This paper studies the per-layer variance of the Jacobian norm at the beginning of the training process of neural networks. The authors study the connection between the network’s architecture and initialization parameters, to the statistical properties of the gradient in random fully connected ReLU networks, through the study of the per layer Jacobian in finite-sized networks. They show that while the variance of Jacobian squared norm is exponential in depth for ResNets and polynomial for DenseNets, there exists an initialization strategy for both, such that the norm is preserved through arbitrary depths, preventing exploding or decaying gradients in deep networks. In addition, the authors also show that the statistics are a function of the architecture and the layer size, but surprisingly, not the layer's depth."
SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes a reinforcement learning approach for optimizing compilers for neural networks. Specifically, the authors propose to learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. They also develop an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experiments on real-world deep models show that CHAMELEON can reduce the time for compilation significantly, and improves the quality of the code."
SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper studies the problem of certified robustness for top-k predictions, i.e., adversarial robustness against adversarial perturbations. The authors propose a method based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. They derive a tight robustness in the $\ell_2$ norm for topk predictions when using randomized smoothed with Gaussian noise. They also empirically evaluate their method on CIFAR10 and ImageNet."
SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the relationship between the gradient signal to noise ratio (GSNR) and the generalization ability of deep neural networks (DNNs). The GSNR is defined as the ratio between its gradient’s squared mean and variance, over the data distribution. The authors show that larger GSNRs during training process leads to better generalization performance. They also show that, different from that of shallow models, the gradient descent optimization dynamics of DNNs naturally produces large GSNr during training, which is probably the key to DNN's remarkable generalisation ability."
SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes an embedding-based framework for reasoning over queries with logical disjunctions (e.g., conjunctions, existential quantifiers) in massive and incomplete knowledge graphs (KGs). The authors propose to represent queries as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to the set of answer entities of the query, and output entities closest to their nearest boxes. This approach is capable of handling all types of queries scalably and accurately. The authors demonstrate the effectiveness of QUERY2BOX on three large KGs."
SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,"This paper studies the problem of efficient SGD for convex and non-convex optimization. In particular, the authors propose a gradient estimator that is biased but consistent. They show that consistent gradient estimators result in the same convergence behavior as unbiased ones in the strongly convex setting. They also show that unbiased estimators can be as expensive to compute as the full gradient because of the fact that the training examples are interconnected."
SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. They can quickly get a specialized sub-network by selecting from the OFA network without additional training. They also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning. It can obtain a surprisingly large number of subnetworks that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently."
SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper presents a set of extensions to Neural Module Networks (NMNs) for answering questions that require symbolic reasoning. Specifically, the authors introduce probabilistic modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates, and propose an unsupervised auxiliary loss to help extract arguments associated with the events in text. They also show that a limited amount of heuristically-obtained question programs and intermediate module output supervision provides sufficient inductive bias for accurate learning. The proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset."
SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,"This paper studies the problem of pruning neural networks at initialization from a signal propagation perspective. The authors note connection sensitivity as a form of gradient and characterize initialization conditions to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. Moreover, they analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. Experiments are conducted to show the effectiveness of the proposed method."
SP:d5899cba36329d863513b91c2db57675086abc49,"This paper studies the problem of training a priori sparse neural networks, i.e. replacing dense and convolutional layers with sparse cascades with topologies selected ahead of time. The authors propose a new sparse neural network initialization scheme that allows them to explore the space of very deep sparse networks. They evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. They develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. They then derive a set of requirements that make a good topology and arrive at a single topology that satisfies all of them."
SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,"This paper proposes a new initialization scheme for recurrent neural networks (RNNs) based on the mean field theory of signal propagation in LSTMs and GRUs. The authors derive the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians and derive a novel initialization scheme that eliminates or reduces training instabilities. They demonstrate the efficacy of their initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower."
SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,This paper proposes a siamese network to improve the discriminative power of convolutional neural networks on a pair of neighboring scene images. It exploits semantic coherence between this pair to enrich the feature vector of the image for which we want to predict a label. Empirical results show that this approach provides a viable alternative to existing methods.
SP:99c10e038939aa88fc112db10fe801b42360c8dc,This paper proposes a method for self-supervised monocular depth estimation using geometry as the only source of supervision. The proposed method leverages fixed pretrained semantic segmentation networks to guide the representation learning via pixel-adaptive convolutions. The authors also propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Experiments on challenging real-world data shows that the proposed architecture consistently improves the performance of different monodepth architectures.
SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,"This paper considers label-flipping attacks, a type of data poisoning attack where an adversary relabels a small number of examples in a training set in order to degrade the performance of the resulting classifier. In this paper, the authors propose a strategy to build linear classifiers based on deep features that are certifiably robust against a strong variant of label- flipping attacks, where the adversary can target each test example independently. For each test point, the classifier makes a prediction and includes certification that its prediction would be the same had some number of training labels been changed adversarially. The approach leverages randomized smoothing, a technique that has previously been used to guarantee test-time robustness to adversarial manipulation of the input to a classifier, and the authors obtain these certified bounds with no additional runtime cost over standard classification. On the Dogfish binary classification task from ImageNet, the baseline undefended classifier achieves no more than 29.3% accuracy; the proposed approach achieves 64.2% certified accuracy against the same adversary."
SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper proposes to use differential privacy to improve the performance of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. The authors provide a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving the detection. They also generalize the idea of measuring model loss to backdoor attack detection, which further improves the performance via differential privacy."
SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper studies the problem of calibration of uncertainty prediction for regression tasks. The authors show that the existing definition for calibration of a regression uncertainty (Kuleshov et al., 2018) has severe limitations in distinguishing informative from non-informative uncertainty predictions. They propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach inspired by reliability diagrams used in classification tasks. They also propose a simple scaling-based calibration that preforms well in our experimental tests."
SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a new model, HUBERT, which combines the structured-representational power of Tensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional Transformer language model. The authors show that BERT cannot effectively transfer its knowledge across NLP tasks, even if the two tasks are fairly closely related. To resolve this problem, they propose a decomposition layer on top of BERT which disentangles symbols from their roles in BERT’s representations. In addition, they use TPRs to construct word representations by binding together two separated properties, the word's (semantic) content and its structural (grammatical) role. Experiments are conducted on GLUE benchmark and HANS dataset."
SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"This paper proposes a multi-agent reinforcement learning (MARL) and hierarchical RL (HRL) approach for humanoid navigation. The main idea is to learn a low-level controller for each agent that is task-agnostic and can be shared by higher-level policies. The lower-level controllers are learned using goal-conditioned policies, and the high-level policy is learned using HRL. The proposed approach is evaluated on two multiagent pursuit and soccer environments, and shows promising results."
SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,This paper proposes a method to improve the performance of graph neural networks (GNNs) by introducing a spatial representation of the graph. The proposed method is motivated by the success of GNNs in analyzing point-cloud data. The spatial representation is obtained by a graph embedding method and is used in the local feature extractor of the GNN to distinguish similar local structures in different locations. The GNN infers the topological structure of the graphs from the spatial distribution of the locally extracted feature vectors. A new graph pooling method is proposed and it is shown that the proposed method achieves competitive or better results in comparison with the state-of-the-art methods.
SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,This paper proposes frequency pooling (F-pooling) to improve the performance of convolutional neural networks (CNNs). The authors provide a formal definition of shift-equivalent when down sampling is involved and propose a strict shift equivalent and anti-aliasing pooling method. The proposed method is achieved by (inverse) Discrete Fourier Transform (DFT). Experiments on image classifications show that F-Pooling improves accuracy and robustness w.r.t shifts of CNNs.
SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,"This paper proposes a method to improve the generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. The authors also consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. They demonstrate the superiority of their method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks."
SP:31772a9122ec998c7c829bc4813f6147cdc30145,"This paper proposes an explanation approach for image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification. The authors propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match. They find that their explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition."
SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes a generative model for time-domain waveforms, which is trained with maximum likelihood without density distillation and auxiliary losses. The proposed method is called WaveFlow, and it is a unified view of flow-based models for raw audio, including autoregressive flow (e.g., WaveNet and WaveGlow) and bipartite flow. The authors systematically study these likelihood-based generative models in terms of test likelihood and speech fidelity. They demonstrate that WaveFlow can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms."
SP:963e85369978dddcd9e3130bc11453696066bbf3,"This paper proposes a new generative model, GT-GAN, that transforms the input graphs into their target output graphs. The proposed method consists of a graph translator equipped with innovative graph convolution and deconvolution layers to learn the translation mapping considering both global and local features. A new conditional graph discriminator is proposed to classify the target graphs by conditioning on input graphs while training. Extensive experiments on multiple synthetic and real-world datasets demonstrate that the proposed method significantly outperforms other baseline methods in terms of both effectiveness and scalability."
SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a new neural sequence modeling unit, called METAGROSS (Meta Gated Recursive Controller), which is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. The authors postulate that their proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks)."
SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a new self-supervised learning objective for speech recognition. The proposed objective, called local prior matching (LPM), leverages a strong language model to provide learning signal given unlabeled speech. The authors show that LPM improves the WER of the proposed approach on a clean and noisy test set by 26% and 31% relative on the clean and noise test set respectively."
SP:e6af249608633f1776b608852a00946a5c09a357,This paper studies the problem of fair and robust model training in the presence of data poisoning. The authors propose a generative adversarial network (GAN) framework that combines two approaches: (1) fairness discriminator that distinguishes predictions w.r.t. one sensitive group from others and (2) robustness discriminator which distinguishes training data with predictions from a clean yet small validation set. They show that the proposed framework is robust to the poisoning and can be adjusted to maintain reasonable accuracy and fairness even if the validation set is too small.
SP:6306417f5a300629ec856495781515c6af05a363,"This paper presents a method for learning point clouds in a Lagrangian world space using a static grid and a moving particle-based material space. The method is motivated by the natural flow phenomena in fluid mechanics. The proposed method is able to evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. The authors demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with SOTA performance."
SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,"This paper proposes a new lens for studying gradient clipping, namely, robustness: informally, one expects clipping to mitigate the effects of noise, since one does not overly trust any single sample. Surprisingly, the authors prove that for the common problem of label noise in classification, standard gradient clipping does not in general provide robustness. On the other hand, they show that a simple variant of gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function. As a special case, this yields a simple, noise-robust modification of the standard cross-entropy loss which is empirically established that gradient clipping by itself does not suffice to endow even simple models with label noise robustness; however, this simple variant resolves this issue."
SP:414b06d86e132357a54eb844036b78a232571301,"This paper proposes SAIL, an imitation learning method that uses state alignment from both local and global perspective. The state alignment is achieved by training the imitator to follow the state sequences in expert demonstrations as much as possible. The proposed method is evaluated on standard imitation learning and imitation learning settings where the expert and imitators have different dynamics models."
SP:91761d68086330ce378507c152e72218ed7b2196,"This paper proposes Deep Gradient Boosting (DGB), a simple extension of SGD that allows for finer control over the intrinsic generalization properties. The key idea of DGB is that back-propagated gradients inferred using the chain rule can be viewed as pseudo-residual targets of a gradient boosting problem. At each layer of a neural network the weight update is calculated by solving the corresponding boosting problem using a linear base learner. The resulting weight update formula can also be seen as a normalization procedure of the data that arrives at each layer during the forward pass. When implemented as a separate input normalization layer (INN) the new architecture shows improved performance on image recognition tasks when compared to the same architecture without normalization layers."
SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,"This paper proposes a novel approach to differentiable architecture search (DARTS) by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, it performs operation search in a subset of channels while bypassing the held out part in a shortcut. It alleviates it using edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, it can be trained with a larger batch size and enjoys both faster speed and higher training stability."
SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a hybrid method that combines the best aspects of gradient-based methods and deep reinforcement learning (DRL) for solving complex control tasks with high efficacy. The authors base their algorithm on the deep deterministic policy gradients (DDPG) algorithm and propose a simple modification that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. Empirical results show that the proposed method boosts the performance of DDPG without sacrificing its robustness to local minima.
SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"This paper proposes a method to learn a task-agnostic world graph, which is an abstraction that enables agents to focus exploration on a subspace of the environment. The nodes of a world graph are important waypoint states and edges represent feasible traversals between them. The proposed method has two learning phases: 1) identifying world graph nodes and edges by training a binary recurrent variational autoencoder (VAE) on trajectory data and 2) a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. The experimental results show that using world graphs significantly accelerates RL, achieving higher reward and faster learning."
SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for reverse engineering neural networks. The main idea is to use a neural network to construct a target function to reveal the structure of the target function. The proposed method, called white box network (WBN), arranges function blocks to construct the target functions and uses discretized layers to render the model interpretable without disordering the function blocks. The authors also introduce an end-to-end PathNet structure through this discretization. "
SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for self-supervised imitation learning in the setting where the goal is to learn a policy that maximizes the likelihood of actions it actually took in its own previous rollouts conditioned on the goal being the state that it actually reached. The method is based on the observation that, in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. The authors propose a simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. Empirical results show that it performs competitively with more complex RL methods on a range of challenging goal reaching problems."
SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper proposes a new asynchronous SGD algorithm, Zeno++, which is robust to Byzantine failures of the workers. The key idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. The authors prove the convergence of the algorithm for non-convex problems under Byzantine failures. Experimental results show that the proposed algorithm outperforms existing approaches."
SP:d16ed9bd4193d99774840783347137e938955b87,"This paper proposes a method to generate adversarial examples that manipulate semantically meaningful image-based visual descriptors, such as color and texture, in order to generate effective and photorealistic adversarial example. The proposed method is able to generate semantically aware perturbations that are effective against JPEG compression, feature squeezing and adversarially trained models. The authors also conduct comprehensive user studies to show that the generated semantic adversarial images are more realistic to humans than other attacks."
SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a method for few-shot learning of entity recognition. The authors propose to use a sparse memory to store information about named entities with two levels of representation: dense and sparse. The sparse memory is used to locally update a small set of memory entries and the dense representation is used by a trainable controller that manages the transition of the sparse memory. This enables a model to incrementally learn in the presence of a few observations per class.
SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,This paper presents an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. The approach is based on jointly learning both the underlying motor primitive and recomposing these primitives to form the original demonstration. The learned primitives are then used in a hierarchical reinforcement learning setup to compose the primitives in a robotic manipulation tasks like reaching and pushing. The proposed approach is evaluated on the MIME dataset.
SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,"This paper proposes a general algorithm for single episode transfer among MDPs with different stationary dynamics, which is a challenging goal with real-world significance that deserves increased effort from the transfer learning and RL community. The method, Single Episode Policy Transfer (SEPT), trains a probe policy and an inference model to discover a latent representation of dynamics using very few initial steps in a single test episode, such that a universal policy can execute optimal control without access to rewards at test time. In diverse experimental domains with a single episode test constraint, the method significantly outperforms existing adaptive approaches and shows favorable performance against baselines for robust transfer."
SP:f2f1aff9a5b91d748b24fee0155367f650401aab,"This paper presents an empirical study on synchronous AlphaZero learning with two- and three-head neural networks. Specifically, the authors use the game of Hex as a test domain to evaluate the effectiveness of the proposed three-headed network architecture in AlpahZero learning. They show that the architecture is also advantageous at the zero-style iterative learning, producing neural network models stronger than those from the two-head counterpart in the same MCTS. "
SP:89d6d55107b6180109affe7522265c751640ad96,"This paper proposes a method for transfer learning in reinforcement learning, where the goal is to adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. The proposed method is based on the idea of learning to combine adaptation reward with environmental reward, which is called Adapt-to-Learn. The authors show that the proposed method leads to a significantly reduced sample complexity of transferring the policies between the tasks."
SP:626021101836a635ad2d896bd66951aff31aa846,This paper introduces a general framework for building scale-equivariant convolutional networks with steerable filters. The authors develop scale-convolution and generalize other common blocks to be scale-Equivariant. They demonstrate the computational efficiency and numerical stability of the proposed method. They compare the proposed models to the previously developed methods for scale equivariance and local scale invariance.
SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,"This paper presents a point-based unpaired shape completion framework that can be applied directly on raw partial scans to obtain clean and complete point clouds. At the core of the algorithm is an adaptation network acting as a generator that transforms latent code encodings of the raw point scans, and maps them to latent codes of complete and clean object scans. The two latent spaces regularize the problem by restricting the transfer problem to respective data manifolds. The authors evaluate the approach qualitatively on several real-world datasets (ScanNet, Matterport3D, KITTI, and 3D-EPN shape completion dataset, and demonstrate realistic completions under varying levels of incompleteness."
SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generative impersonation attacks against authentication systems. The authors cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. Their analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. Based on these insights, they design practical learning approaches and show that they result in models that are more robust to various attacks on real-world data."
SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper proposes a new adversarial learning framework for adversarial training of deep neural networks. The proposed approach is motivated by the fact that the high dimensional distribution is poorly represented by limited data samples. The authors propose a novel and efficient algorithm that trains a robust model with sensible adversarial examples, without a significant drop in natural accuracy. They also theoretically establish that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under the framework. Their model on CIFAR10 yields state-of-the-art results against various attacks."
SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. The proposed method trains multiple networks simultaneously by employing discriminators to distinguish the feature maps distributions of different networks. By training a network to fool the corresponding discriminator, it can learn the other network’s feature map distribution. Also, the authors propose a novel cyclic learning scheme for training more than two networks together."
SP:e43fc8747f823be6497224696adb92d45150b02d,"This paper proposes a new word embedding model for the task of sentiment analysis. The proposed model is based on the idea of group homomorphism, which is used to construct the objective function of the word embeddings, and the maximum likelihood estimator and the Bayesian estimator are used to determine the parameters of the model. Experiments are conducted on various tasks to evaluate the working performance."
SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training strategy for the noisy training data, which they call Prestopping. The first phase retrieves an initial set of true-labeled samples as many as possible, and the second phase, “learning from a maximal safe set,” completes the rest training process only using the true labels with high precision. The authors conduct extensive experiments using various real-world and simulated noisy data sets to verify that either Prestopping or Prestopping+ achieve the lowest test error among the seven compared methods, thus significantly improving the robustness to diverse types of label noise."
SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes an approach to generalize RL policies to unseen actions in the presence of novel actions. The approach is based on representation learning and reinforcement learning, where an agent learns representations of the actions in an unsupervised manner, and then uses them to train a policy that can generalize to the unseen set of actions. In particular, the proposed approach is to learn a representation of the action space, which is then used to train an RL policy. The paper also proposes a regularization term that encourages the policy to learn actions that are similar to the representations learned in the representation learning phase. The method is evaluated on a set of Mujoco tasks, where it is shown that the proposed method outperforms the baselines. "
SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes a method for storing word embedding matrix during training and inference in a highly efficient way. The proposed method, word2ket, is a combination of word2vec and GloVe, which is based on the idea that discrete sequence of words can be represented as a sequence of continuous vectors, which can be stored in memory. The authors propose two related methods, i.e., word2Ket and word2ketXS, which are used to store word embeddings in memory and compute the embedding vectors. Theoretical results show that the proposed method achieves a 100-fold or more reduction in the space required to store embedding vector with almost no relative drop in accuracy in practical natural language processing tasks."
SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning (IL) that learns a policy conditioned on a behavior description that can be precisely modulated. Specifically, the authors propose a method called Behavioral Repertoire Imitation Learning (BRIL) which learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The authors apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors."
SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis, which suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. This paper investigates the structure of the winning ticket, and finds that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. The authors also provide insights into the structure with supporting evidence. "
SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes an unknown aware deep neural network (UDN) to tackle the challenge of detecting unknown objects in image classification. The key idea of UDN is to enhance existing CNNs to support a product operation that models the product relationship among the features produced by convolutional layers. Missing a single key feature of a target class will greatly reduce the probability of assigning an object to this class. UDN uses a learned ensemble of these product operations, which allows it to balance the contradictory requirements of accurately classifying known objects and correctly rejecting unknowns."
SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a method for variational inference (VI) for approximate Bayesian inference that is particularly promising for highly parameterized models such as deep neural networks. A key challenge of VI is to approximate the posterior over model parameters with a distribution that is simpler and tractable yet sufficiently expressive. In this work, the authors propose a method to train highly flexible variational distributions by starting with a coarse approximation and iteratively refining it. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. The authors demonstrate theoretically that their method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks."
SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,"This paper proposes a method for contextual generation of categorical sequences using reinforcement learning. The method is based on a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. The number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to reduce gradient variance. The proposed method is evaluated on two tasks: neural program synthesis and image captioning."
SP:ab51af66e626b1b03bbf0de7a5237370e941925c,"This paper proposes a stochastic goal recognition control (S-GRC) problem with two main stages: (1) deceptive opponent modeling based on maximum entropy regularized Markov decision processes (MDPs) and (2) goal recognition under proactively static interdiction. The authors propose to use the worst case distinctiveness (wcd) as a measure of the nondistinctive path without revealing the true goals, the task of S-G RC is to interdict a set of actions that improve or reduce the wcd. They empirically demonstrate that their proposed approach control the goal recognition process based on opponent’s deceptive behavior."
SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method to generate large mini-batch sizes for training GANs. The method is inspired by the use of Coreset-selection in active learning, which compresses a large batch of samples from the prior and then compresses that batch using Coreset selection. To create effectively large batches of ‘real’ images, the authors create a cached dataset of Inception activations of each training image, and randomly project them down to a smaller dimension, and then use coreset selection on those projected activations at training time. The authors conduct experiments showing that this technique substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it allows GANS to reach a new state of the art in anomaly detection."
SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper investigates the information plane of RNNs trained with maximum likelihood and contrastive loss. The authors show that RNN with maximum-likelihood are sub-optimal in the Information Plane, i.e. they extract additional information that is not relevant for predicting the future. They also show that constraining past information by injecting noise into the hidden state can improve the ability of the RNN to extract predictive information. "
SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper proposes a method to mitigate the delusional bias in Q-learning by training Q-approximators with labels that are “consistent” with the underlying greedy policy class. The authors introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class and propose a search framework that allows multiple Q approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-Learning in a variety of Atari games, sometimes dramatically."
SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial attention and thus is applicable to scenes with a large number of objects without performance degradations."
SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes depthwise separable convolution (FALCON) to compress convolutional neural networks. FALCON is derived by interpreting existing convolution methods based on depthwise convolution using EHP, our proposed mathematical formulation to approximate the standard convolution kernel. It further improves the accuracy while sacrificing a bit of compression and computation reduction rates."
SP:35d45ed014320d8dff22f3531f805d15fa91dafb,"This paper proposes to improve Batch Normalization (BN) by introducing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy, recognizing and validating the powerful regularization effect of Ghost Batch normalization for small and medium batch sizes, examining the effect of weight decay regularization on the scaling and shifting parameters, and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. Experiments on CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet demonstrate the effectiveness of the proposed improvements."
SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,This paper proposes a federated generative model that can be used to identify and fix problems in the data. The proposed approach is based on federated methods and formal differential privacy guarantees. The authors demonstrate the effectiveness of the proposed approach on text and image datasets. The paper is well-written and easy to follow. 
SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,"This paper proposes a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. The proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as source material to a deep network. The synthesis is performed by the deep network that controls the style of the motion material and modifies it to become natural. On skeleton datasets with diverse motion, the proposed method outperforms existing parametric baselines. The generated sequences are useful as subgoals for actual physical execution in the animated world."
SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper studies hierarchical explanation of neural network predictions to explain how the model handles semantic compositions. The authors identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. They propose two methods to quantify the importance of words and phrases: sampling and contextual decomposition (SCD) and Sampling and Occlusion (SOC). Experiments on multiple datasets and models show that the explanation algorithms generate informative hierarchical explanations, help to extract classification rules from models and enhance human trust of models."
SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,This paper proposes a saliency map method for deep convolutional neural networks (CNN) that is much more efficient than popular gradient methods. It is also quantitatively similar and better in accuracy. The technique works by measuring information at the end of each network scale which is then combined into a single saliency maps. The saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. The authors visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information.
SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation that explicitly models the positions of generated words. Specifically, the position is modeled as a latent variable, and training with heuristic searched positions with MC algorithms. The proposed method is evaluated on machine translation and paraphrase generation tasks, outperforming several strong baselines."
SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,"This paper proposes a new generative adversarial network (GAN) architecture, called Random Path Generative Adversarial Network (RPGAN), which is an alternative GAN that can serve as a tool for generative model analysis. In particular, the latent space of RPGAN consists of random paths in a generator network. This design allows to understand factors of variation, captured by different generator layers, providing their natural interpretability. Experiments on standard benchmarks demonstrate that RPGAN reveals several interesting insights about the roles that different layers play in the image generation process."
SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes DeepSphere, a spherical convolution method based on a graph representation of the sampled sphere. The proposed method is based on graph Laplacian-based convolution, where the number of vertices and neighbors in the graph controls the tradeoff between cost and equivariance (which is linked to performance). As computational cost and memory consumption scales linearly with number of pixels, DeepSphere scales to spherical maps made of millions of pixels. Experiments show state-of-the-art performance on relevant problems."
SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,"This paper studies the problem of learning invariant representations for domain adaptation (DA). In particular, the authors propose a new bound on the risk of compression of representations, which they show can be better controlled when the compression risk is taken into account. They also show that weighting representations can align representation distributions without impacting their adaptability. Finally, they show how their framework can be applied for studying adaptation robustness to adversarial attacks."
SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper presents an approach for learning to infer user interface attributes from images. Specifically, given an input image created by a designer, we learn to infer its implementation which when rendered, looks visually the same as the input image. To achieve this, we take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, we also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values."
SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning, in which the goal is to produce a model that is not only accurate but also adversarially robust. The authors first show that robust networks contain robust feature extractors, and then train classifiers on top of these extractors to produce new models that inherit the robustness of their parent networks. They then consider the case of “fine tuning” a network by re-training end-to-end in the target domain. They show that lifelong learning strategies can be used to produce accurate and robust models without the cost of adversarial training."
SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,"This paper proposes a neural iterated learning (NIL) algorithm to encourage the emergence of compositional language in a multi-agent communication game. The authors propose a probabilistic model of NIL and an explanation of why the advantage of compositionality exists. They show that the emergent languages with high topological similarity can incrementally advantage emergent emergent communication protocols. The experiments confirm their analysis, and also demonstrate that the emerged languages largely improve the generalizing power of the neural agent communication."
SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes Adversarial Variational Inference and Learning (AdVIL) to perform inference and learning in a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The authors provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent (SGD) and proven convergent under certain conditions. Empirical results show that the proposed method can deal with a broad family of MRFs and outperforms both the standard contrastive divergence method and the black-box NVIL algorithm."
SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,"This paper proposes a simple reward function for goal-conditioned reinforcement learning (GCRL). The reward function is a simple indicator reward function, which is used when the robot’s observation exactly matches a target goal observation. The authors also propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. They show comparable performance between their method and an oracle which uses the ground-truth state for computing rewards. They also show that their method can perform complex tasks in continuous state spaces."
SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper considers the robustness verification problem for Transformers, which is an important tool for understanding model behavior and obtaining safety guarantees. The authors propose a new method to verify the prediction behavior of neural networks. The proposed method tackles the challenges of cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. The certified bounds computed by the proposed method are significantly tighter than those by naive Interval Bound Propagation. Quantitative and qualitative analyses further show that the bounds are meaningful and can reflect the importance of different words in sentiment analysis."
SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a weakly supervised pretraining method to encourage pretrained language models to learn entity-level knowledge. The proposed method uses minimal entity information during pretraining and does not introduce additional computation, memory or architectural overhead for downstream task fine-tuning. The trained model demonstrates strong performance on a probing fact completion task and two entity-related NLP tasks."
SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper tackles the problem of discovering novel classes in an image collection given labelled examples of other classes. The authors propose to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. They propose to use self-supervised learning to train the representation from scratch on the union of labelled and unlabelling data, and use rank statistics to transfer the model’s knowledge of the labelled classes to the task of clustering the unlabeled images. Finally, the authors train the data representation by optimizing a joint objective function on the labelled/unlabelled subsets of the data."
SP:068c4e93c135968aef2637d2bfcba727a3c0f001,"This paper proposes a method for visual planning based on hallucinative topological memory (HTM). In particular, the authors propose to use contrastive predictive coding (CPC) to learn a contrastive energy function and a conditional VAE model to generate hallucinated images for building the connectivity graph. The proposed method is evaluated in simulated environments and shows better performance than the baseline methods."
SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes a framework of Adversarial Filters to investigate model-based reduction of dataset biases. Specifically, the authors propose AFLITE, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. They apply it to popular benchmarks that are practically solved and present filtered counterparts as new challenge datasets where the model performance drops considerably (e.g., from 84% to 24% for ImageNet and from 92% to 62% for SNLI) while human performance remains high."
SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper proposes a new way of training prototypical few-shot models for just a single class. In particular, it introduces a “null class” centered around zero, and enforcing centering with batch normalization. It also proposes a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples’ distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available."
SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. It first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. The proposed method can be applied to any existing embedding methods to be applied before it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. It has been evaluated on a number of popular graph datasets for both transductive and inductive tasks."
SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes to predict pixels relatively, by predicting new pixels relative to previously generated pixels (or pixels from the conditioning context, when available). The authors show that this form of prediction fare favorably to its absolute counterpart when used independently, but their coordination under an unified probabilistic model yields optimal performance. Experiments on multiple benchmarks for unconditional image generation, image colorization, and super-resolution indicate that the presented mechanism leads to improvements in terms of likelihood compared to the absolute prediction counterparts."
SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper studies the problem of estimating the stationary distribution of the transition operator of a Markov chain, which is an important problem in reinforcement learning and Monte Carlo methods. The authors propose a novel algorithm GenDICE for general stationary distribution correction estimation, which can handle both the discounted and average stationary distribution given multiple behavior-agnostic samples. They prove its consistency under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems, including off-line PageRank and off-policy policy evaluation."
SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper proposes a method for training models that are less sensitive to spurious patterns in natural language processing (NLP) tasks. The authors propose to use human feedback to improve the performance of NLP models. Specifically, the authors first provide a set of documents and their initial labels to humans, and then the humans are tasked with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. The proposed method is evaluated on sentiment analysis and natural language inference tasks. "
SP:b720eb5b6e44473a9392cc572af89270019d4c42,This paper proposes a method to characterize the spatial frequency and orientation tuning of channels in pre-trained deep convolutional neural networks (CNNs) by applying grating stimuli of different spatial frequencies and orientations. The authors show that the behavior of CNN channels as spatial frequency/orientation selective filters can be used to link basic human visual perception models to their characteristics. They conclude that sensitivity to spatial frequencies that have lower contrast masking thresholds and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features.
SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) to explicitly model link type correlations between link labels (e.g., DDI types) for drug-drug interactions (DDIs) prediction task. The authors formulate the DDI prediction task as a structure prediction problem, and introduce a new energy-based model where the energy function is defined by graph neural networks. Experiments on two real world DDI datasets demonstrate that the proposed GENN is superior to many baselines without consideration of link type."
SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a self-supervised method to learn discrete representations of audio segments through a wav2vec-style self supervised context prediction task. The proposed method uses either a Gumbel-Softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.
SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic framework for collaborative filtering on implicit data. The critic learns to approximate the ranking scores, which in turn improves the traditional MLE-based nonlinear LVMs with the learned ranking-critical objectives. To make it practical and efficient, the authors introduce a few techniques: a feature-based critic to reduce the number of learnable parameters, posterior sampling as exploration for better critic estimates, and pre-training of actor and critic for fast convergence. The experimental results on three large-scale datasets demonstrate the effectiveness of the proposed method."
SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a multi-step TD-learning method for off-policy reinforcement learning. The main idea is to use truncated Q-functions to represent the return for the first n steps of a target-policy rollout, and shifted Q-function for the remainder of the rollout. The authors prove that the combination of these short-and long-term predictions is a representation of the full return, leading to the Composite Q-learning algorithm. The efficacy of the proposed method is evaluated in the tabular case and in the function-approximation setting."
SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a reinforcement learning system for dexterous manipulation tasks. The proposed system is based on a combination of vision-based and reward-based reinforcement learning. The paper provides a detailed analysis of the challenges associated with this learning paradigm, including resets, on-board perception, and reward functions. The authors propose simple and scalable solutions to these challenges, and then demonstrate the efficacy of the proposed system on a set of dexterous robotic manipulation tasks, showing that it can learn a variety of vision based skills with a real-world three-fingered hand."
SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the problem of adversarial robustness of adversarially trained neural networks. The authors show that with a sufficiently large amount of unlabeled data, adversarial training can be improved with better adversarial generalization. The main idea is to use the stability part of the expected robust risk decomposition, which measures the prediction stability in the presence of perturbations, and the accuracy part, which evaluates the standard classification accuracy. The stability part does not depend on any label information, so it can be optimized using the data. The paper also shows that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018), adversarial learning can be almost as easy as the standard generalization in supervised learning if the number of unlabelled data is provided."
SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a method to estimate the advantage of an actor-critic algorithm based on the order statistics over the path ensembles of the environment. The proposed method is motivated by the idea of regulatory fit in psychology, which states that when people engage in goal pursuit activities in a manner that fits their regulatory orientation, they feel right about what they are doing. The regulatory focus is implemented in the context of reinforcement learning via the order-statistics over path ensemble, which formed nonlinear combinations of different n-step advantage estimators. The authors incorporated these estimators into three widely used algorithms including A2C, TRPO and PPO and verified the effectiveness of the approach on various domains."
SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in higher precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of execution with almost no accuracy loss. The experiments indicate that PG achieves excellent results on CNNs, including static compressed networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4x less compute on ImageNet."
SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper introduces a new problem called wildly unsupervised domain adaptation (WUDA), which is a more realistic and challenging problem setting where classifiers have to be trained with noisy labeled data from SD and unlabeled data from the target domain (TD). The authors show that WUDA ruins all UDA methods if taking no care of label noise in SD, and propose a Butterfly framework, which maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled to unlabeled, and SD to TD-distributional) and the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving the problem. Experimental results demonstrate that the proposed method significantly outperforms existing baseline methods."
SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,"This paper studies the problem of image-based model-free reinforcement learning (i.e., RL from high-dimensional images) in the context of control tasks. In particular, the authors propose an algorithm that combines an actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across model-based RL algorithms on many challenging control tasks, including Mujoco tasks. The main contributions of the paper are the following:  1. The authors propose a new approach to learn a latent representation of the reward function that can be used in combination with a control policy.  2. They show that the reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image based RL.  3. They propose an off-policy actor critic algorithm that trains an encoder and a decoder and matches the performance of both model free RL algorithms and model based RL algorithms."
SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for accelerating training and improving generalization over the original dropout. The key idea is to create multiple dropout samples at the dropout layer, where each dropout sample is selected from the input in each training iteration. The loss is calculated for each sample, and then the sample losses are averaged to obtain the final loss. This technique can be easily implemented without implementing a new operator by duplicating a part of the network after the drop out layer while sharing the weights among the duplicated fully connected layers. Experimental results show that the proposed method significantly accelerates training by reducing the number of iterations until convergence for image classification tasks."
SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,"This paper proposes a label sensitive gate (LSG) training strategy to train CNNs to disentangle filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. In this way, such training strategy imposes each filter’s attention to just one or few classes, namely class-specific. LSG training encourages top convolutional each filter to focus on fewer classes, which implies its feature map can localize a class better. The paper conducts experiments below to verify the effectiveness of the proposed method."
SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,"This paper proposes a framework for learning nearly decomposable Q-functions (NDQ) via communication minimization. In this framework, agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between agents’ action selection and communication messages while minimizing the entropy of messages between agents. The authors show how to optimize these regularizers in a way that is easily integrated with existing value-function factorization methods such as QMIX. Finally, the authors demonstrate that, on the StarCraft unit micromanagement benchmark, their framework significantly outperforms baseline methods and allows us to cut off more than 80% of communication without sacrificing performance."
SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a new type of problem called Programming Puzzles and a framework for generating a large set of hard problems that can both expose the weaknesses of existing solvers and which can be used in a GAN-like setup to train better solvers. A programming puzzle is a short program for a Boolean function f(x) with the goal of finding an input that makes f return True. The authors propose an algorithm called “Troublemaker” which can generate puzzles adaptively targeted at any given puzzle-solver. It generates a diverse set of puzzles that are difficult for the solver. In the experiments, Troublemaker learns to generate challenging problems for a variety of state-of-the-art puzzle solving techniques."
SP:627b515cc893ff33914dff255f5d6e136441d2e2,"This paper proposes a method for hierarchical reinforcement learning that decomposes the policy into a set of primitives, where each primitive can decide for themselves whether they wish to act in the current state. The authors use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs to make a decision, and the primitive that requests the most information about the state acts in the environment. The overall objective function consists of 3 terms: the expected return from the standard RL objective, which is distributed to the primitives according to their participation, individual bottleneck terms leading the individual primitives to focus on specific parts of the state space, and a regularization term applied to the combined model, Lreg. Experiments demonstrate that this policy architecture improves over both flat and hierarchical policies."
SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a method for learning a latent state representation of the dynamics of a reward prediction model and planning in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which is shown to be the only necessary information for successful planning. The proposed method is evaluated in the multi-pendulum and multi-cheetah environments where the agent has access to several pendulums or cheetah but only one of them produces rewards. In these environments, it is important for the agent to construct a concise latent representation to filter out irrelevant observations while existing model-based methods fail."
SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes two parameter reduction techniques to reduce the memory consumption and increase the training speed of BERT-large. The first method is based on reducing the number of encoders/decoders, and the second one is a self-supervised loss that focuses on modeling inter-sentence coherence. Experiments on GLUE, RACE, and SQuAD show the effectiveness of the proposed methods."
SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,"This paper proposes a spatio-temporal cache mechanism that enables learning spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture enables a single model to support tasks with multiple input modalities as well as asynchronous multi-task learning, thus referred to as OmniNet. The authors demonstrate that training these models together results in about three times compressed model while retaining the performance in comparison to training them individually."
SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,This paper proposes a new method for quantifying the predictive uncertainty of deep neural networks. The proposed method is based on the use of higher-order influence functions (HOIFs) of the trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence intervals. The method is easy to implement and provides rigorous theoretical guarantees on (1) and (2). Experiments demonstrate that the proposed method outperforms existing Bayesian and non-Bayesian baselines.
SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper presents a GAN-based approach for generating high-quality videos. The approach is based on the idea of decomposing the GAN into a discriminator and a generator. The generator is trained on the Kinetics-600 dataset, and the discriminator is trained to predict the output of the generator on the input video. The proposed approach is evaluated on the tasks of video synthesis and video prediction, and achieves new state-of-the-art inception score and FID for Kinetics600 and UCF-101 datasets."
SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper studies the problem of few-shot learning, where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. In doing so, it adapts the representation in two stages, namely on the few base classes data if available and on the even fewer data of new tasks. In the first stage, it learns a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base classes are few, the network cannot learn where to focus implicitly. It also shows that a pre- trained network may be easily adapted to novel classes, without meta-learning."
SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes Constrained Adversarial Networks (CANs), a generalization of GANs in which the generator is encouraged during training to output valid structures. CANs make use of the semantic loss (Xu et al., 2018) to measure the mass allocated by the generator to invalid structures and penalize the latter accordingly. Experiments on constrained images, molecules, and video game levels show that CANs efficiently generate valid structures that are both high-quality and novel."
SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new learnable activation function based on Adaptive Piecewise Linear units (APL), which gives equal expressive power to both the positive and negative halves on the input space and is able to approximate any zero-centered continuous non-linearity in a closed interval. The authors investigate how the shape of the SymmetricAPL function changes during training and perform ablation studies to gain insight into the reason behind these changes. They hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power."
SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,"This paper proposes to use a deep learning model to enrich the output of a classification black-box with a measure of uncertainty. The authors propose a probabilistic neural network that works in parallel to the black box classifier and uses a Dirichlet layer as the fusion layer with the blackbox. The proposed method is evaluated in two scenarios, one for NLP and one for computer vision. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to wrong predictions and misclassifications."
SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,"This paper proposes to use blockwise adaptive stepsize instead of coordinate-wise adaptivity for gradient descent in training deep neural networks. Theoretical analysis shows that the proposed method has comparable regret in online convex learning and convergence rate for optimizing nonconvex objective as its counterpart with coordinate wise adaptivity, but is better up to some constant. Experiments on synthetic datasets, image classification and language modeling confirm these theoretical results. "
SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,"This paper proposes a new video dataset, called CATER, which is a synthetic video dataset with controllable object and scene bias. The authors claim that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. The dataset is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures."
SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes Boundary-Calibration GANs (BCGANs), which leverage the boundary information from a set of pre-trained classifiers using the original data to improve the quality of the generated data for training other classifiers. In particular, the authors introduce an auxiliary boundary-calibration loss (BC-loss) into the generator of GAN to match the statistics between the posterior distributions of original data and generated data with respect to the boundaries of pretrained classifiers, which is provably unbiased and can be easily coupled with different GAN variants to improve their model compatibility. Experimental results demonstrate that BCGANs not only generate realistic images like original GGANs but also achieves superior model compatibility than the original GAN."
SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, the authors learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. The experiments over CIFAR-10 and CifAR-100 datasets demonstrate that the L2L outperforms existing adversarial learning methods in both classification accuracy and computational efficiency. Moreover, the framework can be extended to the generative adversarial imitation learning and stabilize the training."
SP:0a66c3540383c76689258d2fffe0571ed944c1e7,"This paper proposes a method for constraint learning in inverse reinforcement learning (IRL), where the goal is to find a reward function that best explains an expert agent’s policy or demonstrated behavior on a control task. The proposed method is based on the Maximum Entropy IRL framework, which allows the agent to reason about the likelihood of an expert's demonstrations given the knowledge of an MDP. The authors propose an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior and evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."
SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper introduces a deep recurrent neural network architecture that approximates known visual cortical circuits. The authors show that this architecture, which they refer to as the γ-Net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. The experiments indicate that one representative contextual illusion is a consequence of neural strategies for efficient scene segmentation, and the authors directly tested whether this contextual hallucination is a bug or a byproduct of optimized neural computations using the \gamma-dense recurrent prediction model with recurrent dynamics inspired by neural circuits in visual cortex. Overall, this paper suggests that the orientation tilt illusion is an effect of neural circuits and that incorporating such circuits in artificial neural networks can improve computer vision."
SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,"This paper proposes a context-aware CNN (conCNN) for object detection. ConCNN is a combination of conditional random field (CRF) and convolutional neural networks (CNNs). In particular, conCNN features a context aware module that naturally models the mean-field inference method for CRF using a stack of common CNN operations. It can be seamlessly plugged into any existing region-based object detection paradigm. The experiments using COCO datasets showcase that conCNN improves the average precision (AP) of object detection by 2 percentage points, while only introducing negligible extra training overheads."
SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"This paper investigates the adversarial vulnerability of batch normalization (BatchNorm) in deep neural networks. The authors hypothesize that the use of different normalization statistics during training and inference (mini-batch statistics for training and moving average of these values at inference) is the main cause of this adversarial vulnerabilities in the BatchNorm layer. They empirically prove this by experiments on various neural network architectures and datasets. Furthermore, they introduce Robust Normalization (RobustNorm) and experimentally show that it is not only resilient to adversarial perturbation but also inherits the benefits of Batchnorm."
SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the problem of generalization of deep neural networks in the presence of noisy labels. The authors propose two simple regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, they prove that gradient descent training with either of these two methods leads to a generalization guarantee on the clean data distribution despite being trained using noisy labels, and the generalization analysis relies on the connection between wide neural network and neural tangent kernel (NTK). Experimental results verify the effectiveness of these methods on noisily labeled datasets."
SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes to improve the performance of CNN-GP and CNTK by using two ideas: (1) Modifying the kernel using a new operation called Local Average Pooling (LAP), which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. (2) Representing the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches, the resulting kernel achieves 89% accuracy on CIFAR-10."
SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In particular, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of Q-values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTs, resulting in a cooperative relationship between model free learning and model based search. The proposed method can be implemented on top of any Q learning agent with access to a model, which is demonstrated by incorporating it into agents that perform challenging tasks and Atari."
SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a new network architecture for the task of deep 3D pan (deep 3D view synthesis) at arbitrary camera positions along the X-axis with “t-shaped” adaptive kernels equipped with globally and locally adaptive dilations. The proposed network architecture, the “monster-net”, is devised with a novel t-shaped adaptive kernel, which can incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes, and our VICLAB STEREO indoors dataset to prove the efficacy of the proposed method."
SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the variational auto-encoders (VAEs) from an information theoretic perspective. The authors propose the capacity-constrained information maximization (CCIM) objective, which aims to learn a generative model that maximizes the mutual information between the latent representation and the generative capacity of the network. They show that the optimal generative models can be found by optimising the CCIM objective. In particular, they propose a variational lower bound of the objective and show that this lower bound can be used to learn representations that are more informative than the original latent representation. They also show that in certain cases, the optimal solution of the CCim objective can be obtained by optimizing the optimal variational objective of the original VAE."
SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,"This paper studies the problem of learning node embeddings that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. The authors propose two approaches: pooled (AE) and multi-scale (MUSAE) approaches that capture attribute-neighborhood relationships over multiple scales. They prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embedding. Experiments show that their algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets."
SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the phase transitions in the Information Bottleneck (IB) objective. The phase transitions are defined on the encoding distribution p(z|x) for input X, target Y and representation Z, where sudden jumps of dI(Y;Z) dβ and prediction accuracy are observed with increasing beta. The authors introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. They derive a formula that provides a practical condition for IB phases transitions, and draw its connection with the Fisher information matrix for parameterized models. They also provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings."
SP:fecfd5e98540e2d146a726f94802d96472455111,"This paper proposes a new advantage estimation method for reinforcement learning. The main idea is to use importance sampling to identify the independence property between current action and future state, which can be further leveraged to effectively reduce the variance of the advantage estimation. The proposed advantage estimator can be combined with existing Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that the proposed method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments."
SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,"This paper proposes a bias-reduced off-policy evaluation method based on the infinite horizon density ratio and off policy value estimation. The proposed method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect. In general, when either of them is accurate, the bias can also be reduced. Both theoretical and empirical results show that the method yields significant advantages over previous methods."
SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a two-stage framework for few-shot classification. In the first stage, the authors propose to learn task-agnostic feature on base data with a novel Metric-Softmax loss, which is trained against the whole label set and learns more discriminative feature than episodic training. The second stage is to design a task-adaptive transformation which adapts the classifier to each novel class very fast within a few tuning epochs. Experiments show that the proposed approach outperforms current state-of-the-arts by a large margin on commonly used mini-ImageNet and CUB-200-2011 benchmarks."
SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,"This paper presents a data-driven approach that learns to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data, then employs a neural network that infers a correction to move a coarse thus quickly obtainable result closer to the reference data. The authors provide insights into the targeted learning problem with two learning approaches: fully supervised learning methods with a naive and an optimized data acquisition and an unsupervised learning method with a differentiable Navier-Stokes solver. The model successfully improves the accuracy for previously unseen PDE solves."
SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for online continual compression, where the goal is to learn to compress and store a representative dataset from a non-i.i.d data stream, while only observing each sample once. The authors propose a new architecture which Stacks Quantization Modules (SQM), consisting of a series of discrete autoencoders, each equipped with their own memory. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, and 3) unlike previous methods, this approach does not require pretraining, even on challenging datasets."
SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,"This paper proposes an end-to-end deep representation approach for metric learning on multi-label data set that is based on neural networks able to operate on feature data or directly on raw image data. The model scales linearly in the number of instances and trains deep neural networks that encode both input data and output labels, then, obtains a metric space for testing data. In a number of experiments, the authors demonstrate that their approach is better than related methods based on the systematic metric and its extendability."
SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,"This paper studies the stability of asynchronous training from the perspective of dynamical stability. The authors show that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. The learning rate should be inversely proportional to the delay, and momentum should be either turned off, or modified to improve training stability. "
SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes a two-stage reinforcement learning algorithm HiMo, which first learns to extract relevant features of future observations that would be been most helpful for estimating the final value. Then, a forward model is learned to predict these features, which in turn is used as input to an improved value function, yielding better policy evaluation and training at test time. The authors demonstrate that this approach can help tame complexity in environments with rich dynamics at scale, yielding increased data efficiency and improving the performance of model-free methods."
SP:6388fb91f2eaac02d9406672760a237f78735452,"This paper proposes a method for attacking graph neural networks (GNNs) by adding/deleting edges to the graph structure. The proposed method is based on a rewiring operation, which preserves some basic graph properties such as number of nodes and number of edges. The authors then use reinforcement learning to learn the attack strategy based on the proposed method. Experiments on real world graphs demonstrate the effectiveness of the proposed framework."
SP:233b12d422d0ac40026efdf7aab9973181902d70,"This paper proposes a bagging scheme to improve the performance of encoder-decoder convolutional neural networks (CNNs) for inverse problems. The proposed method is based on the SURE-based unsupervised denoising networks (SURE), which is shown to be an unbiased estimator of the prediction error. The authors propose a close-form expression of the unbiased estimators for prediction error, which leads to a novel bootstrap and aggregation scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems, including accelerated MRI and EDX."
SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,"This paper proposes a meta-learning approach for few-shot classification, where the number of instances per task and class is not fixed. The authors propose a Bayesian framework to infer the posterior of these balancing variables, and propose an effective variational inference framework to solve for them. The proposed approach is validated on multiple realistic task-and-class-imbalanced datasets, on which it outperforms existing meta learning approaches."
SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a method for imitation learning based on reinforcement learning. The main idea is to learn a reward function that encourages the agent to return to demonstrated states upon encountering new, out-of-distribution states. This is achieved by giving the agent a constant reward of +1 for matching the demonstrated action in a demonstrated state, and 0 for all other behavior. The method is evaluated on a variety of low-dimensional Mujoco and Box2D environments."
SP:37a4825aaeb899187b957d9ed9ae657617f4a055,"This paper proposes a method to learn stable and temporally coherent feature spaces for dynamic point clouds. The authors propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the aforementioned halos. They combine these techniques in a super-resolution method with a truncation approach to flexibly adapt the size of the generated positions. They show that their method works for large, deforming point sets from different sources."
SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,This paper studies the problem of learning optimal transport (OT) for aligning multiple datasets. The authors propose to learn the cost function using a small amount of side information which is often available. They develop an end-to-end optimizer (OT-SI) that differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. The proposed method outperforms state-of-the-art benchmarks. 
SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,"This paper presents an end-to-end method for anomaly detection under a fully unsupervised setting. The key insight of the algorithm is to model normal data. The authors leverage distribution clustering technique to make an educated guess on the normal data subset. By incorporating clustering to provide supervisory signals, the authors iterate between hypothesizing normal candidate subset and representation learning. This framework iteratively distills out anomalous data and improves the learned representation of normal data, leading to the proposed framework. Extensive experiments on benchmark datasets demonstrate that the proposed method outperforms existing unsupervisory approaches and is comparable to semi-supervised solutions."
SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper considers the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. The authors propose a new algorithm, Projection-Based Constrained Policy Optimization (PCPO), which is an iterative method for optimizing policies in a two-step process: the first step performs a local reward improvement update, while the second step reconciles any constraint violation by projecting the policy back onto the constraint set. They theoretically analyze PCPO and provide a lower bound on reward improvement, and an upper bound on constraint violation, for each policy update. They further characterize the convergence of PCPO based on two different metrics: L norm and Kullback-Leibler divergence. The empirical results over several control tasks demonstrate that PCPO achieves superior performance."
SP:068a0bb2497373acad5f70e66c61b71465b2de3d,"This paper theoretically analyzes the inner mechanism leading to the nice properties of word embedding methods: (1) words having similar semantic meanings are embedded closely; (2) analogy structure exists in the embedding space, such that “Paris is to France as Berlin is to Germany” can be expressed as a low-rank transformation from the word-context co-occurrence space to the embeddings space. The authors also provide a theoretical explanation for the behavior of the parameter alpha, and derive a method to automatically find its optimal value."
SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a method to improve the efficiency and accuracy of similarity measures for clustering tasks. The proposed method is based on a group of approximate Random Projection Trees (RP Trees) that are applied to the task of similarity measurement. The method is evaluated on three real-world datasets and compared with the state-of-the-art. 
SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. The effectiveness of EI was verified on synthetic examples and on popular benchmarks for deep generative models."
SP:64f2744e938bd62cd47c1066dc404a42134953da,This paper proposes a method for treatment effect estimation with missing covariates. The authors propose to use a variational autoencoder (VAE) to learn the latent variables and use them as inputs for doubly robust treatment effect estimators. They also propose a multiple imputation strategy that allows to fully exploit the posterior distribution of latent variables. Numerical experiments demonstrate the effectiveness of the proposed methodology.
SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a neural architecture search (NAS) algorithm for learning compact reinforcement learning (RL) policies, by combining ENAS (Vinyals et al., 2015; Pham et al. 2018; Zoph & Le, 2017) and ES (Salimans et al,. 2017) in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, the authors represent compact architectures via efficient learned edge partitioning. For several RL tasks, they manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing > 90% compression over vanilla policies and 6x compression over state-of-the-art compact policies."
SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper proposes a generalization of the Wavelet Transform (WT) framework to the space of strictly increasing and continuous functions, which is a subset of invertible maps on R. The authors propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific loss and signal. The learnable Group Transform (GT) can be cast into a Deep Neural Network. The experiments on diverse time-series datasets demonstrate the expressivity of this framework, which competes with state-of-the-art performances."
SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a generalization of graph convolutional networks (GCN) to (products of) constant curvature spaces. The authors introduce a unified formalism that can interpolate smoothly between all geometries of constant curvatures, leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Their class of models smoothly recover their Euclidan counterparts when the curvature goes to zero from either side. Empirically, the authors show that the proposed models outperform Euclideans GCNs in the tasks of node classification and distortion minimization for symbolic data."
