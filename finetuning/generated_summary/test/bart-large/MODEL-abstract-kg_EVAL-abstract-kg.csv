,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"Role - based learning USED-FOR scalable multi - agent learning. Role - based learning USED-FOR decomposing complex tasks. decomposing complex tasks USED-FOR scalable multi - agent learning. roles USED-FOR decomposing complex tasks. role selector USED-FOR role discovery. role selector USED-FOR role space. role space CONJUNCTION temporal resolution. temporal resolution CONJUNCTION role space. it USED-FOR bi - level learning hierarchy. primitive action - observation spaces FEATURE-OF role policies. action effects USED-FOR role selector. learning efficiency CONJUNCTION policy generalization. policy generalization CONJUNCTION learning efficiency. method COMPARE MARL algorithms. MARL algorithms COMPARE method. OtherScientificTerm are joint action spaces, and restricted role action spaces. Material is StarCraft II micromanagement benchmark. ",This paper proposes a method for multi-agent multi-task learning based on role-based learning. The key idea is to use a role selector to learn a bi-level learning hierarchy. The proposed method is evaluated on the StarCraft II micromanagement benchmark.,This paper proposes a method for multi-agent multi-task learning based on role-based learning. The key idea is to use a role selector to learn a bi-level learning hierarchy. The proposed method is evaluated on the StarCraft II micromanagement benchmark.
9,SP:7deb61890d97422a0fe141ca807f968c70ab239a,"stochastic subgradient descent ( SSGD ) method USED-FOR over - parameterized nonsmooth optimization problems. interpolation condition FEATURE-OF over - parameterized nonsmooth optimization problems. composite structure USED-FOR SSGD. composite structure USED-FOR empirical risk minimization problems. stochastic gradient descent ( SGD ) method USED-FOR smooth problems. rates COMPARE rates. rates COMPARE rates. rates USED-FOR stochastic gradient descent ( SGD ) method. SGD USED-FOR smooth and nonsmooth machine learning models. SSGD USED-FOR smooth and nonsmooth machine learning models. SGD CONJUNCTION SSGD. SSGD CONJUNCTION SGD. subgradient method USED-FOR convex and interpolation setting. OtherScientificTerm are convex and strongly - convex objectives, and interpolation. ","This paper studies the problem of solving over-parameterized nonsmooth optimization problems in the convex and strongly-convex setting. In particular, the authors consider the case where the objective function is a convex function. The authors show that under the interpolation condition, SGD and SSGD converges to a composite structure, which is a generalization of the composite structure of SGD. They also show that the convergence rate of SSGD and SGD converge to the same rate for convex optimization problems. ","This paper studies the problem of solving over-parameterized nonsmooth optimization problems in the convex and strongly-convex setting. In particular, the authors consider the case where the objective function is a convex function. The authors show that under the interpolation condition, SGD and SSGD converges to a composite structure, which is a generalization of the composite structure of SGD. They also show that the convergence rate of SSGD and SGD converge to the same rate for convex optimization problems. "
18,SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"non - linear “ reservoir ” layers CONJUNCTION regular transformer layers. regular transformer layers CONJUNCTION non - linear “ reservoir ” layers. Method are transformers, and machine learning. OtherScientificTerm is layers. Metric is wall - clock compute time. Task is machine translation. ",This paper studies the problem of learning transformers in machine translation. The authors propose a new way of computing the wall-clock compute time of a transformer layer. They show that the wall clock compute time can be computed in terms of the number of non-linear reservoir layers and regular transformer layers. They also provide a theoretical analysis of the effect of different types of reservoir layers. ,This paper studies the problem of learning transformers in machine translation. The authors propose a new way of computing the wall-clock compute time of a transformer layer. They show that the wall clock compute time can be computed in terms of the number of non-linear reservoir layers and regular transformer layers. They also provide a theoretical analysis of the effect of different types of reservoir layers. 
27,SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"transformation invariance CONJUNCTION equivariance. equivariance CONJUNCTION transformation invariance. transformation invariance FEATURE-OF network architecture. equivariance FEATURE-OF network architecture. geometry transformation of data FEATURE-OF network robustness. Filter transform USED-FOR steerable CNN. group representation theory USED-FOR steerable CNN. group representation theory USED-FOR function space structure. group representation theory USED-FOR steerable kernel function. function space structure FEATURE-OF steerable kernel function. theory COMPARE filter transform technique. filter transform technique COMPARE theory. group representation theory USED-FOR kernel. filter transform USED-FOR kernel. filter transformed kernels USED-FOR group representation. approach USED-FOR steerable convolution operators. Method are Steerable CNN, and steerable CNN theory. OtherScientificTerm is overfitting. ","This paper studies the problem of steerable convolutional neural networks. The authors propose a new group representation theory for steerable CNNs, which is based on the group representation of the kernel of the steerable kernel function. They show that the kernel is invariant to transformation invariance and equivariance to geometry transformation of data. They also show that filter transformed kernels can be used to represent steerable kernels. ","This paper studies the problem of steerable convolutional neural networks. The authors propose a new group representation theory for steerable CNNs, which is based on the group representation of the kernel of the steerable kernel function. They show that the kernel is invariant to transformation invariance and equivariance to geometry transformation of data. They also show that filter transformed kernels can be used to represent steerable kernels. "
36,SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"Multimodal program synthesis USED-FOR program. user input USED-FOR program. Multimodal program synthesis USED-FOR program synthesis. user input USED-FOR Multimodal program synthesis. noisy signals USED-FOR it. natural language HYPONYM-OF noisy signals. neural model USED-FOR program ’s score. natural language ( NL ) CONJUNCTION input - output examples. input - output examples CONJUNCTION natural language ( NL ). user intent FEATURE-OF multimodal synthesis tasks. input - output examples USED-FOR multimodal synthesis tasks. input - output examples USED-FOR user intent. top - down recurrent neural model PART-OF method. automated program analysis techniques USED-FOR search space. user ’s constraints FEATURE-OF infeasibility of partial programs. automated program analysis techniques USED-FOR it. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR method. method COMPARE techniques. techniques COMPARE method. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR techniques. accuracy EVALUATE-FOR techniques. accuracy EVALUATE-FOR method. Method is optimal neural synthesis approach. OtherScientificTerm are user - provided constraints, abstract syntax trees, NL input, and syntactically valid programs. Generic is model. ","This paper proposes a new method for multi-modal program synthesis. The main idea is to use a top-down recurrent neural model to learn the score of a program from input-output examples, which is then used to train a neural network to predict the program’s score. The proposed method is evaluated on the STRUCTUREX dataset and shows that the proposed method outperforms the state-of-the-art.","This paper proposes a new method for multi-modal program synthesis. The main idea is to use a top-down recurrent neural model to learn the score of a program from input-output examples, which is then used to train a neural network to predict the program’s score. The proposed method is evaluated on the STRUCTUREX dataset and shows that the proposed method outperforms the state-of-the-art."
45,SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"protease enzymes HYPONYM-OF proteins. substrate specificity landscape FEATURE-OF protease enzyme. sequence motifs PART-OF substrate specificity landscape. methods USED-FOR predicting protease specificity landscapes. sequence patterns USED-FOR methods. protein graph convolutional neural network ( PGCN ) USED-FOR substrate specificity. Rosetta energy function USED-FOR topology and energetic features. structure - based molecular interaction graph USED-FOR substrate specificity. Rosetta energy function USED-FOR structure - based molecular interaction graph. structure - based molecular interaction graph USED-FOR protein graph convolutional neural network ( PGCN ). PGCN USED-FOR specificity. specificity FEATURE-OF NS3/4 protease. Hepatitic C virus FEATURE-OF NS3/4 protease. PGCN COMPARE machine learning models. machine learning models COMPARE PGCN. classification tasks EVALUATE-FOR PGCN. feature importance USED-FOR sub - graph patterns. sub - graph patterns USED-FOR molecular recognition. physical interactions USED-FOR PGCN. PGCN model USED-FOR enzymes. Task is robustness of key life processes. OtherScientificTerm are protease specificity landscapes, mutational changes, and molecular interactions. ",This paper proposes a novel method for predicting the substrate specificity landscape of proteins. The proposed method is based on a protein graph convolutional neural network (PGCN) that is able to predict the structure-based molecular interaction graph (MIDG) of a protein. The method is evaluated on a variety of classification tasks and shows promising results. ,This paper proposes a novel method for predicting the substrate specificity landscape of proteins. The proposed method is based on a protein graph convolutional neural network (PGCN) that is able to predict the structure-based molecular interaction graph (MIDG) of a protein. The method is evaluated on a variety of classification tasks and shows promising results. 
54,SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"method USED-FOR overestimation bias. Double Q - learning USED-FOR overestimation bias. Double Q - learning HYPONYM-OF method. approximate Bellman operator USED-FOR non - optimal fixed points. underestimation bias PART-OF double Q - learning. approximate dynamic programming USED-FOR approach. method COMPARE baseline algorithms. baseline algorithms COMPARE method. Atari benchmark tasks EVALUATE-FOR baseline algorithms. Atari benchmark tasks EVALUATE-FOR method. Method are Bellman operation, and deep Q - learning paradigm. Task are value prediction, and learning. OtherScientificTerm is non - optimal stationary solutions. ","This paper proposes a new method for double Q-learning, which aims to reduce the overestimation bias in the Bellman operator. The proposed method is based on the idea of approximate dynamic programming. The authors show that the proposed method can reduce the underestimation bias of double Q learning, which is a well-studied problem in deep learning. They also show that their method outperforms the baselines on several Atari benchmark tasks. ","This paper proposes a new method for double Q-learning, which aims to reduce the overestimation bias in the Bellman operator. The proposed method is based on the idea of approximate dynamic programming. The authors show that the proposed method can reduce the underestimation bias of double Q learning, which is a well-studied problem in deep learning. They also show that their method outperforms the baselines on several Atari benchmark tasks. "
63,SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"models USED-FOR high - resolution image generation. BigGAN CONJUNCTION VQVAE-2. VQVAE-2 CONJUNCTION BigGAN. compute resources USED-FOR models. VQVAE-2 HYPONYM-OF models. BigGAN HYPONYM-OF models. ESRGAN HYPONYM-OF GAN - based image super - resolution models. two - step training framework USED-FOR deep generative models ( DGMs ). high - dimensional natural images USED-FOR deep generative models ( DGMs ). wavelet domain USED-FOR sampler. wavelet super - resolution decoder network USED-FOR images. Wavelet - based down - sampling method COMPARE pixel - based methods. pixel - based methods COMPARE Wavelet - based down - sampling method. generative quality EVALUATE-FOR low - resolution sampler. Wavelet - based down - sampling method USED-FOR structural information. generative quality EVALUATE-FOR Wavelet - based down - sampling method. generative quality EVALUATE-FOR pixel - based methods. sampler CONJUNCTION decoder. decoder CONJUNCTION sampler. ImageNet EVALUATE-FOR model. model COMPARE BigGAN model. BigGAN model COMPARE model. Fréchet Inception Distance ( FID ) EVALUATE-FOR BigGAN model. Fréchet Inception Distance ( FID ) EVALUATE-FOR model. OtherScientificTerm are low - frequency bands, pixel - space, and dimensional spaces. Method is end - to - end models. Metric is training cost. ",This paper proposes a two-step training framework for deep generative models (DGMs). The first step is to learn a wavelet-based down-sampling method for the low-frequency band. The second step is a decoder-decoder network to generate high-resolution images. The authors show that the proposed method outperforms pixel-based methods on ImageNet.,This paper proposes a two-step training framework for deep generative models (DGMs). The first step is to learn a wavelet-based down-sampling method for the low-frequency band. The second step is a decoder-decoder network to generate high-resolution images. The authors show that the proposed method outperforms pixel-based methods on ImageNet.
72,SP:b943a73b1ec34867371325748dc3a91ff4011947,"self - supervised learning ( SSL ) algorithms USED-FOR Fewshot learning(FSL ). pre - trained embedding network USED-FOR downstream FSL tasks. self - supervised training USED-FOR pre - trained embedding network. SSL USED-FOR FSL. self - supervised training USED-FOR FSL. supervised training USED-FOR FSL. self - supervised loss CONJUNCTION supervised loss. supervised loss CONJUNCTION self - supervised loss. supervised training CONJUNCTION self - supervised training. self - supervised training CONJUNCTION supervised training. test accuracy EVALUATE-FOR self - supervised FSL. Material are large - scale labeled data, and labeled data. Method are embedding network, and supervised FSL methods. ","This paper studies the problem of self-supervised fewshot learning (SSL) in the context of supervised learning. The authors propose a new method to improve the performance of SSL-based FSL methods. The key idea is to use a pre-trained embedding network for downstream FSL tasks, which is then used to train the embeddings for downstream tasks. The proposed method is evaluated on a few-shot learning task, where it is shown that the proposed method achieves better performance than supervised methods. ","This paper studies the problem of self-supervised fewshot learning (SSL) in the context of supervised learning. The authors propose a new method to improve the performance of SSL-based FSL methods. The key idea is to use a pre-trained embedding network for downstream FSL tasks, which is then used to train the embeddings for downstream tasks. The proposed method is evaluated on a few-shot learning task, where it is shown that the proposed method achieves better performance than supervised methods. "
81,SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"first order methods USED-FOR ultra - wide neural networks. finite width FEATURE-OF neural networks. OtherScientificTerm are global minima, initialization, teacher neurons, local minima, student neurons, and Angular Distance ( AD ) function. Method is two - layer teacher - student networks. Generic is methodology. ",This paper studies the problem of learning two-layer teacher-student networks with finite width. The authors show that the global minima of the teacher network and the local minima in the student network are the same. They show that this is the case for both the teacher and student networks. They then propose a new method to solve this problem. The main contribution of this paper is the use of the Angular Distance (AD) function to solve the problem.,This paper studies the problem of learning two-layer teacher-student networks with finite width. The authors show that the global minima of the teacher network and the local minima in the student network are the same. They show that this is the case for both the teacher and student networks. They then propose a new method to solve this problem. The main contribution of this paper is the use of the Angular Distance (AD) function to solve the problem.
90,SP:0f62846913ec10b44ed32845770da0565479dc75,"framework USED-FOR deep neural networks. user - provided formal knowledge USED-FOR learning from data. Deep Adaptive Semantic Logic ( DASL ) USED-FOR deep neural networks. Deep Adaptive Semantic Logic ( DASL ) HYPONYM-OF framework. knowledge representation USED-FOR first order logic. finite sampling USED-FOR truth values. infinite domains FEATURE-OF finite sampling. prior neuro - symbolic work USED-FOR DASL ’s representation. structure PART-OF image classification task. DASL USED-FOR visual relationship detection task. OtherScientificTerm are formal semantics, vanishing gradients, deeper logical structure, data requirements, commonsense knowledge, and data scarcity. ",This paper proposes a new framework for deep neural networks to learn formal knowledge from data. The key idea is to learn a knowledge representation of the data that can be used as a basis for learning a deep neural network. This knowledge representation is then used to train a neural network on the learned representation. The proposed method is evaluated on the visual relationship detection task and is shown to outperform existing methods.,This paper proposes a new framework for deep neural networks to learn formal knowledge from data. The key idea is to learn a knowledge representation of the data that can be used as a basis for learning a deep neural network. This knowledge representation is then used to train a neural network on the learned representation. The proposed method is evaluated on the visual relationship detection task and is shown to outperform existing methods.
99,SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"feedforward residual neural networks ( ResNets ) USED-FOR iterative recurrent computations. they USED-FOR neural networks. regularization approach USED-FOR learning of iterative solutions. ResNets USED-FOR iterative solutions. iteration CONJUNCTION convergence. convergence CONJUNCTION iteration. ResNets USED-FOR iterative solutions. regularizations USED-FOR iterative convergent computation. this USED-FOR inductive bias. regularizations USED-FOR inductive bias. ResNet CONJUNCTION recurrent ” ResNet. recurrent ” ResNet CONJUNCTION ResNet. method USED-FOR recurrence regularization. recurrent network USED-FOR one. one HYPONYM-OF recurrent ” ResNet. Lipschitz constraint FEATURE-OF residual functions. spectral normalization USED-FOR Lipschitz constraint. gradient coupling CONJUNCTION Lipschitz constraint. Lipschitz constraint CONJUNCTION gradient coupling. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. recurrence regularization CONJUNCTION spectral normalization. spectral normalization CONJUNCTION recurrence regularization. visual recognition tasks EVALUATE-FOR classification accuracy. Digitclutter HYPONYM-OF recognition tasks. MNIST HYPONYM-OF visual recognition tasks. classification accuracy EVALUATE-FOR spectral normalization. classification accuracy EVALUATE-FOR recurrence regularization. CIFAR-10 HYPONYM-OF visual recognition tasks. Iterative convergent computation USED-FOR tasks. inductive bias FEATURE-OF ResNets. Task are Iterative computations, and computer vision tasks. Method are Iterative methods, and soft gradient coupling. Metric is iterative convergence. Generic are them, and networks. ","This paper proposes a regularization approach for iterative recurrent computations. The authors show that the Lipschitz constraint and spectral normalization are useful regularizers for learning of iterative solutions. They also show that these regularizations can be used to reduce the inductive bias of ResNets. Experiments on MNIST, CIFAR-10 and Digitclutter show the effectiveness of the proposed regularization.","This paper proposes a regularization approach for iterative recurrent computations. The authors show that the Lipschitz constraint and spectral normalization are useful regularizers for learning of iterative solutions. They also show that these regularizations can be used to reduce the inductive bias of ResNets. Experiments on MNIST, CIFAR-10 and Digitclutter show the effectiveness of the proposed regularization."
108,SP:6c14506b8b2b06043409d912e6bf877651aaa665,"Normalization techniques USED-FOR deep neural networks. they USED-FOR independent and identically distributed ( IID ) data. normalization methods USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. CrossNorm USED-FOR OOD generalization. SelfNorm USED-FOR OOD generalization. SelfNorm HYPONYM-OF normalization methods. CrossNorm HYPONYM-OF normalization methods. SelfNorm COMPARE CrossNorm. CrossNorm COMPARE SelfNorm. attention USED-FOR SelfNorm. SelfNorm USED-FOR OOD generalization. CrossNorm USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. OtherScientificTerm are channel - wise mean and variance, feature maps, and statistics usage. Task is classification and segmentation. ","This paper studies the problem of OOD generalization in deep neural networks. The authors propose a new normalization method called SelfNorm, which is a combination of CrossNorm and self-attention. They show that SelfNorm is able to generalize to independent and identically distributed (IID) data. They also show that CrossNorm can be used to improve the OOD performance of SelfNorm. ","This paper studies the problem of OOD generalization in deep neural networks. The authors propose a new normalization method called SelfNorm, which is a combination of CrossNorm and self-attention. They show that SelfNorm is able to generalize to independent and identically distributed (IID) data. They also show that CrossNorm can be used to improve the OOD performance of SelfNorm. "
117,SP:2774abdc11917321dd4994af0f0da1ff824bea03,language CONJUNCTION speech. speech CONJUNCTION language. vision CONJUNCTION language. language CONJUNCTION vision. unsupervised pre - training CONJUNCTION generative modeling. generative modeling CONJUNCTION unsupervised pre - training. supervised learning CONJUNCTION unsupervised pre - training. unsupervised pre - training CONJUNCTION supervised learning. generative modeling USED-FOR multiple domains. Attention mechanisms HYPONYM-OF inductive biases. unsupervised pre - training USED-FOR multiple domains. vision HYPONYM-OF multiple domains. speech HYPONYM-OF multiple domains. language HYPONYM-OF multiple domains. neural network architectures USED-FOR reinforcement learning ( RL ). they USED-FOR neural network architectures. high dimensional inputs USED-FOR neural network architectures. pixels HYPONYM-OF high dimensional inputs. attention module PART-OF convolutional encoder. attention module PART-OF RL agent. convolutional encoder PART-OF RL agent. data augmentations CONJUNCTION contrastive losses. contrastive losses CONJUNCTION data augmentations. module USED-FOR interpretable task - relevant information. DeepMind Control Suite environments EVALUATE-FOR module. sampleefficiency EVALUATE-FOR agents. module USED-FOR agents. sampleefficiency EVALUATE-FOR module. attention mechanisms USED-FOR reinforcement learning and control. Generic is approach. ,This paper proposes an attention module for reinforcement learning (RL). The proposed module is based on the attention mechanism of the convolutional encoder. The attention module is used to capture the interpretable task-relevant information from high dimensional inputs. The authors show that the proposed module can be used to improve the sample efficiency of RL agents. The proposed method is evaluated on the DeepMind Control Suite environments.,This paper proposes an attention module for reinforcement learning (RL). The proposed module is based on the attention mechanism of the convolutional encoder. The attention module is used to capture the interpretable task-relevant information from high dimensional inputs. The authors show that the proposed module can be used to improve the sample efficiency of RL agents. The proposed method is evaluated on the DeepMind Control Suite environments.
126,SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"gradient - based approach USED-FOR multitask networks. GradNorm HYPONYM-OF gradient - based approach. extension USED-FOR GradNorm. game theory USED-FOR Rotograd. Rotograd COMPARE approaches. approaches COMPARE Rotograd. approaches USED-FOR multitask learning. Rotograd USED-FOR multitask learning. real - world datasets EVALUATE-FOR Rotograd. real - world datasets CONJUNCTION network architectures. network architectures CONJUNCTION real - world datasets. network architectures EVALUATE-FOR Rotograd. Task is learning. OtherScientificTerm are network parameters, gradient magnitude, gradient magnitudes, and task gradients. Generic is it. Metric is convergence. ","This paper proposes a new gradient-based approach for multitask learning, called Rotograd, which is based on game theory. The main contribution of the paper is the extension of the GradNorm framework to multi-task learning. The authors show that the proposed method is able to converge to the optimal gradient magnitude of the task gradients in a game-theoretic setting. The paper also shows that the convergence rate of the proposed approach is better than that of existing approaches. ","This paper proposes a new gradient-based approach for multitask learning, called Rotograd, which is based on game theory. The main contribution of the paper is the extension of the GradNorm framework to multi-task learning. The authors show that the proposed method is able to converge to the optimal gradient magnitude of the task gradients in a game-theoretic setting. The paper also shows that the convergence rate of the proposed approach is better than that of existing approaches. "
135,SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"geometry distortion problem FEATURE-OF methods. randomness of color transformation FEATURE-OF translation process. unwanted distortions FEATURE-OF translation. Minimal Geometry - Distortion Constraint ( MGC ) HYPONYM-OF I2I translation constraint. approximate representation of mutual information USED-FOR estimation and maximization of MGC. MGC COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MGC. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR MGC. OtherScientificTerm are domain mapping function, mapping function, geometry structure, and consistency of geometry structures. Material are paired data, and translated images. Generic is function. ",This paper studies the problem of minimizing the I2I translation constraint in the context of the geometry distortion problem. The paper proposes a new constraint called Minimal Geometry-Distortion Constraint (MGC) that is based on the mutual information between the mapping function and the geometry structure. The authors show that MGC can be used to improve the performance of existing methods. They also provide a theoretical analysis of MGC.,This paper studies the problem of minimizing the I2I translation constraint in the context of the geometry distortion problem. The paper proposes a new constraint called Minimal Geometry-Distortion Constraint (MGC) that is based on the mutual information between the mapping function and the geometry structure. The authors show that MGC can be used to improve the performance of existing methods. They also provide a theoretical analysis of MGC.
144,SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,point sampling patterns USED-FOR point cloud GANs. DGCNN CONJUNCTION PointConv. PointConv CONJUNCTION DGCNN. PointConv CONJUNCTION KPConv. KPConv CONJUNCTION PointConv. sampling - oversensitive discriminators USED-FOR valid shape generation. PointNet++ CONJUNCTION DGCNN. DGCNN CONJUNCTION PointNet++. sampling - insensitive discriminators USED-FOR shape point clouds. point clustering artifacts FEATURE-OF shape point clouds. KPConv HYPONYM-OF sampling - oversensitive discriminators. PointNet - Max HYPONYM-OF sampling - insensitive discriminators. PointNet++ HYPONYM-OF sampling - oversensitive discriminators. PointConv HYPONYM-OF sampling - oversensitive discriminators. DGCNN HYPONYM-OF sampling - oversensitive discriminators. evaluation metrics EVALUATE-FOR sampling pattern. perceptual metrics PART-OF sampling spectrum of metrics. sampling pattern COMPARE geometry. geometry COMPARE sampling pattern. sampling spectrum USED-FOR middle - point sampling - aware baseline discriminator. PointNet - Mix HYPONYM-OF point cloud generators. sampling - related metrics EVALUATE-FOR point cloud generators. PointNet - Mix HYPONYM-OF middle - point sampling - aware baseline discriminator. Task is generator design. Method is discriminator design. Generic is discriminators. ,"This paper proposes a sampling-over-sensitive discriminator for point cloud GANs. The proposed method is based on PointNet-Max and PointConv, which are sampling-oversensitive discriminators for shape point clouds. The authors show that the proposed method outperforms the baseline PointNet and DGCNN on a variety of evaluation metrics. ","This paper proposes a sampling-over-sensitive discriminator for point cloud GANs. The proposed method is based on PointNet-Max and PointConv, which are sampling-oversensitive discriminators for shape point clouds. The authors show that the proposed method outperforms the baseline PointNet and DGCNN on a variety of evaluation metrics. "
153,SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"images USED-FOR Convolutional Neural Networks ( CNNs ). small quasi - imperceptible artificial perturbations USED-FOR Convolutional Neural Networks ( CNNs ). Capsule Networks ( CapsNets ) COMPARE CNNs. CNNs COMPARE Capsule Networks ( CapsNets ). CNNs COMPARE Capsule Networks ( CapsNets ). Capsule Networks ( CapsNets ) COMPARE CNNs. Capsule Networks ( CapsNets ) USED-FOR white - box attacks. attack protocols USED-FOR CNNs. CapsNets USED-FOR adversarial examples. adversarial robustness FEATURE-OF CapsNets. multi - step attack methods USED-FOR CapsNets. multi - step attack methods USED-FOR CNNs. routing process USED-FOR vote attack. vote attack PART-OF detection - aware attack paradigm. vote attack USED-FOR CapsNets. OtherScientificTerm are votes, computationally expensive routing mechanism, and votes of CapsNets. Metric is computational cost. Method is class - conditional reconstruction based detection method. ","This paper studies the problem of white-box attacks against convolutional neural networks (CNNs). The authors propose a novel method to detect adversarial examples of Capsule Networks (CapsNets) by using a class-conditional reconstruction based detection method. The proposed method is based on the notion of vote attack, which is an extension of the vote-based adversarial robustness of CNNs. The authors show that the proposed method outperforms the state-of-the-art in terms of robustness to adversarial attacks. ","This paper studies the problem of white-box attacks against convolutional neural networks (CNNs). The authors propose a novel method to detect adversarial examples of Capsule Networks (CapsNets) by using a class-conditional reconstruction based detection method. The proposed method is based on the notion of vote attack, which is an extension of the vote-based adversarial robustness of CNNs. The authors show that the proposed method outperforms the state-of-the-art in terms of robustness to adversarial attacks. "
162,SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"Meta - reinforcement learning USED-FOR policy. recurrent neural networks USED-FOR policies. algorithm USED-FOR learning of recurrent policies. privileged information USED-FOR learning of recurrent policies. task descriptor FEATURE-OF privileged information. privileged information USED-FOR algorithm. parameters sharing CONJUNCTION auxiliary objective. auxiliary objective CONJUNCTION parameters sharing. method USED-FOR informed policy. informed policy USED-FOR task embeddings. policy HYPONYM-OF informed policy. parameters sharing USED-FOR recurrent policy. descriptors USED-FOR task embeddings. auxiliary objective USED-FOR recurrent policy. learning sample complexity EVALUATE-FOR approach. task - inference approaches USED-FOR meta - reinforcement learning. Thompson sampling CONJUNCTION task - inference approaches. task - inference approaches CONJUNCTION Thompson sampling. vanilla RNNs CONJUNCTION Thompson sampling. Thompson sampling CONJUNCTION vanilla RNNs. it COMPARE vanilla RNNs. vanilla RNNs COMPARE it. it COMPARE Thompson sampling. Thompson sampling COMPARE it. it COMPARE task - inference approaches. task - inference approaches COMPARE it. it USED-FOR meta - reinforcement learning. Thompson sampling USED-FOR meta - reinforcement learning. exploration / exploitation strategies USED-FOR algorithm. Generic are information, them, and they. Task is online adaptation setting. OtherScientificTerm is behaviour. Method is RNNs. ",This paper proposes a meta-reinforcement learning algorithm for meta-learning. The key idea is to use privileged information to learn an informed policy that can be used to learn the task embeddings. The proposed algorithm is based on the Thompson sampling algorithm. The authors show that the proposed algorithm outperforms Thompson sampling in terms of learning sample complexity and learning sample efficiency. ,This paper proposes a meta-reinforcement learning algorithm for meta-learning. The key idea is to use privileged information to learn an informed policy that can be used to learn the task embeddings. The proposed algorithm is based on the Thompson sampling algorithm. The authors show that the proposed algorithm outperforms Thompson sampling in terms of learning sample complexity and learning sample efficiency. 
171,SP:bd89d254fbf31db61db237d08ab42981e27c52df,"trial - and - errors USED-FOR realworld applications. trial - and - errors USED-FOR RL. simulator USED-FOR optimal policies. dataset USED-FOR simulator. offline dataset USED-FOR policy. paradigm USED-FOR RL policy. model learning technique USED-FOR paradigm. offline data USED-FOR paradigm. offline data USED-FOR RL policy. models USED-FOR policy learning. adaptive policy USED-FOR real - world environments. stochasticity FEATURE-OF dynamics. synthetic environments CONJUNCTION real - world ride - hailing platform. real - world ride - hailing platform CONJUNCTION synthetic environments. method USED-FOR robust recommendations. method USED-FOR distortion problem. Generic is approach. OtherScientificTerm are fidelity of the simulator, and online sampling. Method is learning. ",This paper proposes a method for learning a policy that is robust to distortion in the simulator. The method is based on the idea that the simulator can be used to learn an adaptive policy that can be applied to real-world environments. The authors propose to use a model learning technique to learn the policy from offline data and then use online sampling to improve the performance of the policy. The proposed method is evaluated on a synthetic environment and a real-life ride-hailing task.,This paper proposes a method for learning a policy that is robust to distortion in the simulator. The method is based on the idea that the simulator can be used to learn an adaptive policy that can be applied to real-world environments. The authors propose to use a model learning technique to learn the policy from offline data and then use online sampling to improve the performance of the policy. The proposed method is evaluated on a synthetic environment and a real-life ride-hailing task.
180,SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"sparse rewards USED-FOR goal - reaching behaviors. expert demonstrations CONJUNCTION value function. value function CONJUNCTION expert demonstrations. RL algorithms USED-FOR goal reaching policies. imitation learning USED-FOR goal reaching policies. imitation learning USED-FOR RL algorithms. algorithm USED-FOR goal - reaching behaviors. goal - reaching performance CONJUNCTION robustness. robustness CONJUNCTION goal - reaching performance. robustness EVALUATE-FOR RL algorithms. goal - reaching performance EVALUATE-FOR RL algorithms. iterated supervised learning procedure USED-FOR RL objective. benchmark tasks EVALUATE-FOR RL algorithms. Method are reinforcement learning ( RL ) algorithms, and supervised imitation learning. Generic is it. OtherScientificTerm are demonstrations, policy, and performance bounds. ",This paper studies the problem of goal reaching in reinforcement learning. The authors propose an iterated supervised learning procedure to learn a goal-reaching policy that is robust to expert demonstrations. The proposed method is evaluated on a number of benchmark tasks.,This paper studies the problem of goal reaching in reinforcement learning. The authors propose an iterated supervised learning procedure to learn a goal-reaching policy that is robust to expert demonstrations. The proposed method is evaluated on a number of benchmark tasks.
189,SP:c306530164d677e670554eeba8203c66bb3d9f7a,autoregressive models USED-FOR speech. autoregressive teacher model USED-FOR duration prediction. one - to - many mapping problem PART-OF TTS. knowledge distillation USED-FOR one - to - many mapping problem. autoregressive teacher model USED-FOR FastSpeech model. teacher model USED-FOR mel - spectrograms. teacher model USED-FOR duration. information loss FEATURE-OF mel - spectrograms. pitch CONJUNCTION energy. energy CONJUNCTION pitch. energy CONJUNCTION duration. duration CONJUNCTION energy. FastSpeech 2 USED-FOR FastSpeech. FastSpeech 2 USED-FOR one - to - many mapping problem. variation information of speech USED-FOR conditional inputs. one - to - many mapping problem PART-OF TTS. duration HYPONYM-OF variation information of speech. energy HYPONYM-OF variation information of speech. pitch HYPONYM-OF variation information of speech. pitch CONJUNCTION energy. energy CONJUNCTION pitch. duration CONJUNCTION pitch. pitch CONJUNCTION duration. predicted values USED-FOR inference. conditional inputs USED-FOR training. speech waveform USED-FOR pitch. speech waveform USED-FOR energy. FastSpeech 2s USED-FOR speech waveform. end - to - end inference USED-FOR FastSpeech 2s. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE autoregressive models. autoregressive models COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech 2s. FastSpeech 2s COMPARE FastSpeech 2. training speed - up EVALUATE-FOR FastSpeech. training speed - up EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech. Method is teacher - student distillation pipeline. Task is data simplification. Generic is model. ,"This paper proposes a new method for one-to-many mapping problem in knowledge distillation (TTS). The proposed method is based on a teacher-student distillation pipeline, where the student distills the knowledge of the teacher and the teacher distills it to the student. The student distill the knowledge from the teacher to the teacher, which is then used to predict the duration and energy of a speech waveform. The teacher distills the information from the student to the speaker, and the speaker distill it back into the teacher. The authors show that the proposed method outperforms the state-of-the-art in terms of training speed and inference time. ","This paper proposes a new method for one-to-many mapping problem in knowledge distillation (TTS). The proposed method is based on a teacher-student distillation pipeline, where the student distills the knowledge of the teacher and the teacher distills it to the student. The student distill the knowledge from the teacher to the teacher, which is then used to predict the duration and energy of a speech waveform. The teacher distills the information from the student to the speaker, and the speaker distill it back into the teacher. The authors show that the proposed method outperforms the state-of-the-art in terms of training speed and inference time. "
198,SP:79e9fb20d383816f54738ce70d137131ebc10290,"k - dimensional subspace FEATURE-OF tempered distribution q(x ). tempered distributions USED-FOR unsupervised dimension reduction problem ( UDR ). tempered distribution q(x ) USED-FOR empirical probability density function. q CONJUNCTION pemp. pemp CONJUNCTION q. minimization of the distance USED-FOR problem. generalized functions USED-FOR minimization of the distance. sufficient dimension reduction problem ( SDR ) HYPONYM-OF data science. algorithm USED-FOR problem. algorithm USED-FOR second. algorithm USED-FOR problem. optimization problem USED-FOR optimization problem. distributions USED-FOR optimization problem. ordinary functions USED-FOR optimization problem. algorithm USED-FOR minimization of I(f ) + λR(f ). two - step iterative computation USED-FOR algorithm. two - step iterative computation USED-FOR minimization of I(f ) + λR(f ). synthetic data CONJUNCTION datasets. datasets CONJUNCTION synthetic data. examples EVALUATE-FOR method. datasets USED-FOR method. synthetic data USED-FOR method. datasets USED-FOR examples. synthetic data USED-FOR examples. UDR HYPONYM-OF examples. Method is infinite - dimensional formulation. OtherScientificTerm are nonnegative penalty function R(f ), and λR(f ). Material is real data. ","This paper studies the problem of unsupervised dimension reduction (UDR) in a k-dimensional subspace. The authors propose a new algorithm for solving the problem. The main idea is to use generalized functions to solve the problem, and then use a two-step iterative computation to compute the minimization of the distance between two points in the subspace, which is then used as the second step of the algorithm. The algorithm is evaluated on synthetic and real-world datasets, and is shown to outperform existing methods.","This paper studies the problem of unsupervised dimension reduction (UDR) in a k-dimensional subspace. The authors propose a new algorithm for solving the problem. The main idea is to use generalized functions to solve the problem, and then use a two-step iterative computation to compute the minimization of the distance between two points in the subspace, which is then used as the second step of the algorithm. The algorithm is evaluated on synthetic and real-world datasets, and is shown to outperform existing methods."
207,SP:93e54522e6c2b805905d21fc968fc40866f2898b,methods USED-FOR model. methods USED-FOR robustness. rare or underrepresented patterns FEATURE-OF model. contextual feature utility CONJUNCTION contextual feature sensitivity. contextual feature sensitivity CONJUNCTION contextual feature utility. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. Feature Contrastive Learning ( FCL ) USED-FOR model. contextual utility FEATURE-OF features. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. noise FEATURE-OF generalization. sensitivity EVALUATE-FOR models. robustness EVALUATE-FOR models. generalization EVALUATE-FOR models. FCL USED-FOR models. Task is real - world applications. ,"This paper studies the problem of robustness and generalization in the context of feature contrastive learning. The authors propose a new method, Feature Contrastive Learning (FCL), to improve the robustness of a model. FCL is based on the notion of contextual feature utility, which is a measure of the utility of a feature compared to other features in the dataset. The paper shows that FCL improves robustness, sensitivity, and sensitivity to noise. ","This paper studies the problem of robustness and generalization in the context of feature contrastive learning. The authors propose a new method, Feature Contrastive Learning (FCL), to improve the robustness of a model. FCL is based on the notion of contextual feature utility, which is a measure of the utility of a feature compared to other features in the dataset. The paper shows that FCL improves robustness, sensitivity, and sensitivity to noise. "
216,SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"algorithm USED-FOR autonomous agents. latent representation PART-OF discriminator network. latent representation USED-FOR adversarial learning. adversarial learning USED-FOR algorithm. high dimensional observations USED-FOR autonomous agents. adversarial learning USED-FOR autonomous agents. mutual information constraints USED-FOR latent representation. shared feature space USED-FOR imitation. environment appearance CONJUNCTION agent embodiment. agent embodiment CONJUNCTION environment appearance. balancing CONJUNCTION manipulation and locomotive tasks. manipulation and locomotive tasks CONJUNCTION balancing. algorithm USED-FOR control problems. agent embodiment FEATURE-OF domain differences. environment appearance FEATURE-OF domain differences. manipulation and locomotive tasks HYPONYM-OF control problems. balancing HYPONYM-OF control problems. Method is Imitation learning methods. Generic are they, and constraints. OtherScientificTerm are optimal states, and features. ","This paper proposes a new method for imitation learning for autonomous agents. The key idea is to use adversarial learning to learn the latent representation of the discriminator network, which is then used to learn a shared feature space. The proposed method is evaluated on a variety of tasks, including balancing, manipulation, and locomotion. ","This paper proposes a new method for imitation learning for autonomous agents. The key idea is to use adversarial learning to learn the latent representation of the discriminator network, which is then used to learn a shared feature space. The proposed method is evaluated on a variety of tasks, including balancing, manipulation, and locomotion. "
225,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ","This paper studies the lottery ticket hypothesis (LTH) and pruning multi-layer neural networks. The authors show that pruned neural networks are guaranteed to have zero generalization error, and the generalization of the winning ticket is guaranteed to be convex in the convex region. They also show that the pruned network can achieve better test accuracy than the unpruned network. They show that this is due to the fact that the non-pruned weights of the hidden layer of the network are not pruned. They then propose a new algorithm to speed up the pruning process. ","This paper studies the lottery ticket hypothesis (LTH) and pruning multi-layer neural networks. The authors show that pruned neural networks are guaranteed to have zero generalization error, and the generalization of the winning ticket is guaranteed to be convex in the convex region. They also show that the pruned network can achieve better test accuracy than the unpruned network. They show that this is due to the fact that the non-pruned weights of the hidden layer of the network are not pruned. They then propose a new algorithm to speed up the pruning process. "
234,SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,generalization EVALUATE-FOR neural networks. accuracy CONJUNCTION generalization. generalization CONJUNCTION accuracy. data augmentation approaches USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. generalization EVALUATE-FOR data augmentation approaches. accuracy EVALUATE-FOR data augmentation approaches. augmented data COMPARE clean data. clean data COMPARE augmented data. AutoLabel USED-FOR augmented data. clean distribution CONJUNCTION augmented distribution. augmented distribution CONJUNCTION clean distribution. hold - out validation set USED-FOR calibration - performance. calibration - performance USED-FOR AutoLabel. hold - out validation set USED-FOR AutoLabel. label smoothing USED-FOR AutoLabel. mixup CONJUNCTION adversarial training. adversarial training CONJUNCTION mixup. AugMix CONJUNCTION mixup. mixup CONJUNCTION AugMix. AutoLabel USED-FOR data augmentation methods. adversarial training HYPONYM-OF data augmentation methods. AugMix HYPONYM-OF data augmentation methods. mixup HYPONYM-OF data augmentation methods. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. AutoLabel USED-FOR models. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR AutoLabel. calibration EVALUATE-FOR AutoLabel. accuracy EVALUATE-FOR AutoLabel. AutoLabel USED-FOR adversarial training. clean accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION clean accuracy. OtherScientificTerm is distributional shift. ,"This paper proposes a new data augmentation method, AutoLabel, which is based on label smoothing and adversarial training. The proposed method is evaluated on CIFAR-10, ImageNet, and ImageNet-100 datasets. The authors show that AutoLabel is able to achieve better calibration and generalization performance compared to the baseline methods. They also show that the proposed method can be used to improve the robustness of the model against adversarial attacks.","This paper proposes a new data augmentation method, AutoLabel, which is based on label smoothing and adversarial training. The proposed method is evaluated on CIFAR-10, ImageNet, and ImageNet-100 datasets. The authors show that AutoLabel is able to achieve better calibration and generalization performance compared to the baseline methods. They also show that the proposed method can be used to improve the robustness of the model against adversarial attacks."
243,SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"heuristic proxy classification tasks CONJUNCTION data augmentations. data augmentations CONJUNCTION heuristic proxy classification tasks. methods USED-FOR heuristic proxy classification tasks. data augmentations USED-FOR methods. causal framework USED-FOR self - supervised representation learning. proxy classifiers USED-FOR pretraining. invariance constraints USED-FOR proxy classifiers. invariance constraints USED-FOR data augmentations. selfsupervised objective, Representation Learning USED-FOR invariant prediction of proxy targets. invariance regularizer USED-FOR generalization guarantees. invariance regularizer USED-FOR invariant prediction of proxy targets. Invariant Causal Mechanisms ( RELIC ) USED-FOR selfsupervised objective, Representation Learning. causality USED-FOR contrastive learning. contrastive learning HYPONYM-OF self - supervised method. RELIC COMPARE methods. methods COMPARE RELIC. robustness CONJUNCTION out - of - distribution generalization. out - of - distribution generalization CONJUNCTION robustness. RELIC COMPARE methods. methods COMPARE RELIC. out - of - distribution generalization FEATURE-OF ImageNet. human - level performance EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR RELIC. Atari EVALUATE-FOR methods. robustness EVALUATE-FOR RELIC. out - of - distribution generalization EVALUATE-FOR methods. robustness EVALUATE-FOR methods. Method is Self - supervised learning. OtherScientificTerm are supervised signals, and augmentations. Material is unlabeled data. ",This paper proposes a causal framework for self-supervised representation learning based on invariant causal mechanisms (RELIC). The authors propose to use the invariance regularizer to improve the robustness of proxy classifiers to data augmentations. The authors also propose a contrastive learning approach to improve out-of-distribution generalization. Experiments on Atari and ImageNet demonstrate the effectiveness of the proposed method.,This paper proposes a causal framework for self-supervised representation learning based on invariant causal mechanisms (RELIC). The authors propose to use the invariance regularizer to improve the robustness of proxy classifiers to data augmentations. The authors also propose a contrastive learning approach to improve out-of-distribution generalization. Experiments on Atari and ImageNet demonstrate the effectiveness of the proposed method.
252,SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,visual representations of the observed scene USED-FOR navigation actions. Visual Transformer Network ( VTNet ) USED-FOR informative visual representation in navigation. spatial locations of objects CONJUNCTION image regions. image regions CONJUNCTION spatial locations of objects. VTNet HYPONYM-OF structure. structure USED-FOR visual representations. pre - training scheme USED-FOR navigation policy learning. pre - training scheme USED-FOR visual representations. navigation signals USED-FOR visual representations. informative representation USED-FOR navigation. attention operations USED-FOR informative representation. descriptors USED-FOR informative representation. VTNet USED-FOR informative representation. object and region features USED-FOR spatial - aware descriptors. spatial - aware descriptors USED-FOR VTNet. object and region features USED-FOR VTNet. location cues FEATURE-OF object and region features. attention operations USED-FOR descriptors. artificial environment AI2 - Thor EVALUATE-FOR VTNet. VTNet COMPARE methods. methods COMPARE VTNet. artificial environment AI2 - Thor EVALUATE-FOR methods. Task is Object goal navigation. OtherScientificTerm is directional navigation signals. Method is visual representation. ,"This paper proposes a visual transformer network (VTNet) for object goal navigation. The proposed method is based on a pre-training scheme for navigation policy learning. The key idea is to learn a set of spatial-aware descriptors for each object and region in the scene, which are then used to guide the navigation of the agent. The authors show that the proposed method outperforms the state-of-the-art methods on a number of tasks. ","This paper proposes a visual transformer network (VTNet) for object goal navigation. The proposed method is based on a pre-training scheme for navigation policy learning. The key idea is to learn a set of spatial-aware descriptors for each object and region in the scene, which are then used to guide the navigation of the agent. The authors show that the proposed method outperforms the state-of-the-art methods on a number of tasks. "
261,SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,Federated learning USED-FOR neural network models. model parameters USED-FOR federated learning. solution USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR solution. communication - computation efficient secure aggregation COMPARE secure solution. secure solution COMPARE communication - computation efficient secure aggregation. communication - computation efficient secure aggregation USED-FOR communication / computational resources. scheme USED-FOR topology. sparse random graphs COMPARE complete graph. complete graph COMPARE sparse random graphs. topology FEATURE-OF secret - sharing nodes. sparse random graphs USED-FOR topology. Erdős - Rényi graph USED-FOR G. reliability / privacy EVALUATE-FOR scheme. reliability CONJUNCTION data privacy. data privacy CONJUNCTION reliability. data privacy FEATURE-OF federated learning systems. scheme USED-FOR scheme. federated learning systems EVALUATE-FOR scheme. data privacy EVALUATE-FOR scheme. reliability EVALUATE-FOR scheme. OtherScientificTerm is local data. ,This paper proposes a secure aggregation method for federated learning. The proposed method is based on the idea of Erdős-Rényi graph. The authors show that the proposed method can be used to improve the privacy and reliability of the model. They also show that their method is computationally efficient.,This paper proposes a secure aggregation method for federated learning. The proposed method is based on the idea of Erdős-Rényi graph. The authors show that the proposed method can be used to improve the privacy and reliability of the model. They also show that their method is computationally efficient.
270,SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"incentive compatible auction PART-OF Auction Design. theoretical approaches USED-FOR problem. neural network architectures USED-FOR optimal auctions. theoretical auction design USED-FOR time - independent Lagrangian. inner maximization loop USED-FOR optimal misreports. inner maximization loop USED-FOR optimization procedure. stationary utility functions FEATURE-OF two - player game. two - player game USED-FOR Auction Design. Generic is approach. Method are hyper - parameter search, and neural network. OtherScientificTerm is auctions. ","This paper studies the problem of finding an optimal auction design for a two-player game. The authors propose a novel approach to solve the problem by using a neural network to search for optimal misreports. The proposed approach is based on the idea of hyper-parameter search, where the hyperparameters of the neural network are learned by minimizing the inner maximization of the Lagrangian of the misreports of the two players.  The authors show that the proposed approach outperforms existing approaches in terms of misreports and mis-reports. ","This paper studies the problem of finding an optimal auction design for a two-player game. The authors propose a novel approach to solve the problem by using a neural network to search for optimal misreports. The proposed approach is based on the idea of hyper-parameter search, where the hyperparameters of the neural network are learned by minimizing the inner maximization of the Lagrangian of the misreports of the two players.  The authors show that the proposed approach outperforms existing approaches in terms of misreports and mis-reports. "
279,SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,pre - trained model USED-FOR downstream task. large - scale dataset USED-FOR deep neural network. supervised and unsupervised pre - training approaches USED-FOR learning representations. discriminative knowledge of labels CONJUNCTION intrinsic structure of data. intrinsic structure of data CONJUNCTION discriminative knowledge of labels. discriminative knowledge USED-FOR fine - tuning. former USED-FOR fine - tuning methods. intrinsic structure of data USED-FOR boosting fine - tuning. general learning approach USED-FOR supervised and unsupervised pre - trained representations. Bi - tuning HYPONYM-OF general learning approach. supervised and unsupervised pre - trained representations USED-FOR downstream tasks. classifier head CONJUNCTION projector head. projector head CONJUNCTION classifier head. projector head USED-FOR intrinsic structure of data. contrastive cross - entropy loss USED-FOR label information. classifier head USED-FOR label information. Bi - tuning USED-FOR vanilla fine - tuning. instancecontrast way FEATURE-OF label information. contrastive cross - entropy loss FEATURE-OF classifier head. categorical contrastive learning loss USED-FOR projector head. Bi - tuning USED-FOR fine - tuning tasks. fine - tuning tasks EVALUATE-FOR supervised and unsupervised pre - trained models. low - data regime FEATURE-OF accuracy. Generic is latter. OtherScientificTerm is pre - trained representations. ,"This paper proposes a general learning approach for fine-tuning a pre-trained model for downstream tasks. The main idea is to use the contrastive cross-entropy loss between the classifier head and the projector head to improve the performance of the model. The authors show that the proposed method outperforms existing methods on a variety of downstream tasks (e.g., image classification, classification on MNIST, image classification on CIFAR-10, and classification on ImageNet). ","This paper proposes a general learning approach for fine-tuning a pre-trained model for downstream tasks. The main idea is to use the contrastive cross-entropy loss between the classifier head and the projector head to improve the performance of the model. The authors show that the proposed method outperforms existing methods on a variety of downstream tasks (e.g., image classification, classification on MNIST, image classification on CIFAR-10, and classification on ImageNet). "
288,SP:87e5b552c13d73bd85249062a152c6c140e594a9,"adversarial accuracy CONJUNCTION adversarial training. adversarial training CONJUNCTION adversarial accuracy. robustness EVALUATE-FOR classifiers. adversarial accuracy USED-FOR robustness. adversarial accuracy EVALUATE-FOR classifiers. measure USED-FOR robustness. measure USED-FOR classifiers. robustness EVALUATE-FOR classifiers. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. It USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR classifiers. It USED-FOR classifiers. accuracy FEATURE-OF adversarially perturbed samples. invariance - based adversarial examples USED-FOR model. genuine adversarial accuracy EVALUATE-FOR classifier. test accuracy CONJUNCTION lp norm - based test adversarial robustness. lp norm - based test adversarial robustness CONJUNCTION test accuracy. OtherScientificTerm are generalization, predicted classes, and perceptual classes. Material is clean data. Generic are it, and distance metrics. Method is norm - based distance metric. ",This paper proposes a new metric to measure the robustness of a classifier to adversarial perturbations. The proposed metric is based on the lp norm-based distance metric. The authors show that the proposed metric can be used as a measure of adversarial robustness for classifiers. They also show that it can be applied to test accuracy and test robustness. ,This paper proposes a new metric to measure the robustness of a classifier to adversarial perturbations. The proposed metric is based on the lp norm-based distance metric. The authors show that the proposed metric can be used as a measure of adversarial robustness for classifiers. They also show that it can be applied to test accuracy and test robustness. 
297,SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"fairness PART-OF algorithmic designs. graph - structured data USED-FOR disparate impact. fairness concept FEATURE-OF dyadic fairness. edges PART-OF graph. graph connections USED-FOR dyadic fairness. dyadic fairness FEATURE-OF link predictive scores. algorithm USED-FOR fair adjacency matrix. fair adjacency matrix USED-FOR fair link prediction. graph structural constraints USED-FOR fair link prediction. FairAdj USED-FOR fair adjacency matrix. graph structural constraints FEATURE-OF fair adjacency matrix. method USED-FOR dyadic fairness. fairness - utility tradeoff EVALUATE-FOR method. Task are Disparate impact, machine learning applications, and mitigating discrimination. OtherScientificTerm is predictive relationship. Method is graph neural networks. Metric is predictive accuracy. ",This paper studies the problem of dyadic fairness in the context of graph neural networks. The authors propose a new algorithm for fair link prediction based on the fair adjacency matrix (FairAdj). The proposed algorithm is based on graph structural constraints and is able to achieve better performance than existing methods. The paper also proposes a fair-utility trade-off between the fairness and utility of the proposed algorithm. ,This paper studies the problem of dyadic fairness in the context of graph neural networks. The authors propose a new algorithm for fair link prediction based on the fair adjacency matrix (FairAdj). The proposed algorithm is based on graph structural constraints and is able to achieve better performance than existing methods. The paper also proposes a fair-utility trade-off between the fairness and utility of the proposed algorithm. 
306,SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"Autoencoders HYPONYM-OF information compression framework. generative ability FEATURE-OF it. generative ability FEATURE-OF autoencoder. Gaussian prior knowledge USED-FOR synthesis. Gaussian prior knowledge USED-FOR VAE. interpolation HYPONYM-OF exploration in latent space. disentangled representation CONJUNCTION regularization. regularization CONJUNCTION disentangled representation. regularization USED-FOR exploration in latent space. Disentangled Exploration Autoencoder ( DEAE ) USED-FOR controllable synthesis. regularization USED-FOR controllable synthesis. regularization USED-FOR Disentangled Exploration Autoencoder ( DEAE ). disentangled representation USED-FOR Disentangled Exploration Autoencoder ( DEAE ). encoder USED-FOR DEAE. encoder USED-FOR latent code space. directed interpolation USED-FOR encoder. directed interpolation USED-FOR latent code space. encoder USED-FOR latent representation. disentanglement FEATURE-OF latent representation. disentanglement CONJUNCTION exploration. exploration CONJUNCTION disentanglement. positive loop USED-FOR DEAE. exploration USED-FOR positive loop. disentanglement USED-FOR positive loop. DEAE USED-FOR attribute - controllable augmented samples. DEAE USED-FOR dataset bias. DEAE USED-FOR fairness problems. Method are GAN - based adversarial training, and decoder. OtherScientificTerm are latent code, disentangled latent code, and interpolated latent code. Generic is method. ","This paper proposes a disentangled exploration autoencoder (DEAE) for controllable synthesis of attribute-controllable augmented samples. The proposed method is based on a VAE-based adversarial training method. The authors show that the proposed method outperforms the state-of-the-art in terms of disentanglement, exploration, and adversarial robustness. ","This paper proposes a disentangled exploration autoencoder (DEAE) for controllable synthesis of attribute-controllable augmented samples. The proposed method is based on a VAE-based adversarial training method. The authors show that the proposed method outperforms the state-of-the-art in terms of disentanglement, exploration, and adversarial robustness. "
315,SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"Episodic and semantic memory PART-OF human memory model. serial event ( episodic memory ) USED-FOR compressed representation. Bayesian memory allocation scheme USED-FOR episodic and semantic memory. hierarchical latent variable model USED-FOR Bayesian memory allocation scheme. locally contiguous memory USED-FOR differentiable block allocated latent memory. locally contiguous memory USED-FOR Kanerva Machine. feed forward deterministic process USED-FOR it. binarized MNIST CONJUNCTION binarized Omniglot. binarized Omniglot CONJUNCTION binarized MNIST. allocation scheme USED-FOR memory conditional image generation. binarized MNIST FEATURE-OF conditional likelihood values. DMLab Mazes CONJUNCTION Celeb - A. Celeb - A CONJUNCTION DMLab Mazes. CIFAR10 CONJUNCTION DMLab Mazes. DMLab Mazes CONJUNCTION CIFAR10. Celeb - A CONJUNCTION ImageNet32×32. ImageNet32×32 CONJUNCTION Celeb - A. Method are complementary learning systems, and heap allocation. Task is memory writing. OtherScientificTerm is read key distribution. ",This paper proposes a Bayesian memory allocation scheme for episodic and semantic memory. The proposed method is based on a hierarchical latent variable model and a differentiable block allocation scheme. The authors show that the proposed method outperforms the baselines on a number of datasets. ,This paper proposes a Bayesian memory allocation scheme for episodic and semantic memory. The proposed method is based on a hierarchical latent variable model and a differentiable block allocation scheme. The authors show that the proposed method outperforms the baselines on a number of datasets. 
324,SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,deep learning models USED-FOR machine learning tasks. Attention mechanisms CONJUNCTION deep learning models. deep learning models CONJUNCTION Attention mechanisms. sample complexity CONJUNCTION loss landscape. loss landscape CONJUNCTION sample complexity. loss landscape FEATURE-OF attention - based neural networks. sample complexity FEATURE-OF attention - based neural networks. attention models COMPARE models. models COMPARE attention models. local minimum PART-OF attention model. sample complexity EVALUATE-FOR models. prediction error EVALUATE-FOR local minimum. sample complexity EVALUATE-FOR attention models. OtherScientificTerm is attention. Method is self - attention. ,This paper studies the sample complexity of attention-based neural networks. The authors show that the local minimum of an attention model is a function of the loss landscape and sample complexity. They also show that self-attention is a local minimum.,This paper studies the sample complexity of attention-based neural networks. The authors show that the local minimum of an attention model is a function of the loss landscape and sample complexity. They also show that self-attention is a local minimum.
333,SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,Bayesian modeling USED-FOR Active inference. biologically plausible model USED-FOR Bayesian modeling. free energy principle CONJUNCTION prior preference. prior preference CONJUNCTION free energy principle. reinforcement learning ( RL ) algorithms USED-FOR active inference. negative value function USED-FOR EFE. method USED-FOR prior preference. prior preference CONJUNCTION theoretical connection. theoretical connection CONJUNCTION prior preference. theoretical connection USED-FOR method. active inference USED-FOR inverse RL. prior preference learning USED-FOR active inference. active inference USED-FOR inverse RL problem. prior preference learning USED-FOR inverse RL problem. EFE - based rewards USED-FOR active inference. OtherScientificTerm is expected free energy ( EFE ). ,"This paper studies the active inference problem in Bayesian reinforcement learning, where the goal is to maximize the expected free energy (EF) of a reward function. The authors propose a Bayesian model for active inference, which is based on a biologically plausible model of the prior preference and the free energy principle. In particular, the authors show that prior preference learning can be used to improve the performance of active inference in the inverse RL problem. The paper also provides theoretical connections between the prior preferences and the EFE-based rewards.","This paper studies the active inference problem in Bayesian reinforcement learning, where the goal is to maximize the expected free energy (EF) of a reward function. The authors propose a Bayesian model for active inference, which is based on a biologically plausible model of the prior preference and the free energy principle. In particular, the authors show that prior preference learning can be used to improve the performance of active inference in the inverse RL problem. The paper also provides theoretical connections between the prior preferences and the EFE-based rewards."
342,SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"data augmentation method USED-FOR generalization. data augmentation method USED-FOR adversarial and standard learning. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. OOD data USED-FOR learning scenario. method COMPARE data augmentation methods. data augmentation methods COMPARE method. method COMPARE adversarial training. adversarial training COMPARE method. Method is neural networks. Generic is methods. Material are UID data, and image data. OtherScientificTerm are pseudo - labels, and undesirable features. ","This paper proposes a data augmentation method for adversarial and standard learning. The proposed method is based on the idea of pseudo-labels, which are pseudo-labeled features that are not present in the original data. The authors show that the proposed method outperforms existing methods on CIFAR-10, ImageNet, and ImageNet-100 datasets. ","This paper proposes a data augmentation method for adversarial and standard learning. The proposed method is based on the idea of pseudo-labels, which are pseudo-labeled features that are not present in the original data. The authors show that the proposed method outperforms existing methods on CIFAR-10, ImageNet, and ImageNet-100 datasets. "
351,SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"Fast Linearized Adaptive Policy ( FLAP ) HYPONYM-OF metareinforcement learning ( meta - RL ) method. shared linear representation of the policy USED-FOR FLAP. adapter network USED-FOR linear weights. adapter network USED-FOR policy. MAML HYPONYM-OF prior meta - RL methods. gradient descent USED-FOR meta - policy. adaptation run - time EVALUATE-FOR separate feed - forward network. FLAP COMPARE prior methods. prior methods COMPARE FLAP. continuous - control meta - RL benchmarks EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR prior methods. average return CONJUNCTION adaptation run - time speeds. adaptation run - time speeds CONJUNCTION average return. out - of - distribution tasks EVALUATE-FOR FLAP. average return EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR FLAP. Task are outof - distribution tasks, and adaptation. Generic are task, and tasks. Method is prior MetaRL methods. ",This paper proposes a new meta-RL method called Fast Linearized Adaptive Policy (FLAP) for out-of-distribution (OOD) meta-learning. FLAP uses a shared linear representation of the policy and an adapter network to learn the linear weights of the meta-policy. The authors show that FLAP is able to achieve faster adaptation run-times compared to the existing methods. ,This paper proposes a new meta-RL method called Fast Linearized Adaptive Policy (FLAP) for out-of-distribution (OOD) meta-learning. FLAP uses a shared linear representation of the policy and an adapter network to learn the linear weights of the meta-policy. The authors show that FLAP is able to achieve faster adaptation run-times compared to the existing methods. 
360,SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"communication efficiency FEATURE-OF algorithm. kernel k - means USED-FOR optimization problem. federated settings FEATURE-OF kernel k - means. federated settings USED-FOR optimization problem. communication efficient mech anism ( CEM ) USED-FOR communication cost. feder ated kernelk - means USED-FOR privacy preservation. matrix operations USED-FOR local computational results. federated kernel k - means COMPARE kernel k - means. kernel k - means COMPARE federated kernel k - means. clustering quality EVALUATE-FOR federated kernel k - means. clustering quality EVALUATE-FOR kernel k - means. communication cost EVALUATE-FOR DSPGD. O(1 / T ) rate FEATURE-OF DSPGD. CEM USED-FOR DSPGD. CEM USED-FOR DSPGD. communication cost EVALUATE-FOR federated kerne l k - means. clustering quality EVALUATE-FOR federated kerne l k - means. Method are federated kernel k - means algorithm, and kernelk - means. OtherScientificTerm are approximate solution, and cloud server. ","This paper proposes a federated kernel k-means algorithm for the optimization problem. The proposed algorithm is based on the idea of federated mech anism (CEM), which is a communication efficient algorithm that can be used to reduce the communication cost between server and client in federated settings. The authors show that the proposed algorithm achieves O(1/T) communication efficiency, which is comparable to the existing federated kerne l k means algorithm. They also show that their algorithm can achieve better clustering quality than the existing kernel k means algorithms.","This paper proposes a federated kernel k-means algorithm for the optimization problem. The proposed algorithm is based on the idea of federated mech anism (CEM), which is a communication efficient algorithm that can be used to reduce the communication cost between server and client in federated settings. The authors show that the proposed algorithm achieves O(1/T) communication efficiency, which is comparable to the existing federated kerne l k means algorithm. They also show that their algorithm can achieve better clustering quality than the existing kernel k means algorithms."
369,SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"hardware & latency constraints FEATURE-OF architectures. accuracy EVALUATE-FOR architectures. approach USED-FOR models. approach USED-FOR resource - intensive tasks. deployment targets USED-FOR resource - intensive tasks. CompOFA HYPONYM-OF design space. model search / extraction time COMPARE state of the art. state of the art COMPARE model search / extraction time. heuristics COMPARE state of the art. state of the art COMPARE heuristics. design space USED-FOR models. diversity of hardware and latency targets FEATURE-OF models. Method is CNNs. Metric are constant training cost, and complexity. Generic is cost. OtherScientificTerm are combinatorial explosion of sub - optimal model configurations, search space, training budget, search, accuracy - latency Pareto frontier, model dimensions, and Pareto optimality. Material is ImageNet. ","This paper studies the problem of finding sub-optimal models in the search space. The authors propose CompOFA, a new search space that can be used to find models that are close to the Pareto frontier in terms of accuracy-latency trade-off. The proposed search space is based on a combinatorial explosion of sub-optimality model configurations, and the authors show that it is possible to find a search space in which the search cost is constant and the complexity of the model is small.","This paper studies the problem of finding sub-optimal models in the search space. The authors propose CompOFA, a new search space that can be used to find models that are close to the Pareto frontier in terms of accuracy-latency trade-off. The proposed search space is based on a combinatorial explosion of sub-optimality model configurations, and the authors show that it is possible to find a search space in which the search cost is constant and the complexity of the model is small."
378,SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,Meta - learning USED-FOR model. limited data USED-FOR model. adversarial samples USED-FOR meta - learning. ADML ( ADversarial Meta - Learner ) USED-FOR initialization of a learning model. meta - learning algorithm USED-FOR initialization of a learning model. adversarial manner USED-FOR initialization of a learning model. ADML ( ADversarial Meta - Learner ) HYPONYM-OF meta - learning algorithm. clean and adversarial samples USED-FOR ADML ( ADversarial Meta - Learner ). meta - learning algorithms COMPARE it. it COMPARE meta - learning algorithms. it USED-FOR adversarial samples. it COMPARE meta - learning algorithms. meta - learning algorithms COMPARE it. MiniImageNet CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION MiniImageNet. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. ADML COMPARE representative meta - learning algorithms. representative meta - learning algorithms COMPARE ADML. attack mechanisms USED-FOR adversarial samples. image datasets EVALUATE-FOR ADML. MiniImageNet HYPONYM-OF image datasets. CIFAR100 HYPONYM-OF image datasets. Method is learning model. Material is clean samples. OtherScientificTerm is limited and even contaminated samples. ,"This paper proposes a new meta-learning algorithm, ADML (Adversarial Meta-Learner), for the initialization of a learning model. The proposed method is based on the idea that clean and adversarial samples can be used to improve the robustness of the learning model to adversarial attacks. The authors show that the proposed method outperforms the baselines in terms of accuracy, robustness, and robustness to attacks. ","This paper proposes a new meta-learning algorithm, ADML (Adversarial Meta-Learner), for the initialization of a learning model. The proposed method is based on the idea that clean and adversarial samples can be used to improve the robustness of the learning model to adversarial attacks. The authors show that the proposed method outperforms the baselines in terms of accuracy, robustness, and robustness to attacks. "
387,SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,Error correction codes PART-OF communication applications. maximum likelihood rule USED-FOR decoding of transmitted codewords. permutation USED-FOR permutation decoding. data - driven framework USED-FOR permutation selection. node embedding CONJUNCTION self - attention. self - attention CONJUNCTION node embedding. domain knowledge CONJUNCTION machine learning concepts. machine learning concepts CONJUNCTION domain knowledge. domain knowledge PART-OF data - driven framework. machine learning concepts PART-OF data - driven framework. self - attention HYPONYM-OF machine learning concepts. node embedding HYPONYM-OF machine learning concepts. simulated Bose Chaudhuri Hocquenghem ( BCH ) code COMPARE baseline decoders. baseline decoders COMPARE simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR baseline decoders. self - attention networks USED-FOR physical layer communication systems. Method is suboptimal decoding algorithms. Generic is algorithms. ,"This paper proposes a data-driven framework for permutation decoding of transmitted codewords. The proposed method is based on self-attention, node embedding, and domain knowledge. The authors show that the proposed method outperforms baseline decoders in terms of bit error rate and accuracy. ","This paper proposes a data-driven framework for permutation decoding of transmitted codewords. The proposed method is based on self-attention, node embedding, and domain knowledge. The authors show that the proposed method outperforms baseline decoders in terms of bit error rate and accuracy. "
396,SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"fine - tuning BERT USED-FOR text classification task. unsupervised classification task USED-FOR task. finetuning USED-FOR task. unsupervised classification task EVALUATE-FOR finetuning. unsupervised clustering USED-FOR intermediate task. labeled examples USED-FOR topical classification tasks. classification step USED-FOR topical classification tasks. classification step USED-FOR labeled examples. Material are labeled data, and data sets. Method is BERT. ","This paper proposes a novel method for fine-tuning BERT for text classification tasks. The proposed method is based on unsupervised clustering. The authors show that the proposed method can improve the performance of BERT on the task of text classification. They also show that their method can be applied to a number of topical classification tasks (e.g., topical classification, topical clustering, and topical classification). ","This paper proposes a novel method for fine-tuning BERT for text classification tasks. The proposed method is based on unsupervised clustering. The authors show that the proposed method can improve the performance of BERT on the task of text classification. They also show that their method can be applied to a number of topical classification tasks (e.g., topical classification, topical clustering, and topical classification). "
405,SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"fixed ( random shooting ) control agent USED-FOR generative models. mixture density nets COMPARE models. models COMPARE mixture density nets. they COMPARE probabilistic counterparts. probabilistic counterparts COMPARE they. deterministic models COMPARE probabilistic counterparts. probabilistic counterparts COMPARE deterministic models. heteroscedasticity USED-FOR regularizer. them USED-FOR control problem. sample complexity EVALUATE-FOR MBRL. framework USED-FOR MBRL. sample complexity EVALUATE-FOR framework. Acrobot EVALUATE-FOR MBRL. training schedule USED-FOR MBRL. OtherScientificTerm are multimodal posterior predictives, multimodality, and probabilistic posterior predictives. ","This paper studies the problem of learning a generative model with multimodal posterior predictives. The authors propose a new framework, MBRL, which is based on heteroscedasticity. They show that MBRL is able to achieve better sample complexity than the state-of-the-art in terms of the number of samples required to train a model. They also show that the proposed framework can be used to learn a model that is more robust to noise. ","This paper studies the problem of learning a generative model with multimodal posterior predictives. The authors propose a new framework, MBRL, which is based on heteroscedasticity. They show that MBRL is able to achieve better sample complexity than the state-of-the-art in terms of the number of samples required to train a model. They also show that the proposed framework can be used to learn a model that is more robust to noise. "
414,SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"Affine Disentangled GAN ( ADIS - GAN ) HYPONYM-OF Generative Adversarial Network. affine regularizer USED-FOR inductive bias. affine transformation properties of images USED-FOR affine regularizer. transformation matrices PART-OF affine matrix. maximum likelihood estimation USED-FOR transformation parameters. horizontal and vertical zoom CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION horizontal and vertical zoom. horizontal and vertical skew CONJUNCTION horizontal and vertical translation. horizontal and vertical translation CONJUNCTION horizontal and vertical skew. rotation CONJUNCTION horizontal and vertical zoom. horizontal and vertical zoom CONJUNCTION rotation. rotation CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION rotation. disentangled representations COMPARE features. features COMPARE disentangled representations. ADIS - GAN USED-FOR features. approaches USED-FOR disentangled representations. horizontal and vertical translation HYPONYM-OF transformations. rotation HYPONYM-OF transformations. horizontal and vertical skew HYPONYM-OF transformations. horizontal and vertical zoom HYPONYM-OF transformations. ADIS - GAN USED-FOR features. MNIST, CelebA, and dSprites datasets EVALUATE-FOR ADIS - GAN. MNIST, CelebA, and dSprites datasets EVALUATE-FOR features. OtherScientificTerm is affine transformations. Method is InfoGAN. ","This paper proposes an affine disentangled GAN (ADIS-GAN) framework for disentangling images. The key idea is to use the affine transformation properties of images to improve the disentanglement performance of GANs. The proposed method is based on the InfoGAN framework. The main contribution of the paper is to introduce a new affine regularizer that is able to disentangle the transformation matrices of affine transformations. The method is evaluated on MNIST, CelebA, and dSprites datasets.","This paper proposes an affine disentangled GAN (ADIS-GAN) framework for disentangling images. The key idea is to use the affine transformation properties of images to improve the disentanglement performance of GANs. The proposed method is based on the InfoGAN framework. The main contribution of the paper is to introduce a new affine regularizer that is able to disentangle the transformation matrices of affine transformations. The method is evaluated on MNIST, CelebA, and dSprites datasets."
423,SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"contrastive learning methods COMPARE supervised learning counterparts. supervised learning counterparts COMPARE contrastive learning methods. contrastive learning methods USED-FOR Representation learning. data augmentations USED-FOR methods. augmentations USED-FOR instance discrimination - based contrastive learning. fully supervised upper bound USED-FOR unsupervised learning. distribution divergence USED-FOR retrieval of strongly augmented queries. augmentations USED-FOR contrastive loss. ResNet-50 architecture CONJUNCTION single - layer classifier fine - tuned. single - layer classifier fine - tuned CONJUNCTION ResNet-50 architecture. ImageNet EVALUATE-FOR ResNet-50 architecture. top-1 accuracy EVALUATE-FOR method. ImageNet EVALUATE-FOR method. fully supervised ResNet-50 USED-FOR top-1 accuracy. it COMPARE self - supervised and supervised methods. self - supervised and supervised methods COMPARE it. self - supervised and supervised methods USED-FOR transfer learning and object detection tasks. transfer learning and object detection tasks EVALUATE-FOR it. Metric is generalizability. OtherScientificTerm are distortions, image structures, representation bank, overoptimistic assumption, distorted visual structures, and distributions of weakly augmented counterparts. ",This paper proposes a new contrastive learning method for instance discrimination-based contrastive loss. The proposed method is based on the observation that the distribution of strongly augmented queries is different from that of weakly augmented queries. The authors propose to use the distribution divergence between the two distributions to improve the generalizability of the proposed method. The method is evaluated on ImageNet and transfer learning tasks.,This paper proposes a new contrastive learning method for instance discrimination-based contrastive loss. The proposed method is based on the observation that the distribution of strongly augmented queries is different from that of weakly augmented queries. The authors propose to use the distribution divergence between the two distributions to improve the generalizability of the proposed method. The method is evaluated on ImageNet and transfer learning tasks.
432,SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"magnetic resonance imagery ( MRI ) USED-FOR De - identification. de - identification methods USED-FOR task. MRI de - identification techniques USED-FOR privacy - sensitive facial features. removal - based techniques COMPARE deep learning framework. deep learning framework COMPARE removal - based techniques. segmentation CONJUNCTION age prediction. age prediction CONJUNCTION segmentation. deep learning framework USED-FOR medical analyses. medical analyses FEATURE-OF brain. age prediction HYPONYM-OF medical analyses. segmentation HYPONYM-OF medical analyses. Material are database, and patient ’s MRI scan. Generic are they, and them. OtherScientificTerm is 3D volume. ","This paper proposes a deep learning framework for MRI de-identification for privacy-sensitive facial features. The proposed method is based on a neural network architecture, and is able to learn privacy sensitive facial features from MRI scans. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and privacy. They also show that their method can be applied to a wide range of medical applications.","This paper proposes a deep learning framework for MRI de-identification for privacy-sensitive facial features. The proposed method is based on a neural network architecture, and is able to learn privacy sensitive facial features from MRI scans. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and privacy. They also show that their method can be applied to a wide range of medical applications."
441,SP:0ac3964bd2320341488476d60f57b75d2a79f92c,Graph neural networks USED-FOR modeling graph data. Graph neural networks USED-FOR node classification and link prediction tasks. representation USED-FOR graph. pooling function USED-FOR node representations. pooling function USED-FOR compact form. pooling function USED-FOR representation. task relevance CONJUNCTION structural dependencies. structural dependencies CONJUNCTION task relevance. hierarchical graph pooling methods USED-FOR representation. representation USED-FOR graphs. Weisfeiler - Lehman test FEATURE-OF graphs. Graph Multiset Transformer ( GMT ) HYPONYM-OF multi - head attention based global pooling layer. graph structure FEATURE-OF auxiliary information. auxiliary information FEATURE-OF multiset encoding problem. multiset encoding problem USED-FOR graph pooling problem. injectiveness CONJUNCTION permutation invariance. permutation invariance CONJUNCTION injectiveness. it COMPARE Weisfeiler - Lehman graph isomorphism test. Weisfeiler - Lehman graph isomorphism test COMPARE it. injectiveness FEATURE-OF GMT. permutation invariance FEATURE-OF GMT. node clustering approaches USED-FOR hierarchical graph pooling. methods USED-FOR hierarchical graph pooling. methods USED-FOR node clustering approaches. GMT COMPARE graph pooling methods. graph pooling methods COMPARE GMT. graph classification benchmarks EVALUATE-FOR graph pooling methods. memory and time efficiency EVALUATE-FOR graph pooling methods. graph classification benchmarks EVALUATE-FOR GMT. memory and time efficiency EVALUATE-FOR GMT. Material is graph data. OtherScientificTerm is node features. Generic is they. ,This paper proposes a multi-head attention based global pooling layer for hierarchical graph pooling. The proposed method is based on the Weisfeiler-Lehman graph isomorphism test. The authors show that the proposed method outperforms existing methods on several graph classification and link prediction tasks. ,This paper proposes a multi-head attention based global pooling layer for hierarchical graph pooling. The proposed method is based on the Weisfeiler-Lehman graph isomorphism test. The authors show that the proposed method outperforms existing methods on several graph classification and link prediction tasks. 
450,SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"GNNs USED-FOR prediction task. long - range interaction USED-FOR prediction task. tuning CONJUNCTION weights. weights CONJUNCTION tuning. GCN CONJUNCTION GIN. GIN CONJUNCTION GCN. over - squashing FEATURE-OF GNNs. GNNs COMPARE GAT. GAT COMPARE GNNs. GAT CONJUNCTION GGNN. GGNN CONJUNCTION GAT. GNNs USED-FOR over - squashing. GNNs COMPARE GGNN. GGNN COMPARE GNNs. bottleneck USED-FOR GNNs. GIN HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. Method are graph neural network ( GNN ), and GNN models of long - range problems. OtherScientificTerm are graph, exponentially growing information, fixed - size vectors, long - range signals, and incoming edges. ",This paper studies the problem of over-squashing in graph neural networks (GNNs). The authors show that GGNNs and GCN models of long-range problems suffer from this problem. The authors also show that GCN and GGNN models suffer from the same problem. ,This paper studies the problem of over-squashing in graph neural networks (GNNs). The authors show that GGNNs and GCN models of long-range problems suffer from this problem. The authors also show that GCN and GGNN models suffer from the same problem. 
459,SP:90d8fa381446923902e42b259392e5e975e6caa1,"cross - domain generalizable classifiers USED-FOR methods. methods USED-FOR domain - agnostic representations. annotated data USED-FOR classifier. embedding space USED-FOR domain - agnostic. data distributions USED-FOR domain - agnostic. Task are Sentiment analysis, and marketing strategies. Method are cross - domain sentiment analysis methods, and domain adaptation method. OtherScientificTerm are data annotation, and prototypical distribution. Generic is method. ","This paper proposes a new method for cross-domain generalizable classifiers. The proposed method is based on the idea of domain-agnostic embedding space, which is an extension of the existing domain adaptation method. The authors show that the proposed method can be applied to a wide range of datasets. The method is evaluated on a variety of datasets, and is shown to outperform existing methods. ","This paper proposes a new method for cross-domain generalizable classifiers. The proposed method is based on the idea of domain-agnostic embedding space, which is an extension of the existing domain adaptation method. The authors show that the proposed method can be applied to a wide range of datasets. The method is evaluated on a variety of datasets, and is shown to outperform existing methods. "
468,SP:893fd7440b82f5da0d4c0944928810322eaee2f0,Gender - bias stereotypes PART-OF natural language processing. genderbias FEATURE-OF natural language understanding. evaluation of genderbias PART-OF natural language understanding. inference USED-FOR natural language understanding. inference USED-FOR evaluation of genderbias. gender neutral premise COMPARE gender - specific hypothesis. gender - specific hypothesis COMPARE gender neutral premise. NLI models USED-FOR gender stereotypes. challenge task USED-FOR NLI models. occupations USED-FOR NLI models. BERT CONJUNCTION RoBERTa. RoBERTa CONJUNCTION BERT. RoBERTa CONJUNCTION BART. BART CONJUNCTION RoBERTa. models USED-FOR genderinduced prediction errors. BERT CONJUNCTION BART. BART CONJUNCTION BERT. MNLI and SNLI data - sets USED-FOR models. BART HYPONYM-OF models. BERT HYPONYM-OF models. RoBERTa HYPONYM-OF models. Generic is evaluation methodology. Method is debiasing techniques. Material is gender - balanced dataset. ,"This paper proposes a new evaluation method for evaluating gender bias in NLI models. The authors propose a challenge task to evaluate the performance of gender-biased models on MNLI, SNLI, and BERT datasets. The proposed method is based on a debiasing technique, where the debiased model is trained on a gender-balanced dataset and evaluated on the task of evaluating the gender bias of the model. The paper also proposes a gender bias evaluation method to evaluate gender bias. The method is evaluated on three different datasets: RoBERTa, BERT, and BART. ","This paper proposes a new evaluation method for evaluating gender bias in NLI models. The authors propose a challenge task to evaluate the performance of gender-biased models on MNLI, SNLI, and BERT datasets. The proposed method is based on a debiasing technique, where the debiased model is trained on a gender-balanced dataset and evaluated on the task of evaluating the gender bias of the model. The paper also proposes a gender bias evaluation method to evaluate gender bias. The method is evaluated on three different datasets: RoBERTa, BERT, and BART. "
477,SP:a32ab755bd249c393b70938036ce8e810c0c439f,"variational intrinsic control ( VIC ) HYPONYM-OF unsupervised reinforcement learning method. other HYPONYM-OF VIC algorithms. one HYPONYM-OF VIC algorithms. intrinsic reward USED-FOR latter. transitional probability model CONJUNCTION Gaussian mixture model. Gaussian mixture model CONJUNCTION transitional probability model. transitional probability model USED-FOR methods. Gaussian mixture model USED-FOR methods. OtherScientificTerm are intrinsic options, and stochastic environments. ","This paper studies the problem of unsupervised reinforcement learning in the context of intrinsic control. The authors propose a new algorithm for this problem, called Variational Intrinsic Control (VIC), which uses a Gaussian mixture model to model the transition between the intrinsic reward and the non-interventional reward. They show that the proposed algorithm outperforms the baselines in a number of experiments. ","This paper studies the problem of unsupervised reinforcement learning in the context of intrinsic control. The authors propose a new algorithm for this problem, called Variational Intrinsic Control (VIC), which uses a Gaussian mixture model to model the transition between the intrinsic reward and the non-interventional reward. They show that the proposed algorithm outperforms the baselines in a number of experiments. "
486,SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,Deep neural networks USED-FOR image classification. low data regime FEATURE-OF sample efficiency. ensemble of relatively small deep networks USED-FOR image classification problems. neural ensembling USED-FOR small data domains. technique COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE technique. deep ensembling COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE deep ensembling. deep ensembling HYPONYM-OF technique. Generic is they. Material is small datasets. Method is ensemble configurations. OtherScientificTerm is losses. ,"This paper proposes a new method to improve sample efficiency in the low-data regime of deep neural networks. The method is based on the idea of ensemble of relatively small deep networks (e.g., ensemble of convolutional neural networks). The authors show that the proposed method is able to achieve better sample efficiency than the state-of-the-art in terms of the number of samples. The authors also show that their method can be used to improve the sample efficiency of existing deep ensembling methods. ","This paper proposes a new method to improve sample efficiency in the low-data regime of deep neural networks. The method is based on the idea of ensemble of relatively small deep networks (e.g., ensemble of convolutional neural networks). The authors show that the proposed method is able to achieve better sample efficiency than the state-of-the-art in terms of the number of samples. The authors also show that their method can be used to improve the sample efficiency of existing deep ensembling methods. "
495,SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,computational power and storage requirements CONJUNCTION processing speed. processing speed CONJUNCTION computational power and storage requirements. DNN - based applications USED-FOR InternetOf - Things ( IoT ) devices. them USED-FOR DNN - based applications. quantized networks COMPARE Binary Neural Networks ( BNNs ). Binary Neural Networks ( BNNs ) COMPARE quantized networks. speed - up EVALUATE-FOR Binary Neural Networks ( BNNs ). fixed and limited compression factor FEATURE-OF they. positive 0/1 binary weights COMPARE -1/+1 weights. -1/+1 weights COMPARE positive 0/1 binary weights. Sparse Binary Neural Networks HYPONYM-OF model and training scheme. -1/+1 weights COMPARE binary networks. binary networks COMPARE -1/+1 weights. sparsity FEATURE-OF BNNs. positive 0/1 binary weights USED-FOR sparsity. compression factor EVALUATE-FOR method. MNIST and CIFAR-10 datasets USED-FOR linear and convolutional networks. linear and convolutional networks EVALUATE-FOR method. it USED-FOR DNNs. compression rates CONJUNCTION generalization. generalization CONJUNCTION compression rates. generalization EVALUATE-FOR SBNNs. compression rates EVALUATE-FOR SBNNs. Method is Quantized neural networks. Metric is accuracy. Material is limited resources. ,"This paper proposes a new training scheme for quantized neural networks (BNNs). The proposed method, called Sparse Binary Neural Networks (SBNNs), is based on the idea that quantized BNNs have a fixed and limited compression factor. The authors show that SBNNs are able to achieve better compression rates and generalization performance compared to binary neural networks. They also show that the proposed method is able to reduce the sparsity of binary networks. ","This paper proposes a new training scheme for quantized neural networks (BNNs). The proposed method, called Sparse Binary Neural Networks (SBNNs), is based on the idea that quantized BNNs have a fixed and limited compression factor. The authors show that SBNNs are able to achieve better compression rates and generalization performance compared to binary neural networks. They also show that the proposed method is able to reduce the sparsity of binary networks. "
504,SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"OOD data USED-FOR model calibration. outlier exposure USED-FOR model probabilities. outlier exposure USED-FOR method. estimates of class membership probabilities USED-FOR model predictions. baseline method USED-FOR predictive uncertainty. softmax probabilities USED-FOR model. softmax probabilities USED-FOR baseline method. softmax probabilities USED-FOR predictive uncertainty. Stochastic Variational Bayesian Inference ( SVBI ) USED-FOR deep learning. model ensembles CONJUNCTION Stochastic Variational Bayesian Inference ( SVBI ). Stochastic Variational Bayesian Inference ( SVBI ) CONJUNCTION model ensembles. Stochastic Variational Bayesian Inference ( SVBI ) HYPONYM-OF approaches. temperature scaling HYPONYM-OF approaches. model ensembles HYPONYM-OF approaches. predicted error rates CONJUNCTION actual error rates. actual error rates CONJUNCTION predicted error rates. calibration error FEATURE-OF methods. Metric is accuracy. Task are Predictive uncertainty, and PREDICTIVE UNCERTAINTY. Generic are models, and measures. Method are post hoc calibration method, machine learning model, and Uncertainty estimates. Material is corrupted data. OtherScientificTerm are class membership probabilities, model outputs, pmax, and Brier score. ",This paper proposes a post hoc calibration method for model calibration. The proposed method is based on the idea that the class membership probabilities of a model can be used as a proxy for the predictive uncertainty of the model. The authors show that the proposed method outperforms the baselines in terms of accuracy and calibration error. They also show that their method is more robust to out-of-distribution (OOD) data.,This paper proposes a post hoc calibration method for model calibration. The proposed method is based on the idea that the class membership probabilities of a model can be used as a proxy for the predictive uncertainty of the model. The authors show that the proposed method outperforms the baselines in terms of accuracy and calibration error. They also show that their method is more robust to out-of-distribution (OOD) data.
513,SP:ea503f67e38fce7dee9cc4996b55b8959911f030,Graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION Graph neural networks. graph kernels USED-FOR machine learning problems. Graph neural networks USED-FOR machine learning problems. graphs USED-FOR machine learning problems. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. approaches USED-FOR graph properties. approaches USED-FOR non - isomorphic graphs. graph representations USED-FOR similarity / distance of graphs. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. expressive power EVALUATE-FOR graph kernels. expressive power FEATURE-OF graph neural networks. algorithms COMPARE those. those COMPARE algorithms. graph representations and similarities COMPARE those. those COMPARE graph representations and similarities. algorithms USED-FOR graph representations and similarities. models CONJUNCTION kernels. kernels CONJUNCTION models. node attributes USED-FOR kernels. node attributes USED-FOR models. graph kernels COMPARE graph neural networks. graph neural networks COMPARE graph kernels. ,This paper studies the expressive power of graph neural networks and graph kernels. The authors consider the problem of learning the similarity and distance between two nodes in a graph. They show that graph kernels can be used to learn graph representations and similarities. They also propose a new algorithm for learning the distance between nodes in the graph. ,This paper studies the expressive power of graph neural networks and graph kernels. The authors consider the problem of learning the similarity and distance between two nodes in a graph. They show that graph kernels can be used to learn graph representations and similarities. They also propose a new algorithm for learning the distance between nodes in the graph. 
522,SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"inpainting USED-FOR warping artifacts. data augmentation techniques USED-FOR regularizing non - warp - based image generation. them USED-FOR image animation. difficulty of inpainting FEATURE-OF warped image. CutMix HYPONYM-OF data augmentation techniques. PriorityCut USED-FOR image animation. augmentation approach USED-FOR image animation. PriorityCut HYPONYM-OF augmentation approach. low - level similarity CONJUNCTION keypoint distance. keypoint distance CONJUNCTION low - level similarity. keypoint distance CONJUNCTION feature embedding distance. feature embedding distance CONJUNCTION keypoint distance. pixel - wise difference CONJUNCTION low - level similarity. low - level similarity CONJUNCTION pixel - wise difference. PriorityCut COMPARE vanilla CutMix. vanilla CutMix COMPARE PriorityCut. PriorityCut COMPARE image animation models. image animation models COMPARE PriorityCut. PriorityCut USED-FOR identity. vanilla CutMix COMPARE image animation models. image animation models COMPARE vanilla CutMix. low - level similarity EVALUATE-FOR image animation models. pixel - wise difference EVALUATE-FOR image animation models. inpainting USED-FOR warping artifacts. PriorityCut USED-FOR regularize discriminator predictions. occlusion information USED-FOR regularize discriminator predictions. regularize discriminator predictions USED-FOR inpainting. occlusion information USED-FOR image animation. occlusion information USED-FOR PriorityCut. Method are Image animation, Self - supervised image animation approaches, self - supervised image animation approaches, and Warp - based image animation. OtherScientificTerm are motion of a driving video, pose references, motion of the driving video, pose differences, guidance, inpainted regions, motion of the driving image, smooth transitions, and mixture of context. Task is learning. ","This paper proposes a data augmentation method for self-supervised image animation. The proposed method, called PriorityCut, is based on CutMix, which augments the feature embedding distance and keypoint distance of an image with an occlusion information to improve the performance of the model. The method is evaluated on a driving video dataset and compared to a number of state-of-the-art methods. ","This paper proposes a data augmentation method for self-supervised image animation. The proposed method, called PriorityCut, is based on CutMix, which augments the feature embedding distance and keypoint distance of an image with an occlusion information to improve the performance of the model. The method is evaluated on a driving video dataset and compared to a number of state-of-the-art methods. "
531,SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"approaches USED-FOR disentangled representations. data generation process USED-FOR independent latent variables. independent causal mechanisms ( ICM ) COMPARE disentangled representations. disentangled representations COMPARE independent causal mechanisms ( ICM ). coarse granularity FEATURE-OF data generation processes ( mechanisms ). observational data USED-FOR groundtruth mechanisms. unconventional mixture prior USED-FOR self - supervised generative model. self - supervised generative model USED-FOR mechanisms. mechanisms PART-OF self - supervised scenario. intervention CONJUNCTION covariant shift. covariant shift CONJUNCTION intervention. covariant shift CONJUNCTION noise. noise CONJUNCTION covariant shift. downstream tasks EVALUATE-FOR approach. approach COMPARE disentangled representations. disentangled representations COMPARE approach. approach USED-FOR intervention. covariant shift USED-FOR approach. noise EVALUATE-FOR approach. downstream tasks EVALUATE-FOR disentangled representations. downstream tasks EVALUATE-FOR approach. Generic are model, and methods. OtherScientificTerm is disentanglement. ","This paper proposes a self-supervised method for learning disentangled representations from observational data. The proposed method is based on a mixture prior, which is used to learn the groundtruth mechanisms. The authors show that the proposed method outperforms the baselines on a number of downstream tasks.","This paper proposes a self-supervised method for learning disentangled representations from observational data. The proposed method is based on a mixture prior, which is used to learn the groundtruth mechanisms. The authors show that the proposed method outperforms the baselines on a number of downstream tasks."
540,SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"2D image USED-FOR molecular graph structure ( W ). graph aligning approach USED-FOR rich or detailed labels. 2D images USED-FOR chemical compound graphs. domain adaptation COMPARE pretrained model. pretrained model COMPARE domain adaptation. domain adaptation USED-FOR model. Maybridge data set EVALUATE-FOR self - labeling approach. Task are machine learning, and predicting chemical compound graphs. Method are mediating representation V, and machine learning model. OtherScientificTerm are f, normal labels W, fully mediating layer, and mediating layer. ","This paper proposes a self-labeling approach for predicting chemical compound graphs from 2D images. The proposed approach is based on the idea of self-identification, which is an extension of graph aligning (GAL) to the domain adaptation task. The authors show that the proposed method is able to achieve state-of-the-art performance on the Maybridge dataset. ","This paper proposes a self-labeling approach for predicting chemical compound graphs from 2D images. The proposed approach is based on the idea of self-identification, which is an extension of graph aligning (GAL) to the domain adaptation task. The authors show that the proposed method is able to achieve state-of-the-art performance on the Maybridge dataset. "
549,SP:ad906dd9a176cffd283593321ff6b9ad19595528,domain knowledge based deep learning framework USED-FOR chiller plants energy optimization problems. image classification CONJUNCTION NLP. NLP CONJUNCTION image classification. deep network USED-FOR realworld physical systems. NLP HYPONYM-OF deep learning. image classification HYPONYM-OF deep learning. methods USED-FOR complex systems. methods USED-FOR linear model. linear model USED-FOR complex systems. deep network USED-FOR nonlinear model. domain knowledge USED-FOR deep network. domain knowledge USED-FOR nonlinear model. redundancy function space FEATURE-OF nonlinear model. domain knowledge USED-FOR small sample size problem. energy consumption estimation FEATURE-OF chillers. input - output monotonic problem USED-FOR energy consumption estimation. monotonic constraints FEATURE-OF Neural Network. framework COMPARE ones. ones COMPARE framework. framework USED-FOR energy optimization. method COMPARE ones. ones COMPARE method. data center FEATURE-OF cooling system. cooling system EVALUATE-FOR method. ,This paper proposes a domain knowledge based deep learning framework for chiller plants energy optimization problems. The proposed method is based on the idea of using domain knowledge to learn a non-linear model for the input-output monotonic problem. The authors show that the proposed method outperforms existing methods in terms of energy consumption estimation. ,This paper proposes a domain knowledge based deep learning framework for chiller plants energy optimization problems. The proposed method is based on the idea of using domain knowledge to learn a non-linear model for the input-output monotonic problem. The authors show that the proposed method outperforms existing methods in terms of energy consumption estimation. 
558,SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,retail forecasting CONJUNCTION urban traffic forecasting. urban traffic forecasting CONJUNCTION retail forecasting. weather forecasts CONJUNCTION retail forecasting. retail forecasting CONJUNCTION weather forecasts. spatio - temporal predictions USED-FOR real - world applications. spatio - temporal predictions USED-FOR large - scale systems. weather forecasts HYPONYM-OF real - world applications. urban traffic forecasting HYPONYM-OF real - world applications. retail forecasting HYPONYM-OF real - world applications. methods USED-FOR predicting variables. interpretability EVALUATE-FOR forecasting models. methods USED-FOR forecasting models. collaborative causal spatio - temporal fusion transformer USED-FOR collaborative causal effects of predictors. collaborative causal effects of predictors USED-FOR forecasting targets. CausalTrans HYPONYM-OF collaborative causal spatio - temporal fusion transformer. causal attention USED-FOR causal inference. nodes PART-OF graph. Taylor ’s expansion CONJUNCTION softmax. softmax CONJUNCTION Taylor ’s expansion. time complexity EVALUATE-FOR multi - head attention. softmax USED-FOR multi - head attention. Taylor ’s expansion USED-FOR multi - head attention. time efficiency EVALUATE-FOR CausalTrans. model components USED-FOR CausalTrans. time efficiency EVALUATE-FOR model components. error reduction EVALUATE-FOR baseline methods. CausalTrans framework COMPARE baseline methods. baseline methods COMPARE CausalTrans framework. error reduction EVALUATE-FOR CausalTrans framework. Material is ride - sharing platforms. Method is spatial graph fusion mechanism. ,"This paper proposes a new framework for spatio-temporal forecasting based on collaborative causal inference. The proposed framework is based on the idea of collaborative causal spatio temporal fusion transformer. The authors propose a new model called CausalTrans, which is a multi-head attention model that combines Taylor expansion, softmax, and causal attention to improve the interpretability of forecasting models. Experiments show that the proposed model outperforms baselines in terms of interpretability and error reduction.","This paper proposes a new framework for spatio-temporal forecasting based on collaborative causal inference. The proposed framework is based on the idea of collaborative causal spatio temporal fusion transformer. The authors propose a new model called CausalTrans, which is a multi-head attention model that combines Taylor expansion, softmax, and causal attention to improve the interpretability of forecasting models. Experiments show that the proposed model outperforms baselines in terms of interpretability and error reduction."
567,SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"unsupervised framework USED-FOR problem. coupled mixture VAE ( cpl - mixVAE ) HYPONYM-OF unsupervised framework. interacting autoencoding agents USED-FOR unsupervised framework. variational inference problem USED-FOR it. categorical assignments EVALUATE-FOR approach. MNIST and dSprites EVALUATE-FOR approach. approach USED-FOR type - specific, activity - regulated genes. type - specific, activity - regulated genes PART-OF single - cell gene expression dataset. cortical neuron types FEATURE-OF single - cell gene expression dataset. single - cell gene expression dataset EVALUATE-FOR approach. OtherScientificTerm are mixture of discrete and continuous factors of variability, and continuous factors. Method are mixture representations, and multi - agent framework. ",This paper proposes a multi-agent unsupervised VAE framework for learning a mixture of discrete and continuous factors of variability. The proposed method is based on the idea of coupled mixture VAE (cpl-mixVAE). The authors propose a variational inference problem to solve the problem. The authors show that the proposed method outperforms existing methods on the single-cell gene expression dataset. ,This paper proposes a multi-agent unsupervised VAE framework for learning a mixture of discrete and continuous factors of variability. The proposed method is based on the idea of coupled mixture VAE (cpl-mixVAE). The authors propose a variational inference problem to solve the problem. The authors show that the proposed method outperforms existing methods on the single-cell gene expression dataset. 
576,SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"Group equivariant convolutional networks ( GCNNs ) USED-FOR convolutional networks. symmetry priors FEATURE-OF convolutional networks. convolutions USED-FOR models. equivariance constraint FEATURE-OF kernels. G - steerable kernels USED-FOR convolutions. G HYPONYM-OF compact group. constraints FEATURE-OF steerable kernels. constraints CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION constraints. steerable kernels CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION steerable kernels. quantum mechanics USED-FOR spherical tensor operators. generalized reduced matrix elements CONJUNCTION ClebschGordan coefficients. ClebschGordan coefficients CONJUNCTION generalized reduced matrix elements. ClebschGordan coefficients CONJUNCTION harmonic basis functions. harmonic basis functions CONJUNCTION ClebschGordan coefficients. Wigner - Eckart theorem USED-FOR spherical tensor operators. homogeneous spaces FEATURE-OF harmonic basis functions. generalized reduced matrix elements USED-FOR steerable kernel spaces. Method is GCNNs. OtherScientificTerm are G - steerability constraint, and Gsteerable kernel spaces. Generic is it. ","This paper studies the group equivariant convolutional networks (GCNNs) with steerable kernels. The authors show that the kernel space of G-steerable kernels is a compact group, and that the equivariance constraint of the group is the same as that of spherical tensor operators. They also show that steerable kernel spaces are homogeneous spaces, which is a result of the Wigner-Eckart theorem. ","This paper studies the group equivariant convolutional networks (GCNNs) with steerable kernels. The authors show that the kernel space of G-steerable kernels is a compact group, and that the equivariance constraint of the group is the same as that of spherical tensor operators. They also show that steerable kernel spaces are homogeneous spaces, which is a result of the Wigner-Eckart theorem. "
585,SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"it USED-FOR accuracy disparities. average accuracies EVALUATE-FOR selective classification. accuracy EVALUATE-FOR selective classification. selective classification USED-FOR full - coverage accuracy disparities. models EVALUATE-FOR selective classification. full - coverage accuracies EVALUATE-FOR distributionally - robust models. Method is Selective classification. OtherScientificTerm are abstentions, spurious correlations, margin distribution, symmetric margin distributions, and left - log - concavity. Material is vision and NLP datasets. Metric is accuracies. Generic is distribution. ","This paper studies the problem of selective classification in the presence of abstentions and spurious correlations in the margin distribution. The authors propose a new metric to measure the gap between the average accuracies and the full coverage of a classifier. They show that the gap can be measured as a function of the left-log-concavity of the margin distributions, which is a measure of how well the classifier is able to discriminate between classes. They also show that this metric can be used to evaluate the performance of different models on different datasets.","This paper studies the problem of selective classification in the presence of abstentions and spurious correlations in the margin distribution. The authors propose a new metric to measure the gap between the average accuracies and the full coverage of a classifier. They show that the gap can be measured as a function of the left-log-concavity of the margin distributions, which is a measure of how well the classifier is able to discriminate between classes. They also show that this metric can be used to evaluate the performance of different models on different datasets."
594,SP:f1d57ee27e901daf7e4e2b84139019e945818911,multi - layer network analysis CONJUNCTION temporal document classification. temporal document classification CONJUNCTION multi - layer network analysis. temporal document classification CONJUNCTION video data analysis. video data analysis CONJUNCTION temporal document classification. topic modeling USED-FOR applications. complex multi - modal structure FEATURE-OF applications. complex multi - modal structure FEATURE-OF topic modeling. complex multi - modal structure FEATURE-OF large - scale data. latent hierarchical structure FEATURE-OF multi - modal data. large - scale data USED-FOR topic modeling. video data analysis HYPONYM-OF applications. multi - layer network analysis HYPONYM-OF applications. temporal document classification HYPONYM-OF applications. Neural NCPD USED-FOR hierarchical topic modeling. Neural NCPD HYPONYM-OF training method. multi - modal tensor data USED-FOR hierarchical topic modeling. neural network architecture CONJUNCTION backpropagation. backpropagation CONJUNCTION neural network architecture. backpropagation USED-FOR error propagation. hierarchical NCPD USED-FOR error propagation. neural network architecture USED-FOR Neural NCPD. backpropagation USED-FOR Neural NCPD. ,"This paper proposes a new training method for hierarchical topic modeling based on neural network architecture and backpropagation. The proposed method, called Neural NCPD, is based on the idea that the latent structure of multi-modal tensor data can be used to learn a hierarchical topic model. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of tasks. ","This paper proposes a new training method for hierarchical topic modeling based on neural network architecture and backpropagation. The proposed method, called Neural NCPD, is based on the idea that the latent structure of multi-modal tensor data can be used to learn a hierarchical topic model. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of tasks. "
603,SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"node classification CONJUNCTION image segmentation. image segmentation CONJUNCTION node classification. graph CONJUNCTION image. image CONJUNCTION graph. image segmentation CONJUNCTION named - entity recognition. named - entity recognition CONJUNCTION image segmentation. classifier USED-FOR tasks. named - entity recognition HYPONYM-OF tasks. node classification HYPONYM-OF tasks. image segmentation HYPONYM-OF tasks. adversarial robustness certificates USED-FOR tasks. locality property USED-FOR collective certificate. single - node certificates PART-OF collective certificate. locality property CONJUNCTION perturbations. perturbations CONJUNCTION locality property. collective certificate USED-FOR node classification. Citeseer dataset EVALUATE-FOR collective certificate. OtherScientificTerm are perturbed inputs, and perturbation. Method are collective robustness certificate, and Graph Neural Networks. ",This paper proposes a new adversarial robustness certificate for graph neural networks. The proposed certificate is based on the locality property of the perturbed inputs. The authors show that the proposed certificate can be used for node classification and image segmentation tasks. ,This paper proposes a new adversarial robustness certificate for graph neural networks. The proposed certificate is based on the locality property of the perturbed inputs. The authors show that the proposed certificate can be used for node classification and image segmentation tasks. 
612,SP:cc93dd2f68e415e2457166e78627865dc1b44697,"generative models USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. generative and discriminative neural networks USED-FOR Learning high - dimensional probability distributions. non - convergence problem CONJUNCTION mode collapse. mode collapse CONJUNCTION non - convergence problem. mode collapse CONJUNCTION gradient explosion or vanishing. gradient explosion or vanishing CONJUNCTION mode collapse. non - convergence problem FEATURE-OF GANs. Least Squares GAN ( LSGANs ) CONJUNCTION Wasserstein GANs ( WGAN ). Wasserstein GANs ( WGAN ) CONJUNCTION Least Squares GAN ( LSGANs ). Wasserstein GANs ( WGAN ) HYPONYM-OF GANs. Least Squares GAN ( LSGANs ) HYPONYM-OF GANs. LSGANs USED-FOR mode collapse. quantile regression USED-FOR 1 - Wasserstein distance. modification of loss functions USED-FOR GANs. approach USED-FOR modification of loss functions. approach USED-FOR GANs. quantile regression USED-FOR Quantile Regression GAN ( QRGAN ). discriminator CONJUNCTION gradients. gradients CONJUNCTION discriminator. QRGAN USED-FOR mode collapse problem. robustness EVALUATE-FOR QRGAN. QRGAN COMPARE GANs. GANs COMPARE QRGAN. generation performance assessment EVALUATE-FOR GANs. evaluation EVALUATE-FOR Frechet Inception Distance ( FID ). Frechet Inception Distance ( FID ) USED-FOR generation performance assessment. Frechet Inception Distance ( FID ) EVALUATE-FOR GANs. Frechet Inception Distance ( FID ) EVALUATE-FOR QRGAN. generation performance assessment EVALUATE-FOR QRGAN. evaluation EVALUATE-FOR QRGAN. Method are modification methodology of loss functions, WGANs, and Wasserstein distance approximation. OtherScientificTerm are local minima, inefficient computation, and real and generated data distribution. ",This paper studies the problem of mode collapse and mode collapse in GANs. The authors propose a modification of the loss function of WGANs and LSGANs to alleviate the mode collapse problem. The proposed method is based on quantile regression. The method is evaluated on the Frechet Inception Distance (FID) and the generation performance of QRGAN. ,This paper studies the problem of mode collapse and mode collapse in GANs. The authors propose a modification of the loss function of WGANs and LSGANs to alleviate the mode collapse problem. The proposed method is based on quantile regression. The method is evaluated on the Frechet Inception Distance (FID) and the generation performance of QRGAN. 
621,SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,relevance metrics USED-FOR similarity - based explanation. cosine similarity FEATURE-OF gradients. Method is machine learning models. Generic is metrics. ,"This paper studies the cosine similarity of gradients of neural networks. The authors propose a new metric to measure the similarity between gradients. The proposed metric is based on the idea that cosine similarities are a measure of how well a neural network is able to explain the gradients in the input space.  The authors show that this metric can be used to measure how well neural networks can explain gradients, and that it can also be used as an explanation for how well they can explain the gradient of a particular classifier. ","This paper studies the cosine similarity of gradients of neural networks. The authors propose a new metric to measure the similarity between gradients. The proposed metric is based on the idea that cosine similarities are a measure of how well a neural network is able to explain the gradients in the input space.  The authors show that this metric can be used to measure how well neural networks can explain gradients, and that it can also be used as an explanation for how well they can explain the gradient of a particular classifier. "
630,SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"generalization power EVALUATE-FOR Graph Neural Networks ( GNNs ). algorithmic alignment USED-FOR graph isomorphism test. LRGA module PART-OF GNNs. LRGA USED-FOR it. sample complexity EVALUATE-FOR kernel ’s feature map. 2 - FWL update step USED-FOR RGNN. LRGA USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR kernel ’s feature map. polynomial kernels USED-FOR RGNN. polynomial kernels USED-FOR 2 - FWL update step. LRGA USED-FOR GNN layers. LRGA USED-FOR GNN architectures. Method are dot - product attention, and expressive GNNs. OtherScientificTerm is generalization properties. Generic is kernel. Material is GNN benchmarks. ","This paper studies the generalization properties of graph neural networks (GNNs) in the context of the graph isomorphism test. The authors propose a new algorithm for learning graph neural network (RGNNs), called LRGA, which aims to improve the generalizability of GNNs. The main contribution of the paper is to introduce the LRGA module to the GNN architecture. The proposed algorithm is based on the 2-FWL update step, which is a regularization step of the kernel of a GNN.  The authors show that LRGA can improve the sample complexity of a GPNN by up to a factor of 2. They also show that the proposed algorithm can be applied to any GNN with polynomial kernels.","This paper studies the generalization properties of graph neural networks (GNNs) in the context of the graph isomorphism test. The authors propose a new algorithm for learning graph neural network (RGNNs), called LRGA, which aims to improve the generalizability of GNNs. The main contribution of the paper is to introduce the LRGA module to the GNN architecture. The proposed algorithm is based on the 2-FWL update step, which is a regularization step of the kernel of a GNN.  The authors show that LRGA can improve the sample complexity of a GPNN by up to a factor of 2. They also show that the proposed algorithm can be applied to any GNN with polynomial kernels."
639,SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"objectness measures USED-FOR calibration. objectness measures USED-FOR Convolutional Neural Networks ( CNNs ). calibration EVALUATE-FOR Convolutional Neural Networks ( CNNs ). loss functions USED-FOR classification CNNs. CNNs USED-FOR classifiers. transformation USED-FOR CNN. random crops USED-FOR approaches. Context dependence FEATURE-OF safety - critical applications. objectness CONJUNCTION label smoothing. label smoothing CONJUNCTION objectness. approach USED-FOR classification. label smoothing USED-FOR training. objectness USED-FOR approach. label smoothing USED-FOR approach. relative object size USED-FOR smoothing factor. approach USED-FOR confidences. adaptive label smoothing USED-FOR CNNs. approach COMPARE baselines. baselines COMPARE approach. MS COCO COMPARE hard label approach. hard label approach COMPARE MS COCO. transfer learning USED-FOR MS COCO. transfer learning COMPARE hard label approach. hard label approach COMPARE transfer learning. Generic are they, and methods. Material are ImageNet-1 K, ImageNet, and context only images. Method is class activation maps. Task is classification and transfer learning tasks. ","This paper proposes an adaptive label smoothing method for classification and transfer learning tasks. The proposed method is based on the idea that the class activation maps of a classifier can be transformed into a single class activation map, which can then be used to improve the performance of the classifier. The method is evaluated on ImageNet-1K, ImageNet, and ImageNet with context only images. The results show that the proposed method outperforms existing methods in terms of classification accuracy. ","This paper proposes an adaptive label smoothing method for classification and transfer learning tasks. The proposed method is based on the idea that the class activation maps of a classifier can be transformed into a single class activation map, which can then be used to improve the performance of the classifier. The method is evaluated on ImageNet-1K, ImageNet, and ImageNet with context only images. The results show that the proposed method outperforms existing methods in terms of classification accuracy. "
648,SP:5254658923e594294b69d124a8d004166852822a,"Neural networks USED-FOR inverse problems. convex dual network USED-FOR interpreting training and prediction. convex solvers USED-FOR convex dual network. neural networks USED-FOR path sparsity. weight decay regularization FEATURE-OF neural networks. piecewise linear filtering USED-FOR prediction. MNIST and fastMRI datasets EVALUATE-FOR dual network optimization problem. Task is medical imaging. Method are convex duality framework, and convex optimization. ","This paper studies the problem of learning a convex dual network for inverse problems. In particular, the authors consider the case where the input is a neural network and the output of the dual network is a linear combination of the weights of the two networks. The authors show that the dual dual network can be solved by convex solvers, and show that it is possible to solve the dual problem with weight decay regularization. They also show that this regularization can be used to improve the performance of neural networks in the case of path sparsity.","This paper studies the problem of learning a convex dual network for inverse problems. In particular, the authors consider the case where the input is a neural network and the output of the dual network is a linear combination of the weights of the two networks. The authors show that the dual dual network can be solved by convex solvers, and show that it is possible to solve the dual problem with weight decay regularization. They also show that this regularization can be used to improve the performance of neural networks in the case of path sparsity."
657,SP:085cad6bc143c8713580bddfaa71f06496dac314,processing stages PART-OF text - to - speech synthesis pipelines. models USED-FOR raw speech audio outputs. character or phoneme input sequences USED-FOR models. generator USED-FOR inference. generator USED-FOR training. training CONJUNCTION inference. inference CONJUNCTION training. token length prediction USED-FOR differentiable alignment scheme. differentiable alignment scheme USED-FOR generator. adversarial feedback CONJUNCTION prediction losses. prediction losses CONJUNCTION adversarial feedback. total duration CONJUNCTION mel - spectrogram. mel - spectrogram CONJUNCTION total duration. prediction losses USED-FOR It. adversarial feedback USED-FOR It. soft dynamic time warping USED-FOR spectrogram - based prediction loss. soft dynamic time warping USED-FOR model. model COMPARE models. models COMPARE model. mean opinion score EVALUATE-FOR model. multi - stage training USED-FOR models. OtherScientificTerm is normalised text or phonemes. ,This paper proposes a differentiable alignment scheme for text-to-speech synthesis. The proposed method is based on adversarial feedback and prediction losses. The authors also propose a soft dynamic time warping method to improve the performance of the model. Experiments show that the proposed method outperforms the state-of-the-art.,This paper proposes a differentiable alignment scheme for text-to-speech synthesis. The proposed method is based on adversarial feedback and prediction losses. The authors also propose a soft dynamic time warping method to improve the performance of the model. Experiments show that the proposed method outperforms the state-of-the-art.
666,SP:01148cea55db606aa78d27e900818684a8bce9ab,"attributed graphs FEATURE-OF real - world graphs. non - topological features FEATURE-OF nodes. attributes PART-OF attributed graph. lower - dimensional space FEATURE-OF discrete distributions. Wasserstein metric USED-FOR lower - dimensional space. Wasserstein metric USED-FOR discrete distributions. Wasserstein graph diffusion USED-FOR distribution representations of nodes. topology structure CONJUNCTION attributes. attributes CONJUNCTION topology structure. point representations USED-FOR downstream tasks. it USED-FOR node classification. algorithms USED-FOR node classification. algorithms USED-FOR matrix completion. node classification CONJUNCTION matrix completion. matrix completion CONJUNCTION node classification. it USED-FOR matrix completion. algorithms EVALUATE-FOR representation method. missing attributes USED-FOR node classification. it USED-FOR algorithms. Method are node representation learning approaches, and non - parametric framework. OtherScientificTerm are incomplete information, decomposition of the attribute matrix, node features, Wasserstein space, and local neighborhoods. Metric is distortion. ","This paper proposes a non-parametric representation learning framework for node classification and matrix completion based on Wasserstein graph diffusion. The proposed method is motivated by the observation that the distribution of the attribute matrix of a given node can be represented as a lower-dimensional space. The authors propose to use the W-Wasserstein metric to learn the distribution representation of the attributes of a node. They show that the proposed method can be used to learn node representation learning algorithms for matrix completion, node classification, and node classification with missing attributes. ","This paper proposes a non-parametric representation learning framework for node classification and matrix completion based on Wasserstein graph diffusion. The proposed method is motivated by the observation that the distribution of the attribute matrix of a given node can be represented as a lower-dimensional space. The authors propose to use the W-Wasserstein metric to learn the distribution representation of the attributes of a node. They show that the proposed method can be used to learn node representation learning algorithms for matrix completion, node classification, and node classification with missing attributes. "
675,SP:aeeb5909f7123ef631f569b469af9715205c881f,"Adversarially Motivated Intrinsic GOals USED-FOR goal - conditioned “ student ” policy. AMIGO HYPONYM-OF agent. meta - learning USED-FOR agent. goal - generating teacher PART-OF agent. intrinsic motivation CONJUNCTION RL methods. RL methods CONJUNCTION intrinsic motivation. Task are reinforcement learning ( RL ), and procedurally - generated tasks. OtherScientificTerm are sparse extrinsic rewards, environment reward, and constructively adversarial ” objective. Generic is method. ","This paper proposes a meta-learning method for learning a goal-conditioned policy that can be used in reinforcement learning. The method is based on the idea that the goal-generating teacher should be able to learn a policy that maximizes the intrinsic motivation of the student, and the student should learn to maximize the extrinsic reward of the teacher. The proposed method is evaluated on a number of tasks and shows that the proposed method outperforms the baselines. ","This paper proposes a meta-learning method for learning a goal-conditioned policy that can be used in reinforcement learning. The method is based on the idea that the goal-generating teacher should be able to learn a policy that maximizes the intrinsic motivation of the student, and the student should learn to maximize the extrinsic reward of the teacher. The proposed method is evaluated on a number of tasks and shows that the proposed method outperforms the baselines. "
684,SP:3d05bc7dca97681cb582298e318b9b973841eed3,"user distortion CONJUNCTION user privacy constraint. user privacy constraint CONJUNCTION user distortion. dataset of files USED-FOR information retrieval. distortion FEATURE-OF retrieval process. private information retrieval USED-FOR model. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. distortion CONJUNCTION user privacy leakage. user privacy leakage CONJUNCTION distortion. mutual information FEATURE-OF information - theoretical formulation. download rate EVALUATE-FOR schemes. generative adversarial models USED-FOR data - driven framework. constrained minimax game USED-FOR scheme. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. synthetic Gaussian dataset CONJUNCTION MNIST and CIFAR-10 datasets. MNIST and CIFAR-10 datasets CONJUNCTION synthetic Gaussian dataset. MNIST and CIFAR-10 datasets EVALUATE-FOR scheme. synthetic Gaussian dataset EVALUATE-FOR scheme. MNIST dataset EVALUATE-FOR data - driven approach. data - driven approach COMPARE achievable scheme. achievable scheme COMPARE data - driven approach. MNIST dataset EVALUATE-FOR achievable scheme. source coding USED-FOR achievable scheme. OtherScientificTerm are privacy level, perfect privacy requirement, and distortion constraint. Metric are rate - distortion - leakage tradeoff, and rate - distortion tradeoff curve. Material is CIFAR-10. ","This paper studies the trade-off between the rate-distortion-leakage tradeoff and the private information retrieval tradeoff between user privacy and user distortion. The authors propose a novel framework for the tradeoff, which is based on the information-theoretical formulation of mutual information. The paper shows that the rate distortion-leaky tradeoff can be reduced to zero by using a constrained minimax game. The proposed framework is evaluated on synthetic Gaussian and CIFAR-10 datasets.","This paper studies the trade-off between the rate-distortion-leakage tradeoff and the private information retrieval tradeoff between user privacy and user distortion. The authors propose a novel framework for the tradeoff, which is based on the information-theoretical formulation of mutual information. The paper shows that the rate distortion-leaky tradeoff can be reduced to zero by using a constrained minimax game. The proposed framework is evaluated on synthetic Gaussian and CIFAR-10 datasets."
693,SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"Graph Neural Networks ( GNNs ) USED-FOR graph - related applications. they USED-FOR large scale settings. fidelity FEATURE-OF model. fidelity EVALUATE-FOR sampling - based methods. decoupled greedy learning method USED-FOR GNNs ( DGL - GNN ). greedy auxiliary objectives USED-FOR module. method USED-FOR time or memory limited applications. efficiency EVALUATE-FOR method. sampling - based acceleration COMPARE model. model COMPARE sampling - based acceleration. efficiency CONJUNCTION accuracy. accuracy CONJUNCTION efficiency. decoupled approach COMPARE methods. methods COMPARE decoupled approach. sampling PART-OF GNN training. sampling HYPONYM-OF it. OtherScientificTerm are node embeddings, and GNN layers. Method are GNN, lazy - update scheme, and DGL - GNN model. Generic are modules, and approach. Task is parallel GNN training. ",This paper proposes a new greedy learning method for graph neural networks (GNNs). The main idea is to use greedy auxiliary objectives to improve the fidelity of GNNs. The paper also proposes a lazy update scheme to speed up GNN training. Experiments show that the proposed method outperforms sampling-based methods.,This paper proposes a new greedy learning method for graph neural networks (GNNs). The main idea is to use greedy auxiliary objectives to improve the fidelity of GNNs. The paper also proposes a lazy update scheme to speed up GNN training. Experiments show that the proposed method outperforms sampling-based methods.
702,SP:5ecb1b288f7fc02aead4493f81640867bc349290,Neural link predictors USED-FOR missing edges. missing edges PART-OF large scale Knowledge Graphs. logical conjunctions ( ∧ ) CONJUNCTION disjunctions. disjunctions CONJUNCTION logical conjunctions ( ∧ ). disjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION disjunctions. framework USED-FOR complex queries. incomplete Knowledge Graphs USED-FOR complex queries. solutions USED-FOR optimisation problem. gradient - based and combinatorial search HYPONYM-OF optimisation problem. gradient - based and combinatorial search HYPONYM-OF solutions. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. state - of - the - art methods COMPARE neural models. neural models COMPARE state - of - the - art methods. approach COMPARE neural models. neural models COMPARE approach. Hits@3 EVALUATE-FOR knowledge graphs. factual information FEATURE-OF knowledge graphs. intermediate solutions USED-FOR complex query atoms. intermediate solutions USED-FOR model. Generic is models. OtherScientificTerm is end - to - end differentiable objective. Method is neural link predictor. ,This paper proposes a neural link predictor for missing edges in Knowledge Graphs. The proposed method is based on the idea of end-to-end differentiable objective. The authors show that the proposed method outperforms state-of-the-art neural link predictors for incomplete knowledge graphs.,This paper proposes a neural link predictor for missing edges in Knowledge Graphs. The proposed method is based on the idea of end-to-end differentiable objective. The authors show that the proposed method outperforms state-of-the-art neural link predictors for incomplete knowledge graphs.
711,SP:f04a522fd04c503754fdb8c52da68646d31271a4,"procedure USED-FOR local robustness. procedure USED-FOR feed - forward neural networks. local robustness FEATURE-OF feed - forward neural networks. piecewise - linear activation functions FEATURE-OF feed - forward neural networks. decision boundaries USED-FOR assessing robustness. highly - parallel GPU implementation USED-FOR ` 2 norm. approach COMPARE approximate verification approaches. approximate verification approaches COMPARE approach. approximate verification approaches COMPARE verifiers. verifiers COMPARE approximate verification approaches. approach COMPARE verifiers. verifiers COMPARE approach. Task is Local robustness. Generic are model, networks, network, and algorithm. OtherScientificTerm are ` p - ball consistently, adversarial inputs, convex polyhedral regions, and geometric projections. Metric is robustness. ",This paper proposes a new method for verifying the robustness of a neural network to adversarial inputs. The key idea is to use the decision boundary of the neural network as a measure of robustness. The authors show that the proposed method can be applied to any neural network with piecewise-linear activation functions. They also show that their method is computationally efficient. ,This paper proposes a new method for verifying the robustness of a neural network to adversarial inputs. The key idea is to use the decision boundary of the neural network as a measure of robustness. The authors show that the proposed method can be applied to any neural network with piecewise-linear activation functions. They also show that their method is computationally efficient. 
720,SP:5297651ff873f97c07b9c47ed3eff52251661844,"approach USED-FOR embedding of objects. affordance space FEATURE-OF embedding of objects. embedding COMPARE approaches. approaches COMPARE embedding. dimensions USED-FOR mental representation of objects. human judgements of object similarity USED-FOR mental representation of objects. Generic are knowledge, and they. OtherScientificTerm are object “ affordance ”, and human judgments of affordance. Material is text corpora. ","This paper studies the problem of embedding of objects in text corpora. The authors propose a new embedding method based on human judgements of object affordance, which they call affordance space embedding. They show that the embedding space of an object can be represented as a set of dimensions, and they show that this space can be used to learn a representation of the object. They also show that their approach can be applied to embeddings of images. ","This paper studies the problem of embedding of objects in text corpora. The authors propose a new embedding method based on human judgements of object affordance, which they call affordance space embedding. They show that the embedding space of an object can be represented as a set of dimensions, and they show that this space can be used to learn a representation of the object. They also show that their approach can be applied to embeddings of images. "
729,SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"Individuality USED-FOR human society. efficiency CONJUNCTION productivity. productivity CONJUNCTION efficiency. It USED-FOR division of labor. efficiency EVALUATE-FOR It. productivity EVALUATE-FOR It. it USED-FOR multi - agent cooperation. method USED-FOR emergence of individuality ( EOI ). emergence of individuality ( EOI ) PART-OF multi - agent reinforcement learning ( MARL ). probabilistic classifier USED-FOR probability distribution. EOI USED-FOR probabilistic classifier. regularizers USED-FOR classifier. intrinsic reward USED-FOR emergence of individuality. regularizers USED-FOR emergence of individuality. MARL algorithms USED-FOR EOI. EOI COMPARE methods. methods COMPARE EOI. multi - agent cooperative scenarios EVALUATE-FOR methods. multi - agent cooperative scenarios EVALUATE-FOR EOI. OtherScientificTerm are individuality, and intrinsic reward signals. ","This paper proposes a method for multi-agent reinforcement learning (MARL) based on the emergence of individuality (EOI). The authors propose a probabilistic classifier to learn a probability distribution for each agent, which is then used to learn the intrinsic reward signals. The authors show that the proposed method outperforms existing MARL methods on a variety of tasks.","This paper proposes a method for multi-agent reinforcement learning (MARL) based on the emergence of individuality (EOI). The authors propose a probabilistic classifier to learn a probability distribution for each agent, which is then used to learn the intrinsic reward signals. The authors show that the proposed method outperforms existing MARL methods on a variety of tasks."
738,SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"certified robustness EVALUATE-FOR Randomized smoothing. base classifier USED-FOR randomized smoothing. Smoothed WEighted ENsembling ( SWEEN ) scheme USED-FOR randomized smoothed classifiers. SWEEN USED-FOR optimal certified robustness. adaptive prediction algorithm USED-FOR SWEEN models. adaptive prediction algorithm USED-FOR prediction and certification cost. SWEEN models COMPARE candidate models. candidate models COMPARE SWEEN models. training time EVALUATE-FOR SWEEN models. small models USED-FOR SWEEN models. OtherScientificTerm are l2 - norm adversarial attacks, and ensembling generality. Method is SWEEN model. ","This paper proposes a randomized smoothed ensembling (SWEEN) scheme to improve the certified robustness of classifiers. The proposed method is based on the idea of randomized smoothing, where the classifier is trained to be robust to adversarial attacks. The authors show that the proposed SWEEN method is able to achieve the best certified performance in terms of the number of samples required to certify a classifier. They also show that their method can be applied to small models.","This paper proposes a randomized smoothed ensembling (SWEEN) scheme to improve the certified robustness of classifiers. The proposed method is based on the idea of randomized smoothing, where the classifier is trained to be robust to adversarial attacks. The authors show that the proposed SWEEN method is able to achieve the best certified performance in terms of the number of samples required to certify a classifier. They also show that their method can be applied to small models."
747,SP:ea892e3d199ed6121279b20061a87f43afae8796,hierarchical structures USED-FOR learning process. hierarchical structures USED-FOR generalization. Ordered Memory Policy Network ( OMPN ) USED-FOR subtask hierarchy. subtask hierarchy USED-FOR task decomposition. subtask boundaries PART-OF unstructured demonstration. Craft CONJUNCTION Dial. Dial CONJUNCTION Craft. model COMPARE baselines. baselines COMPARE model. Craft EVALUATE-FOR model. Dial EVALUATE-FOR model. task decomposition EVALUATE-FOR model. unsupervised and weakly supervised settings EVALUATE-FOR model. OMPN USED-FOR partially observable environments. task decomposition EVALUATE-FOR OMPN. subtask hierarchy PART-OF model. Task is complex real - world tasks. OtherScientificTerm is inductive bias. ,"This paper proposes a method for learning a hierarchy of subtask boundaries in an unstructured demonstration setting. The proposed method is based on the idea of Ordered Memory Policy Network (OMPN), which learns a memory policy network for each subtask. The model is evaluated on a number of tasks, and is shown to be able to generalize well to partially observable environments. Experiments show that the proposed method outperforms the baselines in both supervised and weakly supervised settings.","This paper proposes a method for learning a hierarchy of subtask boundaries in an unstructured demonstration setting. The proposed method is based on the idea of Ordered Memory Policy Network (OMPN), which learns a memory policy network for each subtask. The model is evaluated on a number of tasks, and is shown to be able to generalize well to partially observable environments. Experiments show that the proposed method outperforms the baselines in both supervised and weakly supervised settings."
756,SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,semantic factor CONJUNCTION variation factor. variation factor CONJUNCTION semantic factor. deep ones HYPONYM-OF supervised learning methods. methods USED-FOR OOD prediction. causal reasoning USED-FOR Causal Semantic Generative model ( CSG ). learning CONJUNCTION prediction. prediction CONJUNCTION learning. variational Bayes USED-FOR learning. variational Bayes USED-FOR prediction. causal invariance principle USED-FOR methods. CSG USED-FOR semantic factor. semantic - identification USED-FOR adaptation. OOD EVALUATE-FOR baselines. OtherScientificTerm is domain - specific correlation. Metric is OOD generalization error. ,This paper studies the problem of OOD generalization in supervised learning. The authors propose a causal reasoning model for OOD prediction based on the causal invariance principle. The proposed model is based on a Causal Semantic Generative model (CSG). The authors show that the proposed model outperforms existing methods in terms of generalization error. ,This paper studies the problem of OOD generalization in supervised learning. The authors propose a causal reasoning model for OOD prediction based on the causal invariance principle. The proposed model is based on a Causal Semantic Generative model (CSG). The authors show that the proposed model outperforms existing methods in terms of generalization error. 
765,SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"adversarially corrupted rewards FEATURE-OF online learning algorithms. small regret FEATURE-OF algorithms. algorithm USED-FOR corrupted rewards. uncorrupted reward distribution FEATURE-OF regret. robust estimation USED-FOR unsupervised learning problems. stochastic multi - armed bandits CONJUNCTION linear contextual bandits. linear contextual bandits CONJUNCTION stochastic multi - armed bandits. linear contextual bandits CONJUNCTION Markov Decision Processes ( MDPs ). Markov Decision Processes ( MDPs ) CONJUNCTION linear contextual bandits. robust estimation USED-FOR robust online algorithms. robust online algorithms USED-FOR scenarios. stochastic rewards and transitions FEATURE-OF Markov Decision Processes ( MDPs ). near optimal regret FEATURE-OF robust online algorithms. Markov Decision Processes ( MDPs ) HYPONYM-OF scenarios. stochastic multi - armed bandits HYPONYM-OF scenarios. linear contextual bandits HYPONYM-OF scenarios. synthetic and real datasets EVALUATE-FOR algorithms. Method are online algorithm, and online learning. OtherScientificTerm are stochastic reward, and noise rate. ","This paper studies the problem of online learning with adversarially corrupted rewards in the context of stochastic multi-armed bandits, linear contextual bandits, and Markov Decision Processes (MDPs). The authors propose a robust online algorithm that is robust to corrupted rewards. The authors show that the proposed algorithm achieves near optimal regret for the corrupted rewards under the assumption that the noise rate is low and the reward distribution is uncorrupted. They also provide a theoretical analysis of the robustness of the algorithm. ","This paper studies the problem of online learning with adversarially corrupted rewards in the context of stochastic multi-armed bandits, linear contextual bandits, and Markov Decision Processes (MDPs). The authors propose a robust online algorithm that is robust to corrupted rewards. The authors show that the proposed algorithm achieves near optimal regret for the corrupted rewards under the assumption that the noise rate is low and the reward distribution is uncorrupted. They also provide a theoretical analysis of the robustness of the algorithm. "
774,SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"Encoder - decoder architecture USED-FOR neural machine translation ( NMT ). rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. rewriter PART-OF It. evaluator PART-OF It. evaluator USED-FOR translation quality. rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. prioritized gradient descent ( PGD ) method USED-FOR rewriter. PGD method USED-FOR Rewriter - Evaluator. framework USED-FOR NMT models. Transformer HYPONYM-OF NMT models. framework COMPARE baselines. baselines COMPARE framework. translation tasks EVALUATE-FOR framework. NMT models COMPARE baselines. baselines COMPARE NMT models. framework USED-FOR NMT models. Chinese - English and English - German HYPONYM-OF translation tasks. Generic is it. OtherScientificTerm is termination policy. Method are RewriterEvaluator, decoding, and encoder - decoder models. Task is rewriting process. ","This paper proposes Rewriter-Evaluator, a framework for neural machine translation (NMT) models that combines the rewriter and the evaluator to improve the quality of the final translation. The rewriter is trained using prioritized gradient descent (PGD) and the evaluation is performed using a termination policy. The authors show that the proposed framework outperforms baselines on Chinese-English and English-German translation tasks.","This paper proposes Rewriter-Evaluator, a framework for neural machine translation (NMT) models that combines the rewriter and the evaluator to improve the quality of the final translation. The rewriter is trained using prioritized gradient descent (PGD) and the evaluation is performed using a termination policy. The authors show that the proposed framework outperforms baselines on Chinese-English and English-German translation tasks."
783,SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"images CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION images. Ambiguities CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION Ambiguities. Ambiguities CONJUNCTION images. images CONJUNCTION Ambiguities. empirical frequency FEATURE-OF sampled predictions. two - stage, cascaded strategy USED-FOR calibrated adversarial refinement. adversarial network USED-FOR coherent predictions. black - box segmentation framework USED-FOR learning of calibrated stochastic mappings. model USED-FOR learning of calibrated stochastic mappings. model PART-OF black - box segmentation framework. multigrader LIDC dataset CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION multigrader LIDC dataset. multigrader LIDC dataset EVALUATE-FOR approach. Cityscapes dataset EVALUATE-FOR approach. core design USED-FOR tasks. framework USED-FOR semantic segmentation. core design USED-FOR calibrated predictive distribution. toy regression dataset EVALUATE-FOR framework. calibrated predictive distribution USED-FOR tasks. OtherScientificTerm are distribution over predictions, empirical distribution, multimodal predictive distribution, categorical likelihood, and calibrated stochastic mappings. Method is probabilistic networks. Generic is these. ","This paper proposes a black-box segmentation framework for learning of calibrated stochastic mappings. The proposed method is based on a two-stage, cascaded strategy for learning the calibrated adversarial refinement of the empirical frequency of sampled predictions. The method is evaluated on a toy regression dataset and a multigrader LIDC dataset. ","This paper proposes a black-box segmentation framework for learning of calibrated stochastic mappings. The proposed method is based on a two-stage, cascaded strategy for learning the calibrated adversarial refinement of the empirical frequency of sampled predictions. The method is evaluated on a toy regression dataset and a multigrader LIDC dataset. "
792,SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"distributed compute systems USED-FOR stochastic optimization algorithms. stochastic optimization algorithms USED-FOR large - scale machine learning applications. distributed compute systems USED-FOR large - scale machine learning applications. error feedback ( EF ) USED-FOR compressed communication. Top - K or PowerSGD HYPONYM-OF contractive compressors. EF USED-FOR contractive compressors. alternative USED-FOR contractive compressors. alternative USED-FOR EF. construction USED-FOR contractive compressor. construction USED-FOR induced unbiased compressor. contractive compressor CONJUNCTION induced unbiased compressor. induced unbiased compressor CONJUNCTION contractive compressor. approach COMPARE EF. EF COMPARE approach. reduced memory requirements CONJUNCTION communication complexity guarantees. communication complexity guarantees CONJUNCTION reduced memory requirements. communication complexity guarantees CONJUNCTION assumptions. assumptions CONJUNCTION communication complexity guarantees. partial participation FEATURE-OF federated learning. Generic are systems, and transformation. OtherScientificTerm are communication overhead, stochastic gradients, and unbiased compressors. ","This paper proposes a new method for error feedback (EF) in federated learning. EF is an extension of error feedback, which is used to improve the communication efficiency of stochastic gradient descent (SGD) algorithms. The authors show that EF can be used to reduce the communication complexity of SGD algorithms. They also provide a theoretical analysis of EF and show that it can be applied to contractive compressors and induced unbiased compressors. ","This paper proposes a new method for error feedback (EF) in federated learning. EF is an extension of error feedback, which is used to improve the communication efficiency of stochastic gradient descent (SGD) algorithms. The authors show that EF can be used to reduce the communication complexity of SGD algorithms. They also provide a theoretical analysis of EF and show that it can be applied to contractive compressors and induced unbiased compressors. "
801,SP:4fd702490293e481c79614852ba27dd3ce9215a4,"hyperparameter optimization ( HPO ) USED-FOR HPO. HT - AA baseline algorithms CONJUNCTION benchmarks. benchmarks CONJUNCTION HT - AA baseline algorithms. baseline COMPARE HPO algorithm. HPO algorithm COMPARE baseline. HPO PART-OF ML development. python packages USED-FOR baselines. python packages USED-FOR benchmarks. baselines CONJUNCTION benchmarks. benchmarks CONJUNCTION baselines. python packages USED-FOR HT - AA. Method are machine learning ( ML ) algorithm, ML algorithms, and neural architectures. OtherScientificTerm are hyperparameter settings, hyperparameter search space, and hyperparameter search spaces. Generic are approaches, and research framework. ","This paper proposes a new benchmark for hyperparameter optimization (HPO) for machine learning. The benchmark is based on the idea that the search space of hyperparameters can be decomposed into two subspaces, one for the hyper-parameter search space, and the other for the optimization space. The authors show that the proposed benchmark outperforms existing benchmarks in terms of accuracy and efficiency. The paper also provides a theoretical analysis of the performance of the proposed benchmarks.","This paper proposes a new benchmark for hyperparameter optimization (HPO) for machine learning. The benchmark is based on the idea that the search space of hyperparameters can be decomposed into two subspaces, one for the hyper-parameter search space, and the other for the optimization space. The authors show that the proposed benchmark outperforms existing benchmarks in terms of accuracy and efficiency. The paper also provides a theoretical analysis of the performance of the proposed benchmarks."
810,SP:e8f99bae5853de525450fcb8facd23cf973fc161,"audio labels COMPARE categorical probabilities. categorical probabilities COMPARE audio labels. image classifier USED-FOR classification. image classifier USED-FOR audio labels. audio labels COMPARE numerical probabilities. numerical probabilities COMPARE audio labels. numerical probabilities CONJUNCTION text. text CONJUNCTION numerical probabilities. audio labels COMPARE text. text COMPARE audio labels. they USED-FOR error signal. spectrograms CONJUNCTION shuffled spectrograms. shuffled spectrograms CONJUNCTION spectrograms. shuffled spectrograms CONJUNCTION Gaussian mixtures. Gaussian mixtures CONJUNCTION shuffled spectrograms. constant matrices CONJUNCTION spectrograms. spectrograms CONJUNCTION constant matrices. Gaussian mixtures CONJUNCTION uniform random matrices. uniform random matrices CONJUNCTION Gaussian mixtures. dimensionalities FEATURE-OF uniform random matrices. uniform random matrices HYPONYM-OF label representations. constant matrices HYPONYM-OF label representations. Gaussian mixtures HYPONYM-OF label representations. shuffled spectrograms HYPONYM-OF label representations. spectrograms HYPONYM-OF label representations. high dimensional, high entropy labels COMPARE text ( categorical ) labels. text ( categorical ) labels COMPARE high dimensional, high entropy labels. robustness EVALUATE-FOR features. image classification task EVALUATE-FOR high dimensional, high entropy labels. image classification task EVALUATE-FOR text ( categorical ) labels. accuracy EVALUATE-FOR high dimensional, high entropy labels. label representations USED-FOR features. OtherScientificTerm are data labels, and adversarial attacks. Generic is models. Method are high dimensional, high entropy label representations, and label representation. ","This paper studies the robustness of high-dimensional, high-entropy label representations against adversarial attacks. The authors show that high-dimension label representations are more robust to adversarial examples than low-dimension labels. They also show that the high-density label representation is more robust than the low-dimensional label representation. ","This paper studies the robustness of high-dimensional, high-entropy label representations against adversarial attacks. The authors show that high-dimension label representations are more robust to adversarial examples than low-dimension labels. They also show that the high-density label representation is more robust than the low-dimensional label representation. "
819,SP:4e8d924cba7367af0999b30d79250b4dc40413e1,approaches USED-FOR ensemble neural networks. forward passes USED-FOR prediction. forward passes USED-FOR methods. single model ’s capacity USED-FOR subnetworks. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. accuracy CONJUNCTION calibration error. calibration error CONJUNCTION accuracy. negative log - likelihood CONJUNCTION accuracy. accuracy CONJUNCTION negative log - likelihood. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. out - of - distribution variants COMPARE methods. methods COMPARE out - of - distribution variants. ImageNet CONJUNCTION out - of - distribution variants. out - of - distribution variants CONJUNCTION ImageNet. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. Generic is network. OtherScientificTerm is forward pass. Metric is model robustness. ,"This paper studies the robustness of ensemble neural networks (ENs) to forward passes. The authors consider the case where the number of forward passes is limited to a single model, and propose two methods to improve the model robustness. The main contribution of the paper is to show that the proposed methods are robust to out-of-distribution (OOD) perturbations in the forward pass. They also show that their methods are more robust to negative log-likelihood and calibration errors. ","This paper studies the robustness of ensemble neural networks (ENs) to forward passes. The authors consider the case where the number of forward passes is limited to a single model, and propose two methods to improve the model robustness. The main contribution of the paper is to show that the proposed methods are robust to out-of-distribution (OOD) perturbations in the forward pass. They also show that their methods are more robust to negative log-likelihood and calibration errors. "
828,SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"teacher network CONJUNCTION student network. student network CONJUNCTION teacher network. method USED-FOR intermediate knowledge. Sparse Representation Matching ( SRM ) USED-FOR intermediate knowledge. Convolutional Neural Network ( CNN ) USED-FOR intermediate knowledge. sparse representation learning USED-FOR Sparse Representation Matching ( SRM ). sparse representation learning USED-FOR method. pixellevel and image - level labels USED-FOR intermediate feature maps. intermediate feature maps PART-OF student network. sparse representations of the hidden features PART-OF teacher CNN. sparse representations of the hidden features USED-FOR SRM. neural processing block USED-FOR SRM. stochastic gradient descent USED-FOR neural processing block. stochastic gradient descent USED-FOR SRM. SRM COMPARE KD techniques. KD techniques COMPARE SRM. Task is Knowledge Distillation. Method are CNN, and teacher and student networks. ",This paper proposes a new method for knowledge distillation. The proposed method is based on sparse representation learning (SRM). SRM is a method for learning intermediate knowledge between teacher and student networks. The authors show that SRM can be applied to both pixel-level and image-level labels. They also show that the proposed method can be used to improve the performance of KD techniques.,This paper proposes a new method for knowledge distillation. The proposed method is based on sparse representation learning (SRM). SRM is a method for learning intermediate knowledge between teacher and student networks. The authors show that SRM can be applied to both pixel-level and image-level labels. They also show that the proposed method can be used to improve the performance of KD techniques.
837,SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,Reinforcement learning methods USED-FOR policies. sequential structure PART-OF representation learning process. sequential structure PART-OF reinforcement learning. approach COMPARE approaches. approaches COMPARE approach. theoretically motivated policy similarity metric ( PSM ) USED-FOR behavioral similarity. contrastive representation learning procedure USED-FOR state similarity metric. contrastive representation learning procedure USED-FOR policy similarity embeddings ( PSEs1 ). PSM USED-FOR policy similarity embeddings ( PSEs1 ). LQR CONJUNCTION jumping task. jumping task CONJUNCTION LQR. jumping task CONJUNCTION Distracting DM Control Suite. Distracting DM Control Suite CONJUNCTION jumping task. generalization EVALUATE-FOR benchmarks. spurious correlations FEATURE-OF LQR. benchmarks EVALUATE-FOR PSEs. generalization EVALUATE-FOR PSEs. Distracting DM Control Suite HYPONYM-OF benchmarks. jumping task HYPONYM-OF benchmarks. LQR HYPONYM-OF benchmarks. Generic is structure. OtherScientificTerm is optimal policies. ,"This paper proposes a contrastive representation learning method to learn policy similarity embeddings (PSEs1) for reinforcement learning. The key idea is to learn a policy similarity metric (PSM) that is motivated by the behavioral similarity between two policies. The proposed method is evaluated on the Distracting DM Control Suite, LQR, and Jumping task. The results show that the proposed method outperforms existing methods.","This paper proposes a contrastive representation learning method to learn policy similarity embeddings (PSEs1) for reinforcement learning. The key idea is to learn a policy similarity metric (PSM) that is motivated by the behavioral similarity between two policies. The proposed method is evaluated on the Distracting DM Control Suite, LQR, and Jumping task. The results show that the proposed method outperforms existing methods."
846,SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"approach USED-FOR disentanglement. latent representation FEATURE-OF model. approach USED-FOR disentanglement. rotations CONJUNCTION translations. translations CONJUNCTION rotations. topological defects FEATURE-OF transformations. images FEATURE-OF transformations. affine transformations HYPONYM-OF transformations. translations HYPONYM-OF affine transformations. rotations HYPONYM-OF affine transformations. approach USED-FOR disentanglement. distributed equivariant operators USED-FOR approach. approach USED-FOR disentangle affine transformations. distributed operators USED-FOR disentanglement. distributed operators USED-FOR models. Task is Machine Learning. OtherScientificTerm are object shape, encoder, and latent space. Generic is factors. Method is group representation theory. ","This paper proposes a distributed equivariant operator for disentangling affine transformations. The key idea is to use distributed operators to disentangle the latent representation of the encoder and the latent space of the decoder. The proposed method is based on the group representation theory. The authors show that the disentangled latent space can be represented as a set of groups, which can be decomposed into groups of different sizes. They also show that this can be used to learn disentanglement operators.","This paper proposes a distributed equivariant operator for disentangling affine transformations. The key idea is to use distributed operators to disentangle the latent representation of the encoder and the latent space of the decoder. The proposed method is based on the group representation theory. The authors show that the disentangled latent space can be represented as a set of groups, which can be decomposed into groups of different sizes. They also show that this can be used to learn disentanglement operators."
855,SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,statistical framework USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR statistical framework. Hawkes process USED-FOR inhibitory interactions. nonlinear Hawkes process USED-FOR influence pattern. excitatory or inhibitory interactions FEATURE-OF influence pattern. latent marked Poisson processes CONJUNCTION sparsity variables. sparsity variables CONJUNCTION latent marked Poisson processes. Pólya - Gamma variables CONJUNCTION latent marked Poisson processes. latent marked Poisson processes CONJUNCTION Pólya - Gamma variables. auxiliary latent variables USED-FOR functional connection weights. Gaussian form FEATURE-OF functional connection weights. analytical updates USED-FOR iterative algorithm. sparsity variables HYPONYM-OF auxiliary latent variables. latent marked Poisson processes HYPONYM-OF auxiliary latent variables. Pólya - Gamma variables HYPONYM-OF auxiliary latent variables. expectationmaximization ( EM ) algorithm USED-FOR maximum a posteriori ( MAP ) estimate. accuracy EVALUATE-FOR algorithm. synthetic and real data EVALUATE-FOR algorithm. algorithm USED-FOR temporal dynamics of interaction. algorithm USED-FOR interpretable functional connectivity. interpretable functional connectivity FEATURE-OF neural spike trains. real neural recordings EVALUATE-FOR algorithm. ,"This paper proposes a new statistical framework for measuring the influence pattern of neuronal spiking activities. The proposed framework is based on the Hawkes process, which is a nonlinear process that is used to estimate the influence patterns of excitatory or inhibitory interactions. The authors propose an iterative algorithm for estimating the maximum a posteriori (MAP) estimate of the functional connection weights, which can be used as a measure for interpretable functional connectivity of neural spike trains. The method is evaluated on synthetic and real-world data.","This paper proposes a new statistical framework for measuring the influence pattern of neuronal spiking activities. The proposed framework is based on the Hawkes process, which is a nonlinear process that is used to estimate the influence patterns of excitatory or inhibitory interactions. The authors propose an iterative algorithm for estimating the maximum a posteriori (MAP) estimate of the functional connection weights, which can be used as a measure for interpretable functional connectivity of neural spike trains. The method is evaluated on synthetic and real-world data."
864,SP:1156d3deac022829bda930ffcb081947609d972b,"gradient descent ( GD ) algorithm USED-FOR two - layer neural network models. under - parameterized regime FEATURE-OF GD dynamics. quenched ” neurons USED-FOR continued activation and deactivation process. quenching - activation process USED-FOR GD. random featurelike behavior FEATURE-OF it. quenching process USED-FOR implicit regularization ”. mean - field ” scaling FEATURE-OF GD dynamics. OtherScientificTerm are parameter regimes, neural network - like behavior, and inner - layer parameters. Method is random feature model. Generic is dynamics. ","This paper studies the dynamics of gradient descent (GD) in the under-parameterized regime of two-layer neural network models. The authors show that under the parameterized regime, the inner-layer parameters of the model exhibit random featurelike behavior, which is similar to the behavior of random feature models in the random feature regime. They also show that the quenching-activation process in the inner layer of GD can be seen as an implicit regularization mechanism. They show that this regularization can be used to reduce the mean-field scaling of GD dynamics. ","This paper studies the dynamics of gradient descent (GD) in the under-parameterized regime of two-layer neural network models. The authors show that under the parameterized regime, the inner-layer parameters of the model exhibit random featurelike behavior, which is similar to the behavior of random feature models in the random feature regime. They also show that the quenching-activation process in the inner layer of GD can be seen as an implicit regularization mechanism. They show that this regularization can be used to reduce the mean-field scaling of GD dynamics. "
873,SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"constrained Markov decision process ( CMDP ) problems USED-FOR reinforcement learning problems. model USED-FOR CMDP problem. reconnaissance MDP ( R - MDP ) CONJUNCTION planning MDP ( P - MDP ). planning MDP ( P - MDP ) CONJUNCTION reconnaissance MDP ( R - MDP ). MDPs PART-OF CMDP. planning MDP ( P - MDP ) HYPONYM-OF MDPs. reconnaissance MDP ( R - MDP ) HYPONYM-OF MDPs. threat function CONJUNCTION Q - function analogue of danger. Q - function analogue of danger CONJUNCTION threat function. threat function USED-FOR R - MDP. reward - seeking policy USED-FOR P - MDP. fixed threat function USED-FOR reward - seeking policy. generative model USED-FOR threat function. reward CONJUNCTION danger - constraint. danger - constraint CONJUNCTION reward. threat function USED-FOR baseline policy. reward FEATURE-OF CMDP problems. danger - constraint FEATURE-OF CMDP problems. approximation method USED-FOR R - MDP. approximation method USED-FOR threat function. method COMPARE approaches. approaches COMPARE method. benchmark dataset CONJUNCTION complex collision - free navigation tasks. complex collision - free navigation tasks CONJUNCTION benchmark dataset. complex collision - free navigation tasks EVALUATE-FOR method. complex collision - free navigation tasks EVALUATE-FOR approaches. benchmark dataset EVALUATE-FOR method. benchmark dataset EVALUATE-FOR approaches. OtherScientificTerm are prescribed safety constraints, and state - action pair. ",This paper proposes a novel method for solving constrained Markov decision process (CMDP) problems. The authors propose to use a generative model to learn the threat function and reward function for the MDP problem. The proposed method is evaluated on a number of benchmark tasks and shows promising results. ,This paper proposes a novel method for solving constrained Markov decision process (CMDP) problems. The authors propose to use a generative model to learn the threat function and reward function for the MDP problem. The proposed method is evaluated on a number of benchmark tasks and shows promising results. 
882,SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,neural architectures USED-FOR classification tasks. crossentropy loss COMPARE square loss. square loss COMPARE crossentropy loss. crossentropy loss USED-FOR neural architectures. benchmark datasets USED-FOR NLP. automatic speech recognition ( ASR ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION automatic speech recognition ( ASR ). neural architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION neural architectures. neural architectures USED-FOR NLP. benchmark datasets USED-FOR automatic speech recognition ( ASR ). NLP CONJUNCTION automatic speech recognition ( ASR ). automatic speech recognition ( ASR ) CONJUNCTION NLP. hyper - parameter settings USED-FOR architectures. square loss USED-FOR architectures. square loss USED-FOR NLP. Cross - entropy USED-FOR computer vision tasks. square loss USED-FOR classification. cross - entropy FEATURE-OF deep learning. equal footing FEATURE-OF deep learning. OtherScientificTerm is cross - entropy loss. Task is non - vision tasks. ,This paper studies the cross-entropy loss of neural architectures for classification tasks. The authors show that the square loss is equivalent to the cross entropy loss for neural architectures. They also show that square loss can be used for NLP and ASR tasks. ,This paper studies the cross-entropy loss of neural architectures for classification tasks. The authors show that the square loss is equivalent to the cross entropy loss for neural architectures. They also show that square loss can be used for NLP and ASR tasks. 
891,SP:915f1f0fc4850507c28c1d609239b41775863ebe,"self - supervised objectives FEATURE-OF reward maximization. exponential moving average USED-FOR encoder. encoder USED-FOR target representations. prior methods USED-FOR sample - efficient deep RL. future prediction objective COMPARE prior methods. prior methods COMPARE future prediction objective. data augmentation USED-FOR future prediction loss. future prediction CONJUNCTION data augmentation. data augmentation CONJUNCTION future prediction. median human - normalized score EVALUATE-FOR self - supervised objective. Atari EVALUATE-FOR self - supervised objective. future prediction PART-OF self - supervised objective. data augmentation PART-OF self - supervised objective. SPR COMPARE expert human scores. expert human scores COMPARE SPR. limited data regime EVALUATE-FOR SPR. Method are deep reinforcement learning, Self - Predictive Representations ( SPR ), and transition model. OtherScientificTerm are limited interaction, latent state representations, agent ’s representations, and environment interaction. Generic are method, and state - of - the - art. ","This paper proposes a self-supervised reinforcement learning method for reward maximization. The method is based on self-predictive representation learning (SPR), which is an extension of previous work on the problem of sample-efficient deep RL. The key idea is to use an exponential moving average for the encoder and the transition model for the target representations. The authors show that the proposed method outperforms the state-of-the-art in the limited data regime. ","This paper proposes a self-supervised reinforcement learning method for reward maximization. The method is based on self-predictive representation learning (SPR), which is an extension of previous work on the problem of sample-efficient deep RL. The key idea is to use an exponential moving average for the encoder and the transition model for the target representations. The authors show that the proposed method outperforms the state-of-the-art in the limited data regime. "
900,SP:983f01c170909c8c67fd3be25f121bd61bdd8307,method USED-FOR generating single - node representations. InstantEmbedding USED-FOR generating single - node representations. InstantEmbedding HYPONYM-OF method. local PageRank computations USED-FOR InstantEmbedding. local PageRank computations USED-FOR method. approach USED-FOR globally consistent representations. DeepWalk CONJUNCTION node2vec. node2vec CONJUNCTION DeepWalk. node2vec CONJUNCTION VERSE. VERSE CONJUNCTION node2vec. VERSE CONJUNCTION FastRP. FastRP CONJUNCTION VERSE. InstantEmbedding COMPARE methods. methods COMPARE InstantEmbedding. InstantEmbedding USED-FOR single node ’s embedding. computation time CONJUNCTION memory. memory CONJUNCTION computation time. FastRP HYPONYM-OF methods. DeepWalk HYPONYM-OF methods. VERSE HYPONYM-OF methods. node2vec HYPONYM-OF methods. computation time EVALUATE-FOR InstantEmbedding. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. unsupervised representation learning USED-FOR tasks. unsupervised representation learning USED-FOR node classification. unsupervised representation learning USED-FOR link prediction. method USED-FOR representations. method COMPARE unsupervised representation learning. unsupervised representation learning COMPARE method. link prediction HYPONYM-OF tasks. node classification HYPONYM-OF tasks. social networks CONJUNCTION chemical molecules. chemical molecules CONJUNCTION social networks. chemical molecules CONJUNCTION knowledge graphs. knowledge graphs CONJUNCTION chemical molecules. approach USED-FOR graphs. node PART-OF graph. d - dimensional embedding vector USED-FOR node. compact representations of graphs USED-FOR approach. d - dimensional embedding vector USED-FOR graph. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. visualization CONJUNCTION node classification. node classification CONJUNCTION visualization. Unsupervised embeddings USED-FOR machine learning tasks. visualization HYPONYM-OF machine learning tasks. link prediction HYPONYM-OF machine learning tasks. node classification HYPONYM-OF machine,"This paper proposes a new method for generating single-node representations of graphs. The proposed method, InstantEmbedding, learns a d-dimensional embedding vector for each node in a graph, which is then used to compute local PageRank computations for the embedding of each node. The method is evaluated on a number of machine learning tasks, including node classification, link prediction, and node classification. ","This paper proposes a new method for generating single-node representations of graphs. The proposed method, InstantEmbedding, learns a d-dimensional embedding vector for each node in a graph, which is then used to compute local PageRank computations for the embedding of each node. The method is evaluated on a number of machine learning tasks, including node classification, link prediction, and node classification. "
909,SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"Graph coarsening USED-FOR graph. deep learning on graphs USED-FOR graph coarsening. Laplace operator CONJUNCTION projection / lift operators. projection / lift operators CONJUNCTION Laplace operator. Laplace operator USED-FOR coarse graph. framework USED-FOR coarsening algorithm. edge weight USED-FOR coarse graph. it USED-FOR coarsening quality. graph neural networks USED-FOR weight assignment map. reduction ratios CONJUNCTION graph sizes. graph sizes CONJUNCTION reduction ratios. graph sizes CONJUNCTION graph types. graph types CONJUNCTION graph sizes. method COMPARE graph coarsening methods. graph coarsening methods COMPARE method. metrics CONJUNCTION reduction ratios. reduction ratios CONJUNCTION metrics. synthetic and real networks EVALUATE-FOR method. metrics EVALUATE-FOR graph coarsening methods. metrics EVALUATE-FOR method. reduction ratios EVALUATE-FOR method. It USED-FOR graphs. OtherScientificTerm are large - scale graphs, and essential properties. Material is large graph data. Method is data - driven methods. ","This paper proposes a novel method for graph coarsening based on deep learning on graph neural networks. The proposed method is based on the Laplace operator and the projection/lift operators. The authors show that the proposed method can be applied to both synthetic and real-world graph data. The method is evaluated on several metrics, including reduction ratios, graph sizes, and graph types.","This paper proposes a novel method for graph coarsening based on deep learning on graph neural networks. The proposed method is based on the Laplace operator and the projection/lift operators. The authors show that the proposed method can be applied to both synthetic and real-world graph data. The method is evaluated on several metrics, including reduction ratios, graph sizes, and graph types."
918,SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,localization CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION localization. environmental acoustic effects CONJUNCTION localization. localization CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR 3D audio content creation. 3D audio content creation CONJUNCTION environmental acoustic effects. environmental acoustic effects CONJUNCTION 3D audio content creation. environmental acoustic effects CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR environmental acoustic effects. scattering characteristics FEATURE-OF Acoustic properties. numeric solvers USED-FOR acoustic properties. numeric solvers USED-FOR interactive applications. geometric deep learning algorithm USED-FOR characteristics. characteristics USED-FOR 3D objects. interactive rates FEATURE-OF 3D objects. discrete - laplacian and implicit encoders USED-FOR geometric deep learning algorithm. multi - layer network USED-FOR acoustic properties. multi - layer network USED-FOR arbitrary topologies. arbitrary topologies FEATURE-OF acoustic properties. NVIDIA GeForce RTX 2080 Ti GPU USED-FOR multi - layer network. accuracy EVALUATE-FOR learning method. dynamic environments FEATURE-OF generating environmental acoustic effects. Method is point cloud approximation. OtherScientificTerm is high - dimensional latent space. ,"This paper proposes a geometric deep learning method to learn acoustic properties for 3D audio content generation. The method is based on discrete-laplacian and implicit encoders, and is able to learn the acoustic properties of 3D objects in a high-dimensional latent space. The proposed method is evaluated on the NVIDIA GeForce RTX 2080 Ti GPU, and achieves state-of-the-art performance. ","This paper proposes a geometric deep learning method to learn acoustic properties for 3D audio content generation. The method is based on discrete-laplacian and implicit encoders, and is able to learn the acoustic properties of 3D objects in a high-dimensional latent space. The proposed method is evaluated on the NVIDIA GeForce RTX 2080 Ti GPU, and achieves state-of-the-art performance. "
927,SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"model USED-FOR extreme distributional shifts. extreme distributional shifts FEATURE-OF model ’s sensitivity. model ’s sensitivity EVALUATE-FOR model. robust optimization USED-FOR Risk Extrapolation ( REx ). perturbation set of extrapolated domains ( MMREx ) USED-FOR robust optimization. REx USED-FOR causal mechanisms. causally induced distributional shifts CONJUNCTION covariate shift. covariate shift CONJUNCTION causally induced distributional shifts. REx COMPARE methods. methods COMPARE REx. robustness EVALUATE-FOR REx. covariate shift FEATURE-OF robustness. causally induced distributional shifts FEATURE-OF robustness. Invariant Risk Minimization HYPONYM-OF methods. Task is Distributional shift. Method is machine learning prediction systems. OtherScientificTerm is causal and anti - causal elements. Generic are approach, and variant. ","This paper proposes Risk Extrapolation (REx), a method for risk minimization based on robust optimization. The main idea is to learn a perturbation set of extrapolated domains (MMREx) that can be used to estimate the risk of an extreme distributional shift. The authors show that the proposed method is robust to both causal and anti-causal factors. They also show that REx can be combined with Invariant Risk Minimization (IRM) to improve the robustness of the model.","This paper proposes Risk Extrapolation (REx), a method for risk minimization based on robust optimization. The main idea is to learn a perturbation set of extrapolated domains (MMREx) that can be used to estimate the risk of an extreme distributional shift. The authors show that the proposed method is robust to both causal and anti-causal factors. They also show that REx can be combined with Invariant Risk Minimization (IRM) to improve the robustness of the model."
936,SP:411d5bcf7698d534ad60f581d479ff74849ba4de,neural networks USED-FOR mappings between finite - dimensional Euclidean spaces. this USED-FOR neural operators. neural operators USED-FOR mappings between function spaces. neural operators USED-FOR mapping. neural operators USED-FOR partial differential equations ( PDEs ). they USED-FOR PDEs. Fourier space FEATURE-OF integral kernel. integral kernel USED-FOR neural operator. Burgers ’ equation CONJUNCTION Darcy flow. Darcy flow CONJUNCTION Burgers ’ equation. Darcy flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy flow. Fourier neural operator HYPONYM-OF ML - based method. Fourier neural operator USED-FOR turbulent flows. ML - based method USED-FOR turbulent flows. zero - shot super - resolution FEATURE-OF Fourier neural operator. zero - shot super - resolution FEATURE-OF turbulent flows. It COMPARE PDE solvers. PDE solvers COMPARE It. it COMPARE learning - based solvers. learning - based solvers COMPARE it. accuracy EVALUATE-FOR learning - based solvers. fixed resolution FEATURE-OF learning - based solvers. fixed resolution FEATURE-OF it. accuracy EVALUATE-FOR it. OtherScientificTerm is functional parametric dependence. Generic is architecture. ,"This paper proposes a neural operator for solving partial differential equations (PDEs) in the Fourier space. The proposed method is based on the idea of neural operators that can be used to solve PDEs. The authors show that the proposed method can solve the Navier-Stokes equation, Burgers equation, and Darcy flow in a zero-shot super-resolution setting. They also show that their method is able to solve turbulent flows in a fixed resolution setting.","This paper proposes a neural operator for solving partial differential equations (PDEs) in the Fourier space. The proposed method is based on the idea of neural operators that can be used to solve PDEs. The authors show that the proposed method can solve the Navier-Stokes equation, Burgers equation, and Darcy flow in a zero-shot super-resolution setting. They also show that their method is able to solve turbulent flows in a fixed resolution setting."
945,SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"gradient flow USED-FOR linear neural network training. infinitesimal step size FEATURE-OF gradient descent. gradient descent HYPONYM-OF gradient flow. convergence direction FEATURE-OF network parameters. formulation USED-FOR convergence direction. singular vectors PART-OF tensor. network USED-FOR tensor. singular vectors USED-FOR convergence direction. tensor USED-FOR convergence direction. gradient flow USED-FOR separable classification. gradient flow USED-FOR ` 2 / L max - margin problem. transformed ” input space FEATURE-OF ` 2 / L max - margin problem. network USED-FOR transformed ” input space. orthogonally decomposable FEATURE-OF L - layer linear tensor networks. gradient flow USED-FOR global minimum. transformed input space FEATURE-OF weighted ` 1 and ` 2 norms. norm - like function FEATURE-OF global minimum. transformed input space FEATURE-OF norm - like function. weighted ` 1 and ` 2 norms FEATURE-OF norm - like function. gradient flow USED-FOR underdetermined regression. Method are tensor formulation of neural networks, and linear tensor networks. OtherScientificTerm is convergence assumptions. ","This paper studies the convergence of linear tensor networks in the presence of gradient flow. The authors consider the case where the tensor is orthogonally decomposable and the input space is transformed into a transformed input space. They show that gradient flow converges to a global minimum in the transformed space, and show that the gradient flow is invariant to the number of layers. They also show that under certain assumptions, gradient flow can converge to the global minimum. ","This paper studies the convergence of linear tensor networks in the presence of gradient flow. The authors consider the case where the tensor is orthogonally decomposable and the input space is transformed into a transformed input space. They show that gradient flow converges to a global minimum in the transformed space, and show that the gradient flow is invariant to the number of layers. They also show that under certain assumptions, gradient flow can converge to the global minimum. "
954,SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"prediction error CONJUNCTION computational cost. computational cost CONJUNCTION prediction error. storage cost EVALUATE-FOR model. FLOPs HYPONYM-OF computational cost. They USED-FOR resource - constrained settings. mobile devices HYPONYM-OF resource - constrained settings. slimmable neural networks USED-FOR sub - networks. width - multiplier USED-FOR sub - networks. performance profiles FEATURE-OF sub - networks. prediction accuracy EVALUATE-FOR network. width - multiplier USED-FOR slimmable neural networks. approach USED-FOR slimmable networks. approach USED-FOR widthmultipliers. shared weights CONJUNCTION width - multipliers. width - multipliers CONJUNCTION shared weights. algorithm USED-FOR shared weights. algorithm USED-FOR width - multipliers. width - multipliers USED-FOR sub - networks. algorithm USED-FOR sub - networks. multiobjective optimization lens USED-FOR slimmable networks. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. method COMPARE alternatives. alternatives COMPARE method. network and dataset combinations CONJUNCTION cost objectives. cost objectives CONJUNCTION network and dataset combinations. FLOPs HYPONYM-OF cost objectives. memory footprint HYPONYM-OF cost objectives. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. ImageNet dataset EVALUATE-FOR MobileNetV2. channel counts USED-FOR layers. Method is Slimmable neural networks. OtherScientificTerm are FLOP requirements, and heterogeneous width - multipliers. Task is optimizing slimmable networks. Metric is top-1 accuracy. ","This paper proposes a multi-objective optimization lens for slimmable neural networks. The main idea is to optimize the width-multipliers of sub-networks in a multiobjective way. The proposed method is evaluated on MobileNetV2 and ImageNet and achieves state-of-the-art performance in terms of prediction accuracy, FLOPs, and memory footprint.","This paper proposes a multi-objective optimization lens for slimmable neural networks. The main idea is to optimize the width-multipliers of sub-networks in a multiobjective way. The proposed method is evaluated on MobileNetV2 and ImageNet and achieves state-of-the-art performance in terms of prediction accuracy, FLOPs, and memory footprint."
963,SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"Federated SemiSupervised Learning ( FSSL ) HYPONYM-OF federated learning problem. scenarios PART-OF FSSL. method USED-FOR problems. labeled and unlabeled data USED-FOR disjoint learning. inter - client consistency loss USED-FOR FedMatch. federated learning and semi - supervised learning approaches USED-FOR FedMatch. federated learning CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION federated learning. method COMPARE baselines. baselines COMPARE method. method COMPARE local semi - supervised learning. local semi - supervised learning COMPARE method. local semi - supervised learning CONJUNCTION baselines. baselines CONJUNCTION local semi - supervised learning. method COMPARE method. method COMPARE method. federated learning USED-FOR baselines. semi - supervised learning USED-FOR baselines. Method is federated learning approaches. Metric is labeling cost. Task are annotation, and Federated Matching ( FedMatch ). OtherScientificTerm is expert knowledge. Material are private data, and labeled data. Generic is scenario. ","This paper studies the problem of federated semi-supervised learning (FSSL) and federated matching (FedMatch) in the context of disjoint learning. The authors propose a new method for FedMatch, which is based on the idea of inter-client consistency loss (ICL). The authors show that the proposed method is able to achieve better performance than the baselines on both public and private data. The paper also provides a theoretical analysis of the performance of the proposed algorithm.","This paper studies the problem of federated semi-supervised learning (FSSL) and federated matching (FedMatch) in the context of disjoint learning. The authors propose a new method for FedMatch, which is based on the idea of inter-client consistency loss (ICL). The authors show that the proposed method is able to achieve better performance than the baselines on both public and private data. The paper also provides a theoretical analysis of the performance of the proposed algorithm."
972,SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,real - world users FEATURE-OF discrete event sequences. discrete event sequences USED-FOR self - supervised learning. low - dimensional fixed - length vector representations USED-FOR downstream machine learning tasks. low - dimensional fixed - length vector representations USED-FOR Self - supervised learning. contrastive learning USED-FOR audio and computer vision domains. CoLES USED-FOR discrete event sequences domain. self - supervised setting USED-FOR discrete event sequences domain. contrastive learning USED-FOR CoLES. augmentation method USED-FOR discrete event sequences. augmentation method USED-FOR CoLES. CoLES USED-FOR discrete event sequences. CoLES representations COMPARE methods. methods COMPARE CoLES representations. downstream tasks EVALUATE-FOR CoLES representations. downstream tasks EVALUATE-FOR methods. public datasets EVALUATE-FOR CoLES. ,"This paper proposes CoLES, a self-supervised learning method for discrete event sequences. CoLES uses contrastive learning to learn a low-dimensional fixed-length vector representation of the discrete event sequence, which can be used for downstream machine learning tasks such as audio and computer vision tasks. The proposed method is evaluated on a number of public datasets and compared to existing methods.","This paper proposes CoLES, a self-supervised learning method for discrete event sequences. CoLES uses contrastive learning to learn a low-dimensional fixed-length vector representation of the discrete event sequence, which can be used for downstream machine learning tasks such as audio and computer vision tasks. The proposed method is evaluated on a number of public datasets and compared to existing methods."
981,SP:385942a5bcee7384bb722a1669b541f2fac0cd36,constituency grammar USED-FOR assembly of one or several corresponded words. dependency grammar CONJUNCTION constituency grammar. constituency grammar CONJUNCTION dependency grammar. constituency grammar HYPONYM-OF natural language grammars. dependency grammar HYPONYM-OF natural language grammars. StructFormer USED-FOR dependency and constituency structure. model USED-FOR dependency and constituency structure. StructFormer HYPONYM-OF model. constituency tree CONJUNCTION dependency graph. dependency graph CONJUNCTION constituency tree. parsing framework USED-FOR constituency tree. parsing framework USED-FOR dependency graph. induced dependency relations PART-OF transformer. dependency - constrained self - attention mechanism USED-FOR transformer. unsupervised dependency parsing CONJUNCTION masked language modeling. masked language modeling CONJUNCTION unsupervised dependency parsing. unsupervised constituency parsing CONJUNCTION unsupervised dependency parsing. unsupervised dependency parsing CONJUNCTION unsupervised constituency parsing. model USED-FOR unsupervised constituency parsing. model USED-FOR unsupervised dependency parsing. model USED-FOR masked language modeling. Method is unsupervised parsing methods. ,"This paper proposes a transformer model for unsupervised dependency parsing and constituency parsing. The proposed model is based on a dependency-constrained self-attention mechanism, which is used to model the dependency and constituency structure. The paper also proposes a parsing framework to learn the dependency graph and the constituency tree. Experiments show that the proposed model outperforms the baselines on both masked language modeling (MAML) and masked dependency parsing (MDP). ","This paper proposes a transformer model for unsupervised dependency parsing and constituency parsing. The proposed model is based on a dependency-constrained self-attention mechanism, which is used to model the dependency and constituency structure. The paper also proposes a parsing framework to learn the dependency graph and the constituency tree. Experiments show that the proposed model outperforms the baselines on both masked language modeling (MAML) and masked dependency parsing (MDP). "
990,SP:078966ff62775bba6031e47d374bda95f4a7dde3,images USED-FOR structured representations. nodes of scene graphs CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION nodes of scene graphs. annotated mapping USED-FOR nodes of scene graphs. annotated mapping USED-FOR methods. scene graph nodes CONJUNCTION visual objects. visual objects CONJUNCTION scene graph nodes. object features CONJUNCTION relational features. relational features CONJUNCTION object features. visual objects CONJUNCTION scene graph nodes. scene graph nodes CONJUNCTION visual objects. Visual Genome ( VG ) CONJUNCTION Visual Relation Detection ( VRD ) datasets. Visual Relation Detection ( VRD ) datasets CONJUNCTION Visual Genome ( VG ). model COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE model. scene graph grounding task EVALUATE-FOR state - of - the - art approaches. scene graph grounding task EVALUATE-FOR model. scene graph parsing task EVALUATE-FOR method. scene graph parsing task EVALUATE-FOR model. model COMPARE method. method COMPARE model. OtherScientificTerm is weak supervision. ,"This paper proposes a method for scene graph representation learning based on an annotated mapping between nodes of scene graphs and object bounding boxes. The method is based on the observation that existing methods are not able to capture the relationship between object features and relational features in the scene graph. To address this issue, the authors propose a method that maps the object features to the relational features of the scene graphs. The proposed method is evaluated on three datasets: Visual Genome (VG), Visual Relation Detection (VRD) and Scene Graph Parsing (SGP). The method outperforms the state-of-the-art methods on all three datasets.","This paper proposes a method for scene graph representation learning based on an annotated mapping between nodes of scene graphs and object bounding boxes. The method is based on the observation that existing methods are not able to capture the relationship between object features and relational features in the scene graph. To address this issue, the authors propose a method that maps the object features to the relational features of the scene graphs. The proposed method is evaluated on three datasets: Visual Genome (VG), Visual Relation Detection (VRD) and Scene Graph Parsing (SGP). The method outperforms the state-of-the-art methods on all three datasets."
999,SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"Relational regularized autoencoder ( RAE ) USED-FOR distribution of data. framework USED-FOR distribution of data. reconstruction loss CONJUNCTION relational regularization. relational regularization CONJUNCTION reconstruction loss. Relational regularized autoencoder ( RAE ) HYPONYM-OF framework. relational regularization FEATURE-OF latent space. sliced fused GromovWasserstein ( SFG ) USED-FOR distributions. discrepancy CONJUNCTION relational regularization. relational regularization CONJUNCTION discrepancy. relational discrepancy USED-FOR discrepancy. spherical sliced fused Gromov Wasserstein ( SSFG ) HYPONYM-OF relational discrepancy. mixture of von Mises - Fisher distributions USED-FOR vMF distribution. power spherical distribution USED-FOR sampling time. high dimension settings FEATURE-OF sampling time. power spherical distribution USED-FOR vMF distribution. discrepancies USED-FOR variants. discrepancies USED-FOR RAE framework. image generation CONJUNCTION reconstruction. reconstruction CONJUNCTION image generation. learning latent manifold structure CONJUNCTION image generation. image generation CONJUNCTION learning latent manifold structure. autoencoders USED-FOR learning latent manifold structure. autoencoders USED-FOR image generation. autoencoders USED-FOR reconstruction. OtherScientificTerm are inner discrepancy, von Mises - Fisher distribution, and latent manifold structure. Generic are approach, it, and variant. Task is discriminative task. Method is SSFG. ","This paper proposes a relational regularized autoencoder based on sliced fused Gromov Wasserstein (SSFG) to learn the distribution of data. The proposed method is based on the idea of relational discrepancy, which is a variant of the von Mises-Fisher (vMF) distribution. The authors show that SSFG can be combined with relational regularization and reconstruction loss to improve the performance of the RAE. The paper also shows that the proposed method can be used for learning latent manifold structure. ","This paper proposes a relational regularized autoencoder based on sliced fused Gromov Wasserstein (SSFG) to learn the distribution of data. The proposed method is based on the idea of relational discrepancy, which is a variant of the von Mises-Fisher (vMF) distribution. The authors show that SSFG can be combined with relational regularization and reconstruction loss to improve the performance of the RAE. The paper also shows that the proposed method can be used for learning latent manifold structure. "
1008,SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"natural language processing CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing. computational costs CONJUNCTION training time. training time CONJUNCTION computational costs. approach USED-FOR training. training USED-FOR deep networks. approach USED-FOR deep networks. repeated structures PART-OF deep networks. transformer module HYPONYM-OF repeated structures. deep linear networks USED-FOR theoretic analysis. theoretic analysis USED-FOR adaptive untying criterion. deep linear networks USED-FOR adaptive untying criterion. method USED-FOR BERT. training time EVALUATE-FOR BERT. training time EVALUATE-FOR method. OtherScientificTerm are deep learning model sizes, repeated layers, weight sharing, and monitoring gradient statistics. Method is deep network. ","This paper proposes a new method to reduce the training time of deep neural networks. The proposed method is based on the transformer module of deep linear networks (DNNs), which is an extension of the Transformer module. The authors show that the proposed method can be used to reduce training time and reduce the computational cost of training DNNs. The method is evaluated on BERT and CIFAR-10 datasets.","This paper proposes a new method to reduce the training time of deep neural networks. The proposed method is based on the transformer module of deep linear networks (DNNs), which is an extension of the Transformer module. The authors show that the proposed method can be used to reduce training time and reduce the computational cost of training DNNs. The method is evaluated on BERT and CIFAR-10 datasets."
1017,SP:a51710551142316b67e2fccd969fea1ece35ba39,interaction inside adversarial perturbations USED-FOR adversarial transferability. adversarial transferability CONJUNCTION interaction. interaction CONJUNCTION adversarial transferability. negative correlation FEATURE-OF adversarial transferability. DNNs USED-FOR negative correlation. negative correlation USED-FOR transferability - boosting methods. methods USED-FOR transferability. interactions USED-FOR attacking process. OtherScientificTerm is adversarial perturbations. ,"This paper studies the problem of adversarial transferability, which is an important problem in machine learning. The authors show that negative correlation between adversarial perturbations and transferability-boosting methods can be used to boost the transferability of DNNs. They also show that the negative correlation can be exploited to improve the performance of transferability boosting methods. ","This paper studies the problem of adversarial transferability, which is an important problem in machine learning. The authors show that negative correlation between adversarial perturbations and transferability-boosting methods can be used to boost the transferability of DNNs. They also show that the negative correlation can be exploited to improve the performance of transferability boosting methods. "
1033,SP:f1565319075c1442c2cb52d96443facb492c06c2,"neural network ( hidden ) representations CONJUNCTION task semantics. task semantics CONJUNCTION neural network ( hidden ) representations. deeper layers USED-FOR forgetting. sequential training USED-FOR task representational subspaces. Methods USED-FOR forgetting. Methods USED-FOR deeper layers. maximal forgetting FEATURE-OF task sequences. forgetting CONJUNCTION task semantic similarity. task semantic similarity CONJUNCTION forgetting. intermediate similarity FEATURE-OF task sequences. Task is Catastrophic forgetting. Method are deep learning models, and neural representations. Generic are some, and others. OtherScientificTerm are feature reuse, task representations, and interference. ","This paper studies the problem of catastrophic forgetting in deep learning models. The authors show that there are two types of forgetting: maximal forgetting and intermediate forgetting. They show that the maximal forgetting is a function of the number of tasks in the task space, and that the intermediate forgetting is an effect of task semantic similarity and task representational subspaces. They also show that if the task representations are reused, then the forgetting can be reduced to zero. ","This paper studies the problem of catastrophic forgetting in deep learning models. The authors show that there are two types of forgetting: maximal forgetting and intermediate forgetting. They show that the maximal forgetting is a function of the number of tasks in the task space, and that the intermediate forgetting is an effect of task semantic similarity and task representational subspaces. They also show that if the task representations are reused, then the forgetting can be reduced to zero. "
1049,SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"XLNet CONJUNCTION T5. T5 CONJUNCTION XLNet. BERT CONJUNCTION XLNet. XLNet CONJUNCTION BERT. Deep, heavily overparameterized language models USED-FOR natural language processing ( NLP ) tasks. T5 HYPONYM-OF Deep, heavily overparameterized language models. XLNet HYPONYM-OF Deep, heavily overparameterized language models. BERT HYPONYM-OF Deep, heavily overparameterized language models. training time USED-FOR pre - training and fine - tuning. computation resources CONJUNCTION training time. training time CONJUNCTION computation resources. computation resources USED-FOR model complexity. model compression USED-FOR large NLP models. large batch sizes USED-FOR pre - training time. training algorithm USED-FOR pre - training and fine - tuning. Early - Bird Lottery Tickets USED-FOR computer vision tasks. training algorithm USED-FOR large - scale language models. EarlyBERT HYPONYM-OF training algorithm. pre - training and fine - tuning USED-FOR large - scale language models. self - attention CONJUNCTION fully - connected sub - layers. fully - connected sub - layers CONJUNCTION self - attention. fully - connected sub - layers PART-OF transformer. structured winning tickets PART-OF BERT training. tickets USED-FOR BERT training. GLUE and SQuAD downstream tasks EVALUATE-FOR pre - training and fine - tuning. EarlyBERT COMPARE BERT. BERT COMPARE EarlyBERT. training time EVALUATE-FOR EarlyBERT. training time EVALUATE-FOR BERT. Metric is inference time. OtherScientificTerm are training process, and computational resource demands. ","This paper proposes a new training algorithm called EarlyBERT for pre-training and fine-tuning large-scale language models. The authors show that the proposed method can reduce the training time of BERT, XLNet, and XLNet-based language models by up to 50% and up to 80% respectively. They also show that their method is able to reduce the inference time by 50% compared to BERT. ","This paper proposes a new training algorithm called EarlyBERT for pre-training and fine-tuning large-scale language models. The authors show that the proposed method can reduce the training time of BERT, XLNet, and XLNet-based language models by up to 50% and up to 80% respectively. They also show that their method is able to reduce the inference time by 50% compared to BERT. "
1065,SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"classifier ’s predictions CONJUNCTION supervised labels. supervised labels CONJUNCTION classifier ’s predictions. f -divergence measure USED-FOR classifier ’s predictions. variational form USED-FOR decoupling property. variational difference CONJUNCTION bias term. bias term CONJUNCTION variational difference. decoupling property USED-FOR f -divergence measures. clean distribution FEATURE-OF variational difference. variational difference PART-OF divergence. bias term PART-OF divergence. robustness EVALUATE-FOR f -divergence functions. robustness FEATURE-OF f -divergence functions. f -divergence functions USED-FOR metrics. OtherScientificTerm are label noise, noise, and labels ’ noise rate. Task is learning with noisy labels. Generic is they. Material is UCSC - REAL. Metric is Robust - f - divergence - measures. ","This paper studies the robustness of f-divergence measures for learning with noisy labels. The authors propose a new measure called Robust-f-Divergence (R-F-D) to measure the decoupling property between the classifier’s predictions and the label noise. The proposed measure is based on the variational form of the divergence between the clean distribution and the noisy distribution, and the authors show that it can be used as a measure of robustness to label noise in the presence of noisy labels, and that it is more robust than other robustness measures such as UCSC-REAL. ","This paper studies the robustness of f-divergence measures for learning with noisy labels. The authors propose a new measure called Robust-f-Divergence (R-F-D) to measure the decoupling property between the classifier’s predictions and the label noise. The proposed measure is based on the variational form of the divergence between the clean distribution and the noisy distribution, and the authors show that it can be used as a measure of robustness to label noise in the presence of noisy labels, and that it is more robust than other robustness measures such as UCSC-REAL. "
1081,SP:841888179dcdac901889c8d62cb5234311fe28f1,"Q - ensemble USED-FOR uncertainty estimates. uncertainty estimates USED-FOR ensemble - based weighted Bellman backups. method USED-FOR learning. continuous and discrete control benchmarks EVALUATE-FOR method. weighted Bellman backups COMPARE Bellman backups. Bellman backups COMPARE weighted Bellman backups. weighted Bellman backups CONJUNCTION UCB Exploration. UCB Exploration CONJUNCTION weighted Bellman backups. ensemble USED-FOR weighted Bellman backups. off - policy RL algorithms USED-FOR continuous and discrete control tasks. lowdimensional and high - dimensional environments FEATURE-OF continuous and discrete control tasks. Bootstrap USED-FOR diversity. Soft Actor - Critic and Rainbow DQN HYPONYM-OF off - policy RL algorithms. Material is challenging domains. Task is Q - learning. OtherScientificTerm are Q - estimates, and noisy rewards. Metric is signal - to - noise aspect. ","This paper proposes a new method for off-policy reinforcement learning based on ensemble-based weighted Bellman backups. The proposed method is motivated by the observation that Bellman backbones are not robust to noisy rewards. To address this issue, the authors propose to use a Bootstrap-based ensemble to improve the diversity of the ensemble. The method is evaluated on continuous and discrete control tasks. ","This paper proposes a new method for off-policy reinforcement learning based on ensemble-based weighted Bellman backups. The proposed method is motivated by the observation that Bellman backbones are not robust to noisy rewards. To address this issue, the authors propose to use a Bootstrap-based ensemble to improve the diversity of the ensemble. The method is evaluated on continuous and discrete control tasks. "
1097,SP:afc08f203562b841180811aef943bfb63a1659ea,"meta - learning algorithms USED-FOR fewshot classification problems. few - shot classification framework USED-FOR modeling uncertainty. meta - training USED-FOR model. class - wise similarities USED-FOR distributional mismatch. meta - learning models PART-OF method. training strategy USED-FOR model. training strategy USED-FOR calibrated classification. Task are prediction of uncertainty, and random sampling of tasks. OtherScientificTerm is dataset shift. Metric is accuracy. ","This paper proposes a meta-learning framework for few-shot classification. The proposed method is based on meta-training, where the model is trained on a set of tasks and then used to learn a class-wise similarity between tasks. The method is evaluated on a variety of tasks, and is shown to outperform the baselines in terms of accuracy. ","This paper proposes a meta-learning framework for few-shot classification. The proposed method is based on meta-training, where the model is trained on a set of tasks and then used to learn a class-wise similarity between tasks. The method is evaluated on a variety of tasks, and is shown to outperform the baselines in terms of accuracy. "
1113,SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"dominant paradigm USED-FOR video - text representations. generative model USED-FOR method. VATEX CONJUNCTION ActivityNet. ActivityNet CONJUNCTION VATEX. ActivityNet CONJUNCTION MSVD. MSVD CONJUNCTION ActivityNet. MSR - VTT CONJUNCTION VATEX. VATEX CONJUNCTION MSR - VTT. method COMPARE others. others COMPARE method. MSVD EVALUATE-FOR method. ActivityNet EVALUATE-FOR method. VATEX EVALUATE-FOR method. MSR - VTT EVALUATE-FOR others. MSR - VTT EVALUATE-FOR method. Method are noise contrastive learning, and dissimilar representations. Generic is representations. OtherScientificTerm are visually similar videos, and depicted action. ","This paper proposes a novel method for video-text representation learning based on noise contrastive learning. The proposed method is based on a generative model, which is trained on a set of videos, and then used to learn a representation for each video. The authors show that the proposed method outperforms the baselines on a number of tasks, including MSVD, VATEX, MSR-VTT, and ActivityNet.","This paper proposes a novel method for video-text representation learning based on noise contrastive learning. The proposed method is based on a generative model, which is trained on a set of videos, and then used to learn a representation for each video. The authors show that the proposed method outperforms the baselines on a number of tasks, including MSVD, VATEX, MSR-VTT, and ActivityNet."
1129,SP:8a71d8fad25a126aff01431cacf348c05de75667,pre - trained language models ( PLMs ) USED-FOR Chinese natural language processing ( NLP ) tasks. single vocabulary USED-FOR masked language model pre - training. Chinese word segmentation ( CWS ) CONJUNCTION subword tokenization. subword tokenization CONJUNCTION Chinese word segmentation ( CWS ). seg tok USED-FOR Chinese BERT. Chinese word segmentation ( CWS ) USED-FOR method. subword tokenization USED-FOR method. multi - vocabulary pretraining ( MVP ) USED-FOR models expressiveness. char based vocabulary COMPARE seg tok. seg tok COMPARE char based vocabulary. MVP USED-FOR PLMs. seg tok USED-FOR Chinese PLMs. it USED-FOR seg tok. char based vocabulary USED-FOR Chinese PLMs. seg tok COMPARE it. it COMPARE seg tok. sequence labeling tasks EVALUATE-FOR it. sentence level tasks EVALUATE-FOR Chinese PLMs. sequence labeling tasks EVALUATE-FOR seg tok. sentence level tasks EVALUATE-FOR seg tok. OtherScientificTerm is Chinese characters. ,"This paper proposes a multi-vocabulary pretraining (MVP) method for Chinese pre-trained language models to improve the expressiveness of Chinese BERT models. The proposed method is based on the idea of multi-vocabulary pre-training, which is an extension of the multi-word tokenization (MV) method proposed in [1] and [2]. The authors show that the proposed method outperforms the existing methods in terms of expressiveness on a number of tasks. ","This paper proposes a multi-vocabulary pretraining (MVP) method for Chinese pre-trained language models to improve the expressiveness of Chinese BERT models. The proposed method is based on the idea of multi-vocabulary pre-training, which is an extension of the multi-word tokenization (MV) method proposed in [1] and [2]. The authors show that the proposed method outperforms the existing methods in terms of expressiveness on a number of tasks. "
1145,SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - based learning tasks. graph partition CONJUNCTION distributed training. distributed training CONJUNCTION graph partition. memory CONJUNCTION communications. communications CONJUNCTION memory. boundary nodes PART-OF partitioned subgraph. BDS - GCN USED-FOR distributed GCN training. method USED-FOR distributed GCN training. BDS - GCN HYPONYM-OF method. unbiased boundary sampling strategy USED-FOR BDS - GCN. full - graph accuracy EVALUATE-FOR method. full - graph accuracy EVALUATE-FOR BDS - GCN. unbiased boundary sampling strategy USED-FOR method. accuracy EVALUATE-FOR state - of - the - art methods. throughput EVALUATE-FOR BDS - GCN. BDS - GCN USED-FOR GCN training. Method are GCNs, and GCN architectures. Material is real - world large graphs. OtherScientificTerm is GCN structures. Metric is memory usage. ","This paper proposes a novel method for GCN training. The proposed method is based on the idea of partitioned subgraphs, where each subgraph is partitioned into subgraph nodes, and each node is represented as a subset of the entire subgraph. The authors propose to use an unbiased boundary sampling strategy to sample the subgraph boundary nodes. The method is evaluated on a number of benchmark datasets, and shows that the proposed method outperforms state-of-the-art methods.","This paper proposes a novel method for GCN training. The proposed method is based on the idea of partitioned subgraphs, where each subgraph is partitioned into subgraph nodes, and each node is represented as a subset of the entire subgraph. The authors propose to use an unbiased boundary sampling strategy to sample the subgraph boundary nodes. The method is evaluated on a number of benchmark datasets, and shows that the proposed method outperforms state-of-the-art methods."
1161,SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"Machine Learning ( ML ) USED-FOR large - scale physics - based simulations. models USED-FOR real large - scale and complex problems. quantum chemistry simulations USED-FOR catalyst discovery. model USED-FOR quantum chemistry simulations. catalyst discovery USED-FOR renewable energy applications. model USED-FOR catalyst discovery. ForceNet USED-FOR quantum chemistry simulations. ForceNet HYPONYM-OF graph neural network. graph neural network USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR graph neural network. model scaling USED-FOR ForceNet. expressive message passing architecture USED-FOR ForceNet. basis and non - linear activation functions USED-FOR ForceNet. ForceNet COMPARE ML models. ML models COMPARE ForceNet. ForceNet USED-FOR atomic forces. ForceNet USED-FOR large - scale catalyst dataset. OC20 HYPONYM-OF large - scale catalyst dataset. ForceNet USED-FOR quantum chemistry simulations. ForceNet COMPARE ML models. ML models COMPARE ForceNet. success rate EVALUATE-FOR ML models. success rate EVALUATE-FOR ForceNet. ML - based simulations COMPARE physics - based simulations. physics - based simulations COMPARE ML - based simulations. Task is atomic simulations. OtherScientificTerm are 3D space, forces, and out - of - distribution structures. ","This paper proposes a graph neural network for quantum chemistry simulations. The proposed method is based on graph neural networks (GNNs) and is able to learn per-atomic forces in 3D space. The authors show that the proposed method outperforms state-of-the-art methods on OC20, a large-scale quantum chemistry simulation dataset. ","This paper proposes a graph neural network for quantum chemistry simulations. The proposed method is based on graph neural networks (GNNs) and is able to learn per-atomic forces in 3D space. The authors show that the proposed method outperforms state-of-the-art methods on OC20, a large-scale quantum chemistry simulation dataset. "
1177,SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,regularisation USED-FOR fine - tuning. approaches USED-FOR regularisation. approaches USED-FOR fine - tuning. regularisation USED-FOR deep neural networks. Rademacher complexity USED-FOR neural network generalisation bound. bound COMPARE bounds. bounds COMPARE bound. bounds USED-FOR convolutional networks. bound USED-FOR fine - tuning. learning USED-FOR generalisation. initialisation FEATURE-OF network. transfer learning USED-FOR initialisation. transfer learning USED-FOR network. fine - tuning algorithm USED-FOR hypothesis class. generalisation EVALUATE-FOR transfer learning. It COMPARE fine - tuning competitors. fine - tuning competitors COMPARE It. It COMPARE penalty - based alternatives. penalty - based alternatives COMPARE It. Generic is algorithm. OtherScientificTerm is radius of the search space. ,This paper studies the generalization bound for neural networks. The authors show that the Rademacher complexity of the generalisation bound for deep neural networks is a function of the radius of the search space and the number of training samples. They show that this bound depends on the initialisation of the network. They also show that transfer learning can be used as a regularizer to improve the performance of a network. ,This paper studies the generalization bound for neural networks. The authors show that the Rademacher complexity of the generalisation bound for deep neural networks is a function of the radius of the search space and the number of training samples. They show that this bound depends on the initialisation of the network. They also show that transfer learning can be used as a regularizer to improve the performance of a network. 
1193,SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"model evaluation EVALUATE-FOR mask discovery. training configuration USED-FOR mask. mask discovery ( Hfind ) CONJUNCTION mask evaluation ( Heval ). mask evaluation ( Heval ) CONJUNCTION mask discovery ( Hfind ). hyperparameters USED-FOR mask evaluation ( Heval ). hyperparameters USED-FOR mask discovery ( Hfind ). unstructured magnitude pruning USED-FOR vision classification tasks. hyperparameters USED-FOR stages. unstructured magnitude pruning USED-FOR decoupled find - eval phenomenon. hyperparameters USED-FOR masks. Hfind values USED-FOR masks. layerwise pruning ratios FEATURE-OF masks. ratios USED-FOR decoupled find - eval phenomenon. Task is model pruning. Generic are model, and models. Method are lottery ticket framework, and one - shot structured pruning. OtherScientificTerm is decoupling hyperparameters. ",This paper proposes a lottery ticket framework for mask discovery and mask evaluation. The lottery ticket is based on the observation that mask discovery is decoupled from mask evaluation (Hfind). The paper then proposes to use unstructured magnitude pruning to improve the performance of mask discovery. The paper is well written and easy to follow. ,This paper proposes a lottery ticket framework for mask discovery and mask evaluation. The lottery ticket is based on the observation that mask discovery is decoupled from mask evaluation (Hfind). The paper then proposes to use unstructured magnitude pruning to improve the performance of mask discovery. The paper is well written and easy to follow. 
1209,SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"training USED-FOR alignment of per - example gradients. metrics COMPARE m - coherence. m - coherence COMPARE metrics. m - coherence COMPARE O(m ). O(m ) COMPARE m - coherence. memorization CONJUNCTION generalization. generalization CONJUNCTION memorization. ResNet CONJUNCTION EfficientNet models. EfficientNet models CONJUNCTION ResNet. m - coherence USED-FOR evolution of alignment of per - example gradients. label noise FEATURE-OF variants. ImageNet EVALUATE-FOR EfficientNet models. m - coherence COMPARE real labels. real labels COMPARE m - coherence. neural networks USED-FOR generalization. OtherScientificTerm are gradient, and gradient diversity. Method are Coherent Gradients ( CG ) theory, over - parameterized neural networks, and CG. ","This paper proposes a new metric called Coherent Gradients (m-coherence) to measure the alignment of per-example gradients in neural networks. The authors show that m-Coherence is a measure of the generalization ability of neural networks under label noise, and that it can be used as a metric to measure memorization and generalization performance. The paper also shows that the coherence of the per-instance gradients is a function of the gradient diversity of the training data. ","This paper proposes a new metric called Coherent Gradients (m-coherence) to measure the alignment of per-example gradients in neural networks. The authors show that m-Coherence is a measure of the generalization ability of neural networks under label noise, and that it can be used as a metric to measure memorization and generalization performance. The paper also shows that the coherence of the per-instance gradients is a function of the gradient diversity of the training data. "
1225,SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"summary statistics USED-FOR implicit generative models. approach USED-FOR approximate Bayesian computation. approximate Bayesian computation CONJUNCTION neural likelihood methods. neural likelihood methods CONJUNCTION approximate Bayesian computation. approach USED-FOR neural likelihood methods. tasks EVALUATE-FOR approach. Task are evaluation of the likelihood function, and constructing sufficient statistics. OtherScientificTerm are likelihood function, sufficient statistics, and density or density ratio. Generic is model. Method are deep neural networks, and infomax learning procedure. ","This paper proposes a new approach to approximate Bayesian computation for implicit generative models. The proposed approach is based on the idea of infomax learning, which is an extension of prior work on inference in deep neural networks. The main contribution of the paper is to introduce the notion of ""sufficient statistics"", which is defined as the ratio of the density or density ratio between the likelihood function and the sum of the probability distributions of the two distributions. The authors show that the proposed approach can be applied to a variety of tasks, and show that it can be used to improve the performance of neural likelihood methods. ","This paper proposes a new approach to approximate Bayesian computation for implicit generative models. The proposed approach is based on the idea of infomax learning, which is an extension of prior work on inference in deep neural networks. The main contribution of the paper is to introduce the notion of ""sufficient statistics"", which is defined as the ratio of the density or density ratio between the likelihood function and the sum of the probability distributions of the two distributions. The authors show that the proposed approach can be applied to a variety of tasks, and show that it can be used to improve the performance of neural likelihood methods. "
1241,SP:c5997bf2348e94949684f45fbd418661e85220c1,"set - level supervision USED-FOR data collection. paired images CONJUNCTION domain labels. domain labels CONJUNCTION paired images. model COMPARE set - level supervised model. set - level supervised model COMPARE model. pseudo domains HYPONYM-OF hyperparameters. full labels USED-FOR set - level supervised model. TUNIT USED-FOR semi - supervised scenario. Method is image - to - image translation model. Task is image - to - image translation. OtherScientificTerm are image domains, and estimated domains. ","This paper proposes a semi-supervised model for image-to-image translation. The proposed model is based on the idea of pseudo domains, which is an extension of the pseudo domain framework. The authors show that the proposed model outperforms a set-level supervised model on the task of image to image translation.","This paper proposes a semi-supervised model for image-to-image translation. The proposed model is based on the idea of pseudo domains, which is an extension of the pseudo domain framework. The authors show that the proposed model outperforms a set-level supervised model on the task of image to image translation."
1257,SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"probability distribution USED-FOR network parameters. it USED-FOR initialization procedures. probability distribution USED-FOR curvature penalty function. asymmetric initialization USED-FOR constant curvature penalty. natural cubic spline interpolation USED-FOR solution function. uniform distribution USED-FOR asymmetric initialization. multivariate regression CONJUNCTION activation functions. activation functions CONJUNCTION multivariate regression. regularization strength FEATURE-OF spatially adaptive smoothing splines. spatially adaptive smoothing splines USED-FOR training trajectories. Method are wide neural networks, and width - n shallow ReLU network. OtherScientificTerm are implicit bias in function space, and weighted second derivative. Task is 1D regression. ","This paper studies the problem of initialization of ReLU networks in the presence of implicit bias in function space. The authors show that the bias is due to the curvature penalty function, which is a function of the number of parameters in the network. They show that under certain assumptions, the bias can be reduced to a constant curvature. They also show that this bias is reduced to zero under the assumption that the second derivative of the first derivative is uniform. ","This paper studies the problem of initialization of ReLU networks in the presence of implicit bias in function space. The authors show that the bias is due to the curvature penalty function, which is a function of the number of parameters in the network. They show that under certain assumptions, the bias can be reduced to a constant curvature. They also show that this bias is reduced to zero under the assumption that the second derivative of the first derivative is uniform. "
1273,SP:8b885142facbb3b8db41ec9d83822cee81324694,Weight decay HYPONYM-OF regularization technique. regularization technique USED-FOR deep neural networks. L2 regularization USED-FOR weight decay. L2 regularization USED-FOR deep learning libraries. L2 regularization COMPARE weight decay. weight decay COMPARE L2 regularization. weight decay USED-FOR adaptive gradient methods. L2 regularization USED-FOR adaptive gradient methods. Decoupled Weight Decay ( AdamW ) USED-FOR Adam. Adaptive Momentum Estimation ( Adam ) HYPONYM-OF adaptive gradient methods. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. decoupled weight decay PART-OF deep learning libraries. decoupled weight decay HYPONYM-OF weight decay. L2 regularization HYPONYM-OF weight decay. unstable weight decay USED-FOR optimizers. stochastic gradient descent ( SGD ) HYPONYM-OF Momentum. stochastic gradient descent ( SGD ) HYPONYM-OF optimizers. Momentum USED-FOR optimizers. decoupled weight decay USED-FOR adaptive gradient methods. Stable Weight Decay ( SWD ) method USED-FOR unstable weight decay problem. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. SWD method COMPARE decoupled weight decay. decoupled weight decay COMPARE SWD method. SWD method COMPARE L2 regularization. L2 regularization COMPARE SWD method. weight decay PART-OF Adam. hyperparameters FEATURE-OF Adam variants. SWD USED-FOR Adam. SWD USED-FOR weight decay. OtherScientificTerm is hyperparameter. ,"This paper proposes a new regularization technique for adaptive gradient methods, called Stable Weight Decay (SWD), which is based on the idea of weight decay. The authors show that SWD can be used in combination with L2 regularization and decoupled weight decay to improve the performance of adaptive gradient algorithms. SWD is shown to be more stable than L2-based regularization, and can be applied to the unstable weight decay problem. ","This paper proposes a new regularization technique for adaptive gradient methods, called Stable Weight Decay (SWD), which is based on the idea of weight decay. The authors show that SWD can be used in combination with L2 regularization and decoupled weight decay to improve the performance of adaptive gradient algorithms. SWD is shown to be more stable than L2-based regularization, and can be applied to the unstable weight decay problem. "
1289,SP:a3206dc71e32ba1830895bf442d3840f3331a532,"Translation Memory ( TM ) USED-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR Translation Memory ( TM ). encoder USED-FOR TM. TM CONJUNCTION NMT. NMT CONJUNCTION TM. method USED-FOR NMT. method USED-FOR TM. encoder USED-FOR TM information. pre - trained language model ( PLM ) USED-FOR encoder. sentence level retrieval method USED-FOR n - gram retrieval method. methods USED-FOR information flow. TM CONJUNCTION NMT decoder. NMT decoder CONJUNCTION TM. translation quality EVALUATE-FOR methods. OtherScientificTerm are semantic relationship, and similarity score. Method is sentence level retrieval approach. ",This paper proposes a new method to improve the performance of neural machine translation (NMT) and translation memory (TM). The proposed method is based on a pre-trained language model (PLM) and a sentence level retrieval method. The authors show that the proposed method can improve the quality of NMT and TM. ,This paper proposes a new method to improve the performance of neural machine translation (NMT) and translation memory (TM). The proposed method is based on a pre-trained language model (PLM) and a sentence level retrieval method. The authors show that the proposed method can improve the quality of NMT and TM. 
1305,SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,rate reduction CONJUNCTION ( shift ) invariant classification. ( shift ) invariant classification CONJUNCTION rate reduction. rate reduction USED-FOR deep ( convolutional ) networks. ( shift ) invariant classification USED-FOR deep ( convolutional ) networks. iterative gradient ascent scheme USED-FOR rate reduction of learned features. iterative gradient ascent scheme USED-FOR deep network. components PART-OF network. network USED-FOR discriminative deep representation. linear operators PART-OF multi - channel convolutions. spectral domain FEATURE-OF convolutional network. Generic is architectures. Method is back propagation training. Task is classification. ,This paper studies the problem of rate reduction in deep convolutional neural networks. The authors propose an iterative gradient ascent scheme for rate reduction of learned features. The main contribution of the paper is to show that rate reduction can be achieved in the spectral domain. The paper also provides theoretical analysis of the convergence rate of the proposed method. ,This paper studies the problem of rate reduction in deep convolutional neural networks. The authors propose an iterative gradient ascent scheme for rate reduction of learned features. The main contribution of the paper is to show that rate reduction can be achieved in the spectral domain. The paper also provides theoretical analysis of the convergence rate of the proposed method. 
1321,SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"implicit acceleration of gradient flow USED-FOR over - parameterized two - layer linear models. conservation law USED-FOR implicit acceleration. spectrum USED-FOR acceleration. matrix factorization problem CONJUNCTION Riccati type differential equations. Riccati type differential equations CONJUNCTION matrix factorization problem. small, balanced or spectral initialization FEATURE-OF weights. Method is gradient flow. OtherScientificTerm is Gramian matrices. ","This paper studies the implicit acceleration of gradient flow for over-parameterized two-layer linear models. In particular, the authors consider the case where the weights of the gradient flow are initialized with small, balanced or spectral initialization. The authors show that the acceleration is a function of the spectrum of the initialization of the weights. They also show that under certain conditions, the acceleration can be bounded by a conservation law. ","This paper studies the implicit acceleration of gradient flow for over-parameterized two-layer linear models. In particular, the authors consider the case where the weights of the gradient flow are initialized with small, balanced or spectral initialization. The authors show that the acceleration is a function of the spectrum of the initialization of the weights. They also show that under certain conditions, the acceleration can be bounded by a conservation law. "
1337,SP:e5f086c806be88d50e461a782b5b00124f4656fb,"approach USED-FOR opaque model ’s behavior. uniform sampling of user - defined subspaces USED-FOR framework. framework USED-FOR ML model. CLIME USED-FOR ML model. CLIME HYPONYM-OF framework. Method are machine learning techniques, LIME, surrogate interpretable model, LIME framework, and estimation algorithm. Task are explainable AI, OOD sampling problem, OOD sampling, and real - world problems. OtherScientificTerm are LIME ’s explanations, adversarial attacks, perturbation procedure, and logical constraints. Generic is model. Metric are fidelity, and accuracy. ","This paper proposes CLIME, a new framework for explainable machine learning. CLIME is based on the idea of surrogate interpretable model, which is an extension of the LIME framework. The key idea is to use a surrogate interpretability model to learn an explainable model that can be used for OOD sampling of user-defined subspaces. The proposed method is evaluated on a number of real-world problems, including adversarial attacks, adversarial perturbation procedure, and adversarial examples. The paper shows that CLIME outperforms existing methods in terms of fidelity and accuracy.","This paper proposes CLIME, a new framework for explainable machine learning. CLIME is based on the idea of surrogate interpretable model, which is an extension of the LIME framework. The key idea is to use a surrogate interpretability model to learn an explainable model that can be used for OOD sampling of user-defined subspaces. The proposed method is evaluated on a number of real-world problems, including adversarial attacks, adversarial perturbation procedure, and adversarial examples. The paper shows that CLIME outperforms existing methods in terms of fidelity and accuracy."
1353,SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,Pre - trained language models USED-FOR natural language understanding ( NLU ). BERT HYPONYM-OF Pre - trained language models. Chinese HYPONYM-OF languages. English HYPONYM-OF languages. multi - word expressions USED-FOR natural lexical units. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language models. AMBERT HYPONYM-OF Multi - grained BERT. AMBERT HYPONYM-OF pre - trained language model. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language model. contextualized representations of the words CONJUNCTION contextualized representations of the phrases. contextualized representations of the phrases CONJUNCTION contextualized representations of the words. encoder CONJUNCTION encoder. encoder CONJUNCTION encoder. English USED-FOR AMBERT. encoder USED-FOR AMBERT. GLUE CONJUNCTION SQuAD. SQuAD CONJUNCTION GLUE. SQuAD CONJUNCTION RACE. RACE CONJUNCTION SQuAD. CLUE CONJUNCTION GLUE. GLUE CONJUNCTION CLUE. Chinese CONJUNCTION English. English CONJUNCTION Chinese. Chinese HYPONYM-OF benchmark datasets. English HYPONYM-OF benchmark datasets. RACE HYPONYM-OF benchmark datasets. SQuAD HYPONYM-OF benchmark datasets. CLUE HYPONYM-OF benchmark datasets. GLUE HYPONYM-OF benchmark datasets. AMBERT COMPARE models. models COMPARE AMBERT. Chinese EVALUATE-FOR AMBERT. AMBERT COMPARE AMBERT. AMBERT COMPARE AMBERT. Method is coarse - grained tokenization. Metric is inference time. ,"This paper proposes AMBERT, a multi-word pre-trained language model for natural language understanding (NLU). The authors propose to combine fine-grained and coarse-grain tokenization to improve the performance of pre-training language models. The proposed method is evaluated on three benchmark datasets: CLUE, GLUE, and SQuAD. The results show that the proposed method outperforms the baselines on all three benchmarks. ","This paper proposes AMBERT, a multi-word pre-trained language model for natural language understanding (NLU). The authors propose to combine fine-grained and coarse-grain tokenization to improve the performance of pre-training language models. The proposed method is evaluated on three benchmark datasets: CLUE, GLUE, and SQuAD. The results show that the proposed method outperforms the baselines on all three benchmarks. "
1369,SP:fd1cfe80343d3789227d99d836a5674374a234f5,"task USED-FOR natural language utterance. Semantic parsing HYPONYM-OF task. natural language utterance USED-FOR machine - understandable information representation. task USED-FOR machine - understandable information representation. Transformer USED-FOR semantic parsing. PhraseTransformer architecture USED-FOR meaning representation. phrase dependencies USED-FOR PhraseTransformer architecture. phrase dependencies USED-FOR meaning representation. Long Short - Term Memory ( LSTM ) USED-FOR local context of phrases. Self - Attention mechanism USED-FOR local context of phrases. Long Short - Term Memory ( LSTM ) PART-OF Self - Attention mechanism. Self - Attention mechanism PART-OF Transformer. model COMPARE Transformer. Transformer COMPARE model. model USED-FOR detailed meaning. model USED-FOR local context awareness. Neural Network USED-FOR Atis dataset. Geo, MSParS datasets EVALUATE-FOR model. Method is Neural Machine Translation. OtherScientificTerm is long - range word dependencies. ",This paper proposes a Transformer-based model for semantic parsing. The proposed model is based on the Self-Attention mechanism and the Long Short-Term Memory (LSTM) to capture the local context of phrases. The model is evaluated on the Atis dataset and Geo-MSParS dataset.,This paper proposes a Transformer-based model for semantic parsing. The proposed model is based on the Self-Attention mechanism and the Long Short-Term Memory (LSTM) to capture the local context of phrases. The model is evaluated on the Atis dataset and Geo-MSParS dataset.
1385,SP:2056a65a7500d79465685af883083cd706277c1f,"combinations of multiple perturbations FEATURE-OF DNN robustness. composite adversarial training ( CAT ) HYPONYM-OF training method. robustness EVALUATE-FOR individual perturbations. training method USED-FOR multiple adversarial losses. pixel perturbations CONJUNCTION spatial transformations. spatial transformations CONJUNCTION pixel perturbations. CAT COMPARE adversarial training methods. adversarial training methods COMPARE CAT. benchmark datasets EVALUATE-FOR CAT. spatial transformations HYPONYM-OF adversarial perturbation models. Method are deep neural networks ( DNNs ), and individual perturbation models. OtherScientificTerm is adversarial perturbations. ","This paper studies the problem of adversarial robustness. The authors propose a new training method, composite adversarial training (CAT), to improve the robustness of DNNs against multiple adversarial perturbations. The proposed method is evaluated on several benchmark datasets.","This paper studies the problem of adversarial robustness. The authors propose a new training method, composite adversarial training (CAT), to improve the robustness of DNNs against multiple adversarial perturbations. The proposed method is evaluated on several benchmark datasets."
1401,SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"Emergent Symbol Binding Network ( ESBN ) HYPONYM-OF recurrent network. external memory USED-FOR variable - binding. external memory USED-FOR recurrent network. binding mechanism USED-FOR symbol - like representations. ESBN USED-FOR rules. learning process USED-FOR symbol - like representations. architecture COMPARE competitive neural network architectures. competitive neural network architectures COMPARE architecture. Task are human intelligence, and induction of abstract rules. OtherScientificTerm are abstract rules, and symbol - processing machinery. Material are high - dimensional sensory data, and high - dimensional data. Method are Deep neural network algorithms, and symbol - processing mechanisms. ","This paper proposes a new method for learning abstract rules that can be used to learn symbolic representations of abstract rules. The method is based on the emergent symbol binding network (ESBN), which is a recurrent network with a variable-bounding mechanism. The proposed method is evaluated on synthetic and real-world datasets. The authors show that the proposed method outperforms the state-of-the-art in terms of performance and efficiency. ","This paper proposes a new method for learning abstract rules that can be used to learn symbolic representations of abstract rules. The method is based on the emergent symbol binding network (ESBN), which is a recurrent network with a variable-bounding mechanism. The proposed method is evaluated on synthetic and real-world datasets. The authors show that the proposed method outperforms the state-of-the-art in terms of performance and efficiency. "
1417,SP:4171ce45966ac499f51450a19fb233934c0847f0,"nested named entity recognition CONJUNCTION relation classification. relation classification CONJUNCTION nested named entity recognition. framework USED-FOR structured prediction language tasks. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. event extraction CONJUNCTION coreference resolution. coreference resolution CONJUNCTION event extraction. semantic role labeling CONJUNCTION event extraction. event extraction CONJUNCTION semantic role labeling. joint entity and relation extraction CONJUNCTION nested named entity recognition. nested named entity recognition CONJUNCTION joint entity and relation extraction. coreference resolution CONJUNCTION dialogue state tracking. dialogue state tracking CONJUNCTION coreference resolution. dialogue state tracking HYPONYM-OF structured prediction language tasks. coreference resolution HYPONYM-OF structured prediction language tasks. event extraction HYPONYM-OF structured prediction language tasks. semantic role labeling HYPONYM-OF structured prediction language tasks. joint entity and relation extraction HYPONYM-OF structured prediction language tasks. relation classification HYPONYM-OF structured prediction language tasks. nested named entity recognition HYPONYM-OF structured prediction language tasks. translation task USED-FOR it. augmented natural languages USED-FOR translation task. task - specific discriminative classifiers USED-FOR problem. FewRel CONJUNCTION TACRED. TACRED CONJUNCTION FewRel. approach COMPARE task - specific models. task - specific models COMPARE approach. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. joint entity and relation extraction CONJUNCTION relation classification. relation classification CONJUNCTION joint entity and relation extraction. relation classification CONJUNCTION FewRel. FewRel CONJUNCTION relation classification. relation classification CONJUNCTION TACRED. TACRED CONJUNCTION relation classification. approach COMPARE,., COMPARE approach. relation classification EVALUATE-FOR,. joint entity and relation extraction EVALUATE-FOR,. tasks EVALUATE-FOR task - specific models. tasks EVALUATE-FOR approach. joint entity and relation extraction EVALUATE-FOR approach. model USED-FOR tasks. hyperparameters USED-FOR tasks. architecture USED-FOR tasks. architecture CONJUNCTION","This paper proposes a new framework for structured prediction language tasks. The proposed framework is based on task-specific discriminative classifiers, which can be applied to a variety of tasks such as entity recognition, relation classification, semantic role labeling, coreference resolution, event extraction, and dialogue state tracking. The framework is evaluated on a number of tasks, and compared to a set of baseline models. ","This paper proposes a new framework for structured prediction language tasks. The proposed framework is based on task-specific discriminative classifiers, which can be applied to a variety of tasks such as entity recognition, relation classification, semantic role labeling, coreference resolution, event extraction, and dialogue state tracking. The framework is evaluated on a number of tasks, and compared to a set of baseline models. "
1433,SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"named entity recognition ( NER ) models USED-FOR unlabeled entity problem. approach USED-FOR misguidance. unlabeled entities USED-FOR NER models. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. model COMPARE prior baselines. prior baselines COMPARE model. real - world datasets EVALUATE-FOR model. synthetic datasets EVALUATE-FOR model. model USED-FOR unlabeled entity problem. well - annotated datasets EVALUATE-FOR model. OtherScientificTerm is negative instances. Method are pretraining language models, and negative sampling. ",This paper proposes a novel approach to improve the performance of NER models for the unlabeled entity recognition problem. The authors propose to use negative sampling to reduce the misguidance of the model. They show that the proposed approach is able to achieve better performance on synthetic and real-world datasets. They also show that negative sampling can be used to improve performance on well-annotated datasets.,This paper proposes a novel approach to improve the performance of NER models for the unlabeled entity recognition problem. The authors propose to use negative sampling to reduce the misguidance of the model. They show that the proposed approach is able to achieve better performance on synthetic and real-world datasets. They also show that negative sampling can be used to improve performance on well-annotated datasets.
1449,SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"stochastic neighbor embedding ( SNE ) USED-FOR sequential inputs. stochastic neighbor embedding ( SNE ) USED-FOR vector space of fixed, reduced dimensions. Acoustic Neighbor Embeddings HYPONYM-OF acoustic word embedding. Euclidean distance USED-FOR phonetic confusability. acoustic encoder CONJUNCTION text encoder. text encoder CONJUNCTION acoustic encoder. acoustic encoder USED-FOR speech signals. frame - wise subword posterior probabilities USED-FOR acoustic encoder. frame - wise subword posterior probabilities FEATURE-OF speech signals. acoustic model USED-FOR frame - wise subword posterior probabilities. acoustic encoder HYPONYM-OF encoder neural networks. text encoder HYPONYM-OF encoder neural networks. triplet loss criterion COMPARE method. method COMPARE triplet loss criterion. gradients USED-FOR neural network training. method USED-FOR neural network training. gradients EVALUATE-FOR method. text encoder network USED-FOR approximate phonetic matching task. encoder networks USED-FOR word ( name ) recognition task. low - dimensional embeddings USED-FOR it. encoder networks USED-FOR it. recognition accuracy EVALUATE-FOR finite state transducer(FST)-based decoding. test data USED-FOR finite state transducer(FST)-based decoding. Euclidean nearest - neighbor search USED-FOR isolated name recognition task. Material is speech. OtherScientificTerm are embedding space, subword transcriptions, embedding vectors, and embeddings. ","This paper proposes a novel method for learning acoustic word embeddings for speech signals. The key idea is to use stochastic neighbor embedding (SNE) to learn the embedding space of a vector space of fixed, reduced dimensions. The proposed method is based on the Euclidean nearest-neighbor search (ENS) algorithm. The authors show that the proposed method outperforms the triplet loss criterion for neural network training and FST-based decoding. ","This paper proposes a novel method for learning acoustic word embeddings for speech signals. The key idea is to use stochastic neighbor embedding (SNE) to learn the embedding space of a vector space of fixed, reduced dimensions. The proposed method is based on the Euclidean nearest-neighbor search (ENS) algorithm. The authors show that the proposed method outperforms the triplet loss criterion for neural network training and FST-based decoding. "
1465,SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,reinforcement learning algorithm USED-FOR stationary mean - field games. mean - field state USED-FOR Nash equilibrium. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. gradient - descent CONJUNCTION proximal policy optimization. proximal policy optimization CONJUNCTION gradient - descent. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. proximal policy optimization USED-FOR fictitious play algorithm. gradient - descent USED-FOR fictitious play algorithm. gradient - descent USED-FOR policy. proximal policy optimization USED-FOR policy. algorithm USED-FOR single - agent reinforcement learning problem. fictitious play algorithm USED-FOR Nash equilibrium. OtherScientificTerm is optimum. Task is mean - field games. ,"This paper studies the problem of learning a policy for stationary mean-field games. The authors propose a new algorithm for this problem, called fictitious play, which is motivated by the fact that the state of the game can be represented as a function of the policy and the mean field state. They show that under certain assumptions, the proposed algorithm converges to a Nash equilibrium in a single-agent reinforcement learning problem. They also show that the algorithm can be used to learn a proximal policy for a single agent RL problem.","This paper studies the problem of learning a policy for stationary mean-field games. The authors propose a new algorithm for this problem, called fictitious play, which is motivated by the fact that the state of the game can be represented as a function of the policy and the mean field state. They show that under certain assumptions, the proposed algorithm converges to a Nash equilibrium in a single-agent reinforcement learning problem. They also show that the algorithm can be used to learn a proximal policy for a single agent RL problem."
1481,SP:c498f8a199da1818fe64ed88b0825c5aad688aec,joint distribution USED-FOR probabilistic inference. normalizing flow model USED-FOR probabilistic inference. normalizing flow model USED-FOR joint distribution. flow models USED-FOR task. framework USED-FOR approximate probabilistic inference. method USED-FOR generative model. flow model USED-FOR distribution. variational inference USED-FOR it. arbitrary differentiable transformations USED-FOR conditioning. likelihood evaluation CONJUNCTION inversion. inversion CONJUNCTION likelihood evaluation. it USED-FOR likelihood evaluation. inversion CONJUNCTION sampling. sampling CONJUNCTION inversion. it USED-FOR sampling. it USED-FOR inversion. inference tasks USED-FOR inverse problems. inference tasks EVALUATE-FOR method. approach COMPARE MCMC baselines. MCMC baselines COMPARE approach. sample quality EVALUATE-FOR MCMC baselines. sample quality EVALUATE-FOR approach. Generic is model. OtherScientificTerm is approximate posterior. ,This paper proposes a new framework for approximate probabilistic inference based on a normalizing flow model. The framework is based on variational inference. The authors show that the proposed framework can be used to approximate the posterior of a generative model. They also show that it can be applied to a number of inverse problems. ,This paper proposes a new framework for approximate probabilistic inference based on a normalizing flow model. The framework is based on variational inference. The authors show that the proposed framework can be used to approximate the posterior of a generative model. They also show that it can be applied to a number of inverse problems. 
1497,SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,Computer vision technology USED-FOR biological and medical data analysis and understanding. Ultra - high Resolution Image Segmentation dataset USED-FOR Cell membrane. iterative annotations CONJUNCTION uncompressed high - resolution raw data. uncompressed high - resolution raw data CONJUNCTION iterative annotations. annotated Electron Microscopy ( EM ) dataset USED-FOR Cell membrane. U - RISC HYPONYM-OF annotated Electron Microscopy ( EM ) dataset. U - RISC HYPONYM-OF Ultra - high Resolution Image Segmentation dataset. segmentation evaluation criteria COMPARE human perception. human perception COMPARE segmentation evaluation criteria. evaluation criterion USED-FOR cell membrane segmentation. Perceptual Hausdorff Distance ( PHD ) HYPONYM-OF evaluation criterion. evaluation criteria CONJUNCTION PHD. PHD CONJUNCTION evaluation criteria. segmentation methods CONJUNCTION iterative manual annotation. iterative manual annotation CONJUNCTION segmentation methods. evaluation criteria USED-FOR iterative manual annotation. ,This paper proposes a new evaluation criterion for cell membrane segmentation based on the Perceptual Hausdorff Distance (PHD) for the U-RISC dataset. The proposed evaluation criterion is based on two criteria: (1) an iterative manual annotation and (2) an evaluation criterion based on human perception. The paper shows that the proposed evaluation criteria can be used to evaluate the performance of different segmentation methods. The evaluation criteria are evaluated on the Cell Membrane Segmentation dataset.,This paper proposes a new evaluation criterion for cell membrane segmentation based on the Perceptual Hausdorff Distance (PHD) for the U-RISC dataset. The proposed evaluation criterion is based on two criteria: (1) an iterative manual annotation and (2) an evaluation criterion based on human perception. The paper shows that the proposed evaluation criteria can be used to evaluate the performance of different segmentation methods. The evaluation criteria are evaluated on the Cell Membrane Segmentation dataset.
1513,SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"Continual Learning ( CL ) USED-FOR catastrophic forgetting. benchmarks USED-FOR CL algorithms. benchmarks USED-FOR forgetting. short streams of tasks USED-FOR benchmarks. short streams of tasks USED-FOR forgetting. modules USED-FOR atomic skills. modules PART-OF modular architecture. learning algorithm USED-FOR learning. exponential search space FEATURE-OF task - driven prior. task - driven prior USED-FOR learning algorithm. modular architecture CONJUNCTION learning algorithm. learning algorithm CONJUNCTION modular architecture. benchmarks EVALUATE-FOR learning algorithm. CL benchmarks EVALUATE-FOR modular architecture. CL benchmarks EVALUATE-FOR learning algorithm. Method is CL system. Generic are task, and Benchmark. ","This paper proposes a new learning algorithm for continual learning (CL). The proposed method is based on a modular architecture, which consists of modules that can be used to learn atomic skills. The authors show that the proposed method outperforms the state-of-the-art methods on a number of benchmarks. ","This paper proposes a new learning algorithm for continual learning (CL). The proposed method is based on a modular architecture, which consists of modules that can be used to learn atomic skills. The authors show that the proposed method outperforms the state-of-the-art methods on a number of benchmarks. "
1529,SP:cc819c61f408e88f247eb87946187ccec3dad32e,random selection CONJUNCTION clustering and/or augmentation. clustering and/or augmentation CONJUNCTION random selection. unsupervised meta - learning approaches USED-FOR synthetic meta - tasks. techniques USED-FOR synthetic meta - tasks. clustering and/or augmentation HYPONYM-OF techniques. random selection HYPONYM-OF techniques. approach USED-FOR metatasks. generative models USED-FOR approach. generative models USED-FOR metatasks. algorithms USED-FOR synthetic classes. synthetic classes PART-OF meta - task. approach COMPARE unsupervised learning baselines. unsupervised learning baselines COMPARE approach. benchmark datasets EVALUATE-FOR few - shot classification tasks. few - shot classification tasks EVALUATE-FOR unsupervised learning baselines. few - shot classification tasks EVALUATE-FOR approach. benchmark datasets EVALUATE-FOR unsupervised learning baselines. OtherScientificTerm is latent space. ,"This paper proposes a new unsupervised meta-learning approach for synthetic meta-tasks. The proposed approach is based on the idea that the latent space of a meta-task can be represented as a generative model, which can be used to learn a set of synthetic classes that can be applied to a given task. The method is evaluated on a variety of synthetic tasks, including few-shot classification, clustering and/or augmentation. ","This paper proposes a new unsupervised meta-learning approach for synthetic meta-tasks. The proposed approach is based on the idea that the latent space of a meta-task can be represented as a generative model, which can be used to learn a set of synthetic classes that can be applied to a given task. The method is evaluated on a variety of synthetic tasks, including few-shot classification, clustering and/or augmentation. "
1545,SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"inverse problems CONJUNCTION compressed sensing. compressed sensing CONJUNCTION inverse problems. it USED-FOR inference. Injectivity USED-FOR generative models. generative priors USED-FOR compressed sensing. injectivity FEATURE-OF fullyconnected and convolutional ReLU layers and networks. weight matrices USED-FOR injectivity. expansivity USED-FOR global injectivity. iid Gaussian matrices USED-FOR global injectivity. worst - case Lipschitz constants USED-FOR stability. arguments USED-FOR deep networks. differential topology USED-FOR arguments. injective ReLU network USED-FOR Lipschitz map. argument USED-FOR injectivity. random projections USED-FOR argument. neural networks USED-FOR nonlinear inverse and inference problems. OtherScientificTerm is well posedness. Method are layerwise analysis, tractable model, and injective network. ",This paper studies injectivity of ReLU layers and convolutional ReLU networks in the context of inverse and inference problems. Injectivity is defined as the difference between the worst-case Lipschitz constants of the injectivity and the global injectivity. The authors show that injectivity can be expressed as a function of the weight matrices of the weights of the ReLU layer and the weight matrix of the convolution layer.  The authors also show that injecting injectivity is a generalization of injectivity in generative models.  ,This paper studies injectivity of ReLU layers and convolutional ReLU networks in the context of inverse and inference problems. Injectivity is defined as the difference between the worst-case Lipschitz constants of the injectivity and the global injectivity. The authors show that injectivity can be expressed as a function of the weight matrices of the weights of the ReLU layer and the weight matrix of the convolution layer.  The authors also show that injecting injectivity is a generalization of injectivity in generative models.  
1561,SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"generative model USED-FOR image generation. continuous conditional generative adversarial network ( CcGAN ) HYPONYM-OF generative model. conditional GANs ( cGANs ) USED-FOR categorical conditions. class labels HYPONYM-OF categorical conditions. hidden map CONJUNCTION one - hot encoded label. one - hot encoded label CONJUNCTION hidden map. hidden map PART-OF generator / discriminator. one - hot encoded label HYPONYM-OF label input methods. hidden map HYPONYM-OF label input methods. empirical cGAN losses HYPONYM-OF cGAN losses. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. empirical cGAN losses USED-FOR continuous scenario. regression labels PART-OF discriminator. regression labels PART-OF generator. method USED-FOR regression labels. method USED-FOR generator. method USED-FOR discriminator. empirical cGAN losses USED-FOR CcGAN. hard vicinal discriminator loss ( HVDL ) CONJUNCTION soft vicinal discriminator loss ( SVDL ). soft vicinal discriminator loss ( SVDL ) CONJUNCTION hard vicinal discriminator loss ( HVDL ). empirical generator loss HYPONYM-OF empirical discriminator losses. soft vicinal discriminator loss ( SVDL ) HYPONYM-OF empirical discriminator losses. hard vicinal discriminator loss ( HVDL ) HYPONYM-OF empirical discriminator losses. HVDL CONJUNCTION SVDL. SVDL CONJUNCTION HVDL. error bounds FEATURE-OF discriminator. SVDL USED-FOR discriminator. HVDL USED-FOR discriminator. benchmark dataset USED-FOR generative image modeling. RC-49 USED-FOR generative image modeling. RC-49 HYPONYM-OF benchmark dataset. RC-49 CONJUNCTION UTKFace datasets. UTKFace datasets CONJUNCTION RC-49. Circular 2 - D Gaussians CONJUNCTION RC-49. RC-49 CONJUNCTION Circular 2 - D Gaussians. regression label FEATURE-OF image distribution. CcGAN COMPARE cGAN. cGAN COMPARE CcGAN. OtherScientificTerm are continuous, scal",This paper proposes a continuous conditional generative adversarial network (ccGAN) for categorical classification. CcGAN is a generative model that can be seen as a combination of a classifier and a discriminator. The proposed method is based on empirical cGAN losses. The authors show that the proposed method outperforms the state-of-the-art in terms of error bounds. ,This paper proposes a continuous conditional generative adversarial network (ccGAN) for categorical classification. CcGAN is a generative model that can be seen as a combination of a classifier and a discriminator. The proposed method is based on empirical cGAN losses. The authors show that the proposed method outperforms the state-of-the-art in terms of error bounds. 
1577,SP:10dd09ab315870631d1451d200f2c87a023f8226,"sample complexity EVALUATE-FOR deep learning ( DL ). Semisupervised learning ( SSL ) USED-FOR task. unlabeled instances USED-FOR Semisupervised learning ( SSL ). unlabeled instances USED-FOR task. sample complexity EVALUATE-FOR Active learning ( AL ). SSL CONJUNCTION AL. AL CONJUNCTION SSL. SSL USED-FOR fully - supervised learning ( SL ). AL USED-FOR fully - supervised learning ( SL ). labeled samples USED-FOR fully - supervised learning ( SL ). SSL USED-FOR DL - based AL algorithms. annotation efficiency EVALUATE-FOR AL algorithms. diversity FEATURE-OF AL algorithms. SSL USED-FOR AL algorithms. AL algorithm USED-FOR classification network. AL algorithm USED-FOR convergence rate. convergence rate FEATURE-OF classification network. CRC CONJUNCTION SSL algorithm. SSL algorithm CONJUNCTION CRC. deep neural network COMPARE SL. SL COMPARE deep neural network. labeled samples USED-FOR deep neural network. SSL algorithm USED-FOR deep neural network. CRC USED-FOR deep neural network. AL CONJUNCTION SSL. SSL CONJUNCTION AL. our method USED-FOR ASSL. OtherScientificTerm is human - in - the - loop. Method are pool - based AL, convergence rate control ( CRC ), and AL and SSL ( ASSL ) algorithms. Metric is rate of convergence. Generic is method. ","This paper studies the sample complexity of active learning (AL) and semi-supervised learning (SSL) algorithms for deep learning. The authors show that AL and SSL algorithms converge to the same rate of convergence, and propose a method to control the convergence rate of the two algorithms. They show that the proposed method can be used to improve the sample efficiency of AL algorithms. ","This paper studies the sample complexity of active learning (AL) and semi-supervised learning (SSL) algorithms for deep learning. The authors show that AL and SSL algorithms converge to the same rate of convergence, and propose a method to control the convergence rate of the two algorithms. They show that the proposed method can be used to improve the sample efficiency of AL algorithms. "
1593,SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"federated learning method USED-FOR distributively training neural network models. devices USED-FOR parallelizing gradient computation. scheme USED-FOR training. devices CONJUNCTION partial participation and unbalanced data. partial participation and unbalanced data CONJUNCTION devices. scheme USED-FOR convex and non - convex settings. Task are Federated Learning problem, and device level computations. OtherScientificTerm are local - device level empirical loss, global empirical loss, and device heterogeneity. Method are inexact minimization, and dynamic regularizer. Material is real and synthetic data. ","This paper studies the problem of federated learning, which is an important problem in machine learning. The authors propose a dynamic regularizer to mitigate the issue of inexact minimization. The proposed method is evaluated on synthetic and real-world datasets.","This paper studies the problem of federated learning, which is an important problem in machine learning. The authors propose a dynamic regularizer to mitigate the issue of inexact minimization. The proposed method is evaluated on synthetic and real-world datasets."
1609,SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"supervised counterparts USED-FOR computer vision tasks. representations COMPARE supervised counterparts. supervised counterparts COMPARE representations. representations USED-FOR computer vision tasks. self - supervised approaches USED-FOR representations. accuracy EVALUATE-FOR contrastive learning algorithms. contrastive learning USED-FOR similarity ( dissimilarity ). similarity FEATURE-OF intermediate layers. similarity USED-FOR similarity. intermediate losses USED-FOR selection. SimCLR CONJUNCTION SwAV. SwAV CONJUNCTION SimCLR. MOCO CONJUNCTION SimCLR. SimCLR CONJUNCTION MOCO. method USED-FOR MOCO. method USED-FOR SimCLR. method USED-FOR SwAV. ImageNet linear classification CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet linear classification. Method are self - supervised methods, and back - propagation. Task is optimizing similarity ( dissimilarity ). OtherScientificTerm are intermediate contrastive losses, and gradient descent update. Metric is computational cost. ","This paper proposes a method to improve the performance of self-supervised contrastive learning algorithms by optimizing the similarity between the intermediate layers of the network. The authors propose a new loss function, called intermediate contrastive losses, which is based on the idea of back propagation. They show that the proposed method outperforms the baselines in terms of accuracy and computational cost. They also show that their method is computationally efficient. ","This paper proposes a method to improve the performance of self-supervised contrastive learning algorithms by optimizing the similarity between the intermediate layers of the network. The authors propose a new loss function, called intermediate contrastive losses, which is based on the idea of back propagation. They show that the proposed method outperforms the baselines in terms of accuracy and computational cost. They also show that their method is computationally efficient. "
1625,SP:5b5e705ea1ee1b857e17e64d560a39052804949d,actor - critic HYPONYM-OF reinforcement learning algorithms. global convergence CONJUNCTION global optimality. global optimality CONJUNCTION global convergence. global optimality FEATURE-OF actor - critic. global convergence FEATURE-OF actor - critic. bi - level or two - timescale updates USED-FOR actor - critic. policy gradient direction USED-FOR actor. critic USED-FOR policy gradient direction. Bellman evaluation operator USED-FOR critic update. linear or deep neural networks USED-FOR actor. linear or deep neural networks USED-FOR function approximation settings. rate of convergence CONJUNCTION global optimality. global optimality CONJUNCTION rate of convergence. global optimality FEATURE-OF single - timescale actor - critic. rate of convergence FEATURE-OF single - timescale actor - critic. linear function approximation USED-FOR single - timescale actor - critic. actorcritic with deep neural network USED-FOR globally optimal policy. nonlinear function approximation USED-FOR policy optimization. Task is single - timescale setting. Metric is sublinear O(K−1/2 ) rate. OtherScientificTerm is sublinear rate. ,"This paper studies the global optimality of actor-critic with linear or deep neural networks. The authors show that the rate of convergence is sublinear O(K−1/2) in the single-timescale setting. They also show that in the bi-level or two-timecale setting, the global convergence is global O(1/3/4) and O(O(K+1/4/4)) in the multi-scale setting.","This paper studies the global optimality of actor-critic with linear or deep neural networks. The authors show that the rate of convergence is sublinear O(K−1/2) in the single-timescale setting. They also show that in the bi-level or two-timecale setting, the global convergence is global O(1/3/4) and O(O(K+1/4/4)) in the multi-scale setting."
1641,SP:26705a4dc305cce336f657c5937d1f5b4209548a,events CONJUNCTION messages. messages CONJUNCTION events. messages CONJUNCTION transactions. transactions CONJUNCTION messages. Log files USED-FOR events. Log files USED-FOR messages. Log files USED-FOR transactions. computer systems FEATURE-OF Log files. they USED-FOR structured textual and numerical data. natural languages CONJUNCTION temporal signals. temporal signals CONJUNCTION natural languages. logs USED-FOR sequential forms of data. natural languages HYPONYM-OF sequential forms of data. temporal signals HYPONYM-OF sequential forms of data. log level CONJUNCTION log sequence level. log sequence level CONJUNCTION log level. field level CONJUNCTION log level. log level CONJUNCTION field level. representation USED-FOR level. vector format FEATURE-OF representations. Transformer Networks ( TNs ) USED-FOR numerical and textual information. Transformer Networks ( TNs ) USED-FOR log embeddings. representation USED-FOR log processing applications. Material is Logs. ,"This paper studies the representation of log embeddings in Transformer Networks (TNs). The authors propose a vector format for log embedding, which can be used to represent both numerical and textual information. The authors show that the proposed representation is able to capture both the log level and the field level of the input data. They also show that this representation can be applied to log processing applications. ","This paper studies the representation of log embeddings in Transformer Networks (TNs). The authors propose a vector format for log embedding, which can be used to represent both numerical and textual information. The authors show that the proposed representation is able to capture both the log level and the field level of the input data. They also show that this representation can be applied to log processing applications. "
1657,SP:165c51a16f17fb8726e968f8b34742b62011d60e,"CNN kernels CONJUNCTION oriented Gabor filters. oriented Gabor filters CONJUNCTION CNN kernels. freely - trained mixture weights USED-FOR wavelet packet decompositions. AlexNet architecture USED-FOR image classification. AlexNet architecture FEATURE-OF wavelet decompositions. wavelet decompositions USED-FOR approach. directional selectivity CONJUNCTION shift invariance. shift invariance CONJUNCTION directional selectivity. feature extraction properties USED-FOR two. shift invariance HYPONYM-OF feature extraction properties. directional selectivity HYPONYM-OF feature extraction properties. separable wavelet packet transform USED-FOR variant. accuracy rate EVALUATE-FOR AlexNet. mathematical theory USED-FOR network. Task is deep convolutional neural networks ( CNNs ). Generic are formalism, and them. Method is convolutional layers. ","This paper studies the problem of learning wavelet decompositions for deep convolutional neural networks (CNNs). The authors propose a novel wavelet packet decomposition method, called AlexNet, which is a variant of wavelet-based CNNs. The proposed method is based on a theoretical analysis of the shift invariance and directional selectivity properties of CNN kernels and Gabor filters. The authors show that AlexNet is able to achieve better performance than existing wavelet based CNNs on image classification tasks. ","This paper studies the problem of learning wavelet decompositions for deep convolutional neural networks (CNNs). The authors propose a novel wavelet packet decomposition method, called AlexNet, which is a variant of wavelet-based CNNs. The proposed method is based on a theoretical analysis of the shift invariance and directional selectivity properties of CNN kernels and Gabor filters. The authors show that AlexNet is able to achieve better performance than existing wavelet based CNNs on image classification tasks. "
1673,SP:d0a284da462584724ba6a3a48c9e986d391233f6,"dynamic composition FEATURE-OF Coordinating teams. variational objective USED-FOR learning. attention mechanism USED-FOR dynamic team composition. attention mechanism USED-FOR heterogeneous agents. multi - agent particle environment FEATURE-OF resource collection tasks. resource collection tasks EVALUATE-FOR methods. zero - shot generalization USED-FOR team compositions. heterogeneous agents USED-FOR zero - shot generalization. coach USED-FOR dynamic teams. Task are real - world multi - agent teams, and real - world team sports. OtherScientificTerm is optimal team strategy. Method are coach - player framework, adaptive communication method, and adaptive communication strategy. Generic is method. ","This paper proposes a new adaptive communication method for multi-agent multi-task coordination. The proposed method is based on the idea of adaptive communication, which is a variational objective that is used to learn a team composition. The authors show that the proposed method outperforms existing adaptive communication methods on resource collection tasks. ","This paper proposes a new adaptive communication method for multi-agent multi-task coordination. The proposed method is based on the idea of adaptive communication, which is a variational objective that is used to learn a team composition. The authors show that the proposed method outperforms existing adaptive communication methods on resource collection tasks. "
1689,SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"machine learning interpretability CONJUNCTION uncertainty estimation. uncertainty estimation CONJUNCTION machine learning interpretability. Influence functions USED-FOR machine learning interpretability. Influence functions USED-FOR uncertainty estimation. gradients CONJUNCTION Hessian. Hessian CONJUNCTION gradients. Hessian FEATURE-OF model. gradients FEATURE-OF model. Hessian USED-FOR post - hoc method. gradients USED-FOR post - hoc method. influence functions USED-FOR linear models. Influence functions PART-OF deep learning. non - convex loss functions FEATURE-OF deep learning. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Iris CONJUNCTION MNIST. MNIST CONJUNCTION Iris. influence functions PART-OF neural network models. datasets USED-FOR neural network models. Iris HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. model parameterization CONJUNCTION regularization techniques. regularization techniques CONJUNCTION model parameterization. regularization techniques USED-FOR influence functions. influence functions EVALUATE-FOR network architecture. accuracy EVALUATE-FOR influence estimates. influence estimates EVALUATE-FOR shallow networks. influence estimation methods USED-FOR non - convex setups. influence functions PART-OF deep learning. OtherScientificTerm are test - time predictions, convexity of the underlying loss function, and model changes. Task is estimating group influences. Metric is accuracy of influence functions. Generic is deeper networks. Method are network architectures, and weight - decay regularization. ",This paper studies the influence function estimation problem in deep learning. The authors propose a post-hoc method to estimate influence functions for non-convex loss functions. The main contribution of the paper is the use of the Hessian of the underlying loss function as a regularization term to improve the performance of the proposed method.  The authors show that the proposed approach can be used to estimate the influence functions of deep learning models. ,This paper studies the influence function estimation problem in deep learning. The authors propose a post-hoc method to estimate influence functions for non-convex loss functions. The main contribution of the paper is the use of the Hessian of the underlying loss function as a regularization term to improve the performance of the proposed method.  The authors show that the proposed approach can be used to estimate the influence functions of deep learning models. 
1705,SP:5fea74a2031d097a99dacf613bedcb054b0c3831,Autoregressive language models USED-FOR downstream tasks. Autoregressive language models USED-FOR next word prediction. large text corpora USED-FOR next word prediction. large text corpora USED-FOR Autoregressive language models. zero - shot usage USED-FOR downstream tasks. next word prediction CONJUNCTION text classification. text classification CONJUNCTION next word prediction. language modeling HYPONYM-OF pretraining task. sentence completion tasks USED-FOR classification tasks of interest. language modeling USED-FOR downstream tasks. language models USED-FOR classification tasks. language models USED-FOR features. features USED-FOR classification tasks. crossentropy ( log - perplexity ) FEATURE-OF language models. objective function USED-FOR classification tasks. ,"This paper proposes a new pretraining task for autoregressive language models. The authors propose a new task called zero-shot usage, which is designed to test the performance of language models on sentence completion tasks and next word prediction tasks. The task consists of two tasks: (1) sentence completion task, and (2) sentence classification task, where the task is to predict the next word in a given sentence. The proposed task is based on the idea of cross-entropy (log-perplexity), which is a generalization of the log-properity (log perplexity) to the task of sentence classification.  The authors show that the proposed task can be used to train a language model that can be applied to a variety of downstream tasks, including next word predictions, sentence classification, and zero-shots usage. The paper also shows that the model can be trained to perform well on the zero shot usage task. ","This paper proposes a new pretraining task for autoregressive language models. The authors propose a new task called zero-shot usage, which is designed to test the performance of language models on sentence completion tasks and next word prediction tasks. The task consists of two tasks: (1) sentence completion task, and (2) sentence classification task, where the task is to predict the next word in a given sentence. The proposed task is based on the idea of cross-entropy (log-perplexity), which is a generalization of the log-properity (log perplexity) to the task of sentence classification.  The authors show that the proposed task can be used to train a language model that can be applied to a variety of downstream tasks, including next word predictions, sentence classification, and zero-shots usage. The paper also shows that the model can be trained to perform well on the zero shot usage task. "
1721,SP:a67da438e9821010284416170c3699ae7ff96c99,"MIA approaches USED-FOR classification models. image translation HYPONYM-OF conditional image generation models. approach USED-FOR membership attacks. reconstruction error USED-FOR approach. difficulty score CONJUNCTION reconstruction error. reconstruction error CONJUNCTION difficulty score. difficulty score USED-FOR membership error. MIA accuracy EVALUATE-FOR membership error. Task is Membership inference attacks ( MIA ). Method are neural network model, machine learning, and MIA. OtherScientificTerm are overfitting, and Reconstruction error. Metric is reconstruction errors. Material is training set. ","This paper studies the membership inference attacks (MIA) problem in machine learning. The authors propose a new membership inference attack method based on the reconstruction error and difficulty score. The reconstruction error is defined as the difference between the membership error and the difficulty score between the training set and the test set. The difficulty score is used as a measure of the difficulty of the membership attack. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet.","This paper studies the membership inference attacks (MIA) problem in machine learning. The authors propose a new membership inference attack method based on the reconstruction error and difficulty score. The reconstruction error is defined as the difference between the membership error and the difficulty score between the training set and the test set. The difficulty score is used as a measure of the difficulty of the membership attack. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
1737,SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"it USED-FOR distribution learning problem. differentiable architecture search method USED-FOR distribution learning problem. random variables FEATURE-OF continuously relaxed architecture mixing weight. Dirichlet distribution USED-FOR random variables. pathwise derivatives USED-FOR Dirichlet parameters. gradient - based optimizer USED-FOR Dirichlet parameters. formulation USED-FOR stochasticity. generalization ability EVALUATE-FOR formulation. progressive learning scheme USED-FOR searching. searching USED-FOR large - scale tasks. progressive learning scheme USED-FOR large - scale tasks. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method are differentiable NAS, and neural architecture search algorithms. Generic are method, and datasets. Metric is test error. Material is NASBench-201. ","This paper proposes a differentiable architecture search method for differentiable NAS. The main idea is to use a continuous relaxed architecture mixing weight to approximate the Dirichlet distribution of random variables. The authors propose a gradient-based optimizer to find the optimal solution to the optimization problem. The proposed method is evaluated on NASBench-201, CIFAR-10, and ImageNet.","This paper proposes a differentiable architecture search method for differentiable NAS. The main idea is to use a continuous relaxed architecture mixing weight to approximate the Dirichlet distribution of random variables. The authors propose a gradient-based optimizer to find the optimal solution to the optimization problem. The proposed method is evaluated on NASBench-201, CIFAR-10, and ImageNet."
1753,SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"signed distance functions CONJUNCTION neural radiance fields. neural radiance fields CONJUNCTION signed distance functions. function approximators USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR function approximators. high dimensional inputs USED-FOR deep networks. pixel coordinates USED-FOR images. sinusoidal nonlinearities CONJUNCTION Fourier features. Fourier features CONJUNCTION sinusoidal nonlinearities. elements USED-FOR positional encodings. Fourier features USED-FOR positional encodings. elements COMPARE ReLU networks. ReLU networks COMPARE elements. positional encodings COMPARE ReLU networks. ReLU networks COMPARE positional encodings. Fourier features HYPONYM-OF elements. sinusoidal nonlinearities HYPONYM-OF elements. function approximators USED-FOR problems. multiplicative filter networks HYPONYM-OF problems. multiplicative filter networks HYPONYM-OF function approximators. Fourier or Gabor basis functions USED-FOR linear function approximator. ReLU networks CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION ReLU networks. Fourier features CONJUNCTION ReLU networks. ReLU networks CONJUNCTION Fourier features. multiplicative filter networks COMPARE approaches. approaches COMPARE multiplicative filter networks. Fourier features CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION Fourier features. sinusoidal activation networks USED-FOR approaches. ReLU networks USED-FOR approaches. Fourier features USED-FOR approaches. OtherScientificTerm are differential equations, and compositional depth. Generic are networks, and representation. ",This paper studies the problem of learning function approximators for low-dimensional functions. The authors propose to use convolutional neural networks (ReLU networks) to learn functions that can be approximated by Fourier or Gabor basis functions. They show that ReLU networks can be used to approximate functions that are non-convex and non-asymptotic. They also show that Fourier features and sinusoidal nonlinearities are useful for learning functions that have compositional depth. ,This paper studies the problem of learning function approximators for low-dimensional functions. The authors propose to use convolutional neural networks (ReLU networks) to learn functions that can be approximated by Fourier or Gabor basis functions. They show that ReLU networks can be used to approximate functions that are non-convex and non-asymptotic. They also show that Fourier features and sinusoidal nonlinearities are useful for learning functions that have compositional depth. 
1769,SP:f5be855300f63c185a006834302bd4b033b56258,"task - specific models CONJUNCTION meta - model. meta - model CONJUNCTION task - specific models. task - specific models PART-OF Gradient - based meta - learning. gradients USED-FOR meta - model. algorithm USED-FOR task - specific models. algorithm USED-FOR meta - model. meta - gradients USED-FOR meta - model. inner loop USED-FOR algorithm. inner loop USED-FOR task - specific models. teacherstudent scheme USED-FOR gradient - based meta - learning algorithms. student network USED-FOR task - specific models. lightweight computation graph USED-FOR meta - gradients. few - shot learning CONJUNCTION long - tailed classification. long - tailed classification CONJUNCTION few - shot learning. long - tailed classification CONJUNCTION meta - attack. meta - attack CONJUNCTION long - tailed classification. it USED-FOR meta - learning algorithms. it USED-FOR tasks. tasks EVALUATE-FOR meta - learning algorithms. meta - attack HYPONYM-OF tasks. meta - attack HYPONYM-OF meta - learning algorithms. few - shot learning HYPONYM-OF tasks. few - shot learning HYPONYM-OF meta - learning algorithms. long - tailed classification HYPONYM-OF tasks. long - tailed classification HYPONYM-OF meta - learning algorithms. Generic are loop, and approach. OtherScientificTerm are inner - loop optimization steps, high - order derivatives, and big memory footprints. ","This paper proposes a teacher-student scheme for meta-learning. The teacher student scheme is based on the idea that the meta-gradient of a task-specific model can be decomposed into two parts: (1) meta-gradients of the task specific model, and (2) meta gradients of a meta-model of the target task. The proposed method is evaluated on few-shot learning, meta-attack, and long-tailed classification tasks. ","This paper proposes a teacher-student scheme for meta-learning. The teacher student scheme is based on the idea that the meta-gradient of a task-specific model can be decomposed into two parts: (1) meta-gradients of the task specific model, and (2) meta gradients of a meta-model of the target task. The proposed method is evaluated on few-shot learning, meta-attack, and long-tailed classification tasks. "
1785,SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"Offline Reinforcement Learning ( RL ) USED-FOR policies. off - policy RL algorithms USED-FOR Offline RL. Behavior regularization USED-FOR off - policy algorithms. analytical upper bound USED-FOR behavior regularizor. analytical upper bound USED-FOR KL divergence. state - dependent Lagrange multipliers USED-FOR regularization term. state - dependent Lagrange multipliers USED-FOR distributing KL divergence penalty. Lagrange multipliers USED-FOR freedom of deviation. gradient penalty term USED-FOR gradient of the Q value. gradient penalty term USED-FOR policy evaluation objective. out - of - distribution actions FEATURE-OF gradient of the Q value. gradient penalty term USED-FOR catastrophic performance degradation. out - ofdistribution actions FEATURE-OF Q values. offline RL benchmarks EVALUATE-FOR BRAC+. offline RL benchmarks EVALUATE-FOR model - free and model - based approaches. BRAC+ COMPARE model - free and model - based approaches. model - free and model - based approaches COMPARE BRAC+. Method are Reinforcement Learning agent, and behavior regularized offline reinforcement learning. OtherScientificTerm are outof - distribution ( less explored ) actions, sample based estimations, sampled batch, low probability ( less explored ) states, and rare out - of - distribution actions. ",This paper proposes a behavior regularization method for offline reinforcement learning. The main idea is to use a Lagrange multipliers to reduce the variance of the KL divergence between the Q value of the policy and the out-of-distribution actions. The authors show that the proposed method outperforms the baselines on several offline RL benchmarks. ,This paper proposes a behavior regularization method for offline reinforcement learning. The main idea is to use a Lagrange multipliers to reduce the variance of the KL divergence between the Q value of the policy and the out-of-distribution actions. The authors show that the proposed method outperforms the baselines on several offline RL benchmarks. 
1801,SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"Smaller networks USED-FOR edge - devices. one - shot learning paradigm USED-FOR networks. regularization behavior FEATURE-OF adjoint training paradigm. Imagenet USED-FOR resnet-50. CIFAR-100 EVALUATE-FOR architecture. datasets EVALUATE-FOR network. network COMPARE network. network COMPARE network. datasets EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. Task is compressing deep neural networks. Method are deep neural networks, Adjoined networks, CNN - based neural architecture, and adjoint networks. Generic is architectures. Metric are inference time, and accuracy. ","This paper studies the problem of compressing deep neural networks in the context of edge-device inference. The authors propose a new architecture called Resnet-50, which is a CNN-based neural architecture that compresses the training data and the inference time. They show that ResNet-50 outperforms the state-of-the-art on CIFAR-100 and Imagenet datasets. They also show that their architecture is able to improve the performance of ResNet in terms of top-1 accuracy.","This paper studies the problem of compressing deep neural networks in the context of edge-device inference. The authors propose a new architecture called Resnet-50, which is a CNN-based neural architecture that compresses the training data and the inference time. They show that ResNet-50 outperforms the state-of-the-art on CIFAR-100 and Imagenet datasets. They also show that their architecture is able to improve the performance of ResNet in terms of top-1 accuracy."
1817,SP:dba40073f79143e5355d194aa16db9eee0267a5d,"exploration USED-FOR reinforcement learning ( RL ). exploration methods COMPARE counterparts. counterparts COMPARE exploration methods. -greedy HYPONYM-OF counterparts. -greedy USED-FOR exploration algorithm. duration distributions USED-FOR exploration. ecological models of animal foraging behaviour USED-FOR distributions. Generic is problem. Metric are complexity, and generality. OtherScientificTerm are dithering, temporal persistence, local optima, and random duration. Method is greedy exploration. ","This paper studies the problem of exploration in reinforcement learning. The authors propose a greedy exploration algorithm, which is an extension of the greedy exploration method for reinforcement learning (RL). The main contribution of the paper is to propose a new algorithm for greedy exploration. The proposed algorithm is based on the idea of dithering, and the authors show that it can be used to solve the dithering problem. The paper also shows that the proposed algorithm can be combined with existing greedy exploration algorithms. ","This paper studies the problem of exploration in reinforcement learning. The authors propose a greedy exploration algorithm, which is an extension of the greedy exploration method for reinforcement learning (RL). The main contribution of the paper is to propose a new algorithm for greedy exploration. The proposed algorithm is based on the idea of dithering, and the authors show that it can be used to solve the dithering problem. The paper also shows that the proposed algorithm can be combined with existing greedy exploration algorithms. "
1833,SP:5efb581a368ace3bd085d48801a899559d6a43ef,"Matrix factorization USED-FOR implicit regularization of gradient descent. infinitesimal initialization USED-FOR Gradient Flow. gradient flow COMPARE heuristic rank minimization algorithm. heuristic rank minimization algorithm COMPARE gradient flow. infinitesimal initialization USED-FOR gradient flow. Greedy Low - Rank Learning HYPONYM-OF heuristic rank minimization algorithm. gradient flow USED-FOR depth-2 matrix factorization. OtherScientificTerm are nuclear norm, implicit regularization, and initialization magnitude. Method are norm minimization, rank minimization view, and rank minimization. Generic is convergence. ","This paper studies the implicit regularization of gradient descent in the context of matrix factorization. In particular, the authors consider the case where the norm minimization problem is formulated as a heuristic optimization problem, and the authors show that the gradient flow algorithm is equivalent to the heuristic rank minimization algorithm. The authors also show that gradient flow converges to the nuclear norm in the case of infinite initialization. ","This paper studies the implicit regularization of gradient descent in the context of matrix factorization. In particular, the authors consider the case where the norm minimization problem is formulated as a heuristic optimization problem, and the authors show that the gradient flow algorithm is equivalent to the heuristic rank minimization algorithm. The authors also show that gradient flow converges to the nuclear norm in the case of infinite initialization. "
1849,SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"Classifiers PART-OF machine learning. two - stage framework USED-FOR robustness. data augmentations USED-FOR subgroup features. data augmentations USED-FOR classifier. CycleGAN USED-FOR intra - class, inter - subgroup augmentations. theoretically - motivated subgroup consistency regularizer CONJUNCTION robust objective. robust objective CONJUNCTION theoretically - motivated subgroup consistency regularizer. CycleGAN USED-FOR CAMEL. CAMEL USED-FOR model patching. robust error EVALUATE-FOR baseline. CAMEL COMPARE baseline. baseline COMPARE CAMEL. robust error EVALUATE-FOR CAMEL. benchmark datasets EVALUATE-FOR CAMEL. CAMEL USED-FOR model. Generic is models. Task is skin cancer classification. OtherScientificTerm are spurious bandage, subgroup differences, class information, semantic transformations, and spurious features. Method is Model patching. Material is real - world skin cancer dataset. ","This paper proposes a two-stage framework for model patching. The first stage is based on CycleGAN. The second stage is called CAMEL. CAMEL is an extension of CycleGAN, which is a regularizer for intra-class, inter-class and subgroup augmentations. Experiments show that CAMEL outperforms CycleGAN on two benchmark datasets.","This paper proposes a two-stage framework for model patching. The first stage is based on CycleGAN. The second stage is called CAMEL. CAMEL is an extension of CycleGAN, which is a regularizer for intra-class, inter-class and subgroup augmentations. Experiments show that CAMEL outperforms CycleGAN on two benchmark datasets."
1865,SP:de6cea1e35a0555175e17546a93422e9a96a511e,transparent inner structures CONJUNCTION model expressivity. model expressivity CONJUNCTION transparent inner structures. model interpretability FEATURE-OF transparent inner structures. decision trees HYPONYM-OF Rule - based models. large data sets EVALUATE-FOR rule - based models. Ensemble methods CONJUNCTION fuzzy / soft rules. fuzzy / soft rules CONJUNCTION Ensemble methods. interpretable nonfuzzy rules USED-FOR data representation. classifier USED-FOR interpretable nonfuzzy rules. Rulebased Representation Learner ( RRL ) HYPONYM-OF classifier. it USED-FOR continuous space. training method USED-FOR discrete model. Gradient Grafting HYPONYM-OF training method. gradient descent USED-FOR training method. gradient descent USED-FOR discrete model. logical activation functions USED-FOR RRL. scalability EVALUATE-FOR RRL. it USED-FOR continuous features. logical activation functions USED-FOR it. RRL COMPARE approaches. approaches COMPARE RRL. small and 4 large data sets EVALUATE-FOR RRL. RRL COMPARE decision trees. decision trees COMPARE RRL. complexity EVALUATE-FOR decision trees. complexity EVALUATE-FOR RRL. OtherScientificTerm is discrete parameters and structures. Method is non - differentiable RRL. ,This paper proposes a non-differentiable representation learning method for rule-based models. The proposed method is based on the idea of learning a classifier that can learn interpretable non-fuzzy rules for discrete data. The authors propose to use gradient descent to train the classifier and then use gradient grafting to improve the performance of the model. The method is evaluated on small and large data sets. ,This paper proposes a non-differentiable representation learning method for rule-based models. The proposed method is based on the idea of learning a classifier that can learn interpretable non-fuzzy rules for discrete data. The authors propose to use gradient descent to train the classifier and then use gradient grafting to improve the performance of the model. The method is evaluated on small and large data sets. 
1881,SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"molecular property prediction HYPONYM-OF biochemical applications. models USED-FOR biochemical applications. molecular scaffolds CONJUNCTION protein families. protein families CONJUNCTION molecular scaffolds. natural environments USED-FOR tasks. complex descriptors USED-FOR natural environments. protein families HYPONYM-OF complex descriptors. molecular scaffolds HYPONYM-OF complex descriptors. regret minimization ( RGM ) algorithm USED-FOR structured environments. representation USED-FOR predictor. hindsight access FEATURE-OF held - out environments. representation USED-FOR RGM. invariant risk minimization ( IRM ) USED-FOR RGM. specialized domain perturbations USED-FOR structured extension. RGM COMPARE baselines. baselines COMPARE RGM. molecular property prediction CONJUNCTION protein homology and stability prediction. protein homology and stability prediction CONJUNCTION molecular property prediction. applications EVALUATE-FOR RGM. molecular property prediction EVALUATE-FOR method. applications EVALUATE-FOR method. protein homology and stability prediction HYPONYM-OF applications. molecular property prediction HYPONYM-OF applications. OtherScientificTerm are environments, simultaneous optimality condition, and complex environments. Metric is predictive regret. ","This paper proposes a regret minimization (RGM) algorithm for structured environments. The proposed method is based on the invariant risk minimisation (IRM) framework. The authors propose a structured extension of the IRM framework to the case of held-out environments, which is an important problem in the context of molecular property prediction. The method is evaluated on a number of synthetic and real-world datasets, and is shown to outperform the baselines. ","This paper proposes a regret minimization (RGM) algorithm for structured environments. The proposed method is based on the invariant risk minimisation (IRM) framework. The authors propose a structured extension of the IRM framework to the case of held-out environments, which is an important problem in the context of molecular property prediction. The method is evaluated on a number of synthetic and real-world datasets, and is shown to outperform the baselines. "
1897,SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,BERT USED-FOR NLP tasks. BERT USED-FOR text - vision BERT models. cross - modal attentions USED-FOR text - vision BERT models. text - vision BERT models USED-FOR language - vision tasks. text - image retrieval HYPONYM-OF language - vision tasks. cross - modal attentions USED-FOR textvision BERT models. cross - modal attentions USED-FOR textvision retrieval. textvision retrieval USED-FOR large - scale search. computation cost EVALUATE-FOR cross - modal attentions. cross - probe BERT HYPONYM-OF architecture. text and vision probes USED-FOR cross - modal attentions. text and vision probes USED-FOR It. It USED-FOR crossmodal attention. Generic is method. ,This paper proposes a new architecture for cross-modal BERT for text-vision BERT models. The proposed architecture is based on the cross-probe BERT architecture. The authors propose to use both text and vision probes to train the BERT model. The method is evaluated on a number of text-image retrieval tasks and is shown to outperform the baselines.,This paper proposes a new architecture for cross-modal BERT for text-vision BERT models. The proposed architecture is based on the cross-probe BERT architecture. The authors propose to use both text and vision probes to train the BERT model. The method is evaluated on a number of text-image retrieval tasks and is shown to outperform the baselines.
1913,SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,Actor USED-FOR Actor - Critic algorithms. FORK HYPONYM-OF Actor. forward - looking Actor HYPONYM-OF Actor. FORK PART-OF model - free ActorCritic algorithm. continuous state and action spaces FEATURE-OF Box2D and MuJoCo environments. FORK USED-FOR BipedalWalkerHardcore. GPU USED-FOR FORK. Generic is algorithms. ,"This paper proposes a model-free actor-critic algorithm for continuous state and action spaces. The proposed algorithm, called Forward-Looking Actor (FORK), is a forward-looking actor that can be used to improve the performance of Actor-Critic algorithms in Box2D and MuJoCo environments. For example, for BipedalWalkerHardcore, the proposed algorithm is able to achieve state-of-the-art performance on the GPU.","This paper proposes a model-free actor-critic algorithm for continuous state and action spaces. The proposed algorithm, called Forward-Looking Actor (FORK), is a forward-looking actor that can be used to improve the performance of Actor-Critic algorithms in Box2D and MuJoCo environments. For example, for BipedalWalkerHardcore, the proposed algorithm is able to achieve state-of-the-art performance on the GPU."
1929,SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"Federated learning USED-FOR global model. local models PART-OF global model. Bayesian inference perspective USED-FOR aggregation algorithm. FEDBE HYPONYM-OF aggregation algorithm. Bayesian model Ensemble USED-FOR them. Gaussian or Dirichlet distribution USED-FOR local models. Gaussian or Dirichlet distribution USED-FOR model distribution. FEDBE USED-FOR regularizing users ’ model training. Material is non - i.i.d. data. Method are global models, neural networks, aggregation method, and federated learning algorithm. Generic is it. ",This paper proposes a new aggregation method for federated learning. The proposed method is based on the Bayesian model ensemble (BME) framework. The authors show that the proposed method can be used to regularize users’ model training. The method is evaluated on a variety of datasets and shows that it outperforms the baselines.,This paper proposes a new aggregation method for federated learning. The proposed method is based on the Bayesian model ensemble (BME) framework. The authors show that the proposed method can be used to regularize users’ model training. The method is evaluated on a variety of datasets and shows that it outperforms the baselines.
1945,SP:3ac5f437fc349a33810d0645664d1c448528af74,,"This paper presents a study on the effect of the use of the word ""diversity"" as a measure of diversity in advertising. The authors show that the number of different types of diversity is increasing, and that this is due to an increase in the diversity of advertising. They also show that there is a growing trend of advertising that uses the word diversity as a metric to measure diversity. The paper also shows that there are a growing number of companies that use the term diversity as an indicator of diversity.","This paper presents a study on the effect of the use of the word ""diversity"" as a measure of diversity in advertising. The authors show that the number of different types of diversity is increasing, and that this is due to an increase in the diversity of advertising. They also show that there is a growing trend of advertising that uses the word diversity as a metric to measure diversity. The paper also shows that there are a growing number of companies that use the term diversity as an indicator of diversity."
1961,SP:efa2343ead47263a0d09e1c17f9aa044605b9650,settling time FEATURE-OF deep neural networks. priori upper bound FEATURE-OF deep neural networks. Lyapunov based analysis USED-FOR loss function. Lyapunov based analysis USED-FOR priori upper bound. settling time FEATURE-OF priori upper bound. control theory framework USED-FOR deep learning. deterministic control theoretic setting FEATURE-OF priori guarantees of finite - time convergence. tracking problem USED-FOR learning. control problem USED-FOR supervised learning framework. analytical formula USED-FOR finite - time upper bound. analytical formula USED-FOR settling time. settling time FEATURE-OF finite - time upper bound. input perturbations FEATURE-OF loss function. Method is priori finite time convergence analysis. Generic is network. OtherScientificTerm is control inputs. ,"This paper studies the priori convergence of deep neural networks in the deterministic control theory setting. The authors prove a priori upper bound for the settling time of a deep neural network. The upper bound is based on the Lyapunov-based analysis of the loss function of a neural network, and the authors show that it is guaranteed to converge to a finite-time upper bound. They also show that this upper bound can be extended to the tracking problem. ","This paper studies the priori convergence of deep neural networks in the deterministic control theory setting. The authors prove a priori upper bound for the settling time of a deep neural network. The upper bound is based on the Lyapunov-based analysis of the loss function of a neural network, and the authors show that it is guaranteed to converge to a finite-time upper bound. They also show that this upper bound can be extended to the tracking problem. "
1977,SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,Disentanglement of representations USED-FOR representations. incompressible - flow networks ( GIN ) USED-FOR latent variables. incompressible - flow networks ( GIN ) USED-FOR compact and disentangled representation. GIN USED-FOR informative latent variables selection. method USED-FOR informative latent variables selection. GIN USED-FOR method. mutual information USED-FOR informative latent variables. latent variables CONJUNCTION auxiliary variable. auxiliary variable CONJUNCTION latent variables. mutual information USED-FOR auxiliary variable. synthetic data EVALUATE-FOR method. outlier detection CONJUNCTION adversarial attack defence. adversarial attack defence CONJUNCTION outlier detection. classification CONJUNCTION outlier detection. outlier detection CONJUNCTION classification. downstream tasks EVALUATE-FOR method. synthetic and real data EVALUATE-FOR adversarial attack defence. classification EVALUATE-FOR method. adversarial attack defence HYPONYM-OF downstream tasks. classification HYPONYM-OF downstream tasks. outlier detection HYPONYM-OF downstream tasks. Task is machine learning. Method is nonlinear independent component analysis theory. ,This paper proposes a new method for learning disentangled representations of latent variables. The key idea is to use an incompressible flow network (GIN) to learn a compact representation of the latent variables and an auxiliary variable. The proposed method is based on the nonlinear independent component analysis theory. The authors show that the proposed method outperforms existing methods on synthetic and real-world datasets. ,This paper proposes a new method for learning disentangled representations of latent variables. The key idea is to use an incompressible flow network (GIN) to learn a compact representation of the latent variables and an auxiliary variable. The proposed method is based on the nonlinear independent component analysis theory. The authors show that the proposed method outperforms existing methods on synthetic and real-world datasets. 
1993,SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"Pooling PART-OF convolutional neural networks. pooling operations USED-FOR feature maps. feature maps HYPONYM-OF lossy process. LiftDownPool CONJUNCTION LiftUpPool. LiftUpPool CONJUNCTION LiftDownPool. LiftPool USED-FOR bidirectional pooling layers. Lifting Scheme USED-FOR signal processing. LiftUpPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF LiftPool. LiftUpPool HYPONYM-OF LiftPool. LiftDownPool USED-FOR feature map. LiftDownPool USED-FOR downsized sub - bands. pooling function PART-OF LiftDownPool. image classification and semantic segmentation EVALUATE-FOR methods. backbones USED-FOR image classification and semantic segmentation. backbones USED-FOR methods. input corruptions CONJUNCTION perturbations. perturbations CONJUNCTION input corruptions. input corruptions FEATURE-OF robustness. robustness EVALUATE-FOR LiftDownPool. OtherScientificTerm are receptive fields, input variations, and downscaled feature map. Generic is they. Task are downsampling, and image - to - image translation challenges. Method is up - pooling layer LiftUpPool. ","This paper proposes LiftUpPool, a bidirectional pooling layer for convolutional neural networks. The proposed method is based on the lifting scheme, which is used to reduce the size of the pooling operations. The authors show that the proposed method outperforms existing methods in terms of robustness to perturbations, input corruptions, and image-to-image translation. ","This paper proposes LiftUpPool, a bidirectional pooling layer for convolutional neural networks. The proposed method is based on the lifting scheme, which is used to reduce the size of the pooling operations. The authors show that the proposed method outperforms existing methods in terms of robustness to perturbations, input corruptions, and image-to-image translation. "
2009,SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"stable noise - shaping quantization scheme USED-FOR embedding method. fast linear transformation USED-FOR ` 1 norm. fast linear transformation USED-FOR Euclidean distances. ` 1 norm USED-FOR Euclidean distances. well - spread data EVALUATE-FOR method. time complexity EVALUATE-FOR method. space complexity EVALUATE-FOR method. continuous valued Johnson - Lindenstrauss embedding CONJUNCTION quantization error. quantization error CONJUNCTION continuous valued Johnson - Lindenstrauss embedding. polynomial decay FEATURE-OF quantization error. accuracy EVALUATE-FOR binary codes. natural images EVALUATE-FOR method. Material is high - dimensional dataset. OtherScientificTerm are binary sequences, T, sparse Gaussian random matrix, Hamming distance, Walsh - Hadamard matrix, and embedding dimension. Method is binary embedding methods. Task is embedding. Generic are approach, and it. ","This paper proposes a stable noise-shaping quantization scheme for binary embedding. The proposed method is based on the continuous valued Johnson-Lindenstrauss embedding and the quantization error. The authors show that the proposed method outperforms existing quantization schemes in terms of accuracy, time complexity, and space complexity. ","This paper proposes a stable noise-shaping quantization scheme for binary embedding. The proposed method is based on the continuous valued Johnson-Lindenstrauss embedding and the quantization error. The authors show that the proposed method outperforms existing quantization schemes in terms of accuracy, time complexity, and space complexity. "
2025,SP:f65e229bca3904095743e7a501b1083cc60f1e22,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. process USED-FOR ANNs. synaptic plasticity rules USED-FOR Gradient Descent ( GD ). rule parameters USED-FOR GD. GD USED-FOR rules. plasticity rules USED-FOR recurrent neural nets ( RNNs ). GD USED-FOR plasticity rules. rules USED-FOR MNIST / Fashion MNIST. synthetic data USED-FOR rules. process USED-FOR plasticity rules. adversarial perturbations FEATURE-OF tolerance. tolerance FEATURE-OF classifiers. plasticity rules USED-FOR classifiers. perceptron algorithm CONJUNCTION multiplicative weights method. multiplicative weights method CONJUNCTION perceptron algorithm. GD USED-FOR plasticity rule. GD USED-FOR perceptron algorithm. GD USED-FOR multiplicative weights method. GD USED-FOR learning rules. evolutionary time FEATURE-OF it. Task are learning tasks, and genetic setting. Method are artificial neural nets ( ANNs ), backpropagation, and classification network. Generic is data. OtherScientificTerm is numerical parameter. ",This paper proposes a new method for learning plasticity rules for recurrent neural networks (RNNs). The proposed method is based on gradient descent (GD) which is an extension of the gradient descent method for backpropagation. The authors show that the proposed method can be used to learn a set of rules that are robust to adversarial perturbations. They also show that GD can be applied to the training of RNNs. ,This paper proposes a new method for learning plasticity rules for recurrent neural networks (RNNs). The proposed method is based on gradient descent (GD) which is an extension of the gradient descent method for backpropagation. The authors show that the proposed method can be used to learn a set of rules that are robust to adversarial perturbations. They also show that GD can be applied to the training of RNNs. 
2041,SP:f435530146fa975cb27cd375a857df9bcbd87682,"visual question generation ( VQG ) USED-FOR human - like questions. image CONJUNCTION side information. side information CONJUNCTION image. image USED-FOR visual question generation ( VQG ). side information USED-FOR human - like questions. image USED-FOR human - like questions. visual objects PART-OF image. side information CONJUNCTION image. image CONJUNCTION side information. image USED-FOR generating referential and meaningful questions. learning paradigm USED-FOR visual questions. answer - awareness CONJUNCTION region - reference. region - reference CONJUNCTION answer - awareness. region - reference USED-FOR learning paradigm. answer - awareness FEATURE-OF visual questions. Double Hints textual answers CONJUNCTION visual regions of interests. visual regions of interests CONJUNCTION Double Hints textual answers. Double Hints textual answers USED-FOR visual questions. methodology USED-FOR visual hints. dynamic graph USED-FOR them. VQA2.0 CONJUNCTION COCO - QA datasets. COCO - QA datasets CONJUNCTION VQA2.0. model COMPARE baselines. baselines COMPARE model. COCO - QA datasets EVALUATE-FOR model. VQA2.0 EVALUATE-FOR model. COCO - QA datasets EVALUATE-FOR baselines. setting EVALUATE-FOR model. Task are VQG, and one - to - many mapping issue. OtherScientificTerm are human annotations, implicit topology end - to - end, and double hints. Method is graph - to - sequence model. ","This paper proposes a new method for visual question generation (VQG) based on a graph-to-sequence model. The proposed method is based on the idea of double-hints, which is an extension of the Double Hints (DH) framework. The key idea is to use a dynamic graph to generate visual hints for each question, which are then used to generate the answer to the question. The method is evaluated on COCO-QA and VQA2.0.","This paper proposes a new method for visual question generation (VQG) based on a graph-to-sequence model. The proposed method is based on the idea of double-hints, which is an extension of the Double Hints (DH) framework. The key idea is to use a dynamic graph to generate visual hints for each question, which are then used to generate the answer to the question. The method is evaluated on COCO-QA and VQA2.0."
2057,SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,sample size CONJUNCTION model size. model size CONJUNCTION sample size. linear regression CONJUNCTION neural networks. neural networks CONJUNCTION linear regression. linear regression HYPONYM-OF learning algorithms. neural networks HYPONYM-OF learning algorithms. optimal regularization USED-FOR double - descent phenomenon. sample size CONJUNCTION model size. model size CONJUNCTION sample size. isotropic data distribution FEATURE-OF linear regression models. optimally - tuned ` 2 regularization USED-FOR linear regression models. optimally - tuned ` 2 regularization USED-FOR double descent. test risk scalings EVALUATE-FOR algorithms. tuned regularization USED-FOR algorithms. tuned regularization USED-FOR test risk scalings. Task is generalization. ,"This paper studies the double-descent phenomenon in linear regression models. The authors show that under certain assumptions, the sample size and sample size of the linear regression model can be reduced to a small number of samples. They also show that this phenomenon can be alleviated by tuning the regularization of the model.  ","This paper studies the double-descent phenomenon in linear regression models. The authors show that under certain assumptions, the sample size and sample size of the linear regression model can be reduced to a small number of samples. They also show that this phenomenon can be alleviated by tuning the regularization of the model.  "
2073,SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,spatial regularities FEATURE-OF images. spatial regularities USED-FOR generative modeling. neural network USED-FOR building image generators ( decoders ). it USED-FOR variational autoencoders ( VAEs ). sequential gating - based mechanism USED-FOR contextual information. feature maps PART-OF deep neural net. sequential gating - based mechanism USED-FOR feature maps. spatial dependency layers USED-FOR density estimation. decoder USED-FOR density estimation. decoder USED-FOR hierarchical VAE. baseline convolutional architectures USED-FOR density estimation. spatial dependency layers USED-FOR hierarchical VAE. spatial dependency layers USED-FOR decoder. SDN USED-FOR large images. SDN decoder USED-FOR learning disentangled representations. neural architectures USED-FOR task. SDN decoder USED-FOR vanilla VAE setting. spatial dependency COMPARE convolutional layers. convolutional layers COMPARE spatial dependency. Method is spatial dependency networks ( SDNs ). OtherScientificTerm is 2 - D space. Generic is models. Material is VAE settings. ,This paper proposes a spatial dependency network (SDN) for learning disentangled representations of images. The proposed method is based on a sequential gating-based mechanism and is able to learn feature maps in a 2-D space. The authors show that the proposed method outperforms the baseline convolutional layers and a hierarchical VAE in terms of disentanglement and density estimation. ,This paper proposes a spatial dependency network (SDN) for learning disentangled representations of images. The proposed method is based on a sequential gating-based mechanism and is able to learn feature maps in a 2-D space. The authors show that the proposed method outperforms the baseline convolutional layers and a hierarchical VAE in terms of disentanglement and density estimation. 
2089,SP:db91512a90e75675af03c2f197751c8526d6f5e9,"prior approach USED-FOR offline RL. backup operator USED-FOR algorithm. EMaQ USED-FOR sub - optimality bounds. complexity EVALUATE-FOR offline RL problems. proposal distribution USED-FOR EMaQ. offline RL setting EVALUATE-FOR EMaQ. D4RL benchmarks EVALUATE-FOR EMaQ. EMaQ COMPARE Soft Actor Critic ( SAC ). Soft Actor Critic ( SAC ) COMPARE EMaQ. online RL setting EVALUATE-FOR EMaQ. generative model design USED-FOR estimating behavior policies. complexity EVALUATE-FOR offline RL problems. Method are Off - policy reinforcement learning ( RL ), off - policy RL methods, and BCQ. Generic is methods. OtherScientificTerm are policies, dataset of interactions, heuristic design choice, behavior policy, distribution support, behavior policies, and function approximator. ","This paper proposes a new offline RL algorithm called EMaQ, which is a generative model-based approach for offline RL. The authors propose to use the proposal distribution as a backup operator for the offline RL problem. The proposed method is evaluated on a number of D4RL benchmarks and shows that it outperforms baselines in terms of sub-optimality and complexity.","This paper proposes a new offline RL algorithm called EMaQ, which is a generative model-based approach for offline RL. The authors propose to use the proposal distribution as a backup operator for the offline RL problem. The proposed method is evaluated on a number of D4RL benchmarks and shows that it outperforms baselines in terms of sub-optimality and complexity."
2105,SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"fair machine learning model USED-FOR demographic disparity. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. Existing techniques USED-FOR model fairness. outer optimizer USED-FOR inner problem. inner optimizer USED-FOR training algorithm. minibatch sizes USED-FOR model fairness. equal opportunity CONJUNCTION equalized odds. equalized odds CONJUNCTION equal opportunity. equalized odds CONJUNCTION demographic parity. demographic parity CONJUNCTION equalized odds. optimization USED-FOR batch selection algorithm. FairBatch HYPONYM-OF batch selection algorithm. fairness measures PART-OF batch selection algorithm. equal opportunity HYPONYM-OF fairness measures. demographic parity HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. PyTorch code USED-FOR batch selection. batch selection PART-OF model training. FairBatch USED-FOR fairness. fine - tuning USED-FOR FairBatch. It CONJUNCTION batch selection techniques. batch selection techniques CONJUNCTION It. faster convergence HYPONYM-OF batch selection techniques. Method are machine learning systems, and bilevel optimization. Generic are functionality, and it. Material is synthetic and benchmark real data. ","This paper proposes a new batch selection algorithm for fair machine learning models. The proposed algorithm is based on bilevel optimization. The main idea is to use the inner optimizer of the training algorithm as the inner problem and the outer optimizer as the outer problem for the batch selection problem. The inner optimization problem is formulated as a binary optimization problem, where the outer optimization problem can be solved by the inner optimization of the inner one. The authors show that the proposed algorithm outperforms existing batch selection algorithms on synthetic and real-world datasets.","This paper proposes a new batch selection algorithm for fair machine learning models. The proposed algorithm is based on bilevel optimization. The main idea is to use the inner optimizer of the training algorithm as the inner problem and the outer optimizer as the outer problem for the batch selection problem. The inner optimization problem is formulated as a binary optimization problem, where the outer optimization problem can be solved by the inner optimization of the inner one. The authors show that the proposed algorithm outperforms existing batch selection algorithms on synthetic and real-world datasets."
2121,SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"robustness guarantees CONJUNCTION generalization bounds. generalization bounds CONJUNCTION robustness guarantees. Lipschitz constants FEATURE-OF deep networks. generalization bounds CONJUNCTION smoothness of decision boundaries. smoothness of decision boundaries CONJUNCTION generalization bounds. bounds USED-FOR models. deep equilibrium ( DEQ ) model HYPONYM-OF models. monotone DEQs HYPONYM-OF DEQs. Lipschitz constants FEATURE-OF monotone DEQs. input - output mapping CONJUNCTION weight - output mapping. weight - output mapping CONJUNCTION input - output mapping. simple - yet - tight bounds USED-FOR input - output mapping. simple - yet - tight bounds USED-FOR weight - output mapping. networks USED-FOR weight - output mapping. bounds USED-FOR monotone DEQ models. multiscale convolutional structure USED-FOR monotone DEQ models. bounds USED-FOR PAC - Bayes generalization bounds. Method are infinitely - deep network, and DNNs. OtherScientificTerm are monotonicity parameter, Lipschitz constant, and exponential depth - dependence of comparable DNN bounds. Generic is they. ","This paper studies monotone deep equilibrium (DEQ) models with Lipschitz constants, which are monotonicity parameters. The authors show that the generalization bounds of DNNs with monotonic parameters have exponential depth-dependent dependence on the depth of the input-output mapping. They also show that for monotones, the PAC-Bayes generalization bound can be reduced to simple-yet-tight bounds for the input and output mapping. Finally, they show that monotonone DEQs can be approximated by multiscale convolutional networks.","This paper studies monotone deep equilibrium (DEQ) models with Lipschitz constants, which are monotonicity parameters. The authors show that the generalization bounds of DNNs with monotonic parameters have exponential depth-dependent dependence on the depth of the input-output mapping. They also show that for monotones, the PAC-Bayes generalization bound can be reduced to simple-yet-tight bounds for the input and output mapping. Finally, they show that monotonone DEQs can be approximated by multiscale convolutional networks."
2137,SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,imitation learning CONJUNCTION goal - conditioned reinforcement learning. goal - conditioned reinforcement learning CONJUNCTION imitation learning. goal - conditioned reinforcement learning HYPONYM-OF settings. imitation learning HYPONYM-OF settings. probabilistic long - term dynamics CONJUNCTION desired value function. desired value function CONJUNCTION probabilistic long - term dynamics. density estimation USED-FOR approach. it USED-FOR hindsight bias. hindsight bias FEATURE-OF stochastic domains. it USED-FOR sparse rewards. expert data USED-FOR approach. Generic is solutions. Method is goalconditioned reinforcement learning. ,"This paper studies the problem of goal-conditioned reinforcement learning in stochastic domains. The authors propose a new method to learn a value function that can be used to approximate the desired value function. The method is based on density estimation, and the authors show that the proposed method is able to learn the value function in the presence of hindsight bias. They also show that their method can be applied to the case of sparse rewards.","This paper studies the problem of goal-conditioned reinforcement learning in stochastic domains. The authors propose a new method to learn a value function that can be used to approximate the desired value function. The method is based on density estimation, and the authors show that the proposed method is able to learn the value function in the presence of hindsight bias. They also show that their method can be applied to the case of sparse rewards."
2153,SP:d57550b2f323b356d7e609acc35ee33039f376b4,"probabilistic inference framework USED-FOR simultaneously learning multiple related tasks. variational multi - task learning VMTL HYPONYM-OF probabilistic inference framework. variational Bayesian inference problem USED-FOR multi - task learning. priors USED-FOR task relatedness. mixture of variational posteriors USED-FOR prior. representations CONJUNCTION classifiers. classifiers CONJUNCTION representations. VMTL USED-FOR multi - task learning. limited training data USED-FOR VMTL. limited training data USED-FOR multi - task learning. benchmark datasets EVALUATE-FOR it. Method is Multi - task learning. OtherScientificTerm are Gumbel - softmax priors, mixing weights, and shared inductive bias. Generic is tasks. ",This paper proposes a variational multi-task learning (VMTL) framework for learning tasks with limited training data. The main idea is to use a mixture of variational posteriors for the task-specific priors. The authors show that the proposed method outperforms baselines on a number of benchmark datasets. ,This paper proposes a variational multi-task learning (VMTL) framework for learning tasks with limited training data. The main idea is to use a mixture of variational posteriors for the task-specific priors. The authors show that the proposed method outperforms baselines on a number of benchmark datasets. 
2169,SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"Transformers USED-FOR long sequence lengths. fast Transformers USED-FOR problem. model quality EVALUATE-FOR vanilla Transformer models. systematic and unified benchmark EVALUATE-FOR model quality. Long - Range Arena EVALUATE-FOR model quality. long - context scenarios FEATURE-OF model quality. Long - Range Arena HYPONYM-OF systematic and unified benchmark. text CONJUNCTION natural, synthetic images. natural, synthetic images CONJUNCTION text. natural, synthetic images CONJUNCTION mathematical expressions. mathematical expressions CONJUNCTION natural, synthetic images. similarity USED-FOR mathematical expressions. Linear Transformers CONJUNCTION Sinkhorn Transformers. Sinkhorn Transformers CONJUNCTION Linear Transformers. Synthesizers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Synthesizers. Linformers CONJUNCTION Linear Transformers. Linear Transformers CONJUNCTION Linformers. Performers CONJUNCTION Synthesizers. Synthesizers CONJUNCTION Performers. Sparse Transformers CONJUNCTION Longformers. Longformers CONJUNCTION Sparse Transformers. Sinkhorn Transformers CONJUNCTION Performers. Performers CONJUNCTION Sinkhorn Transformers. Reformers CONJUNCTION Linformers. Linformers CONJUNCTION Reformers. Performers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Performers. Synthesizers CONJUNCTION Longformers. Longformers CONJUNCTION Synthesizers. benchmark suite EVALUATE-FOR long - range Transformer models. Longformers HYPONYM-OF long - range Transformer models. Sparse Transformers HYPONYM-OF long - range Transformer models. Sinkhorn Transformers HYPONYM-OF long - range Transformer models. Reformers HYPONYM-OF long - range Transformer models. Synthesizers HYPONYM-OF long - range Transformer models. Linformers HYPONYM-OF long - range Transformer models. Performers HYPONYM-OF long - range Transformer models. Linear Transformers HYPONYM-OF long - range Transformer models. Long - Range Arena USED-FOR Transformer models. Metric are quadratic self - attention complexity, and","This paper presents a systematic and unified benchmark for long-range transformer models. The benchmark is based on Long-Range Arena, which is a systematic benchmark for evaluating the performance of transformer models on long sequence length tasks. The authors show that the model quality of a transformer model is highly correlated with the number of long sequence lengths. They also show that transformer models trained on the Long-range Arena outperform those trained on vanilla Transformer models. ","This paper presents a systematic and unified benchmark for long-range transformer models. The benchmark is based on Long-Range Arena, which is a systematic benchmark for evaluating the performance of transformer models on long sequence length tasks. The authors show that the model quality of a transformer model is highly correlated with the number of long sequence lengths. They also show that transformer models trained on the Long-range Arena outperform those trained on vanilla Transformer models. "
2185,SP:e12e410c3335b76133ceda4c865b244fbbab8580,"Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR machine learning models. Context USED-FOR machine learning models. Structure of source code USED-FOR model. source code CONJUNCTION features. features CONJUNCTION source code. language - agnostic features USED-FOR model. features HYPONYM-OF language - agnostic features. AST USED-FOR features. source code HYPONYM-OF language - agnostic features. programming languages EVALUATE-FOR monolingual code summarization. Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR representation learning on code. Context USED-FOR representation learning on code. OtherScientificTerm are Source code ( Context ), and computer program. Method is multilingual code summarization model. Material are non - parallel data, and low - resource languages. ",This paper proposes a multilingual code summarization model for monolingual programming languages. The proposed model is based on the idea that the structure of source code (context) and features (structure) can be used to learn representation learning on code. The main contribution of the paper is that the model is able to learn representations for both source code and features in low-resource languages. ,This paper proposes a multilingual code summarization model for monolingual programming languages. The proposed model is based on the idea that the structure of source code (context) and features (structure) can be used to learn representation learning on code. The main contribution of the paper is that the model is able to learn representations for both source code and features in low-resource languages. 
2201,SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"sights USED-FOR sound source. recurrent aggregations of the audio observations USED-FOR models. reinforcement learning approach USED-FOR audio - visual navigation. elements USED-FOR reinforcement learning approach. waypoints PART-OF elements. audio and visual data USED-FOR geometry of an unmapped space. real - world 3D scenes CONJUNCTION Replica. Replica CONJUNCTION real - world 3D scenes. real - world 3D scenes EVALUATE-FOR approach. sights CONJUNCTION sounds. sounds CONJUNCTION sights. sounds CONJUNCTION space. space CONJUNCTION sounds. OtherScientificTerm are agent motion, acoustic memory, and audio_visual_waypoints. Method is navigation policy. Generic is model. ","This paper proposes a reinforcement learning approach for audio-visual navigation in an unmapped space. The key idea of the approach is to learn a recurrent aggregation of the audio observations and visual waypoints, which are then used to train a navigation policy. The proposed approach is evaluated on a set of real-world 3D scenes, and is shown to outperform baselines.","This paper proposes a reinforcement learning approach for audio-visual navigation in an unmapped space. The key idea of the approach is to learn a recurrent aggregation of the audio observations and visual waypoints, which are then used to train a navigation policy. The proposed approach is evaluated on a set of real-world 3D scenes, and is shown to outperform baselines."
2217,SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"optimization methods COMPARE weight initializations. weight initializations COMPARE optimization methods. learning abilities FEATURE-OF neural networks. small CNN USED-FOR update rules. architecture USED-FOR task. task USED-FOR networks. architecture USED-FOR networks. initialization parameters USED-FOR gradient descent. single sign change HYPONYM-OF small perturbations. OtherScientificTerm are lottery tickets, and lottery ticket hypothesis. Method are small convolutional networks, and minimal networks. ","This paper studies the lottery ticket hypothesis, which is a well-studied and well-motivated topic in machine learning. The authors show that the lottery tickets hypothesis holds for small convolutional neural networks, and that it holds for minimal networks as well. They also show that this hypothesis holds in the case of single sign perturbations. The paper also shows that for small CNNs, lottery tickets can be used as initialization parameters for gradient descent. ","This paper studies the lottery ticket hypothesis, which is a well-studied and well-motivated topic in machine learning. The authors show that the lottery tickets hypothesis holds for small convolutional neural networks, and that it holds for minimal networks as well. They also show that this hypothesis holds in the case of single sign perturbations. The paper also shows that for small CNNs, lottery tickets can be used as initialization parameters for gradient descent. "
2233,SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"Semi - supervised learning ( SSL ) USED-FOR unlabeled data. consistency regularization USED-FOR SSL approaches. RankingMatch HYPONYM-OF method. computational efficiency EVALUATE-FOR objective function. BatchMean Triplet loss HYPONYM-OF objective function. accuracy EVALUATE-FOR SVHN. accuracy EVALUATE-FOR SVHN. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. accuracy EVALUATE-FOR RankingMatch. accuracy EVALUATE-FOR RankingMatch. SSL benchmarks EVALUATE-FOR RankingMatch. BatchMean Triplet loss COMPARE Triplet loss. Triplet loss COMPARE BatchMean Triplet loss. ablation study EVALUATE-FOR BatchMean Triplet loss. ablation study EVALUATE-FOR Triplet loss. Material are labeled data, CIFAR-10, and CIFAR-100. Generic is model. OtherScientificTerm is labeled data amounts. ","This paper proposes RankingMatch, a new method for semi-supervised learning for unlabeled data. RankingMatch is based on BatchMean Triplet loss, which is a consistency regularization method. The authors show that RankingMatch outperforms SVHN and Triplet in terms of accuracy and computational efficiency. The paper also provides ablation studies to demonstrate the effectiveness of RankingMatch. ","This paper proposes RankingMatch, a new method for semi-supervised learning for unlabeled data. RankingMatch is based on BatchMean Triplet loss, which is a consistency regularization method. The authors show that RankingMatch outperforms SVHN and Triplet in terms of accuracy and computational efficiency. The paper also provides ablation studies to demonstrate the effectiveness of RankingMatch. "
2249,SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"formulation USED-FOR sequential learning setting. meta - training CONJUNCTION adaptation. adaptation CONJUNCTION meta - training. sample complexity CONJUNCTION regret. regret CONJUNCTION sample complexity. sample complexity EVALUATE-FOR empirical risk minimization methods. regret EVALUATE-FOR empirical risk minimization methods. meta - learning USED-FOR online setting. meta - learning COMPARE empirical risk minimization methods. empirical risk minimization methods COMPARE meta - learning. regret EVALUATE-FOR meta - learning. sample complexity EVALUATE-FOR meta - learning. bi - level optimizations FEATURE-OF meta - learning algorithms. meta - training data USED-FOR meta - learning algorithms. meta - training data USED-FOR bi - level optimizations. meta - learning algorithms USED-FOR variable - shot settings. many - shot learning CONJUNCTION zero - shot learning. zero - shot learning CONJUNCTION many - shot learning. variable - shot settings PART-OF sequential learning. meta - learning algorithms USED-FOR sequential learning. zero - shot learning HYPONYM-OF variable - shot settings. meta - learning COMPARE supervised methods. supervised methods COMPARE meta - learning. cumulative performance EVALUATE-FOR supervised methods. sequential learning problems EVALUATE-FOR meta - learning. cumulative performance EVALUATE-FOR meta - learning. meta - learning USED-FOR learning systems. Method are Few - shot meta - learning methods, and metalearning. Generic is problem. ","This paper studies the problem of few-shot meta-learning in the variable-shot setting, where the goal is to minimize the sample complexity and the regret of the learning algorithm. The authors propose a meta-training algorithm for this setting, and show that it outperforms existing methods in terms of sample complexity, the regret, and the cumulative performance. They also show that meta-learned algorithms can be used for zero-shot learning and many-shot optimization. ","This paper studies the problem of few-shot meta-learning in the variable-shot setting, where the goal is to minimize the sample complexity and the regret of the learning algorithm. The authors propose a meta-training algorithm for this setting, and show that it outperforms existing methods in terms of sample complexity, the regret, and the cumulative performance. They also show that meta-learned algorithms can be used for zero-shot learning and many-shot optimization. "
2265,SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"contextual representations USED-FOR NLP tasks. pretrained Transformer models USED-FOR contextual representations. representations USED-FOR sentence - level syntax. self - supervision USED-FOR Transformers networks. probes EVALUATE-FOR Transformer representations. random permutations of n - grams HYPONYM-OF perturbations. syntactic distance FEATURE-OF attention mechanism. local phrase structure FEATURE-OF sensitivity. Generic are they, network, probe, and representation. Task is computational and cognitive neuroscience. OtherScientificTerm are representational invariance, word position, syntactic phrase, global phrase structure, hierarchical phrase structure, and attention weights. Method are Transformer architecture, and Transformers. ","This paper studies the representation invariance of Transformer models to syntactic perturbations. Specifically, the authors show that the attention mechanism is invariant to the syntactic distance between the input and the target phrase. They also show that this invariance holds for the global phrase structure and local phrase structure, and that the global attention mechanism can be used as a self-supervision mechanism for Transformer representations. The authors also provide a theoretical analysis of the effect of syntactic distances between the target and target phrases. ","This paper studies the representation invariance of Transformer models to syntactic perturbations. Specifically, the authors show that the attention mechanism is invariant to the syntactic distance between the input and the target phrase. They also show that this invariance holds for the global phrase structure and local phrase structure, and that the global attention mechanism can be used as a self-supervision mechanism for Transformer representations. The authors also provide a theoretical analysis of the effect of syntactic distances between the target and target phrases. "
2281,SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"high - fidelity images USED-FOR Generative Adversarial Networks ( GAN ). large - scale GPU - clusters USED-FOR Generative Adversarial Networks ( GAN ). few - shot image synthesis task USED-FOR GAN. minimum computing cost FEATURE-OF few - shot image synthesis task. 1024 × 1024 resolution EVALUATE-FOR light - weight GAN structure. skip - layer channel - wise excitation module CONJUNCTION self - supervised discriminator. self - supervised discriminator CONJUNCTION skip - layer channel - wise excitation module. feature - encoder USED-FOR self - supervised discriminator. model COMPARE StyleGAN2. StyleGAN2 COMPARE model. datasets EVALUATE-FOR model. datasets EVALUATE-FOR StyleGAN2. image domains FEATURE-OF datasets. OtherScientificTerm are RTX-2080 GPU, and computing budget. ","This paper proposes a new GAN architecture for few-shot image synthesis. The proposed architecture consists of a skip-layer channel-wise excitation module, a self-supervised discriminator, and a feature-encoder. The authors show that the proposed architecture is able to achieve state-of-the-art performance on several image datasets. ","This paper proposes a new GAN architecture for few-shot image synthesis. The proposed architecture consists of a skip-layer channel-wise excitation module, a self-supervised discriminator, and a feature-encoder. The authors show that the proposed architecture is able to achieve state-of-the-art performance on several image datasets. "
2297,SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"neural network bounding USED-FOR neural network verification systems. specialised dual solvers USED-FOR neural network bounds. linear program HYPONYM-OF relaxation. linear relaxation USED-FOR piecewise linear activations. dual algorithm USED-FOR relaxation. tightness CONJUNCTION linear separation oracle. linear separation oracle CONJUNCTION tightness. method USED-FOR relaxation. dual space FEATURE-OF relaxation. tightness HYPONYM-OF relaxation. linear separation oracle HYPONYM-OF relaxation. dual approaches USED-FOR weaker relaxations. massive parallelism CONJUNCTION GPU implementation. GPU implementation CONJUNCTION massive parallelism. dual approaches USED-FOR it. bounds COMPARE off - the - shelf solvers. off - the - shelf solvers COMPARE bounds. speed - accuracy trade - offs EVALUATE-FOR dual solvers. running time EVALUATE-FOR off - the - shelf solvers. Generic is they. Method is customised solver. OtherScientificTerm are dual variables, computational budget, and formal verification speed - ups. ",This paper proposes a novel dual solver for neural network bounding. The main idea is to use a linear program to learn a relaxation of the neural network bounds. The authors show that the proposed method is able to achieve better performance than off-the-shelf dual solvers in terms of speed-up and accuracy trade-off. They also show that their method can be combined with a linear separation oracle to improve the performance. ,This paper proposes a novel dual solver for neural network bounding. The main idea is to use a linear program to learn a relaxation of the neural network bounds. The authors show that the proposed method is able to achieve better performance than off-the-shelf dual solvers in terms of speed-up and accuracy trade-off. They also show that their method can be combined with a linear separation oracle to improve the performance. 
2313,SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"masked token prediction CONJUNCTION masked span infilling. masked span infilling CONJUNCTION masked token prediction. masked span infilling USED-FOR T5 - style PTLMs. masked token prediction USED-FOR BERT - style PTLMs. everyday concepts FEATURE-OF relational commonsense knowledge. BERT - style PTLMs HYPONYM-OF pre - training objectives. masked token prediction HYPONYM-OF pre - training objectives. masked span infilling HYPONYM-OF pre - training objectives. intermediate self - supervised learning tasks USED-FOR PTLMs. them USED-FOR intermediate self - supervised learning tasks. generative and contrastive objectives USED-FOR common sense. task - specific fine - tuning USED-FOR PTLMs. concept - centric commonsense knowledge USED-FOR PTLMs. pre - training framework USED-FOR generative and contrastive objectives. concept - aware language model ( CALM)1 HYPONYM-OF method. CALM COMPARE PTLMs. PTLMs COMPARE CALM. CALM COMPARE baseline methods. baseline methods COMPARE CALM. CALM USED-FOR PTLM. baseline methods COMPARE PTLMs. PTLMs COMPARE baseline methods. Method are Pre - trained language models ( PTLM ), and text - to - text transformer. Generic is they. OtherScientificTerm are commonsense knowledge, and external knowledge graphs. Task is NLU and NLG tasks. ","This paper proposes a concept-aware language model (CALM) for pre-trained language models. CALM is a pre-training framework for learning commonsense knowledge that can be used for intermediate self-supervised learning tasks such as NLU and NLG tasks. The authors propose a generative and contrastive framework to improve the performance of PTLMs. The proposed method is evaluated on NLU, NLG and text-to-text tasks.","This paper proposes a concept-aware language model (CALM) for pre-trained language models. CALM is a pre-training framework for learning commonsense knowledge that can be used for intermediate self-supervised learning tasks such as NLU and NLG tasks. The authors propose a generative and contrastive framework to improve the performance of PTLMs. The proposed method is evaluated on NLU, NLG and text-to-text tasks."
2329,SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"frameworks USED-FOR 2D segments. object interactions HYPONYM-OF physics. multi - scale pixel cues CONJUNCTION physical motion cues. physical motion cues CONJUNCTION multi - scale pixel cues. synthetic and real scenes EVALUATE-FOR model. object properties USED-FOR physical events. Task is unsupervised physical object discovery. OtherScientificTerm are 3D geometry, developmental psychology, and observable and partially occluded objects. Material is video. ","This paper proposes a method for unsupervised physical object discovery. The method is based on the idea of object interactions in video, which is an important topic in the field of physics. The authors propose a method to learn object interactions between two objects in a video. The proposed method is evaluated on synthetic and real-world scenes, and is shown to outperform existing methods.","This paper proposes a method for unsupervised physical object discovery. The method is based on the idea of object interactions in video, which is an important topic in the field of physics. The authors propose a method to learn object interactions between two objects in a video. The proposed method is evaluated on synthetic and real-world scenes, and is shown to outperform existing methods."
2345,SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"Deep neural networks ( DNNs ) USED-FOR adversarial attacks. convolutional neural networks PART-OF Deep neural networks ( DNNs ). attack algorithms USED-FOR Adversarial samples. training method USED-FOR DNN robustness. adversarial noises FEATURE-OF DNN robustness. Increasing Margin Adversarial ( IMA ) Training HYPONYM-OF training method. IMA method USED-FOR margins. decision boundaries USED-FOR DNN model. IMA method USED-FOR training. robustness EVALUATE-FOR IMA method. clean data EVALUATE-FOR accuracy. noisy data EVALUATE-FOR method. accuracy EVALUATE-FOR method. clean data EVALUATE-FOR method. classification accuracy EVALUATE-FOR method. approach USED-FOR robust DNN applications. COVID-19 diagnosis HYPONYM-OF robust DNN applications. CT images USED-FOR COVID-19 diagnosis. Task is life - critical applications. OtherScientificTerm is white noises. Material are COVID-19 CT image dataset, and 100 - PGD white - box adversarial attacks. ","This paper proposes a method to improve the robustness of deep neural networks against white-box adversarial attacks. The method is based on the idea of Increasing Margin Adversarial (IMA) Training (IMAT), which aims to increase the margin between the decision boundary of the model and the adversarial samples. The authors show that the proposed method outperforms existing methods in terms of robustness to white box attacks. ","This paper proposes a method to improve the robustness of deep neural networks against white-box adversarial attacks. The method is based on the idea of Increasing Margin Adversarial (IMA) Training (IMAT), which aims to increase the margin between the decision boundary of the model and the adversarial samples. The authors show that the proposed method outperforms existing methods in terms of robustness to white box attacks. "
2361,SP:276ffd59fbf49e3ee02756da8920218102214917,"Knowledge distillation USED-FOR compact models. teacher model USED-FOR compact student network. poor local optima FEATURE-OF optimization. ProKT HYPONYM-OF model - agnostic method. supervision signals USED-FOR teacher model. student ’s parameter space FEATURE-OF supervision signals. local intermediate targets FEATURE-OF training objective. approximate mirror descent technique USED-FOR local intermediate targets. approximate mirror descent technique USED-FOR projection. quirks USED-FOR optimization. quirks USED-FOR method. ProKT COMPARE knowledge distillation methods. knowledge distillation methods COMPARE ProKT. image and text datasets EVALUATE-FOR ProKT. Method is Deep neural networks. OtherScientificTerm are computation capacity, and local optima. ","This paper proposes ProKT, a knowledge distillation method for deep neural networks. ProKT is a model-agnostic method that uses a teacher model to learn a compact student network, and a student model to distill the knowledge from the teacher model. The teacher model is trained by minimizing the local optima of the student model, and the student network is trained using a mirror descent technique. The paper shows that ProKT achieves better performance than existing knowledge distilling methods on image and text datasets.","This paper proposes ProKT, a knowledge distillation method for deep neural networks. ProKT is a model-agnostic method that uses a teacher model to learn a compact student network, and a student model to distill the knowledge from the teacher model. The teacher model is trained by minimizing the local optima of the student model, and the student network is trained using a mirror descent technique. The paper shows that ProKT achieves better performance than existing knowledge distilling methods on image and text datasets."
2377,SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"channel pruning method USED-FOR compression. compression FEATURE-OF Convolutional Neural Networks ( CNNs ). hyper - structure network USED-FOR architecture of the main network. hypernet COMPARE hyperstructure network. hyperstructure network COMPARE hypernet. regular backpropagation USED-FOR hyperstructure network. regularization term USED-FOR computational resource. regularization term USED-FOR compact network. computational resource FEATURE-OF compact network. FLOPs USED-FOR computational resource. FLOPs USED-FOR regularization. FLOPs USED-FOR it. layer - wise scaling factors USED-FOR gradients. hyper - gradient descent USED-FOR they. ImageNet EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. CIFAR-10 EVALUATE-FOR method. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method is channel pruning methods. OtherScientificTerm are layers, and gates. ","This paper proposes a channel pruning method for compression of convolutional neural networks (CNNs). The proposed method is based on hyper-gradient descent and regular backpropagation. The main contribution of the paper is the regularization term, which is used to reduce the computational cost of the pruning process. The authors show that the proposed method can achieve competitive results on CIFAR-10 and ImageNet.","This paper proposes a channel pruning method for compression of convolutional neural networks (CNNs). The proposed method is based on hyper-gradient descent and regular backpropagation. The main contribution of the paper is the regularization term, which is used to reduce the computational cost of the pruning process. The authors show that the proposed method can achieve competitive results on CIFAR-10 and ImageNet."
2393,SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"exploration mechanism USED-FOR theorem prover. imitation CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation. It USED-FOR prover. imitation USED-FOR prover. reinforcement learning USED-FOR prover. Task are automated higher - order logic theorem proving, and exploration of premises. OtherScientificTerm are human proofs, deep reinforcement learning scenario, and DeepHOL Zero. Method is exploration approach. ","This paper proposes a method for learning a theorem prover that is capable of solving higher-order logic theorem proving. The method is based on reinforcement learning and imitation learning. The main idea is to use reinforcement learning to learn a prover to solve the problem of proving a theorem, and then use imitation learning to improve the performance of the prover. Experiments show that the proposed method is able to achieve state-of-the-art results on the DeepHOL Zero benchmark.","This paper proposes a method for learning a theorem prover that is capable of solving higher-order logic theorem proving. The method is based on reinforcement learning and imitation learning. The main idea is to use reinforcement learning to learn a prover to solve the problem of proving a theorem, and then use imitation learning to improve the performance of the prover. Experiments show that the proposed method is able to achieve state-of-the-art results on the DeepHOL Zero benchmark."
2409,SP:88209417a8ad07e6103084e41709be900303ce5f,"models USED-FOR machine learning tasks. data augmentation USED-FOR models. augmentation methods USED-FOR data modality. image processing functions CONJUNCTION word - replacing rules. word - replacing rules CONJUNCTION image processing functions. word - replacing rules USED-FOR text data. image processing functions USED-FOR image data. MODALS USED-FOR automated data augmentation. universal data transformation operations USED-FOR transform. MODALS USED-FOR universal data transformation operations. latent space FEATURE-OF universal data transformation operations. Method are Data augmentation, and automated data augmentation approach. Material is artificial data. OtherScientificTerm are modality, and modalities. ","This paper proposes a new method for data augmentation. The proposed method is based on the idea of universal data transformation operations, which can be applied to any data modality. The authors show that the proposed method can be used to augment any modality, and that it can be combined with existing methods. The method is evaluated on synthetic and real-world datasets.","This paper proposes a new method for data augmentation. The proposed method is based on the idea of universal data transformation operations, which can be applied to any data modality. The authors show that the proposed method can be used to augment any modality, and that it can be combined with existing methods. The method is evaluated on synthetic and real-world datasets."
2425,SP:6d84670d321b0d584b097c630574bd748e85c9a2,"nonlinear and nontrivial dynamical limit FEATURE-OF learning dynamics. mean field limit HYPONYM-OF nonlinear and nontrivial dynamical limit. neural networks USED-FOR mean field regime. mean field limit USED-FOR large - width neural networks. analysis USED-FOR two - layer networks. mean field regime FEATURE-OF optimization efficiency. global convergence result FEATURE-OF unregularized feedforward three - layer networks. mean field regime FEATURE-OF unregularized feedforward three - layer networks. rigorous framework USED-FOR mean field limit. mean field limit FEATURE-OF three - layer networks. stochastic gradient descent training USED-FOR three - layer networks. stochastic gradient descent training USED-FOR mean field limit. probability space FEATURE-OF neural networks. neural networks PART-OF neuronal embedding. probability space PART-OF neuronal embedding. mean field limit USED-FOR global convergence guarantee. regularity CONJUNCTION convergence mode assumptions. convergence mode assumptions CONJUNCTION regularity. convergence mode assumptions FEATURE-OF global convergence guarantee. regularity USED-FOR global convergence guarantee. universal approximation property FEATURE-OF neural networks. algebraic topology argument USED-FOR universal approximation property. algebraic topology argument USED-FOR neural networks. OtherScientificTerm are global convergence guarantees, multilayer ones, and convexity. ","This paper studies the global convergence of neural networks in the mean field regime, which is a non-linear and nontrivial dynamical limit of learning dynamics. The authors consider the case of unregularized feedforward three-layer networks, where the learning dynamics is non-convex. They show that global convergence guarantees are guaranteed for multilayer neural networks. They also show that this global convergence guarantee is guaranteed for large-width neural networks, and show that it is also guaranteed for two-layer neural networks with regularity. ","This paper studies the global convergence of neural networks in the mean field regime, which is a non-linear and nontrivial dynamical limit of learning dynamics. The authors consider the case of unregularized feedforward three-layer networks, where the learning dynamics is non-convex. They show that global convergence guarantees are guaranteed for multilayer neural networks. They also show that this global convergence guarantee is guaranteed for large-width neural networks, and show that it is also guaranteed for two-layer neural networks with regularity. "
2441,SP:b90f893f927db9c439595fd119a565cf43c971f4,"interpretable parameterizations USED-FOR real - world decision - making. interpretable parameterizations USED-FOR introspecting and auditing policies. counterfactual reasoning USED-FOR batch inverse reinforcement learning. counterfactual reasoning USED-FOR costbenefit tradeoffs. reward functions USED-FOR expert behavior. counterfactuals USED-FOR policy evaluation. batch setting FEATURE-OF policy evaluation. real and simulated medical environments EVALUATE-FOR batch, counterfactual inverse reinforcement learning approach. OtherScientificTerm are unknown reward function, reward function, expert ’s actions, and expert policies. Task are learning explanations of expert decisions, and active experimentation. Material is healthcare. ",This paper proposes a method for learning counterfactual explanations of expert decision-making in the context of batch inverse reinforcement learning (BIRL). The method is based on the idea that the expert’s actions can be used to explain the cost-benefit trade-off between a policy and an expert policy. The authors show that the proposed method outperforms baselines in both simulated and real-world settings. ,This paper proposes a method for learning counterfactual explanations of expert decision-making in the context of batch inverse reinforcement learning (BIRL). The method is based on the idea that the expert’s actions can be used to explain the cost-benefit trade-off between a policy and an expert policy. The authors show that the proposed method outperforms baselines in both simulated and real-world settings. 
2457,SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"generalisation CONJUNCTION data efficiency. data efficiency CONJUNCTION generalisation. data efficiency CONJUNCTION robustness. robustness CONJUNCTION data efficiency. Multitask Reinforcement Learning USED-FOR models. generalisation EVALUATE-FOR models. they USED-FOR graphs. physical morphology USED-FOR graph. edges USED-FOR nodes. morphological information PART-OF graph. AMORPHEUS HYPONYM-OF transformer - based approach. graph structure USED-FOR GNNs. it COMPARE GNN - based methods. GNN - based methods COMPARE it. morphological information USED-FOR message - passing scheme. GNN - based methods USED-FOR message - passing scheme. AMORPHEUS COMPARE it. it COMPARE AMORPHEUS. AMORPHEUS USED-FOR morphological information. morphological information USED-FOR GNN - based methods. OtherScientificTerm are state and action space dimensions, and limb features. Method is Graph Neural Networks ( GNN ). Generic are They, and methods. Task are graph - based continuous control, and message passing. ","This paper proposes a transformer-based approach for continuous control. The proposed method is based on graph neural networks (GNNs) and uses morphological information for message passing. The authors show that the proposed method outperforms existing GNN-based methods in terms of robustness, generalization, and data efficiency. ","This paper proposes a transformer-based approach for continuous control. The proposed method is based on graph neural networks (GNNs) and uses morphological information for message passing. The authors show that the proposed method outperforms existing GNN-based methods in terms of robustness, generalization, and data efficiency. "
2473,SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"modulated convolutions USED-FOR alternative. forward - pass USED-FOR inference. forward - pass USED-FOR MoVie. MoVie USED-FOR counting. module USED-FOR number ’ related questions. number ’ related questions PART-OF generic VQA models. module PART-OF generic VQA models. COCO HYPONYM-OF common object counting. module USED-FOR VQA challenge. counting - specific VQA tasks EVALUATE-FOR MoVie. common object counting HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR prior - art. COCO HYPONYM-OF benchmarks. modulated convolutions USED-FOR reasoning tasks. counting HYPONYM-OF reasoning tasks. MoVie HYPONYM-OF modulated convolutions. Task is visual counting. Material is natural image. Method are explicit, symbolic models, residual bottleneck, and Modulated conVolutional bottlenecks. ","This paper proposes a new method for visual QA (VQA) based on modulated convolutional neural networks (MoVie). MoVie is based on the idea of residual bottlenecks, which can be seen as an alternative to the traditional forward pass. The authors show that the proposed method is able to achieve state-of-the-art performance on COCO and common object counting tasks. ","This paper proposes a new method for visual QA (VQA) based on modulated convolutional neural networks (MoVie). MoVie is based on the idea of residual bottlenecks, which can be seen as an alternative to the traditional forward pass. The authors show that the proposed method is able to achieve state-of-the-art performance on COCO and common object counting tasks. "
2489,SP:c64e77507e562f236cb69361b22fb1a7951ffb22,poisoning attack USED-FOR model. online convex optimization USED-FOR poisoning attack. model - targeted poisoning attacks COMPARE attack. attack COMPARE model - targeted poisoning attacks. provable convergence FEATURE-OF target classifier. provable convergence FEATURE-OF attack. attack HYPONYM-OF model - targeted poisoning attack. it COMPARE attacks. attacks COMPARE it. attack success rate EVALUATE-FOR it. attack success rate EVALUATE-FOR attacks. attack USED-FOR online attack. Method is classifier. ,"This paper proposes an online poisoning attack for model-targeted poisoning attacks. The proposed method is based on online convex optimization. The authors show that the proposed method achieves provable convergence to the target classifier. They also show that their method can be used for online poisoning attacks, and show that it is provably more effective than the existing poisoning attacks in terms of attack success rate. ","This paper proposes an online poisoning attack for model-targeted poisoning attacks. The proposed method is based on online convex optimization. The authors show that the proposed method achieves provable convergence to the target classifier. They also show that their method can be used for online poisoning attacks, and show that it is provably more effective than the existing poisoning attacks in terms of attack success rate. "
2505,SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"BiPointNet HYPONYM-OF model binarization approach. model binarization approach USED-FOR deep learning on point clouds. resource constraint USED-FOR real - time point cloud applications. edge devices USED-FOR real - time point cloud applications. binarized models USED-FOR point clouds. scale distortion USED-FOR optimization. scale distortion HYPONYM-OF challenges. aggregation - induced feature homogenization HYPONYM-OF challenges. Entropy - Maximizing Aggregation ( EMA ) USED-FOR distribution. Layer - wise Scale Recovery ( LSR ) USED-FOR feature representation capacity. Entropy - Maximizing Aggregation ( EMA ) USED-FOR BiPointNet. Layer - wise Scale Recovery ( LSR ) PART-OF BiPointNet. BiPointNet COMPARE full precision counterpart. full precision counterpart COMPARE BiPointNet. binarization methods COMPARE full precision counterpart. full precision counterpart COMPARE binarization methods. BiPointNet COMPARE binarization methods. binarization methods COMPARE BiPointNet. tasks EVALUATE-FOR techniques. speedup CONJUNCTION storage saving. storage saving CONJUNCTION speedup. storage saving EVALUATE-FOR BiPointNet. real - world resource - constrained devices EVALUATE-FOR BiPointNet. speedup EVALUATE-FOR BiPointNet. Metric are information entropy, and maximum information entropy. OtherScientificTerm is scale - sensitive structures. ","This paper proposes a model binarization approach for real-time point cloud applications. The proposed method is based on the idea of entropy-maximizing aggregation (EMA), which aims to improve the performance of binarized models in the presence of scale distortion. The authors show that the proposed method outperforms full-precision methods in terms of speedup and storage saving. They also show that their method is able to achieve better performance than the full precision counterparts.","This paper proposes a model binarization approach for real-time point cloud applications. The proposed method is based on the idea of entropy-maximizing aggregation (EMA), which aims to improve the performance of binarized models in the presence of scale distortion. The authors show that the proposed method outperforms full-precision methods in terms of speedup and storage saving. They also show that their method is able to achieve better performance than the full precision counterparts."
2521,SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"natural language processing tasks EVALUATE-FOR Transformer - based models. self - attention architecture USED-FOR transformer. transformer USED-FOR context - aware representations. trainable memory USED-FOR Transformer model. Memory - augmented neural networks ( MANNs ) USED-FOR representations. general - purpose memory USED-FOR representations. neural architectures USED-FOR representations. neural architectures USED-FOR Memory - augmented neural networks ( MANNs ). general - purpose memory USED-FOR neural architectures. MANNs USED-FOR algorithms. MANNs USED-FOR tasks. question answering CONJUNCTION language modeling. language modeling CONJUNCTION question answering. backpropagation USED-FOR tasks. RNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION RNNs. complexity EVALUATE-FOR RNNs. language modeling HYPONYM-OF tasks. backpropagation USED-FOR MANNs. question answering HYPONYM-OF tasks. complexity EVALUATE-FOR LSTMs. Copy HYPONYM-OF algorithms. memory tokens USED-FOR non - local representations. memory bottleneck USED-FOR global information. dedicated layer USED-FOR memory update. machine translation and language modelling tasks EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR tasks. memory tokens USED-FOR masked language model. it USED-FOR global context. it USED-FOR model. model USED-FOR global context. Method are element - wise representations, Transformer baseline, and memory augmented Transformers. OtherScientificTerm is memory. ",This paper proposes a memory-augmented transformer architecture for Transformer-based models. The authors propose a self-attention architecture that learns a global context-aware representation for each element in a Transformer model. The model is trained with a dedicated layer that stores global information in memory. The proposed architecture is evaluated on GLUE and machine translation and language modelling tasks.,This paper proposes a memory-augmented transformer architecture for Transformer-based models. The authors propose a self-attention architecture that learns a global context-aware representation for each element in a Transformer model. The model is trained with a dedicated layer that stores global information in memory. The proposed architecture is evaluated on GLUE and machine translation and language modelling tasks.
2537,SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,Prototypical Contrastive Learning ( PCL ) HYPONYM-OF unsupervised representation learning method. contrastive learning CONJUNCTION clustering. clustering CONJUNCTION contrastive learning. it USED-FOR semantic structures. PCL USED-FOR instance discrimination. PCL USED-FOR low - level features. low - level features USED-FOR instance discrimination. PCL USED-FOR semantic structures. embedding space FEATURE-OF semantic structures. clustering USED-FOR semantic structures. prototypes USED-FOR maximum - likelihood estimation. maximum - likelihood estimation USED-FOR network parameters. prototypes USED-FOR latent variables. Expectation - Maximization framework USED-FOR maximum - likelihood estimation. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. distribution of prototypes USED-FOR E - step. clustering USED-FOR distribution of prototypes. contrastive learning USED-FOR network. InfoNCE loss USED-FOR contrastive learning. ProtoNCE loss HYPONYM-OF InfoNCE loss. PCL COMPARE instance - wise contrastive learning methods. instance - wise contrastive learning methods COMPARE PCL. low - resource transfer learning EVALUATE-FOR PCL. low - resource transfer learning EVALUATE-FOR instance - wise contrastive learning methods. Task is maximum - likelihood estimation of the network parameters. ,"This paper proposes a new unsupervised representation learning method called Prototype Contrastive Learning (PCL). PCL is based on the idea of prototypical contrastive learning (PCL), which aims to learn low-level features for instance discrimination. PCL uses prototypes as the low level features and uses clustering to learn the distribution of prototypes in the embedding space. The authors show that PCL outperforms instance-wise contrastive methods in low-resource transfer learning. ","This paper proposes a new unsupervised representation learning method called Prototype Contrastive Learning (PCL). PCL is based on the idea of prototypical contrastive learning (PCL), which aims to learn low-level features for instance discrimination. PCL uses prototypes as the low level features and uses clustering to learn the distribution of prototypes in the embedding space. The authors show that PCL outperforms instance-wise contrastive methods in low-resource transfer learning. "
2553,SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,clean images USED-FOR deep neural networks. invisible perturbations FEATURE-OF clean images. block USED-FOR robust features. block USED-FOR adversarial attacks. Orthogonal Multi - Path ( OMP ) block PART-OF neural network. forward learning CONJUNCTION backward correction. backward correction CONJUNCTION forward learning. OMP block USED-FOR neural networks. forward learning USED-FOR OMP block. backward correction USED-FOR OMP block. neural networks USED-FOR features. OMP block USED-FOR features. robustness EVALUATE-FOR neural networks. variety CONJUNCTION accuracy. accuracy CONJUNCTION variety. accuracy EVALUATE-FOR vanilla neural networks. l∞ bound FEATURE-OF white - box PGD attack. accuracy EVALUATE-FOR VGG16. CIFAR10 EVALUATE-FOR vanilla neural networks. OMP block USED-FOR VGG16. OMP block USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. neural networks USED-FOR black - box attacks. white - box and black - box attacks COMPARE adversarial defenders. adversarial defenders COMPARE white - box and black - box attacks. Generic is paths. OtherScientificTerm is orthogonality constraint. ,This paper studies the problem of adversarial robustness of deep neural networks against white-box and black-box PGD attacks. The authors propose an orthogonal multi-path (OMP) block to improve the robustness. The OMP block consists of two components: forward learning and backward correction. The forward learning is based on the orthogonality constraint and the backward correction is a regularization term.  The authors show that the OMP blocks can be used to improve robustness against white box and black box attacks. ,This paper studies the problem of adversarial robustness of deep neural networks against white-box and black-box PGD attacks. The authors propose an orthogonal multi-path (OMP) block to improve the robustness. The OMP block consists of two components: forward learning and backward correction. The forward learning is based on the orthogonality constraint and the backward correction is a regularization term.  The authors show that the OMP blocks can be used to improve robustness against white box and black box attacks. 
2569,SP:776df66274ed12449fde8dcef873a593980f397c,Attention mechanism USED-FOR graph neural networks. graph attention model USED-FOR noisy graphs. self - supervised task USED-FOR edges. attention forms USED-FOR edges. attention forms USED-FOR self - supervised task. expressive attention USED-FOR mislinked neighbors. SuperGAT USED-FOR expressive attention. edges USED-FOR SuperGAT. homophily CONJUNCTION average degree. average degree CONJUNCTION homophily. attention forms CONJUNCTION self - supervision. self - supervision CONJUNCTION attention forms. graph characteristics USED-FOR attention forms. self - supervision HYPONYM-OF graph characteristics. average degree HYPONYM-OF self - supervision. homophily HYPONYM-OF self - supervision. average degree HYPONYM-OF graph characteristics. homophily HYPONYM-OF graph characteristics. recipe USED-FOR attention design. graph characteristics USED-FOR attention design. models COMPARE baselines. baselines COMPARE models. real - world datasets EVALUATE-FOR recipe. recipe USED-FOR models. Method is graph attention. Material is graphs. ,"This paper proposes a new graph attention model, called SuperGAT, for noisy graphs. The proposed method is based on the idea of self-supervision, which is an extension of the self supervision task. The authors also propose a new self-attention form, called expressive attention, which can be used to improve the performance of the proposed method. Experiments show that the proposed approach outperforms baselines on a variety of datasets.","This paper proposes a new graph attention model, called SuperGAT, for noisy graphs. The proposed method is based on the idea of self-supervision, which is an extension of the self supervision task. The authors also propose a new self-attention form, called expressive attention, which can be used to improve the performance of the proposed method. Experiments show that the proposed approach outperforms baselines on a variety of datasets."
2585,SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,Dialogue system USED-FOR medical automatic diagnosis ( DSMAD ). Dialogue system USED-FOR agent. reinforcement learning methods USED-FOR it. Markov decisionmaking process USED-FOR DSMAD. medical rationality FEATURE-OF inquiring process. diagnostic accuracy EVALUATE-FOR DSMAD agents. agent USED-FOR diagnosis. agent USED-FOR diagnosing processes. agent USED-FOR medical application. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. inquiry module USED-FOR symptom - inquiries. cooperative modules PART-OF DSMAD agent. introspective module PART-OF DSMAD agent. introspective module HYPONYM-OF cooperative modules. inquiry module HYPONYM-OF cooperative modules. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. evaluation metrics EVALUATE-FOR DSMAD methods. reliability EVALUATE-FOR DSMAD methods. evaluation metrics EVALUATE-FOR reliability. INS - DS COMPARE methods. methods COMPARE INS - DS. robustness EVALUATE-FOR methods. reliability EVALUATE-FOR methods. reliability CONJUNCTION robustness. robustness CONJUNCTION reliability. robustness EVALUATE-FOR INS - DS. reliability EVALUATE-FOR INS - DS. OtherScientificTerm is inquiring symptoms. Generic is interventions. ,This paper proposes a new method for medical automatic diagnosis (DSMAD) based on reinforcement learning. The proposed method is based on a Markov decision-making (MDP) framework. The authors propose two modules for DSMAD: an inquiry module and an introspective module. The evaluation of the proposed method shows that it outperforms baselines in terms of diagnostic accuracy and robustness.,This paper proposes a new method for medical automatic diagnosis (DSMAD) based on reinforcement learning. The proposed method is based on a Markov decision-making (MDP) framework. The authors propose two modules for DSMAD: an inquiry module and an introspective module. The evaluation of the proposed method shows that it outperforms baselines in terms of diagnostic accuracy and robustness.
2601,SP:10ae09d90d465125433a9b4f15b1405ab017920d,"adaptive batch - wise regularization USED-FOR natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR natural world distribution. fine - grained and long - tailed properties PART-OF natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR adaptive batch - wise regularization. inter - class similarity CONJUNCTION intra - class variations. intra - class variations CONJUNCTION inter - class similarity. task USED-FOR FGVC classifier. attention mechanism USED-FOR discriminative parts. long - tailed distribution of visual classification USED-FOR class imbalance problem. class - balancing strategies CONJUNCTION classifier normalization. classifier normalization CONJUNCTION class - balancing strategies. classifier normalization CONJUNCTION negative gradient of tailed categories. negative gradient of tailed categories CONJUNCTION classifier normalization. adaptive confusion concept USED-FOR problems. BCN term USED-FOR overfitting. network learning USED-FOR cross - entropy loss. class predictions USED-FOR BCN loss. confusion energy - based framework USED-FOR long - tailed scenario. BCN USED-FOR distribution of confusion strength. BCN USED-FOR confusion energy - based framework. extra attention mechanism USED-FOR FGVC model. BCN technique USED-FOR FGVC model. iNaturalist2018 HYPONYM-OF natural world distribution dataset. iNaturalist2018 EVALUATE-FOR approach. natural world distribution dataset EVALUATE-FOR approach. Generic is approaches. OtherScientificTerm are image features of fine details, and tailed and head categories. Material is FGVC datasets. ","This paper proposes a new method for adaptive batch-wise regularization for long-tailed visual classification (FGVC). The method is based on the Batch Confusion Norm (BCN) term, which is a regularization term that can be applied to both fine-grained and long tailed properties of the natural world distribution. The authors show that the BCN term can be used to improve the performance of FGVC models in the long-tail setting. They also show that BCN can also be used for class-balancing and class-normalization. ","This paper proposes a new method for adaptive batch-wise regularization for long-tailed visual classification (FGVC). The method is based on the Batch Confusion Norm (BCN) term, which is a regularization term that can be applied to both fine-grained and long tailed properties of the natural world distribution. The authors show that the BCN term can be used to improve the performance of FGVC models in the long-tail setting. They also show that BCN can also be used for class-balancing and class-normalization. "
2617,SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"Bayesian inference USED-FOR ill - posed nature. ill - posed nature FEATURE-OF inverse reinforcement learning problem. Bayesian inference USED-FOR inverse reinforcement learning problem. reward USED-FOR Bayesian inference. methods COMPARE small tabular setting. small tabular setting COMPARE methods. variational approach USED-FOR latent reward. reward FEATURE-OF approximate posterior distribution. variational approach USED-FOR policy. method USED-FOR Bayesian reward inference. real medical data CONJUNCTION control simulations. control simulations CONJUNCTION real medical data. real medical data USED-FOR method. methods USED-FOR Bayesian reward inference. Method are inner - loop MDP solver, non - Bayesian methods, Approximate Variational Reward Imitation Learning ( AVRIL ), and offline imitation learning algorithms. Task is healthcare. ",This paper proposes a new method for Bayesian imitation learning in the context of inverse reinforcement learning. The proposed method is based on a variational approach to approximate the posterior distribution of the latent reward of the policy. The authors show that the proposed method outperforms existing methods in the small tabular setting. The method is evaluated on a number of real-world medical data and control simulations.,This paper proposes a new method for Bayesian imitation learning in the context of inverse reinforcement learning. The proposed method is based on a variational approach to approximate the posterior distribution of the latent reward of the policy. The authors show that the proposed method outperforms existing methods in the small tabular setting. The method is evaluated on a number of real-world medical data and control simulations.
2633,SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,Search USED-FOR policies. singleand multiagent environments FEATURE-OF policies. prior search approaches USED-FOR partially observable environments. computational cost EVALUATE-FOR hidden information. search procedure USED-FOR partially observable environments. Learned Belief Search ( LBS ) HYPONYM-OF search procedure. supervised task USED-FOR approximate auto - regressive counterfactual belief. supervised task USED-FOR LBS. approximate auto - regressive counterfactual belief USED-FOR LBS. public - private model architecture USED-FOR policies. LBS USED-FOR policies. LBS USED-FOR multi - agent settings. rollouts FEATURE-OF policies. public - private model architecture USED-FOR LBS. Hanabi EVALUATE-FOR LBS. exact search USED-FOR LBS. compute requirements EVALUATE-FOR LBS. OtherScientificTerm is exact belief distribution. Generic is it. Method is search methods. ,This paper proposes a new method for learning a counterfactual belief in a partially observable environment. The proposed method is based on the idea of learning an approximate auto-regressive belief for a given task. The method is evaluated on the Hanabi dataset and is shown to outperform the baselines in terms of accuracy and computational cost. ,This paper proposes a new method for learning a counterfactual belief in a partially observable environment. The proposed method is based on the idea of learning an approximate auto-regressive belief for a given task. The method is evaluated on the Hanabi dataset and is shown to outperform the baselines in terms of accuracy and computational cost. 
2649,SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"MCTS CONJUNCTION random shooting. random shooting CONJUNCTION MCTS. It USED-FOR bias - variance trade - off. Task is Planning in large state spaces. Method are Shoot Tree Search ( STS ), TD(n ), and STS. Generic is algorithm. OtherScientificTerm is tree search context. ","This paper studies the problem of planning in large state spaces. The authors propose a new algorithm, Shoot Tree Search (STS), to solve the problem. The proposed algorithm is based on the notion of bias-variance trade-off, and the authors show that the proposed algorithm outperforms previous work on this problem.","This paper studies the problem of planning in large state spaces. The authors propose a new algorithm, Shoot Tree Search (STS), to solve the problem. The proposed algorithm is based on the notion of bias-variance trade-off, and the authors show that the proposed algorithm outperforms previous work on this problem."
2665,SP:5efc271ccc555fd9aa542548838170bd4c98e957,"transformer networks USED-FOR inductive bias. inductive bias PART-OF neural architectures. transformer networks USED-FOR tasks. tasks USED-FOR inductive bias. datasets USED-FOR inductive bias. induction CONJUNCTION abduction. abduction CONJUNCTION induction. deduction CONJUNCTION induction. induction CONJUNCTION deduction. model USED-FOR synthetic tasks. tasks USED-FOR reasoning biases. Inductive bias USED-FOR Mathematical rEasoning. LIME HYPONYM-OF pre - training methodology. Models COMPARE vanilla transformers. vanilla transformers COMPARE Models. LIME USED-FOR Models. large mathematical reasoning benchmarks EVALUATE-FOR vanilla transformers. large mathematical reasoning benchmarks EVALUATE-FOR Models. computation cost EVALUATE-FOR pre - training approaches. computation cost FEATURE-OF downstream task. pre - training approaches USED-FOR LIME. downstream task USED-FOR LIME. computation cost USED-FOR LIME. Method is architecture engineering. OtherScientificTerm are reasoning primitives, and mathematical knowledge. Generic is they. ","This paper proposes a pre-training method to improve the inductive bias of transformer-based neural networks for mathematical reasoning tasks. The proposed method, LIME, is based on the idea of learning a set of tasks that can be used to train a transformer network for a given task. These tasks are defined as a combination of induction, abduction, deduction, and deduction. The authors show that the proposed method outperforms the baselines on a number of benchmark datasets. ","This paper proposes a pre-training method to improve the inductive bias of transformer-based neural networks for mathematical reasoning tasks. The proposed method, LIME, is based on the idea of learning a set of tasks that can be used to train a transformer network for a given task. These tasks are defined as a combination of induction, abduction, deduction, and deduction. The authors show that the proposed method outperforms the baselines on a number of benchmark datasets. "
2681,SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"gradient descent USED-FOR weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF gradient descent. gradient flow USED-FOR networks. gradient flow path COMPARE gradient flow. gradient flow COMPARE gradient flow path. EWN USED-FOR gradient flow path. adaptive learning rate USED-FOR gradient flow. adaptive learning rate USED-FOR gradient descent. weight normalization ( SWN ) CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION weight normalization ( SWN ). inductive bias CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION inductive bias. inductive bias FEATURE-OF weight normalization ( SWN ). synthetic data sets EVALUATE-FOR unnormalized architectures. simple data sets CONJUNCTION architectures. architectures CONJUNCTION simple data sets. architectures EVALUATE-FOR sparse EWN solutions. SGD USED-FOR sparse EWN solutions. OtherScientificTerm are exponential or cross - entropy loss, radial direction, and asymptotic relative sparsity. Method is exponential weight normalization ( EWN ). Metric is asymptotic convergence rate. Task is learning prunable neural networks. ","This paper studies the asymptotic convergence rate of exponential weight normalization (EWN) for weight normalized smooth homogeneous neural networks. The authors show that EWN converges faster than gradient flow, which is an adaptive learning rate for gradient descent. They also show that SGD can be used to approximate the gradient flow path of EWN, and show that it converges to a stationary point in the radial direction. Finally, they show that the EWN convergence rate can be approximated by SGD. ","This paper studies the asymptotic convergence rate of exponential weight normalization (EWN) for weight normalized smooth homogeneous neural networks. The authors show that EWN converges faster than gradient flow, which is an adaptive learning rate for gradient descent. They also show that SGD can be used to approximate the gradient flow path of EWN, and show that it converges to a stationary point in the radial direction. Finally, they show that the EWN convergence rate can be approximated by SGD. "
2697,SP:c71f9d2a602516865a0b103028186e83b52e5f00,Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. mode collapse FEATURE-OF generator. Catastrophic Forgetting PART-OF continual learning. classification accuracy EVALUATE-FOR discriminator. training procedure USED-FOR discriminators. training scheme USED-FOR mode collapse. metrics EVALUATE-FOR GAN evaluation. GAN frameworks USED-FOR mode collapse. training scheme USED-FOR GAN frameworks. metrics EVALUATE-FOR training scheme. Generic is they. Task is mode collapse problem. Method is data generation procedure. ,This paper studies the problem of mode collapse in GANs. The authors propose a new training scheme to mitigate the mode collapse problem. The proposed training scheme is based on the Catastrophic Forgetting (CFS) method. The paper also proposes a new evaluation metric to evaluate the performance of GAN models. ,This paper studies the problem of mode collapse in GANs. The authors propose a new training scheme to mitigate the mode collapse problem. The proposed training scheme is based on the Catastrophic Forgetting (CFS) method. The paper also proposes a new evaluation metric to evaluate the performance of GAN models. 
2713,SP:52c48198c95826e042f9e5a512ef3265daaff882,"proxy score USED-FOR head importance. proxy score USED-FOR attention heads. AUBER HYPONYM-OF regularization method. attention heads PART-OF BERT. reinforcement learning USED-FOR regularization method. heuristics CONJUNCTION rule - based policies. rule - based policies CONJUNCTION heuristics. pruning policy USED-FOR attention heads. AUBER USED-FOR pruning policy. rule - based policies USED-FOR AUBER. heuristics USED-FOR AUBER. AUBER COMPARE pruning methods. pruning methods COMPARE AUBER. accuracy EVALUATE-FOR pruning methods. accuracy EVALUATE-FOR AUBER. Generic are it, and they. Method is heuristic - based methods. OtherScientificTerm is regularization. ","This paper proposes a reinforcement learning-based regularization method for attention heads in BERT. Specifically, the authors propose to use a proxy score to measure the importance of each attention head. The proxy score is then used to train a pruning policy that prunes the attention heads based on the proxy score. Experiments show that the proposed method outperforms other heuristic-based pruning methods.","This paper proposes a reinforcement learning-based regularization method for attention heads in BERT. Specifically, the authors propose to use a proxy score to measure the importance of each attention head. The proxy score is then used to train a pruning policy that prunes the attention heads based on the proxy score. Experiments show that the proposed method outperforms other heuristic-based pruning methods."
2729,SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"physics parameters CONJUNCTION morphology. morphology CONJUNCTION physics parameters. representation CONJUNCTION physics parameters. physics parameters CONJUNCTION representation. unpaired and randomly collected data USED-FOR correspondences. dynamics cycles USED-FOR dynamic robot behavior. cycle - consistency constraint USED-FOR dynamics cycles. simulation CONJUNCTION real robot. real robot CONJUNCTION simulation. real robot HYPONYM-OF problem domains. simulation HYPONYM-OF problem domains. dynamic state - action trajectories FEATURE-OF simulated arm. framework USED-FOR uncalibrated monocular video. framework USED-FOR dynamic state - action trajectories. real robot arm FEATURE-OF uncalibrated monocular video. Task are robotics problems, imitation learning, sim - to - real, and transfer learning. OtherScientificTerm are physics simulators, and robotics environments. Generic are correspondence, and policy. Method is fine - tuning. Material is paired data. ",This paper proposes a method for learning dynamics cycles for sim-to-real transfer learning. The key idea is to use a cycle-consistency constraint to ensure that the dynamics of the simulation and the real robot are consistent. The authors show that the proposed method is able to learn dynamics cycles that are similar to the dynamics in the real world. They also show that their method can be used for fine-tuning and transfer learning in a variety of robotics environments.,This paper proposes a method for learning dynamics cycles for sim-to-real transfer learning. The key idea is to use a cycle-consistency constraint to ensure that the dynamics of the simulation and the real robot are consistent. The authors show that the proposed method is able to learn dynamics cycles that are similar to the dynamics in the real world. They also show that their method can be used for fine-tuning and transfer learning in a variety of robotics environments.
2745,SP:006434d56992836ab9420d7d4215bc70664de304,"Explainability USED-FOR model development. Explainability USED-FOR AI. solutions USED-FOR Shapley explainability. data manifold USED-FOR solutions. data manifold USED-FOR Shapley explainability. Shapley value - function USED-FOR other. generative modelling USED-FOR solution. unintelligible explanations FEATURE-OF higher - dimensional data. OtherScientificTerm are operational nuance, model ’s features, off - manifold ” Shapley values, implicit model dependence, and sensitive attributes. Method are Shapley framework, and on - manifold explainability. Task is data imputations. ","This paper studies the problem of explainability in the context of generative models. The authors propose a new framework, Shapley explainability, which is based on the notion of off-manifold Shapley values. The proposed framework can be applied to a wide range of applications. The paper also proposes a generative model that can be used to learn Shapley value functions.","This paper studies the problem of explainability in the context of generative models. The authors propose a new framework, Shapley explainability, which is based on the notion of off-manifold Shapley values. The proposed framework can be applied to a wide range of applications. The paper also proposes a generative model that can be used to learn Shapley value functions."
2761,SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,Existing methods USED-FOR opponent modelling. local observations USED-FOR Existing methods. chosen actions CONJUNCTION received rewards. received rewards CONJUNCTION chosen actions. observed world state CONJUNCTION chosen actions. chosen actions CONJUNCTION observed world state. variational autoencoders USED-FOR local actions. local observations USED-FOR embeddings. observed world state HYPONYM-OF local observations. chosen actions HYPONYM-OF local observations. variational autoencoders USED-FOR modelling technique. embeddings USED-FOR modelling agent ’s decision policy. deep reinforcement learning USED-FOR embeddings. deep reinforcement learning USED-FOR modelling agent ’s decision policy. method COMPARE baseline method. baseline method COMPARE method. method COMPARE baseline. baseline COMPARE method. baseline COMPARE baseline method. baseline method COMPARE baseline. opponent ’s information USED-FOR baseline. embeddings USED-FOR baseline method. Generic is policy. OtherScientificTerm is opponent observations. ,"This paper proposes a new method for opponent modelling. The proposed method is based on deep reinforcement learning. The authors propose to use a variational autoencoder to learn the embeddings of the opponent’s actions and rewards, which are then used to train the model. The method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms baselines.","This paper proposes a new method for opponent modelling. The proposed method is based on deep reinforcement learning. The authors propose to use a variational autoencoder to learn the embeddings of the opponent’s actions and rewards, which are then used to train the model. The method is evaluated on a number of benchmark datasets and shows that the proposed method outperforms baselines."
2777,SP:c239bc531bcf7293032748af29a1b786e9d893dd,"Contrastive learning USED-FOR unsupervised visual representation learning. consistency regularization term PART-OF contrastive learning framework. consistency regularization USED-FOR semi - supervised learning. Consistent Contrast ( CO2 ) USED-FOR contrastive learning framework. unlabeled data USED-FOR consistency regularization. unlabeled data USED-FOR semi - supervised learning. consistency regularization term PART-OF Consistent Contrast ( CO2 ). CO2 USED-FOR Momentum Contrast ( MoCo ). top-1 accuracy EVALUATE-FOR Momentum Contrast ( MoCo ). ImageNet linear protocol EVALUATE-FOR CO2. top-1 accuracy EVALUATE-FOR CO2. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. It USED-FOR image classification. It USED-FOR semantic segmentation. It USED-FOR object detection. PASCAL VOC USED-FOR semantic segmentation. PASCAL VOC USED-FOR object detection. PASCAL VOC USED-FOR image classification. visual representations USED-FOR tasks. CO2 USED-FOR visual representations. CO2 USED-FOR tasks. OtherScientificTerm are human annotation, heterogeneous similarity, semantic class, consistency term, and labeled semi - supervised settings. Task is instance discrimination task. Generic is task. Method is label assignment strategy. Metric is top-5 accuracy. ","This paper proposes a new contrastive learning framework for semi-supervised learning. The proposed method, Consistent Contrast (CO2), is based on the consistency regularization term. The authors show that CO2 improves the top-5 accuracy of ImageNet linear protocol and PASCAL VOC for image classification and semantic segmentation tasks.","This paper proposes a new contrastive learning framework for semi-supervised learning. The proposed method, Consistent Contrast (CO2), is based on the consistency regularization term. The authors show that CO2 improves the top-5 accuracy of ImageNet linear protocol and PASCAL VOC for image classification and semantic segmentation tasks."
2793,SP:d18bab21790713e2facb053c47298fc9079ab783,"Optimistic Gradient Descent Ascent ( OGDA ) CONJUNCTION Optimistic Multiplicative Weights Update ( OMWU ). Optimistic Multiplicative Weights Update ( OMWU ) CONJUNCTION Optimistic Gradient Descent Ascent ( OGDA ). Optimistic Multiplicative Weights Update ( OMWU ) USED-FOR saddle - point optimization. Optimistic Gradient Descent Ascent ( OGDA ) USED-FOR saddle - point optimization. uniqueness of the optimal solution HYPONYM-OF assumptions. probability simplex FEATURE-OF bilinear games. OGDA CONJUNCTION OMWU. OMWU CONJUNCTION OGDA. OMWU USED-FOR constrained setting. last - iterate convergence FEATURE-OF OGDA. bilinear games FEATURE-OF OMWU. universal constant FEATURE-OF learning rate. simplex FEATURE-OF bilinear games. learning rate USED-FOR linear last - iterate convergence. constant learning rate FEATURE-OF last - iterate convergence rates. last - iterate convergence rates FEATURE-OF OGDA. condition FEATURE-OF bilinear games. polytope FEATURE-OF bilinear games. condition USED-FOR strongly - convex - stronglyconcave functions. Metric is convergence rates. OtherScientificTerm are exponentially small learning rate, equilibrium, smoothness of the objective function, and unique equilibrium assumption. Method is projected OGDA algorithm. ","This paper studies the convergence of saddle-point optimization algorithms for bilinear games. The authors show that the convergence rate of OGDA and OMWU converges linearly to a universal constant. They also show that for linear last-iterate convergence, the learning rate converges to a constant in the case of strongly-convex-strongly-concave functions.  ","This paper studies the convergence of saddle-point optimization algorithms for bilinear games. The authors show that the convergence rate of OGDA and OMWU converges linearly to a universal constant. They also show that for linear last-iterate convergence, the learning rate converges to a constant in the case of strongly-convex-strongly-concave functions.  "
2809,SP:bbc7f77308b298c332a39747f693bc396f00a89f,"federated setup USED-FOR User Verification ( UV ) models. framework USED-FOR private and secure training of UV models. Federated User Verification ( FedUV ) USED-FOR private and secure training of UV models. secret user - defined linear combination USED-FOR instance embeddings. FedUV COMPARE approaches. approaches COMPARE FedUV. voice, face, and handwriting data USED-FOR user verification. Method are loss functions, and UV models. OtherScientificTerm are user embeddings, linear combinations, error - correcting code, embedding vectors, and embeddings. Generic is model. ","This paper proposes a federated setup for user verification (UV) models. The proposed method is based on the Federated User Verification (FedUV) framework. In FedUV, each user is given a set of instance embeddings, and the goal is to verify the correctness of the embedding vectors of each instance. The method is evaluated on voice, face, and handwriting data.","This paper proposes a federated setup for user verification (UV) models. The proposed method is based on the Federated User Verification (FedUV) framework. In FedUV, each user is given a set of instance embeddings, and the goal is to verify the correctness of the embedding vectors of each instance. The method is evaluated on voice, face, and handwriting data."
2825,SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"geometry FEATURE-OF class manifolds ( CMs ). geometry FEATURE-OF model. technique USED-FOR boundaries. technique USED-FOR CMs. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. geometry of CMs CONJUNCTION generalization. generalization CONJUNCTION geometry of CMs. label randomization CONJUNCTION training set size. training set size CONJUNCTION label randomization. ensemble size CONJUNCTION label randomization. label randomization CONJUNCTION ensemble size. stage of training CONJUNCTION class. class CONJUNCTION stage of training. training set size CONJUNCTION model robustness. model robustness CONJUNCTION training set size. architecture CONJUNCTION random initialization. random initialization CONJUNCTION architecture. random initialization CONJUNCTION stage of training. stage of training CONJUNCTION random initialization. class CONJUNCTION ensemble size. ensemble size CONJUNCTION class. data corruption FEATURE-OF model robustness. dataset USED-FOR CM dimension. CMs USED-FOR ensembling. Method are Deep neural network classifiers, and real neural networks. OtherScientificTerm are margin, and random affine subspaces. Generic are method, and models. ","This paper studies the problem of learning class manifolds (CMs) for deep neural network classifiers. The authors propose a new method to learn CMs by using random affine subspaces of the class space. They show that this method is able to learn a class manifold that is more robust to data corruption and robustness to model robustness. They also show that the proposed method can be used to train a classifier that is robust to label randomization, class size, and ensemble size. ","This paper studies the problem of learning class manifolds (CMs) for deep neural network classifiers. The authors propose a new method to learn CMs by using random affine subspaces of the class space. They show that this method is able to learn a class manifold that is more robust to data corruption and robustness to model robustness. They also show that the proposed method can be used to train a classifier that is robust to label randomization, class size, and ensemble size. "
2841,SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. RL methods USED-FOR problem. action noise USED-FOR policies. entropy FEATURE-OF policy. entropy temperature FEATURE-OF policy. entropy temperature USED-FOR entropy. Soft Actor - Critic ( SAC ) HYPONYM-OF policies. Curiosity - Aware entropy Temperature USED-FOR SAC ( CAT - SAC ). curiosity mechanism USED-FOR instance - level entropy temperature. curiosity mechanism USED-FOR Curiosity - Aware entropy Temperature. state prediction error USED-FOR curiosity. state prediction error USED-FOR CAT - SAC. entropy USED-FOR CAT - SAC. MuJoCo benchmark EVALUATE-FOR CAT - SAC. sample efficiency EVALUATE-FOR CAT - SAC. Task is reinforcement learning ( RL ). Generic is temperature. OtherScientificTerm are environment states, prediction error, and unfamiliar states. ",This paper proposes a new method for learning a policy that is able to learn an entropy-aware policy that can be used for exploration and exploitation in reinforcement learning. The proposed method is based on the observation that the entropy of a policy is a function of the state prediction error and the curiosity of the policy. The authors propose a curiosity-aware entropy temperature (CAT-SAC) method to learn such a policy. They show that the proposed method outperforms existing methods on the MuJoCo benchmark. ,This paper proposes a new method for learning a policy that is able to learn an entropy-aware policy that can be used for exploration and exploitation in reinforcement learning. The proposed method is based on the observation that the entropy of a policy is a function of the state prediction error and the curiosity of the policy. The authors propose a curiosity-aware entropy temperature (CAT-SAC) method to learn such a policy. They show that the proposed method outperforms existing methods on the MuJoCo benchmark. 
2857,SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"Reinforcement learning algorithms USED-FOR policies. meta - reinforcement learning methods USED-FOR agents. model identification CONJUNCTION experience relabeling ( MIER ). experience relabeling ( MIER ) CONJUNCTION model identification. experience relabeling ( MIER ) HYPONYM-OF meta - reinforcement learning algorithm. meta - reinforcement learning algorithm USED-FOR out - of - distribution tasks. policies CONJUNCTION value functions. value functions CONJUNCTION policies. dynamics models COMPARE value functions. value functions COMPARE dynamics models. dynamics models COMPARE policies. policies COMPARE dynamics models. off - policy data USED-FOR dynamics models. synthetic experience USED-FOR task. dynamics models USED-FOR policies. Generic are approaches, and method. Method are on - policy meta - training, and meta - reinforcement learning. ",This paper proposes a meta-reinforcement learning algorithm for out-of-distribution tasks. The proposed method is based on experience relabeling (MIER) and model identification. Experiments show that the proposed method outperforms the baselines. ,This paper proposes a meta-reinforcement learning algorithm for out-of-distribution tasks. The proposed method is based on experience relabeling (MIER) and model identification. Experiments show that the proposed method outperforms the baselines. 
2873,SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"meta - learning techniques USED-FOR few - shot learning ( FSL ) problem. label noise USED-FOR FSL. label noise FEATURE-OF meta - learner. gradient noise problem USED-FOR meta - overfitting problem. Eigen - Reptile ( ER ) USED-FOR gradient noise. Eigen - Reptile ( ER ) USED-FOR meta - parameters. historical taskspecific parameters USED-FOR gradient noise. historical taskspecific parameters USED-FOR Eigen - Reptile ( ER ). Introspective Self - paced Learning ( ISPL ) USED-FOR prior models. Eigen - Reptile CONJUNCTION ISPL. ISPL CONJUNCTION Eigen - Reptile. tasks EVALUATE-FOR methods. methods COMPARE state - of - the - art methods. state - of - the - art methods COMPARE methods. tasks EVALUATE-FOR state - of - the - art methods. noisy labels USED-FOR state - of - the - art methods. OtherScientificTerm are meta - overfit, sampling noise, and gradient step. Task is overfitting. ",This paper studies the problem of few-shot learning (FSL) with noisy labels. The authors propose a new method called Eigen-Reptile (ER) to mitigate the overfitting problem in the meta-learning setting. The proposed method is based on the idea that the gradient noise of a meta-learner can be decomposed into two parts: (1) the label noise and (2) the sampling noise of the gradient step. The paper shows that the proposed method outperforms state-of-the-art methods on a number of tasks. ,This paper studies the problem of few-shot learning (FSL) with noisy labels. The authors propose a new method called Eigen-Reptile (ER) to mitigate the overfitting problem in the meta-learning setting. The proposed method is based on the idea that the gradient noise of a meta-learner can be decomposed into two parts: (1) the label noise and (2) the sampling noise of the gradient step. The paper shows that the proposed method outperforms state-of-the-art methods on a number of tasks. 
2889,SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"Adversarial training USED-FOR models. feature statistics COMPARE image pixels. image pixels COMPARE feature statistics. distributional shifts USED-FOR models. adversarially crafted distributions USED-FOR images. Stylized - ImageNet CONJUNCTION ImageNetInstagram. ImageNetInstagram CONJUNCTION Stylized - ImageNet. AdvBN USED-FOR semantic segmentation. generalization EVALUATE-FOR semantic segmentation. generalization EVALUATE-FOR AdvBN. goldfinch CONJUNCTION sulphur butterfly. sulphur butterfly CONJUNCTION goldfinch. goldfinch CONJUNCTION bulbul. bulbul CONJUNCTION goldfinch. hummingbird CONJUNCTION goldfinch. goldfinch CONJUNCTION hummingbird. house finch CONJUNCTION goldfinch. goldfinch CONJUNCTION house finch. bulbul CONJUNCTION house finch. house finch CONJUNCTION bulbul. brambling CONJUNCTION guillotine. guillotine CONJUNCTION brambling. bolete CONJUNCTION fox squirrel. fox squirrel CONJUNCTION bolete. fox squirrel CONJUNCTION hen - of - the - woods. hen - of - the - woods CONJUNCTION fox squirrel. gong CONJUNCTION bolete. bolete CONJUNCTION gong. Ibizan hound CONJUNCTION flamingo. flamingo CONJUNCTION Ibizan hound. ResNet-50 model USED-FOR classification scores. OtherScientificTerm are small adversarial perturbations, and mean and variance of deep image features. Method are adversarial training, Adversarial Batch Normalization ( AdvBN ), ResNet-50, ImageNet variants, and Adversarial Batch Normalization module. Generic is method. ","This paper proposes Adversarial batch normalization (Adversarial Batch Normalization (AdvBN), a method to improve the generalization performance of image classification models. AdvBN is based on the idea that adversarial perturbations can cause distributional shifts in the training data, which can lead to distributional shift in the classification scores. The authors show that AdvBN improves the classification performance of ResNet-50 and ImageNet variants on ImageNet and Stylized-ImageNet. They also show the effectiveness of AdvBN on semantic segmentation.","This paper proposes Adversarial batch normalization (Adversarial Batch Normalization (AdvBN), a method to improve the generalization performance of image classification models. AdvBN is based on the idea that adversarial perturbations can cause distributional shifts in the training data, which can lead to distributional shift in the classification scores. The authors show that AdvBN improves the classification performance of ResNet-50 and ImageNet variants on ImageNet and Stylized-ImageNet. They also show the effectiveness of AdvBN on semantic segmentation."
2905,SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"proxy metric USED-FOR outliers. outliers PART-OF data distribution. Variance of Gradients ( VoG ) HYPONYM-OF proxy metric. Task are machine learning, and human - in - the - loop auditing. Method are model, models, and VoG. OtherScientificTerm is VoG scores. ","This paper proposes a proxy metric for outliers in machine learning, called Variance of Gradients (VoG). The proposed metric is based on the observation that outliers are more likely to exist in the training data than in the test data. The authors show that VoG can be used to identify outliers that are not present in training data. They also show that the VoG score is a good proxy for out-of-distribution (OOD) outliers. ","This paper proposes a proxy metric for outliers in machine learning, called Variance of Gradients (VoG). The proposed metric is based on the observation that outliers are more likely to exist in the training data than in the test data. The authors show that VoG can be used to identify outliers that are not present in training data. They also show that the VoG score is a good proxy for out-of-distribution (OOD) outliers. "
2930,SP:074bfacc75837bb19049be8a2890e10de073dd8e,"real - world data FEATURE-OF simulated samples. images HYPONYM-OF simulated samples. generation quality EVALUATE-FOR model. technique USED-FOR generated samples. non - linear Fokker - Plank equation USED-FOR gradient flow. wasteful sample rejection USED-FOR methods. DRS HYPONYM-OF methods. refinement approach USED-FOR GANs. VAEs CONJUNCTION Normalizing Flows. Normalizing Flows CONJUNCTION VAEs. GANs CONJUNCTION deep generative models. deep generative models CONJUNCTION GANs. vector - valued critics CONJUNCTION deep generative models. deep generative models CONJUNCTION vector - valued critics. vector - valued critics USED-FOR GANs. Normalizing Flows HYPONYM-OF deep generative models. VAEs HYPONYM-OF deep generative models. DGf low COMPARE Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods COMPARE DGf low. Discriminator Optimal Transport ( DOT ) CONJUNCTION Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods CONJUNCTION Discriminator Optimal Transport ( DOT ). quality of generated samples EVALUATE-FOR generative models. synthetic, image, and text datasets EVALUATE-FOR DGf low. DGf low USED-FOR generative models. DGf low COMPARE Discriminator Optimal Transport ( DOT ). Discriminator Optimal Transport ( DOT ) COMPARE DGf low. quality of generated samples EVALUATE-FOR DGf low. Method are Deep generative modeling, Discriminator Gradient f low ( DGf low ), McKean - Vlasov process, and GAN variants. OtherScientificTerm are entropy - regularized f -divergences, and real and the generated data distributions. ","This paper proposes a new method to improve the quality of generated samples for deep generative models (GANs, VAEs, and Normalizing Flows). The proposed method, Discriminator Gradient f low (DGf low), is based on the non-linear Fokker-Plank equation (Fokker Plank equation). The authors show that DGf low can improve the generation quality of the generated samples compared to existing methods. The authors also show that the proposed method is more efficient than the existing methods in terms of the number of samples generated. ","This paper proposes a new method to improve the quality of generated samples for deep generative models (GANs, VAEs, and Normalizing Flows). The proposed method, Discriminator Gradient f low (DGf low), is based on the non-linear Fokker-Plank equation (Fokker Plank equation). The authors show that DGf low can improve the generation quality of the generated samples compared to existing methods. The authors also show that the proposed method is more efficient than the existing methods in terms of the number of samples generated. "
2955,SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,encoder - only Transformer CONJUNCTION encoder - decoder Transformer. encoder - decoder Transformer CONJUNCTION encoder - only Transformer. encoder - decoder Transformer USED-FOR generation tasks. encoder - only Transformer USED-FOR understanding tasks. tasks CONJUNCTION frameworks. frameworks CONJUNCTION tasks. encoder - decoder Transformer USED-FOR understanding tasks. model architectures CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION model architectures. sub - modules USED-FOR understanding and generation tasks. inference USED-FOR understanding and generation tasks. sub - modules PART-OF Transformer block. sub - modules PART-OF VECO. innersequence and cross - sequence masked language modeling USED-FOR sub - modules. sequence labeling CONJUNCTION question answering. question answering CONJUNCTION sequence labeling. question answering CONJUNCTION sentence retrieval. sentence retrieval CONJUNCTION question answering. text classification CONJUNCTION sequence labeling. sequence labeling CONJUNCTION text classification. XTREME benchmark EVALUATE-FOR cross - lingual understanding tasks. cross - lingual understanding tasks EVALUATE-FOR VECO. text classification HYPONYM-OF XTREME benchmark. text classification HYPONYM-OF cross - lingual understanding tasks. sentence retrieval HYPONYM-OF cross - lingual understanding tasks. sequence labeling HYPONYM-OF cross - lingual understanding tasks. XTREME benchmark EVALUATE-FOR VECO. question answering HYPONYM-OF cross - lingual understanding tasks. VECO COMPARE Transformer variants. Transformer variants COMPARE VECO. VECO COMPARE cross - lingual models. cross - lingual models COMPARE VECO. cross - lingual models CONJUNCTION Transformer variants. Transformer variants CONJUNCTION cross - lingual models. generation tasks EVALUATE-FOR Transformer variants. VECO USED-FOR generation tasks. cross - lingual models USED-FOR generation tasks. Method is multilingual representations. Task is downstream cross - lingual tasks. ,"This paper proposes a transformer-based model for cross-lingual understanding and generation tasks. The proposed model, called VECO, consists of three modules: (1) an encoder-decoder Transformer, (2) a decoder-only transformer, and (3) an inner-sequence masked language modeling module. The model is evaluated on the XTREME benchmark and is shown to outperform the state-of-the-art models on several downstream tasks. ","This paper proposes a transformer-based model for cross-lingual understanding and generation tasks. The proposed model, called VECO, consists of three modules: (1) an encoder-decoder Transformer, (2) a decoder-only transformer, and (3) an inner-sequence masked language modeling module. The model is evaluated on the XTREME benchmark and is shown to outperform the state-of-the-art models on several downstream tasks. "
2980,SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"intrinsic motivation USED-FOR Reinforcement Learning ( RL ). K - means USED-FOR auditory event clusters. neural network USED-FOR auditory events. prediction errors USED-FOR intrinsic rewards. Atari games USED-FOR module. model USED-FOR audio - visual exploration. model USED-FOR active learning. Habitat simulator CONJUNCTION active learning. active learning CONJUNCTION Habitat simulator. Habitat simulator USED-FOR model. Habitat simulator USED-FOR audio - visual exploration. ThreeDWorld ( TDW ) simulator USED-FOR active learning. audio signals USED-FOR intrinsic rewards. vision - based models USED-FOR intrinsic rewards. vision - based models USED-FOR RL explorations. audio signals USED-FOR RL explorations. audio signals COMPARE vision - based models. vision - based models COMPARE audio signals. Task are causal understanding of the physical world, and RL exploration. Method is auditory event prediction. Material is acoustic data. ","This paper proposes a method for learning intrinsic rewards for audio-visual exploration in reinforcement learning (RL). The method is based on a neural network that predicts the K-means of auditory event clusters, which are then used to predict the intrinsic rewards. The authors show that the proposed method outperforms vision-based models on the Habitat simulator and the ThreeDWorld (TDW) simulator. ","This paper proposes a method for learning intrinsic rewards for audio-visual exploration in reinforcement learning (RL). The method is based on a neural network that predicts the K-means of auditory event clusters, which are then used to predict the intrinsic rewards. The authors show that the proposed method outperforms vision-based models on the Habitat simulator and the ThreeDWorld (TDW) simulator. "
3005,SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,singleand multimodal data USED-FOR category discovery. end - to - end framework USED-FOR representation. unlabelled data USED-FOR clusters. it USED-FOR labelled and unlabelled data. noise - contrastive estimation USED-FOR self - supervised representation learning. category discrimination CONJUNCTION cross - modal discrimination. cross - modal discrimination CONJUNCTION category discrimination. instance discrimination USED-FOR contrastive learning approaches. cross - modal discrimination USED-FOR instance discrimination. category discrimination USED-FOR instance discrimination. cross - modal discrimination USED-FOR multi - modal data. category discrimination CONJUNCTION labelled data. labelled data CONJUNCTION category discrimination. pairwise pseudo labels USED-FOR unlabelled data. pairwise pseudo labels USED-FOR cluster assignments. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. Kinetics-400 CONJUNCTION VGG - Sound. VGG - Sound CONJUNCTION Kinetics-400. image benchmarks CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION image benchmarks. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. CIFAR10 HYPONYM-OF image benchmarks. ImageNet HYPONYM-OF image benchmarks. image benchmarks EVALUATE-FOR framework. OtherScientificTerm is shared representation space. ,This paper proposes an end-to-end framework for self-supervised representation learning for multi-modal data. The proposed framework is based on the idea of instance discrimination and category discrimination. The authors show that instance discrimination can be applied to both labelled and unlabelled data. They also show that the proposed method can be used for instance discrimination. ,This paper proposes an end-to-end framework for self-supervised representation learning for multi-modal data. The proposed framework is based on the idea of instance discrimination and category discrimination. The authors show that instance discrimination can be applied to both labelled and unlabelled data. They also show that the proposed method can be used for instance discrimination. 
3030,SP:4df640f502e88ddba2d7e183625231d70b083e82,"image - level tags CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION image - level tags. image - level tags HYPONYM-OF partial annotations. object bounding boxes HYPONYM-OF partial annotations. broad region coverage FEATURE-OF sparse annotations. Class activation maps USED-FOR coarse labels. conditional random fields USED-FOR sparse labels. Class activation maps USED-FOR segmentation model. Existing methods USED-FOR weak supervision. Class activation maps CONJUNCTION conditional random fields. conditional random fields CONJUNCTION Class activation maps. semi - supervised metric learning problem USED-FOR weakly supervised segmentation. semantic annotation CONJUNCTION co - occurrence. co - occurrence CONJUNCTION semantic annotation. low - level image similarity CONJUNCTION semantic annotation. semantic annotation CONJUNCTION low - level image similarity. co - occurrence CONJUNCTION feature affinity. feature affinity CONJUNCTION co - occurrence. contrastive relationships USED-FOR low - level image similarity. contrastive relationships USED-FOR semantic annotation. partial annotations USED-FOR pixel - wise feature. data - driven grouping CONJUNCTION discriminative feature learning. discriminative feature learning CONJUNCTION data - driven grouping. Pascal VOC EVALUATE-FOR universal weakly supervised segmenter. Task are Weakly supervised segmentation, and pixel localization. Generic are task, and code. OtherScientificTerm are coarse annotations, feature space, and priors. ","This paper proposes a new method for weakly supervised segmentation based on semi-supervised metric learning. The proposed method is based on the idea of class activation maps and conditional random fields. The authors show that the proposed method outperforms existing methods in terms of low-level image similarity, co-occurrence, and feature affinity. They also show that their method can be applied to the task of pixel localization.","This paper proposes a new method for weakly supervised segmentation based on semi-supervised metric learning. The proposed method is based on the idea of class activation maps and conditional random fields. The authors show that the proposed method outperforms existing methods in terms of low-level image similarity, co-occurrence, and feature affinity. They also show that their method can be applied to the task of pixel localization."
3055,SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"slow convergence FEATURE-OF pretext task. small scale models COMPARE supervised counterpart. supervised counterpart COMPARE small scale models. distillation strategy USED-FOR unsupervised learning. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. top-1 accuracies EVALUATE-FOR linear evaluation. BINGO COMPARE baselines. baselines COMPARE BINGO. ImageNet EVALUATE-FOR linear evaluation. small scale models EVALUATE-FOR BINGO. top-1 accuracies EVALUATE-FOR baselines. top-1 accuracies EVALUATE-FOR BINGO. Method are self - supervised learning, contrastive learning based methods, and distillation. Task are optimization, and Bag of InstaNces aGgregatiOn. Generic is method. OtherScientificTerm is bag of instances. ",This paper studies the problem of self-supervised learning in the context of the Bag of InstaNces aGgregatiOn (BINGO) task. BINGO is a distillation strategy that uses contrastive learning to learn a bag of instances. The authors show that the proposed method outperforms the state-of-the-art baselines.,This paper studies the problem of self-supervised learning in the context of the Bag of InstaNces aGgregatiOn (BINGO) task. BINGO is a distillation strategy that uses contrastive learning to learn a bag of instances. The authors show that the proposed method outperforms the state-of-the-art baselines.
3071,SP:328866aad6544c81ded8980934df31dc4472435f,"Simulation - based inference ( SBI ) HYPONYM-OF statistical inference. stochastic models USED-FOR statistical inference. SBI algorithms COMPARE generative adversarial networks ( GANs ). generative adversarial networks ( GANs ) COMPARE SBI algorithms. adversarial approach USED-FOR SBI. GATSBI HYPONYM-OF adversarial approach. SBI CONJUNCTION GANs. GANs CONJUNCTION SBI. GATSBI USED-FOR variational objective. variational objective USED-FOR implicit posterior distributions. adversarial setting FEATURE-OF variational objective. high - dimensional posterior spaces USED-FOR Inference. GATSBI USED-FOR Inference. implicit priors USED-FOR Inference. SBI benchmark problems CONJUNCTION high - dimensional simulators. high - dimensional simulators CONJUNCTION SBI benchmark problems. SBI benchmark problems EVALUATE-FOR GATSBI. high - dimensional simulators EVALUATE-FOR GATSBI. GATSBI USED-FOR well - calibrated posterior estimates. model USED-FOR wave propagation. surface of a shallow water body FEATURE-OF wave propagation. high dimensions FEATURE-OF well - calibrated posterior estimates. it USED-FOR high - dimensional posterior. it COMPARE SBI approach. SBI approach COMPARE it. model of camera optics USED-FOR it. implicit prior USED-FOR it. implicit prior USED-FOR high - dimensional posterior. GATSBI USED-FOR sequential posterior estimation. GANs USED-FOR Bayesian inference. GATSBI USED-FOR Bayesian inference. high - dimensional simulation - based models USED-FOR Bayesian inference. GANs USED-FOR GATSBI. OtherScientificTerm are likelihoods, and explicit likelihoods. ",This paper proposes a new adversarial approach for simulation-based inference (SBI) called GATSBI (Generative Adversarial Inference for Simulated Bayesian Inference). The proposed approach is based on a variational objective that is used to estimate the implicit posterior distribution of the posterior of the model. The authors show that the proposed approach outperforms existing SBI baselines on a variety of SBI benchmarks. ,This paper proposes a new adversarial approach for simulation-based inference (SBI) called GATSBI (Generative Adversarial Inference for Simulated Bayesian Inference). The proposed approach is based on a variational objective that is used to estimate the implicit posterior distribution of the posterior of the model. The authors show that the proposed approach outperforms existing SBI baselines on a variety of SBI benchmarks. 
3087,SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"latent variable USED-FOR prognostic score. prognostic score USED-FOR biostatistics. prognostic score USED-FOR TEs. model USED-FOR individualized treatment effects. latent variable USED-FOR prognostic score. method COMPARE methods. methods COMPARE method. ( semi-)synthetic datasets EVALUATE-FOR method. ( semi-)synthetic datasets EVALUATE-FOR methods. Task is causal inference. OtherScientificTerm are limited overlap, features, TE error bounds, and individualized features. Method are generative prognostic model, and variational autoencoder ( VAE ). ",This paper proposes a generative prognostic model for treatment effect estimation (TE) based on a variational autoencoder (VAE) model. The proposed method is based on the VAE model and is able to learn a prognostic score for individualized treatment effects (TEs). The authors show that the proposed method outperforms existing methods on a number of synthetic and real-world datasets. ,This paper proposes a generative prognostic model for treatment effect estimation (TE) based on a variational autoencoder (VAE) model. The proposed method is based on the VAE model and is able to learn a prognostic score for individualized treatment effects (TEs). The authors show that the proposed method outperforms existing methods on a number of synthetic and real-world datasets. 
3103,SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"benchmark tasks PART-OF RL. RL algorithms USED-FOR episodic simulated environments. RL algorithms USED-FOR them. real - world platforms USED-FOR them. robots HYPONYM-OF real - world platforms. framework PART-OF simulated benchmark EARL1. simulated tasks PART-OF simulated benchmark EARL1. algorithms USED-FOR reinforcement learning. approaches USED-FOR episodic RL. approaches COMPARE approaches. approaches COMPARE approaches. autonomy FEATURE-OF algorithms. Method are Reinforcement learning ( RL ), and real - world embodied learning. OtherScientificTerm are human supervision, extrinsic intervention, and interventions. ","This paper proposes a new benchmark for episodic reinforcement learning. The benchmark is based on a simulated environment where the agent has access to a set of episodic tasks, and the agent is able to explore the environment in an episodic way. The authors show that the proposed benchmark can be used to evaluate the performance of different RL algorithms on a variety of tasks, including reinforcement learning, reinforcement learning with extrinsic intervention, and reinforcement learning without intervention. The proposed benchmark is evaluated on a number of real-world environments, and compared to several baselines. ","This paper proposes a new benchmark for episodic reinforcement learning. The benchmark is based on a simulated environment where the agent has access to a set of episodic tasks, and the agent is able to explore the environment in an episodic way. The authors show that the proposed benchmark can be used to evaluate the performance of different RL algorithms on a variety of tasks, including reinforcement learning, reinforcement learning with extrinsic intervention, and reinforcement learning without intervention. The proposed benchmark is evaluated on a number of real-world environments, and compared to several baselines. "
3119,SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"Question Answering ( QA ) HYPONYM-OF AI and NLP fields. human - level reasoning capability FEATURE-OF QA systems. modules USED-FOR reasoning. QA systems USED-FOR human reasoning process. modules USED-FOR QA systems. Graph Neural Networks ( GNNs ) USED-FOR modules. knowledge graphs ( KGs ) USED-FOR reasoning. pre - trained language models ( LMs ) USED-FOR QA systems. reasoning functionality FEATURE-OF GNN - based modules. GNN - based modules USED-FOR reasoning process. they USED-FOR QA. GNN modules USED-FOR QA. reasoning capability FEATURE-OF GNN modules. CommonsenseQA CONJUNCTION OpenBookQA. OpenBookQA CONJUNCTION CommonsenseQA. graph neural counter COMPARE GNN modules. GNN modules COMPARE graph neural counter. OpenBookQA HYPONYM-OF QA benchmark datasets. knowledge - aware reasoning USED-FOR QA benchmark datasets. OpenBookQA EVALUATE-FOR GNN modules. CommonsenseQA EVALUATE-FOR GNN modules. knowledge - aware GNN modules USED-FOR reasoning. counting HYPONYM-OF reasoning. reasoning modules USED-FOR knowledge - powered QA. Method are LMs, and GNN. ","This paper presents a method for learning knowledge-based reasoning modules for QA systems. The method is based on graph neural networks (GNNs), which can be seen as a combination of graph neural counter (GNC) and knowledge graph (KG). The authors show that GNNs are able to learn reasoning modules that can be used for knowledge-powered QA. The authors also show that the GNN-based modules can be combined with knowledge graph-based QA modules to improve the QA performance. ","This paper presents a method for learning knowledge-based reasoning modules for QA systems. The method is based on graph neural networks (GNNs), which can be seen as a combination of graph neural counter (GNC) and knowledge graph (KG). The authors show that GNNs are able to learn reasoning modules that can be used for knowledge-powered QA. The authors also show that the GNN-based modules can be combined with knowledge graph-based QA modules to improve the QA performance. "
3135,SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. pruning HYPONYM-OF Deep Neural Networks ( DNN ) compression. quantization HYPONYM-OF Deep Neural Networks ( DNN ) compression. low - cost devices USED-FOR them. performance CONJUNCTION space consumption. space consumption CONJUNCTION performance. three - stage framework USED-FOR DNN inference. Succinct Compression HYPONYM-OF three - stage framework. Succinct Compression USED-FOR DNN inference. near - optimal compression FEATURE-OF DNN inference. Succinct Data Structures USED-FOR fast queries. compressed representation USED-FOR fast queries. Succinct Data Structures USED-FOR method. method USED-FOR DNN models. formulations USED-FOR DNN models. Element - wise or Block - wise manner FEATURE-OF formulations. method USED-FOR transformed DNN models. Succinct Data Structures USED-FOR method. Succinct Data Structures USED-FOR transformed DNN models. execution pipelines USED-FOR model formulations. method USED-FOR DNN inference. execution pipelines USED-FOR method. method COMPARE Huffman Coding. Huffman Coding COMPARE method. AlexNet / VGG-16 inference EVALUATE-FOR Huffman Coding. near - optimal compression FEATURE-OF method. AlexNet / VGG-16 inference EVALUATE-FOR method. Pruning CONJUNCTION Quantization. Quantization CONJUNCTION Pruning. method COMPARE Quantization. Quantization COMPARE method. method CONJUNCTION Pruning. Pruning CONJUNCTION method. Generic is techniques. OtherScientificTerm is inference runtime. ,"This paper proposes a three-stage framework for Deep Neural Network (DNN) compression. The proposed method is based on the idea of Succinct Compression (SCC), which is an extension of Quantization (Q) and Pruning (P). The authors show that the proposed method achieves near-optimal compression for DNN inference. The authors also show that their method can be applied to a variety of DNN models. ","This paper proposes a three-stage framework for Deep Neural Network (DNN) compression. The proposed method is based on the idea of Succinct Compression (SCC), which is an extension of Quantization (Q) and Pruning (P). The authors show that the proposed method achieves near-optimal compression for DNN inference. The authors also show that their method can be applied to a variety of DNN models. "
3151,SP:94c395afc794a9cc163e362078769ff83f3d20d0,"training method USED-FOR tiny neural networks. Network Augmentation ( NetAug ) HYPONYM-OF training method. data augmentation CONJUNCTION dropout. dropout CONJUNCTION data augmentation. noise USED-FOR over - fitting. regularization techniques USED-FOR large neural networks. dropout HYPONYM-OF regularization techniques. data augmentation HYPONYM-OF regularization techniques. techniques USED-FOR tiny neural networks. tiny models COMPARE large models. large models COMPARE tiny models. under - fitting CONJUNCTION over - fitting. over - fitting CONJUNCTION under - fitting. NetAug USED-FOR network ( reverse dropout ). It USED-FOR tiny model. tiny model PART-OF larger models. tiny model USED-FOR inference. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. NetAug USED-FOR image classification. NetAug USED-FOR object detection. ImageNet CONJUNCTION Cars. Cars CONJUNCTION ImageNet. NetAug USED-FOR tiny models. ImageNet EVALUATE-FOR NetAug. Pascal VOC EVALUATE-FOR NetAug. computational cost EVALUATE-FOR NetAug. Generic are model, and it. OtherScientificTerm are limited capacity, network, supervision, and inference overhead. Method is independent model. ","This paper proposes a new training method for tiny neural networks. The proposed method, called Network Augmentation (NetAug), is based on the reverse dropout (RDE) technique. The authors show that the proposed method is able to improve the performance of the tiny models in terms of under-fitting and over-fitting. ","This paper proposes a new training method for tiny neural networks. The proposed method, called Network Augmentation (NetAug), is based on the reverse dropout (RDE) technique. The authors show that the proposed method is able to improve the performance of the tiny models in terms of under-fitting and over-fitting. "
3167,SP:9c24549b980e415616f818acbf4cf680ef8edb52,"Point cloud sequence HYPONYM-OF data representation. flexible shape and motion information FEATURE-OF data representation. model USED-FOR temporally coherent feature spaces. real - world environments FEATURE-OF point correspondence information. generator USED-FOR temporally coherent output. point cloud sequence USED-FOR temporal coherence. learnable masking module USED-FOR upsampling ratio. point distribution USED-FOR learnable masking module. point distribution USED-FOR upsampling ratio. fluid dynamical system CONJUNCTION human action scanned data. human action scanned data CONJUNCTION fluid dynamical system. particles CONJUNCTION human action scanned data. human action scanned data CONJUNCTION particles. particles PART-OF fluid dynamical system. domains FEATURE-OF point cloud sequences. particles HYPONYM-OF point cloud sequences. fluid dynamical system HYPONYM-OF domains. human action scanned data HYPONYM-OF domains. particles HYPONYM-OF domains. quantitative and qualitative evaluation EVALUATE-FOR method. quantitative and qualitative evaluation EVALUATE-FOR upsampling task. method USED-FOR temporal coherence. quantitative and qualitative evaluation EVALUATE-FOR learning temporal coherence. upsampling task CONJUNCTION learning temporal coherence. learning temporal coherence CONJUNCTION upsampling task. irregular point cloud sequences USED-FOR temporal coherence. upsampling task EVALUATE-FOR method. OtherScientificTerm are scene flow information, and point correspondence annotation. Material is dynamic point cloud sequences. ","This paper proposes a method for learning temporal coherence in dynamic point cloud sequences. The proposed method is based on the idea of learning a masking module that maps the point distribution to the upsampling ratio, which is then used to learn a temporal coherent feature space. The method is evaluated on a number of real-world environments and is shown to outperform existing methods. ","This paper proposes a method for learning temporal coherence in dynamic point cloud sequences. The proposed method is based on the idea of learning a masking module that maps the point distribution to the upsampling ratio, which is then used to learn a temporal coherent feature space. The method is evaluated on a number of real-world environments and is shown to outperform existing methods. "
3183,SP:67efe60ad37807505369b7852bc0abed29ffdda8,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. pre - training USED-FOR detection transformers. it USED-FOR object detection. task adapter USED-FOR it. textual prompts USED-FOR NLP. query positional embeddings USED-FOR model. visual prompts USED-FOR model. visual prompts USED-FOR query positional embeddings. self - attention USED-FOR task adapter. COCO dataset EVALUATE-FOR PT - DETR. it COMPARE detection transformers. detection transformers COMPARE it. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. generalization EVALUATE-FOR detection transformers. small - size datasets EVALUATE-FOR detection transformers. generalization FEATURE-OF small - size datasets. generalization EVALUATE-FOR it. robustness EVALUATE-FOR it. small - size datasets EVALUATE-FOR it. Method are Large - scale pre - training, 12 - layer transformer, FP - DETR, and encoder - only transformer. OtherScientificTerm are separated training paradigm, and common corruptions. Task is upstream and downstream tasks. Generic is method. ","This paper proposes PT-DETR, a pre-training method for object detection. The proposed method is based on self-attention and self-supervision. The authors show that the proposed method outperforms the state-of-the-art methods on COCO and small-scale datasets. The main contribution of the paper is the use of task-specific pre-trainers to improve the robustness and generalization of the model. ","This paper proposes PT-DETR, a pre-training method for object detection. The proposed method is based on self-attention and self-supervision. The authors show that the proposed method outperforms the state-of-the-art methods on COCO and small-scale datasets. The main contribution of the paper is the use of task-specific pre-trainers to improve the robustness and generalization of the model. "
3199,SP:a1f9897496303984fc7ad469222106b14b4a6233,"Federated Averaging ( FedAvg HYPONYM-OF federated learning algorithm. FedPAGE HYPONYM-OF federated learning algorithm. communication complexity EVALUATE-FOR FedPAGE. optimal PAGE method USED-FOR federated learning algorithm. optimal PAGE method USED-FOR FedPAGE. local methods USED-FOR federated convex and nonconvex optimization. FedPAGE COMPARE local methods. local methods COMPARE FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication rounds USED-FOR FedPAGE. communication rounds FEATURE-OF FedPAGE. nonconvex setting FEATURE-OF FedPAGE. number of communication rounds EVALUATE-FOR FedPAGE. FedPAGE CONJUNCTION SCAFFOLD. SCAFFOLD CONJUNCTION FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication complexity EVALUATE-FOR FedPAGE. Method are Local - SGD ), local SGD steps, and federated learning. Task is convex setting. OtherScientificTerm are communication round, and communication cost. ","This paper proposes FedPAGE, a federated learning algorithm for convex and non-convex optimization. The main idea is to reduce the number of communication rounds in the convex setting and the communication cost in the non-Convex setting. The proposed algorithm is based on the idea of local-SGD (LGSD). The authors show that the communication complexity of the proposed algorithm can be reduced by a factor of two. The authors also provide a theoretical analysis of the communication costs of their algorithm.","This paper proposes FedPAGE, a federated learning algorithm for convex and non-convex optimization. The main idea is to reduce the number of communication rounds in the convex setting and the communication cost in the non-Convex setting. The proposed algorithm is based on the idea of local-SGD (LGSD). The authors show that the communication complexity of the proposed algorithm can be reduced by a factor of two. The authors also provide a theoretical analysis of the communication costs of their algorithm."
3215,SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"mathematical operations USED-FOR Artificial neural networks ( ANNs ). networks USED-FOR adversarial input perturbations. decision boundary geometry FEATURE-OF ANN classifiers. adversarial perturbations USED-FOR decision boundary geometry. adversarial subspace COMPARE random subspace. random subspace COMPARE adversarial subspace. adversarial attacks PART-OF training procedure. redistribution of proximal class labels CONJUNCTION boundary curvature. boundary curvature CONJUNCTION redistribution of proximal class labels. boundary distance CONJUNCTION redistribution of proximal class labels. redistribution of proximal class labels CONJUNCTION boundary distance. Generic are network, and analysis. OtherScientificTerm are adversarial subspaces, minimal perturbation, decision boundary, boundary, and dimensionality of the subspace. Task is test - time adversarial attacks. Method is adversarial training. ",This paper studies the problem of adversarial training of deep neural networks. The authors consider the decision boundary geometry of the adversarial subspace. They show that adversarial perturbations can be used to improve the performance of the classifier. They also show that the boundary curvature and the redistribution of proximal class labels are important factors in the performance improvement. ,This paper studies the problem of adversarial training of deep neural networks. The authors consider the decision boundary geometry of the adversarial subspace. They show that adversarial perturbations can be used to improve the performance of the classifier. They also show that the boundary curvature and the redistribution of proximal class labels are important factors in the performance improvement. 
3231,SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"hashtags USED-FOR auxiliary information. similar representations CONJUNCTION dissimilar representations. dissimilar representations CONJUNCTION similar representations. self - supervised representations COMPARE auxiliary - information - infused representations. auxiliary - information - infused representations COMPARE self - supervised representations. auxiliary - information - infused representations COMPARE supervised representations. supervised representations COMPARE auxiliary - information - infused representations. direct downstream labels USED-FOR supervision signals. self - supervised representations COMPARE supervised representations. supervised representations COMPARE self - supervised representations. direct downstream labels USED-FOR supervised representations. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. approach COMPARE approach. approach COMPARE approach. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. auxiliary data information USED-FOR approach. auxiliary data information USED-FOR baseline representation learning methods. approach USED-FOR unsupervised constructed clusters. approach USED-FOR unsupervised representation learning approach. auxiliary information HYPONYM-OF unsupervised constructed clusters. OtherScientificTerm are data clustering information, and cluster. Material is Instagram image. Method is weakly - supervised contrastive learning approach. ",This paper proposes a weakly-supervised contrastive learning approach for unsupervised clustering. The proposed approach is based on the idea that the auxiliary information of a cluster can be used to improve the performance of the supervised representation. The authors show that the proposed approach outperforms the baselines on the Instagram dataset.,This paper proposes a weakly-supervised contrastive learning approach for unsupervised clustering. The proposed approach is based on the idea that the auxiliary information of a cluster can be used to improve the performance of the supervised representation. The authors show that the proposed approach outperforms the baselines on the Instagram dataset.
3247,SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"Recovering sparse parameters PART-OF machine learning. observational data USED-FOR Recovering sparse parameters. algorithms USED-FOR problem. path - following algorithm USED-FOR PLISA. recovery accuracy EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR generalization ability. generalization ability EVALUATE-FOR PLISA. PLISA USED-FOR sparse estimation problems. stability CONJUNCTION convergence. convergence CONJUNCTION stability. generalization ability CONJUNCTION algorithmic properties. algorithmic properties CONJUNCTION generalization ability. convergence FEATURE-OF unrolled algorithm. stability FEATURE-OF unrolled algorithm. convergence HYPONYM-OF algorithmic properties. stability HYPONYM-OF algorithmic properties. techniques USED-FOR learning - based algorithms. OtherScientificTerm are hyperparameters, and problem distribution of interest. Generic are they, and analysis. Method are Provable Learning - based Iterative Sparse recovery Algorithm, and generalization analysis. ","This paper studies the problem of recovering sparse parameters from observational data. The authors propose an iterative iterative sparse recovery algorithm called PLISA, which is based on a path-following algorithm. The main contribution of the paper is to provide theoretical analysis of the generalization properties of the proposed algorithm. They show that PLISA is able to achieve better generalization performance than existing algorithms. They also provide theoretical guarantees for the convergence and stability of the algorithm.","This paper studies the problem of recovering sparse parameters from observational data. The authors propose an iterative iterative sparse recovery algorithm called PLISA, which is based on a path-following algorithm. The main contribution of the paper is to provide theoretical analysis of the generalization properties of the proposed algorithm. They show that PLISA is able to achieve better generalization performance than existing algorithms. They also provide theoretical guarantees for the convergence and stability of the algorithm."
3263,SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"robot control CONJUNCTION game AI. game AI CONJUNCTION robot control. unified homogeneous action space FEATURE-OF hybrid action space. discretization USED-FOR unified homogeneous action space. Hybrid Action Representation ( HyAR ) USED-FOR compact and decodable latent representation space. compact and decodable latent representation space USED-FOR hybrid action space. embedding table CONJUNCTION conditional Variational Auto - Encoder ( VAE ). conditional Variational Auto - Encoder ( VAE ) CONJUNCTION embedding table. HyAR USED-FOR latent space. embedding table USED-FOR HyAR. unsupervised environmental dynamics prediction USED-FOR action representation. DRL algorithms USED-FOR representation space. action space FEATURE-OF hybrid action embeddings. discrete - continuous action space USED-FOR HyAR. HyAR COMPARE baselines. baselines COMPARE HyAR. baselines USED-FOR high - dimensional action spaces. OtherScientificTerm are Discrete - continuous hybrid action space, discrete or continuous action space, approximation difficulties, discrete action, and continuous parameter. Method are Reinforcement Learning ( RL ), and RL algorithms. Task is hybrid action RL. ",This paper proposes a hybrid action representation (HyAR) method for reinforcement learning. The key idea is to learn a compact and decodable latent representation space for the discrete and continuous action space. The proposed method is based on an unsupervised environment dynamics prediction method and a conditional Variational Auto-Encoder (VAE). The authors show that the proposed method outperforms existing methods in terms of performance and accuracy. ,This paper proposes a hybrid action representation (HyAR) method for reinforcement learning. The key idea is to learn a compact and decodable latent representation space for the discrete and continuous action space. The proposed method is based on an unsupervised environment dynamics prediction method and a conditional Variational Auto-Encoder (VAE). The authors show that the proposed method outperforms existing methods in terms of performance and accuracy. 
3279,SP:5128bf712f6b197de113c7a371b4bec36f978eca,SGEM USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR SGEM. AEGD method USED-FOR SGEM. energy CONJUNCTION momentum. momentum CONJUNCTION energy. energy USED-FOR SGEM. momentum PART-OF SGEM. energydependent convergence rates CONJUNCTION regret bound. regret bound CONJUNCTION energydependent convergence rates. regret bound USED-FOR online convex setting. energydependent convergence rates FEATURE-OF nonconvex stochastic setting. unconditional energy stability property FEATURE-OF SGEM. threshold USED-FOR energy variable. SGEM COMPARE AEGD. AEGD COMPARE SGEM. SGDM USED-FOR deep neural networks. SGEM USED-FOR deep neural networks. SGEM COMPARE SGDM. SGDM COMPARE SGEM. Method is Adaptive Gradient Descent. ,"This paper studies the problem of stochastic gradient descent for non-convex optimization problems. In particular, the authors consider the case of online convex optimization, where the objective function is a convex function of the energy and momentum. The authors show that under certain conditions, SGEM converges to an energy-dependent convergence rate and a regret bound. They also provide an unconditional energy stability property of SGEM. ","This paper studies the problem of stochastic gradient descent for non-convex optimization problems. In particular, the authors consider the case of online convex optimization, where the objective function is a convex function of the energy and momentum. The authors show that under certain conditions, SGEM converges to an energy-dependent convergence rate and a regret bound. They also provide an unconditional energy stability property of SGEM. "
3295,SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"non - autoregressive ( NAR ) approaches USED-FOR inference. NAR models COMPARE AR counterparts. AR counterparts COMPARE NAR models. training CONJUNCTION inference. inference CONJUNCTION training. multiple datasets EVALUATE-FOR NAR models. NAR EVALUATE-FOR CMLMC. raw data USED-FOR CMLMC. multiple datasets EVALUATE-FOR AR. multiple datasets EVALUATE-FOR CMLMC. Metric is human - level accuracy. Method are AR framework, and distillation. OtherScientificTerm is raw data without distillation. ","This paper proposes a new method for training non-autoregressive (NAR) models for inference. The authors propose a new framework called CMLMC, which is based on the distillation of raw data without distillation. They show that the proposed method outperforms the state-of-the-art NAR models on multiple datasets. They also show that their method is able to achieve human-level accuracy.","This paper proposes a new method for training non-autoregressive (NAR) models for inference. The authors propose a new framework called CMLMC, which is based on the distillation of raw data without distillation. They show that the proposed method outperforms the state-of-the-art NAR models on multiple datasets. They also show that their method is able to achieve human-level accuracy."
3311,SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,Ultra - low power local signal processing USED-FOR edge applications. always - on devices USED-FOR edge applications. limited power budget FEATURE-OF domain. spiking neural networks USED-FOR Neuromorphic processors. computational power FEATURE-OF Neuromorphic processors. limited power budget FEATURE-OF Neuromorphic processors. spiking neural dynamics COMPARE dilated temporal convolutions. dilated temporal convolutions COMPARE spiking neural dynamics. WaveSense HYPONYM-OF spiking neural network. WaveNet architecture USED-FOR spiking neural network. neural dynamics CONJUNCTION fixed time - constants. fixed time - constants CONJUNCTION neural dynamics. fixed time - constants CONJUNCTION feed - forward architecture. feed - forward architecture CONJUNCTION fixed time - constants. WaveSense USED-FOR neuromorphic implementation. feed - forward architecture USED-FOR WaveSense. fixed time - constants USED-FOR WaveSense. neural dynamics USED-FOR WaveSense. datasets USED-FOR keyword - spotting. keyword - spotting EVALUATE-FOR model. datasets EVALUATE-FOR model. CNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION CNNs. network COMPARE spiking neural networks. spiking neural networks COMPARE network. network COMPARE artificial neural networks. artificial neural networks COMPARE network. LSTMs HYPONYM-OF artificial neural networks. CNNs HYPONYM-OF artificial neural networks. ,"This paper proposes WaveSense, a neural network architecture for always-on devices. WaveSense is based on neural networks with spiking neural dynamics. The authors show that WaveSense outperforms existing neural networks on keyword-spotting tasks. They also show that the neural dynamics of WaveSense can be decomposed into fixed time-constant, neural dynamics and neural dynamics with neural dynamics, which are used to train WaveSense. ","This paper proposes WaveSense, a neural network architecture for always-on devices. WaveSense is based on neural networks with spiking neural dynamics. The authors show that WaveSense outperforms existing neural networks on keyword-spotting tasks. They also show that the neural dynamics of WaveSense can be decomposed into fixed time-constant, neural dynamics and neural dynamics with neural dynamics, which are used to train WaveSense. "
3327,SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"machine learning USED-FOR social applications. machine learning USED-FOR injustice. algorithms USED-FOR high - confidence behavioral guarantees. Shifty algorithms HYPONYM-OF algorithms. algorithms USED-FOR demographic shift ’s challenges. Shifty HYPONYM-OF technique. real - world dataset of university entrance exams EVALUATE-FOR Shifty. algorithm USED-FOR models. high - confidence fairness guarantees FEATURE-OF algorithm. Method is machine learning algorithms. OtherScientificTerm are unfair behavior, and demographic shift. Generic are approaches, and methods. Metric is fairness assurances. ","This paper studies the problem of fairness in machine learning. The authors propose Shifty, a new algorithm that is based on the Shifty algorithm. Shifty is a variant of Shifty that is designed to provide high-confidence behavioral guarantees for fairness. The algorithm is evaluated on a real-world dataset of university entrance exams. ","This paper studies the problem of fairness in machine learning. The authors propose Shifty, a new algorithm that is based on the Shifty algorithm. Shifty is a variant of Shifty that is designed to provide high-confidence behavioral guarantees for fairness. The algorithm is evaluated on a real-world dataset of university entrance exams. "
3343,SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"Stochastic dual dynamic programming ( SDDP ) USED-FOR multi - stage stochastic optimization. Stochastic dual dynamic programming ( SDDP ) USED-FOR modeling real - world process optimization tasks. worst - case complexity EVALUATE-FOR SDDP. trainable neural model USED-FOR problem instances. trainable neural model USED-FOR piece - wise linear value function. trainable neural model USED-FOR SDDP. intrinsic low - dimension space FEATURE-OF piece - wise linear value function. SDDP CONJUNCTION reinforcement learning algorithms. reinforcement learning algorithms CONJUNCTION SDDP. solution quality EVALUATE-FOR competitors. ν - SDDP COMPARE competitors. competitors COMPARE ν - SDDP. problem solving cost EVALUATE-FOR ν - SDDP. reinforcement learning algorithms HYPONYM-OF competitors. SDDP HYPONYM-OF competitors. solution quality EVALUATE-FOR ν - SDDP. synthetic and real - world process optimization problems EVALUATE-FOR ν - SDDP. OtherScientificTerm are decision variables, and successive problems. Material is low dimensional problems. Method is SDDP solver. Task is optimization. ","This paper proposes a new method for solving multi-stage stochastic dual dynamic programming (SDDP) problems. The main idea is to train a neural network to predict the solution of a given problem in the intrinsic low-dimensional space. The neural network is then used to learn a piece-wise linear value function, which is used to solve the problem. The authors show that the proposed method is able to achieve the best-case complexity and worst-case performance of existing methods.","This paper proposes a new method for solving multi-stage stochastic dual dynamic programming (SDDP) problems. The main idea is to train a neural network to predict the solution of a given problem in the intrinsic low-dimensional space. The neural network is then used to learn a piece-wise linear value function, which is used to solve the problem. The authors show that the proposed method is able to achieve the best-case complexity and worst-case performance of existing methods."
3359,SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,protocol USED-FOR private next - token prediction. protocol USED-FOR privacy violations. language models USED-FOR privacy violations. private corpus USED-FOR language models. relaxation of group differentially private prediction USED-FOR SUBMIX. data - dependent privacy accounting mechanism USED-FOR it. it USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR SUBMIX. SUBMIX HYPONYM-OF protocol. transformer - based models USED-FOR next - token predictions. GPT-2 HYPONYM-OF transformer - based models. Generic is model. Method is language model. ,This paper proposes a new protocol for next-token prediction. The proposed method is based on the relaxation of group differentially private prediction (GPT-2) and a data-dependent privacy accounting mechanism (SUBMIX). The authors show that the proposed method can be used to mitigate data-extraction attacks. The authors also show that it can be applied to the transformer-based models.,This paper proposes a new protocol for next-token prediction. The proposed method is based on the relaxation of group differentially private prediction (GPT-2) and a data-dependent privacy accounting mechanism (SUBMIX). The authors show that the proposed method can be used to mitigate data-extraction attacks. The authors also show that it can be applied to the transformer-based models.
3375,SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,unsupervised method USED-FOR OOD samples. classification model USED-FOR k - NN density estimate. k - NN density estimate USED-FOR unsupervised method. k - NN density estimator COMPARE OOD detection method. OOD detection method COMPARE k - NN density estimator. Label Smoothed Embedding Hypothesis HYPONYM-OF label smoothing. label smoothing USED-FOR model. proposal COMPARE OOD baselines. OOD baselines COMPARE proposal. k - NN density estimation USED-FOR OOD examples. finite - sample high - probability statistical results USED-FOR k - NN density estimation. Material is indistribution samples. ,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using label smoothing. The proposed method is based on the label smoothed embedding hypothesis, which is an extension of the Label Smoothed Embedding Hypothesis (LSE) framework. The authors show that the proposed method outperforms existing OOD detection methods on a number of synthetic and real-world datasets. ","This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using label smoothing. The proposed method is based on the label smoothed embedding hypothesis, which is an extension of the Label Smoothed Embedding Hypothesis (LSE) framework. The authors show that the proposed method outperforms existing OOD detection methods on a number of synthetic and real-world datasets. "
3391,SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"continuous time domain FEATURE-OF stochastic differential equations. stochastic differential equations USED-FOR Diffusion - based methods. denoising score matching USED-FOR models. denoising score matching framework USED-FOR representation learning. GANs CONJUNCTION VAEs. VAEs CONJUNCTION GANs. VAEs USED-FOR representations. GANs USED-FOR representations. denoising score matching objective USED-FOR diffusion - based representation learning. approach USED-FOR infinite - dimensional latent code. infinite - dimensional latent code USED-FOR state - of - the - art models. semi - supervised image classification EVALUATE-FOR state - of - the - art models. adversarial training USED-FOR diffusionbased models. adversarial training USED-FOR sample quality. smaller noise scales FEATURE-OF approximation of the prior. sampling speed EVALUATE-FOR adversarial training. approximation of the prior USED-FOR adversarial training. Method are non - adversarial generative model, and multi - scale denoising autoencoders. OtherScientificTerm are supervised signal, and latent codes. Task is denoising. Generic is representation. ",This paper proposes a new method for denoising score matching for deep generative models. The proposed method is based on diffusion-based representation learning. The authors show that the proposed method can be used to improve the sample quality of the denoised latent code. The method is evaluated on semi-supervised image classification tasks. ,This paper proposes a new method for denoising score matching for deep generative models. The proposed method is based on diffusion-based representation learning. The authors show that the proposed method can be used to improve the sample quality of the denoised latent code. The method is evaluated on semi-supervised image classification tasks. 
3407,SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"Goal - conditioned reinforcement learning ( RL ) USED-FOR tasks. navigation CONJUNCTION manipulation. manipulation CONJUNCTION navigation. expert demonstrations CONJUNCTION reward shaping. reward shaping CONJUNCTION expert demonstrations. offline data CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION offline data. planning USED-FOR curriculum of intermediate states. algorithm USED-FOR distant goal - reaching task. planning USED-FOR algorithm. M - step USED-FOR goal - conditioned policy. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. expectation maximization USED-FOR goal - conditioned policies. goal - conditioned RL CONJUNCTION graph search. graph search CONJUNCTION goal - conditioned RL. prior methods COMPARE ours. ours COMPARE prior methods. planning USED-FOR ours. goal - conditioned RL USED-FOR prior methods. graph search USED-FOR prior methods. method COMPARE prior methods. prior methods COMPARE method. it USED-FOR tasks. graph search USED-FOR methods. Method are Classifier - Planning ( C - Planning ), and graph planning. Generic is policy. ","This paper proposes a method for goal-conditioned reinforcement learning. The method is based on the idea of learning a curriculum of intermediate states that can be used to guide the policy towards a goal-reaching task. The authors propose a method to learn the curriculum by learning a classifier-planning algorithm. The algorithm is evaluated on a number of tasks, including navigation, manipulation, and reward shaping. ","This paper proposes a method for goal-conditioned reinforcement learning. The method is based on the idea of learning a curriculum of intermediate states that can be used to guide the policy towards a goal-reaching task. The authors propose a method to learn the curriculum by learning a classifier-planning algorithm. The algorithm is evaluated on a number of tasks, including navigation, manipulation, and reward shaping. "
3423,SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"regularization technique USED-FOR deep neural networks. Mixup HYPONYM-OF regularization technique. mixup USED-FOR k - mixup. Wasserstein metric FEATURE-OF interpolation. interpolation HYPONYM-OF displacement interpolation. mixup USED-FOR k - mixup case. k - mixup USED-FOR cluster and manifold structures. network architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION network architectures. mixup HYPONYM-OF data augmentation approach. data augmentation approach USED-FOR models. beta distribution USED-FOR Averaging weights. mixup USED-FOR Perturbations. embedded manifold USED-FOR distributions. α USED-FOR procedure. fully - connected network USED-FOR binary classification. synthetic datasets USED-FOR binary classification. 1 - mixup CONJUNCTION 32 - mixup regularization. 32 - mixup regularization CONJUNCTION 1 - mixup. synthetic datasets USED-FOR fully - connected network. k - mixup USED-FOR local structure. blur CONJUNCTION contrast. contrast CONJUNCTION blur. displacement interpolation USED-FOR optimal transport. global cluster CONJUNCTION manifold support structure. manifold support structure CONJUNCTION global cluster. k - mixup USED-FOR perturbed training datasets. manifold support structure FEATURE-OF perturbed training datasets. global cluster FEATURE-OF perturbed training datasets. Metric are generalization, adversarial robustness, and robustness. Generic is It. OtherScientificTerm are local distributional structure, clusters, data manifold, and discrete distributions. Task is regularization. Method is mixup regularization. ","This paper proposes a new regularization technique called k-mixup to improve the generalization performance of deep neural networks. The proposed method is based on the Wasserstein metric, which is a generalization of the displacement interpolation. The authors show that k-Mixup can be used to improve generalization and robustness to perturbations. They also show that the proposed method can improve the robustness against adversarial attacks. ","This paper proposes a new regularization technique called k-mixup to improve the generalization performance of deep neural networks. The proposed method is based on the Wasserstein metric, which is a generalization of the displacement interpolation. The authors show that k-Mixup can be used to improve generalization and robustness to perturbations. They also show that the proposed method can improve the robustness against adversarial attacks. "
3439,SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,lightweight network USED-FOR embeddings. nonlinear classification layer USED-FOR lightweight network. nonlinear classification layer USED-FOR embeddings. nonlinearity USED-FOR representation ( embedding ) learning. deep networks USED-FOR representation ( embedding ) learning. embeddings USED-FOR linear classifier. linear classifier USED-FOR they. nonlinearity USED-FOR deep networks. embedding vector space FEATURE-OF nonlinear classifiers. limited - capacity backbone USED-FOR network. nonlinear kernelized classification layer USED-FOR deep networks. classification layer USED-FOR nonlinear classifier. radial kernel functions USED-FOR nonlinear classifier. embeddings FEATURE-OF radial kernel functions. radial kernel functions USED-FOR classification layer. layer USED-FOR model - efficient classifiers. layer USED-FOR computer vision and natural language processing tasks. ,"This paper studies the problem of learning embeddings for nonlinearity in deep networks. The authors propose a novel nonlinear classification layer for embedding learning. The proposed method is based on the radial kernel function, which is an extension of the linear classifier. The main contribution of the paper is that the authors show that the proposed method can be used to learn nonlinear classifiers in the embedding space. ","This paper studies the problem of learning embeddings for nonlinearity in deep networks. The authors propose a novel nonlinear classification layer for embedding learning. The proposed method is based on the radial kernel function, which is an extension of the linear classifier. The main contribution of the paper is that the authors show that the proposed method can be used to learn nonlinear classifiers in the embedding space. "
3455,SP:01ee8ec81619784788eb0ce9785098e437d17a7c,Graph Neural Networks ( GNNs ) USED-FOR node representations. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. fairness - aware data augmentation frameworks USED-FOR intrinsic bias. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. nodal features USED-FOR fairness - aware data augmentation frameworks. graph structure USED-FOR fairness - aware data augmentation frameworks. schemes USED-FOR GNN - based learning mechanisms. schemes USED-FOR fairness. fairness EVALUATE-FOR GNN - based learning mechanisms. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. real networks USED-FOR graph contrastive learning. real networks USED-FOR node classification. real networks USED-FOR link prediction. statistical parity CONJUNCTION equal opportunity. equal opportunity CONJUNCTION statistical parity. augmentation strategies COMPARE contrastive methods. contrastive methods COMPARE augmentation strategies. augmentation strategies USED-FOR fairness. statistical parity FEATURE-OF fairness. Method is Node representation learning. Material is graphs. Generic is representations. ,"This paper studies the fairness of GNN-based data augmentation methods for node representation learning. In particular, the authors focus on the issue of intrinsic bias in GNNs. The authors show that the fairness trade-off between node classification and link prediction is important for fairness. To address this issue, this paper proposes two data-augmentation strategies: (1) node contrastive learning and (2) node fairness augmentation. Experiments are conducted to show the effectiveness of the proposed methods. ","This paper studies the fairness of GNN-based data augmentation methods for node representation learning. In particular, the authors focus on the issue of intrinsic bias in GNNs. The authors show that the fairness trade-off between node classification and link prediction is important for fairness. To address this issue, this paper proposes two data-augmentation strategies: (1) node contrastive learning and (2) node fairness augmentation. Experiments are conducted to show the effectiveness of the proposed methods. "
3471,SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"observational data USED-FOR estimating treatment effects. instrumental variable ( IV ) USED-FOR two - stage regression. 2SLS HYPONYM-OF two - stage regression. IVs CONJUNCTION confounders. confounders CONJUNCTION IVs. nonlinear IV regression variants USED-FOR confounding bias. confounders USED-FOR nonlinear IV regression variants. balancing USED-FOR treatment effect estimation. bias - variance trade - off FEATURE-OF imbalanced treatment distributions. balanced confounders representation USED-FOR treatment effect estimation. nonlinear IV methods USED-FOR confounding. balanced representation of confounders USED-FOR confounder balancing. treatment regression PART-OF modules. modules PART-OF CB - IV algorithm. outcome regression PART-OF CB - IV algorithm. treatment regression PART-OF CB - IV algorithm. confounder balancing USED-FOR treatment effect estimation. IV regression USED-FOR treatment effect estimation. confounder balancing USED-FOR IV regression. multiplicative assumption COMPARE additive separability assumption. additive separability assumption COMPARE multiplicative assumption. multiplicative assumption FEATURE-OF CB - IV algorithm. IV regression CONJUNCTION confounder balancing methods. confounder balancing methods CONJUNCTION IV regression. CB - IV algorithm USED-FOR treatment effect estimation. CB - IV algorithm COMPARE state - of - the - art methods. state - of - the - art methods COMPARE CB - IV algorithm. state - of - the - art methods USED-FOR treatment effect estimation. confounder balancing methods USED-FOR treatment effect estimation. CB - IV algorithm COMPARE confounder balancing methods. confounder balancing methods COMPARE CB - IV algorithm. IV regression HYPONYM-OF CB - IV algorithm. confounder balancing methods HYPONYM-OF state - of - the - art methods. IV regression HYPONYM-OF state - of - the - art methods. OtherScientificTerm are unmeasured confounders, additive separability of noise, and observed confounders. Generic are they, and second stage. Material is linear setting. ","This paper studies the problem of two-stage regression with instrumental variable (IV) and confounders. The authors propose a new algorithm, CB-IV, which is based on the additive separability of noise and the bias-variance trade-off between treatment effect estimation and confounding bias. They show that the proposed algorithm outperforms state-of-the-art methods in terms of bias variance, confounding bias, and confounding error. They also show that their algorithm can be applied to nonlinear IV regression.","This paper studies the problem of two-stage regression with instrumental variable (IV) and confounders. The authors propose a new algorithm, CB-IV, which is based on the additive separability of noise and the bias-variance trade-off between treatment effect estimation and confounding bias. They show that the proposed algorithm outperforms state-of-the-art methods in terms of bias variance, confounding bias, and confounding error. They also show that their algorithm can be applied to nonlinear IV regression."
3487,SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"stochastic gradient descent steps USED-FOR models. MAML objective COMPARE non - adaptive learning ( NAL ). non - adaptive learning ( NAL ) COMPARE MAML objective. MAML COMPARE NAL. NAL COMPARE MAML. easy and hard tasks PART-OF linear regression setting. MAML COMPARE NAL. NAL COMPARE MAML. hardness EVALUATE-FOR tasks. MAML USED-FOR hard tasks. Method are gradient descent, and two - layer neural networks. OtherScientificTerm is easy tasks optimal solutions. Task is few - shot image classification. ","This paper studies the problem of few-shot image classification in the linear regression setting. The authors propose a new objective called MAML, which aims to find the optimal solution for both easy and hard tasks. The main idea is to use a two-layer neural network to learn the optimal solutions for both the easy tasks and the hard ones. The paper shows that the proposed objective can be used to improve the performance of the model on both the hard and easy tasks. ","This paper studies the problem of few-shot image classification in the linear regression setting. The authors propose a new objective called MAML, which aims to find the optimal solution for both easy and hard tasks. The main idea is to use a two-layer neural network to learn the optimal solutions for both the easy tasks and the hard ones. The paper shows that the proposed objective can be used to improve the performance of the model on both the hard and easy tasks. "
3503,SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"astrophysics CONJUNCTION remote sensing. remote sensing CONJUNCTION astrophysics. Sparse Blind Source Separation ( BSS ) USED-FOR applications. remote sensing HYPONYM-OF applications. astrophysics HYPONYM-OF applications. Proximal Alternating Linearized Minimization ( PALM ) algorithm HYPONYM-OF sparse BSS methods. PALM hyperparameters CONJUNCTION variables. variables CONJUNCTION PALM hyperparameters. PALM hyperparameters USED-FOR Unrolling PALM. data - driven knowledge USED-FOR Unrolling PALM. Learned PALM ( LPALM ) algorithm USED-FOR semi - blind source separation. algorithm COMPARE PALM. PALM COMPARE algorithm. LPALM USED-FOR astrophysical multispectral imaging. cumbersome hyperparameter FEATURE-OF PALM. LPALM COMPARE PALM. PALM COMPARE LPALM. separation quality EVALUATE-FOR algorithm. unrolled source separation methods USED-FOR semi - blind setting. LPALM COMPARE unrolled source separation methods. unrolled source separation methods COMPARE LPALM. LPALM USED-FOR semi - blind setting. OtherScientificTerm are hyperparameter choice, and variable mixing matrices. Method are algorithm unfolding / unrolling, and unrolled algorithms. Task is real - world applications. ",This paper proposes a new method for semi-blind source separation. The proposed method is based on the Proximal Alternating Linearized Minimization (PALM) algorithm. The main difference between the proposed method and existing unrolled source separation methods is the use of data-driven knowledge to guide the learning of the hyperparameters. The method is evaluated on a variety of semi-blind source separation tasks.,This paper proposes a new method for semi-blind source separation. The proposed method is based on the Proximal Alternating Linearized Minimization (PALM) algorithm. The main difference between the proposed method and existing unrolled source separation methods is the use of data-driven knowledge to guide the learning of the hyperparameters. The method is evaluated on a variety of semi-blind source separation tasks.
3519,SP:7716315001949ab88c8a216302fe51bae872fc87,"transformers USED-FOR language modeling. power - law relationship FEATURE-OF transformers. memory CONJUNCTION computation. computation CONJUNCTION memory. attention module USED-FOR Legendre Memory Unit based model. implicit self - attention HYPONYM-OF attention module. loss EVALUATE-FOR transformers. transformers COMPARE LSTMs. LSTMs COMPARE transformers. model COMPARE transformers. transformers COMPARE model. model COMPARE transformers. transformers COMPARE model. model COMPARE LSTMs. LSTMs COMPARE model. transformers COMPARE LSTMs. LSTMs COMPARE transformers. loss EVALUATE-FOR model. global self - attention USED-FOR architecture. OtherScientificTerm are model size, and sequence length. Metric is computational and memory requirements. ","This paper proposes a Legendre Memory Unit based model for language modeling. The proposed model is based on implicit self-attention, which is an extension of the Legendre memory unit (LSTM) architecture. The authors show that the proposed model can achieve better performance than transformers and LSTMs in terms of computational and memory requirements. They also show that their model is more efficient than LSTM and transformers.","This paper proposes a Legendre Memory Unit based model for language modeling. The proposed model is based on implicit self-attention, which is an extension of the Legendre memory unit (LSTM) architecture. The authors show that the proposed model can achieve better performance than transformers and LSTMs in terms of computational and memory requirements. They also show that their model is more efficient than LSTM and transformers."
3535,SP:832f422b3554e89702e13c8c5690ee26f2289e3b,Generative adversarial networks ( GANs ) USED-FOR image generation. photo - realistic quality EVALUATE-FOR Generative adversarial networks ( GANs ). LatentKeypointGAN HYPONYM-OF two - stage GAN. internal conditioning FEATURE-OF space keypoints. appearance embeddings PART-OF keypoints. domain knowledge CONJUNCTION supervision signals. supervision signals CONJUNCTION domain knowledge. network architectures CONJUNCTION training schemes. training schemes CONJUNCTION network architectures. LatentKeypointGAN USED-FOR interpretable latent space. re - positioning USED-FOR LatentKeypointGAN. generating portraits HYPONYM-OF keypoint embeddings. GAN - based method USED-FOR unsupervised keypoint detection. Material is image content. OtherScientificTerm is spatial and appearance factors. ,This paper proposes a two-stage GAN-based method for unsupervised keypoint detection. The key idea is to learn a latent space of keypoints that can be used to train a GAN to identify keypoints in an image. The proposed method LatentKeypointGAN is based on the idea of re-positioning the keypoints to improve the performance of the GAN. The authors show that the proposed method outperforms baselines in terms of keypoint accuracy and image quality. ,This paper proposes a two-stage GAN-based method for unsupervised keypoint detection. The key idea is to learn a latent space of keypoints that can be used to train a GAN to identify keypoints in an image. The proposed method LatentKeypointGAN is based on the idea of re-positioning the keypoints to improve the performance of the GAN. The authors show that the proposed method outperforms baselines in terms of keypoint accuracy and image quality. 
3551,SP:9206ae6e31077569313838504ef6daa89ad3b59c,"layer normalization USED-FOR deep fully - connected neural networks. mean field formalism USED-FOR deep fully - connected neural networks. initialization scheme CONJUNCTION activation function. activation function CONJUNCTION initialization scheme. normalization techniques USED-FOR problems. method USED-FOR residual networks. method USED-FOR initialization variances. Task is non - perturbative analysis of signal propagation. OtherScientificTerm are depth, gradient explosion, and representation shrinkage. Method is fully - connected architecture. ",This paper proposes a mean field formalism for fully-connected neural networks. The authors show that the initialization variances of a fully connected network can be controlled by the initialization scheme and activation function of the network. They also show that this can be used to improve the performance of residual networks. ,This paper proposes a mean field formalism for fully-connected neural networks. The authors show that the initialization variances of a fully connected network can be controlled by the initialization scheme and activation function of the network. They also show that this can be used to improve the performance of residual networks. 
3567,SP:2177be818b5843c580c787f1b2d725154846feb6,"optimal step sizes USED-FOR stochastic gradient descent. line searches USED-FOR step sizes. line searches PART-OF optimization. step sizes FEATURE-OF full - batch loss. line - search method USED-FOR full - batch loss. parabola USED-FOR line - search method. parabolas USED-FOR Learning rates. approach COMPARE SGD. SGD COMPARE approach. line search approaches USED-FOR Deep Learning across models. approach COMPARE line search approaches. line search approaches COMPARE approach. SGD COMPARE line search approaches. line search approaches COMPARE SGD. piece - wise constant learning rate schedule USED-FOR approach. validation and test accuracy EVALUATE-FOR batch sizes. piece - wise constant learning rate schedule USED-FOR SGD. validation and test accuracy EVALUATE-FOR approach. Task is Deep Learning. OtherScientificTerm are inherent noise, noisy update step directions, and optimal update step size. ","This paper proposes a line search method to find the optimal step size for stochastic gradient descent (SGD). The proposed method is based on the idea of line search, which is an extension of the line search framework. The authors show that the proposed method outperforms existing line search methods in terms of both validation and test accuracy. ","This paper proposes a line search method to find the optimal step size for stochastic gradient descent (SGD). The proposed method is based on the idea of line search, which is an extension of the line search framework. The authors show that the proposed method outperforms existing line search methods in terms of both validation and test accuracy. "
3583,SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"Noise - contrastive estimation ( NCE ) HYPONYM-OF statistically consistent method. statistically consistent method USED-FOR unnormalized probabilistic models. noise distribution USED-FOR NCE. noise distribution USED-FOR NCE. exponential loss USED-FOR eNCE. eNCE HYPONYM-OF NCE. exponential loss USED-FOR NCE. OtherScientificTerm are flat ) loss landscape, and exponential family. Method is normalized gradient descent. ",This paper studies noise-contrastive estimation (NCE) for unnormalized probabilistic models. The authors show that the exponential loss of NCE can be viewed as a function of the noise distribution of the model. They show that eNCE is a statistically consistent method for NCE. They also provide a theoretical analysis of the exponential family of loss landscape. ,This paper studies noise-contrastive estimation (NCE) for unnormalized probabilistic models. The authors show that the exponential loss of NCE can be viewed as a function of the noise distribution of the model. They show that eNCE is a statistically consistent method for NCE. They also provide a theoretical analysis of the exponential family of loss landscape. 
3599,SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"Privacy HYPONYM-OF distributed machine learning. distributed SGD algorithm USED-FOR model. noisy information USED-FOR differential privacy ( DP ). parameter - server architecture FEATURE-OF distributed SGD algorithm. DP CONJUNCTION BR. BR CONJUNCTION DP. convergence FEATURE-OF distributed SGD. Byzantine faults FEATURE-OF distributed SGD. ( α, f)-Byzantine resilience USED-FOR those. ( α, f)-BR USED-FOR approximate convergence guarantee. hyperparameter optimization USED-FOR guarantee. approaches USED-FOR DP. approaches USED-FOR BR. DP CONJUNCTION BR. BR CONJUNCTION DP. DP CONJUNCTION BR. BR CONJUNCTION DP. Method is learning algorithm. Metric is learning accuracy. ","This paper studies the problem of differential privacy (DP) in distributed machine learning. In particular, the authors consider the case where the parameter-server architecture of a distributed SGD algorithm has Byzantine faults. The authors show that under certain assumptions on the number of parameters, the convergence of the algorithm is guaranteed to be (alpha, f)-Byzantine, which is a generalization of the (α, f)-BR algorithm. They also provide an approximate convergence guarantee for the algorithm. ","This paper studies the problem of differential privacy (DP) in distributed machine learning. In particular, the authors consider the case where the parameter-server architecture of a distributed SGD algorithm has Byzantine faults. The authors show that under certain assumptions on the number of parameters, the convergence of the algorithm is guaranteed to be (alpha, f)-Byzantine, which is a generalization of the (α, f)-BR algorithm. They also provide an approximate convergence guarantee for the algorithm. "
3615,SP:bc783f0c829f90931535e63687d13172879631b3,code editing USED-FOR query code snippet. support exemplars USED-FOR query code snippet. editing exemplar USED-FOR editorial pattern. common pattern USED-FOR code editing. support exemplars USED-FOR common pattern. deep learning approach USED-FOR code editing problem. them USED-FOR query code snippet editing. support exemplars USED-FOR edit representations. edit representations PART-OF learning approach. multi - extent similarities ensemble USED-FOR query code snippet editing. language - specific grammar USED-FOR abstract syntax trees. similarities measurement USED-FOR collective tree representations. collective tree representations USED-FOR query and support sample matching. method COMPARE non - composition baselines. non - composition baselines COMPARE method. C # and Python datasets EVALUATE-FOR method. Task is computer source code editing. Material is support and query code snippets. Method is similarity - ranking error estimator. ,"This paper proposes a method for code editing. The proposed method is based on a multi-extent similarities ensemble (MES) approach. The authors propose to learn a set of support exemplars and edit exemplars for each query code snippet, which are then used for matching query and support samples. The method is evaluated on C# and Python code editing tasks. ","This paper proposes a method for code editing. The proposed method is based on a multi-extent similarities ensemble (MES) approach. The authors propose to learn a set of support exemplars and edit exemplars for each query code snippet, which are then used for matching query and support samples. The method is evaluated on C# and Python code editing tasks. "
3631,SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,text CONJUNCTION music. music CONJUNCTION text. deep generative models USED-FOR realistic sequence data. text HYPONYM-OF realistic sequence data. music HYPONYM-OF realistic sequence data. high - level structure USED-FOR generative process. local coherence CONJUNCTION global coherence. global coherence CONJUNCTION local coherence. global coherence EVALUATE-FOR models. local coherence EVALUATE-FOR models. approach USED-FOR global structure. relational constraints FEATURE-OF global structure. model USED-FOR realistic data. model USED-FOR relational constraints. model PART-OF generative model. model PART-OF generative model. program synthesis algorithm USED-FOR relational constraints. constraint data USED-FOR generative model. approach COMPARE state - of - the - art. state - of - the - art COMPARE approach. approach USED-FOR high - level structure. state - of - the - art USED-FOR high - level structure. capturing high - level structure EVALUATE-FOR approach. low - level structure EVALUATE-FOR approach. OtherScientificTerm is measures of music. Generic is constraints. ,This paper proposes a method to capture high-level structure in a generative model. The key idea is to use relational constraints to capture the global coherence and local coherence of the generated data. The method is based on a program synthesis algorithm to generate a set of relational constraints. The authors show that the proposed method is able to capture both global and local structure. ,This paper proposes a method to capture high-level structure in a generative model. The key idea is to use relational constraints to capture the global coherence and local coherence of the generated data. The method is based on a program synthesis algorithm to generate a set of relational constraints. The authors show that the proposed method is able to capture both global and local structure. 
3647,SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"biological systems CONJUNCTION combinatorial optimization. combinatorial optimization CONJUNCTION biological systems. particle physics CONJUNCTION biological systems. biological systems CONJUNCTION particle physics. scaling problems FEATURE-OF set - to - hypergraph tasks. run - time complexity HYPONYM-OF scaling problems. training method USED-FOR iterative refinement. efficiency CONJUNCTION constant memory usage. constant memory usage CONJUNCTION efficiency. contributions PART-OF set - to - hypergraph model. model COMPARE state - of - the - art. state - of - the - art COMPARE model. Task is set - to - hypergraph prediction. OtherScientificTerm are hyperedges, and positive edges. Metric are memory requirements, and asymptotic memory scaling. ",This paper studies the problem of set-to-hypergraph scaling. The authors propose a new training method for iterative refinement of the hypergraph model. They show that the proposed method is able to achieve asymptotic memory scaling. ,This paper studies the problem of set-to-hypergraph scaling. The authors propose a new training method for iterative refinement of the hypergraph model. They show that the proposed method is able to achieve asymptotic memory scaling. 
3663,SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"biases FEATURE-OF models. post - processing method USED-FOR models. deep embeddings PART-OF pre - trained model. shallow neural network USED-FOR It. Ethical Module HYPONYM-OF shallow neural network. methodology COMPARE bias mitigation. bias mitigation COMPARE methodology. Method is deep learning algorithms. OtherScientificTerm are representation power, von Mises - Fisher loss, and latent space. Task is gender bias in facial recognition. ",This paper proposes a post-processing method to address the problem of bias mitigation in deep neural networks. The authors propose to use the Ethical Module (EM) as a pre-processing module to improve the performance of a deep neural network. Empirical results show that the proposed method outperforms baselines in terms of accuracy and bias mitigation. ,This paper proposes a post-processing method to address the problem of bias mitigation in deep neural networks. The authors propose to use the Ethical Module (EM) as a pre-processing module to improve the performance of a deep neural network. Empirical results show that the proposed method outperforms baselines in terms of accuracy and bias mitigation. 
3679,SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"new class data USED-FOR KD loss. phase model USED-FOR old class knowledge. free image stream USED-FOR placebo data. placebo data USED-FOR KD loss. Google Images HYPONYM-OF free image stream. ImageNet-1k CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION ImageNet-1k. supervision CONJUNCTION memory budget. memory budget CONJUNCTION supervision. memory budget FEATURE-OF old class exemplars. CIL methods EVALUATE-FOR method. higher - resolution benchmarks EVALUATE-FOR top - performing CIL methods. ImageNet-1k HYPONYM-OF higher - resolution benchmarks. ImageNet - Subset HYPONYM-OF higher - resolution benchmarks. Task are class - incremental learning ( CIL ), and learning of new classes. Method are knowledge distillation ( KD ), evaluation function, and reinforcement learning algorithm. Material are old - class data, and image stream. OtherScientificTerm are class overlap, placebos, and pseudo CIL tasks. Generic is function. ",This paper proposes a new method for class-incremental learning (CIL) based on the knowledge distillation (KD) loss. The proposed method is based on a phase model and a reinforcement learning algorithm. The authors show that the proposed method outperforms state-of-the-art CIL methods on a number of benchmark datasets. ,This paper proposes a new method for class-incremental learning (CIL) based on the knowledge distillation (KD) loss. The proposed method is based on a phase model and a reinforcement learning algorithm. The authors show that the proposed method outperforms state-of-the-art CIL methods on a number of benchmark datasets. 
3695,SP:506e0a888c03a955b708464eed3670c04baf4912,"approach USED-FOR modeling discrete structure. Energy - based Models ( EBMs ) USED-FOR modeling discrete structure. inference CONJUNCTION learning of EBM. learning of EBM CONJUNCTION inference. Energy - based Models ( EBMs ) USED-FOR approach. inference USED-FOR EBM. sampling from discrete distributions USED-FOR it. Markov Chain Monte Carlo ( MCMC ) USED-FOR sampling. informed proposal USED-FOR Markov Chain Monte Carlo ( MCMC ). local updates FEATURE-OF informed proposal. energy changes USED-FOR it. composition of local moves USED-FOR path auxiliary algorithm. sampling CONJUNCTION inference. inference CONJUNCTION sampling. inference CONJUNCTION learning. learning CONJUNCTION inference. path auxiliary algorithms COMPARE generic samplers. generic samplers COMPARE path auxiliary algorithms. generic samplers USED-FOR sampling. generic samplers USED-FOR discrete models. path auxiliary algorithms USED-FOR discrete models. discrete models USED-FOR sampling. discrete models USED-FOR inference. generic samplers USED-FOR inference. high dimensional discrete data USED-FOR deep EBMs. OtherScientificTerm are discrete distributions, evaluation of energy function, and linearization of the energy function. Generic is algorithm. ","This paper proposes a path auxiliary algorithm for sampling from discrete distributions for energy-based models (EBMs). The proposed algorithm is based on the Markov Chain Monte Carlo (MCMC) algorithm. The authors show that the MCMC algorithm can be used to sample from a discrete distribution, and that it can also be used for inference and learning of discrete models. The paper also shows that the proposed algorithm outperforms other path auxiliary algorithms in terms of sample efficiency. ","This paper proposes a path auxiliary algorithm for sampling from discrete distributions for energy-based models (EBMs). The proposed algorithm is based on the Markov Chain Monte Carlo (MCMC) algorithm. The authors show that the MCMC algorithm can be used to sample from a discrete distribution, and that it can also be used for inference and learning of discrete models. The paper also shows that the proposed algorithm outperforms other path auxiliary algorithms in terms of sample efficiency. "
3711,SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"Discovery and learning of an underlying spatiotemporal hierarchy PART-OF machine learning. sequential data USED-FOR Discovery and learning of an underlying spatiotemporal hierarchy. layerwise representations USED-FOR hierarchical generative models. Variational Predictive Routing ( VPR ) HYPONYM-OF neural probabilistic inference system. neural probabilistic inference system USED-FOR latent representations of video features. temporal hierarchy FEATURE-OF latent representations of video features. hierarchical renewal process USED-FOR continuous data. VPR USED-FOR organisation of representations. event detection mechanism USED-FOR VPR. organisation of representations PART-OF model. latent hierarchy USED-FOR organisation of representations. system USED-FOR event detection mechanism. latent representations USED-FOR event detection mechanism. VPR USED-FOR event boundaries. VPR USED-FOR timeagnostic rollouts. video datasets EVALUATE-FOR VPR. framework USED-FOR model - based reinforcement learning. approach USED-FOR framework. approach USED-FOR model - based reinforcement learning. neuroscience USED-FOR approach. OtherScientificTerm are spatiotemporal hierarchy, temporal dynamics, spatiotemporal features, hierarchy, and flexible and informative state - space rollouts. ","This paper proposes a neural probabilistic inference system for learning the latent representations of video features. The proposed method is based on the idea of hierarchical generative models. The authors propose a hierarchical renewal process to learn the latent representation of the video features, which is then used to predict the next state-space rollout of the model. The method is evaluated on a variety of video datasets and shows promising results. ","This paper proposes a neural probabilistic inference system for learning the latent representations of video features. The proposed method is based on the idea of hierarchical generative models. The authors propose a hierarchical renewal process to learn the latent representation of the video features, which is then used to predict the next state-space rollout of the model. The method is evaluated on a variety of video datasets and shows promising results. "
3727,SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,global features USED-FOR methods. re - ranking process USED-FOR global features. it USED-FOR accurate and semantic local information. spatial and channel attention CONJUNCTION intermediate supervision. intermediate supervision CONJUNCTION spatial and channel attention. convolutional neural networks COMPARE RANSAC algorithm. RANSAC algorithm COMPARE convolutional neural networks. it USED-FOR UGALR. spatial and channel attention USED-FOR it. intermediate supervision USED-FOR it. RANSAC algorithm USED-FOR local feature matching. spatial and channel attention USED-FOR accurate and semantic local information. convolutional neural networks USED-FOR local feature matching. Oxford and Paris datasets EVALUATE-FOR approach. Task is Image retrieval. OtherScientificTerm is features. Method is local feature learning. Metric is memory consumption. ,"This paper proposes a new local feature learning method for image retrieval. The proposed method is based on RANSAC, which combines spatial and channel attention to improve the performance of local feature matching. The method is evaluated on the Oxford and Paris datasets. The results show that the proposed method outperforms the state-of-the-art methods. ","This paper proposes a new local feature learning method for image retrieval. The proposed method is based on RANSAC, which combines spatial and channel attention to improve the performance of local feature matching. The method is evaluated on the Oxford and Paris datasets. The results show that the proposed method outperforms the state-of-the-art methods. "
3743,SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"computer vision CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION computer vision. Multitask learning USED-FOR applications domains. computer vision HYPONYM-OF applications domains. reinforcement learning HYPONYM-OF applications domains. algorithm USED-FOR negative transfer. it USED-FOR gradient magnitudes. RotoGrad USED-FOR negative transfer. RotoGrad HYPONYM-OF algorithm. multi - label classification CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION multi - label classification. RotoGrad COMPARE methods. methods COMPARE RotoGrad. CelebA CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION CelebA. RotoGrad USED-FOR complex problems. methods USED-FOR complex problems. NYUv2 dataset USED-FOR computer vision tasks. CelebA USED-FOR multi - label classification. computer vision tasks HYPONYM-OF complex problems. multi - label classification HYPONYM-OF complex problems. OtherScientificTerm are shared network parameters, gradient magnitude, gradient directions, and training convergence. Method is Pytorch implementation. ","This paper proposes a new method for multi-task learning based on the Pytorch implementation of RotoGrad. The main contribution of the paper is to propose a new algorithm for negative transfer, which is a generalization of the Roto-Grad algorithm. The key idea of the proposed method is to use the gradient magnitudes of the shared network parameters to learn the gradient directions. The authors show that the proposed algorithm is able to achieve better negative transfer performance than existing methods on the NYUv2 dataset.","This paper proposes a new method for multi-task learning based on the Pytorch implementation of RotoGrad. The main contribution of the paper is to propose a new algorithm for negative transfer, which is a generalization of the Roto-Grad algorithm. The key idea of the proposed method is to use the gradient magnitudes of the shared network parameters to learn the gradient directions. The authors show that the proposed algorithm is able to achieve better negative transfer performance than existing methods on the NYUv2 dataset."
3759,SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,soft neuron association USED-FOR pre - trained networks. soft neuron association USED-FOR Layer - wise model fusion. optimal transport USED-FOR Layer - wise model fusion. networks USED-FOR OTFusion. model fusion framework USED-FOR neural networks. CLAFusion HYPONYM-OF model fusion framework. cross - layer alignment USED-FOR model fusion framework. cross - layer alignment USED-FOR heterogeneous neural networks. unbalanced assignment problem USED-FOR cross - layer alignment problem. dynamic programming USED-FOR cross - layer alignment problem. cross - layer alignment USED-FOR framework. framework USED-FOR layer - wise model fusion. number of layers of neural networks USED-FOR framework. CLAFusion USED-FOR fused network. finetuning process USED-FOR it. finetuning process USED-FOR residual networks. residual networks EVALUATE-FOR it. CIFAR10 dataset EVALUATE-FOR residual networks. CIFAR10 dataset EVALUATE-FOR it. model compression CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION model compression. knowledge distillation USED-FOR teacher - student setting. Material is heterogeneous data. OtherScientificTerm is retraining. ,"This paper proposes a model fusion framework for heterogeneous neural networks. The proposed framework is based on the idea of cross-layer alignment, which is an unbalanced assignment problem. The authors propose a dynamic programming approach to solve this problem. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the baselines.","This paper proposes a model fusion framework for heterogeneous neural networks. The proposed framework is based on the idea of cross-layer alignment, which is an unbalanced assignment problem. The authors propose a dynamic programming approach to solve this problem. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the baselines."
3775,SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"generalization EVALUATE-FOR deep networks. supervised learning USED-FOR deep networks. implicit regularization FEATURE-OF overparameterized deep networks. stochastic gradient descent USED-FOR implicit regularization. SGD USED-FOR supervised learning. SGD USED-FOR implicit regularization. regularizer USED-FOR degenerate solutions. implicit regularization USED-FOR temporal difference learning. regularizer COMPARE supervised learning case. supervised learning case COMPARE regularizer. representations USED-FOR state - action pairs. bootstrapping USED-FOR deep network value function. deep network value function USED-FOR feature representations. bootstrapping USED-FOR feature representations. explicit regularizer USED-FOR implicit regularizer. DR3 HYPONYM-OF explicit regularizer. D4RL domains CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION D4RL domains. Atari 2600 games CONJUNCTION D4RL domains. D4RL domains CONJUNCTION Atari 2600 games. DR3 USED-FOR unlearning. DR3 USED-FOR robotic manipulation. performance CONJUNCTION stability. stability CONJUNCTION performance. unlearning CONJUNCTION D4RL domains. D4RL domains CONJUNCTION unlearning. offline RL methods COMPARE DR3. DR3 COMPARE offline RL methods. unlearning CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION unlearning. Atari 2600 games FEATURE-OF unlearning. stability EVALUATE-FOR DR3. images USED-FOR robotic manipulation. performance EVALUATE-FOR DR3. Method are overparameterization, and deep reinforcement learning ( RL ) methods. OtherScientificTerm are parsimonious solutions, degenerate feature representations, and Bellman backup. Task is offline deep RL setting. ","This paper proposes a new implicit regularization method for offline deep reinforcement learning (RL) called DR3. DR3 is based on the idea of bootstrapping, which is an implicit regularizer that is used to improve the generalization performance of deep RL models. The authors show that DR3 improves the performance and stability of offline RL models in a number of tasks. The main contribution of the paper is the introduction of DR3, a new regularization technique for offline RL. ","This paper proposes a new implicit regularization method for offline deep reinforcement learning (RL) called DR3. DR3 is based on the idea of bootstrapping, which is an implicit regularizer that is used to improve the generalization performance of deep RL models. The authors show that DR3 improves the performance and stability of offline RL models in a number of tasks. The main contribution of the paper is the introduction of DR3, a new regularization technique for offline RL. "
3791,SP:6fd793b27123bf80504e2ad5957455b7ec311612,RLSVI USED-FOR posterior samples. algorithm USED-FOR deep RL. HyperDQN HYPONYM-OF algorithm. non - linear neural network USED-FOR Q - values. base model HYPONYM-OF non - linear neural network. meta model HYPONYM-OF probabilistic hypermodel. probabilistic hypermodel USED-FOR method. hypermodel USED-FOR approximate posterior samples. Q - value functions USED-FOR exploratory action sequences. RLSVI USED-FOR exploration. posterior samples USED-FOR Q - value function. Atari suite EVALUATE-FOR HyperDQN. Atari suite EVALUATE-FOR DQN. HyperDQN COMPARE DQN. DQN COMPARE HyperDQN. maximum human - normalized score EVALUATE-FOR DQN. maximum human - normalized score EVALUATE-FOR HyperDQN. HyperDQN COMPARE exploration bonus and randomized exploration methods. exploration bonus and randomized exploration methods COMPARE HyperDQN. HyperDQN USED-FOR SuperMarioBros. exploration bonus and randomized exploration methods USED-FOR SuperMarioBros. Method is exploration method. OtherScientificTerm is feature. Generic is models. Metric is efficiency. ,"This paper proposes HyperDQN, an exploration method for deep RL. The proposed method is based on RLSVI, which is a probabilistic hyper-network. The authors propose to use a meta-model to learn the Q-value function for exploration. The hyper-model is then used to estimate the posterior samples of the action sequences. The method is evaluated on the SuperMarioBros benchmark and compared to DQN and other exploration methods. ","This paper proposes HyperDQN, an exploration method for deep RL. The proposed method is based on RLSVI, which is a probabilistic hyper-network. The authors propose to use a meta-model to learn the Q-value function for exploration. The hyper-model is then used to estimate the posterior samples of the action sequences. The method is evaluated on the SuperMarioBros benchmark and compared to DQN and other exploration methods. "
3807,SP:b428383660928374c953f659ea1e05852dbdcd6e,"representation learning USED-FOR model. image classification CONJUNCTION recommender systems. recommender systems CONJUNCTION image classification. representation learning USED-FOR downstream tasks. downstream tasks EVALUATE-FOR model. recommender systems HYPONYM-OF real - world scenarios. image classification HYPONYM-OF real - world scenarios. cause, effect and spurious correlated variables PART-OF representation. hypothetical causal graph USED-FOR mutual information measures. mutual information measures USED-FOR learning procedure. learning procedure USED-FOR causal representation. hypothetical causal graph USED-FOR learning procedure. observational data USED-FOR causal representation. reduced sample complexity CONJUNCTION generalization ability. generalization ability CONJUNCTION reduced sample complexity. counterfactual loss PART-OF optimization. reduced sample complexity EVALUATE-FOR causality - inspired learning. generalization ability EVALUATE-FOR causality - inspired learning. adversarial attacks CONJUNCTION distribution shift. distribution shift CONJUNCTION adversarial attacks. causal representations USED-FOR models. adversarial attacks USED-FOR models. approach USED-FOR causal representations. approach USED-FOR models. Method is learning approaches. OtherScientificTerm is features. Metric is generalizability. ",This paper proposes a method for learning causal representations from observational data. The proposed method is based on counterfactual loss. The authors show that the proposed method can reduce the sample complexity and improve the generalization ability of the model. The method is evaluated on image classification and recommender systems.,This paper proposes a method for learning causal representations from observational data. The proposed method is based on counterfactual loss. The authors show that the proposed method can reduce the sample complexity and improve the generalization ability of the model. The method is evaluated on image classification and recommender systems.
3823,SP:1258c05a80a17949b50e6dae13deea1d2235f456,Federated learning HYPONYM-OF distributed learning scheme. edge devices USED-FOR model. training USED-FOR edge devices. gradient compression CONJUNCTION distillation. distillation CONJUNCTION gradient compression. gradient compression HYPONYM-OF compact formats. distillation HYPONYM-OF compact formats. progressive training framework USED-FOR federated learning. ProgFed HYPONYM-OF progressive training framework. It COMPARE models. models COMPARE It. asymptotic rate EVALUATE-FOR ProgFed. ResNet CONJUNCTION ConvNets. ConvNets CONJUNCTION ResNet. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. ConvNets CONJUNCTION U - nets. U - nets CONJUNCTION ConvNets. computation CONJUNCTION communication costs. communication costs CONJUNCTION computation. simple classification CONJUNCTION medical image segmentation. medical image segmentation CONJUNCTION simple classification. communication costs EVALUATE-FOR converged models. VGG CONJUNCTION ConvNets. ConvNets CONJUNCTION VGG. training approach USED-FOR converged models. tasks EVALUATE-FOR training approach. tasks HYPONYM-OF architectures. medical image segmentation HYPONYM-OF tasks. simple classification HYPONYM-OF tasks. computation EVALUATE-FOR training approach. communication costs EVALUATE-FOR training approach. VGG HYPONYM-OF architectures. ConvNets HYPONYM-OF architectures. ResNet HYPONYM-OF architectures. U - nets HYPONYM-OF architectures. approach COMPARE compression. compression COMPARE approach. OtherScientificTerm is limited network bandwidth. Generic is full models. ,This paper proposes a progressive training framework for federated learning. The proposed method is based on gradient compression and distillation. The authors show that the proposed method achieves asymptotic rate and communication efficiency. ,This paper proposes a progressive training framework for federated learning. The proposed method is based on gradient compression and distillation. The authors show that the proposed method achieves asymptotic rate and communication efficiency. 
3839,SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"adversarial attacks FEATURE-OF Deep neural networks. Adversarial training USED-FOR model. adversarial Rademacher complexity FEATURE-OF adversarial training. two - layer neural networks USED-FOR adversarial Rademacher complexity. adversarial Rademacher complexity FEATURE-OF deep neural networks. Rademacher complexity EVALUATE-FOR neural nets. product of weight norms PART-OF bound. adversarially trained weight norms COMPARE trained weight norms. trained weight norms COMPARE adversarially trained weight norms. Generic are models, and method. Task is adversarial settings. OtherScientificTerm is layer. ",This paper studies the adversarial Rademacher complexity of two-layer neural networks. The authors show that adversarial training can lead to a reduction in adversarial robustness. They also provide a bound on the adversarially robustness of the weight norms.,This paper studies the adversarial Rademacher complexity of two-layer neural networks. The authors show that adversarial training can lead to a reduction in adversarial robustness. They also provide a bound on the adversarially robustness of the weight norms.
3855,SP:925d6bb051e9b384669fb695085b678c11f7c11a,Estimation of ( differential ) entropy CONJUNCTION mutual information. mutual information CONJUNCTION Estimation of ( differential ) entropy. estimators USED-FOR differential entropy. approach USED-FOR KNIFE - based estimators. KNIFE - based estimators USED-FOR mutual information. neural networks USED-FOR real - world tasks. it USED-FOR neural networks. high - dimensional synthetic data EVALUATE-FOR method. visual domain adaptation CONJUNCTION textual fair classification. textual fair classification CONJUNCTION visual domain adaptation. textual fair classification CONJUNCTION textual fine - tuning. textual fine - tuning CONJUNCTION textual fair classification. tasks EVALUATE-FOR KNIFE - based estimation. textual fine - tuning EVALUATE-FOR KNIFE - based estimation. textual fine - tuning HYPONYM-OF tasks. visual domain adaptation HYPONYM-OF tasks. textual fair classification HYPONYM-OF tasks. Method is KNIFE. ,This paper proposes a novel method for estimating differential entropy for neural networks. The proposed method is based on the framework of differential entropy estimation (KNIFE) and mutual information estimation (MIE). The authors show that the proposed method can be used to estimate the differential entropy of neural networks on synthetic data and real-world datasets. The authors also show that their method is able to achieve better performance than existing methods. ,This paper proposes a novel method for estimating differential entropy for neural networks. The proposed method is based on the framework of differential entropy estimation (KNIFE) and mutual information estimation (MIE). The authors show that the proposed method can be used to estimate the differential entropy of neural networks on synthetic data and real-world datasets. The authors also show that their method is able to achieve better performance than existing methods. 
3871,SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. action - value methods USED-FOR reinforcement learning. Soft - greedy operators USED-FOR exploration. exploration USED-FOR action - value methods. ε - greedy HYPONYM-OF Soft - greedy operators. softmax HYPONYM-OF Soft - greedy operators. resmax HYPONYM-OF soft - greedy operator. It USED-FOR coverage of the state - space. It USED-FOR exploration. it COMPARE softmax. softmax COMPARE it. non - expansion USED-FOR it. exploration hyperparameter USED-FOR non - expansion. mellowmax HYPONYM-OF non - expansion. state - action specific temperature USED-FOR softmax policy. resmax COMPARE softmax. softmax COMPARE resmax. resmax COMPARE ε - greedy. ε - greedy COMPARE resmax. ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. Generic is operators. OtherScientificTerm are suboptimality gap, and overemphasizing sub - optimal actions. Task is learning. Material is tabular and deep RL. ","This paper studies the problem of exploration in reinforcement learning. The authors propose soft-greedy operators for action-value reinforcement learning, which they call softmax and resmax. They show that softmax is a non-expansion of the existing soft greedy operators, and that it can be used for exploration in tabular and deep RL. They also propose a new exploration hyperparameter, called mellowmax, which is based on the softmax policy. ","This paper studies the problem of exploration in reinforcement learning. The authors propose soft-greedy operators for action-value reinforcement learning, which they call softmax and resmax. They show that softmax is a non-expansion of the existing soft greedy operators, and that it can be used for exploration in tabular and deep RL. They also propose a new exploration hyperparameter, called mellowmax, which is based on the softmax policy. "
3887,SP:792ae8808aa6902758146aef1548c975492b833c,"learnability FEATURE-OF deep learning models. concept USED-FOR model. concept USED-FOR learnability. learnability lock USED-FOR model. learnability lock USED-FOR learnability. learnability EVALUATE-FOR model. learnability FEATURE-OF dataset. universal transformation function USED-FOR class - wise perturbation. class - wise perturbation USED-FOR learnability lock. inverse transformation USED-FOR learnability. visual classification tasks EVALUATE-FOR method. Task are information technology, learnability attack, and preventing unauthorized exploitation. Method are deep learning, commercial models, adversarial invertible transformation, and machine learning models. OtherScientificTerm are digital formats, and visual features. Material is image. Generic is models. ","This paper studies the problem of adversarial invertible transformation (AI) attacks on deep learning models. The authors propose a new concept called learnability lock, which is a universal transformation function that can be applied to any class-wise perturbation. The proposed method is evaluated on a variety of image classification tasks and shows that the proposed method outperforms the baselines.","This paper studies the problem of adversarial invertible transformation (AI) attacks on deep learning models. The authors propose a new concept called learnability lock, which is a universal transformation function that can be applied to any class-wise perturbation. The proposed method is evaluated on a variety of image classification tasks and shows that the proposed method outperforms the baselines."
3903,SP:9af10703605e620e563241e2602a50b629f3d37a,"Graph Neural Networks ( GNNs ) USED-FOR modeling relational data. node or edge features FEATURE-OF graph. features PART-OF real - world applications. approach USED-FOR missing features. approach USED-FOR graph machine learning applications. approach USED-FOR diffusion - type differential equation. graph FEATURE-OF diffusion - type differential equation. minimization of the Dirichlet energy USED-FOR approach. Feature Propagation HYPONYM-OF algorithm. approach COMPARE methods. methods COMPARE approach. missing features EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR methods. nodes CONJUNCTION edges. edges CONJUNCTION nodes. edges PART-OF GPU. nodes FEATURE-OF graph. edges PART-OF graph. Generic are they, and equation. Material is social networks. ","This paper proposes a novel method for learning missing features in graph neural networks (GNNs). The proposed method is based on the diffusion-type differential equation (DDE), which is a generalization of the Dirichlet energy of the graph. The authors show that the proposed method, Feature Propagation (SPP), can be used to learn missing features of a graph. They show that SPP is able to learn the missing features for node classification and edge classification tasks. They also show that their method can be applied to graph machine learning applications. ","This paper proposes a novel method for learning missing features in graph neural networks (GNNs). The proposed method is based on the diffusion-type differential equation (DDE), which is a generalization of the Dirichlet energy of the graph. The authors show that the proposed method, Feature Propagation (SPP), can be used to learn missing features of a graph. They show that SPP is able to learn the missing features for node classification and edge classification tasks. They also show that their method can be applied to graph machine learning applications. "
3919,SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"limited labeled data USED-FOR model. heuristics USED-FOR sample selection strategies. integer optimization problem USED-FOR core set. discrete Wasserstein distance FEATURE-OF unlabeled pool. Generalized Benders Decomposition algorithm USED-FOR problem. unlabeled pool USED-FOR unsupervised learning. unsupervised learning USED-FOR latent features. latent features USED-FOR strategy. optimization approach COMPARE baselines. baselines COMPARE optimization approach. data sets EVALUATE-FOR optimization approach. optimization approach COMPARE them. them COMPARE optimization approach. data sets EVALUATE-FOR baselines. them USED-FOR low budget regime. optimization approach USED-FOR low budget regime. Method are Active learning, and deep learning. OtherScientificTerm is unlabeled data pool. ","This paper studies the problem of unsupervised active learning with unlabeled data. The authors propose a generalized Benders Decomposition algorithm to solve the problem. The proposed method is based on the idea of discrete Wasserstein distance between the core set and the unlabelled pool, which is an integer optimization problem. They show that the proposed method outperforms the baselines in the low budget regime. ","This paper studies the problem of unsupervised active learning with unlabeled data. The authors propose a generalized Benders Decomposition algorithm to solve the problem. The proposed method is based on the idea of discrete Wasserstein distance between the core set and the unlabelled pool, which is an integer optimization problem. They show that the proposed method outperforms the baselines in the low budget regime. "
3935,SP:4c72923f78ca6590dc11e10d1a2403076a583718,"manual inspection USED-FOR genome reconstruction. approach USED-FOR assembling genomes. method USED-FOR approach. method USED-FOR assembling genomes. geometric deep learning USED-FOR genome assembly. geometric deep learning USED-FOR assembly graph. genomic sequence USED-FOR assembly graph. graph convolutional network USED-FOR genome. dataset USED-FOR graph convolutional network. human genomic data USED-FOR graph convolutional network. human genomic data USED-FOR dataset. greedy search algorithm COMPARE greedy search. greedy search COMPARE greedy search algorithm. greedy search algorithm USED-FOR graph topology. greedy search algorithm USED-FOR model. graph machine learning algorithms USED-FOR de novo genome assembly problem. graph machine learning algorithms COMPARE human handcrafted techniques. human handcrafted techniques COMPARE graph machine learning algorithms. Material is human DNA sequence. OtherScientificTerm are telomere, and graph. Metric is assembly speed. Generic is it. Method is de novo assemblers. ",This paper proposes a method for genome assembly based on geometric deep learning. The method is based on graph convolutional neural networks. The authors propose a greedy search algorithm to find the assembly graph topology of a genome. The proposed method is evaluated on a synthetic and real-world dataset. The results show that the proposed method outperforms existing methods.,This paper proposes a method for genome assembly based on geometric deep learning. The method is based on graph convolutional neural networks. The authors propose a greedy search algorithm to find the assembly graph topology of a genome. The proposed method is evaluated on a synthetic and real-world dataset. The results show that the proposed method outperforms existing methods.
3951,SP:24de906e4289c9073b6c55c747b0913b8df5e053,"catastrophic forgetting FEATURE-OF Continual learning. meta - learning USED-FOR metacontinual learning algorithms. experience replay ( ER ) PART-OF meta - testing. ER USED-FOR meta - testing. ER USED-FOR metatraining. ER USED-FOR continual learning representations. ER PART-OF meta - training. reservoir sampling USED-FOR replay buffer. meta - learned Predictive Sample Selection USED-FOR replay buffer. meta - learned Predictive Sample Selection COMPARE reservoir sampling. reservoir sampling COMPARE meta - learned Predictive Sample Selection. method COMPARE state - of - the - art. state - of - the - art COMPARE method. clustering structures FEATURE-OF learned representations. Method are online aware meta - learning ( OML ), and OML. Generic is model. Task is online - aware nature of OML. OtherScientificTerm is randomness. ",This paper proposes an online-aware meta-learning method that uses experience replay (ER) for meta-testing in continual learning. ER is an important component of meta-training and is used to improve the performance of continual learning algorithms. The proposed method is based on the idea that experience replay can be used as a replay buffer for continual learning in order to mitigate catastrophic forgetting. The authors show that the proposed method outperforms the baselines in terms of performance on a variety of tasks. ,This paper proposes an online-aware meta-learning method that uses experience replay (ER) for meta-testing in continual learning. ER is an important component of meta-training and is used to improve the performance of continual learning algorithms. The proposed method is based on the idea that experience replay can be used as a replay buffer for continual learning in order to mitigate catastrophic forgetting. The authors show that the proposed method outperforms the baselines in terms of performance on a variety of tasks. 
3967,SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"Multi - agent joint Q - learning USED-FOR multi - agent cooperation. Centralized Training with Decentralized Execution ( CTDE ) USED-FOR Multi - agent joint Q - learning. methods USED-FOR multi - agent credit assignment problem. Bellman optimality equation FEATURE-OF joint Q - value. Bellman optimality FEATURE-OF joint Q - value. Q - values USED-FOR joint Q - value. gradient ascent solution USED-FOR problem. ECAQ COMPARE baselines. baselines COMPARE ECAQ. ECAQ USED-FOR credit assignment. Method are centralized training, and deep neural networks. Task are explicit credit assignment problem, and multi - agent cooperation in complex problems. OtherScientificTerm is time horizon. ","This paper studies the problem of multi-agent joint Q-learning in the context of explicit credit assignment. The authors propose a new method, called ECAQ, that combines centralized training with Decentralized Execution (CTDE) to solve the problem. The proposed method is based on the Bellman optimality equation, and the authors show that the proposed method achieves better performance than the baselines. ","This paper studies the problem of multi-agent joint Q-learning in the context of explicit credit assignment. The authors propose a new method, called ECAQ, that combines centralized training with Decentralized Execution (CTDE) to solve the problem. The proposed method is based on the Bellman optimality equation, and the authors show that the proposed method achieves better performance than the baselines. "
3983,SP:0d2b225ac697679d10df25f371b2a718d4949b42,"transductive learning USED-FOR adversarial robustness. defenses COMPARE defense mechanisms. defense mechanisms COMPARE defenses. defenses USED-FOR bilevel optimization problem. test - time input USED-FOR model. threat analysis perspective EVALUATE-FOR defense mechanisms. threat models USED-FOR transductive - learning based defenses. attacking model space USED-FOR bilevel attack objectives. Greedy Model Space Attack ( GMSA ) HYPONYM-OF attack framework. GMSA USED-FOR transductive - learning based defenses. weak instantiations USED-FOR GMSA. Material is NeurIPS 2020. Task are ICML 2020, and adaptive attacks. Method are transductivelearning based defenses, and transductive adversarial training. OtherScientificTerm is AutoAttack. Metric is robustness. ","This paper proposes a new adversarial robustness metric to measure the robustness of a model to adversarial attacks. The metric is based on the bilevel optimization problem. The authors propose a new attack framework called Greedy Model Space Attack (GMSA), which is a variant of AutoAttack (AutoAttack). GMSA exploits the fact that the test-time input of the model is not always the same as the target model's test time input, and proposes to use a weak instantiation of weak instantiations to attack the model. GMSA is evaluated on the NeurIPS 2020 benchmark and shows that GMSA outperforms AutoAttack.","This paper proposes a new adversarial robustness metric to measure the robustness of a model to adversarial attacks. The metric is based on the bilevel optimization problem. The authors propose a new attack framework called Greedy Model Space Attack (GMSA), which is a variant of AutoAttack (AutoAttack). GMSA exploits the fact that the test-time input of the model is not always the same as the target model's test time input, and proposes to use a weak instantiation of weak instantiations to attack the model. GMSA is evaluated on the NeurIPS 2020 benchmark and shows that GMSA outperforms AutoAttack."
3999,SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"batch normalization USED-FOR training of neural networks. batch renormalization USED-FOR small minibatches. function class FEATURE-OF inference model. Method are neural networks, and per - example training procedure. OtherScientificTerm are gradient, and identity shortcuts. Generic are approximation, and normalization. Metric are training step computation, and model accuracy. ","This paper studies the problem of batch normalization for training of neural networks. The authors show that for small minibatches, batch renormalization can be used to improve the performance of the model. They show that this is a generalization of the idea of identity shortcuts. They also show that batch normalisation can be applied to the per-example training procedure. ","This paper studies the problem of batch normalization for training of neural networks. The authors show that for small minibatches, batch renormalization can be used to improve the performance of the model. They show that this is a generalization of the idea of identity shortcuts. They also show that batch normalisation can be applied to the per-example training procedure. "
4015,SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,large - scale pretraining CONJUNCTION adaptation. adaptation CONJUNCTION large - scale pretraining. general domain data USED-FOR large - scale pretraining. large - scale pretraining PART-OF natural language processing. fine - tuning USED-FOR models. trainable parameters USED-FOR downstream tasks. trainable rank decomposition matrices PART-OF Transformer architecture. LoRA HYPONYM-OF Low - Rank Adaptation. GPT-3 175B COMPARE LoRA. LoRA COMPARE GPT-3 175B. LoRA USED-FOR trainable parameters. GPU memory requirement EVALUATE-FOR LoRA. Adam USED-FOR GPT-3 175B. LoRA COMPARE finetuning. finetuning COMPARE LoRA. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. RoBERTa EVALUATE-FOR finetuning. model quality EVALUATE-FOR finetuning. model quality EVALUATE-FOR LoRA. Method is fine - tuned models. Metric is training throughput. OtherScientificTerm is inference latency. Task is language model adaptation. ,This paper proposes a low-rank adaptation (LoRA) method for language model adaptation. The proposed method is based on the Adam Transformer architecture. The main idea of LoRA is to use the rank decomposition matrices of the Transformer to learn a set of trainable parameters for downstream tasks. Experiments show that LoRA outperforms finetuning and fine-tuning in terms of inference latency.,This paper proposes a low-rank adaptation (LoRA) method for language model adaptation. The proposed method is based on the Adam Transformer architecture. The main idea of LoRA is to use the rank decomposition matrices of the Transformer to learn a set of trainable parameters for downstream tasks. Experiments show that LoRA outperforms finetuning and fine-tuning in terms of inference latency.
4031,SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"CRFs USED-FOR distributions with nonlocal dependencies. nonlocal constraints FEATURE-OF CRFs. global arity constraints HYPONYM-OF nonlocal constraints. CRFs USED-FOR constraints. nonlocal ones HYPONYM-OF constraints. regular - constrained CRF ( RegCCRF ) COMPARE CRF. CRF COMPARE regular - constrained CRF ( RegCCRF ). RegCCRFs COMPARE models. models COMPARE RegCCRFs. constraints PART-OF training. constraints FEATURE-OF decoding. constraints PART-OF RegCCRFs. constrained training COMPARE constrained decoding. constrained decoding COMPARE constrained training. deep neural model USED-FOR semantic role labeling. RegCCRF PART-OF deep neural model. RegCCRF USED-FOR semantic role labeling. dataset EVALUATE-FOR RegCCRF. RegCCRF USED-FOR downstream tasks. Task is structured prediction. OtherScientificTerm are local dependencies, CRF ’s Markov assumption, and output structures. Generic is it. ","This paper proposes a regular-constrained CRF (RegCCRF) model for structured prediction. RegCCRF is a variant of CRF with global arity constraints. The main difference between the two models is that the global constraints are not imposed on the input distribution, but on the output distribution. The authors show that the proposed model outperforms the state-of-the-art models in the task of semantic role labeling. ","This paper proposes a regular-constrained CRF (RegCCRF) model for structured prediction. RegCCRF is a variant of CRF with global arity constraints. The main difference between the two models is that the global constraints are not imposed on the input distribution, but on the output distribution. The authors show that the proposed model outperforms the state-of-the-art models in the task of semantic role labeling. "
4047,SP:74c186a96c12adff178264aa84ace8d04dc7d725,"preprocessing steps USED-FOR methods. computational budget EVALUATE-FOR core ” network. normalization CONJUNCTION color space transformation. color space transformation CONJUNCTION normalization. segmentation CONJUNCTION normalization. normalization CONJUNCTION segmentation. face detection CONJUNCTION segmentation. segmentation CONJUNCTION face detection. neural models USED-FOR camera - based physiological measurement. color space transformation CONJUNCTION preprocessing steps. preprocessing steps CONJUNCTION color space transformation. face detection USED-FOR neural models. segmentation HYPONYM-OF neural models. normalization HYPONYM-OF neural models. preprocessing steps PART-OF neural models. EfficientPhys HYPONYM-OF neural models. raw video frames USED-FOR models. accuracy EVALUATE-FOR models. latency EVALUATE-FOR networks. efficiency EVALUATE-FOR light weight network. Task are Camera - based physiological measurement, and replication. Method are endto - end ” models, and transformer or convolutional backbone. Generic is operations. ","This paper proposes a new method for camera-based physiological measurement. The method is based on the transformer or convolutional backbone. The authors show that the proposed method is able to achieve comparable performance to state-of-the-art methods in terms of latency and computational budget. They also show that their method can be applied to face detection, segmentation, and normalization.","This paper proposes a new method for camera-based physiological measurement. The method is based on the transformer or convolutional backbone. The authors show that the proposed method is able to achieve comparable performance to state-of-the-art methods in terms of latency and computational budget. They also show that their method can be applied to face detection, segmentation, and normalization."
4063,SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"Structural pruning USED-FOR network architecture. inference speed EVALUATE-FOR Structural pruning. Hardware - Aware Latency Pruning ( HALP ) USED-FOR structural pruning. global resource allocation optimization problem USED-FOR structural pruning. latency reduction potential CONJUNCTION global saliency score. global saliency score CONJUNCTION latency reduction potential. latency lookup table USED-FOR latency reduction potential. latency lookup table USED-FOR global saliency score. HALP USED-FOR accuracy drop. HALP USED-FOR global saliency score. HALP USED-FOR latency reduction potential. latency lookup table USED-FOR HALP. HALP USED-FOR filter importance ranking. latency lookup table USED-FOR filter importance ranking. metrics EVALUATE-FOR pruning. metrics USED-FOR global structural pruning. reward maximization problem USED-FOR global structural pruning. pruning efficacy CONJUNCTION accuracy - efficiency trade - off. accuracy - efficiency trade - off CONJUNCTION pruning efficacy. pruning efficacy EVALUATE-FOR HALP. accuracy - efficiency trade - off EVALUATE-FOR HALP. augmented knapsack solver USED-FOR problem. HALP USED-FOR classification and detection tasks. ImageNet and VOC datasets USED-FOR classification and detection tasks. ImageNet and VOC datasets EVALUATE-FOR HALP. ImageNet USED-FOR ResNet-50/-101 pruning. HALP USED-FOR ResNet-50/-101 pruning. network throughput EVALUATE-FOR HALP. HALP USED-FOR SSD pruning. VOC EVALUATE-FOR SSD pruning. throughput EVALUATE-FOR HALP. HALP COMPARE prior art. prior art COMPARE HALP. Metric is accuracy. OtherScientificTerm are latency, and top-1 accuracy changes. ","This paper proposes Hardware-Aware Latency Pruning (HALP), a new method for structural pruning. The proposed method is based on a global resource allocation optimization problem, where the goal is to optimize the global saliency score and the latency reduction potential. The paper also proposes an augmented knapsack solver to solve the problem. Experiments on ImageNet and VOC datasets show that HALP outperforms prior methods.","This paper proposes Hardware-Aware Latency Pruning (HALP), a new method for structural pruning. The proposed method is based on a global resource allocation optimization problem, where the goal is to optimize the global saliency score and the latency reduction potential. The paper also proposes an augmented knapsack solver to solve the problem. Experiments on ImageNet and VOC datasets show that HALP outperforms prior methods."
4079,SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"permutation invariance CONJUNCTION multi - objective generation. multi - objective generation CONJUNCTION permutation invariance. GraphEBM HYPONYM-OF molecular graph generation method. GraphEBM USED-FOR permutation invariant and multi - objective molecule generation. energy - based models ( EBMs ) USED-FOR molecular graph generation method. GraphEBM USED-FOR permutation invariant distribution. EBMs CONJUNCTION parameterized permutation - invariant energy function. parameterized permutation - invariant energy function CONJUNCTION EBMs. parameterized permutation - invariant energy function USED-FOR GraphEBM. molecular graphs FEATURE-OF permutation invariant distribution. contrastive divergence USED-FOR energy function. compositional generation USED-FOR drug discovery. compositional generation USED-FOR GraphEBM. Task is molecular graph generation. Method are Langevin dynamics, and learning strategy. OtherScientificTerm is flexible degrees. Generic is method. ",This paper proposes a new method for molecular graph generation based on energy-based models (EBMs). The main idea is to learn a parameterized permutation-invariant energy function that is invariant to permutation invariance and multi-objective molecule generation. The proposed method is based on the Langevin dynamics of the molecular graph. The authors show that the proposed method outperforms existing methods in terms of the number of molecules generated and the quality of the generated molecules. ,This paper proposes a new method for molecular graph generation based on energy-based models (EBMs). The main idea is to learn a parameterized permutation-invariant energy function that is invariant to permutation invariance and multi-objective molecule generation. The proposed method is based on the Langevin dynamics of the molecular graph. The authors show that the proposed method outperforms existing methods in terms of the number of molecules generated and the quality of the generated molecules. 
4095,SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"approaches USED-FOR program synthesis. neural models USED-FOR combinatorial search algorithms. neural model USED-FOR hands - on search policy. hands - on search policy USED-FOR bottom - up synthesis. search history CONJUNCTION partial program executions. partial program executions CONJUNCTION search history. neural model USED-FOR approach. bottom - up searches USED-FOR data. - policy USED-FOR CROSSBEAM. data USED-FOR - policy. data USED-FOR CROSSBEAM. string manipulation CONJUNCTION logic programming. logic programming CONJUNCTION string manipulation. string manipulation EVALUATE-FOR CROSSBEAM. domains EVALUATE-FOR CROSSBEAM. string manipulation HYPONYM-OF domains. logic programming HYPONYM-OF domains. OtherScientificTerm are search space, search space blowup, and program space. Method is combinatorial search algorithm. Task is structured prediction. Generic is state - of - the - art. ","This paper proposes a new combinatorial search algorithm for program synthesis. The proposed method is based on a neural model and a hand-on search policy. The model is trained on top of the search history and partial program executions, and the search policy is trained to find the best candidate program in the search space. Experiments show that the proposed method outperforms the state-of-the-art.","This paper proposes a new combinatorial search algorithm for program synthesis. The proposed method is based on a neural model and a hand-on search policy. The model is trained on top of the search history and partial program executions, and the search policy is trained to find the best candidate program in the search space. Experiments show that the proposed method outperforms the state-of-the-art."
4111,SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,minimizing the squared Bellman error USED-FOR Deep Reinforcement Learning. target networks USED-FOR training. lagging parameters USED-FOR target networks. functional regularizer USED-FOR squared Bellman error. target networks COMPARE regularization. regularization COMPARE target networks. up - to - date parameters USED-FOR regularization. Atari environments EVALUATE-FOR target - network based methods. sample efficiency CONJUNCTION performance. performance CONJUNCTION sample efficiency. performance EVALUATE-FOR target - network based methods. sample efficiency EVALUATE-FOR target - network based methods. approach COMPARE squared Bellman error. squared Bellman error COMPARE approach. OtherScientificTerm is fast - changing target Q - values. Method is training method. ,"This paper proposes a new training method for deep reinforcement learning based on the squared Bellman error. The proposed method is based on a functional regularizer, which can be applied to the target Q-values. The authors show that the proposed method outperforms the baselines in terms of sample efficiency, sample complexity, and training time. They also show that their method can be used to improve the sample efficiency of target-network based methods.","This paper proposes a new training method for deep reinforcement learning based on the squared Bellman error. The proposed method is based on a functional regularizer, which can be applied to the target Q-values. The authors show that the proposed method outperforms the baselines in terms of sample efficiency, sample complexity, and training time. They also show that their method can be used to improve the sample efficiency of target-network based methods."
4127,SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"neighborhood subgraphs FEATURE-OF hierarchy of local isomorphism. message - passing GNNs COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE message - passing GNNs. model COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE model. model USED-FOR graph structures. Weisfeiler Lehman test USED-FOR graph structures. GraphSNN HYPONYM-OF neural model. graph learning tasks EVALUATE-FOR model. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. benchmark tasks EVALUATE-FOR state - of - the - art methods. benchmark tasks EVALUATE-FOR model. Method are Graph Neural Networks ( GNNs ), and GNNs. OtherScientificTerm is structural properties of graphs. ","This paper proposes a graph neural network (GNN) based on the Weisfeiler-Lehman test for graph structure learning. The proposed method, GraphSNN, is able to learn the structure of a graph by learning a hierarchy of local isomorphism. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark tasks. ","This paper proposes a graph neural network (GNN) based on the Weisfeiler-Lehman test for graph structure learning. The proposed method, GraphSNN, is able to learn the structure of a graph by learning a hierarchy of local isomorphism. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark tasks. "
4143,SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"prediction interval ( PI ) method USED-FOR uncertainty quantification. retraining of neural networks ( NNs ) USED-FOR confidence level. retraining of neural networks ( NNs ) USED-FOR PI methods. fine tuning USED-FOR well - calibrated PI. sensitive hyperparameters FEATURE-OF customized loss functions. customized loss functions USED-FOR they. PI3NN method USED-FOR PIs. standard mean squared error loss USED-FOR NNs. root - finding algorithms USED-FOR PIs. root - finding algorithms USED-FOR linear combinations. PI3NN USED-FOR PIs. it USED-FOR crossing issue. OOD samples COMPARE in - distribution samples. in - distribution samples COMPARE OOD samples. initialization scheme USED-FOR PIs. PIs FEATURE-OF OOD samples. initialization scheme USED-FOR OOD samples. initialization scheme USED-FOR in - distribution samples. initialization scheme USED-FOR OOD identification challenge. predictive uncertainty quality CONJUNCTION robustness. robustness CONJUNCTION predictive uncertainty quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. robustness CONJUNCTION OOD samples identification. OOD samples identification CONJUNCTION robustness. OOD samples identification EVALUATE-FOR method. robustness EVALUATE-FOR state - of - the - art approaches. robustness EVALUATE-FOR method. predictive uncertainty quality EVALUATE-FOR state - of - the - art approaches. predictive uncertainty quality EVALUATE-FOR method. OtherScientificTerm are over - confident PIs, confidence levels, and hyperparameters. Method is retraining NNs. ","This paper proposes a method to improve the predictive uncertainty quantification (PI) of neural networks (NNs) by retraining them on over-confident PIs. The authors propose a new method called PI3NN, which is based on the root-finding algorithm. The main idea of the method is to use the standard mean squared error loss (SDE) of the NN to compute the confidence level of the prediction interval (PI). The authors show that the proposed method outperforms existing methods in terms of OOD samples identification and robustness. ","This paper proposes a method to improve the predictive uncertainty quantification (PI) of neural networks (NNs) by retraining them on over-confident PIs. The authors propose a new method called PI3NN, which is based on the root-finding algorithm. The main idea of the method is to use the standard mean squared error loss (SDE) of the NN to compute the confidence level of the prediction interval (PI). The authors show that the proposed method outperforms existing methods in terms of OOD samples identification and robustness. "
4159,SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"deep networks USED-FOR functions. classifiers CONJUNCTION detectors. detectors CONJUNCTION classifiers. detectors CONJUNCTION trackers. trackers CONJUNCTION detectors. deep networks USED-FOR classifiers. models USED-FOR applications. trackers HYPONYM-OF functions. classifiers HYPONYM-OF functions. detectors HYPONYM-OF functions. learning algorithms USED-FOR slow adaptation. gradient descent HYPONYM-OF learning algorithms. learning algorithms USED-FOR model. Meta - learning USED-FOR adaptation. metalearning USED-FOR online problems. meta - learning USED-FOR online setting. known ground - truth task boundaries FEATURE-OF discrete notion of tasks. discrete notion of tasks USED-FOR they. discrete boundaries PART-OF real - world settings. ground truth knowledge FEATURE-OF task boundaries. FOML COMPARE online learning methods. online learning methods COMPARE FOML. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR online learning methods. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR FOML. OtherScientificTerm are input distributions, and environmental conditions. Method are intelligent system, and Fully Online MetaLearning ( FOML ) algorithm. Task is complex and high - dimensional problems. Generic is methods. ","This paper proposes a meta-learning method for online learning. Meta-learning is an extension of metalearning to the online setting, where the goal is to learn a model that can adapt to new tasks in an online setting. The proposed method is based on the notion of discrete notion of tasks, which is a generalization of the work of [1] and [2]. The authors show that the proposed method outperforms existing methods on Rainbow-MNIST and CIFAR-100 datasets. ","This paper proposes a meta-learning method for online learning. Meta-learning is an extension of metalearning to the online setting, where the goal is to learn a model that can adapt to new tasks in an online setting. The proposed method is based on the notion of discrete notion of tasks, which is a generalization of the work of [1] and [2]. The authors show that the proposed method outperforms existing methods on Rainbow-MNIST and CIFAR-100 datasets. "
4175,SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"chemical science and engineering task USED-FOR applications. structural design of functional molecules HYPONYM-OF chemical science and engineering task. molecular optimization HYPONYM-OF structural design of functional molecules. drug discovery HYPONYM-OF applications. Deep generative models CONJUNCTION combinatorial optimization methods. combinatorial optimization methods CONJUNCTION Deep generative models. knowledge network USED-FOR discrete chemical structures. knowledge network USED-FOR differentiable scaffolding tree ( DST ). DST USED-FOR gradient - based optimization. chemical graph structure USED-FOR gradient - based optimization. Method are brute - force enumeration, graph neural network ( GNN ), and gradient - based molecular optimizations. OtherScientificTerm are molecule structures, locally differentiable ones, derivatives, graph parameters, and domain experts. ","This paper proposes a novel approach for molecular optimization based on graph neural networks (GNNs). The authors propose a differentiable scaffolding tree (DST) to learn the chemical graph structure of a molecule, which is then used for gradient-based optimization. The DST is based on a graph neural network that learns the graph parameters of the molecule. The authors show that the DST can be used to solve the optimization problem of molecular optimization. ","This paper proposes a novel approach for molecular optimization based on graph neural networks (GNNs). The authors propose a differentiable scaffolding tree (DST) to learn the chemical graph structure of a molecule, which is then used for gradient-based optimization. The DST is based on a graph neural network that learns the graph parameters of the molecule. The authors show that the DST can be used to solve the optimization problem of molecular optimization. "
4191,SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"diseases CONJUNCTION lab tests. lab tests CONJUNCTION diseases. patient representation USED-FOR sequential information. drug - lab interactions CONJUNCTION diagnosis - lab interactions. diagnosis - lab interactions CONJUNCTION drug - lab interactions. graphs USED-FOR drug - lab interactions. graphs USED-FOR diagnosis - lab interactions. real - world datasets EVALUATE-FOR solution. prediction errors EVALUATE-FOR solution. Method are Personalized medical systems, and knowledge - augmented approach. OtherScientificTerm is lab test responses. ","This paper proposes a knowledge-augmented approach to improve the predictive performance of medical systems. The proposed method is based on the idea of knowledge augmentation, which is an extension of prior work on knowledge-autoencoders. The key idea is to learn a graph representation of a patient’s response to a test, and then use this representation to predict the next test response. The method is evaluated on a number of real-world datasets and shows that the proposed method outperforms prior work. ","This paper proposes a knowledge-augmented approach to improve the predictive performance of medical systems. The proposed method is based on the idea of knowledge augmentation, which is an extension of prior work on knowledge-autoencoders. The key idea is to learn a graph representation of a patient’s response to a test, and then use this representation to predict the next test response. The method is evaluated on a number of real-world datasets and shows that the proposed method outperforms prior work. "
4207,SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"Single domain generalization ( SDG ) HYPONYM-OF domain generalization. diversity of source domain USED-FOR robust model. adversarial data augmentation strategy USED-FOR SDG methods. OS - SDG USED-FOR model. CrossMatch approach USED-FOR SDG methods. SDG methods USED-FOR identifying unknown classes. multi - binary classifier USED-FOR SDG methods. adversarial data augmentation strategy USED-FOR CrossMatch. model USED-FOR unknown class identification. multibinary classifiers CONJUNCTION model. model CONJUNCTION multibinary classifiers. consistency regularization USED-FOR auxiliary samples. consistency regularization USED-FOR model. SDG methods USED-FOR model. CrossMatch USED-FOR SDG methods. benchmark datasets EVALUATE-FOR CrossMatch. SDG methods USED-FOR OS - SDG setting. benchmark datasets EVALUATE-FOR SDG methods. OtherScientificTerm are label space, source label space, target domains, and unknown classes. Task is real - world applications. ","This paper proposes CrossMatch, a method for single-domain generalization (SDG) that is based on adversarial data augmentation to improve the robustness of source domain generalization. CrossMatch is an extension of the CrossMatch approach to the multi-binary classifier setting, where the source domain and target domain have different labels. The proposed method is evaluated on a number of benchmark datasets and shows that CrossMatch outperforms existing SDG methods. ","This paper proposes CrossMatch, a method for single-domain generalization (SDG) that is based on adversarial data augmentation to improve the robustness of source domain generalization. CrossMatch is an extension of the CrossMatch approach to the multi-binary classifier setting, where the source domain and target domain have different labels. The proposed method is evaluated on a number of benchmark datasets and shows that CrossMatch outperforms existing SDG methods. "
4223,SP:126f8ffb855aa22eda4d681a499953879ed3679e,Trust - region methods USED-FOR policy optimization. policy optimization USED-FOR reinforcement learning. Kullback - Leibler divergence USED-FOR Trust - region methods. Wasserstein policy optimization ( WPO ) CONJUNCTION Sinkhorn policy optimization ( SPO ). Sinkhorn policy optimization ( SPO ) CONJUNCTION Wasserstein policy optimization ( WPO ). Wasserstein and Sinkhorn trust regions FEATURE-OF policy optimization. parametric distribution class FEATURE-OF policy. Lagrangian duality USED-FOR close - form policy updates. SPO COMPARE WPO. WPO COMPARE SPO. monotonic performance improvement FEATURE-OF WPO. robotic locomotion tasks EVALUATE-FOR approaches. tabular domains CONJUNCTION robotic locomotion tasks. robotic locomotion tasks CONJUNCTION tabular domains. SPO COMPARE policy gradient methods. policy gradient methods COMPARE SPO. tabular domains EVALUATE-FOR approaches. approaches COMPARE policy gradient methods. policy gradient methods COMPARE approaches. sample insufficiency FEATURE-OF WPO. OtherScientificTerm is policy distribution. Method is entropic regularizer. ,"This paper studies the problem of policy optimization in reinforcement learning. The authors propose a new trust region based on Wasserstein and Sinkhorn trust regions. The key idea is to use the Kullback-Leibler divergence between the policy and the trust region as a regularizer to improve the performance of WPO and SPO. The proposed method is evaluated on a variety of tasks, including tabular and tabular domains. ","This paper studies the problem of policy optimization in reinforcement learning. The authors propose a new trust region based on Wasserstein and Sinkhorn trust regions. The key idea is to use the Kullback-Leibler divergence between the policy and the trust region as a regularizer to improve the performance of WPO and SPO. The proposed method is evaluated on a variety of tasks, including tabular and tabular domains. "
4239,SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"forgetting USED-FOR learning. learning trajectories FEATURE-OF artificial neural networks. forget - and - relearn USED-FOR learning trajectories. relearning step USED-FOR features. forgetting step USED-FOR undesirable information. forget - and - relearn framework USED-FOR iterative training algorithms. image classification FEATURE-OF iterative training algorithms. forgetting operations USED-FOR algorithms. iterative training PART-OF neural networks. OtherScientificTerm are Forgetting, and disproportionate forgetting of undesirable information. Task is human and machine learning. Generic is model. ",This paper proposes a new forgetting-and-reluarn framework for iterative training of neural networks. The authors show that the forgetting step can be used to reduce the forgetting of undesirable information in the learning trajectories. They also show that forgetting operations can be combined with the relearning step to improve the performance of the model.,This paper proposes a new forgetting-and-reluarn framework for iterative training of neural networks. The authors show that the forgetting step can be used to reduce the forgetting of undesirable information in the learning trajectories. They also show that forgetting operations can be combined with the relearning step to improve the performance of the model.
4255,SP:2789859517b6624730b14a7e010444a72d3dd3ed,"sufficient coverage CONJUNCTION policy. policy CONJUNCTION sufficient coverage. RL agents COMPARE agents. agents COMPARE RL agents. offline - online manner USED-FOR RL agents. Method are Batch RL, batch RL agents, and batch RL. OtherScientificTerm are data - collection process, and agent. Task is offline - online setting. Generic is setting. ","This paper studies the offline-online setting of batch RL, where the data is collected from multiple agents and the goal is to maximize the coverage of all agents. The authors propose a new method to improve the performance of the agent in this setting. The method is based on the notion of ""sufficient coverage"", which is defined as the number of agents that are able to cover the entire dataset in a given time step. They show that the proposed method outperforms the baselines in terms of coverage and policy performance. They also show that their method is able to achieve better performance than existing methods. ","This paper studies the offline-online setting of batch RL, where the data is collected from multiple agents and the goal is to maximize the coverage of all agents. The authors propose a new method to improve the performance of the agent in this setting. The method is based on the notion of ""sufficient coverage"", which is defined as the number of agents that are able to cover the entire dataset in a given time step. They show that the proposed method outperforms the baselines in terms of coverage and policy performance. They also show that their method is able to achieve better performance than existing methods. "
4271,SP:76625a25e770415599a34122110d61cb3b7e614c,learning USED-FOR domain shift. episodic training procedure USED-FOR learning. episodic training procedure USED-FOR domain generalization ( DG ). learning USED-FOR domain generalization ( DG ). episodic training procedure USED-FOR domain shift. Y - discrepancy USED-FOR domain shift. source - domain samples USED-FOR Y - discrepancy. ERM CONJUNCTION domain - invariant learning. domain - invariant learning CONJUNCTION ERM. PAC - style generalization bound USED-FOR discrepancyoptimal meta - learning. PAC - style generalization bound COMPARE DG bounds. DG bounds COMPARE PAC - style generalization bound. domain - invariant learning HYPONYM-OF DG bounds. ERM HYPONYM-OF DG bounds. computational complexity EVALUATE-FOR discrepancy - optimal meta - learning. classification EVALUATE-FOR discrepancy - optimal meta - learning. classification CONJUNCTION computational complexity. computational complexity CONJUNCTION classification. bilevel optimization algorithm USED-FOR DG. DomainBed EVALUATE-FOR algorithm. DG benchmarks EVALUATE-FOR algorithm. ,"This paper studies the problem of domain generalization (DGD) in the context of episodic learning. The authors propose a new generalization bound for the problem, which is based on the notion of discrepancy-optimal meta-learning. The main contribution of the paper is to prove that the generalization of the proposed bound is a function of the number of source-domain samples and the discrepancy between the source and target domains. The paper also provides a bilevel optimization algorithm to solve the problem. ","This paper studies the problem of domain generalization (DGD) in the context of episodic learning. The authors propose a new generalization bound for the problem, which is based on the notion of discrepancy-optimal meta-learning. The main contribution of the paper is to prove that the generalization of the proposed bound is a function of the number of source-domain samples and the discrepancy between the source and target domains. The paper also provides a bilevel optimization algorithm to solve the problem. "
4287,SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"deep reinforcement learning USED-FOR two - player games. combinatorial search methods USED-FOR NP - complete domains. SAT CONJUNCTION CSP. CSP CONJUNCTION SAT. deep reinforcement learning USED-FOR combinatorial search methods. Go HYPONYM-OF two - player games. CSP HYPONYM-OF NP - complete domains. SAT HYPONYM-OF NP - complete domains. exponential combinatorial search space FEATURE-OF hard instances. best - first search CONJUNCTION Monte Carlo tree search. Monte Carlo tree search CONJUNCTION best - first search. Monte Carlo tree search HYPONYM-OF search methods. best - first search HYPONYM-OF search methods. methods USED-FOR hard planning instances. policy and value networks USED-FOR DNN - based best - first search. Sokoban domain EVALUATE-FOR DNN - based best - first search. value network USED-FOR policy network. cost distribution FEATURE-OF search algorithms. heavy - tailed runtime distributions FEATURE-OF Sokoban planning instances. abstract tree model USED-FOR tails. policy network USED-FOR search. polynomial scaling FEATURE-OF left heavy tails. random restart strategies USED-FOR DNN - based search. random restart strategies USED-FOR combinatorial solvers. DNN - based search USED-FOR left and right heavy tails. Task is PSPACE - hard planning problems. Method is domain - specific solvers. Generic are specialized solvers, approaches, and model. OtherScientificTerm is exponentially sized sub - trees. ",This paper studies the problem of combinatorial search for PSPACE-hard planning in two-player games. The authors propose a DNN-based best-first search method for solving PSPACE problems. The main idea is to use an abstract tree model to learn a policy and a value network to solve the problem. The proposed method is evaluated on the Sokoban and CSP domains. The paper shows that the proposed method outperforms the state-of-the-art algorithms in terms of runtime.,This paper studies the problem of combinatorial search for PSPACE-hard planning in two-player games. The authors propose a DNN-based best-first search method for solving PSPACE problems. The main idea is to use an abstract tree model to learn a policy and a value network to solve the problem. The proposed method is evaluated on the Sokoban and CSP domains. The paper shows that the proposed method outperforms the state-of-the-art algorithms in terms of runtime.
4303,SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"approach USED-FOR meta - policy. adaptive loss USED-FOR meta - policy. human videos USED-FOR approach. method COMPARE baseline. baseline COMPARE method. vision - based tasks EVALUATE-FOR method. Method are Meta - Imitation Learning, and meta - imitation learning. OtherScientificTerm are human demonstrations, robot demonstrations, robot demonstration, and human imitation behavior. Generic is it. Task are meta - training phase, and data collection. ","This paper proposes a method for meta-imitation learning based on human videos. The method is based on the idea of meta-learning, where the goal is to learn a meta-policy that can be used to imitate the behavior of a robot. The authors propose a method that learns a policy that can learn to imitate a robot from a video. The proposed method is evaluated on a number of vision-based tasks and shows that the proposed method outperforms baselines.","This paper proposes a method for meta-imitation learning based on human videos. The method is based on the idea of meta-learning, where the goal is to learn a meta-policy that can be used to imitate the behavior of a robot. The authors propose a method that learns a policy that can learn to imitate a robot from a video. The proposed method is evaluated on a number of vision-based tasks and shows that the proposed method outperforms baselines."
4319,SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"Over - parameterized deep networks USED-FOR classification and ranking problems. gradient - based optimizers USED-FOR Over - parameterized deep networks. weight space FEATURE-OF adaptivity. Adam HYPONYM-OF Adaptive optimizers. weight decay ( WD ) CONJUNCTION normal hyper - parameter tuning. normal hyper - parameter tuning CONJUNCTION weight decay ( WD ). adaptive optimizers COMPARE SGD. SGD COMPARE adaptive optimizers. normal hyper - parameter tuning USED-FOR adaptive optimizers. weight decay ( WD ) USED-FOR adaptive optimizers. image classification domain EVALUATE-FOR SGD. image classification domain EVALUATE-FOR adaptive optimizers. generalization performance EVALUATE-FOR SGD. generalization performance EVALUATE-FOR adaptive optimizers. OtherScientificTerm are tuned regularization, network weights, training loss, and train loss. Generic are networks, and network. ","This paper studies the adaptive optimizers for over-parameterized deep neural networks. The authors propose Adam, a new adaptive optimizer that combines weight decay (WD) and normal hyperparameter tuning (NMT) to improve the generalization performance of SGD. They show that Adam outperforms SGD in the classification and ranking tasks. They also provide theoretical analysis on the convergence of Adam. ","This paper studies the adaptive optimizers for over-parameterized deep neural networks. The authors propose Adam, a new adaptive optimizer that combines weight decay (WD) and normal hyperparameter tuning (NMT) to improve the generalization performance of SGD. They show that Adam outperforms SGD in the classification and ranking tasks. They also provide theoretical analysis on the convergence of Adam. "
4335,SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"model USED-FOR equivariance. edge orientations FEATURE-OF face. edge orientations CONJUNCTION face poses. face poses CONJUNCTION edge orientations. symmetries USED-FOR low and high - level features. face poses FEATURE-OF camera. edge orientations HYPONYM-OF low and high - level features. edge orientations HYPONYM-OF symmetries. equivariant networks USED-FOR partial and full equivariances. Partial G - CNNs HYPONYM-OF equivariant networks. full equivariance FEATURE-OF Partial G - CNNs. Partial G - CNNs COMPARE G - CNNs. G - CNNs COMPARE Partial G - CNNs. discrete groups CONJUNCTION continuous groups. continuous groups CONJUNCTION discrete groups. method USED-FOR discrete groups. method USED-FOR continuous groups. Task are generalization, and natural image classification. Method is group equivariant architectures. OtherScientificTerm are distribution, and rotations. Material is rotated MNIST. Generic is them. ","This paper proposes a new model for group equivariance. The model is based on partial G-CNNs, which are equivariant to group rotations. The authors show that the model is able to generalize well to discrete groups and continuous groups. They also show that their model can generalize to full equivariances. ","This paper proposes a new model for group equivariance. The model is based on partial G-CNNs, which are equivariant to group rotations. The authors show that the model is able to generalize well to discrete groups and continuous groups. They also show that their model can generalize to full equivariances. "
4351,SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,Markov chain Monte Carlo ( MCMC ) USED-FOR approximating intractable distributions. Langevin dynamics HYPONYM-OF Markov chain Monte Carlo ( MCMC ). datapoint - wise iterations CONJUNCTION slow convergence. slow convergence CONJUNCTION datapoint - wise iterations. its USED-FOR deep latent variable models. inference model USED-FOR latent variables. ALD USED-FOR scalable inference. large - scale datasets USED-FOR ALD. datapoint - wise iterations USED-FOR it. large - scale datasets USED-FOR scalable inference. MCMC USED-FOR it. stationary distribution USED-FOR ALD. ALD USED-FOR generative modeling. it USED-FOR prior distribution. it USED-FOR latent variable. prior distribution FEATURE-OF latent variable. ALD USED-FOR unconditional distribution. it USED-FOR generative modeling. energy - based model HYPONYM-OF unconditional distribution. ALD USED-FOR deep latent variable model. Langevin autoencoder ( LAE ) HYPONYM-OF deep latent variable model. ALD USED-FOR autoencoder - like posterior inference. LAE USED-FOR autoencoder - like posterior inference. latent space EBM USED-FOR LAE. ALD USED-FOR LAE. ALD COMPARE LD. LD COMPARE ALD. ALD USED-FOR target distributions. toy datasets EVALUATE-FOR ALD. conditional and unconditional cases FEATURE-OF target distributions. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. CIFAR-10 CONJUNCTION CelebA - HQ. CelebA - HQ CONJUNCTION CIFAR-10. datasets USED-FOR image generation task. image generation task EVALUATE-FOR LAE. SVHN HYPONYM-OF datasets. datasets EVALUATE-FOR LAE. CelebA - HQ HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. LAE COMPARE non - amortized MCMC methods. non - amortized MCMC methods COMPARE LAE. LAE COMPARE AVI - based methods. AVI - based methods COMPARE LAE. Fréchet,This paper proposes a Langevin autoencoder (LAE) model for deep latent variable models. The proposed model is based on the Langevin dynamics (LD) framework. The authors show that the proposed model can be used for both conditional and unconditional cases. They also show that it can be applied to large-scale datasets.,This paper proposes a Langevin autoencoder (LAE) model for deep latent variable models. The proposed model is based on the Langevin dynamics (LD) framework. The authors show that the proposed model can be used for both conditional and unconditional cases. They also show that it can be applied to large-scale datasets.
4367,SP:5631097031c7e599bdeae64366ffa6e4558837c6,hypergraph reasoning USED-FOR large domains. logical rules PART-OF logical reasoning. structured neural network USED-FOR hypergraph reasoning. neural networks CONJUNCTION finite - domain quantification operations. finite - domain quantification operations CONJUNCTION neural networks. SpaLoc USED-FOR grounding of relationships. sparse tensors USED-FOR SpaLoc. sparse tensors USED-FOR grounding of relationships. finite - domain quantification operations USED-FOR SpaLoc. neural networks USED-FOR SpaLoc. sparsification loss USED-FOR SpaLoc model. intermediate layers PART-OF SpaLoc model. sparsification loss USED-FOR intermediate layers. training and inference - time sub - sampling USED-FOR SpaLoc. real - world knowledge graphs HYPONYM-OF large - scale graphs. information loss FEATURE-OF sampled sub - graphs. information - theoretic measure information sufficiency USED-FOR sampling and label calibration paradigm. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. accuracy EVALUATE-FOR SpaLoc. efficiency EVALUATE-FOR SpaLoc. synthetic datasets EVALUATE-FOR SpaLoc. real - world knowledge graph reasoning benchmarks EVALUATE-FOR SpaLoc. OtherScientificTerm is grandparent relationship. Method is hypergraph neural networks. ,"This paper proposes a new method for hypergraph reasoning based on sparse tensors. The proposed method, called SpaLoc, is based on the idea of sparsification loss, which is an information-theoretic measure of the information sufficiency of sampled sub-graphs. The authors show that the proposed method is able to achieve state-of-the-art performance on synthetic and real-world knowledge graph tasks. ","This paper proposes a new method for hypergraph reasoning based on sparse tensors. The proposed method, called SpaLoc, is based on the idea of sparsification loss, which is an information-theoretic measure of the information sufficiency of sampled sub-graphs. The authors show that the proposed method is able to achieve state-of-the-art performance on synthetic and real-world knowledge graph tasks. "
4383,SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"top - k classification accuracy EVALUATE-FOR machine learning. probability distribution USED-FOR k. top-1 CONJUNCTION top-5 accuracy. top-5 accuracy CONJUNCTION top-1. ImageNet EVALUATE-FOR models. top-5 accuracy EVALUATE-FOR models. top-1 EVALUATE-FOR models. Task is differentiable sorting and ranking. Metric are top-5 accuracies, and top-1 accuracy. Method is ImageNet models. ","This paper studies the problem of differentiable sorting and ranking of top-k classification accuracy. The authors propose a new metric, top-1 accuracy, which measures the difference between the top-5 accuracy and top-2 accuracy of a classifier trained on the same dataset. The paper shows that the difference in the two measures is a function of the number of classes, and the probability distribution of the classifier. They also show that for a given classifier, the difference of the two sets of classes is the same for all classes in the dataset. They show that this is the case for all classifiers trained on ImageNet. ","This paper studies the problem of differentiable sorting and ranking of top-k classification accuracy. The authors propose a new metric, top-1 accuracy, which measures the difference between the top-5 accuracy and top-2 accuracy of a classifier trained on the same dataset. The paper shows that the difference in the two measures is a function of the number of classes, and the probability distribution of the classifier. They also show that for a given classifier, the difference of the two sets of classes is the same for all classes in the dataset. They show that this is the case for all classifiers trained on ImageNet. "
4399,SP:cb3188f435c54a365890e20e4d582c250d919833,"speed CONJUNCTION accuracy. accuracy CONJUNCTION speed. accuracy EVALUATE-FOR method. speed EVALUATE-FOR method. method USED-FOR OT problem. Douglas - Rachford splitting technique USED-FOR method. method USED-FOR approximate regularized problem. entropic regularization USED-FOR methods. algorithm COMPARE Sinkhorn method. Sinkhorn method COMPARE algorithm. method COMPARE Sinkhorn method. Sinkhorn method COMPARE method. iteration complexity EVALUATE-FOR method. linear convergence rate USED-FOR OT problem. primal - dual stopping criterion FEATURE-OF method. computation times CONJUNCTION robustness. robustness CONJUNCTION computation times. robustness EVALUATE-FOR method. computation times EVALUATE-FOR method. OtherScientificTerm are sparse transport plans, and numerical issues. ","This paper studies the problem of sparse transport planning (OT) with sparse transport plans. The authors propose a method to solve the problem by splitting the problem into two parts. The first part is a regularized version of the OT problem, and the second part is the primal-dual stopping problem. The proposed method is based on the Douglas-Rachford splitting technique, and is shown to converge to a linear convergence rate. The method is evaluated on a variety of datasets, and shows better performance than the Sinkhorn method.","This paper studies the problem of sparse transport planning (OT) with sparse transport plans. The authors propose a method to solve the problem by splitting the problem into two parts. The first part is a regularized version of the OT problem, and the second part is the primal-dual stopping problem. The proposed method is based on the Douglas-Rachford splitting technique, and is shown to converge to a linear convergence rate. The method is evaluated on a variety of datasets, and shows better performance than the Sinkhorn method."
4415,SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"distribution of distributions USED-FOR Federated learning data. generalization studies USED-FOR federated learning. framework USED-FOR performance gaps. dataset synthesis strategy USED-FOR realistic simulations of generalization. OtherScientificTerm are meta - distribution, local data distributions, out - of - sample gap, and participation gap. Material are natural and synthetic federated datasets, and naturally - partitioned data. Method is semantic synthesis strategy. ","This paper studies the problem of generalization in federated learning. The authors propose a new dataset synthesis strategy to address the out-of-sample gap between the meta-distribution and local data distributions, and the participation gap between natural and synthetic federated datasets. The proposed method is based on the semantic synthesis strategy, and is evaluated on synthetic and natural-partitioned data. ","This paper studies the problem of generalization in federated learning. The authors propose a new dataset synthesis strategy to address the out-of-sample gap between the meta-distribution and local data distributions, and the participation gap between natural and synthetic federated datasets. The proposed method is based on the semantic synthesis strategy, and is evaluated on synthetic and natural-partitioned data. "
4431,SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"generalization EVALUATE-FOR LMs. self - supervision objectives USED-FOR LMs. supervised finetuning USED-FOR pre - trained language models ( PLMs ). model capacity CONJUNCTION data size. data size CONJUNCTION model capacity. pre - training techniques USED-FOR PLMs. PLMs USED-FOR few - shot setting. PLMs USED-FOR few - shot tasks. PLMs USED-FOR zero - shot setting. BERT family USED-FOR models. IMDB dataset CONJUNCTION Amazon dataset. Amazon dataset CONJUNCTION IMDB dataset. IMDB dataset HYPONYM-OF datasets. Amazon dataset HYPONYM-OF datasets. PLMs USED-FOR language understanding tasks. zero - shot setting FEATURE-OF PLMs. GLUE HYPONYM-OF language understanding tasks. Method are deep learning, language models ( LMs ), and prompt - based learning. OtherScientificTerm are manually / automatically created prompts, and manually created prompts. Metric is accuracy. ","This paper studies the problem of pre-training language models (PLMs) for few-shot tasks. The authors propose a new self-supervision objective for pre-trained language models, which is based on self-training and self-finetuning. The paper also proposes a new method to improve the performance of PLMs in the zero-shot setting. Experiments are conducted on GLUE, IMDB, and Amazon.","This paper studies the problem of pre-training language models (PLMs) for few-shot tasks. The authors propose a new self-supervision objective for pre-trained language models, which is based on self-training and self-finetuning. The paper also proposes a new method to improve the performance of PLMs in the zero-shot setting. Experiments are conducted on GLUE, IMDB, and Amazon."
4447,SP:9817dccb1a121058b23a2ef825ed339cf8b53674,Attention mechanism USED-FOR tasks. sharpener module USED-FOR attention mechanism. it USED-FOR representation. alignment FEATURE-OF attention. real - world scene text recognition datasets EVALUATE-FOR approach. approach COMPARE ones. ones COMPARE approach. approach COMPARE soft and hard attention. soft and hard attention COMPARE approach. soft and hard attention HYPONYM-OF ones. ,"This paper proposes a novel attention mechanism for text recognition tasks. The proposed method is based on the sharpener module, which is an extension of the attention mechanism. The authors show that the proposed method outperforms soft and hard attention in terms of accuracy and alignment. They also show that their method can be applied to real-world scene text recognition datasets.","This paper proposes a novel attention mechanism for text recognition tasks. The proposed method is based on the sharpener module, which is an extension of the attention mechanism. The authors show that the proposed method outperforms soft and hard attention in terms of accuracy and alignment. They also show that their method can be applied to real-world scene text recognition datasets."
4463,SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"Learning USED-FOR combinatorial optimization problems. operations research solvers CONJUNCTION heuristics. heuristics CONJUNCTION operations research solvers. vehicle routing problem HYPONYM-OF combinatorial optimization problems. apriori given number of available vehicles USED-FOR complex assignment problem. bounded fleet size FEATURE-OF logistic service providers. post - processing scheme USED-FOR supervised approach. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. Method are deep reinforcement learning approaches, learning - based approaches, and supervised deep learning framework. Generic is them. OtherScientificTerm is apriori fixed number of available vehicles. ",This paper proposes a reinforcement learning framework for the vehicle routing problem. The proposed framework is based on the idea of learning a set of agents that can be assigned to a given vehicle. The agents are trained using a post-processing scheme. The authors show that the proposed method outperforms the state-of-the-art baselines.,This paper proposes a reinforcement learning framework for the vehicle routing problem. The proposed framework is based on the idea of learning a set of agents that can be assigned to a given vehicle. The agents are trained using a post-processing scheme. The authors show that the proposed method outperforms the state-of-the-art baselines.
4479,SP:594a813c0d0baa66738b9c8331370f861ad3c416,"Existing methods USED-FOR variables. clustering effect HYPONYM-OF observed graph structure. observed graph structure HYPONYM-OF variables. clustering effect HYPONYM-OF variables. causal relationship FEATURE-OF variables. link prediction method USED-FOR graph learning. counterfactual inference USED-FOR link prediction method. counterfactual inference USED-FOR graph learning. It USED-FOR counterfactual links. It USED-FOR representations. observed and counterfactual links USED-FOR representations. benchmark datasets EVALUATE-FOR graph learning method. link prediction EVALUATE-FOR graph learning method. Task is graph - based applications. Generic is it. Method are causal models, and graph representations. OtherScientificTerm is global graph structural properties. ",This paper proposes a link prediction method for graph learning. The proposed method is based on counterfactual inference. The method is evaluated on several benchmark datasets.,This paper proposes a link prediction method for graph learning. The proposed method is based on counterfactual inference. The method is evaluated on several benchmark datasets.
4495,SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"unsupervised feature selection methods USED-FOR ranking features. second - order covariance matrix CONJUNCTION first - order data matrix. first - order data matrix CONJUNCTION second - order covariance matrix. sparse attention matrix USED-FOR second - order relations between features. graph segmentation USED-FOR feature selection. attention matrix USED-FOR relational graph. SOFT COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SOFT. SOFT COMPARE method. method COMPARE SOFT. Task are Unsupervised feature selection, and unsupervised feature selection. OtherScientificTerm is external guidance information. Method is knowledge contrastive disTillation ( SOFT ) model. ",This paper proposes a new unsupervised feature selection method based on knowledge contrastive disTillation (SOFT) model. SOFT uses a sparse attention matrix to learn the second-order relations between features in a relational graph. The authors show that SOFT outperforms state-of-the-art methods on several benchmark datasets.,This paper proposes a new unsupervised feature selection method based on knowledge contrastive disTillation (SOFT) model. SOFT uses a sparse attention matrix to learn the second-order relations between features in a relational graph. The authors show that SOFT outperforms state-of-the-art methods on several benchmark datasets.
4511,SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"Multimodal variational autoencoders ( VAEs ) USED-FOR joint distribution. vision CONJUNCTION language. language CONJUNCTION vision. heterogeneous data USED-FOR joint distribution. language HYPONYM-OF heterogeneous data. vision HYPONYM-OF heterogeneous data. idiosyncratic representations PART-OF recognition model. mixtures CONJUNCTION factorisations. factorisations CONJUNCTION mixtures. mixtures USED-FOR idiosyncratic representations. MEME COMPARE baselines. baselines COMPARE MEME. metrics EVALUATE-FOR partial and complete observation schemes. metrics EVALUATE-FOR MEME. metrics EVALUATE-FOR baselines. representations COMPARE approaches. approaches COMPARE representations. mutual supervision USED-FOR representations. OtherScientificTerm are modalities, and relatedness between data. Generic are alternative, and formulation. Method are Mutually supErvised Multimodal VAE ( MEME ), and semisupervised VAEs. Material is partiallyobserved data. ","This paper proposes a new multi-modal variational autoencoder (VAE) model for learning from partially-observed data. The proposed model is based on the notion of mutual supervision, which is an extension of the mutual supervision (MSE) framework. The authors show that the proposed model outperforms the baselines on a variety of metrics, including partial and complete observation schemes. ","This paper proposes a new multi-modal variational autoencoder (VAE) model for learning from partially-observed data. The proposed model is based on the notion of mutual supervision, which is an extension of the mutual supervision (MSE) framework. The authors show that the proposed model outperforms the baselines on a variety of metrics, including partial and complete observation schemes. "
4527,SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"Reinforcement Learning agent USED-FOR directed behaviors. Intrinsic Motivation USED-FOR Reinforcement Learning agent. options USED-FOR simple tabular cases. Deep Explore Options USED-FOR complex visual problems. Explore Options PART-OF Deep Reinforcement Learning paradigm. unrelated intrinsic rewards USED-FOR Deep Explore Options. J - PER HYPONYM-OF transitionselection algorithm. interest of multiple agents USED-FOR transitionselection algorithm. intrinsic reward learning USED-FOR auxiliary task. architecture USED-FOR shared representation. Atari Suite FEATURE-OF hard and easy exploration games. Atari Suite EVALUATE-FOR Deep Explore Options. hard and easy exploration games EVALUATE-FOR Deep Explore Options. they COMPARE weighted sum of rewards. weighted sum of rewards COMPARE they. weighted sum of rewards COMPARE baselines. baselines COMPARE weighted sum of rewards. they COMPARE baselines. baselines COMPARE they. intrinsic rewards USED-FOR they. OtherScientificTerm are sparse or noisy rewards, intrinsic and extrinsic rewards, interesting behaviors, high dimensional spaces, and exploitative or exploratory behaviors. Method is intrinsically motivated agent. Task is exploration. ",This paper proposes a novel approach to explore options in deep reinforcement learning. The authors propose to use intrinsic reward learning to learn a shared representation of the intrinsic and extrinsic rewards. They also propose a transition selection algorithm to select the best option for each agent to explore. They show that the proposed method outperforms the baselines on Atari games. ,This paper proposes a novel approach to explore options in deep reinforcement learning. The authors propose to use intrinsic reward learning to learn a shared representation of the intrinsic and extrinsic rewards. They also propose a transition selection algorithm to select the best option for each agent to explore. They show that the proposed method outperforms the baselines on Atari games. 
4543,SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,method USED-FOR Hamiltonian dynamical systems. stiffness FEATURE-OF dynamical system. SANN USED-FOR stiff and nonstiff portions. stiffness - aware index USED-FOR SANN. classification CONJUNCTION resampling technique. resampling technique CONJUNCTION classification. time integration strategies USED-FOR dynamical characteristics. dynamical characteristics FEATURE-OF Hamiltonian vector fields. step size adaptation USED-FOR dynamical characteristics. resampling technique USED-FOR time integration strategies. classification USED-FOR time integration strategies. step size adaptation HYPONYM-OF time integration strategies. three - body problem CONJUNCTION billiard model. billiard model CONJUNCTION three - body problem. billiard model EVALUATE-FOR SANN. complex physical systems EVALUATE-FOR SANN. three - body problem HYPONYM-OF complex physical systems. billiard model HYPONYM-OF complex physical systems. SANN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SANN. energy EVALUATE-FOR SANN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR SANN. Method is stiffness - aware neural network ( SANN ). ,This paper proposes a new method for learning the stiffness-aware neural network (SANN) for Hamiltonian dynamical systems. The proposed method is based on the idea that the stiffness of a dynamical system can be used as a measure of the energy of the system. The authors show that the proposed method outperforms state-of-the-art methods on the three-body problem and the billiard model. The paper also shows that SANN is able to improve the performance of existing methods.,This paper proposes a new method for learning the stiffness-aware neural network (SANN) for Hamiltonian dynamical systems. The proposed method is based on the idea that the stiffness of a dynamical system can be used as a measure of the energy of the system. The authors show that the proposed method outperforms state-of-the-art methods on the three-body problem and the billiard model. The paper also shows that SANN is able to improve the performance of existing methods.
4559,SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"language models USED-FOR tasks. generating realistic text CONJUNCTION synthesizing computer programs. synthesizing computer programs CONJUNCTION generating realistic text. generating realistic text HYPONYM-OF tasks. synthesizing computer programs HYPONYM-OF tasks. they USED-FOR tasks. unbounded multi - step computation USED-FOR tasks. models USED-FOR multistep computations. Transformers USED-FOR multi - step computations. intermediate computation steps PART-OF scratchpad ”. language models USED-FOR multi - step computations. scratchpads USED-FOR language models. OtherScientificTerm are computer programs, integers, and programs. Method is intermediate computations. ","This paper proposes a new method for learning multi-step computations from scratchpads (i.e. intermediate computations) that can be used for multi-task computations. The method is based on the idea of scratchpad, which is an extension of the scratchpad framework. The authors propose to use scratchpad as a way to learn intermediate computation steps in a language model. The paper shows that the proposed method is able to achieve better performance than existing methods on a variety of tasks. ","This paper proposes a new method for learning multi-step computations from scratchpads (i.e. intermediate computations) that can be used for multi-task computations. The method is based on the idea of scratchpad, which is an extension of the scratchpad framework. The authors propose to use scratchpad as a way to learn intermediate computation steps in a language model. The paper shows that the proposed method is able to achieve better performance than existing methods on a variety of tasks. "
4575,SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"deep networks USED-FOR adversarial attacks. methods USED-FOR adversarial perturbations. deep image generators CONJUNCTION optimization objective. optimization objective CONJUNCTION deep image generators. deep image generators USED-FOR feature - level adversarial perturbations. optimization objective USED-FOR feature - level adversarial perturbations. them USED-FOR targeted feature - level attacks. they USED-FOR targeted feature - level attacks. ImageNet scale FEATURE-OF targeted feature - level attacks. them USED-FOR copy / paste ” adversaries. OtherScientificTerm are feature - class associations, natural objects, and targeted misclassification. Method is featurefool attacks. Generic is attacks. Material is natural image. ",This paper studies the problem of feature-level adversarial attacks on deep neural networks. The authors propose a novel optimization objective for adversarial perturbations on the ImageNet scale. They show that the proposed objective can be used to improve the robustness of deep networks against adversarial examples. They also show that their method can be applied to the adversarial training of deep image generators. ,This paper studies the problem of feature-level adversarial attacks on deep neural networks. The authors propose a novel optimization objective for adversarial perturbations on the ImageNet scale. They show that the proposed objective can be used to improve the robustness of deep networks against adversarial examples. They also show that their method can be applied to the adversarial training of deep image generators. 
4591,SP:873618263dc4246a39c44d0abfecfb5f688817e3,Simulated annealing ( SA ) HYPONYM-OF stochastic global optimisation technique. stochastic global optimisation technique USED-FOR discrete and continuous variable problems. neighbour proposal distribution CONJUNCTION temperature annealing schedule. temperature annealing schedule CONJUNCTION neighbour proposal distribution. SA optimiser USED-FOR problem. temperature annealing schedule HYPONYM-OF handpicked components. handpicked components USED-FOR SA optimiser. neighbour proposal distribution HYPONYM-OF handpicked components. policy USED-FOR solution quality. policy USED-FOR proposal distribution. Knapsack problem CONJUNCTION Bin Packing problem. Bin Packing problem CONJUNCTION Knapsack problem. Bin Packing problem CONJUNCTION Travelling Salesperson problem. Travelling Salesperson problem CONJUNCTION Bin Packing problem. Rosenbrock ’s function CONJUNCTION Knapsack problem. Knapsack problem CONJUNCTION Rosenbrock ’s function. Neural SA COMPARE SA baselines. SA baselines COMPARE Neural SA. hand - selected parameters USED-FOR SA baselines. hand - selected parameters USED-FOR Neural SA. problems EVALUATE-FOR Neural SA. problems EVALUATE-FOR SA baselines. proposal distribution USED-FOR Neural SA. Travelling Salesperson problem HYPONYM-OF problems. Rosenbrock ’s function HYPONYM-OF problems. Bin Packing problem HYPONYM-OF problems. Knapsack problem HYPONYM-OF problems. solution quality CONJUNCTION wall clock time. wall clock time CONJUNCTION solution quality. Neural SA COMPARE solvers. solvers COMPARE Neural SA. wall clock time EVALUATE-FOR Neural SA. solution quality EVALUATE-FOR Neural SA. wall clock time EVALUATE-FOR solvers. solution quality EVALUATE-FOR solvers. Method is SA. OtherScientificTerm is computational budget. ,"This paper proposes Neural SA, a method for simulated annealing (SA) for discrete and continuous variable problems. The proposed method is based on the idea of hand-selected components of the proposed method. The authors show that Neural SA outperforms existing SA baselines on a number of problems. They also show that their method is able to achieve state-of-the-art performance on a variety of tasks. ","This paper proposes Neural SA, a method for simulated annealing (SA) for discrete and continuous variable problems. The proposed method is based on the idea of hand-selected components of the proposed method. The authors show that Neural SA outperforms existing SA baselines on a number of problems. They also show that their method is able to achieve state-of-the-art performance on a variety of tasks. "
4607,SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"policy changes of agents USED-FOR learning process. measurement indicators USED-FOR non - stationarity. notion USED-FOR non - stationarity. non - stationarity FEATURE-OF policy sequence. δ - stationarity measurement HYPONYM-OF notion. trust - region constraint FEATURE-OF joint policy. policy factorization USED-FOR policy divergence. trust - region constraints FEATURE-OF factorized policies. mean - field approximation HYPONYM-OF policy factorization. computational complexity EVALUATE-FOR it. pairwise Markov random field USED-FOR joint policy. MAMT HYPONYM-OF Multi - Agent Mirror descent policy algorithm. end - to - end manner USED-FOR trust - region of the local policies. Trust region decomposition USED-FOR Multi - Agent Mirror descent policy algorithm. MAMT USED-FOR non - stationarity problem. MAMT USED-FOR joint policies ’ divergence. method COMPARE baselines. baselines COMPARE method. complexity FEATURE-OF cooperative tasks. cooperative tasks EVALUATE-FOR baselines. cooperative tasks EVALUATE-FOR method. complexity EVALUATE-FOR baselines. complexity EVALUATE-FOR method. Task is Non - stationarity. Generic is algorithms. Metric is KL - divergence. OtherScientificTerm are trust - region decomposition dilemma, joint policy divergence, and δ - stationarity. Method is message passing. ","This paper studies the problem of non-stationarity in the multi-agent mirror descent setting. The authors propose a new algorithm called Multi-Agent Mirror Descent Policy Algorithm (MAMT) to solve this problem. MAMT is based on the trust-region decomposition of the local policies of the agents, which is an end-to-end method for solving the joint policy divergence problem. The main contribution of the paper is the use of the mean-field approximation of the policy factorization of the factorized policies to compute the KL-divergence of the joint policies. The proposed algorithm is shown to be computationally efficient compared to the baselines.","This paper studies the problem of non-stationarity in the multi-agent mirror descent setting. The authors propose a new algorithm called Multi-Agent Mirror Descent Policy Algorithm (MAMT) to solve this problem. MAMT is based on the trust-region decomposition of the local policies of the agents, which is an end-to-end method for solving the joint policy divergence problem. The main contribution of the paper is the use of the mean-field approximation of the policy factorization of the factorized policies to compute the KL-divergence of the joint policies. The proposed algorithm is shown to be computationally efficient compared to the baselines."
4623,SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"correlated audio and visual information PART-OF Video recordings of speech. self - supervised representation learning framework USED-FOR audio - visual speech. AV - HuBERT USED-FOR audio - visual speech representation. audio - visual speech representation USED-FOR lip - reading and automatic speech recognition. AV - HuBERT COMPARE state - of - the - art approach. state - of - the - art approach COMPARE AV - HuBERT. labeled data USED-FOR AV - HuBERT. transcribed video data USED-FOR state - of - the - art approach. WER EVALUATE-FOR AV - HuBERT. labeled data USED-FOR LRS3. audio - visual representation USED-FOR audio - only speech recognition. benchmark USED-FOR audio - only speech recognition. benchmark EVALUATE-FOR audio - visual representation. relative WER reduction EVALUATE-FOR audio - visual representation. Method are speech representation, and self - training. Material is multi - stream video input. OtherScientificTerm is multimodal hidden units. Metric is lip - reading WER. ","This paper proposes a self-supervised representation learning framework for audio-visual speech recognition. The key idea is to learn a multi-modal hidden unit for multi-stream video input. The proposed method is based on HuBERT, which learns a representation of audio and visual information from video recordings of speech. The authors show that the proposed method outperforms state-of-the-art methods on lip reading and automatic speech recognition tasks. ","This paper proposes a self-supervised representation learning framework for audio-visual speech recognition. The key idea is to learn a multi-modal hidden unit for multi-stream video input. The proposed method is based on HuBERT, which learns a representation of audio and visual information from video recordings of speech. The authors show that the proposed method outperforms state-of-the-art methods on lip reading and automatic speech recognition tasks. "
4639,SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,combinatorial optimisation USED-FOR real - world applications. logistics CONJUNCTION natural sciences. natural sciences CONJUNCTION logistics. natural sciences FEATURE-OF combinatorial optimisation. graphs USED-FOR combinatorial optimisation. pre - solved instances USED-FOR problems. graph neural networks ( GNNs ) USED-FOR decision step. ECORD HYPONYM-OF RL algorithm. pre - processing step USED-FOR GNN. recurrent unit USED-FOR fast - acting exploratory phase. SOTA USED-FOR RL algorithms. ECORD USED-FOR RL algorithms. RL algorithms USED-FOR Maximum Cut problem. ECORD USED-FOR SOTA. SOTA USED-FOR Maximum Cut problem. Maximum Cut problem EVALUATE-FOR ECORD. scalability EVALUATE-FOR ECORD. speed EVALUATE-FOR ECORD. nearest competitor COMPARE ECORD. ECORD COMPARE nearest competitor. optimality gap EVALUATE-FOR ECORD. Method is Reinforcement learning ( RL ). Generic is it. OtherScientificTerm is wall - clock time. ,"This paper proposes a new algorithm for combinatorial optimisation, called ECORD, which is based on graph neural networks (GNNs). ECORD is an extension of SOTA (SOTA) that uses a recurrent unit for the exploration phase and a pre-processing step for the decision step. ECORD achieves state-of-the-art performance on the Maximum Cut problem. The authors show that ECORD outperforms SOTA in terms of speed and scalability.","This paper proposes a new algorithm for combinatorial optimisation, called ECORD, which is based on graph neural networks (GNNs). ECORD is an extension of SOTA (SOTA) that uses a recurrent unit for the exploration phase and a pre-processing step for the decision step. ECORD achieves state-of-the-art performance on the Maximum Cut problem. The authors show that ECORD outperforms SOTA in terms of speed and scalability."
4655,SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"Discrete latent variables USED-FOR generation process of real world data. discrete latents USED-FOR Variational Autoencoders ( VAEs ). discrete VAEs COMPARE ones. ones COMPARE discrete VAEs. direct discrete optimization USED-FOR encoding model. direct discrete optimization USED-FOR discrete nature of the latents. reparameterization trick CONJUNCTION amortization. amortization CONJUNCTION reparameterization trick. sampling approximation CONJUNCTION reparameterization trick. reparameterization trick CONJUNCTION sampling approximation. amortization HYPONYM-OF VAE mechanisms. reparameterization trick HYPONYM-OF VAE mechanisms. sampling approximation HYPONYM-OF VAE mechanisms. truncated posteriors CONJUNCTION evolutionary algorithms. evolutionary algorithms CONJUNCTION truncated posteriors. variational setting USED-FOR Discrete optimization. approach USED-FOR evolutionary algorithms. evolutionary algorithms USED-FOR Discrete optimization. truncated posteriors USED-FOR variational setting. truncated posteriors USED-FOR Discrete optimization. decoder network USED-FOR latent states. gradient ascent USED-FOR network weights. gradient ascent USED-FOR discrete variational method. binary latents FEATURE-OF VAEs. discrete variational method USED-FOR VAEs. amortized training COMPARE direct discrete optimization. direct discrete optimization COMPARE amortized training. direct discrete optimization USED-FOR large neural networks. amortized training USED-FOR large neural networks. smaller networks USED-FOR direct optimization. direct optimization COMPARE zero - shot ’ learning. zero - shot ’ learning COMPARE direct optimization. large supervised neural networks COMPARE VAEs. VAEs COMPARE large supervised neural networks. sampling - based approximation CONJUNCTION reparameterization. reparameterization CONJUNCTION sampling - based approximation. approach USED-FOR training of VAEs. sampling - based approximation USED-FOR training of VAEs. direct optimization USED-FOR VAEs. direct optimization USED-FOR denoising. VAEs USED-FOR denoising. they COMPARE non - generative approaches. non - generative approaches COMPARE they. VAEs COMPARE non - generative approaches. non - generative approaches COMPARE VAEs. Method are VAE training, and small",This paper proposes a discrete variational method for training VAEs. The authors show that this method can be used to train VAEs with discrete latents. They show that the proposed method is able to achieve better performance than the state-of-the-art in terms of denoising and zero-shot learning. They also show that their method can also be used for training large neural networks.,This paper proposes a discrete variational method for training VAEs. The authors show that this method can be used to train VAEs with discrete latents. They show that the proposed method is able to achieve better performance than the state-of-the-art in terms of denoising and zero-shot learning. They also show that their method can also be used for training large neural networks.
4671,SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,reward - based task EVALUATE-FOR approach. action - prediction USED-FOR methods. Controlled Effect Network ( CEN ) HYPONYM-OF unsupervised method. counterfactual measures of blame USED-FOR unsupervised method. it USED-FOR controlled effects. it PART-OF exploration method. CEN USED-FOR intrinsic motivator. it COMPARE action - prediction models. action - prediction models COMPARE it. CEN COMPARE action - prediction models. action - prediction models COMPARE CEN. Task is Identifying controllable aspects of the environment. Method is reinforcement learning agents. ,This paper proposes an unsupervised reinforcement learning approach to identify controllable aspects of the environment. The proposed method is based on the Controlled Effect Network (CEN) framework. CEN is an extension of the action-prediction framework that uses counterfactual measures of blame to guide exploration. The authors show that the proposed method outperforms state-of-the-art action prediction models on a reward-based task.,This paper proposes an unsupervised reinforcement learning approach to identify controllable aspects of the environment. The proposed method is based on the Controlled Effect Network (CEN) framework. CEN is an extension of the action-prediction framework that uses counterfactual measures of blame to guide exploration. The authors show that the proposed method outperforms state-of-the-art action prediction models on a reward-based task.
4687,SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"they USED-FOR networks. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. neural architecture search HYPONYM-OF model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. regularization FEATURE-OF pruned structure. regularization USED-FOR structure - regularized pruning ( SRP ). filters USED-FOR unimportant filters. residual USED-FOR layers. filters USED-FOR layers. SRP USED-FOR image SR networks. lightweight network SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION lightweight network SRPN - L. SRP USED-FOR lightweight network SRPN - L. SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION SRPN - L. OtherScientificTerm are moderate model size, and pruned filters. Generic is network. Method are L2 regularization, and lightweight and larger image SR networks. ","This paper proposes structure-regularized pruning (SRP), a method for reducing the size of a neural network by pruning the filters. The authors show that SRP can be applied to both lightweight and larger image SR networks. They also show that it can be used to reduce the number of filters in the network. ","This paper proposes structure-regularized pruning (SRP), a method for reducing the size of a neural network by pruning the filters. The authors show that SRP can be applied to both lightweight and larger image SR networks. They also show that it can be used to reduce the number of filters in the network. "
4703,SP:0dee45001ae9600f485614dfe6874a516ac01db5,"model USED-FOR few - shot learning methods. sparsely labeled novel category data USED-FOR model. abundantly labeled base category data USED-FOR model. abundantly labeled base category data USED-FOR few - shot learning methods. framework USED-FOR large domain shift. framework USED-FOR few - shot learning. contrastive loss FEATURE-OF base category data. feature extracting backbone USED-FOR framework. contrastive loss USED-FOR feature extracting backbone. masking module USED-FOR target domain classification. backbone USED-FOR features. backbone USED-FOR classifier. it EVALUATE-FOR framework. cross - domain few - shot learning benchmark EVALUATE-FOR it. cross - domain few - shot learning benchmark EVALUATE-FOR framework. framework COMPARE cross - domain methods. cross - domain methods COMPARE framework. framework COMPARE meta - learning approaches. meta - learning approaches COMPARE framework. meta - learning approaches COMPARE cross - domain methods. cross - domain methods COMPARE meta - learning approaches. Generic is methods. OtherScientificTerm are distant domain categories, and supervision. ","This paper proposes a new framework for few-shot learning based on contrastive loss. The proposed framework is based on the idea of masking, which is an extension of the masking module in meta-learning. The authors show that the proposed framework can be applied to a large domain shift, and that it can be used to improve the performance of cross-domain few shot learning methods. ","This paper proposes a new framework for few-shot learning based on contrastive loss. The proposed framework is based on the idea of masking, which is an extension of the masking module in meta-learning. The authors show that the proposed framework can be applied to a large domain shift, and that it can be used to improve the performance of cross-domain few shot learning methods. "
4719,SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"gradient descent USED-FOR generalisation. networks USED-FOR gradient descent. infinite width networks CONJUNCTION finite width networks. finite width networks CONJUNCTION infinite width networks. Bayesian inference USED-FOR infinite width networks. gradient descent USED-FOR finite width networks. error COMPARE chance. chance COMPARE error. gradient descent USED-FOR functions. implicit biases of architecture CONJUNCTION gradient descent. gradient descent CONJUNCTION implicit biases of architecture. gradient descent PART-OF generalisation. implicit biases of architecture PART-OF generalisation. Method are neural networks, and network architecture. OtherScientificTerm are implicit bias of architecture, architecture, NNGP posterior, and minimum a posteriori functions. Metric is average test error. Generic is function. ","This paper studies the generalization properties of neural networks. The authors show that under certain assumptions on the architecture of the network, gradient descent can be seen as a function of the implicit bias of the architecture. They show that this is the case for infinite width networks, finite width networks and infinite width neural networks with infinite width. They also show that the error of gradient descent is bounded by the minimum a posteriori of the NNGP posterior, which is the sum of the average test error and the gradient descent error. ","This paper studies the generalization properties of neural networks. The authors show that under certain assumptions on the architecture of the network, gradient descent can be seen as a function of the implicit bias of the architecture. They show that this is the case for infinite width networks, finite width networks and infinite width neural networks with infinite width. They also show that the error of gradient descent is bounded by the minimum a posteriori of the NNGP posterior, which is the sum of the average test error and the gradient descent error. "
4735,SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"large - scale pre - trained multilingual representations USED-FOR cross - lingual transfer methods. transfer EVALUATE-FOR cross - lingual transfer methods. XTREME benchmark EVALUATE-FOR X - Mixup. X - Mixup COMPARE baselines. baselines COMPARE X - Mixup. text understanding tasks EVALUATE-FOR baselines. XTREME benchmark EVALUATE-FOR baselines. text understanding tasks EVALUATE-FOR X - Mixup. OtherScientificTerm are cross - lingual representation discrepancy, and representation discrepancy. Task is cross - lingual transfer. Generic is representations. ","This paper studies the problem of cross-lingual transfer in the context of pre-trained multilingual representations. The authors propose X-Mixup, a method to improve the performance of multilingual transfer methods. The method is based on the idea of large-scale pre-training multilingual representation learning. The proposed method is evaluated on the XTREME benchmark, where it is shown to outperform the baselines.","This paper studies the problem of cross-lingual transfer in the context of pre-trained multilingual representations. The authors propose X-Mixup, a method to improve the performance of multilingual transfer methods. The method is based on the idea of large-scale pre-training multilingual representation learning. The proposed method is evaluated on the XTREME benchmark, where it is shown to outperform the baselines."
4751,SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"defenses USED-FOR attacks. robust algorithms USED-FOR heterogeneous datasets. bucketing scheme USED-FOR robust algorithms. bucketing CONJUNCTION robust algorithms. robust algorithms CONJUNCTION bucketing. robust algorithms USED-FOR attacks. bucketing USED-FOR attacks. guaranteed convergence FEATURE-OF non - iid Byzantine robust problem. Method are Byzantine robust distributed or federated learning, and machine learning model. Generic are algorithm, problem, and approach. OtherScientificTerm are heterogeneous ( non - iid ), and realistic assumptions. ","This paper studies the Byzantine robust distributed or federated learning problem with heterogeneous (non-iid) data. The authors propose a new algorithm for this problem, which is based on the bucketing scheme. They show that under certain assumptions, the algorithm converges to a solution that is guaranteed to converge to the iid solution. They also show that the algorithm is robust to attacks. ","This paper studies the Byzantine robust distributed or federated learning problem with heterogeneous (non-iid) data. The authors propose a new algorithm for this problem, which is based on the bucketing scheme. They show that under certain assumptions, the algorithm converges to a solution that is guaranteed to converge to the iid solution. They also show that the algorithm is robust to attacks. "
4767,SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,disentanglement CONJUNCTION multi - task learning. multi - task learning CONJUNCTION disentanglement. hard parameter sharing USED-FOR multi - task learning. hard parameter sharing USED-FOR disentanglement. automatically generated supervised tasks USED-FOR neural networks. automatically generated supervised tasks USED-FOR representations. neural networks USED-FOR representations. disentanglement PART-OF multi - task neural network training. metrics EVALUATE-FOR disentanglement. Method is disentangled representations. Generic is they. Task is multi - task learning setting. ,"This paper studies the problem of disentanglement in multi-task learning. The authors propose two metrics to measure disentangled representations. The first metric is based on hard parameter sharing, and the second metric measures the distance between disentangling representations. ","This paper studies the problem of disentanglement in multi-task learning. The authors propose two metrics to measure disentangled representations. The first metric is based on hard parameter sharing, and the second metric measures the distance between disentangling representations. "
4783,SP:9851adb72e2918780f661f83f7da06eb866787be,Certifying Robust Policies ( CROP ) USED-FOR reinforcement learning. framework USED-FOR Certifying Robust Policies ( CROP ). framework USED-FOR state level robustness certification. adversarial state perturbations FEATURE-OF reinforcement learning. robustness CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION robustness. per - state actions CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION per - state actions. robustness CONJUNCTION per - state actions. per - state actions CONJUNCTION robustness. per - state actions HYPONYM-OF robustness certification criteria. lower bound of cumulative rewards HYPONYM-OF robustness certification criteria. robustness HYPONYM-OF robustness certification criteria. policy USED-FOR robustness. Gaussian noise USED-FOR Q - functions. Q - functions USED-FOR policy. policy USED-FOR local smoothing algorithm. Gaussian noise USED-FOR policy. global smoothing algorithm USED-FOR robustness. robustness FEATURE-OF finite - horizon cumulative reward. global smoothing algorithm USED-FOR finite - horizon cumulative reward. adversarial state perturbations FEATURE-OF finite - horizon cumulative reward. tight certification bounds FEATURE-OF reward. adaptive search USED-FOR tight certification bounds. adaptive search USED-FOR local smoothing approach. adversarial training CONJUNCTION regularization. regularization CONJUNCTION adversarial training. methods USED-FOR empirically robust RL. RL robustness certification framework EVALUATE-FOR methods. regularization HYPONYM-OF methods. adversarial training HYPONYM-OF methods. adversarial training USED-FOR empirically robust RL. Atari games EVALUATE-FOR methods. RegCVX CONJUNCTION RadialRL. RadialRL CONJUNCTION RegCVX. RegPGD CONJUNCTION RegCVX. RegCVX CONJUNCTION RegPGD. certified robustness EVALUATE-FOR RadialRL. adversarial attacks EVALUATE-FOR algorithms. OtherScientificTerm is cumulative rewards. Generic is trajectory. ,"This paper proposes a framework for certifying robust policies (CROP) in reinforcement learning. CROP is based on the notion of state-level robustness, which is defined as the lower bound of cumulative rewards and per-state actions. The authors propose a global smoothing algorithm to certify the robustness of a policy to adversarial state perturbations, and a local smoothing approach to certify a policy's robustness to a finite-horizon cumulative reward. The proposed method is evaluated on several Atari games and compared to several baselines. ","This paper proposes a framework for certifying robust policies (CROP) in reinforcement learning. CROP is based on the notion of state-level robustness, which is defined as the lower bound of cumulative rewards and per-state actions. The authors propose a global smoothing algorithm to certify the robustness of a policy to adversarial state perturbations, and a local smoothing approach to certify a policy's robustness to a finite-horizon cumulative reward. The proposed method is evaluated on several Atari games and compared to several baselines. "
4799,SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"approach USED-FOR conformal prediction. conformal prediction USED-FOR model uncertainty. calibrated candidate set USED-FOR conformal prediction. in - silico screening USED-FOR drug discovery. coverage CONJUNCTION precision. precision CONJUNCTION coverage. natural language processing CONJUNCTION computer vision. computer vision CONJUNCTION natural language processing. computer vision CONJUNCTION computational chemistry. computational chemistry CONJUNCTION computer vision. natural language processing FEATURE-OF classification tasks. computational chemistry FEATURE-OF classification tasks. classification tasks EVALUATE-FOR approach. computer vision HYPONYM-OF classification tasks. OtherScientificTerm are coverage property, conformal sets, noisy candidates, predicted conformal sets, and user - specified tolerance. Task is large - scale settings. Generic are constraint, and algorithm. Metric is true positive rate. ","This paper proposes a new method for conformal prediction for in-silico screening. The proposed method is based on the observation that the true positive rate of the conformal set is not always the same as the true negative rate. To address this issue, the authors propose to use a calibrated candidate set to calibrate the candidate set, which is then used to train a conformal classifier. The method is evaluated on a variety of classification tasks, including computer vision, natural language processing, and computational chemistry.","This paper proposes a new method for conformal prediction for in-silico screening. The proposed method is based on the observation that the true positive rate of the conformal set is not always the same as the true negative rate. To address this issue, the authors propose to use a calibrated candidate set to calibrate the candidate set, which is then used to train a conformal classifier. The method is evaluated on a variety of classification tasks, including computer vision, natural language processing, and computational chemistry."
4815,SP:b126d2f3c397633745c8833e22ace93a2470e963,complexity EVALUATE-FOR functions. neural network USED-FOR functions. unit - length curve USED-FOR network. random initialization USED-FOR ReLU networks. higher moments of the length distortion CONJUNCTION distortion of higher - dimensional volumes. distortion of higher - dimensional volumes CONJUNCTION higher moments of the length distortion. upper bounds USED-FOR higher moments of the length distortion. upper bounds USED-FOR distortion of higher - dimensional volumes. OtherScientificTerm is curve of outputs. Metric is expected length distortion. ,This paper studies the effect of the length distortion of ReLU networks on the complexity of functions. The authors consider the case where the output of a ReLU network is a unit-length curve and the network is initialized with a random initialization. They show that the expected length distortion is a function of the number of parameters of the network. They also provide upper bounds for the distortion of higher-dimensional volumes. ,This paper studies the effect of the length distortion of ReLU networks on the complexity of functions. The authors consider the case where the output of a ReLU network is a unit-length curve and the network is initialized with a random initialization. They show that the expected length distortion is a function of the number of parameters of the network. They also provide upper bounds for the distortion of higher-dimensional volumes. 
4831,SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"methods USED-FOR policies. safety constraints CONJUNCTION sparse rewards. sparse rewards CONJUNCTION safety constraints. learning policies PART-OF reinforcement learning ( RL ) problems. Behavioral priors USED-FOR RL. Behavioral priors USED-FOR policy primitives. behavioral priors USED-FOR safe policy learning. SAFEty skill pRiors ( SAFER ) HYPONYM-OF behavioral prior learning algorithm. policy learning USED-FOR complex control tasks. behavioral prior learning algorithm USED-FOR policy learning. SAFER USED-FOR safety variable. contrastive training USED-FOR SAFER. offline data USED-FOR safety requirements. safe and unsafe data USED-FOR contrastive training. abstract actions USED-FOR safe primitive skills. offline data USED-FOR safety variable. SAFER USED-FOR safe and successful policy. safety variable CONJUNCTION abstract action. abstract action CONJUNCTION safety variable. SAFER USED-FOR inference stage. safety variable USED-FOR safety skills. safety skills USED-FOR SAFER. safety skills USED-FOR safe and successful policy. SAFER USED-FOR policies. baseline methods USED-FOR policies. SAFER COMPARE baseline methods. baseline methods COMPARE SAFER. SAFER USED-FOR safety. policies CONJUNCTION safety. safety CONJUNCTION policies. baseline methods USED-FOR safety. Material is offline datasets. OtherScientificTerm are unsafe behavior, and undesirable behaviors. Task is complex safety - critical robotic grasping tasks. ","This paper proposes SAFER, a behavioral prior learning algorithm for safety-critical robotic grasping tasks. The proposed method is based on contrastive training to learn safety skills that can be used in the inference stage of policy learning. The safety skills are learned from offline data, and are then used to train a policy that is safe and successful in the offline setting. The authors show that the proposed method outperforms baselines on a number of tasks.","This paper proposes SAFER, a behavioral prior learning algorithm for safety-critical robotic grasping tasks. The proposed method is based on contrastive training to learn safety skills that can be used in the inference stage of policy learning. The safety skills are learned from offline data, and are then used to train a policy that is safe and successful in the offline setting. The authors show that the proposed method outperforms baselines on a number of tasks."
4847,SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"multi - branch restoration model USED-FOR restoration tasks. Human Visual System USED-FOR multi - branch restoration model. Retinal Ganglion Cells HYPONYM-OF Human Visual System. deraindrop CONJUNCTION deblurring. deblurring CONJUNCTION deraindrop. image dehazing CONJUNCTION deraindrop. deraindrop CONJUNCTION image dehazing. datasets EVALUATE-FOR multi - branch architecture. CMFNet HYPONYM-OF multi - branch architecture. deblurring HYPONYM-OF datasets. deraindrop HYPONYM-OF datasets. image dehazing HYPONYM-OF datasets. pretrained models USED-FOR restoration tasks. Task are Image restoration, and autonomous cars. Method is learning based restoration methods. OtherScientificTerm is generalization. ","This paper proposes a multi-branch restoration model for image restoration. The proposed model is based on the Human Visual System (HVS) and is trained on three different restoration tasks: deblurring, deraindrop, and image dehazing. The authors show that the proposed model achieves state-of-the-art performance on all three tasks. ","This paper proposes a multi-branch restoration model for image restoration. The proposed model is based on the Human Visual System (HVS) and is trained on three different restoration tasks: deblurring, deraindrop, and image dehazing. The authors show that the proposed model achieves state-of-the-art performance on all three tasks. "
4863,SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"FL USED-FOR data heterogeneity. Personalized federated learning ( PFL ) USED-FOR data heterogeneity. FL USED-FOR Personalized federated learning ( PFL ). FL CONJUNCTION PFL. PFL CONJUNCTION FL. unlabeled clients USED-FOR model. hypernetwork module CONJUNCTION encoder module. encoder module CONJUNCTION hypernetwork module. approach USED-FOR IT - PFLHN. encoder module USED-FOR approach. hypernetwork module USED-FOR approach. encoder module USED-FOR IT - PFLHN. hypernetwork module USED-FOR IT - PFLHN. hypernetwork USED-FOR personalized model. client representation PART-OF hypernetwork. benchmark datasets EVALUATE-FOR IT - PFL - HN. IT - PFL - HN COMPARE FL and PFL methods. FL and PFL methods COMPARE IT - PFL - HN. benchmark datasets EVALUATE-FOR FL and PFL methods. multi - task learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION multi - task learning. multi - task learning USED-FOR it. Method are Federated learning ( FL ), personalized models, prediction service, and encoder network. Material is labeled data. OtherScientificTerm are unlabeled data, large domain shift, data privacy, and differential privacy. Generic is learning setup. Metric is generalization error. ","This paper proposes a new personalized federated learning (PFL) framework, called IT-PFL-HN, which is based on the hypernetwork module and encoder module. The hypernetwork is used to model the client representation of the personalized model, and the encoder is used for the prediction service. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization error, multi-task learning, and domain adaptation. ","This paper proposes a new personalized federated learning (PFL) framework, called IT-PFL-HN, which is based on the hypernetwork module and encoder module. The hypernetwork is used to model the client representation of the personalized model, and the encoder is used for the prediction service. The authors show that the proposed method outperforms the state-of-the-art in terms of generalization error, multi-task learning, and domain adaptation. "
4879,SP:960d0a63a82593f6e72275b65f0501f0469d1924,"classification PART-OF selfsupervised learning. conditional diffusion based generative model ( RCDM ) USED-FOR representations. self - supervised models USED-FOR representations. model COMPARE generative models. generative models COMPARE model. generation quality COMPARE generative models. generative models COMPARE generation quality. generation quality EVALUATE-FOR model. tool USED-FOR self - supervised models. SSL projector embedding USED-FOR tasks. classifications HYPONYM-OF tasks. SSL model USED-FOR image manipulation. inherent structure USED-FOR image manipulation. SSL model USED-FOR inherent structure. supervised representation CONJUNCTION SSL representation. SSL representation CONJUNCTION supervised representation. Method are neural networks, SSL ( backbone ) representation, and SSL representations. Generic is representation. OtherScientificTerm is conditioning. ",This paper proposes a self-supervised learning method for image classification. The method is based on a conditional diffusion based generative model (RCDM). The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of image classification tasks. The authors also show that their method can be applied to image manipulation tasks. ,This paper proposes a self-supervised learning method for image classification. The method is based on a conditional diffusion based generative model (RCDM). The authors show that the proposed method is able to achieve better performance than the state-of-the-art on a variety of image classification tasks. The authors also show that their method can be applied to image manipulation tasks. 
4895,SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,Fp sketch HYPONYM-OF well - celebrated streaming algorithm. well - celebrated streaming algorithm USED-FOR frequency moments estimation. DP baselines COMPARE non - private baseline. non - private baseline COMPARE DP baselines. Fp sketch COMPARE DP baselines. DP baselines COMPARE Fp sketch. Fp sketch COMPARE non - private baseline. non - private baseline COMPARE Fp sketch. logarithmic factor USED-FOR non - private baseline. polylogarithmic space USED-FOR Fp sketch. differential privacy guarantee FEATURE-OF accuracy. differential privacy guarantee FEATURE-OF Fp sketch. accuracy EVALUATE-FOR Fp sketch. OtherScientificTerm is evaluation code. ,This paper proposes a new algorithm for frequency moments estimation based on the well-celebrated Fp sketch algorithm. The authors show that the proposed algorithm is more private than the non-private baseline and achieves better performance than DP baselines. The main contribution of the paper is to provide a differential privacy guarantee for the Fp algorithm. ,This paper proposes a new algorithm for frequency moments estimation based on the well-celebrated Fp sketch algorithm. The authors show that the proposed algorithm is more private than the non-private baseline and achieves better performance than DP baselines. The main contribution of the paper is to provide a differential privacy guarantee for the Fp algorithm. 
4911,SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"paradigm USED-FOR diverse strategies. Reward - Switching Policy Optimization ( RSPO ) USED-FOR diverse strategies. diverse strategies USED-FOR complex RL environments. Reward - Switching Policy Optimization ( RSPO ) HYPONYM-OF paradigm. trajectory - based novelty measurement USED-FOR optimization process. RSPO USED-FOR extrinsic and intrinsic rewards. trajectory - based novelty measurement USED-FOR RSPO. trajectory - based novelty measurement USED-FOR extrinsic and intrinsic rewards. policy optimization USED-FOR RSPO. extrinsic rewards USED-FOR policy optimization. extrinsic rewards USED-FOR RSPO. intrinsic diversity reward USED-FOR exploration. RSPO USED-FOR exploration. RSPO USED-FOR trajectories. policies USED-FOR trajectories. intrinsic diversity reward USED-FOR RSPO. single - agent particle - world tasks CONJUNCTION MuJoCo continuous control. MuJoCo continuous control CONJUNCTION single - agent particle - world tasks. multi - agent stag - hunt games CONJUNCTION StarCraftII challenges. StarCraftII challenges CONJUNCTION multi - agent stag - hunt games. RSPO USED-FOR strategies. MuJoCo continuous control CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION MuJoCo continuous control. single - agent particle - world tasks CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION single - agent particle - world tasks. OtherScientificTerm are learning policy, and sampled trajectory. ","This paper proposes a new reward-switching policy optimization framework for RL. The proposed method is based on a trajectory-based novelty measurement, which is able to capture both extrinsic and intrinsic rewards. The authors show that the proposed method outperforms the baselines in a range of RL environments. They also show that RSPO can be applied to multi-agent reinforcement learning tasks.","This paper proposes a new reward-switching policy optimization framework for RL. The proposed method is based on a trajectory-based novelty measurement, which is able to capture both extrinsic and intrinsic rewards. The authors show that the proposed method outperforms the baselines in a range of RL environments. They also show that RSPO can be applied to multi-agent reinforcement learning tasks."
4927,SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models HYPONYM-OF generative models. GANs COMPARE autoregressive models. autoregressive models COMPARE GANs. sample quality CONJUNCTION autoregressive models. autoregressive models CONJUNCTION sample quality. autoregressive models USED-FOR likelihood scores. GANs HYPONYM-OF generative models. autoregressive models HYPONYM-OF generative models. sample quality EVALUATE-FOR GANs. method USED-FOR fast samplers. fast samplers USED-FOR diffusion model. flexible non - Markovian samplers USED-FOR diffusion models. Generalized Gaussian Diffusion Models ( GGDM ) HYPONYM-OF flexible non - Markovian samplers. degrees of freedom FEATURE-OF GGDM samplers. sample quality scores USED-FOR GGDM samplers. gradient descent USED-FOR sample quality scores. sample quality scores USED-FOR degrees of freedom. gradient descent USED-FOR GGDM samplers. reparametrization trick CONJUNCTION gradient rematerialization. gradient rematerialization CONJUNCTION reparametrization trick. sampling process USED-FOR optimization procedure. reparametrization trick USED-FOR optimization procedure. gradient rematerialization USED-FOR optimization procedure. DDSS USED-FOR unconditional image generation. datasets EVALUATE-FOR DDSS. FID scores HYPONYM-OF datasets. fine - tuning CONJUNCTION re - training. re - training CONJUNCTION fine - tuning. method CONJUNCTION pre - trained diffusion model. pre - trained diffusion model CONJUNCTION method. Generic is model. OtherScientificTerm are LSUN, and inference steps. Method is DDPM / DDIM baselines. ",This paper proposes a new sampling method for diffusion models. The proposed method is based on gradient descent and reparametrization trick. The authors show that the proposed method outperforms existing methods in terms of sample quality and FID scores. They also show that their method can be used to improve the sample quality of autoregressive models. ,This paper proposes a new sampling method for diffusion models. The proposed method is based on gradient descent and reparametrization trick. The authors show that the proposed method outperforms existing methods in terms of sample quality and FID scores. They also show that their method can be used to improve the sample quality of autoregressive models. 
4943,SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,Large Language Models ( LLMs ) USED-FOR factual information. embedding layer PART-OF LLMs. lightweight models HYPONYM-OF P - Adapters. embedding layer PART-OF lightweight models. continuous prompts USED-FOR LLM. LLM embeddings USED-FOR continuous prompts. LLM embeddings USED-FOR They. Mixture of Experts ( MoE ) models USED-FOR continuous prompts. classifier USED-FOR natural language prompts. classifier USED-FOR They. human - annotated data USED-FOR classifier. P - Adapters COMPARE MoE models. MoE models COMPARE P - Adapters. MoE models USED-FOR factual information. P - Adapters USED-FOR factual information. consistency EVALUATE-FOR baseline. PAdapters COMPARE baseline. baseline COMPARE PAdapters. precision CONJUNCTION consistency. consistency CONJUNCTION precision. consistency EVALUATE-FOR PAdapters. natural language queries USED-FOR baseline. precision EVALUATE-FOR PAdapters. LLM ’s embeddings USED-FOR natural language prompt. OtherScientificTerm is continuous ones. Method is P - Adapter. ,"This paper proposes a method for learning a classifier for continuous natural language prompts. The proposed method is based on the P-Adapters architecture, which is a lightweight model that can be applied to any LLM. The authors show that the proposed method outperforms MoE models in terms of accuracy, consistency, and precision. ","This paper proposes a method for learning a classifier for continuous natural language prompts. The proposed method is based on the P-Adapters architecture, which is a lightweight model that can be applied to any LLM. The authors show that the proposed method outperforms MoE models in terms of accuracy, consistency, and precision. "
4959,SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"one - shot classification COMPARE CCTS. CCTS COMPARE one - shot classification. catastrophic forgetting CONJUNCTION over fitting. over fitting CONJUNCTION catastrophic forgetting. unclear distribution division FEATURE-OF continual learning task. continual learning task USED-FOR CCTS. Adaptive model training policy USED-FOR CCTS. Adaptive multi - distribution extraction policy PART-OF adaptability. fixed rules CONJUNCTION prior knowledge. prior knowledge CONJUNCTION fixed rules. ACCTS USED-FOR data distributions. data distributions USED-FOR time series evolution. time series evolution CONJUNCTION model change. model change CONJUNCTION time series evolution. method COMPARE baselines. baselines COMPARE method. real - world datasets EVALUATE-FOR method. Task is real - world applications. OtherScientificTerm are vital signs, features, multi - distribution form, independent identically distributed premise, and fixed division rule. Generic are concept, models, and model. Method are Continuous Classification of Time Series ( CCTS ), and Adaptive importance - based replay policy. ",This paper proposes a continual learning method for continuous classification of time series (CTS). The proposed method is based on Adaptive Importance-based Replay Policy (ACCTS) and Adaptive Multi-Distribution Extraction Policy (AMDP). ACCTS uses a multi-distribution extraction policy to extract important features from the data distribution. The authors show that the proposed method outperforms existing methods on a variety of datasets. ,This paper proposes a continual learning method for continuous classification of time series (CTS). The proposed method is based on Adaptive Importance-based Replay Policy (ACCTS) and Adaptive Multi-Distribution Extraction Policy (AMDP). ACCTS uses a multi-distribution extraction policy to extract important features from the data distribution. The authors show that the proposed method outperforms existing methods on a variety of datasets. 
4975,SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,internal representations of past inputs USED-FOR language models. approximate kNN lookup USED-FOR language modeling. benchmarks CONJUNCTION tasks. tasks CONJUNCTION benchmarks. approximate kNN lookup PART-OF memory. tasks EVALUATE-FOR language modeling. benchmarks EVALUATE-FOR language modeling. generic webtext ( C4 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF benchmarks. generic webtext ( C4 ) HYPONYM-OF benchmarks. theorems USED-FOR model. Method is Language models. ,"This paper studies the problem of approximate kNN lookup for language models. The authors propose a new approach to compute approximate k-NN lookup in the memory of a language model. The approach is based on the observation that k-Nets can be used to compute the k-values of past inputs, which can then be used as input for the current task. They show that this approach can be applied to a variety of tasks, and show that it is able to achieve better performance than previous approaches. ","This paper studies the problem of approximate kNN lookup for language models. The authors propose a new approach to compute approximate k-NN lookup in the memory of a language model. The approach is based on the observation that k-Nets can be used to compute the k-values of past inputs, which can then be used as input for the current task. They show that this approach can be applied to a variety of tasks, and show that it is able to achieve better performance than previous approaches. "
4991,SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"MLMs USED-FOR probability distribution. masked language modeling ( MLM ) objective USED-FOR models. energy - based sequence models USED-FOR MLMs. MLMs USED-FOR energy parametrizations. Metropolis – Hastings Monte Carlo algorithm USED-FOR tractable sampling scheme. masked conditionals USED-FOR masked language models. energybased models USED-FOR open - ended unconditional generation. approach COMPARE undirected generation approaches. undirected generation approaches COMPARE approach. stationary distribution FEATURE-OF Markov chain. Method are parametrizations, and sampling algorithm. Task is machine translation. ",This paper proposes a sampling scheme for masked language modeling (MLM) based on the Metropolis-Hastings Monte Carlo (MHMC) algorithm. The proposed sampling scheme is based on an energy-based sequence model. The authors show that the proposed method is tractable for open-ended unconditional generation and can be used for machine translation. The paper also provides a theoretical analysis of the sampling scheme.,This paper proposes a sampling scheme for masked language modeling (MLM) based on the Metropolis-Hastings Monte Carlo (MHMC) algorithm. The proposed sampling scheme is based on an energy-based sequence model. The authors show that the proposed method is tractable for open-ended unconditional generation and can be used for machine translation. The paper also provides a theoretical analysis of the sampling scheme.
5007,SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"data augmentation USED-FOR deep neural networks. deep neural networks USED-FOR NLP tasks. data augmentation USED-FOR NLP tasks. labeled samples USED-FOR It. low - data or classimbalanced regimes HYPONYM-OF labeled samples. parameter tuning CONJUNCTION inherent randomness. inherent randomness CONJUNCTION parameter tuning. inherent randomness USED-FOR augmentation techniques. parameter tuning USED-FOR augmentation techniques. reward function USED-FOR policy. augmentation USED-FOR NLP tasks. augmentation strategy USED-FOR task. learning data augmentation policy USED-FOR augmentation strategy. reward function USED-FOR augmentation policy. learning - based augmentation COMPARE augmentation schemes. augmentation schemes COMPARE learning - based augmentation. augmentations USED-FOR task. text classification tasks EVALUATE-FOR augmentation schemes. text classification tasks EVALUATE-FOR learning - based augmentation. augmentation policy USED-FOR tasks. method USED-FOR low - data and class - imbalanced regimes. OtherScientificTerm are informative training signals, and semantic similarity. Method are data augmentation policy, and sample re - weighting scheme. Generic is model. ","This paper studies the problem of data augmentation for NLP tasks. The authors propose a new learning-based augmentation method for low-data and class-imbalanced settings. The proposed method is based on the idea of learning a reward function for the augmentation policy, which is then used to learn a new augmentation strategy for the task. The paper shows that the proposed method outperforms existing methods on a variety of tasks.","This paper studies the problem of data augmentation for NLP tasks. The authors propose a new learning-based augmentation method for low-data and class-imbalanced settings. The proposed method is based on the idea of learning a reward function for the augmentation policy, which is then used to learn a new augmentation strategy for the task. The paper shows that the proposed method outperforms existing methods on a variety of tasks."
5023,SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"Meta - learning USED-FOR offline reinforcement learning ( OMRL ). augmented state USED-FOR task identity. intra - task attention mechanism CONJUNCTION inter - task contrastive learning objectives. inter - task contrastive learning objectives CONJUNCTION intra - task attention mechanism. sparse reward CONJUNCTION distribution shift. distribution shift CONJUNCTION sparse reward. sparse reward USED-FOR task representation learning. distribution shift USED-FOR task representation learning. FOCAL HYPONYM-OF SOTA OMRL algorithms. meta - RL benchmarks EVALUATE-FOR prior algorithms. Method are RL algorithms, and context - based encoder. ","This paper proposes a meta-learning framework for offline reinforcement learning (OMRL). The authors propose a context-based encoder for meta-RL, which is able to learn the task identity of each task from the context of the previous task. The proposed method is evaluated on a variety of offline RL benchmarks. The authors show that the proposed method outperforms the state-of-the-art algorithms on the FOCAL benchmark. ","This paper proposes a meta-learning framework for offline reinforcement learning (OMRL). The authors propose a context-based encoder for meta-RL, which is able to learn the task identity of each task from the context of the previous task. The proposed method is evaluated on a variety of offline RL benchmarks. The authors show that the proposed method outperforms the state-of-the-art algorithms on the FOCAL benchmark. "
5039,SP:ed86c60850d5c8302dcf1c2167db303e778fe681,belief state FEATURE-OF partially observable Markov system. parametric sequential generative modeling methods USED-FOR problem setting. methods USED-FOR belief state modeling. belief state modeling USED-FOR multi - agent settings. policies PART-OF belief model. inference - time improvement framework USED-FOR parametric sequential generative modeling methods. belief fine - tuning ( BFT ) HYPONYM-OF inference - time improvement framework. approximate dynamic programming USED-FOR model parameters. fine - tuning USED-FOR model parameters. BFT USED-FOR model parameters. fine - tuning FEATURE-OF approximate dynamic programming. fine - tuning USED-FOR BFT. approximate dynamic programming USED-FOR BFT. accuracy EVALUATE-FOR belief model. It USED-FOR belief model. it USED-FOR model. accuracy EVALUATE-FOR It. belief model USED-FOR BFT. BFT USED-FOR approximate public belief state search. imperfect - information games FEATURE-OF approximate public belief state search. Method is dynamics model. OtherScientificTerm is specialization. ,"This paper proposes a belief fine-tuning (BFT) framework to improve the performance of belief models in multi-agent settings. BFT is a method for learning a belief model for a partially observable Markov system. The method is based on the idea of approximate dynamic programming (ADP), which is an extension of the ADP framework. The key idea of ADP is to learn a model that can be used to fine-tune the model parameters. The authors show that the proposed method is able to achieve state-of-the-art performance on the public belief state search task. ","This paper proposes a belief fine-tuning (BFT) framework to improve the performance of belief models in multi-agent settings. BFT is a method for learning a belief model for a partially observable Markov system. The method is based on the idea of approximate dynamic programming (ADP), which is an extension of the ADP framework. The key idea of ADP is to learn a model that can be used to fine-tune the model parameters. The authors show that the proposed method is able to achieve state-of-the-art performance on the public belief state search task. "
5055,SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"accuracy loss CONJUNCTION slow training runtime. slow training runtime CONJUNCTION accuracy loss. accuracy loss EVALUATE-FOR methods. sparse matrices USED-FOR sparsity mask. fixed structure FEATURE-OF sparse matrices. products of butterfly matrices HYPONYM-OF fixed structure. hardware USED-FOR butterfly ( block and flat ). attention CONJUNCTION MLP. MLP CONJUNCTION attention. fixed sparsity pattern USED-FOR network layers. method USED-FOR network layers. Pixelated Butterfly USED-FOR method. flat block butterfly and low - rank matrices USED-FOR fixed sparsity pattern. fixed sparsity pattern USED-FOR Pixelated Butterfly. fixed sparsity pattern USED-FOR method. flat block butterfly and low - rank matrices USED-FOR method. attention HYPONYM-OF network layers. MLP HYPONYM-OF network layers. Pixelated Butterfly COMPARE butterfly. butterfly COMPARE Pixelated Butterfly. Pixelated Butterfly USED-FOR training. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR sparse models. dense MLP - Mixer CONJUNCTION Vision Transformer. Vision Transformer CONJUNCTION dense MLP - Mixer. sparse models COMPARE dense MLP - Mixer. dense MLP - Mixer COMPARE sparse models. Vision Transformer CONJUNCTION GPT-2 medium. GPT-2 medium CONJUNCTION Vision Transformer. sparse models COMPARE GPT-2 medium. GPT-2 medium COMPARE sparse models. sparse models COMPARE Vision Transformer. Vision Transformer COMPARE sparse models. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR dense MLP - Mixer. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR GPT-2 medium. accuracy EVALUATE-FOR sparse models. Method are Overparameterized neural networks, Sparse model training, and model components. Metric are computational cost, and accuracy – efficiency tradeoffs. OtherScientificTerm is butterfly matrices. ","This paper proposes a new method for training sparse neural networks. The proposed method, Pixelated Butterfly, is based on the idea that the sparsity mask of sparse matrices is a product of the products of butterfly matrices and low-rank matrices. The paper shows that the proposed method is able to achieve competitive performance on ImageNet classification and WikiText-103 language modeling tasks. ","This paper proposes a new method for training sparse neural networks. The proposed method, Pixelated Butterfly, is based on the idea that the sparsity mask of sparse matrices is a product of the products of butterfly matrices and low-rank matrices. The paper shows that the proposed method is able to achieve competitive performance on ImageNet classification and WikiText-103 language modeling tasks. "
5071,SP:136e31054a55abca840f6478491972023c2296cb,"score matching USED-FOR data distribution. formulation USED-FOR controllable generation. class center PART-OF forward and reverse process. class center USED-FOR conditional diffusion probabilistic model. faster sampling USED-FOR method. inception score CONJUNCTION FID score. FID score CONJUNCTION inception score. state - of - the - art methods COMPARE conditional image generation. conditional image generation COMPARE state - of - the - art methods. framework COMPARE state - of - the - art methods. state - of - the - art methods COMPARE framework. CIFAR-10 USED-FOR conditional image generation. FID score EVALUATE-FOR conditional image generation. inception score EVALUATE-FOR conditional image generation. Method are Score - based generative models, and diffusion probabilistic models. OtherScientificTerm are Markov chain, and class clustering phenomenon. ",This paper proposes a conditional diffusion probabilistic model for conditional image generation based on score matching. The proposed method is based on the class center of the forward and reverse process of the Markov chain. The authors show that the proposed method outperforms state-of-the-art methods on CIFAR-10 and ImageNet. ,This paper proposes a conditional diffusion probabilistic model for conditional image generation based on score matching. The proposed method is based on the class center of the forward and reverse process of the Markov chain. The authors show that the proposed method outperforms state-of-the-art methods on CIFAR-10 and ImageNet. 
5087,SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"fixed domain - invariant features CONJUNCTION common hypotheses. common hypotheses CONJUNCTION fixed domain - invariant features. generalization EVALUATE-FOR domain generalization ( DG ) approaches. generalization EVALUATE-FOR prediction tasks. label - informative features USED-FOR label prediction task. label - informative features USED-FOR latent sub - spaces. DG benchmarks EVALUATE-FOR it. DG benchmarks EVALUATE-FOR method. Metric is generalization capacity. Generic are assumption, and approaches. OtherScientificTerm are invariant hypothesis, and sub - spaces. Method is LASSO. ","This paper studies the problem of domain generalization (DG) in the context of label prediction tasks. The authors propose a new domain generalisation method, LASSO, which is based on the assumption that the latent sub-spaces are invariant to invariant hypothesis and common hypotheses. The main contribution of the paper is to provide a theoretical analysis of the generalization capacity of the proposed method. The method is evaluated on a number of benchmark datasets and shows that it outperforms existing methods. ","This paper studies the problem of domain generalization (DG) in the context of label prediction tasks. The authors propose a new domain generalisation method, LASSO, which is based on the assumption that the latent sub-spaces are invariant to invariant hypothesis and common hypotheses. The main contribution of the paper is to provide a theoretical analysis of the generalization capacity of the proposed method. The method is evaluated on a number of benchmark datasets and shows that it outperforms existing methods. "
5103,SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"kernel thinning ( KT ) algorithm USED-FOR probability distribution. kernel thinning ( KT ) algorithm COMPARE independent sampling. independent sampling COMPARE kernel thinning ( KT ) algorithm. reproducing kernel Hilbert space ( RKHS ) USED-FOR independent sampling. KT USED-FOR RKHS. kernel CONJUNCTION distribution. distribution CONJUNCTION kernel. KT USED-FOR kernel. KT USED-FOR dimension - free guarantees. dimension - free guarantees FEATURE-OF kernel. inverse multiquadric CONJUNCTION sinc. sinc CONJUNCTION inverse multiquadric. Gaussian CONJUNCTION inverse multiquadric. inverse multiquadric CONJUNCTION Gaussian. target KT COMPARE square - root KT. square - root KT COMPARE target KT. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR square - root KT. target KT HYPONYM-OF analytic kernels. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR target KT. sinc HYPONYM-OF analytic kernels. Gaussian HYPONYM-OF analytic kernels. inverse multiquadric HYPONYM-OF analytic kernels. Laplace CONJUNCTION Matérn. Matérn CONJUNCTION Laplace. fractional power kernel USED-FOR KT. Laplace HYPONYM-OF non - smooth kernels. Matérn HYPONYM-OF non - smooth kernels. MMD guarantees CONJUNCTION individual function guarantees. individual function guarantees CONJUNCTION MMD guarantees. KT USED-FOR target and power kernels. individual function guarantees FEATURE-OF target KT. target KT CONJUNCTION KT+. KT+ CONJUNCTION target KT. integration error EVALUATE-FOR target KT. integration error EVALUATE-FOR KT+. OtherScientificTerm are square - root kernel, square - roots, and differential equation posteriors. ","This paper studies the problem of kernel thinning (KT) in reproducing kernel Hilbert space (RKHS). In particular, the authors consider the case where the target and power kernels of the RKHS are non-smooth. The authors show that the target KT and the power KT are dimension-free, and that the maximum mean discrepancy (MMD) guarantees of the target kernel and power kernel are guaranteed. They also provide a theoretical analysis of the MMD guarantees for the target, power and Laplace kernels.","This paper studies the problem of kernel thinning (KT) in reproducing kernel Hilbert space (RKHS). In particular, the authors consider the case where the target and power kernels of the RKHS are non-smooth. The authors show that the target KT and the power KT are dimension-free, and that the maximum mean discrepancy (MMD) guarantees of the target kernel and power kernel are guaranteed. They also provide a theoretical analysis of the MMD guarantees for the target, power and Laplace kernels."
5119,SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"Combinatorial optimization USED-FOR real - world problems. open - source benchmark suite USED-FOR NP - hard MAXIMUM INDEPENDENT SET problem. weighted and unweighted variants FEATURE-OF open - source benchmark suite. benchmark suite USED-FOR guided tree search algorithm. graph convolution network USED-FOR tree search. code quality CONJUNCTION extensibility. extensibility CONJUNCTION code quality. graph convolution network USED-FOR solution structure. random values USED-FOR graph convolution network. extensibility EVALUATE-FOR algorithm. code quality EVALUATE-FOR algorithm. graph kernelization HYPONYM-OF algorithmic techniques. algorithmic techniques USED-FOR tree search. tree search implementations COMPARE solvers. solvers COMPARE tree search implementations. competitive solution quality EVALUATE-FOR GNN. GNN USED-FOR solver. reinforcement learning USED-FOR solver. Method are graph neural networks ( GNNs ), machine learning - based solvers, and classical algorithmic solvers. OtherScientificTerm are NP - hard problems, and problem - specific solution structure. Generic is suite. ",This paper proposes a guided tree search algorithm for solving NP-hard problems. The proposed algorithm is based on graph convolutional neural networks (GNNs) and is trained with reinforcement learning. The authors show that the proposed algorithm outperforms existing tree search algorithms on a suite of open-source benchmark problems. ,This paper proposes a guided tree search algorithm for solving NP-hard problems. The proposed algorithm is based on graph convolutional neural networks (GNNs) and is trained with reinforcement learning. The authors show that the proposed algorithm outperforms existing tree search algorithms on a suite of open-source benchmark problems. 
5135,SP:155ecd17d264a084b014abdfd0362146d8fb07e0,Quantization USED-FOR compressing Convolutional Neural Networks ( CNNs ). computational resources USED-FOR compressing Convolutional Neural Networks ( CNNs ). semantic segmentation CONJUNCTION depth prediction. depth prediction CONJUNCTION semantic segmentation. prediction accuracy EVALUATE-FOR networks. depth prediction HYPONYM-OF image - to - image tasks. semantic segmentation HYPONYM-OF image - to - image tasks. approach USED-FOR activation maps compression. activation maps compression USED-FOR 1 × 1 convolutions. 1 × 1 convolutions PART-OF CNNs. compression ratios CONJUNCTION computational savings. computational savings CONJUNCTION compression ratios. computational savings CONJUNCTION low bit quantization rates. low bit quantization rates CONJUNCTION computational savings. low bit quantization rates EVALUATE-FOR WCC. computational savings EVALUATE-FOR WCC. accuracy EVALUATE-FOR WCC. compression ratios EVALUATE-FOR WCC. hardware - friendly Haar - wavelet transform USED-FOR image compression. convolution PART-OF compressed activation map. 1× 1 convolution PART-OF network architecture. 1× 1 convolution USED-FOR WCC. network architecture USED-FOR WCC. WCC CONJUNCTION light quantization. light quantization CONJUNCTION WCC. Method is Convolutional Neural Networks ( CNNs ). OtherScientificTerm is quantization. Metric is compression rates. ,"This paper proposes a new method for compression of convolutional neural networks (CNNs). The proposed method is based on the Haar-wavelet transform (WCC) method, which is a hardware-friendly Haar wavelet transform for image compression. The authors show that WCC is able to achieve low bit quantization rates and low bit compression ratios. They also show that the proposed method can be applied to 1×1 convolutions.","This paper proposes a new method for compression of convolutional neural networks (CNNs). The proposed method is based on the Haar-wavelet transform (WCC) method, which is a hardware-friendly Haar wavelet transform for image compression. The authors show that WCC is able to achieve low bit quantization rates and low bit compression ratios. They also show that the proposed method can be applied to 1×1 convolutions."
5151,SP:004865e6affad32403b7965493a53c8a7ffdda0a,"accelerated learning dynamics USED-FOR correlated and coarse correlated equilibria. normal - form games FEATURE-OF correlated and coarse correlated equilibria. sequential and simultaneous moves CONJUNCTION imperfect information. imperfect information CONJUNCTION sequential and simultaneous moves. imperfect information FEATURE-OF extensive - form games. sequential and simultaneous moves FEATURE-OF extensive - form games. no - regret learning dynamics USED-FOR extensive - form correlated equilibrium ( EFCE ). O(T 3/4)-approximate EFCE FEATURE-OF correlated distribution of play. structured Markov chain USED-FOR refined perturbation analysis. refined perturbation analysis USED-FOR stability of certain fixed point strategies. Task is learning in games. Method are accelerated dynamics, and framework of -regret. OtherScientificTerm is prior rate. ","This paper studies accelerated learning dynamics for correlated and coarse correlated equilibria in normal-form games. In particular, the authors consider the case of extensive-form correlated equilibrium (EFCE) where the prior rate is O(T 3/4)-approximate EFCE. The authors propose a new framework of-regret, which is based on a structured Markov chain, to study the stability of certain fixed point strategies. ","This paper studies accelerated learning dynamics for correlated and coarse correlated equilibria in normal-form games. In particular, the authors consider the case of extensive-form correlated equilibrium (EFCE) where the prior rate is O(T 3/4)-approximate EFCE. The authors propose a new framework of-regret, which is based on a structured Markov chain, to study the stability of certain fixed point strategies. "
5167,SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"discrete actions COMPARE continuous actions. continuous actions COMPARE discrete actions. maximum of the action - value function USED-FOR dynamic programmingbased methods. Action Quantization from Demonstrations ( AQuaDem ) USED-FOR discretization of continuous action spaces. method USED-FOR discretization of continuous action spaces. priors of demonstrations USED-FOR discretization of continuous action spaces. discrete action deep RL algorithm USED-FOR continuous control problem. RL CONJUNCTION RL. RL CONJUNCTION RL. demonstrations CONJUNCTION RL. RL CONJUNCTION demonstrations. play data USED-FOR RL. Imitation Learning EVALUATE-FOR method. demonstrations USED-FOR RL. setups EVALUATE-FOR method. Imitation Learning HYPONYM-OF setups. RL HYPONYM-OF setups. RL HYPONYM-OF setups. human data COMPARE synthetic data. synthetic data COMPARE human data. human data USED-FOR setups. AQuaDem COMPARE continuous control methods. continuous control methods COMPARE AQuaDem. hard manipulation tasks EVALUATE-FOR continuous control methods. hard manipulation tasks EVALUATE-FOR AQuaDem. sample efficiency EVALUATE-FOR continuous control methods. sample efficiency EVALUATE-FOR AQuaDem. Task are Reinforcement Learning ( RL ), exploration problems, and exploration problem. OtherScientificTerm is action space. ","This paper proposes a method for discretization of continuous action spaces from demonstrations. The method is based on discrete action deep RL (DADRL), which is a dynamic programming-based method that uses the maximum of the action-value function to learn a discretized continuous action space. The authors show that DADRL is able to achieve better sample efficiency than existing continuous control methods. They also show that AQuaDem can be used to improve the sample efficiency of continuous control algorithms. ","This paper proposes a method for discretization of continuous action spaces from demonstrations. The method is based on discrete action deep RL (DADRL), which is a dynamic programming-based method that uses the maximum of the action-value function to learn a discretized continuous action space. The authors show that DADRL is able to achieve better sample efficiency than existing continuous control methods. They also show that AQuaDem can be used to improve the sample efficiency of continuous control algorithms. "
5183,SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,domain generalization USED-FOR robust model. domain generalization USED-FOR semantic segmentation. labeled synthetic ( source ) data USED-FOR robust model. channelwise mean USED-FOR style features. adversarial style augmentation ( AdvStyle ) approach USED-FOR hard stylized images. style feature USED-FOR AdvStyle. adversarial training USED-FOR it. adversarial style feature USED-FOR adversarial image. adversarial image USED-FOR robust model training. AdvStyle USED-FOR models. synthetic - to - real semantic segmentation benchmarks EVALUATE-FOR AdvStyle. AdvStyle USED-FOR model. AdvStyle USED-FOR domain generalized image classification. datasets EVALUATE-FOR AdvStyle. OtherScientificTerm is image style variation. Task is overfitting. ,This paper proposes a new adversarial style augmentation (AdvStyle) approach for hard stylized images. AdvStyle augments the style feature of the adversarial image with the channelwise mean of the style features of the source image. The proposed method is evaluated on synthetic and real-world semantic segmentation tasks. The authors show that AdvStyle outperforms existing methods on both synthetic-to-real and domain-generalized image classification.,This paper proposes a new adversarial style augmentation (AdvStyle) approach for hard stylized images. AdvStyle augments the style feature of the adversarial image with the channelwise mean of the style features of the source image. The proposed method is evaluated on synthetic and real-world semantic segmentation tasks. The authors show that AdvStyle outperforms existing methods on both synthetic-to-real and domain-generalized image classification.
5199,SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"rigid, dramatic gestures USED-FOR recognition. rigid, dramatic gestures USED-FOR systems. neuromorphic gesture analysis system USED-FOR event - based gesture data. high temporal resolution FEATURE-OF neuromorphic gesture analysis system. high temporal resolution FEATURE-OF event - based gesture data. latent space representation USED-FOR similarity of mid - air gesture data. Dynamic Vision Sensor ( DVS ) USED-FOR event - based data. it USED-FOR sparse, noisy inputs. it USED-FOR interpretable latent space representation. interpretable latent space representation USED-FOR sparse, noisy inputs. DVSGesture dataset EVALUATE-FOR Hybrid GuidedVAE. classification accuracy EVALUATE-FOR Hybrid GuidedVAE. T - SNE plots USED-FOR interpretable latent space representation. neuromorphic hardware USED-FOR model. Method is mid - air gesture recognition systems. Generic are they, approach, and algorithm. ",This paper proposes a neuromorphic gesture recognition system based on the Dynamic Vision Sensor (DVS) to capture the similarity of mid-air gesture data. The proposed method is evaluated on the DVSGesture dataset and shows promising results. ,This paper proposes a neuromorphic gesture recognition system based on the Dynamic Vision Sensor (DVS) to capture the similarity of mid-air gesture data. The proposed method is evaluated on the DVSGesture dataset and shows promising results. 
5215,SP:2e66468a6b94177e54b0052b97713ee63902c278,"accuracy EVALUATE-FOR neuron - based networks. tabular data USED-FOR Deep learning. Internet of Things ( IoT ) CONJUNCTION drone. drone CONJUNCTION Internet of Things ( IoT ). drone CONJUNCTION Natural User Interface ( NUI ) application. Natural User Interface ( NUI ) application CONJUNCTION drone. annealing mechanism USED-FOR S - HTE inference. S - HTE USED-FOR internal representations. ferns USED-FOR S - HTE. classification and regression benchmark EVALUATE-FOR accuracy. OtherScientificTerm are computational capacity, deep learning capabilities, and neurons. Method are deep learning methods, and PyTorch implementation. Generic is it. Metric is computational complexity. ","This paper proposes a new method for training neuron-based deep learning models. The method is based on the idea of annealing, which is used to reduce the computational complexity of the network. The authors show that the proposed method is able to achieve state-of-the-art performance on classification and regression tasks. The paper also shows that the method is computationally efficient. ","This paper proposes a new method for training neuron-based deep learning models. The method is based on the idea of annealing, which is used to reduce the computational complexity of the network. The authors show that the proposed method is able to achieve state-of-the-art performance on classification and regression tasks. The paper also shows that the method is computationally efficient. "
5231,SP:b238db9252d83a13438bb747d70e635bb9945958,undirected stateonly experience USED-FOR learning value functions. tabular Q - learning USED-FOR value function. tabular Q - learning USED-FOR discrete Markov decision processes ( MDPs ). refinement of the action space FEATURE-OF value function. offline RL method USED-FOR value functions. Latent Action Q - learning HYPONYM-OF offline RL method. state - only experience USED-FOR value functions. Latent Action Q - learning ( LAQ ) USED-FOR value functions. discrete latent actions USED-FOR Q - learning. latent - variable future prediction model USED-FOR Q - learning. latent - variable future prediction model USED-FOR discrete latent actions. Q - learning USED-FOR Latent Action Q - learning ( LAQ ). Q - learning USED-FOR value functions. LAQ USED-FOR value functions. value functions CONJUNCTION value functions. value functions CONJUNCTION value functions. ground truth actions USED-FOR value functions. Value functions USED-FOR acquisition of goal - directed behavior. Value functions USED-FOR domain - specific low - level controllers. LAQ USED-FOR acquisition of goal - directed behavior. LAQ USED-FOR Value functions. imitation learning oracles CONJUNCTION competing methods. competing methods CONJUNCTION imitation learning oracles. alternatives CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION alternatives. 2D grid world CONJUNCTION 3D visual navigation. 3D visual navigation CONJUNCTION 2D grid world. LAQ COMPARE alternatives. alternatives COMPARE LAQ. environments EVALUATE-FOR LAQ. LAQ COMPARE competing methods. competing methods COMPARE LAQ. LAQ CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION LAQ. 3D visual navigation HYPONYM-OF environments. 2D grid world HYPONYM-OF environments. ,"This paper proposes Latent Action Q-learning (LAQ), a new offline RL method for discrete Markov decision processes (MDPs). The proposed method is based on tabular Q-Learning (Q-learning) and uses discrete latent actions to learn the value function of discrete MDPs. The authors show that the proposed method outperforms the state-of-the-art offline RL methods in a number of environments. ","This paper proposes Latent Action Q-learning (LAQ), a new offline RL method for discrete Markov decision processes (MDPs). The proposed method is based on tabular Q-Learning (Q-learning) and uses discrete latent actions to learn the value function of discrete MDPs. The authors show that the proposed method outperforms the state-of-the-art offline RL methods in a number of environments. "
5247,SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"large models USED-FOR deep learning applications. low - latency and high - bandwidth interconnect USED-FOR distributed training algorithms. distributed training algorithms USED-FOR models. GPU clusters USED-FOR models. model parallelism USED-FOR large models. SWARM Parallelism1 HYPONYM-OF model - parallel training algorithm. model - parallel training algorithm USED-FOR swarms of poorly connected, heterogeneous unreliable devices. SWARM USED-FOR temporary randomized pipelines. compression - aware architecture modifications USED-FOR approach. network throughput FEATURE-OF swarm of preemptible T4 GPUs. shared parameters USED-FOR large Transformer language model. swarm of preemptible T4 GPUs USED-FOR large Transformer language model. OtherScientificTerm are dedicated GPU clusters, distributed training setups, and preemptible ” instances. Generic is setups. ","This paper proposes SWARM, a model-parallel training algorithm for large models. The main idea of SWARM is to train a large model on a small number of preemptible GPUs. The authors show that SWARM can be used to train large models with low latency and high bandwidth interconnects. The proposed method is evaluated on a Transformer language model and is shown to outperform baselines.","This paper proposes SWARM, a model-parallel training algorithm for large models. The main idea of SWARM is to train a large model on a small number of preemptible GPUs. The authors show that SWARM can be used to train large models with low latency and high bandwidth interconnects. The proposed method is evaluated on a Transformer language model and is shown to outperform baselines."
5263,SP:91d2f094d5481651b554f58aecc2a6207057a47c,"Offline reinforcement learning USED-FOR policies. fixed dataset USED-FOR real - world applications. fixed dataset USED-FOR Offline reinforcement learning. fixed dataset USED-FOR policies. behavior policy COMPARE policy. policy COMPARE behavior policy. transition dynamics PART-OF offline experiences. offline training CONJUNCTION online tuning. online tuning CONJUNCTION offline training. online data USED-FOR agent policy. deployment efficiency CONJUNCTION sample efficiency. sample efficiency CONJUNCTION deployment efficiency. online transition correction ( OTC ) USED-FOR biased transition dynamics. offline and online experiences USED-FOR online transition correction ( OTC ). sampling probabilities USED-FOR online transition correction ( OTC ). distances USED-FOR similarity between transitions. transition similarity USED-FOR adaptive rank - based prioritization. embedding - based and valuebased distance HYPONYM-OF distances. OTC USED-FOR agent policies. OTC USED-FOR online tuning. agent policies USED-FOR online tuning. data efficiency EVALUATE-FOR OTC. OTC COMPARE baselines. baselines COMPARE OTC. tasks EVALUATE-FOR baselines. tasks EVALUATE-FOR OTC. Task is offline decentralized multi - agent reinforcement learning. OtherScientificTerm are online execution, value estimates, uncoordinated and suboptimal policies, and transition bias. Material is online experiences. ","This paper proposes an online transition correction (OTC) method for offline reinforcement learning. OTC is based on the idea that the transition dynamics between offline and online experiences can be biased. To address this issue, the authors propose to use an embedding-based and value-based distance to measure the similarity between transitions. The authors also propose an adaptive rank-based prioritization method for online tuning. Experiments show that OTC improves the sample efficiency and deployment efficiency.","This paper proposes an online transition correction (OTC) method for offline reinforcement learning. OTC is based on the idea that the transition dynamics between offline and online experiences can be biased. To address this issue, the authors propose to use an embedding-based and value-based distance to measure the similarity between transitions. The authors also propose an adaptive rank-based prioritization method for online tuning. Experiments show that OTC improves the sample efficiency and deployment efficiency."
5279,SP:d0e650d568214481b07a0452ec606ccbf6d05410,"computational footprint EVALUATE-FOR Deep Neural Networks ( DNNs ) training. 4 - bit quantization USED-FOR methods. computational footprint EVALUATE-FOR training process. loss gradients HYPONYM-OF neural gradients. unbiased quantization USED-FOR quantized neural network training. logarithmic unbiased quantization ( LUQ ) method USED-FOR forward and backward phase. high precision fine - tuning CONJUNCTION variance reduction method. variance reduction method CONJUNCTION high precision fine - tuning. ImageNet FEATURE-OF ResNet50. method USED-FOR low precision format. OtherScientificTerm are intermediate neural layers, multiplications, and multiplier. Generic is it. Method is 4 - bit training. ",This paper proposes a new quantization method for quantized neural networks. The proposed method is based on the logarithmic unbiased quantization (LUQ) method for forward and backward phase quantization. The authors show that the proposed method can be used to reduce the computational cost of quantized networks.,This paper proposes a new quantization method for quantized neural networks. The proposed method is based on the logarithmic unbiased quantization (LUQ) method for forward and backward phase quantization. The authors show that the proposed method can be used to reduce the computational cost of quantized networks.
5295,SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"Polythetic classifications COMPARE monothetic classifications. monothetic classifications COMPARE Polythetic classifications. features USED-FOR monothetic classifications. shared patterns of features USED-FOR Polythetic classifications. threshold meta - learners USED-FOR functions. embedding dimension USED-FOR functions. embedding dimension USED-FOR threshold meta - learners. Prototypical Networks HYPONYM-OF threshold meta - learners. attentional classifiers USED-FOR problems. linear embedding dimension USED-FOR attentional classifiers. Matching Networks HYPONYM-OF attentional classifiers. linear embedding dimension USED-FOR problems. attentional models USED-FOR misclassification. selfattention feature - selection mechanism USED-FOR non - discriminative features. approach USED-FOR meta - learning Boolean functions. Material is natural world. OtherScientificTerm are task - relevant features, and task - irrelevant features. Task is meta - learning problems. ",This paper studies the problem of meta-learning for Boolean functions. The authors propose a self-attention feature-selection mechanism to select features that are useful for the task-relevant and task-irrelevant features. The proposed method is based on Prototypical Networks (PNNs) and Matching Networks (MNNs). The authors show that the proposed method outperforms the baselines on a number of tasks. ,This paper studies the problem of meta-learning for Boolean functions. The authors propose a self-attention feature-selection mechanism to select features that are useful for the task-relevant and task-irrelevant features. The proposed method is based on Prototypical Networks (PNNs) and Matching Networks (MNNs). The authors show that the proposed method outperforms the baselines on a number of tasks. 
5311,SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"multi - agent reinforcement learning USED-FOR emergent communication. continuous acoustic channel USED-FOR Human communication. reinforcement learning USED-FOR continuous communication channel. continuous communication channel USED-FOR emergent language. channel characteristics USED-FOR emerging language. vocoder USED-FOR continuous waveform. noise FEATURE-OF communication channel. continuous signalling USED-FOR language learning. platform USED-FOR continuous signalling. deep reinforcement learning USED-FOR platform. OtherScientificTerm are discrete symbols, lossy continuous channel, continuous signal, and concept combinations. Method are environment and training methodology, and deep Q - learning. Material is messaging environment. ",This paper proposes a reinforcement learning framework for learning emergent language in a continuous communication setting. The authors propose a method for learning a communication channel that is composed of discrete symbols and a continuous signal. The proposed method is based on deep Q-learning. The method is evaluated on a number of real-world environments and shows that the proposed method outperforms the baselines. ,This paper proposes a reinforcement learning framework for learning emergent language in a continuous communication setting. The authors propose a method for learning a communication channel that is composed of discrete symbols and a continuous signal. The proposed method is based on deep Q-learning. The method is evaluated on a number of real-world environments and shows that the proposed method outperforms the baselines. 
5327,SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"backdoor attacks FEATURE-OF NLP models. NLP backdoor attacks USED-FOR tasks. NLP models CONJUNCTION tasks. tasks CONJUNCTION NLP models. attacks USED-FOR NLP models. attacks USED-FOR tasks. BadPre HYPONYM-OF task - agnostic backdoor attack. task - agnostic backdoor attack USED-FOR pre - trained NLP models. backdoor USED-FOR pre - trained model. backdoor USED-FOR downstream models. transfer learning process USED-FOR downstream models. approach USED-FOR downstream NLP tasks. Task is downstream language tasks. Method is language models. OtherScientificTerm are model misprediction, and prior information. Generic are attack, malicious model, and strategy. ","This paper proposes a new backdoor attack for NLP models. The proposed method is based on the BadPre backdoor attack, which is a task-agnostic backdoor attack that can be applied to any pre-trained NLP model. The authors show that the proposed method can be used to improve the performance of downstream NLP tasks. ","This paper proposes a new backdoor attack for NLP models. The proposed method is based on the BadPre backdoor attack, which is a task-agnostic backdoor attack that can be applied to any pre-trained NLP model. The authors show that the proposed method can be used to improve the performance of downstream NLP tasks. "
5343,SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"skill pre - training methods COMPARE RL techniques. RL techniques COMPARE skill pre - training methods. skill learning USED-FOR evolving or expanding environment. evolving environment USED-FOR skill discovery. framework USED-FOR skill discovery. incremental skills COMPARE skill discovery methods. skill discovery methods COMPARE incremental skills. evolving and static environments EVALUATE-FOR incremental skills. skill quality EVALUATE-FOR skill discovery methods. skill quality EVALUATE-FOR incremental skills. Task are Reward - free, unsupervised discovery of skills, hand - designing rewards, task supervision, and discovery - of - incremental - skill. OtherScientificTerm are stationary environments, agent dynamics, and learned skills. Generic are methods, and them. ","This paper proposes a framework for skill discovery in an evolving or expanding environment. The proposed framework is based on the idea of skill learning in the context of reward-free, unsupervised learning, where the agent is given a set of skills to learn and the goal is to learn a new skill. The authors show that the proposed method outperforms existing skill discovery methods in both evolving and static environments. They also show that their method is able to learn new skills faster than existing skill learning methods. ","This paper proposes a framework for skill discovery in an evolving or expanding environment. The proposed framework is based on the idea of skill learning in the context of reward-free, unsupervised learning, where the agent is given a set of skills to learn and the goal is to learn a new skill. The authors show that the proposed method outperforms existing skill discovery methods in both evolving and static environments. They also show that their method is able to learn new skills faster than existing skill learning methods. "
5359,SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"Convolutional neural networks USED-FOR features. regular quadrilateral convolution kernels USED-FOR features. regular quadrilateral convolution kernels USED-FOR Convolutional neural networks. small convolution kernels USED-FOR models. relative directions CONJUNCTION logarithmic distances. logarithmic distances CONJUNCTION relative directions. LPSC USED-FOR local spatial structures. LPSC USED-FOR single - layer receptive field. LPSC USED-FOR network architecture. convolutions PART-OF network architecture. convolution USED-FOR LPSC. log - polar space pooling USED-FOR convolution. log - polar space pooling USED-FOR LPSC. tasks EVALUATE-FOR LPSC. OtherScientificTerm are convolution kernel, small local receptive fields, and local receptive field. ","This paper proposes a new convolutional neural network architecture called LPSC, which is based on the idea of local receptive fields (LPSC). The authors show that the proposed architecture is able to capture the local spatial structure of the input space, and can be used to learn a single-layer receptive field. The authors also propose a new space pooling method to improve the performance of the proposed network. ","This paper proposes a new convolutional neural network architecture called LPSC, which is based on the idea of local receptive fields (LPSC). The authors show that the proposed architecture is able to capture the local spatial structure of the input space, and can be used to learn a single-layer receptive field. The authors also propose a new space pooling method to improve the performance of the proposed network. "
5375,SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"non - vacuous bounds USED-FOR NNs. PAC - Bayes theorem USED-FOR NNs generalization. IIW ’s property USED-FOR deep learning. algorithm USED-FOR approximation of IIW. information complexity EVALUATE-FOR NNs. accuracy CONJUNCTION information complexity. information complexity CONJUNCTION accuracy. accuracy EVALUATE-FOR NNs. PIB HYPONYM-OF NNs. IIW compression CONJUNCTION generalization. generalization CONJUNCTION IIW compression. IIW USED-FOR NNs. overparameterization CONJUNCTION noisy labels. noisy labels CONJUNCTION overparameterization. varying batch sizes CONJUNCTION overparameterization. overparameterization CONJUNCTION varying batch sizes. IIW USED-FOR NNs. MCMC - based algorithm USED-FOR optimal weight posterior. PIB FEATURE-OF optimal weight posterior. Task are ML research, and compressing phase transition. Method are IIW - based information bottleneck, and NNs ’ training. ","This paper studies the information bottleneck in the training of neural networks (NNs). In particular, the authors consider the problem of computing the posterior of a neural network (NN) that is trained in a phase transition (i.e., the phase transition between training and test). The authors show that the posterior can be approximated by the PAC-Bayes theorem (PAC-Bays theorem), which is a non-vacuous bound on the generalization ability of NNs. The authors then show that this is a generalization property of the posterior, which can be used to compute the optimal weight posterior of an NN. They also provide an algorithm for computing this posterior. ","This paper studies the information bottleneck in the training of neural networks (NNs). In particular, the authors consider the problem of computing the posterior of a neural network (NN) that is trained in a phase transition (i.e., the phase transition between training and test). The authors show that the posterior can be approximated by the PAC-Bayes theorem (PAC-Bays theorem), which is a non-vacuous bound on the generalization ability of NNs. The authors then show that this is a generalization property of the posterior, which can be used to compute the optimal weight posterior of an NN. They also provide an algorithm for computing this posterior. "
5391,SP:a733847ade77ffbf38760fc79da17893dea8d53f,"perturbations USED-FOR attacks. linear separable FEATURE-OF perturbations. linear separability USED-FOR attacks. synthetic perturbations COMPARE deliberately crafted attacks. deliberately crafted attacks COMPARE synthetic perturbations. linear separable data USED-FOR perturbations. imperceptible scale FEATURE-OF they. shortcuts USED-FOR deep models. Method are Indiscriminate data poisoning attacks, and pre - trained feature extractors. OtherScientificTerm are imperceptible perturbations, and normal features. Task is shortcut learning problem. ","This paper studies the problem of data poisoning attacks against deep neural networks. The authors propose a new class of adversarial perturbations that are imperceptible to the human eye. They show that these attacks are linear separable, which means that they can be applied to any class of data. They also show that this class of attacks can be used as a shortcut learning problem for deep models. ","This paper studies the problem of data poisoning attacks against deep neural networks. The authors propose a new class of adversarial perturbations that are imperceptible to the human eye. They show that these attacks are linear separable, which means that they can be applied to any class of data. They also show that this class of attacks can be used as a shortcut learning problem for deep models. "
5407,SP:7b50be406138ad01db3ee112899f622637896fe9,Offline policy optimization USED-FOR real - world decisionmaking problems. estimator USED-FOR offline policy evaluation. function approximations USED-FOR value functions. Importance sampling HYPONYM-OF estimator. value functions CONJUNCTION process models. process models CONJUNCTION value functions. function approximations USED-FOR process models. Importance sampling USED-FOR offline policy evaluation. algorithm USED-FOR overfitting. overfitting phenomenon FEATURE-OF importance weighted return. per - state - neighborhood normalization condition USED-FOR algorithm. healthcare - inspired simulator CONJUNCTION logged dataset. logged dataset CONJUNCTION healthcare - inspired simulator. healthcare - inspired simulator EVALUATE-FOR method. logged dataset EVALUATE-FOR method. method COMPARE batch reinforcement learning algorithms. batch reinforcement learning algorithms COMPARE method. overfitting EVALUATE-FOR method. Task is online learning. Generic is approach. ,This paper proposes a new method for offline policy evaluation. The main idea is to use the importance weighted return as an estimator for value function approximations of value functions and process models. The paper also proposes a per-state-neighborhood normalization condition to reduce the overfitting phenomenon. The proposed method is evaluated on a healthcare-inspired simulator and a logged dataset. ,This paper proposes a new method for offline policy evaluation. The main idea is to use the importance weighted return as an estimator for value function approximations of value functions and process models. The paper also proposes a per-state-neighborhood normalization condition to reduce the overfitting phenomenon. The proposed method is evaluated on a healthcare-inspired simulator and a logged dataset. 
5423,SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,model USED-FOR continual learning. CoLLIE HYPONYM-OF model. CoLLIE USED-FOR continual learning. transformation function USED-FOR language embeddings. language CONJUNCTION images. images CONJUNCTION language. transformation function USED-FOR new language use. transformation function USED-FOR CoLLIE. semantic space FEATURE-OF images. few - shot learning COMPARE model. model COMPARE few - shot learning. it COMPARE model. model COMPARE it. zero - shot EVALUATE-FOR model. continual learning EVALUATE-FOR model. Material is vision. Method is multimodal embedding model. OtherScientificTerm is similar language use. ,"This paper proposes a multi-modal embedding model for continual learning. The proposed model is based on the CoLLIE model, which learns a transformation function for language embeddings and an image embedding function for images. The authors show that the proposed model outperforms the state-of-the-art on zero-shot learning and continual learning tasks. ","This paper proposes a multi-modal embedding model for continual learning. The proposed model is based on the CoLLIE model, which learns a transformation function for language embeddings and an image embedding function for images. The authors show that the proposed model outperforms the state-of-the-art on zero-shot learning and continual learning tasks. "
5439,SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"captioning models USED-FOR visual data. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. Fidelity CONJUNCTION Fluency. Fluency CONJUNCTION Fidelity. Visual - Linguistic Adequacy CONJUNCTION Fidelity. Fidelity CONJUNCTION Visual - Linguistic Adequacy. VLAF2 USED-FOR Visual - Linguistic Adequacy. VLAF2 USED-FOR Fidelity. VLAF2 USED-FOR Fluency. linguistics USED-FOR Fluency. linguistics USED-FOR VLAF2. BERT CONJUNCTION CLIP. CLIP CONJUNCTION BERT. intrinsic language knowledge USED-FOR models. nocaps dataset EVALUATE-FOR framework. method COMPARE captioning models. captioning models COMPARE method. method COMPARE SPICE scores of human baseline. SPICE scores of human baseline COMPARE method. caption evaluation metrics EVALUATE-FOR captioning models. caption evaluation metrics EVALUATE-FOR method. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. model USED-FOR object captions. quantitative and qualitative analysis EVALUATE-FOR model. fluency EVALUATE-FOR model. adequacy EVALUATE-FOR model. fidelity EVALUATE-FOR model. Method are object captioning ( NOC ), image captioning models, object captioning models, and visual / language models. OtherScientificTerm is caption annotations. ","This paper proposes a new method for evaluating the fidelity and fluency of object captioning models. The method is based on the idea of Visual-Linguistic Adequacy (VLAF2) and Fidelity-Fidelity (Fidelity+Fidelity), which is an extension of the VLAF2 framework. The authors show that the proposed method is able to achieve higher fidelity and higher fluency than existing methods on the nocaps dataset. ","This paper proposes a new method for evaluating the fidelity and fluency of object captioning models. The method is based on the idea of Visual-Linguistic Adequacy (VLAF2) and Fidelity-Fidelity (Fidelity+Fidelity), which is an extension of the VLAF2 framework. The authors show that the proposed method is able to achieve higher fidelity and higher fluency than existing methods on the nocaps dataset. "
5455,SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"foundation models USED-FOR representations. representations USED-FOR classification. special - purpose algorithms USED-FOR problems. classifier USED-FOR representations. special - purpose algorithms USED-FOR representations. clustering property FEATURE-OF features. neural collapse HYPONYM-OF clustering property. overparameterized classification networks USED-FOR features. foundation models USED-FOR feature maps. foundation models USED-FOR transfer learning. feature maps USED-FOR transfer learning. Task are few - shot learning problems, and few - shot setting. ",This paper studies the clustering property of few-shot learning problems. The authors show that the neural collapse property of the feature maps of a classifier can be used to explain the transfer learning performance of the classifier. They also show that this property can be exploited for transfer learning. The paper also provides a theoretical analysis of this property. ,This paper studies the clustering property of few-shot learning problems. The authors show that the neural collapse property of the feature maps of a classifier can be used to explain the transfer learning performance of the classifier. They also show that this property can be exploited for transfer learning. The paper also provides a theoretical analysis of this property. 
5471,SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"3D scanning USED-FOR Point cloud. tasks USED-FOR point cloud reconstruction. 3D sparse stacked - hourglass network USED-FOR densification and denoising. 3D sparse stacked - hourglass network HYPONYM-OF stages. refinement HYPONYM-OF stages. stages PART-OF deep point cloud reconstruction network. 3D sparse stacked - hourglass network PART-OF deep point cloud reconstruction network. transformers USED-FOR refinement. refinement PART-OF deep point cloud reconstruction network. amplified positional encoding HYPONYM-OF module. module USED-FOR transformer. points ’ distances USED-FOR adaptive refinements. module USED-FOR positional encoding vectors. points ’ distances USED-FOR module. points ’ distances USED-FOR positional encoding vectors. ScanNet CONJUNCTION ICL - NUIM. ICL - NUIM CONJUNCTION ScanNet. ICL - NUIM CONJUNCTION ShapeNetPart datasets. ShapeNetPart datasets CONJUNCTION ICL - NUIM. ICL - NUIM EVALUATE-FOR network. ShapeNetPart datasets EVALUATE-FOR network. ScanNet EVALUATE-FOR network. network USED-FOR real - world and unmet scenes. OtherScientificTerm are discrete voxels, and 3D points. ","This paper proposes a method for 3D point cloud reconstruction based on a sparse stacked-hourglass network. The method consists of three stages: (1) denoising, (2) densification and (3) refinement. The densification stage is based on the 3D sparse stacked hourglass network, and the refinement stage uses a transformer to refine the point cloud. The proposed method is evaluated on three different datasets, including ScanNet, ICL-NUIM, and ShapeNet.","This paper proposes a method for 3D point cloud reconstruction based on a sparse stacked-hourglass network. The method consists of three stages: (1) denoising, (2) densification and (3) refinement. The densification stage is based on the 3D sparse stacked hourglass network, and the refinement stage uses a transformer to refine the point cloud. The proposed method is evaluated on three different datasets, including ScanNet, ICL-NUIM, and ShapeNet."
5487,SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"communicating node features CONJUNCTION feature gradients. feature gradients CONJUNCTION communicating node features. training efficiency CONJUNCTION model scalability. model scalability CONJUNCTION training efficiency. stale features CONJUNCTION stale feature gradients. stale feature gradients CONJUNCTION stale features. convergence rate EVALUATE-FOR GCN training. stale features USED-FOR GCN training. stale feature gradients USED-FOR GCN training. convergence rate EVALUATE-FOR PipeGCN. PipeGCN COMPARE vanilla distributed GCN training. vanilla distributed GCN training COMPARE PipeGCN. convergence rate EVALUATE-FOR vanilla distributed GCN training. smoothing method USED-FOR PipeGCN. PipeGCN COMPARE full - graph training methods. full - graph training methods COMPARE PipeGCN. accuracy EVALUATE-FOR full - graph training methods. training throughput EVALUATE-FOR PipeGCN. accuracy EVALUATE-FOR PipeGCN. Method are Graph Convolutional Networks ( GCNs ), large - scale GCNs, and distributed GCN training. Material is graph - structured data. OtherScientificTerm are partitioned subgraph, GCN layer, communication overhead, intra - partition computation, convergence, and staleness. Metric is theoretical convergence guarantee. ","This paper studies the convergence of GCN training with stale features and stale feature gradients. The authors propose a new method, called PipeGCN, to address the issue of stale features in GCN. They show that the convergence rate of the proposed method is better than that of vanilla distributed GCN, and also show that their method can be used for full-graph training. They also provide theoretical convergence guarantees for their method. ","This paper studies the convergence of GCN training with stale features and stale feature gradients. The authors propose a new method, called PipeGCN, to address the issue of stale features in GCN. They show that the convergence rate of the proposed method is better than that of vanilla distributed GCN, and also show that their method can be used for full-graph training. They also provide theoretical convergence guarantees for their method. "
5503,SP:8302d49558ee0f16392d623d4e604e92db10d041,"robustness USED-FOR applications. in - distribution test points EVALUATE-FOR deep neural networks. accuracy EVALUATE-FOR deep neural networks. methods USED-FOR test time adaptation. ResNet-50 models CONJUNCTION robust vision transformer model. robust vision transformer model CONJUNCTION ResNet-50 models. approach COMPARE prior augmentation and adaptation strategies. prior augmentation and adaptation strategies COMPARE approach. approach COMPARE model evaluation. model evaluation COMPARE approach. baseline ResNet models CONJUNCTION ResNet-50 models. ResNet-50 models CONJUNCTION baseline ResNet models. ImageNet - C CONJUNCTION ImageNet - R. ImageNet - R CONJUNCTION ImageNet - C. ResNet-50 models CONJUNCTION ImageNet - A distribution shift benchmarks. ImageNet - A distribution shift benchmarks CONJUNCTION ResNet-50 models. OtherScientificTerm are distribution shift, model training process, data augmentations, and model parameters. Task is test time robustification. Metric is model robustness. Generic are assumptions, model, and augmentations. ","This paper studies the problem of test-time robustification of deep neural networks. The authors propose a new method to evaluate the robustness of a deep neural network on in-distribution test points. The proposed method is based on the idea that the test time robustness is a function of the model training process and the data augmentations. The method is evaluated on ImageNet-C, ImageNet R, and ResNet-50 models. The results show that the proposed method outperforms the baselines in terms of robustness. ","This paper studies the problem of test-time robustification of deep neural networks. The authors propose a new method to evaluate the robustness of a deep neural network on in-distribution test points. The proposed method is based on the idea that the test time robustness is a function of the model training process and the data augmentations. The method is evaluated on ImageNet-C, ImageNet R, and ResNet-50 models. The results show that the proposed method outperforms the baselines in terms of robustness. "
5519,SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"RL CONJUNCTION planning. planning CONJUNCTION RL. model USED-FOR planning. model USED-FOR RL. accuracy EVALUATE-FOR they. model CONJUNCTION policy. policy CONJUNCTION model. single objective USED-FOR policy. single objective USED-FOR model. expected return FEATURE-OF lower bound. joint optimization USED-FOR objective mismatch. global lower bound FEATURE-OF expected return. global lower bound FEATURE-OF objective. algorithm ( MnM ) COMPARE GAN. GAN COMPARE algorithm ( MnM ). classifier USED-FOR real and fake transitions. Generic are template, models, it, bound, and algorithm. Metric is MSE. Task is control. Method is RL agent. OtherScientificTerm is policies. ","This paper studies the problem of estimating the global lower bound of the expected return of a model and a policy in a multi-agent reinforcement learning (MSE) setting. In particular, the authors consider the case where the model and the policy are jointly trained, and the goal is to minimize the global upper bound. The authors propose a new algorithm, called MnM, which is based on the joint optimization of the model-and-policy objective and the objective of the policy. They show that the proposed algorithm is able to achieve better global lower bounds than the standard GAN algorithm.","This paper studies the problem of estimating the global lower bound of the expected return of a model and a policy in a multi-agent reinforcement learning (MSE) setting. In particular, the authors consider the case where the model and the policy are jointly trained, and the goal is to minimize the global upper bound. The authors propose a new algorithm, called MnM, which is based on the joint optimization of the model-and-policy objective and the objective of the policy. They show that the proposed algorithm is able to achieve better global lower bounds than the standard GAN algorithm."
5535,SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"single observations CONJUNCTION observation histories. observation histories CONJUNCTION single observations. single observations USED-FOR behavioral cloning policies. observation histories USED-FOR behavioral cloning policies. human decision making USED-FOR model combination approach. instantaneous observation USED-FOR coarse action. images CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION images. this COMPARE baselines. baselines COMPARE this. CARLA autonomous driving CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION CARLA autonomous driving. images USED-FOR CARLA autonomous driving. MuJoCo continuous control tasks EVALUATE-FOR this. CARLA autonomous driving EVALUATE-FOR this. MuJoCo continuous control tasks EVALUATE-FOR baselines. CARLA autonomous driving EVALUATE-FOR baselines. Attention maps of baseline imitation methods CONJUNCTION method. method CONJUNCTION Attention maps of baseline imitation methods. CARLA driving task EVALUATE-FOR method. single observation ( BC - SO ) USED-FOR Behavioral cloning. observation history ( BC - OH ) USED-FOR Behavioral cloning. method USED-FOR coarse - to - fine ” imitator. Method are control policy, and imitation - learned policies. Generic are approach, them, and it. OtherScientificTerm are instantaneous observations, historical information, and visual cues. ",This paper proposes a method for behavioral cloning based on the observation history (BC-SO) and behavioral cloning policies. BC-SO is a model-based approach to learn a coarse-to-fine imitation-learned policy from a single observation. Behavioral cloning policies are learned by learning a policy that takes into account the history of the previous actions taken by the imitator. The method is evaluated on MuJoCo continuous control tasks and CARLA autonomous driving.,This paper proposes a method for behavioral cloning based on the observation history (BC-SO) and behavioral cloning policies. BC-SO is a model-based approach to learn a coarse-to-fine imitation-learned policy from a single observation. Behavioral cloning policies are learned by learning a policy that takes into account the history of the previous actions taken by the imitator. The method is evaluated on MuJoCo continuous control tasks and CARLA autonomous driving.
5551,SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,deep learning models USED-FOR dynamics forecasting. generalization EVALUATE-FOR deep learning models. external forces CONJUNCTION boundary conditions. boundary conditions CONJUNCTION external forces. external forces FEATURE-OF systems. them USED-FOR tasks. DyAd HYPONYM-OF model - based meta - learning method. forecaster USED-FOR shared dynamics. encoder USED-FOR time - invariant hidden features. encoder USED-FOR task. parts PART-OF DyAd. encoder HYPONYM-OF parts. forecaster HYPONYM-OF parts. weak supervision USED-FOR encoder. forecaster PART-OF DyAd. encoder PART-OF DyAd. adaptive instance normalization CONJUNCTION adaptive padding. adaptive padding CONJUNCTION adaptive instance normalization. encoder USED-FOR forecaster. forecaster USED-FOR inference. adaptive padding USED-FOR encoder. adaptive instance normalization USED-FOR encoder. adaptive instance normalization USED-FOR forecaster. generalization error EVALUATE-FOR procedure. model COMPARE approaches. approaches COMPARE model. Generic is They. ,"This paper proposes a meta-learning method for dynamics forecasting. The proposed method is based on the idea of meta-training a model-based meta-learner to predict the dynamics of a system. The model is trained on a set of tasks, where the encoder and the forecaster are trained separately. The encoder is used to learn a shared latent representation of the system dynamics, while the forecasters are used to infer the dynamics from the latent representation. The authors show that the proposed method outperforms the baselines in terms of generalization error. ","This paper proposes a meta-learning method for dynamics forecasting. The proposed method is based on the idea of meta-training a model-based meta-learner to predict the dynamics of a system. The model is trained on a set of tasks, where the encoder and the forecaster are trained separately. The encoder is used to learn a shared latent representation of the system dynamics, while the forecasters are used to infer the dynamics from the latent representation. The authors show that the proposed method outperforms the baselines in terms of generalization error. "
5567,SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"Monocular 3D object detection HYPONYM-OF 3D scene understanding. manually annotated 3D box labels USED-FOR LiDAR point clouds. manually annotated 3D box labels USED-FOR monocular 3D detection methods. 2D boxes PART-OF image. 2D boxes USED-FOR RoI LiDAR points. network USED-FOR 3D boxes. 3D box estimates CONJUNCTION RoI LiDAR points. RoI LiDAR points CONJUNCTION 3D box estimates. 3D alignment loss FEATURE-OF 3D box estimates. 3D alignment loss USED-FOR network. method COMPARE fully supervised methods. fully supervised methods COMPARE method. KITTI EVALUATE-FOR method. OtherScientificTerm are ill - posed nature of monocular imagery, 3D box labels, weak supervision, and 3D box label. Task are annotation process, weakly supervised monocular 3D detection, and learning problem. ","This paper proposes a method for weakly supervised monocular 3D object detection. The proposed method is based on the observation that the 3D box labels are not well-supervised, which is a limitation of existing methods. To address this issue, the authors propose a method that uses a network to learn a 3D label for each object in the scene. The method is evaluated on the KITTI dataset and shows that the proposed method outperforms existing methods in terms of accuracy. ","This paper proposes a method for weakly supervised monocular 3D object detection. The proposed method is based on the observation that the 3D box labels are not well-supervised, which is a limitation of existing methods. To address this issue, the authors propose a method that uses a network to learn a 3D label for each object in the scene. The method is evaluated on the KITTI dataset and shows that the proposed method outperforms existing methods in terms of accuracy. "
5583,SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"models USED-FOR natural language processing. rigid subword tokenization algorithms USED-FOR models. model inductive bias USED-FOR subword tokenization. characters USED-FOR latent subword representations. block scoring network USED-FOR GBST. CHARFORMER HYPONYM-OF deep Transformer model. GBST PART-OF deep Transformer model. CHARFORMER COMPARE subword - based models. subword - based models COMPARE CHARFORMER. CHARFORMER COMPARE byte - level baselines. byte - level baselines COMPARE CHARFORMER. multilingual, and noisy text datasets EVALUATE-FOR CHARFORMER. English GLUE CONJUNCTION multilingual, and noisy text datasets. multilingual, and noisy text datasets CONJUNCTION English GLUE. English GLUE EVALUATE-FOR CHARFORMER. byte - level baselines COMPARE subword - based models. subword - based models COMPARE byte - level baselines. Generic is model. OtherScientificTerm is byte level. Metric is competitive quality. ",This paper proposes a deep Transformer-based model for the task of subword tokenization. The proposed model is based on the block scoring network (GBST) framework. The authors show that the proposed model outperforms existing subword-based models in terms of competitive quality and efficiency. The main contribution of the paper is the use of block scoring to improve the performance of the model. ,This paper proposes a deep Transformer-based model for the task of subword tokenization. The proposed model is based on the block scoring network (GBST) framework. The authors show that the proposed model outperforms existing subword-based models in terms of competitive quality and efficiency. The main contribution of the paper is the use of block scoring to improve the performance of the model. 
5599,SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"Deep neural networks ( DNNs ) USED-FOR backdoor attacks. backdoor PART-OF DNNs. backdoor trigger USED-FOR DNNs. on - device deployed DNNs HYPONYM-OF real - world applications. adversarial objective USED-FOR backdoor detection. optimization perspective USED-FOR problem. adversarial objective USED-FOR solution. singularity FEATURE-OF adversarial map. adversarial map FEATURE-OF backdoorinfected example. skewed distribution FEATURE-OF solution. adversarial extreme value analysis ( AEVA ) USED-FOR backdoors. backdoors PART-OF black - box neural networks. extreme value analysis of the adversarial map USED-FOR AEVA. monte - carlo gradient estimation USED-FOR extreme value analysis of the adversarial map. approach USED-FOR backdoor attacks. backdoor attacks EVALUATE-FOR approach. black - box hard - label scenarios FEATURE-OF detecting backdoor attacks. Method are backdoor detection methods, and DNN. Material is poisoned training data. OtherScientificTerm are predictive confidence, and adversarial singularity phenomenon. ","This paper studies the problem of detecting backdoor attacks on black-box neural networks. The authors propose a novel adversarial objective for backdoor detection based on adversarial extreme value analysis (AEVA). AEVA is based on the observation that the adversarial singularity phenomenon is observed in adversarial maps, which is a result of the skewed distribution of the data distribution. To mitigate this phenomenon, the authors propose to use monte-carlo gradient estimation to estimate the extreme value of an adversarial map. Experiments show that AEVA can detect backdoor attacks in both black box and on-device settings.","This paper studies the problem of detecting backdoor attacks on black-box neural networks. The authors propose a novel adversarial objective for backdoor detection based on adversarial extreme value analysis (AEVA). AEVA is based on the observation that the adversarial singularity phenomenon is observed in adversarial maps, which is a result of the skewed distribution of the data distribution. To mitigate this phenomenon, the authors propose to use monte-carlo gradient estimation to estimate the extreme value of an adversarial map. Experiments show that AEVA can detect backdoor attacks in both black box and on-device settings."
5615,SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"uncertainty estimation USED-FOR classifier. KLoS HYPONYM-OF Kullback – Leibler divergence criterion. class - probability simplex FEATURE-OF Kullback – Leibler divergence criterion. evidential models USED-FOR secondorder uncertainty representation. distributional information USED-FOR KLoS. KLoS USED-FOR class confusion. class - wise divergence measure EVALUATE-FOR KLoS. in - distribution samples USED-FOR class - wise divergence measure. auxiliary neural network USED-FOR refined criterion. KLoSNet USED-FOR refined criterion. KLoSNet HYPONYM-OF auxiliary neural network. misclassifications CONJUNCTION OOD samples. OOD samples CONJUNCTION misclassifications. KLoSNet USED-FOR OOD samples. KLoSNet USED-FOR misclassifications. measures COMPARE KLoS. KLoS COMPARE measures. Material are OOD training data, OOD data, and OOD dataset. Metric is second - order uncertainty measures. OtherScientificTerm is evidential training objective. ",This paper proposes a new class-wise divergence criterion based on the Kullback-Leibler divergence criterion (KLoS). The proposed KLoS is an extension of the class-probability simplex (CLS) which is used to estimate the second-order uncertainty of a classifier. The authors show that the proposed KL-LoS can be used to improve the performance of classifiers on OOD data and OOD samples. The proposed method is evaluated on a variety of OOD and in-distribution datasets.,This paper proposes a new class-wise divergence criterion based on the Kullback-Leibler divergence criterion (KLoS). The proposed KLoS is an extension of the class-probability simplex (CLS) which is used to estimate the second-order uncertainty of a classifier. The authors show that the proposed KL-LoS can be used to improve the performance of classifiers on OOD data and OOD samples. The proposed method is evaluated on a variety of OOD and in-distribution datasets.
5631,SP:8b4f3916dca4e627931558e14836749bd4a6792f,natural image data USED-FOR CNNs. semi - supervised algorithm USED-FOR linear classifier. datadependent features USED-FOR linear classifier. unlabeled data USED-FOR datadependent features. algorithm USED-FOR CNNs. natural distributional assumptions USED-FOR algorithm. it USED-FOR CNNs. low - dimensional structure FEATURE-OF distribution of patches. low - dimensional manifold USED-FOR patches. dimension of the patch distribution USED-FOR algorithm. Method is Convolutional networks ( CNN ). OtherScientificTerm is lower bound. ,This paper proposes a semi-supervised algorithm for learning a linear classifier from unlabeled data. The main contribution of the paper is to prove a lower bound on the dimension of the patch distribution of the distribution of patches in a low-dimensional manifold. The lower bound is based on the natural distributional assumptions of natural distributions of patches. The authors show that this lower bound can be used to derive an algorithm to learn a classifier that can be trained on unlabelled data. ,This paper proposes a semi-supervised algorithm for learning a linear classifier from unlabeled data. The main contribution of the paper is to prove a lower bound on the dimension of the patch distribution of the distribution of patches in a low-dimensional manifold. The lower bound is based on the natural distributional assumptions of natural distributions of patches. The authors show that this lower bound can be used to derive an algorithm to learn a classifier that can be trained on unlabelled data. 
5647,SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"face images USED-FOR Face clustering. representation capacity FEATURE-OF Graph Convolutional Networks ( GCN ). GCN - based methods USED-FOR face graphs. feature space FEATURE-OF kNN relations. kNN relations USED-FOR GCN - based methods. clean graphs USED-FOR GCNs. clean graphs USED-FOR algorithm. Ada - NETS HYPONYM-OF algorithm. face features USED-FOR robust features. adaptive neighbour discovery strategy USED-FOR edges. It USED-FOR graph. It USED-FOR noise edges. graph USED-FOR GCNs. clean yet rich edges FEATURE-OF graph. public clustering datasets EVALUATE-FOR Ada - NETS. Ada - NETS COMPARE state - of - the - art methods. state - of - the - art methods COMPARE Ada - NETS. public clustering datasets EVALUATE-FOR state - of - the - art methods. generalization EVALUATE-FOR Ada - NETS. OtherScientificTerm are structure space, and face image. ","This paper proposes Ada-NetS, a new algorithm for face clustering based on graph convolutional networks (GCN). The authors propose a novel adaptive neighbour discovery strategy, Ada-NETS, which is based on the idea of learning a graph with clean yet rich edges. The proposed method is evaluated on a number of public clustering datasets and shows promising results. ","This paper proposes Ada-NetS, a new algorithm for face clustering based on graph convolutional networks (GCN). The authors propose a novel adaptive neighbour discovery strategy, Ada-NETS, which is based on the idea of learning a graph with clean yet rich edges. The proposed method is evaluated on a number of public clustering datasets and shows promising results. "
5663,SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"crossdomain representations USED-FOR direct cross - data evaluation. domain information CONJUNCTION camera IDs. camera IDs CONJUNCTION domain information. It USED-FOR features. demographics information USED-FOR features. demographics information USED-FOR It. domain information HYPONYM-OF demographics information. camera IDs HYPONYM-OF demographics information. distributionally robust optimization ( DRO ) USED-FOR learning robust models. uncertainty set HYPONYM-OF data distributions. convex condition FEATURE-OF KL DRO. convex condition FEATURE-OF overparameterized neural networks. real scenarios FEATURE-OF distribution shifts. change - of - measure technique USED-FOR approach. Unit DRO HYPONYM-OF approach. reweighted dataset USED-FOR Unit DRO. large - scale DG ReID CONJUNCTION cross - domain ReID benchmarks. cross - domain ReID benchmarks CONJUNCTION large - scale DG ReID. Unit DRO COMPARE baselines. baselines COMPARE Unit DRO. cross - domain ReID benchmarks EVALUATE-FOR baselines. large - scale DG ReID EVALUATE-FOR baselines. cross - domain ReID benchmarks EVALUATE-FOR Unit DRO. large - scale DG ReID EVALUATE-FOR Unit DRO. OtherScientificTerm are protected demographic features, and demographics. Method are robust models, and models. ","This paper proposes a new method for cross-domain representation learning based on distributionally robust optimization (DRO). The proposed method is based on the idea that the uncertainty set of a data distribution is a convex function of the data distribution. The authors show that under certain conditions, the proposed method can be used to learn robust models that are robust to distribution shifts. They also show that their method outperforms existing methods on several benchmark datasets. ","This paper proposes a new method for cross-domain representation learning based on distributionally robust optimization (DRO). The proposed method is based on the idea that the uncertainty set of a data distribution is a convex function of the data distribution. The authors show that under certain conditions, the proposed method can be used to learn robust models that are robust to distribution shifts. They also show that their method outperforms existing methods on several benchmark datasets. "
5679,SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"Graph Neural Networks ( GNNs ) USED-FOR molecular property prediction. noise correction loss USED-FOR oversmoothing. noise USED-FOR overfitting. generic architectures USED-FOR quantum chemistry. methods USED-FOR regulariser. Noisy Nodes CONJUNCTION non - spatial architectures. non - spatial architectures CONJUNCTION Noisy Nodes. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR non - spatial architectures. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR Noisy Nodes. GNN toolkit USED-FOR 3D molecular property prediction. Method is GNNs. OtherScientificTerm are noise correcting node - level loss, and node latents. ","This paper studies the problem of noise correction in graph neural networks (GNNs) for 3D molecular property prediction. The authors propose a novel noise correction loss for GNNs that can be used to mitigate the oversmoothing problem. The proposed method is based on the idea of noise correcting node-level loss, which can be applied to any node latents. The method is evaluated on Open Graph Benchmark (OGB) datasets and shows promising results.","This paper studies the problem of noise correction in graph neural networks (GNNs) for 3D molecular property prediction. The authors propose a novel noise correction loss for GNNs that can be used to mitigate the oversmoothing problem. The proposed method is based on the idea of noise correcting node-level loss, which can be applied to any node latents. The method is evaluated on Open Graph Benchmark (OGB) datasets and shows promising results."
5695,SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"approaches USED-FOR complex interaction between set elements. ( Set)Transformers HYPONYM-OF approaches. self attention USED-FOR approaches. ( Set)Transformers HYPONYM-OF self attention. inducing - point attention CONJUNCTION optimal transport kernel embedding ( OTKE ). optimal transport kernel embedding ( OTKE ) CONJUNCTION inducing - point attention. mixture distribution FEATURE-OF i.i.d. samples. marginal likelihood maximization CONJUNCTION empirical Bayes. empirical Bayes CONJUNCTION marginal likelihood maximization. balanced assignment constraints FEATURE-OF E - step. OTKE HYPONYM-OF framework. balanced assignment constraints FEATURE-OF single - step EM. single - step EM HYPONYM-OF framework. set embedding CONJUNCTION prior - induced model regularization. prior - induced model regularization CONJUNCTION set embedding. OTKE COMPARE approach. approach COMPARE OTKE. approach USED-FOR set embedding. approach USED-FOR prior - induced model regularization. tasks EVALUATE-FOR approach. Task is set2vec problem. Method are vector representation, set embedding feed - forward network, ExpectationMaximization ( EM ) steps, MAP - EM steps, auto - diff backpropagation, and mixture set data fitting framework. OtherScientificTerm are variable number of feature vectors, mixture, and mixture parameters. Metric are computational overhead, and reduced computational cost. ",This paper proposes a new approach for learning set embeddings for set2vec problems. The proposed approach is based on a mixture set data fitting framework. The authors show that the proposed approach can achieve better performance than existing methods on a variety of tasks. The main contribution of the paper is the use of a set embedding feed-forward network to learn the set representations. ,This paper proposes a new approach for learning set embeddings for set2vec problems. The proposed approach is based on a mixture set data fitting framework. The authors show that the proposed approach can achieve better performance than existing methods on a variety of tasks. The main contribution of the paper is the use of a set embedding feed-forward network to learn the set representations. 
5711,SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,unsupervised feature selection USED-FOR informative features. informative features USED-FOR unknown downstream tasks. unsupervised feature selection USED-FOR unknown downstream tasks. CA setting USED-FOR feature selection. machine learning community USED-FOR feature selection. method USED-FOR feature selection. feature selection USED-FOR CA setting. semi - synthetic dataset CONJUNCTION real - world biomedical datasets. real - world biomedical datasets CONJUNCTION semi - synthetic dataset. state - of - the - art methods USED-FOR unsupervised feature selection scenarios. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. semi - synthetic dataset EVALUATE-FOR method. real - world biomedical datasets EVALUATE-FOR method. Task is contrastive analysis ( CA ) setting. Generic is background dataset. OtherScientificTerm is genes. Material is genomic data. ,"This paper proposes a method for unsupervised feature selection in the contrastive analysis (CA) setting, where the goal is to find informative features that are useful for downstream tasks. The proposed method is based on the idea of feature selection, which is an important problem in the machine learning community. The authors propose a novel method for feature selection for the CA setting. The method is evaluated on a synthetic dataset and a semi-synthetic dataset, and compared to state-of-the-art methods.","This paper proposes a method for unsupervised feature selection in the contrastive analysis (CA) setting, where the goal is to find informative features that are useful for downstream tasks. The proposed method is based on the idea of feature selection, which is an important problem in the machine learning community. The authors propose a novel method for feature selection for the CA setting. The method is evaluated on a synthetic dataset and a semi-synthetic dataset, and compared to state-of-the-art methods."
5727,SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"Early stopping USED-FOR over - training neural networks. optimal early stopping time CONJUNCTION model dimension. model dimension CONJUNCTION optimal early stopping time. optimal early stopping USED-FOR double descent ”. early stopping USED-FOR generalization. Method are linear regression models, linear models, and deep neural network. Task is deep learning tasks. Generic is model. OtherScientificTerm is features. ",This paper studies the problem of early stopping in deep learning. The authors show that the optimal early stopping time for linear regression models is a function of the model dimension and the number of features. They also show that early stopping can be used to improve the generalization performance of deep neural networks. ,This paper studies the problem of early stopping in deep learning. The authors show that the optimal early stopping time for linear regression models is a function of the model dimension and the number of features. They also show that early stopping can be used to improve the generalization performance of deep neural networks. 
5743,SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"Policy gradient algorithms USED-FOR reinforcement learning ( RL ) problems. Regularization USED-FOR exploration. Regularization USED-FOR stability. entropy functions USED-FOR stability. entropy functions USED-FOR exploration. entropy functions FEATURE-OF Regularization. quasi - Newton method USED-FOR policy gradient algorithm. entropy regularization USED-FOR quasi - Newton method. algorithm USED-FOR natural policy gradient ( NPG ) algorithm. method USED-FOR policy gradient algorithms. method USED-FOR entropy functions. Newton - type quadratic convergence FEATURE-OF algorithms. quasi - Newton method COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE quasi - Newton method. synthetic and industrial - scale examples EVALUATE-FOR quasi - Newton method. single - digit iterations FEATURE-OF quasi - Newton method. OtherScientificTerm are Shannon entropy, and optimal policy. ","This paper studies the problem of learning policy gradient algorithms for reinforcement learning (RL) problems. The authors propose a quasi-Newton method, which is based on the notion of entropy regularization. The main contribution of the paper is to show that the entropy regularisation of the policy gradient algorithm can be viewed as an extension of the regularization of the natural policy gradient (NPG) algorithm. The proposed method is shown to converge to a Newton-type quadratic convergence rate. The paper also shows that the proposed method can be used to improve the performance of existing RL algorithms.","This paper studies the problem of learning policy gradient algorithms for reinforcement learning (RL) problems. The authors propose a quasi-Newton method, which is based on the notion of entropy regularization. The main contribution of the paper is to show that the entropy regularisation of the policy gradient algorithm can be viewed as an extension of the regularization of the natural policy gradient (NPG) algorithm. The proposed method is shown to converge to a Newton-type quadratic convergence rate. The paper also shows that the proposed method can be used to improve the performance of existing RL algorithms."
5759,SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"generalization CONJUNCTION sample efficiency. sample efficiency CONJUNCTION generalization. Text - based games ( TBG ) USED-FOR grounded language understanding. generalization HYPONYM-OF problems. sample efficiency HYPONYM-OF problems. deep reinforcement learning ( RL ) methods USED-FOR TBGs. case - based reasoning USED-FOR general method. on - policy neural agent USED-FOR TBGs. method CONJUNCTION on - policy neural agent. on - policy neural agent CONJUNCTION method. method USED-FOR TBGs. approach COMPARE methods. methods COMPARE approach. out - of - distribution generalization EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR approach. OtherScientificTerm is distributional shifts. Method are deep RL approaches, and case - based reasoner. ","This paper proposes a method for text-based games (TBG) that uses case-based reasoning to improve generalization and sample efficiency. The method is based on the idea of case-reasoning, which is an extension of the case-by-case RL framework. The authors show that the proposed method is able to generalize better than the baselines in terms of out-of-distribution generalization, sample efficiency, and sample complexity. ","This paper proposes a method for text-based games (TBG) that uses case-based reasoning to improve generalization and sample efficiency. The method is based on the idea of case-reasoning, which is an extension of the case-by-case RL framework. The authors show that the proposed method is able to generalize better than the baselines in terms of out-of-distribution generalization, sample efficiency, and sample complexity. "
5775,SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,Pre - trained contextual language models USED-FOR language understanding tasks. sense information USED-FOR multi - sense embeddings. multi - sense embeddings PART-OF skip - gram - like framework. pre - trained language model ( BERT ) USED-FOR two - stage method. approach USED-FOR sense disambiguation mechanism. sense disambiguation mechanism PART-OF model. output layer embeddings PART-OF BERT. distribution over word senses USED-FOR approach. BERT USED-FOR distribution over word senses. output layer embeddings USED-FOR distribution over word senses. method COMPARE multi - sense embeddings. multi - sense embeddings COMPARE method. contextual word similarity CONJUNCTION sense induction tasks. sense induction tasks CONJUNCTION contextual word similarity. sense induction tasks EVALUATE-FOR method. contextual word similarity EVALUATE-FOR method. embedding - based topic model ( ETM ) EVALUATE-FOR multi - sense embedding. multiple benchmark data sets EVALUATE-FOR method. multiple benchmark data sets EVALUATE-FOR multi - sense embeddings. Task is resource - constrained systems. Method is Noncontextual word embeddings. Generic is methods. OtherScientificTerm is polysemy. ,This paper proposes a skip-gram-like framework for multi-sense embeddings. The proposed method is based on a pre-trained language model (BERT) and uses a sense disambiguation mechanism to disentangle the sense information from the distribution over word senses. Experiments show that the proposed method outperforms existing methods on a number of tasks.,This paper proposes a skip-gram-like framework for multi-sense embeddings. The proposed method is based on a pre-trained language model (BERT) and uses a sense disambiguation mechanism to disentangle the sense information from the distribution over word senses. Experiments show that the proposed method outperforms existing methods on a number of tasks.
5800,SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"3D point - clouds CONJUNCTION 2D images. 2D images CONJUNCTION 3D point - clouds. 3D point - clouds HYPONYM-OF visual representations of the physical world. 2D images HYPONYM-OF visual representations of the physical world. computer vision models USED-FOR 2D image and 3D point - cloud understanding. human vision USED-FOR representations. 2D model architectures USED-FOR 3D point - clouds. neural net model USED-FOR images. architecture USED-FOR neural net model. image - pretrained model CONJUNCTION point - cloud model. point - cloud model CONJUNCTION image - pretrained model. 2D convolutional filters CONJUNCTION 3D convolutional filters. 3D convolutional filters CONJUNCTION 2D convolutional filters. finetuning efforts FEATURE-OF models. batch normalization layers USED-FOR models. 3D point - cloud classification EVALUATE-FOR models. task - specific architectures USED-FOR point - cloud models. few - shot classification EVALUATE-FOR FIP. data efficiency EVALUATE-FOR FIP. It USED-FOR point - cloud models. Generic are transfer, and model. Method is inflated imagepretrained models ( FIP ). ","This paper presents a method for training point-cloud models for few-shot image classification. The method is based on an image-pretrained model (IPP) and a 3D point cloud model (PCLM). The main idea is to use a neural network to learn the representations of 2D and 3D images, and then apply the learned representations to the 3D data to improve the performance of the model. The authors show that the proposed method is able to achieve state-of-the-art performance on a few shot classification task. ","This paper presents a method for training point-cloud models for few-shot image classification. The method is based on an image-pretrained model (IPP) and a 3D point cloud model (PCLM). The main idea is to use a neural network to learn the representations of 2D and 3D images, and then apply the learned representations to the 3D data to improve the performance of the model. The authors show that the proposed method is able to achieve state-of-the-art performance on a few shot classification task. "
5825,SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"Autoregressive generative models USED-FOR tasks. sequential data USED-FOR tasks. exposure bias CONJUNCTION long - range coherence. long - range coherence CONJUNCTION exposure bias. method USED-FOR autoregressive generative model. energy - based learning objective USED-FOR method. method USED-FOR exposure bias problem. constraint USED-FOR joint distributions. method USED-FOR temporal coherence. exposure bias problem CONJUNCTION temporal coherence. temporal coherence CONJUNCTION exposure bias problem. constraint USED-FOR method. energy - based models USED-FOR energy scores. autoregressive network USED-FOR energy scores. importance sampling USED-FOR model. language modeling CONJUNCTION neural machine translation. neural machine translation CONJUNCTION language modeling. neural machine translation CONJUNCTION image generation. image generation CONJUNCTION neural machine translation. benchmarks EVALUATE-FOR approach. image generation HYPONYM-OF benchmarks. language modeling HYPONYM-OF benchmarks. neural machine translation HYPONYM-OF benchmarks. Generic are They, and network. Method are chain - style conditional modeling, and MCMC process. OtherScientificTerm is distributions. ",This paper proposes an energy-based learning objective for autoregressive generative models. The proposed method is based on the chain-style conditional modeling (MCMC) framework. The authors show that the proposed method can be applied to both the exposure bias and long-range coherence problems. They also show that their method is able to achieve better performance than the state-of-the-art.,This paper proposes an energy-based learning objective for autoregressive generative models. The proposed method is based on the chain-style conditional modeling (MCMC) framework. The authors show that the proposed method can be applied to both the exposure bias and long-range coherence problems. They also show that their method is able to achieve better performance than the state-of-the-art.
5850,SP:51e748c55bd4134047098559577fa3f37aa7433a,"adversarial attacks FEATURE-OF deep neural networks ( DNNs ). adversarial training ( AT ) method USED-FOR DNN - based classifier. adversarial training ( AT ) method USED-FOR robustness. robustness EVALUATE-FOR DNN - based classifier. adversarial examples USED-FOR adversarial training ( AT ) method. pointwise adversary USED-FOR worst - case adversarial example. PGD - AT and TRADES HYPONYM-OF AT - based methods. pointwise adversary USED-FOR AT - based methods. unified framework USED-FOR Wasserstein distributional robustness. Wasserstein distributional robustness COMPARE AT methods. AT methods COMPARE Wasserstein distributional robustness. Wasserstein cost function CONJUNCTION risk functions. risk functions CONJUNCTION Wasserstein cost function. AT methods PART-OF framework. distributional robustness AT algorithms COMPARE AT counterparts. AT counterparts COMPARE distributional robustness AT algorithms. Method are deep learning systems, classifier, and distributional robustness AT - based algorithms. OtherScientificTerm is adversarial effects. ",This paper proposes a unified framework for Wasserstein distributional robustness (WDR) and adversarial training (AT) methods. The proposed framework is based on the WDR-AT and TRADES algorithms. The authors show that the proposed framework can be used to improve the robustness of DNNs against adversarial attacks. The paper also provides theoretical analysis of the proposed method. ,This paper proposes a unified framework for Wasserstein distributional robustness (WDR) and adversarial training (AT) methods. The proposed framework is based on the WDR-AT and TRADES algorithms. The authors show that the proposed framework can be used to improve the robustness of DNNs against adversarial attacks. The paper also provides theoretical analysis of the proposed method. 
5875,SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"complex dynamics CONJUNCTION sparse annotations. sparse annotations CONJUNCTION complex dynamics. Unsupervised representation learning USED-FOR multivariate time series. it COMPARE problem. problem COMPARE it. data augmentation techniques USED-FOR contrastive training. time slicing USED-FOR segmentlevel augmentation. Bilinear Temporal - Spectral Fusion ( BTSF ) HYPONYM-OF framework. dropout USED-FOR capturing long - term dependencies. dropout USED-FOR global context. instance - level augmentation USED-FOR capturing long - term dependencies. dropout USED-FOR time series. segment - level augmentation COMPARE instance - level augmentation. instance - level augmentation COMPARE segment - level augmentation. global context CONJUNCTION capturing long - term dependencies. capturing long - term dependencies CONJUNCTION global context. dropout USED-FOR instance - level augmentation. iterative bilinear temporal - spectral fusion module USED-FOR affinities. iterative bilinear temporal - spectral fusion module USED-FOR representations of time series. cross - domain interactions USED-FOR representations of time series. alignment CONJUNCTION uniformity. uniformity CONJUNCTION alignment. BTSF USED-FOR bilinear feature representations. forecasting CONJUNCTION anomaly detection. anomaly detection CONJUNCTION forecasting. classification CONJUNCTION forecasting. forecasting CONJUNCTION classification. anomaly detection HYPONYM-OF tasks. tasks USED-FOR time series. anomaly detection HYPONYM-OF time series. classification HYPONYM-OF time series. forecasting HYPONYM-OF time series. anomaly detection HYPONYM-OF tasks. classification HYPONYM-OF tasks. forecasting HYPONYM-OF tasks. BTSF COMPARE state - of - the - art methods. state - of - the - art methods COMPARE BTSF. BTSF COMPARE them. them COMPARE BTSF. Method are contrastive learning, representation learning framework, augmentation methods, and feature representation. OtherScientificTerm is sampling bias. Task is optimization. ","This paper proposes a novel method for unsupervised representation learning for multivariate time series. The proposed method, Bilinear Temporal-Spectral Fusion (BTSF), is based on the idea of bilinear temporal-spectral fusion (BTF), which is an extension of BTSF to multivariate data augmentation. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of time series classification, anomaly detection, and forecasting tasks. ","This paper proposes a novel method for unsupervised representation learning for multivariate time series. The proposed method, Bilinear Temporal-Spectral Fusion (BTSF), is based on the idea of bilinear temporal-spectral fusion (BTF), which is an extension of BTSF to multivariate data augmentation. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of time series classification, anomaly detection, and forecasting tasks. "
5900,SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"model architecture CONJUNCTION batch size. batch size CONJUNCTION model architecture. learning rate USED-FOR deep neural networks. algorithm USED-FOR learning rate. gradient descent USED-FOR learning rate. learning rate CONJUNCTION model weights. model weights CONJUNCTION learning rate. approach USED-FOR learning rate. gradient descent step USED-FOR Learning rate. learning rate FEATURE-OF first and second - order gradients. scheme USED-FOR learning rates. learning rate CONJUNCTION batch size. batch size CONJUNCTION learning rate. it USED-FOR optimizing scheme. optimizing scheme EVALUATE-FOR method. Method are line - search, and neural networks. OtherScientificTerm is weight gradients. ",This paper proposes an algorithm to optimize the learning rate of deep neural networks. The main idea is to use the gradient descent step of the first-order gradients of the weights as a regularizer for learning rate. The authors show that this regularizer can be used to optimize learning rate and batch size in line-search. They also show that their algorithm can be applied to any learning rate optimization scheme. ,This paper proposes an algorithm to optimize the learning rate of deep neural networks. The main idea is to use the gradient descent step of the first-order gradients of the weights as a regularizer for learning rate. The authors show that this regularizer can be used to optimize learning rate and batch size in line-search. They also show that their algorithm can be applied to any learning rate optimization scheme. 
5925,SP:263c787361cd6d4443ce516d389c694d0fe44b28,"continual meta - learning method USED-FOR sequential multi - task learning. RL USED-FOR offline meta - learning. prior continual learning CONJUNCTION off - policy meta - reinforcement methods. off - policy meta - reinforcement methods CONJUNCTION prior continual learning. CoMPS COMPARE off - policy meta - reinforcement methods. off - policy meta - reinforcement methods COMPARE CoMPS. CoMPS COMPARE prior continual learning. prior continual learning COMPARE CoMPS. continuous control tasks EVALUATE-FOR off - policy meta - reinforcement methods. continuous control tasks EVALUATE-FOR CoMPS. Method are Prior meta - reinforcement learning algorithms, continual reinforcement learning algorithms, continual meta - policy search ( CoMPS ), and meta - training. Generic are they, and method. ",This paper proposes a continual meta-learning method for sequential multi-task learning. The proposed method is based on continual reinforcement learning (RL) and meta-training (meta-training). The authors show that the proposed method outperforms prior continual learning and off-policy meta-reinforcement learning methods on a number of continuous control tasks. The authors also show that CoMPS can be used to improve the performance of prior continual RL methods.,This paper proposes a continual meta-learning method for sequential multi-task learning. The proposed method is based on continual reinforcement learning (RL) and meta-training (meta-training). The authors show that the proposed method outperforms prior continual learning and off-policy meta-reinforcement learning methods on a number of continuous control tasks. The authors also show that CoMPS can be used to improve the performance of prior continual RL methods.
5950,SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"“ backdoor ” poisoning attack USED-FOR classification models. threat model USED-FOR poisoned classifier. threat model USED-FOR poisoned classifier. adversarial examples USED-FOR classifier. human interaction FEATURE-OF smoothed adversarial images. ImageNet CONJUNCTION TrojAI. TrojAI CONJUNCTION ImageNet. ImageNet HYPONYM-OF high - resolution datasets. TrojAI HYPONYM-OF high - resolution datasets. high - resolution datasets EVALUATE-FOR attack. method USED-FOR triggers. approach COMPARE method. method COMPARE approach. approach COMPARE modeling trigger distributions. modeling trigger distributions COMPARE approach. backdoors PART-OF poisoned classifiers. secret backdoor PART-OF poisoned classifiers. Method are backdoored classifiers, and Denoised Smoothing. Generic is procedure. OtherScientificTerm is trigger distributions. ","This paper proposes a new poisoning attack method for backdoored classifiers. The proposed method is based on Denoised Smoothing (Dsmoothing), which is a method for smoothing the adversarial examples of a poisoned classifier. The idea is to use a threat model as a proxy for the classifier, and then use a classifier that has been poisoned by a backdoor attack. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark datasets.","This paper proposes a new poisoning attack method for backdoored classifiers. The proposed method is based on Denoised Smoothing (Dsmoothing), which is a method for smoothing the adversarial examples of a poisoned classifier. The idea is to use a threat model as a proxy for the classifier, and then use a classifier that has been poisoned by a backdoor attack. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark datasets."
5975,SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"Generative adversarial networks ( GANs ) USED-FOR content generation tasks. GAN compression methods USED-FOR conditional GANs. distilling unconditional GAN USED-FOR StyleGAN2 architecture. output discrepancy issue FEATURE-OF unconditional GAN distillation. heterogeneous distillation scenario FEATURE-OF knowledge distillation losses. style module USED-FOR semantic information. initialization strategy USED-FOR student model. initialization strategy USED-FOR output consistency. semantic consistency FEATURE-OF teacher and student model. latent - direction - based distillation loss USED-FOR semantic relations. latent space FEATURE-OF semantic relations. latent - direction - based distillation loss USED-FOR semantic consistency. approach USED-FOR StyleGAN2. approach COMPARE GAN distillation methods. GAN distillation methods COMPARE approach. GAN distillation methods USED-FOR distilling StyleGAN2. OtherScientificTerm are computation, resource - constrained devices, latent code, and discrepancy issue. ","This paper proposes a novel method for distilling unconditional GANs. The method is based on the idea of distilling the output of a GAN to the latent space of the teacher and student model. The idea is to use a style module to encode the semantic information between the teacher model and the student model, and then distill the output to a latent space. The authors show that the proposed method is able to achieve better distillation performance than existing distillation methods. ","This paper proposes a novel method for distilling unconditional GANs. The method is based on the idea of distilling the output of a GAN to the latent space of the teacher and student model. The idea is to use a style module to encode the semantic information between the teacher model and the student model, and then distill the output to a latent space. The authors show that the proposed method is able to achieve better distillation performance than existing distillation methods. "
6000,SP:2c2231743fa33b95828c6615263954ce1c05f95d,methodology USED-FOR offline algorithms. online settings FEATURE-OF offline algorithms. multi - task learning model USED-FOR behavioral structures. graphs USED-FOR offline algorithms. synthetic data CONJUNCTION historical stock market data. historical stock market data CONJUNCTION synthetic data. historical stock market data EVALUATE-FOR methodology. synthetic data EVALUATE-FOR methodology. ,This paper proposes a multi-task learning model for offline algorithms. The proposed method is based on graph neural networks. The model is trained on synthetic data and historical stock market data. The authors show that the proposed method outperforms existing offline algorithms on both synthetic and real-world datasets. ,This paper proposes a multi-task learning model for offline algorithms. The proposed method is based on graph neural networks. The model is trained on synthetic data and historical stock market data. The authors show that the proposed method outperforms existing offline algorithms on both synthetic and real-world datasets. 
6025,SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"Gaussian Processes ( GPs ) HYPONYM-OF Bayesian models. Bayesian models USED-FOR uncertainty estimates. variational inference USED-FOR inferring q. Sparse GPs CONJUNCTION variational inference. variational inference CONJUNCTION Sparse GPs. Sparse GPs USED-FOR GPs. it COMPARE sparse variational GP approaches. sparse variational GP approaches COMPARE it. neural network USED-FOR inducing points locations. training and prediction times EVALUATE-FOR method. Generic are They, them, model, and they. Method is sparse GP approximations. OtherScientificTerm is latent function. Task are learning tasks, and prediction. ",This paper proposes a new method for learning Gaussian Processes (GP) models. The method is based on variational inference (VAE) and a neural network. The authors show that the proposed method is able to learn sparse GP approximations of the latent function of a GP model. They also show that their method can be used to improve the training and prediction times of GP models. ,This paper proposes a new method for learning Gaussian Processes (GP) models. The method is based on variational inference (VAE) and a neural network. The authors show that the proposed method is able to learn sparse GP approximations of the latent function of a GP model. They also show that their method can be used to improve the training and prediction times of GP models. 
6050,SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"scientific collaborations CONJUNCTION volunteer computing. volunteer computing CONJUNCTION scientific collaborations. hardest problems PART-OF deep learning. Byzantine tolerance FEATURE-OF distributed training algorithms. algorithms USED-FOR large - scale distributed deep learning. protocol USED-FOR secure ( Byzantinetolerant ) decentralized training. communication efficiency FEATURE-OF protocol. theoretical bounds USED-FOR resistance. Byzantine and Sybil attacks FEATURE-OF resistance. communication overhead FEATURE-OF it. Byzantine attackers FEATURE-OF image classification and language modeling. Generic are systems, and models. Metric is efficiency. OtherScientificTerm are redundant communication, and trusted server. ","This paper proposes a Byzantine-tolerant decentralized training protocol for distributed deep learning. The proposed method is based on the idea of Byzantine tolerance, which is a generalization of Byzantine and Sybil attacks. The authors show that the proposed method can be used to defend against Byzantine attacks in the presence of redundant communication. They also provide theoretical bounds on the communication overhead of the proposed protocol. ","This paper proposes a Byzantine-tolerant decentralized training protocol for distributed deep learning. The proposed method is based on the idea of Byzantine tolerance, which is a generalization of Byzantine and Sybil attacks. The authors show that the proposed method can be used to defend against Byzantine attacks in the presence of redundant communication. They also provide theoretical bounds on the communication overhead of the proposed protocol. "
6075,SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"Smoothed particle hydrodynamics ( SPH ) HYPONYM-OF mesh - free Lagrangian method. astrophysics and engineering applications FEATURE-OF weaklyand strongly compressible turbulence. physics based parameters CONJUNCTION Neural Networks. Neural Networks CONJUNCTION physics based parameters. Neural Networks USED-FOR universal function approximators. Neural Networks USED-FOR SPH informed fluid simulators. physics based parameters USED-FOR SPH informed fluid simulators. forward and adjoint based sensitivity analyses USED-FOR gradient based optimization. learning algorithm USED-FOR mixed mode approach. physics informed learning method USED-FOR inverse problems. physically interpretable parameter space FEATURE-OF inverse problems. time scales CONJUNCTION Reynolds numbers. Reynolds numbers CONJUNCTION time scales. interpretability CONJUNCTION generalizability. generalizability CONJUNCTION interpretability. hierarchy of models USED-FOR physical structure. Reynolds numbers FEATURE-OF generalizability. time scales FEATURE-OF generalizability. OtherScientificTerm are Neural Network parameters, Lagrangian statistics of turbulence, and physical symmetries. Material is training data. ","This paper proposes a new method for learning physics-based parameters for smoothed particle hydrodynamics (SPH) based on Neural Networks. The proposed method is based on forward and adjoint based sensitivity analyses. The authors show that the proposed method outperforms existing methods in terms of generalizability, interpretability, and generalization. They also show that their method is more interpretable than existing methods. ","This paper proposes a new method for learning physics-based parameters for smoothed particle hydrodynamics (SPH) based on Neural Networks. The proposed method is based on forward and adjoint based sensitivity analyses. The authors show that the proposed method outperforms existing methods in terms of generalizability, interpretability, and generalization. They also show that their method is more interpretable than existing methods. "
6100,SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"approach USED-FOR deterministic neural network. accuracy CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION accuracy. entropy maximization regularizer USED-FOR predictive distribution. entropy maximization regularizer USED-FOR approach. embedding space FEATURE-OF predictive distribution. cross - entropy loss USED-FOR approach. entropy FEATURE-OF samples. images USED-FOR convex combination. convex combination USED-FOR synthetically generating between - cluster samples. data - dependent regularization USED-FOR maximum likelihood estimation. data - dependent regularization USED-FOR solution. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. real - world datasets EVALUATE-FOR Mix - MaxEnt. calibrated probabilities USED-FOR in - distribution data. Mix - MaxEnt USED-FOR uncertainty estimates. ResNet and Wide - ResNet architectures USED-FOR real - world datasets. CIFAR-10 HYPONYM-OF real - world datasets. CIFAR-100 HYPONYM-OF real - world datasets. classification accuracy EVALUATE-FOR Mix - MaxEnt. OtherScientificTerm are class clusters, out - of - distribution samples, high entropy regions, entropy barrier, and superficial input perturbations. ","This paper proposes Mix-MaxEnt, a novel approach to improve the classification performance of a deterministic neural network. The proposed approach is motivated by the observation that in-distribution samples tend to be high-entropy and high-uniformity, and the authors propose to use a convex combination of images to generate between-cluster samples. The authors also propose a data-dependent regularization for the maximum likelihood estimation. Experiments on CIFAR-10, Cifar-100, and Wide-ResNet demonstrate the effectiveness of the proposed approach. ","This paper proposes Mix-MaxEnt, a novel approach to improve the classification performance of a deterministic neural network. The proposed approach is motivated by the observation that in-distribution samples tend to be high-entropy and high-uniformity, and the authors propose to use a convex combination of images to generate between-cluster samples. The authors also propose a data-dependent regularization for the maximum likelihood estimation. Experiments on CIFAR-10, Cifar-100, and Wide-ResNet demonstrate the effectiveness of the proposed approach. "
6125,SP:365490b872464f00634dc7a50d024fceaf0a61ee,"Generative Adversarial Networks ( GANs ) CONJUNCTION auto - encoder animating images. auto - encoder animating images CONJUNCTION Generative Adversarial Networks ( GANs ). driving videos USED-FOR structure representation. structure representation USED-FOR animation - approaches. modules USED-FOR animation - model. modules USED-FOR extraction of structure information. Latent Image Animator ( LIA ) HYPONYM-OF self - supervised autoencoder. linear combination USED-FOR latent space. model COMPARE state - of - art methods. state - of - art methods COMPARE model. TED - talk datasets EVALUATE-FOR state - of - art methods. TED - talk datasets EVALUATE-FOR model. VoxCeleb EVALUATE-FOR state - of - art methods. VoxCeleb EVALUATE-FOR model. LIA USED-FOR motion of a driving video. landmarks CONJUNCTION region representations. region representations CONJUNCTION landmarks. LIA USED-FOR images. structure representations USED-FOR LIA. region representations HYPONYM-OF structure representations. landmarks HYPONYM-OF structure representations. Material are still images, LIA animation examples, and VoxCeleb dataset. Generic are approaches, and models. OtherScientificTerm are appearance variation, motion, and orthogonal motion directions. Metric are complexity, and generated quality. ",This paper proposes a self-supervised auto-encoder for driving videos. The proposed method is based on a linear combination of the latent space of the driving video and the structure representation of the video. The authors show that the proposed method outperforms state-of-the-art methods on the VoxCeleb dataset.,This paper proposes a self-supervised auto-encoder for driving videos. The proposed method is based on a linear combination of the latent space of the driving video and the structure representation of the video. The authors show that the proposed method outperforms state-of-the-art methods on the VoxCeleb dataset.
6150,SP:86f9f89f84e117c86478b9afaf087f65524f5472,"meta - training tasks USED-FOR meta - learning algorithms. approach USED-FOR tasks. data - adaptive meta - regularization USED-FOR MLTI. generalization EVALUATE-FOR MLTI. pose prediction CONJUNCTION molecule property prediction. molecule property prediction CONJUNCTION pose prediction. molecule property prediction CONJUNCTION medical image classification. medical image classification CONJUNCTION molecule property prediction. image recognition CONJUNCTION pose prediction. pose prediction CONJUNCTION image recognition. MLTI framework COMPARE state - of - the - art strategies. state - of - the - art strategies COMPARE MLTI framework. MLTI framework CONJUNCTION representative meta - learning algorithms. representative meta - learning algorithms CONJUNCTION MLTI framework. datasets EVALUATE-FOR MLTI framework. image recognition HYPONYM-OF datasets. medical image classification HYPONYM-OF datasets. pose prediction HYPONYM-OF datasets. molecule property prediction HYPONYM-OF datasets. Method is Meta - learning. OtherScientificTerm are real - world scenarios, and interpolation. ","This paper proposes a data-adaptive meta-regularization (MLTI) framework for meta-learning. The proposed method is based on the idea of data-adversarial meta-training, which aims to improve the generalization performance of meta-learners. The authors propose to use the data-accuracy regularization to reduce the number of samples needed to train the model. The paper also proposes a new meta-parameterization method to improve generalization of the proposed method. Experiments show that the proposed MLTI method outperforms the baselines on a variety of tasks.","This paper proposes a data-adaptive meta-regularization (MLTI) framework for meta-learning. The proposed method is based on the idea of data-adversarial meta-training, which aims to improve the generalization performance of meta-learners. The authors propose to use the data-accuracy regularization to reduce the number of samples needed to train the model. The paper also proposes a new meta-parameterization method to improve generalization of the proposed method. Experiments show that the proposed MLTI method outperforms the baselines on a variety of tasks."
6175,SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"approach USED-FOR fairness of downstream predictors. encoding sensitive data USED-FOR fairness of downstream predictors. unfairness FEATURE-OF adversarial predictors. representations USED-FOR sensitive attributes. fairness guarantees FEATURE-OF learned representations. probability density USED-FOR sensitive groups. normalizing flow USED-FOR statistical distance. statistical distance FEATURE-OF latent representations. normalizing flow USED-FOR encoder. maximum unfairness EVALUATE-FOR adversarial downstream predictor. interpretability CONJUNCTION transfer learning. transfer learning CONJUNCTION interpretability. FNF USED-FOR group fairness notions. FNF USED-FOR properties. real - world datasets EVALUATE-FOR properties. transfer learning HYPONYM-OF properties. interpretability HYPONYM-OF properties. real - world datasets EVALUATE-FOR FNF. Method are Fair representation learning, Fair Normalizing Flows ( FNF ), and likelihood computation. ","This paper studies the problem of fair representation learning in the context of adversarial prediction. The authors propose a new normalizing flow (FFN) method to improve the fairness of downstream predictors. The proposed method is based on normalizing flows (NFs) that are used to compute the probability density of sensitive groups in the latent space of the encoder, which is then used to estimate the fairness of the downstream predictor.  The authors show that the proposed method can be used to improve transfer learning and interpretability of the learned representations. They also show that FNF can be applied to group fairness notions. ","This paper studies the problem of fair representation learning in the context of adversarial prediction. The authors propose a new normalizing flow (FFN) method to improve the fairness of downstream predictors. The proposed method is based on normalizing flows (NFs) that are used to compute the probability density of sensitive groups in the latent space of the encoder, which is then used to estimate the fairness of the downstream predictor.  The authors show that the proposed method can be used to improve transfer learning and interpretability of the learned representations. They also show that FNF can be applied to group fairness notions. "
6200,SP:404d5643327f60f0f06f820033a56081f9e01900,"subgraph patterns on graphs USED-FOR graph - based tasks. graph neural networks ( GNNs ) USED-FOR low - dimensional representation. node - centric message passing mechanism USED-FOR GNNs. they USED-FOR complex structure matching. complex structure matching USED-FOR isomorphism counting. COUNT - GNN USED-FOR subgraph isomorphism counting. GNN USED-FOR subgraph isomorphism counting. COUNT - GNN HYPONYM-OF GNN. edge - centric message passing scheme USED-FOR edge level. COUNT - GNN USED-FOR fine - grained structural information. edge USED-FOR encoding graph structures. graph representation USED-FOR graph level. benchmark datasets EVALUATE-FOR COUNT - GNN. COUNT - GNN COMPARE baselines. baselines COMPARE COUNT - GNN. OtherScientificTerm are graph structures, subgraph isomorphisms, nodes, graph, query graphs, structured query graphs, edges, edge adjacency, and first - class citizens. Material is graph data. Method is backtracking framework. Metric is computational cost. Task are node - oriented tasks, and matching. ",This paper proposes a new method for graph neural networks (GNNs) for subgraph isomorphism counting. The proposed method is based on the edge-centric message passing mechanism. The key idea is to use edge-based message passing to encode information about the edge level of the graph. The authors show that the proposed method outperforms baselines on several benchmark datasets. ,This paper proposes a new method for graph neural networks (GNNs) for subgraph isomorphism counting. The proposed method is based on the edge-centric message passing mechanism. The key idea is to use edge-based message passing to encode information about the edge level of the graph. The authors show that the proposed method outperforms baselines on several benchmark datasets. 
6225,SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"Agnostic Personalized Federated Learning ( APFL ) HYPONYM-OF loosely constrained federated learning. Similarity Matching CONJUNCTION Kernel Factorization ( SimFed ). Kernel Factorization ( SimFed ) CONJUNCTION Similarity Matching. Kernel Factorization ( SimFed ) HYPONYM-OF method. Similarity Matching HYPONYM-OF method. ones USED-FOR personalized knowledge reflection. method USED-FOR task - level similarity. locally learned knowledge USED-FOR method. locally learned knowledge USED-FOR task - level similarity. knowledge collapse CONJUNCTION information loss. information loss CONJUNCTION knowledge collapse. dimensionlaity of parameter space USED-FOR knowledge collapse. information loss FEATURE-OF heterogeneous knowledge. basis vectors PART-OF model parameters. method COMPARE federated learning methods. federated learning methods COMPARE method. singleand multi - domain datasets EVALUATE-FOR method. singleand multi - domain datasets EVALUATE-FOR method. Task is federated learning. OtherScientificTerm are personalized labels, Label Heterogeneity, and Domain Heterogeneity. Generic is they. Method are labeling schemes, and agnostic personalized federated learning. Material is local data. ","This paper proposes a new method for personalized federated learning. The proposed method is based on the notion of label homogeneity, which is an important problem in the context of personalized learning. In this paper, the authors propose a method to address the problem of knowledge collapse in the case of heterogeneous knowledge. The authors propose to use the dimensionlaity of parameter space to reduce the information loss in the parameter space. The method is evaluated on both single-domain and multi-domain datasets.","This paper proposes a new method for personalized federated learning. The proposed method is based on the notion of label homogeneity, which is an important problem in the context of personalized learning. In this paper, the authors propose a method to address the problem of knowledge collapse in the case of heterogeneous knowledge. The authors propose to use the dimensionlaity of parameter space to reduce the information loss in the parameter space. The method is evaluated on both single-domain and multi-domain datasets."
6250,SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"Object Dynamics Distillation Network ( ODDN ) USED-FOR object dynamic representations. velocity HYPONYM-OF object dynamic representations. raw video input USED-FOR object dynamic representations. it USED-FOR dynamic representations of objects. relation module USED-FOR object - pair interactions. relation module USED-FOR dynamic representations of objects. video events reasoning CONJUNCTION video prediction. video prediction CONJUNCTION video events reasoning. video events reasoning EVALUATE-FOR approach. scene representation methods USED-FOR representaions. occlusion CONJUNCTION objects collision. objects collision CONJUNCTION occlusion. object dynamic clues USED-FOR model. segmentation CONJUNCTION reconstruction. reconstruction CONJUNCTION segmentation. scene decomposition quality EVALUATE-FOR reconstruction. scene decomposition quality EVALUATE-FOR segmentation. scene decomposition quality EVALUATE-FOR model. OtherScientificTerm are abstract entities, and physical events. Method are object - centric representations of scenes, and ODDN. Material is static images. Task are object dynamics, and video understanding. ","This paper proposes an object dynamics distillation network (ODDN) for video understanding. The proposed method is based on object-pair interactions between two objects and is able to learn dynamic representations of objects. The method is evaluated on a variety of video tasks, including segmentation, reconstruction, and video prediction. ","This paper proposes an object dynamics distillation network (ODDN) for video understanding. The proposed method is based on object-pair interactions between two objects and is able to learn dynamic representations of objects. The method is evaluated on a variety of video tasks, including segmentation, reconstruction, and video prediction. "
6275,SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"Graph neural networks ( GNN ) USED-FOR graph - based learning tasks. nodes USED-FOR task. link / motif prediction HYPONYM-OF nodes. random node features CONJUNCTION node distance features. node distance features CONJUNCTION random node features. slow convergence CONJUNCTION inaccurate prediction. inaccurate prediction CONJUNCTION slow convergence. Laplacian Eigenmap CONJUNCTION Deepwalk. Deepwalk CONJUNCTION Laplacian Eigenmap. positional encoding ( PE ) techniques USED-FOR positional features. PE USED-FOR GNNs. positional features USED-FOR GNNs. Laplacian Eigenmap HYPONYM-OF positional encoding ( PE ) techniques. Deepwalk HYPONYM-OF positional encoding ( PE ) techniques. mathematical analysis USED-FOR PEG. PEG HYPONYM-OF GNN layers. mathematical analysis USED-FOR GNN layers. node features CONJUNCTION positional features. positional features CONJUNCTION node features. node features USED-FOR PEG. positional features USED-FOR PEG. permutation equivariance FEATURE-OF PEG. node features CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION node features. link prediction EVALUATE-FOR PEG. real - world networks USED-FOR link prediction. generalization EVALUATE-FOR PEG. Generic are they, and solution. Metric is complexity. ","This paper studies the problem of link prediction and motif prediction in graph neural networks (GNNs). In particular, the authors propose a new GNN layer called Positional Encoder-Encoder (PEG) that is based on the Laplacian Eigenmap (PE) and Deepwalk (Deepwalk) techniques. The authors show that the proposed PEG is able to achieve better performance than existing GNNs on link prediction tasks. They also show that PEG can be used for link prediction in real-world networks.","This paper studies the problem of link prediction and motif prediction in graph neural networks (GNNs). In particular, the authors propose a new GNN layer called Positional Encoder-Encoder (PEG) that is based on the Laplacian Eigenmap (PE) and Deepwalk (Deepwalk) techniques. The authors show that the proposed PEG is able to achieve better performance than existing GNNs on link prediction tasks. They also show that PEG can be used for link prediction in real-world networks."
6300,SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,non - parallel datasets USED-FOR models. non - parallel datasets USED-FOR text style transfer models. weak supervision USED-FOR style transfer models. LaMer HYPONYM-OF text style transfer framework. large - scale language models USED-FOR text style transfer framework. MLE training CONJUNCTION imitation learning refinement. imitation learning refinement CONJUNCTION MLE training. imitation learning refinement USED-FOR intrinsic parallelism. parallel expressions PART-OF non - parallel datasets. intrinsic parallelism FEATURE-OF data. imitation learning refinement USED-FOR LaMer. MLE training USED-FOR LaMer. scene graphs USED-FOR LaMer. scene graphs USED-FOR parallel expressions. content preservation CONJUNCTION fluency. fluency CONJUNCTION content preservation. transfer accuracy CONJUNCTION content preservation. content preservation CONJUNCTION transfer accuracy. task EVALUATE-FOR model. sentiment & formality transfer EVALUATE-FOR model. sentiment & formality transfer CONJUNCTION political stance transfer. political stance transfer CONJUNCTION sentiment & formality transfer. political stance transfer EVALUATE-FOR model. political stance transfer HYPONYM-OF task. fluency EVALUATE-FOR model. content preservation EVALUATE-FOR model. transfer accuracy EVALUATE-FOR model. model COMPARE models. models COMPARE model. OtherScientificTerm is style - independent information. ,"This paper proposes LaMer, a text style transfer framework for large-scale language models. The proposed method is based on imitation learning refinement and MLE training to improve the performance of the model on non-parallel datasets. The method is evaluated on three tasks: sentiment & formality transfer, political stance transfer, and content preservation. LaMer achieves state-of-the-art performance on all three tasks. ","This paper proposes LaMer, a text style transfer framework for large-scale language models. The proposed method is based on imitation learning refinement and MLE training to improve the performance of the model on non-parallel datasets. The method is evaluated on three tasks: sentiment & formality transfer, political stance transfer, and content preservation. LaMer achieves state-of-the-art performance on all three tasks. "
6325,SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"Multi - hop logical reasoning HYPONYM-OF representation learning on knowledge graphs ( KGs ). one - hop link prediction CONJUNCTION logical queries. logical queries CONJUNCTION one - hop link prediction. one - hop link prediction PART-OF It. logical queries PART-OF It. classical, triple - based graphs USED-FOR algorithms. hyper - relational modeling paradigm USED-FOR KGs. key - value pairs FEATURE-OF typed edges. approaches USED-FOR approximate query answering ( QA ). Hyper - relational queries PART-OF real - world KG applications. qualifier pairs USED-FOR approaches. hyper - relational KGs USED-FOR complex queries. multi - hop reasoning problem USED-FOR complex queries. multi - hop reasoning problem USED-FOR hyper - relational KGs. Graph Neural Networks CONJUNCTION query embedding techniques. query embedding techniques CONJUNCTION Graph Neural Networks. method USED-FOR queries. qualifiers USED-FOR QA. query patterns USED-FOR QA. query patterns EVALUATE-FOR qualifiers. Generic is paradigm. OtherScientificTerm are fine - grained context, and hyper - relational conjunctive queries. ","This paper proposes a new approach to approximate query answering (QA) for hyper-relational knowledge graphs (KGs). The approach is based on multi-hop logical reasoning (MJL), which is a generalization of the classical triple-based QA framework. The authors propose a new query answering method that uses a query embedding technique to learn a set of queries that can be used to answer queries in a hyper-reconstruction of a KG. The proposed method is evaluated on a number of real-world QA tasks and shows that the proposed method outperforms existing approaches.","This paper proposes a new approach to approximate query answering (QA) for hyper-relational knowledge graphs (KGs). The approach is based on multi-hop logical reasoning (MJL), which is a generalization of the classical triple-based QA framework. The authors propose a new query answering method that uses a query embedding technique to learn a set of queries that can be used to answer queries in a hyper-reconstruction of a KG. The proposed method is evaluated on a number of real-world QA tasks and shows that the proposed method outperforms existing approaches."
6350,SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"DYHPO HYPONYM-OF method. Bayesian optimization USED-FOR gray - box setup. Bayesian optimization USED-FOR technique. surrogate USED-FOR Gaussian Processes. multi - budget information FEATURE-OF acquisition function. acquisition function PART-OF surrogate. learning curve dynamics PART-OF surrogate. DYHPO COMPARE hyperparameter optimization baselines. hyperparameter optimization baselines COMPARE DYHPO. Method are Gray - box hyperparameter optimization techniques, and multibudget search mechanisms. OtherScientificTerm is hyperparameter configurations. ","This paper proposes a new method for multi-budget hyperparameter optimization (MBO) for Gaussian Processes. The proposed method is based on Bayesian optimization. The main idea is to learn a surrogate for the acquisition function of the Gaussian process, which is then used to search for the optimal hyperparameters. The authors show that the proposed method outperforms existing methods in terms of performance. ","This paper proposes a new method for multi-budget hyperparameter optimization (MBO) for Gaussian Processes. The proposed method is based on Bayesian optimization. The main idea is to learn a surrogate for the acquisition function of the Gaussian process, which is then used to search for the optimal hyperparameters. The authors show that the proposed method outperforms existing methods in terms of performance. "
6375,SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"learned image compression COMPARE image coding techniques. image coding techniques COMPARE learned image compression. rate - distortion EVALUATE-FOR learned image compression. deterministic inference USED-FOR Gaussian mixture models. methods USED-FOR image compression models. cross - platform consistent manner FEATURE-OF image compression models. Method are non - deterministic calculation, and training and fine - tuning based approaches. Task is decoding. OtherScientificTerm are post - training quantization, and entropy parameters. ","This paper studies the problem of image compression. The authors propose to use deterministic inference for image compression, which is a non-deterministic inference method. The main contribution of the paper is to show that the rate-distortion of learned image compression models can be reduced by using a deterministic approach. They also show that this approach can be applied to Gaussian mixture models. ","This paper studies the problem of image compression. The authors propose to use deterministic inference for image compression, which is a non-deterministic inference method. The main contribution of the paper is to show that the rate-distortion of learned image compression models can be reduced by using a deterministic approach. They also show that this approach can be applied to Gaussian mixture models. "
6400,SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"nanoscale resolution FEATURE-OF imaging and analysis of cellular ultrastructure. fully unsupervised Noise Reconstruction and Removal Network USED-FOR denoising scanning electron microscopy images. architecture USED-FOR noise. gated recurrent units USED-FOR architecture. sequential data USED-FOR noise. fully unsupervised training USED-FOR network. fully unsupervised training COMPARE supervised approaches. supervised approaches COMPARE fully unsupervised training. 3D electron microscopy data sets EVALUATE-FOR supervised approaches. Material is labels and/or noise - free data sets. OtherScientificTerm are time consuming manual annotations, and imaging artifacts. Metric is empirical metrics. ","This paper proposes a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images. The proposed method is based on the idea of gated recurrent units, which can be used to remove the noise from the image. The authors show that the proposed method outperforms the state-of-the-art in terms of noise removal and reconstruction accuracy. The paper also shows that the method can be applied to a variety of image classification tasks.","This paper proposes a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images. The proposed method is based on the idea of gated recurrent units, which can be used to remove the noise from the image. The authors show that the proposed method outperforms the state-of-the-art in terms of noise removal and reconstruction accuracy. The paper also shows that the method can be applied to a variety of image classification tasks."
6425,SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"Graph neural networks ( GNNs ) CONJUNCTION label propagation. label propagation CONJUNCTION Graph neural networks ( GNNs ). graph structure USED-FOR tasks. node property prediction HYPONYM-OF tasks. stacked message - passing layers USED-FOR predictive embeddings. stacked message - passing layers USED-FOR node features. neighborhood information FEATURE-OF stacked message - passing layers. stacked message - passing layers USED-FOR former. spreading label information USED-FOR unlabeled nodes. parameter - free diffusion process USED-FOR spreading label information. features CONJUNCTION labels. labels CONJUNCTION features. statistical properties PART-OF training pipeline. label trick USED-FOR training pipeline. deterministic training objective USED-FOR stochastic label trick. data - fitting term USED-FOR label leakage issues. graph structure FEATURE-OF regularization factor. Generic are latter, and two. OtherScientificTerm is GNN inputs. Material is Open Graph Benchmark ( OGB ) leaderboard. Task is label trick use cases. ",This paper studies the problem of label leakage in GNNs. The authors propose a stochastic label trick to mitigate the problem. The proposed method is based on the idea of spreading label information between unlabeled nodes. The method is evaluated on the Open Graph Benchmark (OGB) leaderboard and shows promising results. ,This paper studies the problem of label leakage in GNNs. The authors propose a stochastic label trick to mitigate the problem. The proposed method is based on the idea of spreading label information between unlabeled nodes. The method is evaluated on the Open Graph Benchmark (OGB) leaderboard and shows promising results. 
6450,SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"Theory of mind ( ToM ) HYPONYM-OF human intelligence. machines USED-FOR theory of mind. ToM agents USED-FOR tasks. predefined roles FEATURE-OF tasks. speaker - listener scenarios HYPONYM-OF predefined roles. strategy USED-FOR SymmToM. theory of mind USED-FOR strategy. theory of mind USED-FOR SymmToM. multi - agent deep reinforcement learning models USED-FOR mental states. modeling of theory of mind USED-FOR multi - agent scenarios. OtherScientificTerm are machine theory of mind, and grid world. Method are multiagent environment SymmToM, and ToM model. ",This paper proposes a method for learning a theory of mind for multi-agent multi-task reinforcement learning. The method is based on the theory of the mind (TMM) which is used to model the dynamics of the environment. The proposed method is tested on speaker-listener and speaker-receiver tasks. The authors show that the proposed method outperforms the baselines.,This paper proposes a method for learning a theory of mind for multi-agent multi-task reinforcement learning. The method is based on the theory of the mind (TMM) which is used to model the dynamics of the environment. The proposed method is tested on speaker-listener and speaker-receiver tasks. The authors show that the proposed method outperforms the baselines.
6475,SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"visual data collecting and processing technology USED-FOR robots. orientations CONJUNCTION illumination. illumination CONJUNCTION orientations. smart manufacturing CONJUNCTION high - mix - low - volume production. high - mix - low - volume production CONJUNCTION smart manufacturing. Zero - shot object detection HYPONYM-OF unsupervised learning. car CONJUNCTION people. people CONJUNCTION car. bikes CONJUNCTION car. car CONJUNCTION bikes. car HYPONYM-OF outdoor scenes. people HYPONYM-OF outdoor scenes. bikes HYPONYM-OF outdoor scenes. indoor scenes FEATURE-OF zero - shot detection of daily objects. zero - shot detection USED-FOR object size level. dataset EVALUATE-FOR zero - shot detection. Method are robot vision system, vision system, zero - shot object detection, and zero - shot object detection algorithm. Task are manufacturing environment, production process, and detection of daily objects. Generic is it. OtherScientificTerm is manufacturing setup. Material is YCB Video Dataset. ",This paper proposes a zero-shot object detection method for robots. The proposed method is based on unsupervised learning. The method is evaluated on the YCB Video Dataset and shows that it outperforms the state-of-the-art in terms of object detection performance. ,This paper proposes a zero-shot object detection method for robots. The proposed method is based on unsupervised learning. The method is evaluated on the YCB Video Dataset and shows that it outperforms the state-of-the-art in terms of object detection performance. 
6500,SP:aa1dcd9217270010f16a00004facede942efea17,"generating future frames CONJUNCTION learning environment dynamics. learning environment dynamics CONJUNCTION generating future frames. Video prediction HYPONYM-OF problem. autoregressive prediction model USED-FOR image generator. autoregressive latent video models USED-FOR video prediction tool. autoregressive latent video prediction model USED-FOR high - fidelity future frames. autoregressive latent video prediction model USED-FOR high - resolution ( 256x256 ) videos. top - k sampling CONJUNCTION data augmentation. data augmentation CONJUNCTION top - k sampling. data augmentation USED-FOR video prediction quality. top - k sampling USED-FOR video prediction quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. method USED-FOR highresolution video prediction. complex and large - scale datasets USED-FOR highresolution video prediction. video prediction benchmarks EVALUATE-FOR state - of - the - art approaches. video prediction benchmarks EVALUATE-FOR method. Task are video prediction, and predicting high - fidelity future frames. Method are image generator model, prior models, and causal transformer model. OtherScientificTerm is latent space of the image generator. Generic is models. ","This paper proposes a video prediction method for high-resolution (256x256) videos. The proposed method is based on an autoregressive latent video prediction model, where the latent space of the image generator is used to predict the future frames of the video. The authors show that the proposed method outperforms state-of-the-art methods on several video prediction benchmarks. ","This paper proposes a video prediction method for high-resolution (256x256) videos. The proposed method is based on an autoregressive latent video prediction model, where the latent space of the image generator is used to predict the future frames of the video. The authors show that the proposed method outperforms state-of-the-art methods on several video prediction benchmarks. "
6525,SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,vision - specific inductive biases USED-FOR Vision Transformers ( ViTs ). image recognition EVALUATE-FOR Vision Transformers ( ViTs ). ViT architecture PART-OF generative adversarial networks ( GANs ). regularization methods USED-FOR GANs. regularization methods USED-FOR self - attention. regularization methods USED-FOR ViT discriminators. regularization techniques USED-FOR GANs. ViTs USED-FOR GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. approach COMPARE CNNbased GAN models. CNNbased GAN models COMPARE approach. CelebA CONJUNCTION LSUN bedroom. LSUN bedroom CONJUNCTION CelebA. ViTGAN HYPONYM-OF approach. LSUN bedroom HYPONYM-OF datasets. datasets EVALUATE-FOR CNNbased GAN models. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR approach. CelebA HYPONYM-OF datasets. Task is image generation. Method is ViT generators. OtherScientificTerm is latent and pixel mapping layers. ,"This paper proposes a method to improve the performance of ViT-based generative adversarial networks (GANs) by adding self-attention to the latent and pixel mapping layers of the ViT generator. The proposed method, ViTGAN, is evaluated on a number of image recognition tasks, including CelebA, LSUN bedroom, and CelebA-CIFAR-10. The results show that the proposed method outperforms the state-of-the-art GAN-based methods.","This paper proposes a method to improve the performance of ViT-based generative adversarial networks (GANs) by adding self-attention to the latent and pixel mapping layers of the ViT generator. The proposed method, ViTGAN, is evaluated on a number of image recognition tasks, including CelebA, LSUN bedroom, and CelebA-CIFAR-10. The results show that the proposed method outperforms the state-of-the-art GAN-based methods."
6550,SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"sample quality EVALUATE-FOR models. variational autoencoders USED-FOR image generative modeling. entropy FEATURE-OF natural image distributions. visually imperceptible information FEATURE-OF entropy. models USED-FOR competitive likelihoods. imperceptible information PART-OF likelihood signal. good sample quality EVALUATE-FOR modeling of visually perceptible information. OtherScientificTerm are Good likelihoods, high - dimensional image data distributions, and visually perceptible bits. Task is generative modeling. ","This paper studies the problem of generative modeling of high-dimensional image data. The authors propose a variational autoencoder (VAE) model for image generative models. The proposed VAE model is motivated by the observation that the entropy of natural image distributions (e.g., MNIST, CIFAR-10) is highly correlated with the quality of the generated images. They propose to use this observation to improve the sample quality of VAE models. ","This paper studies the problem of generative modeling of high-dimensional image data. The authors propose a variational autoencoder (VAE) model for image generative models. The proposed VAE model is motivated by the observation that the entropy of natural image distributions (e.g., MNIST, CIFAR-10) is highly correlated with the quality of the generated images. They propose to use this observation to improve the sample quality of VAE models. "
6575,SP:bfed56018134ec66cde9a7e958df964d4cca3164,"Diffusion probabilistic models ( DPMs ) HYPONYM-OF generative models. inference USED-FOR variance. reverse process FEATURE-OF variance. optimal reverse variance CONJUNCTION optimal KL divergence. optimal KL divergence CONJUNCTION optimal reverse variance. optimal KL divergence FEATURE-OF DPM. optimal reverse variance FEATURE-OF DPM. analytic forms FEATURE-OF optimal KL divergence. analytic forms FEATURE-OF optimal reverse variance. Monte Carlo method CONJUNCTION pretrained score - based model. pretrained score - based model CONJUNCTION Monte Carlo method. Analytic - DPM HYPONYM-OF training - free inference framework. training - free inference framework USED-FOR analytic forms. Analytic - DPM USED-FOR analytic forms. Monte Carlo method USED-FOR Analytic - DPM. pretrained score - based model USED-FOR Analytic - DPM. analytic - DPM USED-FOR DPMs. analytic - DPM USED-FOR log - likelihood. log - likelihood FEATURE-OF DPMs. Task is inference of DPMs. OtherScientificTerm are score function, and lower and upper bounds. Method is score - based model. Generic is estimate. ","This paper considers the problem of inference of DPMs. The authors propose a training-free inference framework, called Analytic-DPM, which is based on a score-based model and a Monte Carlo method. The main contribution of the paper is to derive lower and upper bounds for the optimal reverse variance and optimal KL divergence of the DPM. The paper also provides a theoretical analysis of the analytic form of the reverse process. ","This paper considers the problem of inference of DPMs. The authors propose a training-free inference framework, called Analytic-DPM, which is based on a score-based model and a Monte Carlo method. The main contribution of the paper is to derive lower and upper bounds for the optimal reverse variance and optimal KL divergence of the DPM. The paper also provides a theoretical analysis of the analytic form of the reverse process. "
6600,SP:3f935ba5784c3e86db72421426bc479061af1a4b,"vision transformers ( ViTs ) COMPARE CNNs. CNNs COMPARE vision transformers ( ViTs ). transformer - based models USED-FOR medical image classification. CNNs CONJUNCTION transformers. transformers CONJUNCTION CNNs. medical image benchmark datasets CONJUNCTION tasks. tasks CONJUNCTION medical image benchmark datasets. vision transformers COMPARE CNNs. CNNs COMPARE vision transformers. CNNs COMPARE vision transformers. vision transformers COMPARE CNNs. ImageNet EVALUATE-FOR CNNs. Method is Convolutional Neural Networks ( CNNs ). Task are automated medical image diagnosis, classification, detection and segmentation tasks, medical imaging tasks, and supervised and self - supervised setting. Material is natural image domain. ","This paper proposes a method for medical image classification based on convolutional neural networks (CNNs) and vision transformers (ViTs). The authors show that CNNs are able to perform better than ViTs on a variety of medical imaging tasks, including classification, detection and segmentation tasks. The authors also show that the performance of CNNs is comparable to that of ViTs in the self-supervised setting. ","This paper proposes a method for medical image classification based on convolutional neural networks (CNNs) and vision transformers (ViTs). The authors show that CNNs are able to perform better than ViTs on a variety of medical imaging tasks, including classification, detection and segmentation tasks. The authors also show that the performance of CNNs is comparable to that of ViTs in the self-supervised setting. "
6625,SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"pretrained NLM COMPARE it. it COMPARE pretrained NLM. NLM training heuristics USED-FOR pretraining and fine - tuning stages. NLM pretraining USED-FOR Natural Language Understanding tasks. sentence representations CONJUNCTION open domain question answering abilities. open domain question answering abilities CONJUNCTION sentence representations. Method are Pretraining Neural Language Models ( NLMs ), neural architecture, pretraining example design, and self - improving representations. OtherScientificTerm is semantically related non - neighboring sentences. ","This paper proposes a method to improve the performance of pretrained neural language models (NLM) in the context of natural language understanding tasks. The method is based on the idea of self-improvement, which is an extension of the self-supervised learning framework. The key idea is to train a pretrained NLM on a set of semantically related non-neighboring sentences, and then fine-tune the model on the pretrained pretrained model. The authors show that the proposed method is able to achieve state-of-the-art performance on a number of tasks.","This paper proposes a method to improve the performance of pretrained neural language models (NLM) in the context of natural language understanding tasks. The method is based on the idea of self-improvement, which is an extension of the self-supervised learning framework. The key idea is to train a pretrained NLM on a set of semantically related non-neighboring sentences, and then fine-tune the model on the pretrained pretrained model. The authors show that the proposed method is able to achieve state-of-the-art performance on a number of tasks."
6650,SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"L2O models USED-FOR optimization rules. neural networks USED-FOR L2O models. neural networks USED-FOR optimization rules. meta - training USED-FOR numerical rules. optimization rule USED-FOR L2O model. neural networks USED-FOR numerical rules. holistic symbolic representation and analysis framework USED-FOR L2O. L2O model COMPARE human - designed and tuned optimizers. human - designed and tuned optimizers COMPARE L2O model. large - scale problems EVALUATE-FOR L2O model. Task are Learning to Optimize ( L2O ), optimization procedure, and L2O research. Metric are scalability, and interpretability. OtherScientificTerm is memory overhead. Method is symbolic regression. ",This paper proposes a new method for learning to Optimize (L2O) based on symbolic regression. The method is based on meta-training of neural networks to learn a set of optimization rules that can be used to train a neural network to solve the optimization problem. The proposed method is evaluated on a number of optimization problems and shows that it outperforms human-designed and tuned optimizers. ,This paper proposes a new method for learning to Optimize (L2O) based on symbolic regression. The method is based on meta-training of neural networks to learn a set of optimization rules that can be used to train a neural network to solve the optimization problem. The proposed method is evaluated on a number of optimization problems and shows that it outperforms human-designed and tuned optimizers. 
6675,SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"provable adversarial robustness FEATURE-OF deep neural networks ( DNNs ). provable adversarial robustness USED-FOR static supervised learning tasks. image classification HYPONYM-OF static supervised learning tasks. DNNs USED-FOR real - world adaptive tasks. reinforcement learning ( RL ) HYPONYM-OF real - world adaptive tasks. methods USED-FOR static setting. provable robustness FEATURE-OF RL. RL adversary USED-FOR defense strategy. procedure USED-FOR adaptive RL adversary. worst - case scenario USED-FOR certificates. Pong CONJUNCTION Freeway. Freeway CONJUNCTION Pong. Freeway CONJUNCTION Mountain Car. Mountain Car CONJUNCTION Freeway. Cartpole CONJUNCTION Pong. Pong CONJUNCTION Cartpole. environments EVALUATE-FOR method. robustness guarantees EVALUATE-FOR method. Mountain Car HYPONYM-OF environments. Cartpole HYPONYM-OF environments. Freeway HYPONYM-OF environments. Pong HYPONYM-OF environments. Generic are systems, and attacks. OtherScientificTerm are adversarial attacks, non - adaptive adversary, policy, Neyman - Pearson Lemma, adversarial perturbation, Gaussian noise, policy function, and robustness certificates. Task is randomized smoothing based defenses. Method are smoothingbased certificates, and policy smoothing. ","This paper studies the problem of provable adversarial robustness of deep neural networks (DNNs) in the context of reinforcement learning (RL). The authors propose a new method to improve the robustness guarantee of DNNs against adversarial attacks. The method is based on the Neyman-Pearson Lemma (NP Lemma) and is motivated by the observation that the adversarial perturbation of a DNN can be modeled as a Gaussian noise, which can be used as an adaptive RL adversary. The authors show that the proposed method is able to achieve better robustness guarantees than existing methods. ","This paper studies the problem of provable adversarial robustness of deep neural networks (DNNs) in the context of reinforcement learning (RL). The authors propose a new method to improve the robustness guarantee of DNNs against adversarial attacks. The method is based on the Neyman-Pearson Lemma (NP Lemma) and is motivated by the observation that the adversarial perturbation of a DNN can be modeled as a Gaussian noise, which can be used as an adaptive RL adversary. The authors show that the proposed method is able to achieve better robustness guarantees than existing methods. "
6700,SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"labeled source data CONJUNCTION unlabeled target data. unlabeled target data CONJUNCTION labeled source data. methods USED-FOR predicting the target domain accuracy. labeled source data USED-FOR methods. unlabeled target data USED-FOR methods. method USED-FOR threshold. BREEDS CONJUNCTION CIFAR. CIFAR CONJUNCTION BREEDS. ImageNet CONJUNCTION BREEDS. BREEDS CONJUNCTION ImageNet. WILDS CONJUNCTION ImageNet. ImageNet CONJUNCTION WILDS. CIFAR CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR. ATC COMPARE methods. methods COMPARE ATC. synthetic corruptions CONJUNCTION dataset reproduction. dataset reproduction CONJUNCTION synthetic corruptions. ImageNet CONJUNCTION CIFAR. CIFAR CONJUNCTION ImageNet. WILDS CONJUNCTION BREEDS. BREEDS CONJUNCTION WILDS. model architectures EVALUATE-FOR methods. datasets EVALUATE-FOR ATC. dataset reproduction HYPONYM-OF distribution shifts. WILDS HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. BREEDS HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. ATC COMPARE prior methods. prior methods COMPARE ATC. toy distributions USED-FOR method. Task is Real - world machine learning deployments. Method is Average Thresholded Confidence ( ATC ). OtherScientificTerm are model ’s confidence, and model confidence. Metric is predicting accuracy. Generic are problem, and it. ","This paper proposes a new method for predicting the target domain accuracy. The proposed method is based on the idea of Average Thresholded Confidence (ATC), which is a new metric that measures the difference between the model’s confidence and that of the target data. The authors show that the proposed method outperforms existing methods on several datasets. ","This paper proposes a new method for predicting the target domain accuracy. The proposed method is based on the idea of Average Thresholded Confidence (ATC), which is a new metric that measures the difference between the model’s confidence and that of the target data. The authors show that the proposed method outperforms existing methods on several datasets. "
6725,SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"registration USED-FOR transformation. outliers CONJUNCTION unknown non - rigid deformations. unknown non - rigid deformations CONJUNCTION outliers. outliers FEATURE-OF robustness. partial distribution matching ( PDM ) problem USED-FOR registration problem. method USED-FOR large scale PDM problem. partial Wasserstein-1 ( PW ) discrepancy USED-FOR method. Kantorovich – Rubinstein duality USED-FOR PW discrepancy. partial Wasserstein adversarial network ( PWAN ) USED-FOR PW discrepancy. neural network USED-FOR partial Wasserstein adversarial network ( PWAN ). coherence regularizer USED-FOR non - rigid transformations. coherence regularizer USED-FOR unrealistic deformations. coherence regularizer PART-OF It. PWAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PWAN. point set registration tasks EVALUATE-FOR PWAN. point set registration tasks EVALUATE-FOR PWAN. Generic are task, and network. OtherScientificTerm are discrete distributions, and gradient. ","This paper studies the partial distribution matching (PDM) problem, where the goal is to match two discrete distributions. The authors propose a partial Wasserstein adversarial network (PWAN) to solve the PDM problem. PWAN is based on the Kantorovich-Rubinstein duality of the PW discrepancy between the two distributions, and the authors show that PWAN outperforms state-of-the-art methods on a number of point set registration tasks. ","This paper studies the partial distribution matching (PDM) problem, where the goal is to match two discrete distributions. The authors propose a partial Wasserstein adversarial network (PWAN) to solve the PDM problem. PWAN is based on the Kantorovich-Rubinstein duality of the PW discrepancy between the two distributions, and the authors show that PWAN outperforms state-of-the-art methods on a number of point set registration tasks. "
6750,SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"Hyperparameter optimization ( HPO ) PART-OF machine learning models. transfer learning USED-FOR HPO. approaches COMPARE Deep Kernel Gaussian Process surrogate. Deep Kernel Gaussian Process surrogate COMPARE approaches. Landmark Meta - features ( DKLM ) PART-OF Deep Kernel Gaussian Process surrogate. DKLM USED-FOR similarity between hyperparameter configurations. DKLM USED-FOR contextualized dataset - specific similarity representations. DKLM USED-FOR hyperparameter configurations. method COMPARE stateof - the - art baselines. stateof - the - art baselines COMPARE method. OpenML FEATURE-OF HPO meta - datasets. HPO meta - datasets EVALUATE-FOR DKLM. Generic is it. OtherScientificTerm are hyperparameter evaluations, and evaluated configurations. ","This paper proposes a meta-learning approach for hyperparameter optimization (HPO) based on Landmark Meta-features (DKLM). DKLM is an extension of the Deep Kernel Gaussian Process (DKGP) framework, which is used to learn the similarity between hyperparameters. DKLM can be used as a surrogate for transfer learning, and the authors show that DKLM outperforms DKGP on a number of HPO meta-datasets. ","This paper proposes a meta-learning approach for hyperparameter optimization (HPO) based on Landmark Meta-features (DKLM). DKLM is an extension of the Deep Kernel Gaussian Process (DKGP) framework, which is used to learn the similarity between hyperparameters. DKLM can be used as a surrogate for transfer learning, and the authors show that DKLM outperforms DKGP on a number of HPO meta-datasets. "
6775,SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"Generated data COMPARE real data. real data COMPARE Generated data. technology USED-FOR deep fakes. technology USED-FOR misinformation. 128 - bit fingerprint USED-FOR identifiable models. deep fake detection CONJUNCTION attribution. attribution CONJUNCTION deep fake detection. method USED-FOR deep fake detection. method USED-FOR attribution. fingerprinting mechanism USED-FOR method. Method are deep generative models, deep fake detection methods, and generative models. Generic are work, and technique. OtherScientificTerm is fingerprint. ",This paper proposes a new method for deep fake detection. The key idea is to use a 128-bit fingerprinting mechanism to identify the identifiable models. The proposed method is evaluated on a number of datasets and shows that the proposed method outperforms several baselines. ,This paper proposes a new method for deep fake detection. The key idea is to use a 128-bit fingerprinting mechanism to identify the identifiable models. The proposed method is evaluated on a number of datasets and shows that the proposed method outperforms several baselines. 
6800,SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"Post - hoc explanations USED-FOR black box models. Post - hoc explanations PART-OF classification and regression settings. model agnostic local explanations USED-FOR similarity learners. tabular and text data USED-FOR model agnostic local explanations. tabular and text data USED-FOR similarity learners. method USED-FOR feature attributions. analogies USED-FOR machine learning. analogies USED-FOR explanation. explanation PART-OF machine learning. ( latent ) factors USED-FOR model. feature attributions USED-FOR analogies. submodular FEATURE-OF analogy objective function. Generic are models, and approaches. Method are black box similarity learner, and sentence encoder. OtherScientificTerm are similarity, and complementarity. Task is healthcare utilization application. ","This paper proposes a new method for learning post-hoc explanations for black box models. The method is based on the notion of complementarity, which is a submodular objective function that can be applied to both tabular and text data. The authors show that the proposed method outperforms existing methods on a number of classification and regression tasks. ","This paper proposes a new method for learning post-hoc explanations for black box models. The method is based on the notion of complementarity, which is a submodular objective function that can be applied to both tabular and text data. The authors show that the proposed method outperforms existing methods on a number of classification and regression tasks. "
6825,SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"deep neural networks ( DNN ) USED-FOR adversarial examples. empirical and theoretical defense approaches USED-FOR single ML model. robustness EVALUATE-FOR ensemble protocols. certified robustness EVALUATE-FOR ensemble ML models. ensemble models COMPARE single model. single model COMPARE ensemble models. ensemble models COMPARE single model. single model COMPARE ensemble models. certified robustness EVALUATE-FOR ensemble models. diversified gradient CONJUNCTION confidence margin. confidence margin CONJUNCTION diversified gradient. diversified gradient USED-FOR ensemble models. Ensemble - before - Smoothing strategy USED-FOR bounded model - smoothness analysis. ensemble model COMPARE single base model. single base model COMPARE ensemble model. certified robustness EVALUATE-FOR single base model. certified robustness EVALUATE-FOR ensemble model. lightweight Diversity Regularized Training ( DRT ) USED-FOR certifiably robust ensemble ML models. DRT enhanced ensembles COMPARE single and ensemble ML models. single and ensemble ML models COMPARE DRT enhanced ensembles. certified robustness EVALUATE-FOR single and ensemble ML models. ImageNet datasets EVALUATE-FOR certified L2 - robustness. certified robustness EVALUATE-FOR DRT enhanced ensembles. Method is DNNs. OtherScientificTerm are perturbations, and model - smoothness assumption. ",This paper studies the robustness of ensemble-based deep neural networks (DNNs) against adversarial perturbations. The authors propose a novel ensemble-before-smoothing strategy to improve the certified L2-robustness of DNNs. The proposed method is based on the ensemble-regularized training (DRT) framework. The method is evaluated on ImageNet and CIFAR-10 datasets and shows that it improves the certified robustness compared to single-model and ensemble models. ,This paper studies the robustness of ensemble-based deep neural networks (DNNs) against adversarial perturbations. The authors propose a novel ensemble-before-smoothing strategy to improve the certified L2-robustness of DNNs. The proposed method is based on the ensemble-regularized training (DRT) framework. The method is evaluated on ImageNet and CIFAR-10 datasets and shows that it improves the certified robustness compared to single-model and ensemble models. 
6850,SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"message passing Graph Neural Networks ( GNNs ) USED-FOR learning with graphs. expressive power EVALUATE-FOR higherorder GNNs. strategies CONJUNCTION lower bounds. lower bounds CONJUNCTION strategies. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. recursive pooling COMPARE higher - order GNNs. higher - order GNNs COMPARE recursive pooling. computational complexity EVALUATE-FOR higher - order GNNs. sparsity USED-FOR recursive pooling. computational complexity EVALUATE-FOR recursive pooling. near ) matching information - theoretic lower bound USED-FOR counting subgraphs. graph representations USED-FOR representations of derived ( sub-)graphs. graph representations USED-FOR near ) matching information - theoretic lower bound. lower bounds FEATURE-OF time complexity. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. ","This paper studies the computational complexity of recursive pooling in graph neural networks (GNNs) for learning subgraphs. In particular, the authors consider the case where the number of nodes in a subgraph is large and the pooling technique of local neighborhoods is used to reduce the computational cost. The authors propose a new lower bound on the time complexity of pooling, which is based on the near-matching information-theoretic lower bound. The lower bound is obtained by considering the sparsity of the subgraph, and the authors show that this lower bound can be extended to the case of low-order GNNs. ","This paper studies the computational complexity of recursive pooling in graph neural networks (GNNs) for learning subgraphs. In particular, the authors consider the case where the number of nodes in a subgraph is large and the pooling technique of local neighborhoods is used to reduce the computational cost. The authors propose a new lower bound on the time complexity of pooling, which is based on the near-matching information-theoretic lower bound. The lower bound is obtained by considering the sparsity of the subgraph, and the authors show that this lower bound can be extended to the case of low-order GNNs. "
6875,SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,Pretrained language models ( LMs ) USED-FOR factual knowledge. external knowledge PART-OF pretrained LMs. knowledge integration ( KI ) methods USED-FOR pretrained LMs. external knowledge PART-OF knowledge integration ( KI ) methods. KI methods COMPARE vanilla LMs. vanilla LMs COMPARE KI methods. integration USED-FOR catastrophic forgetting of already learned knowledge. graph convolution operation USED-FOR KI. probe model USED-FOR knowledge - enhanced LMs. Graph Convolution Simulator ( GCS ) HYPONYM-OF probe model. GCS model USED-FOR KI process. it USED-FOR knowledge - enhanced LMs. K - Adapter CONJUNCTION ERNIE. ERNIE CONJUNCTION K - Adapter. ERNIE HYPONYM-OF knowledge - enhanced LMs. K - Adapter HYPONYM-OF knowledge - enhanced LMs. models USED-FOR factual knowledge. complex relational knowledge USED-FOR ERNIE. K - Adapter USED-FOR simple relational knowledge. K - Adapter USED-FOR time - related knowledge. Generic is methods. Method is LMs. OtherScientificTerm is relations. Material is KI corpus. ,"This paper proposes a new knowledge integration (KI) method for pre-trained language models. The proposed method is based on the Graph Convolution Simulator (GCS) model, which is a graph convolutional model that can be used as a probe model for knowledge-integrated LMs. The authors show that the proposed method outperforms existing KI methods on a variety of tasks, including time-related knowledge and relational knowledge. ","This paper proposes a new knowledge integration (KI) method for pre-trained language models. The proposed method is based on the Graph Convolution Simulator (GCS) model, which is a graph convolutional model that can be used as a probe model for knowledge-integrated LMs. The authors show that the proposed method outperforms existing KI methods on a variety of tasks, including time-related knowledge and relational knowledge. "
6900,SP:7e73948421e98307fceb69a316d8a4e7c4926cda,adaptation ( inner loop ) learning rate USED-FOR fast adaptation. adaptation ( inner loop ) learning rate USED-FOR MAML. adaptation learning rate USED-FOR meta - learning. adaptation learning rate FEATURE-OF mixed linear regression. mixed linear regression USED-FOR meta - learning. optimal adaptation learning rates USED-FOR MAML. optimal adaptation learning rates USED-FOR population risk. population risk FEATURE-OF MAML. empirical risk minimization ( ERM ) COMPARE MAML. MAML COMPARE empirical risk minimization ( ERM ). MAML USED-FOR initialization. average distance FEATURE-OF initialization. Metric is adaptation error. OtherScientificTerm is optimal adaptation learning rate. ,"This paper studies the problem of meta-learning in the context of mixed linear regression. The authors show that the optimal adaptation learning rate for MAML is a function of the inner loop learning rate and the population risk. They also show that for the case where the number of samples is small, the adaptation error is bounded by the average distance between the initialization and the optimal learning rate. ","This paper studies the problem of meta-learning in the context of mixed linear regression. The authors show that the optimal adaptation learning rate for MAML is a function of the inner loop learning rate and the population risk. They also show that for the case where the number of samples is small, the adaptation error is bounded by the average distance between the initialization and the optimal learning rate. "
6925,SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"source - domain data USED-FOR adaptation. Source - free domain adaptation ( SFDA ) USED-FOR model. unlabelled data USED-FOR model. labelled data USED-FOR model. methods USED-FOR SFDA. source model USED-FOR feature - space class - separation. entropy - minimization techniques USED-FOR methods. measurement shift HYPONYM-OF domain shift. it USED-FOR features. bottom - up training scheme USED-FOR FR. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. BUFR COMPARE SFDA methods. SFDA methods COMPARE BUFR. calibration CONJUNCTION data efficiency. data efficiency CONJUNCTION calibration. BUFR COMPARE source model. source model COMPARE BUFR. data efficiency EVALUATE-FOR SFDA methods. calibration EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR SFDA methods. data efficiency EVALUATE-FOR BUFR. calibration EVALUATE-FOR BUFR. accuracy EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR BUFR. accuracy EVALUATE-FOR BUFR. Task are classification, and model calibration. OtherScientificTerm are measurement system, source features, and approximate feature distribution. Method are feature - extractor, and Feature Restoration ( FR ). Generic is network. ","This paper proposes a new method for source-free domain adaptation (SFDA) based on the idea of feature restoration (BUFR). The authors propose a bottom-up training scheme to improve the accuracy and calibration of the source model. The authors show that the proposed method outperforms existing methods in terms of accuracy, calibration, and data efficiency. ","This paper proposes a new method for source-free domain adaptation (SFDA) based on the idea of feature restoration (BUFR). The authors propose a bottom-up training scheme to improve the accuracy and calibration of the source model. The authors show that the proposed method outperforms existing methods in terms of accuracy, calibration, and data efficiency. "
6950,SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"distributed learning schema USED-FOR model. Federated learning ( FL ) HYPONYM-OF distributed learning schema. adversarial training ( AT ) USED-FOR centralized learning. learning setting USED-FOR adversarial robustness. adversarial robustness FEATURE-OF non - iid users. batch - normalization statistics USED-FOR propagation approach. method USED-FOR FL remarkable robustness. Material is raw data. Method are FL, and FL techniques. OtherScientificTerm are FL users, highresource users, and low - resource users. Metric is model robustness. Generic is it. Task are FL process, and learning. ",This paper proposes a method to improve the adversarial robustness of federated learning (FL) models. The proposed method is based on batch-normalization statistics. The authors show that adversarial training (AT) can be used to improve robustness to adversarial perturbations in the FL setting. They also show that the proposed method can improve the robustness in the low-resource setting. ,This paper proposes a method to improve the adversarial robustness of federated learning (FL) models. The proposed method is based on batch-normalization statistics. The authors show that adversarial training (AT) can be used to improve robustness to adversarial perturbations in the FL setting. They also show that the proposed method can improve the robustness in the low-resource setting. 
6975,SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"utility function FEATURE-OF game. utility function USED-FOR Existing methods. transformer - like architecture USED-FOR mapping. transformer - like architecture USED-FOR symmetries. network structure inference EVALUATE-FOR methods. method COMPARE methods. methods COMPARE method. network structure inference EVALUATE-FOR method. network games EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. synthetic and real - world data USED-FOR network games. OtherScientificTerm are Strategic interactions, network structure, equilibrium actions, real - world scenarios, and network structure of the game. Task is economics and social sciences. ","This paper proposes a transformer-based method for learning the utility function of a game. The utility function is defined as the sum of the mutual information between two players in the game, and is then used to learn the equilibrium actions of the game. In particular, the authors propose to use a transformer architecture to learn a mapping between the utility functions of the two players. The proposed method is evaluated on synthetic and real-world data. ","This paper proposes a transformer-based method for learning the utility function of a game. The utility function is defined as the sum of the mutual information between two players in the game, and is then used to learn the equilibrium actions of the game. In particular, the authors propose to use a transformer architecture to learn a mapping between the utility functions of the two players. The proposed method is evaluated on synthetic and real-world data. "
7000,SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"heterogeneous graphs USED-FOR relation prediction. ANalogy SubGraph Embedding Learning ( GraphANGEL ) HYPONYM-OF relation prediction framework. graph pattern USED-FOR logical rule. graph pattern USED-FOR explainable predictive models. inductive bias USED-FOR generalization. heterogeneous graph based recommendation CONJUNCTION knowledge graph completion. knowledge graph completion CONJUNCTION heterogeneous graph based recommendation. model COMPARE models. models COMPARE model. knowledge graph completion EVALUATE-FOR model. heterogeneous graph based recommendation EVALUATE-FOR model. knowledge graph completion EVALUATE-FOR models. heterogeneous graph based recommendation EVALUATE-FOR models. model USED-FOR explainable heat maps of attention scores. Material is knowledge graphs. OtherScientificTerm are embeddings, and subgraphs. Task is transductive setting. ","This paper proposes a novel approach for relation prediction based on knowledge graph embedding learning. The proposed approach is based on the idea of graph pattern embedding, which is a logical rule that can be used to explain the generalization ability of a predictive model. The method is evaluated on heterogeneous graph based recommendation, knowledge graph completion, and knowledge graph clustering tasks.","This paper proposes a novel approach for relation prediction based on knowledge graph embedding learning. The proposed approach is based on the idea of graph pattern embedding, which is a logical rule that can be used to explain the generalization ability of a predictive model. The method is evaluated on heterogeneous graph based recommendation, knowledge graph completion, and knowledge graph clustering tasks."
7025,SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"well - labeled datasets CONJUNCTION rare abnormal samples. rare abnormal samples CONJUNCTION well - labeled datasets. Few - shot learning HYPONYM-OF natural images. cross - domain tasks USED-FOR real clinics problems. contrastive learning ( CL ) USED-FOR few - shot system. label - efficient learning CONJUNCTION generalizability. generalizability CONJUNCTION label - efficient learning. contrastive learning ( CL ) CONJUNCTION latent augmentation ( LA ). latent augmentation ( LA ) CONJUNCTION contrastive learning ( CL ). latent augmentation ( LA ) USED-FOR few - shot system. LA USED-FOR semantic variations. CL COMPARE LA. LA COMPARE CL. components USED-FOR label - hungry problems. unlabeled training data USED-FOR components. LA COMPARE baselines. baselines COMPARE LA. supervised learning USED-FOR histology images. models COMPARE supervised learning. supervised learning COMPARE models. CL COMPARE supervised learning. supervised learning COMPARE CL. CL USED-FOR models. ImageNet - like images USED-FOR self - supervised learning. CL COMPARE supervised learning. supervised learning COMPARE CL. generalization EVALUATE-FOR data. generalization EVALUATE-FOR CL. generalization EVALUATE-FOR supervised learning. representation learning CONJUNCTION histological image analysis. histological image analysis CONJUNCTION representation learning. model USED-FOR representation learning. model USED-FOR histological image analysis. Task is few - shot learning in histology images. OtherScientificTerm is manual labels. Material are images, and Histology images. ",This paper proposes a few-shot learning method that combines contrastive learning (CL) and latent augmentation (LA) to improve the performance of self-supervised learning on histology image classification tasks. The authors show that CL and LA outperform supervised learning on ImageNet-like histology images. They also show that the proposed method outperforms supervised learning in the few shot learning setting. ,This paper proposes a few-shot learning method that combines contrastive learning (CL) and latent augmentation (LA) to improve the performance of self-supervised learning on histology image classification tasks. The authors show that CL and LA outperform supervised learning on ImageNet-like histology images. They also show that the proposed method outperforms supervised learning in the few shot learning setting. 
7050,SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"Recurrent neural networks ( RNNs ) USED-FOR irregularly - sampled time series. continuous - time hidden states USED-FOR Recurrent neural networks ( RNNs ). training FEATURE-OF gradient. memory compartment FEATURE-OF continuous - time networks. continuous - time dynamical flow PART-OF RNN. constant error propagation FEATURE-OF memory path. Mixed - MemoryRNNs COMPARE RNN - based counterparts. RNN - based counterparts COMPARE Mixed - MemoryRNNs. long - term dependencies FEATURE-OF non - uniformly sampled data. non - uniformly sampled data EVALUATE-FOR RNN - based counterparts. non - uniformly sampled data EVALUATE-FOR Mixed - MemoryRNNs. Generic are models, and it. Method are RNNs, ODE solver, and Mixed - Memory - RNNs ( mmRNNs ). OtherScientificTerm are hidden state, timecontinuous state, and time - lags. ","This paper studies the problem of learning continuous-time recurrent neural networks (RNNs) for irregularly-sampled time series. The authors propose Mixed-Memory RNNs (MMMRNN), which is an extension of the memory-based RNN framework. The main contribution of the paper is to extend the memory compartment of the RNN to the time-continuous state of the hidden state.  The authors show that the memory of the mixed memory RNN can be used to solve the ODE solver problem.  ","This paper studies the problem of learning continuous-time recurrent neural networks (RNNs) for irregularly-sampled time series. The authors propose Mixed-Memory RNNs (MMMRNN), which is an extension of the memory-based RNN framework. The main contribution of the paper is to extend the memory compartment of the RNN to the time-continuous state of the hidden state.  The authors show that the memory of the mixed memory RNN can be used to solve the ODE solver problem.  "
7075,SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,pre - trained BERT USED-FOR Natural Language Processing ( NLP ) tasks. 1 - bit parameters CONJUNCTION bitwise operations. bitwise operations CONJUNCTION 1 - bit parameters. binarization HYPONYM-OF compression approaches. 1 - bit parameters USED-FOR binarization. computation and memory consumption EVALUATE-FOR binarization. bitwise operations USED-FOR binarization. 1 - bit weight CONJUNCTION embedding. embedding CONJUNCTION 1 - bit weight. embedding CONJUNCTION activation. activation CONJUNCTION embedding. activation HYPONYM-OF BERT. 1 - bit weight HYPONYM-OF BERT. embedding HYPONYM-OF BERT. information degradation CONJUNCTION optimization direction mismatch. optimization direction mismatch CONJUNCTION information degradation. BiBERT USED-FOR performance bottlenecks. BiBERT HYPONYM-OF fully binarized BERT. optimization direction mismatch FEATURE-OF forward and backward propagation. DirectionMatching Distillation ( DMD ) scheme USED-FOR full binarized BERT. Bi - Attention structure USED-FOR representation information statistically. Bi - Attention structure CONJUNCTION DirectionMatching Distillation ( DMD ) scheme. DirectionMatching Distillation ( DMD ) scheme CONJUNCTION Bi - Attention structure. DirectionMatching Distillation ( DMD ) scheme USED-FOR BiBERT. Bi - Attention structure USED-FOR BiBERT. BiBERT COMPARE quantized BERTs. quantized BERTs COMPARE BiBERT. BiBERT COMPARE baseline. baseline COMPARE BiBERT. baseline COMPARE quantized BERTs. quantized BERTs COMPARE baseline. ultra - low bit activations FEATURE-OF quantized BERTs. NLP benchmark EVALUATE-FOR quantized BERTs. NLP benchmark EVALUATE-FOR BiBERT. FLOPs CONJUNCTION model size. model size CONJUNCTION FLOPs. fully binarized BERT model USED-FOR real - world resource - constrained scenarios. method USED-FOR fully binarized BERT model. fully binarized BERT EVALUATE-FOR method. model size EVALUATE-FOR method,"This paper proposes BiBERT, a method for fully binarized BERT models. The proposed method is based on the Bi-Attention structure and the DirectionMatching Distillation (DMD) scheme. The authors show that bi-attention structure can be used to improve the performance of binarization in the context of NLP tasks. They also show that the proposed method can reduce the number of FLOPs and model size.","This paper proposes BiBERT, a method for fully binarized BERT models. The proposed method is based on the Bi-Attention structure and the DirectionMatching Distillation (DMD) scheme. The authors show that bi-attention structure can be used to improve the performance of binarization in the context of NLP tasks. They also show that the proposed method can reduce the number of FLOPs and model size."
7100,SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"method USED-FOR keypoint detection. method USED-FOR instance association. keypoint detection CONJUNCTION instance association. instance association CONJUNCTION keypoint detection. Transformer USED-FOR method. Transformer USED-FOR problems. association information USED-FOR keypoints grouping. self - attention PART-OF Transformer. supervising self - attention USED-FOR multi - person keypoint detection. supervising self - attention USED-FOR instance association. approach USED-FOR supervising self - attention. multi - person keypoint detection CONJUNCTION instance association. instance association CONJUNCTION multi - person keypoint detection. approach USED-FOR instance association. approach USED-FOR multi - person keypoint detection. instance masks USED-FOR self - attention. pre - defined offset vector fields CONJUNCTION embedding. embedding CONJUNCTION pre - defined offset vector fields. embedding CONJUNCTION CNN - based bottom - up models. CNN - based bottom - up models CONJUNCTION embedding. supervised attention matrix USED-FOR instance segmentation. person instance segmentation task EVALUATE-FOR method. COCO multi - person keypoint detection challenge EVALUATE-FOR method. COCO multi - person keypoint detection challenge CONJUNCTION person instance segmentation task. person instance segmentation task CONJUNCTION COCO multi - person keypoint detection challenge. OtherScientificTerm are associative information, naive attention patterns, pairwise attention scores, and self - attention behavior. Method is pixel assignment pipeline. ","This paper proposes a Transformer-based approach for multi-person keypoint detection, instance association and instance segmentation tasks. The key idea is to use the self-attention matrix of the Transformer to learn a pairwise attention score for each instance. The proposed method is evaluated on the COCO multi-people keypoints detection task and the person segmentation task. ","This paper proposes a Transformer-based approach for multi-person keypoint detection, instance association and instance segmentation tasks. The key idea is to use the self-attention matrix of the Transformer to learn a pairwise attention score for each instance. The proposed method is evaluated on the COCO multi-people keypoints detection task and the person segmentation task. "
7125,SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"reinforcement learning ( RL ) USED-FOR sequential decision making. Pareto efficiency FEATURE-OF MV - efficient policies. Generic are existing methods, approach, it, and methods. OtherScientificTerm are variance term, MV trade - off, expected quadratic utility function, Pareto efficient policy, computational difficulties, and gradient estimation of the variance. ",This paper studies the problem of Pareto efficient policy optimization for sequential decision making in reinforcement learning (RL). The authors consider the case where the variance term of the trade-off between the expected quadratic utility function and the expected utility function of the policy is not known. The authors propose a method to estimate the variance of the utility function using gradient estimation of the variance. They show that the proposed method is computationally tractable and can be used to solve the problem. ,This paper studies the problem of Pareto efficient policy optimization for sequential decision making in reinforcement learning (RL). The authors consider the case where the variance term of the trade-off between the expected quadratic utility function and the expected utility function of the policy is not known. The authors propose a method to estimate the variance of the utility function using gradient estimation of the variance. They show that the proposed method is computationally tractable and can be used to solve the problem. 
7150,SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"end - to - end learning USED-FOR communication system. autoencoder USED-FOR communication system. autoencoder USED-FOR end - to - end learning. test - time domain adaptation USED-FOR autoencoder system. fully - trained channel model CONJUNCTION autoencoder. autoencoder CONJUNCTION fully - trained channel model. error rate EVALUATE-FOR autoencoder. method USED-FOR autoencoder. feature transformations USED-FOR channel distribution. feature transformations USED-FOR decoder. feature transformations USED-FOR method. method USED-FOR MDN channel. simulated datasets CONJUNCTION real mmWave wireless channels. real mmWave wireless channels CONJUNCTION simulated datasets. error rate EVALUATE-FOR autoencoder. real mmWave wireless channels EVALUATE-FOR method. simulated datasets EVALUATE-FOR method. method USED-FOR autoencoder. error rate EVALUATE-FOR method. Generic is approach. Method are mixture density network ( MDN ), encoder and decoder neural networks, and MDN channel model. Material is unlabeled data. OtherScientificTerm are wireless link, and source distribution. ",This paper proposes a method for end-to-end learning in wireless communication. The method is based on a mixture density network (MDN) and a fully-trained autoencoder. The authors show that the proposed method is able to achieve better error rates than existing methods. ,This paper proposes a method for end-to-end learning in wireless communication. The method is based on a mixture density network (MDN) and a fully-trained autoencoder. The authors show that the proposed method is able to achieve better error rates than existing methods. 
7175,SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"joint softmax focal loss HYPONYM-OF structural loss. model USED-FOR αNLI. ACC CONJUNCTION AUC. AUC CONJUNCTION ACC. AUC EVALUATE-FOR IMSL. RoBERTa - large pretrained model EVALUATE-FOR IMSL. ACC EVALUATE-FOR IMSL. Task are abductive natural language inference task ( αNLI ), and αNLI task. Method are inference network, interactive language model, and Interactive Model with Structural Loss ( IMSL ). OtherScientificTerm is reasoning abilities. ",This paper proposes an interactive model with Structural Loss (IMSL) for the abductive natural language inference task (αNLI). IMSL is based on the joint softmax focal loss (JFL) and the interactive language model (ILM). The authors show that the proposed model outperforms the state-of-the-art models on the αNLI task. ,This paper proposes an interactive model with Structural Loss (IMSL) for the abductive natural language inference task (αNLI). IMSL is based on the joint softmax focal loss (JFL) and the interactive language model (ILM). The authors show that the proposed model outperforms the state-of-the-art models on the αNLI task. 
7200,SP:17cd72df5fc19398f582d27516fd742b073f79e3,"machine learning USED-FOR safety - critical systems. assessment of uncertainy USED-FOR machine learning. deep neural networks USED-FOR overconfident predictions. certifiable OOD detector CONJUNCTION classifier. classifier CONJUNCTION certifiable OOD detector. classifier PART-OF OOD aware classifier. OOD aware classifier PART-OF method. classifier PART-OF method. certifiable OOD detector PART-OF method. prediction accuracy CONJUNCTION detection. detection CONJUNCTION prediction accuracy. detection performance EVALUATE-FOR non - manipulated OOD data. classifier USED-FOR asymptotic overconfidence problem. classifier COMPARE neural networks. neural networks COMPARE classifier. asymptotic overconfidence problem FEATURE-OF neural networks. Material is OOD data. Task is certifiably adversarially robust OOD detection. OtherScientificTerm are OOD samples, and in - distribution. ","This paper proposes a method for certifiably adversarially robust out-of-distribution (OOD) detection. The method is based on the idea of OOD-aware classifiers, which is an extension of the OOD detector. The authors show that the proposed method is able to detect OOD samples in a more robust manner than the existing OOD detection methods. They also show that their method can be applied to the asymptotic overconfidence problem.","This paper proposes a method for certifiably adversarially robust out-of-distribution (OOD) detection. The method is based on the idea of OOD-aware classifiers, which is an extension of the OOD detector. The authors show that the proposed method is able to detect OOD samples in a more robust manner than the existing OOD detection methods. They also show that their method can be applied to the asymptotic overconfidence problem."
7225,SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"Deep Neural Networks ( DNNs ) USED-FOR transfer attacks. query - free black - box setting FEATURE-OF transfer attacks. white - box surrogate models CONJUNCTION black - box victim models. black - box victim models CONJUNCTION white - box surrogate models. datasets USED-FOR surrogate models. method USED-FOR classification information. Image Classification Eraser ( ICE ) USED-FOR classification information. Image Classification Eraser ( ICE ) HYPONYM-OF method. Cifar-10 CONJUNCTION Cifar-100. Cifar-100 CONJUNCTION Cifar-10. Cifar-100 CONJUNCTION TieredImageNet. TieredImageNet CONJUNCTION Cifar-100. ICE USED-FOR GTA problem. Cifar-10 EVALUATE-FOR ICE. TieredImageNet EVALUATE-FOR ICE. transfer attack methods USED-FOR GTA problem. transfer attack methods COMPARE ICE. ICE COMPARE transfer attack methods. Task are transfer attack, and Generalized Transferable Attack ( GTA ) problem. Generic are dataset, and them. Method is victim model. ","This paper studies the problem of generalized transferable attack (GTA) in the query-free black-box setting. The authors propose Image Classification Eraser (ICE), a method to remove the classification information from the target model in order to improve the transferability of the victim model. The proposed method is evaluated on CIFAR-10, TieredImageNet, and Cifar-100 datasets. ","This paper studies the problem of generalized transferable attack (GTA) in the query-free black-box setting. The authors propose Image Classification Eraser (ICE), a method to remove the classification information from the target model in order to improve the transferability of the victim model. The proposed method is evaluated on CIFAR-10, TieredImageNet, and Cifar-100 datasets. "
7250,SP:2e0447c741a3f09be1095633d870200355211260,discriminative PrLM USED-FOR contextualized representation. robustness EVALUATE-FOR PrLMs. pre - training methods USED-FOR false negative predictions. pre - training methods USED-FOR pre - training language models. false negative issue FEATURE-OF discriminative PrLMs. false negative predictions FEATURE-OF gradient updates. Generic is model. Material is GLUE and SQuAD benchmarks. ,This paper studies the problem of false negative predictions in pre-training language models. The authors show that the discriminative PrLM model is susceptible to false negative prediction due to the gradient updates of the pre-trained language model. The paper also shows that the false negative issue can be alleviated by the use of gradient updates. ,This paper studies the problem of false negative predictions in pre-training language models. The authors show that the discriminative PrLM model is susceptible to false negative prediction due to the gradient updates of the pre-trained language model. The paper also shows that the false negative issue can be alleviated by the use of gradient updates. 
7275,SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"semi - supervised learning USED-FOR real - world settings. ORCA HYPONYM-OF end - to - end approach. uncertainty adaptive margin USED-FOR ORCA. discriminability FEATURE-OF model. discriminability EVALUATE-FOR ORCA. image classification datasets CONJUNCTION single - cell dataset. single - cell dataset CONJUNCTION image classification datasets. single - cell dataset EVALUATE-FOR ORCA. ORCA COMPARE baselines. baselines COMPARE ORCA. seen CONJUNCTION novel classes. novel classes CONJUNCTION seen. image classification datasets EVALUATE-FOR ORCA. image classification datasets EVALUATE-FOR baselines. single - cell dataset EVALUATE-FOR baselines. ORCA USED-FOR novel classes. novel classes PART-OF ImageNet dataset. seen EVALUATE-FOR ORCA. ImageNet dataset EVALUATE-FOR ORCA. Material are unlabeled test data, labeled training data, open - world semi - supervised learning setting, and labeled and unlabeled data. Generic is assumption. Task is class distribution mismatch problem. OtherScientificTerm is prior knowledge. ","This paper proposes an end-to-end approach for semi-supervised learning in the open-world setting. The proposed method is based on the idea of uncertainty adaptive margin (ORCA), which is an extension of previous work on the class distribution mismatch problem. The main contribution of this work is the use of uncertainty-adaptive margin to improve the discriminability of the model. The authors show that the proposed method outperforms baselines on the ImageNet dataset and single-cell dataset. ","This paper proposes an end-to-end approach for semi-supervised learning in the open-world setting. The proposed method is based on the idea of uncertainty adaptive margin (ORCA), which is an extension of previous work on the class distribution mismatch problem. The main contribution of this work is the use of uncertainty-adaptive margin to improve the discriminability of the model. The authors show that the proposed method outperforms baselines on the ImageNet dataset and single-cell dataset. "
7300,SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"SLIM - QN HYPONYM-OF light stochastic quasi - Newton optimizer. second - order methods USED-FOR large - scale DNNs. SLIM - QN USED-FOR second - order methods. L - BFGS HYPONYM-OF stochastic training. BFGS update rule USED-FOR Hessian inverse. gradients USED-FOR BFGS update rule. BFGS update rule USED-FOR SLIM - QN. momentum CONJUNCTION adaptive damping mechanism. adaptive damping mechanism CONJUNCTION momentum. momentum USED-FOR Hessian updates. adaptive damping mechanism USED-FOR SLIM - QN. momentum USED-FOR SLIM - QN. SLIM - QN USED-FOR stable convergence. SLIM - QN USED-FOR stochastic setting. convergence EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE second - order methods. second - order methods COMPARE SLIM - QN. compute and memory overhead EVALUATE-FOR second - order methods. compute and memory overhead EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE SGD. SGD COMPARE SLIM - QN. large datasets EVALUATE-FOR SLIM - QN. near optimal accuracy EVALUATE-FOR SGD. wall - clock time EVALUATE-FOR SGD. near optimal accuracy EVALUATE-FOR SLIM - QN. ImageNet HYPONYM-OF large datasets. compute resources USED-FOR SGD. compute resources USED-FOR SLIM - QN. SLIM - QN USED-FOR non - convolutional architectures. Transformers HYPONYM-OF non - convolutional architectures. Metric is computational cost. OtherScientificTerm are Hessian matrix, KFAC, and convergence instability. ",This paper proposes a light stochastic quasi-Newton optimizer (SLIM-QN) for learning deep neural networks (DNNs). The proposed method is based on the L-BFGS update rule and the adaptive damping mechanism. The authors show that the proposed method outperforms existing second-order methods in terms of convergence and compute and memory overhead. ,This paper proposes a light stochastic quasi-Newton optimizer (SLIM-QN) for learning deep neural networks (DNNs). The proposed method is based on the L-BFGS update rule and the adaptive damping mechanism. The authors show that the proposed method outperforms existing second-order methods in terms of convergence and compute and memory overhead. 
7325,SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"Graph Neural Networks ( GNNs ) USED-FOR graph - related tasks. GNNs USED-FOR problems. redundant components PART-OF large graphs. pre - processing step USED-FOR graph. node or edge removals USED-FOR inference. LocalitySensitive Pruning ( LSP ) USED-FOR graph pruning. systematic method USED-FOR graph pruning. Locality - Sensitive Hashing USED-FOR LocalitySensitive Pruning ( LSP ). Locality - Sensitive Hashing USED-FOR systematic method. pruning COMPARE pruning strategies. pruning strategies COMPARE pruning. locality properties USED-FOR pruning. local graph properties USED-FOR pruning. synthetic and real - world datasets EVALUATE-FOR LSP. LSP USED-FOR edges. edges PART-OF large graphs. LSP USED-FOR large graphs. Task is real - world problems. OtherScientificTerm are real - world graphs, and sparsified graph. Method is GNNs layers. ",This paper proposes a systematic method for graph pruning. The proposed method is based on locality sensitive hashing (LSPH) which is a pre-processing step for node or edge removals. The method is evaluated on both synthetic and real-world datasets. The results show that the proposed method outperforms existing pruning methods. ,This paper proposes a systematic method for graph pruning. The proposed method is based on locality sensitive hashing (LSPH) which is a pre-processing step for node or edge removals. The method is evaluated on both synthetic and real-world datasets. The results show that the proposed method outperforms existing pruning methods. 
7350,SP:c5e024f4e2079586298519ca868630efd7579eca,"Data augmentation USED-FOR contrastive self - supervised learning. identitydistinctive information FEATURE-OF R(x ). it PART-OF R(x ). VAE ’s bottleneck space FEATURE-OF G(x ). identity - disentangled adversarial augmentation ( IDAA ) USED-FOR self - supervised learning methods. benchmark datasets EVALUATE-FOR IDAA. efficiency CONJUNCTION generalization performance. generalization performance CONJUNCTION efficiency. efficiency EVALUATE-FOR IDAA. generalization performance EVALUATE-FOR IDAA. dataset USED-FOR IDAA. Generic is augmentations. Task is ineffective learning. Method are adversarial augmentation method, and contrastive learning. OtherScientificTerm are hard positives / negatives, variational auto - encoder ( VAE ) reconstruction, VAE objective, augmentation, and sample identity. ",This paper proposes an adversarial augmentation method for self-supervised learning. The proposed method is based on identity-disentangled adversarial augmentations (IDAA). IDAA is a VAE-based method that learns the identity-distinctive information of the input data. The authors show that IDAA improves the efficiency and generalization performance of the proposed method. ,This paper proposes an adversarial augmentation method for self-supervised learning. The proposed method is based on identity-disentangled adversarial augmentations (IDAA). IDAA is a VAE-based method that learns the identity-distinctive information of the input data. The authors show that IDAA improves the efficiency and generalization performance of the proposed method. 
7375,SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"tests USED-FOR distribution shifts. these USED-FOR arbitrary shifts. non - sequential methods USED-FOR these. method USED-FOR harmful shifts. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. sequential tools USED-FOR risk function of interest. calibration HYPONYM-OF risk function of interest. accuracy HYPONYM-OF risk function of interest. aggregation of statistical evidence USED-FOR tracking process. constructing time - uniform confidence sequences USED-FOR aggregation of statistical evidence. simulated and real datasets EVALUATE-FOR framework. Method is machine learning models. OtherScientificTerm are data distribution, and benign shifts. Generic is model. Metric is false alarm rate. ","This paper proposes a new method for detecting distributional shifts in machine learning models. The proposed method is based on the idea of time-uniform confidence sequences (TUCs), which can be seen as sequential tools for detecting the shift in the data distribution. TUCs can be used to track the distributional shift in a given model. The authors show that the proposed method outperforms existing methods on both simulated and real-world datasets.","This paper proposes a new method for detecting distributional shifts in machine learning models. The proposed method is based on the idea of time-uniform confidence sequences (TUCs), which can be seen as sequential tools for detecting the shift in the data distribution. TUCs can be used to track the distributional shift in a given model. The authors show that the proposed method outperforms existing methods on both simulated and real-world datasets."
7400,SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,Neural networks USED-FOR dynamics of diverse physical systems. neural implicit representations USED-FOR appearance modeling. neural implicit representations CONJUNCTION neural ordinary differential equations ( ODEs ). neural ordinary differential equations ( ODEs ) CONJUNCTION neural implicit representations. neural ordinary differential equations ( ODEs ) USED-FOR interpretable physical models. visual observations USED-FOR interpretable physical models. neural implicit representations USED-FOR processing of high - resolution videos. neural implicit representations USED-FOR synthesis of photo - realistic imagery. processing of high - resolution videos CONJUNCTION synthesis of photo - realistic imagery. synthesis of photo - realistic imagery CONJUNCTION processing of high - resolution videos. model COMPARE approaches. approaches COMPARE model. model USED-FOR physical parameters. large training datasets USED-FOR approaches. embedded neural ODE USED-FOR identification of interpretable physical parameters. known parametric form FEATURE-OF embedded neural ODE. scenes USED-FOR photo - realistic rendering. physical parameters USED-FOR scenes. physical parameters USED-FOR photo - realistic rendering. method USED-FOR physical parameters. real - world videos USED-FOR method. pendulum motion HYPONYM-OF real - world videos. real - world videos USED-FOR physical parameters. physical parameters USED-FOR reconstruction. model USED-FOR metric length of the pendulum. monocular video USED-FOR model. monocular video USED-FOR metric length of the pendulum. Generic is they. Material is high - resolution videos. Task is long - term prediction in state space. OtherScientificTerm is state space. Metric is relative error. ,This paper proposes a method for learning physical parameters from videos. The method is based on neural ODEs and is able to learn a parametric form of the pendulum motion. The authors show that the proposed method outperforms existing methods on a number of real-world datasets. They also show that their method can be applied to high-resolution videos.,This paper proposes a method for learning physical parameters from videos. The method is based on neural ODEs and is able to learn a parametric form of the pendulum motion. The authors show that the proposed method outperforms existing methods on a number of real-world datasets. They also show that their method can be applied to high-resolution videos.
7425,SP:51efd1451343f4994d857daa5490e299b812bc2d,"abrupt ( discontinuous ) context changes CONJUNCTION Markovian context evolution. Markovian context evolution CONJUNCTION abrupt ( discontinuous ) context changes. Bayesian approach CONJUNCTION variational inference. variational inference CONJUNCTION Bayesian approach. Bayesian approach USED-FOR it. variational inference USED-FOR it. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR model learning. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR Markov process modeling. context distillation procedure USED-FOR spurious contexts. optimal policy USED-FOR policy learning. RL algorithms USED-FOR policy learning. state - of - the - art methods USED-FOR frameworks. Generic are case, components, and approach. OtherScientificTerm are context cardinality assumption, and drone. ",This paper studies the problem of context distillation in the context cardinality setting. The authors propose a new framework for learning the optimal policy in this setting based on the sticky Hierarchical Dirichlet Process (HDP) prior. They show that the proposed framework is able to learn an optimal policy for a given set of contexts. They also show that their framework can be used to learn a new policy for any given context. ,This paper studies the problem of context distillation in the context cardinality setting. The authors propose a new framework for learning the optimal policy in this setting based on the sticky Hierarchical Dirichlet Process (HDP) prior. They show that the proposed framework is able to learn an optimal policy for a given set of contexts. They also show that their framework can be used to learn a new policy for any given context. 
7450,SP:ea167b126212b2092bc1190d7f8376bf7c54a888,Knowledge enriched language representation learning USED-FOR knowledge - intensive NLP tasks. monolingual knowledge graph data USED-FOR knowledge based language models. framework USED-FOR knowledge based multilingual language models ( KMLMs ). pretraining tasks USED-FOR knowledge learning. language models USED-FOR logical patterns. intraand inter - sentence structures USED-FOR pretraining tasks. intraand inter - sentence structures FEATURE-OF data. language models USED-FOR factual knowledge. factual knowledge retrieval CONJUNCTION relation classification. relation classification CONJUNCTION factual knowledge retrieval. named entity recognition CONJUNCTION factual knowledge retrieval. factual knowledge retrieval CONJUNCTION named entity recognition. pretrained KMLMs USED-FOR knowledge - intensive cross - lingual NLP tasks. relation classification CONJUNCTION task. task CONJUNCTION relation classification. task HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. logic reasoning HYPONYM-OF task. named entity recognition HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. relation classification HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. factual knowledge retrieval HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. Material is Wikidata knowledge graphs. Method is pretrained language models. ,"This paper proposes a framework for knowledge-based multilingual language models (KMLMs) that can be used for knowledge learning in cross-lingual NLP tasks. The proposed framework is based on the Wikidata knowledge graph dataset, which contains monolingual knowledge graph data from multiple languages. The authors show that the proposed framework can be applied to a number of knowledge-intensive tasks, such as relation classification, factual knowledge retrieval, and named entity recognition. ","This paper proposes a framework for knowledge-based multilingual language models (KMLMs) that can be used for knowledge learning in cross-lingual NLP tasks. The proposed framework is based on the Wikidata knowledge graph dataset, which contains monolingual knowledge graph data from multiple languages. The authors show that the proposed framework can be applied to a number of knowledge-intensive tasks, such as relation classification, factual knowledge retrieval, and named entity recognition. "
7475,SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"agents USED-FOR altruistic behaviour. task - agnostic manner USED-FOR altruistic behaviour. multi - agent environments EVALUATE-FOR approach. unsupervised agents COMPARE them. them COMPARE unsupervised agents. Method are artificial agents, reinforcement learning agents, altruistic agent, and human agents. OtherScientificTerm are external supervision, and altruistic agent ’s behaviour. Generic is concept. ","This paper proposes a method for learning an agent that is capable of performing altruistic behaviour in multi-agent reinforcement learning. The proposed method is based on the idea that an agent can be trained in a task-agnostic manner to learn to behave in a way that is similar to that of a human agent. The authors show that the proposed method can be applied to a variety of tasks, including reinforcement learning, reinforcement learning with reinforcement learning and reinforcement learning without supervision. They also show that their method is able to learn an agent to perform altruistic behavior in the presence of external supervision.","This paper proposes a method for learning an agent that is capable of performing altruistic behaviour in multi-agent reinforcement learning. The proposed method is based on the idea that an agent can be trained in a task-agnostic manner to learn to behave in a way that is similar to that of a human agent. The authors show that the proposed method can be applied to a variety of tasks, including reinforcement learning, reinforcement learning with reinforcement learning and reinforcement learning without supervision. They also show that their method is able to learn an agent to perform altruistic behavior in the presence of external supervision."
7500,SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"Neural Tangent Kernel USED-FOR neural networks. finite - width neural networks USED-FOR double descent. interpolation threshold FEATURE-OF double descent behaviour. optimum FEATURE-OF Hessian. loss function USED-FOR double descent. neural networks CONJUNCTION Hessian spectra. Hessian spectra CONJUNCTION neural networks. OtherScientificTerm are Double descent, and population loss. Generic is models. Method are linear and kernel regression models, influence functions, and parametric model. ","This paper studies the double descent of neural networks in the context of neural tangent kernel regression. The authors show that under certain assumptions, double descent in neural networks can be seen as a function of the population loss of the neural network. They also show that the Hessian of a neural network can be approximated by a parametric model. They show that double descent can be viewed as an interpolation of the mean and covariance of the two Hessian functions. They then show that this interpolation threshold can be used to define a new Hessian for double descent. ","This paper studies the double descent of neural networks in the context of neural tangent kernel regression. The authors show that under certain assumptions, double descent in neural networks can be seen as a function of the population loss of the neural network. They also show that the Hessian of a neural network can be approximated by a parametric model. They show that double descent can be viewed as an interpolation of the mean and covariance of the two Hessian functions. They then show that this interpolation threshold can be used to define a new Hessian for double descent. "
7525,SP:b485114712055f39a7afb951dbc3db482ff523fd,"Graph convolutional networks ( GCNs ) USED-FOR graph - structured data. over - smoothing problem FEATURE-OF deep GCNs. expressive power CONJUNCTION trainability. trainability CONJUNCTION expressive power. trainability CONJUNCTION optimization perspective. optimization perspective CONJUNCTION trainability. expressive power CONJUNCTION optimization perspective. optimization perspective CONJUNCTION expressive power. expressive power EVALUATE-FOR deep GCNs. expressivity COMPARE trainability. trainability COMPARE expressivity. Graph Neural Tangent Kernel ( GNTK ) USED-FOR optimization trajectory. Graph Neural Tangent Kernel ( GNTK ) USED-FOR wide GCNs. gradient descent USED-FOR wide GCNs. Graph Neural Tangent Kernel ( GNTK ) USED-FOR gradient descent. gradient descent USED-FOR optimization trajectory. asymptotic behaviors USED-FOR dropping trainability. dropping trainability FEATURE-OF wide and deep GCNs. exponential rate FEATURE-OF optimization process. asymptotic behaviors FEATURE-OF GNTK. large depth FEATURE-OF asymptotic behaviors. exponential rate FEATURE-OF dropping trainability. large depth FEATURE-OF GNTK. exponential rate FEATURE-OF wide and deep GCNs. theoretical framework USED-FOR residual connection - based techniques. residual connection - based techniques USED-FOR exponential decay of trainability. method COMPARE counterparts. counterparts COMPARE method. infinite - width and finite - width FEATURE-OF counterparts. Method are node representations, gradient descentbased optimizer, and DropEdge. OtherScientificTerm are expressive space, and exponential decay problem. ","This paper studies the exponential decay of trainability of deep GCNs in the expressive space. The authors propose a new method, called DropEdge, which is based on gradient descent and graph neural tangent kernel (GNTK). The authors show that DropEdge outperforms existing methods in terms of expressive power and drop-rate. They also provide a theoretical analysis of the asymptotic behavior of DropEdge.","This paper studies the exponential decay of trainability of deep GCNs in the expressive space. The authors propose a new method, called DropEdge, which is based on gradient descent and graph neural tangent kernel (GNTK). The authors show that DropEdge outperforms existing methods in terms of expressive power and drop-rate. They also provide a theoretical analysis of the asymptotic behavior of DropEdge."
7550,SP:25a92b3583afdc6892e59f1e769125d52c8011af,Computer vision methods USED-FOR first - order dynamics. optical flow HYPONYM-OF first - order dynamics. acceleration HYPONYM-OF higher - order changes. blood pressure CONJUNCTION arterial disease. arterial disease CONJUNCTION blood pressure. second derivative USED-FOR blood pressure. second derivative USED-FOR arterial disease. heart rate HYPONYM-OF summary statistics. videos USED-FOR cardiac measurements. waveform morphology USED-FOR clinically impactful scenarios. accuracy EVALUATE-FOR waveform morphology. loss function USED-FOR neural models. neural models USED-FOR higher - order dynamics. second - derivative inputs USED-FOR second - order dynamics. model USED-FOR left ventricle ejection time ( LVET ) intervals. second derivative PART-OF training procedure. second derivative USED-FOR model. second derivative FEATURE-OF vital sign signals. OtherScientificTerm is cardiac pulse. Task is camera - based vital sign measurement. ,"This paper proposes a method for learning second-order dynamics of cardiac pulse signals from videos. The method is based on the second derivative of the cardiac pulse signal, which is used to train a neural network to predict the second order dynamics of the pulse signal. The authors show that the proposed method is able to learn the second derivatives of pulse signals in a way that is comparable to the first-order ones. They also show that their method can be used to learn a second derivative for the left ventricle ejection time (LVET) intervals. ","This paper proposes a method for learning second-order dynamics of cardiac pulse signals from videos. The method is based on the second derivative of the cardiac pulse signal, which is used to train a neural network to predict the second order dynamics of the pulse signal. The authors show that the proposed method is able to learn the second derivatives of pulse signals in a way that is comparable to the first-order ones. They also show that their method can be used to learn a second derivative for the left ventricle ejection time (LVET) intervals. "
7575,SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"simple interactions USED-FOR human language. architecture USED-FOR communication system. symbolic mapping PART-OF communication system. symbolic mapping HYPONYM-OF architecture. symbolic mapping USED-FOR language learning. referential games USED-FOR symbolic mapping. symbolic mapping USED-FOR language learning. process USED-FOR multi - agent language learning. simplicity CONJUNCTION complexity. complexity CONJUNCTION simplicity. Task are emergent communication, and vocabulary expansion. OtherScientificTerm are language, and compositional and symmetric language. Material is dialog games. ",This paper proposes a new communication system for multi-agent language learning. The proposed system is based on a symbolic mapping between referential games and dialog games. The authors show that the proposed system can be used to improve the communication efficiency of multi agent language learning in the context of emergent communication. They also show that their system is able to learn a compositional and symmetric language that is more expressive than the state-of-the-art. ,This paper proposes a new communication system for multi-agent language learning. The proposed system is based on a symbolic mapping between referential games and dialog games. The authors show that the proposed system can be used to improve the communication efficiency of multi agent language learning in the context of emergent communication. They also show that their system is able to learn a compositional and symmetric language that is more expressive than the state-of-the-art. 
7600,SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,Robotic agents USED-FOR domestic chores. natural language directives USED-FOR Robotic agents. hierarchical modular approach USED-FOR agents. hierarchy FEATURE-OF policy. language instructions USED-FOR subgoals. navigation policy CONJUNCTION independent interaction policies. independent interaction policies CONJUNCTION navigation policy. master policy USED-FOR agent ’s navigation. interaction policy USED-FOR object masks. object masks USED-FOR manipulation actions. interaction policy USED-FOR manipulation actions. ALFRED benchmark EVALUATE-FOR hierarchical agent. OtherScientificTerm is divide - andconquer manner. Method is Compositional Reasoning. ,"This paper proposes a hierarchical modular approach for multi-agent reinforcement learning. The authors propose a hierarchical approach to learn a set of subgoals that can be used to guide the agent’s navigation and interaction policies. The sub-goals are composed of a navigation policy, an interaction policy, and an object mask policy. The interaction policy is used for navigation and the object mask is used to control manipulation actions. The proposed approach is evaluated on the ALFRED benchmark and shows promising results.","This paper proposes a hierarchical modular approach for multi-agent reinforcement learning. The authors propose a hierarchical approach to learn a set of subgoals that can be used to guide the agent’s navigation and interaction policies. The sub-goals are composed of a navigation policy, an interaction policy, and an object mask policy. The interaction policy is used for navigation and the object mask is used to control manipulation actions. The proposed approach is evaluated on the ALFRED benchmark and shows promising results."
7625,SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"relationship USED-FOR model. Nuisance - Randomized Distillation ( NURD ) USED-FOR predictive models. nuisance - label relationship FEATURE-OF distributions. nuisance - randomized distribution HYPONYM-OF distribution. representations COMPARE representations. representations COMPARE representations. NURD USED-FOR representation. distribution PART-OF nuisance - varying family. NURD USED-FOR models. models USED-FOR pneumonia. NURD USED-FOR pneumonia. non - lung patches USED-FOR NURD. tasks EVALUATE-FOR NURD. non - lung patches USED-FOR nuisance. spurious correlations USED-FOR models. tasks EVALUATE-FOR NURD. chest X - ray classification EVALUATE-FOR NURD. chest X - ray classification HYPONYM-OF tasks. Task is prediction problems. OtherScientificTerm are nuisance variable, covariates, and background. Material is natural images. ","This paper studies the problem of nuisance-randomized distillation (NURD) for predictive models. NURD is an approach to learn a representation of the nuisance-label relationship between a model and its covariates. The authors propose a novel family of distributions, called nuisance-variance-variant distributions, that can be used to learn representations that are more robust to spurious correlations between the model and the covariates, and can be applied to a wide range of datasets. The proposed approach is evaluated on a variety of datasets, including chest X-ray classification and pneumonia detection. The results show that the proposed approach outperforms existing methods in terms of robustness and robustness against spurious correlations.","This paper studies the problem of nuisance-randomized distillation (NURD) for predictive models. NURD is an approach to learn a representation of the nuisance-label relationship between a model and its covariates. The authors propose a novel family of distributions, called nuisance-variance-variant distributions, that can be used to learn representations that are more robust to spurious correlations between the model and the covariates, and can be applied to a wide range of datasets. The proposed approach is evaluated on a variety of datasets, including chest X-ray classification and pneumonia detection. The results show that the proposed approach outperforms existing methods in terms of robustness and robustness against spurious correlations."
7650,SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"computer vision models USED-FOR predefined categories. natural language USED-FOR supervision. InfoNCE loss USED-FOR model. model USED-FOR text captions. images CONJUNCTION text captions. text captions CONJUNCTION images. image - text pairs USED-FOR CLIP. Optimal TransporT distillation USED-FOR zero - shot Recognition. online entropic optimal transport USED-FOR soft image - text match. online entropic optimal transport USED-FOR contrastive learning. soft image - text match USED-FOR contrastive learning. Optimal TransporT distillation USED-FOR OTTER. online entropic optimal transport USED-FOR OTTER. pretrained image and text encoders USED-FOR models. image text pairs USED-FOR models. OTTER USED-FOR models. label smoothing CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION label smoothing. InfoNCE loss CONJUNCTION label smoothing. label smoothing CONJUNCTION InfoNCE loss. InfoNCE loss COMPARE OTTER. OTTER COMPARE InfoNCE loss. OTTER COMPARE baselines. baselines COMPARE OTTER. Google Open Images CONJUNCTION multi - labeled ImageNet. multi - labeled ImageNet CONJUNCTION Google Open Images. label smoothing COMPARE OTTER. OTTER COMPARE label smoothing. knowledge distillation COMPARE OTTER. OTTER COMPARE knowledge distillation. Google Open Images EVALUATE-FOR zero - shot evaluation. zero - shot evaluation EVALUATE-FOR baselines. zero - shot evaluation EVALUATE-FOR OTTER. multi - labeled ImageNet EVALUATE-FOR OTTER. OTTER COMPARE baselines. baselines COMPARE OTTER. dataset / architecture settings EVALUATE-FOR OTTER. OtherScientificTerm are visual concepts, and supervised "" gold "" labels. ",This paper proposes Optimal TransporT distillation (OTTER) for zero-shot Recognition (ZR). OTTER is based on the idea of online entropic optimal transport for soft image-text match and contrastive learning. OTTER achieves state-of-the-art performance on CLIP and multi-label ImageNet datasets.,This paper proposes Optimal TransporT distillation (OTTER) for zero-shot Recognition (ZR). OTTER is based on the idea of online entropic optimal transport for soft image-text match and contrastive learning. OTTER achieves state-of-the-art performance on CLIP and multi-label ImageNet datasets.
7675,SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,Pix2Seq USED-FOR object detection. prior knowledge USED-FOR task. prior knowledge USED-FOR approaches. language modeling task USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. class labels HYPONYM-OF Object descriptions. bounding boxes HYPONYM-OF Object descriptions. it COMPARE detection algorithms. detection algorithms COMPARE it. task - specific data augmentations USED-FOR approach. COCO dataset EVALUATE-FOR detection algorithms. COCO dataset EVALUATE-FOR approach. COCO dataset EVALUATE-FOR it. Pix2Seq framework USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. ,"This paper proposes Pix2Seq, a framework for object detection based on a language modeling task. The proposed framework is based on the idea of augmenting an image with task-specific data augmentations. The method is evaluated on the COCO dataset and shows that the proposed method outperforms the state-of-the-art.","This paper proposes Pix2Seq, a framework for object detection based on a language modeling task. The proposed framework is based on the idea of augmenting an image with task-specific data augmentations. The method is evaluated on the COCO dataset and shows that the proposed method outperforms the state-of-the-art."
7700,SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"Deep vision models PART-OF visual reinforcement learning ( RL ). visual reinforcement learning ( RL ) USED-FOR policy networks. Deep vision models USED-FOR policy networks. hierarchical reasoning PART-OF stage - wise approach. geometric and numerical symbols CONJUNCTION operators. operators CONJUNCTION geometric and numerical symbols. approach USED-FOR policy network. approach USED-FOR interpretable symbolic policy. policy network USED-FOR interpretable symbolic policy. geometric and numerical symbols PART-OF interpretable symbolic policy. operators PART-OF interpretable symbolic policy. policy regression algorithm USED-FOR symbolic rules. RoundTourMix HYPONYM-OF policy regression algorithm. distilled symbolic policy COMPARE CNN based RL agents. CNN based RL agents COMPARE distilled symbolic policy. symbolic distillation approach USED-FOR CNN policy. policy distillation USED-FOR geometric relations. numerical state USED-FOR Detected bounding box Velocity. CNN policy network knowledge USED-FOR symbolic policy. Pong CONJUNCTION CircusCharlie. CircusCharlie CONJUNCTION Pong. Airstriker - Genesis CONJUNCTION Pong. Pong CONJUNCTION Airstriker - Genesis. CircusCharlie CONJUNCTION Seaquest. Seaquest CONJUNCTION CircusCharlie. Generic is policies. OtherScientificTerm are interpretability, input distribution shifts, and teacher - student. Method are end - to - end learning pipeline, and symbolic distillation. ","This paper proposes a symbolic distillation method for visual reinforcement learning. The method is based on hierarchical reasoning and uses a policy regression algorithm to learn symbolic rules. The proposed method is evaluated on a number of visual RL tasks, including Airstriker-Genesis, CircusCharlie, Seaquest, and Pong.","This paper proposes a symbolic distillation method for visual reinforcement learning. The method is based on hierarchical reasoning and uses a policy regression algorithm to learn symbolic rules. The proposed method is evaluated on a number of visual RL tasks, including Airstriker-Genesis, CircusCharlie, Seaquest, and Pong."
7725,SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"coarse - level object arrangements ( posture ) CONJUNCTION fine - grained level styling ( identity ). fine - grained level styling ( identity ) CONJUNCTION coarse - level object arrangements ( posture ). exemplar sources USED-FOR fine - grained level styling ( identity ). techniques PART-OF StyleGAN2. techniques USED-FOR PIVQGAN. generator USED-FOR pose - identity disentanglement. VQSN module USED-FOR shaping and composition information. GANInversion encoder CONJUNCTION generator. generator CONJUNCTION GANInversion encoder. self - supervision methods USED-FOR GANInversion encoder. joint - training scheme USED-FOR generator. joint - training scheme CONJUNCTION self - supervision methods. self - supervision methods CONJUNCTION joint - training scheme. joint - training scheme USED-FOR GANInversion encoder. self - supervision methods USED-FOR generator. one CONJUNCTION other. other CONJUNCTION one. other USED-FOR identity. one USED-FOR pose. one HYPONYM-OF ones. training scheme USED-FOR VQSN module. VQSN module USED-FOR pose - related representations. VQSN module CONJUNCTION training scheme. training scheme CONJUNCTION VQSN module. synthesis image quality EVALUATE-FOR model. disentangling scores EVALUATE-FOR model. synthesis image quality CONJUNCTION disentangling scores. disentangling scores CONJUNCTION synthesis image quality. latent - space reducing feature USED-FOR VQSN module. posture - identity disentangling FEATURE-OF model applications. VQSN module USED-FOR model applications. latent - space reducing feature USED-FOR model applications. PIVQGAN USED-FOR Unsupervised image - to - image translation. disentangled posture and identity control USED-FOR PIVQGAN. PIVQGAN USED-FOR segmentation - like ” masks. Task is image - to - image translation task. Material are training - set images, pose images, and referential identity images. ","This paper proposes a method for unsupervised image-to-image translation. The proposed method is based on StyleGAN2, which is an extension of StyleGAN. The key idea is to combine the self-supervision and self-training techniques in StyleGAN to improve the disentanglement performance. The method is evaluated on the task of pose-identity disentangling. ","This paper proposes a method for unsupervised image-to-image translation. The proposed method is based on StyleGAN2, which is an extension of StyleGAN. The key idea is to combine the self-supervision and self-training techniques in StyleGAN to improve the disentanglement performance. The method is evaluated on the task of pose-identity disentangling. "
7750,SP:e51a7f45493064972585109f203a867e9828eb15,"speech synthesis CONJUNCTION speech enhancement. speech enhancement CONJUNCTION speech synthesis. speech recognition CONJUNCTION speech synthesis. speech synthesis CONJUNCTION speech recognition. Transformers USED-FOR speech processing tasks. speech enhancement HYPONYM-OF speech processing tasks. speech recognition HYPONYM-OF speech processing tasks. speech synthesis HYPONYM-OF speech processing tasks. models USED-FOR speech related tasks. speech - MLP HYPONYM-OF multi - layer perceptron ( MLP ) architecture. speech - MLP USED-FOR multiscale local temporal dependency. keyword spotting CONJUNCTION speech enhancement. speech enhancement CONJUNCTION keyword spotting. keyword spotting EVALUATE-FOR model. speech enhancement EVALUATE-FOR model. tasks EVALUATE-FOR model. speech enhancement HYPONYM-OF tasks. keyword spotting HYPONYM-OF tasks. Google speech command V2 - 35 CONJUNCTION LibriWords. LibriWords CONJUNCTION Google speech command V2 - 35. dataset ( VoiceBank ) USED-FOR speech enhancement. benchmark datasets USED-FOR keyword spotting. benchmark datasets EVALUATE-FOR speech enhancement. benchmark datasets CONJUNCTION dataset ( VoiceBank ). dataset ( VoiceBank ) CONJUNCTION benchmark datasets. Google speech command V2 - 35 HYPONYM-OF benchmark datasets. LibriWords HYPONYM-OF benchmark datasets. speech - MLP COMPARE transformer - based solutions. transformer - based solutions COMPARE speech - MLP. Material is speech signals. OtherScientificTerm are feature channels, contextual window sizes, and resource - constrained scenarios. Generic is chunks. Metric is GFLOPS. Method is transformers. ","This paper proposes a multi-layer perceptron (MLP) architecture for speech processing. The proposed model is based on the multi-layered MLP architecture and is able to handle multi-scale local temporal dependency. The model is evaluated on a number of tasks including keyword spotting, speech enhancement, speech synthesis, and speech recognition. Experiments show that the proposed model outperforms transformer-based models on most tasks.","This paper proposes a multi-layer perceptron (MLP) architecture for speech processing. The proposed model is based on the multi-layered MLP architecture and is able to handle multi-scale local temporal dependency. The model is evaluated on a number of tasks including keyword spotting, speech enhancement, speech synthesis, and speech recognition. Experiments show that the proposed model outperforms transformer-based models on most tasks."
7775,SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"labeled data USED-FOR massive models. data collection CONJUNCTION labeling. labeling CONJUNCTION data collection. generalization error EVALUATE-FOR transfer learning algorithm. transfer learning algorithm USED-FOR lower bound. generalization error EVALUATE-FOR lower bound. computational complexity EVALUATE-FOR transfer learning algorithm. it USED-FOR source / target data distributions. source domains USED-FOR knowledge transfer. bounds USED-FOR setting. bounds USED-FOR generalization error. real image classification CONJUNCTION action recognition data sets. action recognition data sets CONJUNCTION real image classification. lower bounds COMPARE upper - bounds. upper - bounds COMPARE lower bounds. lower bounds COMPARE transfer learning base - lines. transfer learning base - lines COMPARE lower bounds. transfer learning base - lines USED-FOR upper - bounds. source(s ) and target data sets USED-FOR weighted empirical risk minimization. weighted empirical risk minimization USED-FOR upper - bounds. weighted empirical risk minimization USED-FOR transfer learning base - lines. Task are machine learning, and binary classification problems. Method is Transfer learning. Material are labeled training data, and real world data sets. ",This paper proposes a new lower bound for the generalization error of a transfer learning algorithm for binary classification problems. The lower bound is based on a weighted empirical risk minimization (WERM) algorithm. The authors show that the proposed lower bound can be used as a base-line for transfer learning algorithms. The upper bound is also based on the WERM algorithm. ,This paper proposes a new lower bound for the generalization error of a transfer learning algorithm for binary classification problems. The lower bound is based on a weighted empirical risk minimization (WERM) algorithm. The authors show that the proposed lower bound can be used as a base-line for transfer learning algorithms. The upper bound is also based on the WERM algorithm. 
7800,SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"probabilistic shape completion method USED-FOR continuous geometry of large - scale 3D scenes. Generative Cellular Automata USED-FOR multi - modal distribution. formulation USED-FOR large - scale continuous geometry. latent code PART-OF sparse voxel embedding. sparse voxel embedding USED-FOR local continuous shape. progressive generation USED-FOR generative model. training objective USED-FOR sparse voxel embedding. variational lower bound FEATURE-OF complete shape distribution. variational lower bound USED-FOR training objective. probabilistic formulation USED-FOR geometry completion. approach COMPARE deterministic models. deterministic models COMPARE approach. Material are Real - world scans of 3D scenes, and missing data. Task is shape completion. Generic is model. ","This paper proposes a probabilistic shape completion method for large-scale continuous 3D scenes. The proposed method is based on generative Cellular Automata (GCA), which is a generative model that learns a multi-modal distribution over latent code. The authors propose a variational lower bound for the complete shape distribution, which is used as a training objective. Experiments show that the proposed method outperforms the baselines.","This paper proposes a probabilistic shape completion method for large-scale continuous 3D scenes. The proposed method is based on generative Cellular Automata (GCA), which is a generative model that learns a multi-modal distribution over latent code. The authors propose a variational lower bound for the complete shape distribution, which is used as a training objective. Experiments show that the proposed method outperforms the baselines."
7825,SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"exploration PART-OF deep reinforcement learning. Behavioral priors USED-FOR problem. reduced generality CONJUNCTION restricted transferability. restricted transferability CONJUNCTION reduced generality. exploration USED-FOR reinforcement learning. temporal consistency USED-FOR state - independent temporal priors. probabilistic mixture of policy and temporal prior USED-FOR off - policy reinforcement learning. approach COMPARE baselines. baselines COMPARE approach. long - horizon continuous control tasks EVALUATE-FOR baselines. sparse reward settings USED-FOR baselines. sparse reward settings USED-FOR long - horizon continuous control tasks. long - horizon continuous control tasks EVALUATE-FOR approach. sparse reward settings USED-FOR approach. OtherScientificTerm are temporal priors, and behavioral priors. ",This paper proposes a probabilistic mixture of policy and temporal priors for off-policy reinforcement learning. The proposed method is motivated by the observation that the temporal consistency of the policy and the behavioral priors is important for exploration in RL. The authors propose to use this temporal consistency to improve the generalizability of the proposed method. Experiments are conducted on a number of continuous control tasks to demonstrate the effectiveness of the method.,This paper proposes a probabilistic mixture of policy and temporal priors for off-policy reinforcement learning. The proposed method is motivated by the observation that the temporal consistency of the policy and the behavioral priors is important for exploration in RL. The authors propose to use this temporal consistency to improve the generalizability of the proposed method. Experiments are conducted on a number of continuous control tasks to demonstrate the effectiveness of the method.
7850,SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,stochastic optimization USED-FOR deep neural networks. Learning rate scheduling HYPONYM-OF stochastic optimizers. Adam HYPONYM-OF stochastic optimizers. pre - defined rules USED-FOR scheduling. GNS USED-FOR dynamics. directed graph USED-FOR neural network. agent USED-FOR learning rate. directed graph USED-FOR GNS. GNS USED-FOR agent. reinforcement learning USED-FOR GNS. graph message passing network USED-FOR GNS. reinforcement learning USED-FOR agent. reinforcement learning USED-FOR learning rate. graph message passing network USED-FOR dynamics. scheduler USED-FOR intermediate layer information. reward collection procedure USED-FOR training. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION GLUE. GLUE CONJUNCTION CIFAR10. Fashion - MNIST CONJUNCTION GLUE. GLUE CONJUNCTION Fashion - MNIST. GLUE USED-FOR language understanding. Fashion - MNIST CONJUNCTION image classification. image classification CONJUNCTION Fashion - MNIST. CIFAR10 USED-FOR image classification. image classification CONJUNCTION GLUE. GLUE CONJUNCTION image classification. Fashion - MNIST HYPONYM-OF benchmarking datasets. CIFAR10 HYPONYM-OF benchmarking datasets. GLUE HYPONYM-OF benchmarking datasets. Fashion - MNIST EVALUATE-FOR framework. GLUE EVALUATE-FOR framework. benchmarking datasets EVALUATE-FOR framework. GNS COMPARE baselines. baselines COMPARE GNS. GNS USED-FOR CNN and Transformer models. baselines USED-FOR CNN and Transformer models. network structures USED-FOR GNS. Method is scheduling mechanism. ,This paper proposes a new learning rate scheduling method for deep neural networks. The proposed method is based on reinforcement learning. The authors propose a reward collection procedure to learn the learning rate and a graph message passing network to control the dynamics of the network. Experiments show that the proposed method outperforms baselines on several benchmark datasets.,This paper proposes a new learning rate scheduling method for deep neural networks. The proposed method is based on reinforcement learning. The authors propose a reward collection procedure to learn the learning rate and a graph message passing network to control the dynamics of the network. Experiments show that the proposed method outperforms baselines on several benchmark datasets.
7875,SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"high - level relational reasoning CONJUNCTION scalable machine intelligence. scalable machine intelligence CONJUNCTION high - level relational reasoning. point cloud USED-FOR high - level relational reasoning. point cloud USED-FOR scalable machine intelligence. point cloud USED-FOR deep object - centric learning. framework USED-FOR 3D point cloud. framework USED-FOR spatial mixture model. Chamfer Mixture Loss PART-OF variational training pipeline. point clouds USED-FOR spatial mixture model. scheme USED-FOR SPAIR3D. unsupervised scene decomposition EVALUATE-FOR method. Method are object - specification scheme, and unsupervised manner. OtherScientificTerm is local voxel grid cell. ","This paper proposes a new method for object-centric learning based on Chamfer Mixture Loss. The proposed method is based on the idea of a point cloud, which is a local voxel grid cell that is used for object specification. The point cloud is then used to train a spatial mixture model. The method is evaluated on unsupervised scene decomposition tasks.","This paper proposes a new method for object-centric learning based on Chamfer Mixture Loss. The proposed method is based on the idea of a point cloud, which is a local voxel grid cell that is used for object specification. The point cloud is then used to train a spatial mixture model. The method is evaluated on unsupervised scene decomposition tasks."
7900,SP:3c57e921c1bf23e482551ceb71702931a7f07439,"world knowledge USED-FOR interactive environments. large language models ( LLMs ) USED-FOR interactive environments. large language models ( LLMs ) USED-FOR world knowledge. natural language USED-FOR high - level tasks. they USED-FOR high - level tasks. LMs USED-FOR high - level tasks. LMs USED-FOR they. VirtualHome environment EVALUATE-FOR method. method COMPARE LLM baseline. LLM baseline COMPARE method. executability EVALUATE-FOR LLM baseline. VirtualHome environment EVALUATE-FOR LLM baseline. executability EVALUATE-FOR method. executability CONJUNCTION correctness. correctness CONJUNCTION executability. language models1 USED-FOR actionable knowledge. OtherScientificTerm are actionable steps, low - level plans, and admissible actions. Method is LLMs. Generic is procedure. Metric is human evaluation. ","This paper proposes a method for evaluating the performance of large language models (LLMs) in interactive environments. The method is based on the observation that LLMs are not able to learn actionable knowledge for high-level tasks. To address this issue, the authors propose a method that learns a set of low-level actions that can be executed by a large language model. The proposed method is evaluated on the VirtualHome environment and shows that the proposed method outperforms the baseline method.","This paper proposes a method for evaluating the performance of large language models (LLMs) in interactive environments. The method is based on the observation that LLMs are not able to learn actionable knowledge for high-level tasks. To address this issue, the authors propose a method that learns a set of low-level actions that can be executed by a large language model. The proposed method is evaluated on the VirtualHome environment and shows that the proposed method outperforms the baseline method."
7925,SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,geometrical interpretation USED-FOR Variational Autoencoder framework. VAEs USED-FOR Riemannian structure of the learned latent space. vanilla VAE COMPARE VAE models. VAE models COMPARE vanilla VAE. geometrical considerations USED-FOR vanilla VAE. benchmark datasets EVALUATE-FOR VAE models. VAE USED-FOR Riemannian manifold. Riemannian manifold USED-FOR uniform distribution. method USED-FOR deep generative models. low data regime EVALUATE-FOR method. high dimensional data CONJUNCTION low sample sizes. low sample sizes CONJUNCTION high dimensional data. high dimensional data FEATURE-OF complex neuroimaging dataset. low sample sizes FEATURE-OF complex neuroimaging dataset. complex neuroimaging dataset EVALUATE-FOR method. ,This paper proposes a Variational Autoencoder framework for deep generative models. The key idea is to use the Riemannian structure of the learned latent space of the VAE model to learn a uniform distribution over the latent space. The authors show that the proposed method outperforms vanilla VAE models on a number of benchmark datasets. ,This paper proposes a Variational Autoencoder framework for deep generative models. The key idea is to use the Riemannian structure of the learned latent space of the VAE model to learn a uniform distribution over the latent space. The authors show that the proposed method outperforms vanilla VAE models on a number of benchmark datasets. 
7950,SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"natural language processing ( NLP ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing ( NLP ). computer vision tasks EVALUATE-FOR transformers. natural language processing ( NLP ) EVALUATE-FOR transformers. attention heads USED-FOR applications. redundant heads PART-OF transformers. mixture of keys PART-OF transformer architecture. redundant heads PART-OF transformer architecture. Gaussian mixture model USED-FOR mixtures of keys. Transformer - MGK USED-FOR training. transformer counterpart COMPARE Transformer - MGK. Transformer - MGK COMPARE transformer counterpart. FLOPs USED-FOR Transformer - MGK. accuracy EVALUATE-FOR Transformer - MGK. linear attentions USED-FOR Transformer - MGK. language modeling CONJUNCTION tasks. tasks CONJUNCTION language modeling. applications EVALUATE-FOR Transformer - MGK. tasks HYPONYM-OF applications. tasks EVALUATE-FOR Transformer - MGK. language modeling HYPONYM-OF applications. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR Transformer - MGKs. Transformer - MGKs COMPARE baseline transformers. baseline transformers COMPARE Transformer - MGKs. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR baseline transformers. Method are Multi - head attention, and Transformer. OtherScientificTerm are redundant embedding, and attention head. Generic is model. ",This paper proposes a new multi-head attention architecture for transformers. The key idea is to use a Gaussian mixture model to model the mixtures of keys in the attention head. The authors show that the proposed architecture is able to achieve better performance than the baseline transformers on a variety of tasks. The proposed method is evaluated on the Wikitext-103 and Long Range Arena benchmark.,This paper proposes a new multi-head attention architecture for transformers. The key idea is to use a Gaussian mixture model to model the mixtures of keys in the attention head. The authors show that the proposed architecture is able to achieve better performance than the baseline transformers on a variety of tasks. The proposed method is evaluated on the Wikitext-103 and Long Range Arena benchmark.
7975,SP:82731dcce233e748f63382e09b6224a513fe9689,"biological agents USED-FOR Spatial navigation. proprioception CONJUNCTION linear and angular velocity. linear and angular velocity CONJUNCTION proprioception. direct - inverse model of environment dynamics USED-FOR image and action related signals. direct - inverse model of environment dynamics USED-FOR reconstruction of the action. direct - inverse model of environment dynamics USED-FOR two – dimensional continuous environment. Resetting Path Integrator ( RPI ) HYPONYM-OF minimalistic recurrent architecture. RPI USED-FOR internal state. it USED-FOR cognitive map. internal state FEATURE-OF minimal model. architecture COMPARE LSTM networks. LSTM networks COMPARE architecture. architecture USED-FOR internal dynamics. tasks EVALUATE-FOR LSTM networks. tasks EVALUATE-FOR architecture. Generic is models. OtherScientificTerm are image signal, resetting, and integration of past movement. Method is direct - inverse models. ","This paper proposes a new model for navigation in a two-dimensional continuous environment. The proposed model is based on a direct-inverse model of the environment dynamics, which is used to model the internal state of the agent. The authors show that the proposed model outperforms the state-of-the-art LSTM models on a variety of navigation tasks. ","This paper proposes a new model for navigation in a two-dimensional continuous environment. The proposed model is based on a direct-inverse model of the environment dynamics, which is used to model the internal state of the agent. The authors show that the proposed model outperforms the state-of-the-art LSTM models on a variety of navigation tasks. "
8000,SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"features USED-FOR prediction. feature learning USED-FOR neural networks. practical data USED-FOR learning problems. neural networks USED-FOR problems. gradient descent USED-FOR neural networks. linear models USED-FOR data - independent features. polynomial sizes FEATURE-OF linear models. polynomial sizes FEATURE-OF data - independent features. polynomial algorithm PART-OF Statistical Query model. neural networks USED-FOR feature learning. OtherScientificTerm are class relevant patterns, background patterns, and structure of the input distribution. Material is synthetic and real data. ",This paper studies the problem of feature learning in deep neural networks. The authors propose a polynomial algorithm for learning data-independent features. The main contribution of the paper is to show that polynomials can be used to learn features that are independent of the input distribution. The paper also shows that the proposed algorithm can be applied to the Statistical Query model. ,This paper studies the problem of feature learning in deep neural networks. The authors propose a polynomial algorithm for learning data-independent features. The main contribution of the paper is to show that polynomials can be used to learn features that are independent of the input distribution. The paper also shows that the proposed algorithm can be applied to the Statistical Query model. 
8025,SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,robustness EVALUATE-FOR machine learning models. machine learning models USED-FOR adversarial examples. robustness FEATURE-OF adversarial examples. test - time adversaries FEATURE-OF adversarial examples. data distribution CONJUNCTION attacker constraints. attacker constraints CONJUNCTION data distribution. lower bounds USED-FOR model. bounds USED-FOR arbitrary classification functions. architectures CONJUNCTION models. models CONJUNCTION architectures. neural networks HYPONYM-OF architectures. neural networks HYPONYM-OF models. methodology USED-FOR robustness. robustness EVALUATE-FOR classifier. methodology USED-FOR fixed feature extractors. robustness EVALUATE-FOR fixed feature extractors. bounds USED-FOR classifier. bounds FEATURE-OF robustness. it USED-FOR classifier. method USED-FOR collisions. closed - form expressions USED-FOR collision finding. bespoke algorithm USED-FOR arbitrary feature extractors. closed - form expressions USED-FOR linear feature extractors. convex program USED-FOR bespoke algorithm. Method is training methods. ,"This paper studies the problem of robustness to adversarial examples in the presence of adversarial attacks. The authors propose a new method for adversarial training, which is based on closed-form expressions. The key idea is to use a convex program to learn a classifier that is robust against adversarial attack. The proposed method is evaluated on a variety of datasets, and is shown to outperform the baselines. ","This paper studies the problem of robustness to adversarial examples in the presence of adversarial attacks. The authors propose a new method for adversarial training, which is based on closed-form expressions. The key idea is to use a convex program to learn a classifier that is robust against adversarial attack. The proposed method is evaluated on a variety of datasets, and is shown to outperform the baselines. "
8050,SP:874b5fa51924cbcceed490d98a0ea80f74586b32,RL USED-FOR real - world problems. Offline reinforcement learning ( RL ) USED-FOR RL. regularization or constraints USED-FOR extrapolation error. regularization or constraints USED-FOR offline RL algorithms. framework USED-FOR V -function. learning procedure PART-OF offline dataset. optimal value learning CONJUNCTION behavior cloning. behavior cloning CONJUNCTION optimal value learning. conservatism FEATURE-OF offline learning. Expectile V -Learning ( EVL ) USED-FOR generalization. implicit planning USED-FOR V -values. offline trajectories USED-FOR implicit planning. Value - based Episodic Memory ( VEM ) HYPONYM-OF offline method. D4RL benchmark EVALUATE-FOR method. sparse - reward tasks HYPONYM-OF tasks. tasks EVALUATE-FOR method. sparse - reward tasks EVALUATE-FOR method. D4RL benchmark EVALUATE-FOR VEM method. OtherScientificTerm is Q - function. ,"This paper proposes a new method for offline reinforcement learning (RL) called Value-based Episodic Memory (VEM). The proposed method is based on the idea of Expectile V-Learning (EVL), which aims to improve the generalization performance of offline RL algorithms. The authors propose a new learning procedure called Expectile Expectile Value-Learning, which learns a set of offline trajectories for each Q-function, and then uses this set of trajectories to learn the optimal value function for the Q function. The method is evaluated on the D4RL benchmark and shows that the proposed method outperforms the baselines. ","This paper proposes a new method for offline reinforcement learning (RL) called Value-based Episodic Memory (VEM). The proposed method is based on the idea of Expectile V-Learning (EVL), which aims to improve the generalization performance of offline RL algorithms. The authors propose a new learning procedure called Expectile Expectile Value-Learning, which learns a set of offline trajectories for each Q-function, and then uses this set of trajectories to learn the optimal value function for the Q function. The method is evaluated on the D4RL benchmark and shows that the proposed method outperforms the baselines. "
8086,SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"robust accuracy CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robust accuracy. neural network classifiers USED-FOR adversarial perturbations. robustness FEATURE-OF neural network classifiers. adversarial training USED-FOR adversarial perturbations. adversarial training framework USED-FOR robust generalization. importance weight USED-FOR parametric function. bilevel optimization problem USED-FOR weighted adversarial training. approach COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE approach. approach COMPARE techniques. techniques COMPARE approach. clean and robust accuracy EVALUATE-FOR techniques. techniques CONJUNCTION state - of - the - art baselines. state - of - the - art baselines CONJUNCTION techniques. clean and robust accuracy EVALUATE-FOR state - of - the - art baselines. clean and robust accuracy EVALUATE-FOR approach. OtherScientificTerm are class - conditioned margin, and sample ’s multi - class margin. Method are MAML - based approaches, and robust classifier. Generic is upper - level task. ","This paper proposes a weighted adversarial training framework to improve the robustness of neural network classifiers against adversarial perturbations. The proposed method is based on a bilevel optimization problem, where the class-conditioned margin and the multi-class margin are considered. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of clean and robust accuracy.","This paper proposes a weighted adversarial training framework to improve the robustness of neural network classifiers against adversarial perturbations. The proposed method is based on a bilevel optimization problem, where the class-conditioned margin and the multi-class margin are considered. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of clean and robust accuracy."
8122,SP:3ad36be6b6900aabe43da043461cf178ce977082,"force CONJUNCTION velocity. velocity CONJUNCTION force. position CONJUNCTION force. force CONJUNCTION position. velocity CONJUNCTION spin. spin CONJUNCTION velocity. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. spin HYPONYM-OF covariant information. velocity HYPONYM-OF covariant information. position HYPONYM-OF covariant information. force HYPONYM-OF covariant information. vectors HYPONYM-OF covariant information. geometric and physical information USED-FOR message and update functions. steerable MLPs PART-OF model. geometric and physical information PART-OF model. MLPs USED-FOR activation functions. activation functions USED-FOR steerable feature fields. MLPs USED-FOR steerable feature fields. components PART-OF SEGNNs. non - linear message aggregation CONJUNCTION linear ( steerable ) point convolutions. linear ( steerable ) point convolutions CONJUNCTION non - linear message aggregation. invariant messages FEATURE-OF equivariant graph networks. equivariant graph networks USED-FOR steerable messages. non - linear message aggregation HYPONYM-OF components. non - linear message aggregation PART-OF SEGNNs. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. chemistry EVALUATE-FOR method. computational physics EVALUATE-FOR method. OtherScientificTerm are node and edge attributes, invariant scalars, and steerable node attributes. Method is equivariant non - linear convolutions. ",This paper proposes a method for steerable message aggregation and non-linear message aggregation in equivariant graph networks (SEGNNs). The main idea is to use steerable MLPs for the message and update functions. The authors show that the proposed method is able to achieve better performance than existing methods on a variety of tasks. ,This paper proposes a method for steerable message aggregation and non-linear message aggregation in equivariant graph networks (SEGNNs). The main idea is to use steerable MLPs for the message and update functions. The authors show that the proposed method is able to achieve better performance than existing methods on a variety of tasks. 
8158,SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"physics models CONJUNCTION gradient - based learning. gradient - based learning CONJUNCTION physics models. model explicability CONJUNCTION data efficiency. data efficiency CONJUNCTION model explicability. Differentiable physics modeling USED-FOR model explicability. gradient - based learning PART-OF Differentiable physics modeling. physics models PART-OF Differentiable physics modeling. It USED-FOR dynamics. It USED-FOR inverse problems. It USED-FOR design. inverse problems CONJUNCTION design. design CONJUNCTION inverse problems. dynamics CONJUNCTION inverse problems. inverse problems CONJUNCTION dynamics. rigid bodies CONJUNCTION deformable sheets. deformable sheets CONJUNCTION rigid bodies. rigid bodies HYPONYM-OF physics models. deformable sheets HYPONYM-OF physics models. material structures CONJUNCTION force interactions. force interactions CONJUNCTION material structures. Fine - grained models USED-FOR material structures. gradient - based learning USED-FOR Fine - grained models. Fine - grained models USED-FOR force interactions. gradient - based learning USED-FOR force interactions. individual yarn physics CONJUNCTION yarn - to - yarn interactions. yarn - to - yarn interactions CONJUNCTION individual yarn physics. differentiable fabrics model USED-FOR composite materials. cloths HYPONYM-OF composite materials. differentiable forces USED-FOR gradient - based learning. differentiable forces PART-OF empirical physics. forces USED-FOR cloths. complex physical structures CONJUNCTION heterogeneous materials. heterogeneous materials CONJUNCTION complex physical structures. data - efficiency CONJUNCTION high - fidelity. high - fidelity CONJUNCTION data - efficiency. model USED-FOR physical parameters. high - fidelity USED-FOR subtle dynamics. model USED-FOR subtle dynamics. high - fidelity EVALUATE-FOR model. data - efficiency EVALUATE-FOR model. OtherScientificTerm are complex physical phenomena, and granularity of yarns. Material is physical systems. ",This paper proposes a differentiable physics model for differentiable fabrics. The model is based on gradient-based learning. The authors show that the model is able to capture the dynamics and design of differentiable materials. ,This paper proposes a differentiable physics model for differentiable fabrics. The model is based on gradient-based learning. The authors show that the model is able to capture the dynamics and design of differentiable materials. 
8194,SP:2c8358c095b10981d3015b9f6c75765419a9480d,"logical composition USED-FOR framework. logical composition USED-FOR reinforcement learning. OtherScientificTerm are task - specific skill, optimal policy, Boolean expression, unknown distribution, and task distribution. Generic are algorithm, distribution, approach, and tasks. Method are transferred policy, transfer learning, and transfer learning approach. ",This paper proposes a transfer learning framework for reinforcement learning. The key idea is to learn a new task-specific skill by learning an optimal policy that maximizes the performance of the current task on the new task. This is achieved by learning a new skill that maximises the performance on the task that was learned in the previous task. The authors show that the learned skill can be used to transfer to new tasks. ,This paper proposes a transfer learning framework for reinforcement learning. The key idea is to learn a new task-specific skill by learning an optimal policy that maximizes the performance of the current task on the new task. This is achieved by learning a new skill that maximises the performance on the task that was learned in the previous task. The authors show that the learned skill can be used to transfer to new tasks. 
8230,SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"machine and deep learning solutions USED-FOR multivariate time series classification ( MTSC ). prediction accuracy EVALUATE-FOR complex models. accuracy EVALUATE-FOR solutions. ROCKET HYPONYM-OF MTSC solution. random convolutional kernels USED-FOR ROCKET. random convolutional kernels USED-FOR MTSC solution. distributed solution USED-FOR MTSC. LightWaveS HYPONYM-OF distributed solution. solution COMPARE deep learning solutions. deep learning solutions COMPARE solution. wavelet scattering transformation CONJUNCTION distributed feature selection. distributed feature selection CONJUNCTION wavelet scattering transformation. distributed feature selection USED-FOR solution. accuracy EVALUATE-FOR deep learning solutions. wavelet scattering transformation USED-FOR solution. wavelet scattering transformation USED-FOR time series. ROCKET features USED-FOR solution. accuracy EVALUATE-FOR solution. nodes CONJUNCTION channels. channels CONJUNCTION nodes. nodes USED-FOR LightWaveS. channels USED-FOR LightWaveS. it USED-FOR MTSC problem. inference speedup CONJUNCTION scalability. scalability CONJUNCTION inference speedup. accuracy CONJUNCTION inference speedup. inference speedup CONJUNCTION accuracy. training time CONJUNCTION accuracy. accuracy CONJUNCTION training time. training time EVALUATE-FOR algorithm. ROCKET USED-FOR inference. speedup EVALUATE-FOR ROCKET. speedup EVALUATE-FOR inference. edge device USED-FOR inference. datasets EVALUATE-FOR speedup. datasets EVALUATE-FOR inference. OtherScientificTerm are real - world environments, and features. Metric are prediction speed, and inference time. Task is training. ","This paper proposes a new method for multivariate time series classification (MTSC) based on wavelet scattering transformation and distributed feature selection. The proposed method, called ROCKET, is based on random convolutional kernels (RKernels) and distributed features. The authors show that the proposed method can achieve better performance than the state-of-the-art in terms of prediction speed and inference speedup. ","This paper proposes a new method for multivariate time series classification (MTSC) based on wavelet scattering transformation and distributed feature selection. The proposed method, called ROCKET, is based on random convolutional kernels (RKernels) and distributed features. The authors show that the proposed method can achieve better performance than the state-of-the-art in terms of prediction speed and inference speedup. "
8266,SP:db43614ca016280a79448f44a97c81c8ff5ba981,"AMOS USED-FOR text encoders. Mixture Of Signals USED-FOR auxiliary generators. Mixture Of Signals USED-FOR Adversarial learning curriculum. Adversarial learning curriculum USED-FOR text encoders. discriminator USED-FOR replaced tokens. discriminator USED-FOR encoder. encoder USED-FOR replaced tokens. auxiliary masked language models ( MLMs ) USED-FOR replaced tokens. MLMs USED-FOR training signals. mixture weights USED-FOR discriminator loss. gradient PART-OF discriminator. mixture weights USED-FOR auxiliary MLMs ’ outputs. Gumbel - Softmax USED-FOR gradient. MLMs PART-OF unified auxiliary model. AMOS COMPARE pretrained models. pretrained models COMPARE AMOS. AMOS COMPARE ELECTRA. ELECTRA COMPARE AMOS. GLUE and SQuAD benchmarks EVALUATE-FOR BERT base - sized models. ELECTRA COMPARE pretrained models. pretrained models COMPARE ELECTRA. BERT base - sized models EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR AMOS. Method are ELECTRA - style pretraining, and MLM. Metric is pretraining efficiency. ","This paper proposes a new adversarial learning curriculum for text encoders, called AMOS. The proposed method is based on the idea of Mixture of Signals (MOS), which is an extension of the ELECTRA-style pretraining method. The authors propose to use a mixture of masked language models (MLMs) to train the discriminator and the encoder, and then use a Gumbel-Softmax-based loss function to improve the performance of the decoder. AMOS is evaluated on GLUE, SQuAD, and BERT.","This paper proposes a new adversarial learning curriculum for text encoders, called AMOS. The proposed method is based on the idea of Mixture of Signals (MOS), which is an extension of the ELECTRA-style pretraining method. The authors propose to use a mixture of masked language models (MLMs) to train the discriminator and the encoder, and then use a Gumbel-Softmax-based loss function to improve the performance of the decoder. AMOS is evaluated on GLUE, SQuAD, and BERT."
8302,SP:db3825633ab5d0671340390b23ab655838cc38b2,"pre - trained language models USED-FOR relational knowledge. clozestyle sentence USED-FOR pre - trained language models. clozestyle sentence USED-FOR relational knowledge. language models COMPARE knowledge graphs. knowledge graphs COMPARE language models. precision EVALUATE-FOR language models. adaptive fine - tuning USED-FOR fill - mask task. pre - trained language model USED-FOR fill - mask task. pre - trained language model USED-FOR adaptive fine - tuning. complex prompting techniques CONJUNCTION adaptive fine - tuning. adaptive fine - tuning CONJUNCTION complex prompting techniques. adaptive fine - tuning COMPARE baselines. baselines COMPARE adaptive fine - tuning. transfer learning capabilities FEATURE-OF language model. Task is relational fact extraction task. OtherScientificTerm are knowledge graph facts, and knowledge graph. Generic are model, and approach. Metric is knowledge extraction quality. ","This paper proposes a method to improve the transfer learning capabilities of pre-trained language models for relational knowledge extraction tasks. The method is based on the idea of clozestyle sentence generation, which is an extension of the previous work on relational fact extraction. The authors show that the proposed method can achieve better transfer learning performance compared to the baselines on the fill-mask task. They also show that adaptive fine-tuning (AFT) can improve the performance of the model.","This paper proposes a method to improve the transfer learning capabilities of pre-trained language models for relational knowledge extraction tasks. The method is based on the idea of clozestyle sentence generation, which is an extension of the previous work on relational fact extraction. The authors show that the proposed method can achieve better transfer learning performance compared to the baselines on the fill-mask task. They also show that adaptive fine-tuning (AFT) can improve the performance of the model."
8311,SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"multi - relations FEATURE-OF Knowledge bases. symmetry CONJUNCTION inversion. inversion CONJUNCTION symmetry. inversion CONJUNCTION composition. composition CONJUNCTION inversion. symmetry HYPONYM-OF properties. composition HYPONYM-OF properties. inversion HYPONYM-OF properties. Euclidean embedding models USED-FOR properties. hyperbolic space USED-FOR transitivity. representation learning framework USED-FOR relation properties. geometric spaces FEATURE-OF knowledge base embeddings. out - of - taxonomy entity typing task EVALUATE-FOR aligned embeddings. knowledge graph USED-FOR entities. datasets EVALUATE-FOR approach. low dimensions CONJUNCTION small training rates. small training rates CONJUNCTION low dimensions. low dimensions EVALUATE-FOR approach. YAGO3 USED-FOR datasets. small training rates EVALUATE-FOR approach. OtherScientificTerm are Euclidean space, and tree - like properties. Method is manifold alignment. ",This paper proposes a representation learning framework for knowledge base embeddings. The key idea is to use hyperbolic space to learn the relation properties of a knowledge graph. The authors show that the proposed method is able to achieve better performance on out-of-taxonomy entity typing tasks and small training rates. ,This paper proposes a representation learning framework for knowledge base embeddings. The key idea is to use hyperbolic space to learn the relation properties of a knowledge graph. The authors show that the proposed method is able to achieve better performance on out-of-taxonomy entity typing tasks and small training rates. 
8320,SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,frequency distribution FEATURE-OF real - world knowledge graphs. approaches USED-FOR static knowledge graphs. one - shot learning framework USED-FOR link prediction. temporal knowledge graphs USED-FOR one - shot learning framework. temporal knowledge graphs USED-FOR link prediction. self - attention mechanism USED-FOR temporal interactions between entities. network USED-FOR similarity score. self - attention mechanism CONJUNCTION network. network CONJUNCTION self - attention mechanism. network USED-FOR method. self - attention mechanism USED-FOR method. algorithm COMPARE baselines. baselines COMPARE algorithm. algorithm USED-FOR sparse relations. Method is low - shot learning methods. Task is temporal settings. OtherScientificTerm is data scarcity. ,"This paper proposes a new one-shot learning framework for link prediction based on temporal knowledge graphs. The key idea is to use a self-attention mechanism to learn a similarity score between entities in a temporal knowledge graph, and then use the similarity score to learn sparse relations between entities. The authors show that the proposed method outperforms baselines on a number of datasets. ","This paper proposes a new one-shot learning framework for link prediction based on temporal knowledge graphs. The key idea is to use a self-attention mechanism to learn a similarity score between entities in a temporal knowledge graph, and then use the similarity score to learn sparse relations between entities. The authors show that the proposed method outperforms baselines on a number of datasets. "
8336,SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"solver USED-FOR task. neural module USED-FOR solver. module USED-FOR task. module USED-FOR modules. visual reasoning tasks EVALUATE-FOR model. model COMPARE attention - based baseline. attention - based baseline COMPARE model. human judges USED-FOR reasoning process. Generic is tasks. OtherScientificTerm are Lower modules, and forgetting. ","This paper proposes a new neural module for visual reasoning tasks. The proposed module is based on the idea of lower modules, which can be seen as an extension of the attention module. The authors show that the proposed module outperforms the attention-based baseline on a number of tasks. ","This paper proposes a new neural module for visual reasoning tasks. The proposed module is based on the idea of lower modules, which can be seen as an extension of the attention module. The authors show that the proposed module outperforms the attention-based baseline on a number of tasks. "
8345,SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"channel - selectivity FEATURE-OF convolutional layer. Selective Convolutional Unit ( SCU ) HYPONYM-OF architectural unit. parameter efficiency EVALUATE-FOR CNNs. architectural unit USED-FOR CNNs. Selective Convolutional Unit ( SCU ) HYPONYM-OF CNNs. bottlenecks FEATURE-OF CNNs. parameter efficiency EVALUATE-FOR architectural unit. SCU USED-FOR channel - selectivity. SCU USED-FOR training. pruning unimportant channels USED-FOR SCU. SCU - based models COMPARE baselines. baselines COMPARE SCU - based models. model compression EVALUATE-FOR baselines. postprocessing USED-FOR SCU - based models. model compression EVALUATE-FOR SCU - based models. OtherScientificTerm are identity ( e.g., residual ) connection, identity connection, pruned parameters, rewired parameters, and convolutional kernels. Method is deep convolutional neural networks ( CNN ). ","This paper proposes a new architecture for convolutional neural networks. The proposed architecture is called Selective Convolutional Unit (SCU) and is based on the idea of channel-selectivity. The authors show that SCU improves the parameter efficiency of CNNs by pruning unimportant channels, rewiring parameters, and postprocessing. ","This paper proposes a new architecture for convolutional neural networks. The proposed architecture is called Selective Convolutional Unit (SCU) and is based on the idea of channel-selectivity. The authors show that SCU improves the parameter efficiency of CNNs by pruning unimportant channels, rewiring parameters, and postprocessing. "
8354,SP:2d80fa4bc440061be2234b5070503d3fa056baed,positive data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION positive data. positive data USED-FOR binary classifier. unlabeled data USED-FOR binary classifier. labeled positive data COMPARE unlabeled positive data. unlabeled positive data COMPARE labeled positive data. selection bias FEATURE-OF labeling process. it USED-FOR selection bias. PU learning USED-FOR Bayes optimal classifier. method USED-FOR classifier. algorithm USED-FOR scoring function. algorithm USED-FOR classifier. threshold USED-FOR classifier. method COMPARE methods. methods COMPARE method. methods USED-FOR PU learning. method USED-FOR PU learning. real - world datasets EVALUATE-FOR PU learning. real - world datasets EVALUATE-FOR method. real - world datasets EVALUATE-FOR methods. OtherScientificTerm is class posterior. ,"This paper studies the problem of learning a Bayesian optimal classifier from unlabeled data. The authors propose a method to learn a classifier that maximizes the selection bias between positive and negative samples. The proposed method is based on the idea of PU learning, which is an extension of Bayes optimal classifiers (BOPs) to the case of binary classifiers. The main contribution of this paper is to propose a scoring function for the classifier, which can be used to improve the performance of the proposed method. The method is evaluated on a number of real-world datasets and shows that it outperforms the baselines.","This paper studies the problem of learning a Bayesian optimal classifier from unlabeled data. The authors propose a method to learn a classifier that maximizes the selection bias between positive and negative samples. The proposed method is based on the idea of PU learning, which is an extension of Bayes optimal classifiers (BOPs) to the case of binary classifiers. The main contribution of this paper is to propose a scoring function for the classifier, which can be used to improve the performance of the proposed method. The method is evaluated on a number of real-world datasets and shows that it outperforms the baselines."
8363,SP:5f312626b0613d2e07c59214c5f00db208a98717,"auxiliary losses USED-FOR representations. approach USED-FOR statistical inefficiency. statistical inefficiency FEATURE-OF neural networks. auxiliary losses USED-FOR approach. auxiliary loss USED-FOR main loss. reinforcement learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION reinforcement learning. multi - task supervised learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION multi - task supervised learning. Atari games USED-FOR reinforcement learning. gridworld USED-FOR reinforcement learning. domains EVALUATE-FOR algorithm. ImageNet USED-FOR multi - task supervised learning. multi - task supervised learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. OtherScientificTerm are auxiliary task, and adaptive weight. ",This paper proposes a new auxiliary loss for neural networks. The main idea is to use the auxiliary loss as a regularizer to improve the performance of the main loss. The authors show that the proposed auxiliary loss can be used to reduce the variance of the loss in the auxiliary task. The proposed method is evaluated on ImageNet and reinforcement learning tasks. ,This paper proposes a new auxiliary loss for neural networks. The main idea is to use the auxiliary loss as a regularizer to improve the performance of the main loss. The authors show that the proposed auxiliary loss can be used to reduce the variance of the loss in the auxiliary task. The proposed method is evaluated on ImageNet and reinforcement learning tasks. 
8372,SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"Adversarial examples HYPONYM-OF machine learning models. geometric framework USED-FOR high - dimensional geometry of adversarial examples. manifold reconstruction literature USED-FOR geometric framework. low - dimensional data manifolds PART-OF high - dimensional space. decision boundary USED-FOR low - dimensional data manifold. decision boundary USED-FOR Adversarial examples. nearest neighbor classifiers CONJUNCTION ball - based adversarial training. ball - based adversarial training CONJUNCTION nearest neighbor classifiers. robustness EVALUATE-FOR norms. sufficient sampling conditions USED-FOR nearest neighbor classifiers. sufficient sampling conditions USED-FOR ball - based adversarial training. OtherScientificTerm are misclassifications, codimension, adversarial examples, and manifold. Method is adversarial training. ","This paper proposes a geometric framework for learning adversarial examples from low-dimensional data. The framework is based on the notion of decision boundary, which is used to define the high-dimensional geometry of the adversarial example. The authors show that the decision boundary can be used to train adversarial classifiers and nearest neighbor classifiers. They also show that sufficient sampling conditions can be obtained for ball-based adversarial training.","This paper proposes a geometric framework for learning adversarial examples from low-dimensional data. The framework is based on the notion of decision boundary, which is used to define the high-dimensional geometry of the adversarial example. The authors show that the decision boundary can be used to train adversarial classifiers and nearest neighbor classifiers. They also show that sufficient sampling conditions can be obtained for ball-based adversarial training."
8381,SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"human cognition USED-FOR high - dimensional spaces. interpretable low - dimensional representations USED-FOR areas. representation learning algorithms USED-FOR time series data. interpretable discrete dimensionality reduction CONJUNCTION deep generative modeling. deep generative modeling CONJUNCTION interpretable discrete dimensionality reduction. deep generative modeling USED-FOR representation learning framework. interpretable discrete dimensionality reduction USED-FOR representation learning framework. framework USED-FOR discrete representations of time series. discrete representations of time series USED-FOR smooth and interpretable embeddings. non - differentiability FEATURE-OF discrete representation learning. way USED-FOR non - differentiability. self - organizing map algorithm COMPARE original. original COMPARE self - organizing map algorithm. representation space FEATURE-OF Markov model. Markov model USED-FOR probabilistic interpretation of our method. model USED-FOR natural representation of uncertainty. model USED-FOR temporal transition structure. model USED-FOR clustering. static ( Fashion-)MNIST data CONJUNCTION time series of linearly interpolated ( Fashion-)MNIST images. time series of linearly interpolated ( Fashion-)MNIST images CONJUNCTION static ( Fashion-)MNIST data. clustering CONJUNCTION interpretability. interpretability CONJUNCTION clustering. eICU data set FEATURE-OF real world medical time series application. macro states FEATURE-OF chaotic Lorenz attractor system. clustering EVALUATE-FOR model. real world medical time series application EVALUATE-FOR model. interpretability EVALUATE-FOR model. static ( Fashion-)MNIST data EVALUATE-FOR model. Material are High - dimensional time series, and real world data. OtherScientificTerm is data features. Generic are representation, method, and representations. ",This paper proposes a method for learning discrete representations of time series data. The proposed method is based on deep generative modeling and interpretable discrete dimensionality reduction. The authors propose to use a Markov model to learn a discrete representation of the temporal transition structure of the time series. The model is trained using a self-organizing map algorithm. The method is evaluated on the Fashion-MNIST dataset and the eICU data set.,This paper proposes a method for learning discrete representations of time series data. The proposed method is based on deep generative modeling and interpretable discrete dimensionality reduction. The authors propose to use a Markov model to learn a discrete representation of the temporal transition structure of the time series. The model is trained using a self-organizing map algorithm. The method is evaluated on the Fashion-MNIST dataset and the eICU data set.
8390,SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"multidimensional probability distributions USED-FOR latent space prior distributions. latent space prior distributions USED-FOR implicit generative models. linear interpolations USED-FOR latent space. random latent vectors USED-FOR decoding linear interpolations. non - linear interpolations USED-FOR distribution mismatch. latent probability distribution USED-FOR distribution mismatch. multidimensional Cauchy distribution USED-FOR prior distribution. OtherScientificTerm are latent distribution, finite mean, and latent distributions. ","This paper studies the problem of learning latent space prior distributions for implicit generative models. The authors propose a multidimensional Cauchy distribution for the latent space, which can be used for decoding linear interpolations. They show that the latent distribution can be decomposed into two parts: (1) a latent distribution with finite mean, and (2) a non-linear distribution with infinite mean. They also show that linear interpolation can be decoded by decoding the latent vectors. ","This paper studies the problem of learning latent space prior distributions for implicit generative models. The authors propose a multidimensional Cauchy distribution for the latent space, which can be used for decoding linear interpolations. They show that the latent distribution can be decomposed into two parts: (1) a latent distribution with finite mean, and (2) a non-linear distribution with infinite mean. They also show that linear interpolation can be decoded by decoding the latent vectors. "
8399,SP:19b63ca635712f1509ca6e0141303c192f2709e0,"hyperbolic space FEATURE-OF shallow networks. embeddings USED-FOR ubiquitous attention mechanisms. ubiquitous attention mechanisms USED-FOR neural networks architectures. hyperbolic geometry FEATURE-OF embeddings. hyperbolic geometry COMPARE Euclidean geometry. Euclidean geometry COMPARE hyperbolic geometry. generalization EVALUATE-FOR neural machine translation. WMT’14 FEATURE-OF neural machine translation. learning on graphs EVALUATE-FOR method. synthetic and real - world graph tasks EVALUATE-FOR learning on graphs. visual question answering ( CLEVR ) tasks EVALUATE-FOR method. neural machine translation EVALUATE-FOR method. generalization EVALUATE-FOR method. Generic are approaches, and model. Method are geometry of embedding of object representations, and neural representations. OtherScientificTerm are embedding space, and semantic distance. Material is graphs. ",This paper proposes a method for learning embeddings of objects in hyperbolic space. The key idea is to use the hyper-bolic geometry of the embedding space to learn representations of objects. The authors show that the proposed method is able to generalize well on synthetic and real-world graph tasks. The method is evaluated on the CLEVR and WMT’14 tasks.,This paper proposes a method for learning embeddings of objects in hyperbolic space. The key idea is to use the hyper-bolic geometry of the embedding space to learn representations of objects. The authors show that the proposed method is able to generalize well on synthetic and real-world graph tasks. The method is evaluated on the CLEVR and WMT’14 tasks.
8408,SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"attacks USED-FOR architecture information. attacks USED-FOR deep neural networks ( DNN ). architecture information FEATURE-OF deep neural networks ( DNN ). cache side - channels FEATURE-OF DNN fingerprinting attacks. threat model USED-FOR attacks. attack USED-FOR architecture. Flush+Reload HYPONYM-OF cache side - channel technique. DeepRecon HYPONYM-OF attack. cache side - channel technique USED-FOR internal information. Flush+Reload USED-FOR internal information. internal information USED-FOR attack. VGG19 CONJUNCTION ResNet50. ResNet50 CONJUNCTION VGG19. forward propagation USED-FOR complex networks. ResNet50 HYPONYM-OF complex networks. VGG19 HYPONYM-OF complex networks. meta - model USED-FOR pretrained model. transfer learning setting FEATURE-OF pretrained model. empirical security analysis USED-FOR DNNs ’ vulnerability. cache side - channel attacks FEATURE-OF DNNs ’ vulnerability. OtherScientificTerm are black - box networks, shared framework, network architecture, and architecture attributes. Method are victim model, co - located process, deep learning ( DL ) system, framework - level defense techniques, and DNNs. Task is fingerprinting process. ","This paper studies the problem of DNN fingerprinting attacks. The authors propose a new attack technique called Flush+Reload, which is based on the cache side-channel technique. They show that the proposed technique can be used to identify the architecture information of a black-box neural network. They also show that it can be applied to a transfer learning setting. ","This paper studies the problem of DNN fingerprinting attacks. The authors propose a new attack technique called Flush+Reload, which is based on the cache side-channel technique. They show that the proposed technique can be used to identify the architecture information of a black-box neural network. They also show that it can be applied to a transfer learning setting. "
8417,SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"representational hierarchy USED-FOR predicting future video frames. spatiotemporal memories PART-OF representational hierarchy. hierarchical network model USED-FOR spatiotemporal memories. Hierarchical Prediction Network ( HPNet ) HYPONYM-OF hierarchical network model. feedforward, feedback and lateral recurrent circuits PART-OF mammalian hierarchical visual system. feedforward, feedback and lateral recurrent circuits USED-FOR model. recurrent connections USED-FOR spatiotemporal memories. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feed - forward path USED-FOR spatiotemporal features. feed - forward path PART-OF model. feedback path PART-OF model. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feedback path PART-OF recurrent gated circuit. benchmark datasets EVALUATE-FOR long range video sequence predictions. hierarchical interaction PART-OF network. predictive self - supervised learning USED-FOR representational learning. visual cortex FEATURE-OF representational learning. OtherScientificTerm are hierarchy, internal memory states, prediction errors, frame - to - frame basis, memories of global movement patterns, and early visual cortex. Generic is level. ","This paper proposes a hierarchical network model for video sequence prediction. The model is based on the hierarchical visual system of the early visual cortex. The authors propose to use feedforward, feedback and lateral recurrent circuits to model the representation of spatiotemporal memories. The proposed model is evaluated on a number of benchmark datasets and shows promising results. ","This paper proposes a hierarchical network model for video sequence prediction. The model is based on the hierarchical visual system of the early visual cortex. The authors propose to use feedforward, feedback and lateral recurrent circuits to model the representation of spatiotemporal memories. The proposed model is evaluated on a number of benchmark datasets and shows promising results. "
8426,SP:fb74e57f35666742caf651e6da33b5defcf259a8,continuous embeddings USED-FOR kmers. method USED-FOR continuous embeddings. raw RNA - seq data USED-FOR continuous embeddings. raw RNA - seq data USED-FOR kmers. DNA sequence similarity CONJUNCTION DNA sequence abundance. DNA sequence abundance CONJUNCTION DNA sequence similarity. model USED-FOR DNA sequence similarity. model USED-FOR DNA sequence abundance. DNA sequence abundance FEATURE-OF embedding latent space. latent space USED-FOR exon information. them COMPARE known gene sub - structures. known gene sub - structures COMPARE them. acute myeloid leukemia patients FEATURE-OF raw RNA - Seq data. raw RNA - Seq data USED-FOR exon information. latent space USED-FOR detection of genomic abnormalities. visualization CONJUNCTION analysis. analysis CONJUNCTION visualization. representation space USED-FOR visualization. representation space USED-FOR analysis. translocations CONJUNCTION patient - specific mutations. patient - specific mutations CONJUNCTION translocations. patient - specific mutations HYPONYM-OF detection of genomic abnormalities. translocations HYPONYM-OF detection of genomic abnormalities. Generic is vectors. OtherScientificTerm is genomic abnormalities. ,"This paper proposes a method for learning continuous embeddings of RNA-seq data. The method is based on the idea of learning a latent space of exon information that is used to learn a continuous embedding of the data. This latent space is then used to train a model that is able to detect genomic abnormalities. The proposed method is evaluated on a number of datasets, and compared to existing methods.","This paper proposes a method for learning continuous embeddings of RNA-seq data. The method is based on the idea of learning a latent space of exon information that is used to learn a continuous embedding of the data. This latent space is then used to train a model that is able to detect genomic abnormalities. The proposed method is evaluated on a number of datasets, and compared to existing methods."
8435,SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"approach USED-FOR model compression. weight or filter space FEATURE-OF network. architecture space USED-FOR approach. 1 - D CNN encoder / decoder USED-FOR mapping. continuous embedding CONJUNCTION back. back CONJUNCTION continuous embedding. embedding USED-FOR parameter count. dataset EVALUATE-FOR architecture. gradient descent USED-FOR compression objective function. accuracy CONJUNCTION parameter count. parameter count CONJUNCTION accuracy. accuracy EVALUATE-FOR compression objective function. parameter count FEATURE-OF compression objective function. continuous space FEATURE-OF gradient descent. gradient descent USED-FOR compression phase. continuous feature USED-FOR discrete architecture. decoder USED-FOR discrete architecture. FMNIST CONJUNCTION SVHN. SVHN CONJUNCTION FMNIST. CIFAR-10/100 CONJUNCTION FMNIST. FMNIST CONJUNCTION CIFAR-10/100. CIFAR-10 EVALUATE-FOR compression. visual recognition tasks EVALUATE-FOR approach. compression EVALUATE-FOR approach. SVHN HYPONYM-OF visual recognition tasks. CIFAR-10/100 HYPONYM-OF visual recognition tasks. FMNIST HYPONYM-OF visual recognition tasks. Method are Architecture Compression, and model compression methods. OtherScientificTerm is discrete architecture space. ","This paper proposes a new approach for model compression based on the idea of architecture compression. The proposed method is based on gradient descent and uses a 1-D CNN encoder and decoder to map the input to a discrete architecture space. The authors show that the proposed method can achieve better compression performance than existing methods on CIFAR-10, SVHN, FMNIST, and FM-10/100 datasets. ","This paper proposes a new approach for model compression based on the idea of architecture compression. The proposed method is based on gradient descent and uses a 1-D CNN encoder and decoder to map the input to a discrete architecture space. The authors show that the proposed method can achieve better compression performance than existing methods on CIFAR-10, SVHN, FMNIST, and FM-10/100 datasets. "
8444,SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"local model - based control CONJUNCTION global value function learning. global value function learning CONJUNCTION local model - based control. global value function learning CONJUNCTION exploration. exploration CONJUNCTION global value function learning. local trajectory optimization USED-FOR value function learning. approximate value functions USED-FOR policies. approximate value functions USED-FOR planning horizon. trajectory optimization USED-FOR temporally coordinated exploration. estimating uncertainty USED-FOR value function approximation. trajectory optimization CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION trajectory optimization. temporally coordinated exploration CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION temporally coordinated exploration. humanoid locomotion CONJUNCTION dexterous in - hand manipulation. dexterous in - hand manipulation CONJUNCTION humanoid locomotion. components USED-FOR control tasks. humanoid locomotion HYPONYM-OF control tasks. dexterous in - hand manipulation HYPONYM-OF control tasks. Method are plan online and learn offline ” framework, and internal model. OtherScientificTerm are value function, and local solutions. ","This paper proposes a new framework for planning online and learning offline. The framework is based on the idea of value function learning, which is an extension of the value function optimization framework. The authors show that the proposed framework is able to learn a value function that can be used for planning and learning. The proposed framework can be applied to a variety of control tasks, including humanoid locomotion, dexterous in-hand manipulation, and exploration.","This paper proposes a new framework for planning online and learning offline. The framework is based on the idea of value function learning, which is an extension of the value function optimization framework. The authors show that the proposed framework is able to learn a value function that can be used for planning and learning. The proposed framework can be applied to a variety of control tasks, including humanoid locomotion, dexterous in-hand manipulation, and exploration."
8453,SP:771494fda4702cd8c7efbf225b19028f91b449b9,"parallel data USED-FOR Neural Machine Translation ( NMT ) systems. zero - shot and dual learning PART-OF approach. reinforcement learning USED-FOR duality of the machine translation task. reinforcement learning USED-FOR latter. UN corpus EVALUATE-FOR zero - shot dual system. zero - shot dual system COMPARE NMT system. NMT system COMPARE zero - shot dual system. NMT system USED-FOR zero - shot translation. English - French and English - Spanish USED-FOR zero - shot dual system. SpanishFrench EVALUATE-FOR NMT system. zero - shot dual method COMPARE LSTM - based unsupervised NMT system. LSTM - based unsupervised NMT system COMPARE zero - shot dual method. en− →fr task EVALUATE-FOR LSTM - based unsupervised NMT system. en− →fr task EVALUATE-FOR zero - shot dual method. Material are low - resource languages, monolingual data, and newstest2014. Task are unsupervised and semi - supervised methods, machine translation task, and fr− →en task. ",This paper proposes a zero-shot dual learning method for machine translation. The proposed method is based on reinforcement learning to learn the duality of the machine translation task. The authors show that the proposed method outperforms the state-of-the-art unsupervised and semi-supervised NMT systems on the en-to-en task and the en−to-fr task. ,This paper proposes a zero-shot dual learning method for machine translation. The proposed method is based on reinforcement learning to learn the duality of the machine translation task. The authors show that the proposed method outperforms the state-of-the-art unsupervised and semi-supervised NMT systems on the en-to-en task and the en−to-fr task. 
8462,SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"framework USED-FOR Information - Retrieval ( IR ). IRGAN USED-FOR Information - Retrieval ( IR ). framework USED-FOR IRGAN. generator USED-FOR distribution. minimax loss function USED-FOR generator. adversarial fashion USED-FOR models. Method is Generative Adversarial Networks. Material is multiple domains. Generic are task, and model. OtherScientificTerm are conditional probability distribution, adversarial formulation, loss curves, loss functions, and co - training like setup. ","This paper proposes a new framework for information-retrieval (IR) based on adversarial training. The proposed framework is based on the adversarial formulation of the conditional probability distribution (CPD) and the loss function. The authors show that the proposed framework can be applied to a number of different domains, including image classification, image classification and image classification. They also show that it can be used to train a generative adversarial network (GAN) that can be trained in a co-training setting.","This paper proposes a new framework for information-retrieval (IR) based on adversarial training. The proposed framework is based on the adversarial formulation of the conditional probability distribution (CPD) and the loss function. The authors show that the proposed framework can be applied to a number of different domains, including image classification, image classification and image classification. They also show that it can be used to train a generative adversarial network (GAN) that can be trained in a co-training setting."
8471,SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"Variational auto - encoders ( VAEs ) USED-FOR approximate inference. approximate inference USED-FOR intractable generative models. representations USED-FOR auxiliary tasks. VAEs USED-FOR latent codes. human interpretation HYPONYM-OF auxiliary tasks. classification HYPONYM-OF auxiliary tasks. variational auto - encoders CONJUNCTION sparse coding. sparse coding CONJUNCTION variational auto - encoders. sparsity FEATURE-OF latent space. latent space FEATURE-OF VAE. Spike and Slab prior distribution USED-FOR latent space. Spike and Slab prior distribution USED-FOR sparsity. evidence lower bound USED-FOR approximate posterior inference. approximate posterior inference COMPARE VAE case. VAE case COMPARE approximate posterior inference. discrete mixture recognition function USED-FOR approximate posterior inference. discrete mixture recognition function USED-FOR evidence lower bound. approach USED-FOR sparse representations. intractable non - linear probabilistic models USED-FOR sparse representations. sparse representations COMPARE VAE representations. VAE representations COMPARE sparse representations. classification accuracy CONJUNCTION robustness. robustness CONJUNCTION classification accuracy. robustness EVALUATE-FOR sparse representations. classification accuracy EVALUATE-FOR sparse representations. sparse elements USED-FOR subjectively understandable sources of variation. OtherScientificTerm are interpretability, and latent dimensions. Material is MNIST. ","This paper studies the problem of learning sparse representations for variational auto-encoders (VAEs). The authors propose a new approach to approximate posterior inference based on the Spike and Slab prior distribution for the latent space of a VAE. The proposed approach is based on a discrete mixture recognition function, which can be used to approximate the sparsity of a latent space. The authors show that the proposed approach outperforms existing approaches in terms of classification accuracy and robustness. ","This paper studies the problem of learning sparse representations for variational auto-encoders (VAEs). The authors propose a new approach to approximate posterior inference based on the Spike and Slab prior distribution for the latent space of a VAE. The proposed approach is based on a discrete mixture recognition function, which can be used to approximate the sparsity of a latent space. The authors show that the proposed approach outperforms existing approaches in terms of classification accuracy and robustness. "
8480,SP:06a22143186fa2948fbe324ccae96a62ff12064e,non - adversarial feature matching - based approach USED-FOR generative models. pretrained neural networks USED-FOR feature extraction. autoencoders CONJUNCTION ConvNet classifiers. ConvNet classifiers CONJUNCTION autoencoders. pretrained neural networks USED-FOR Generative Feature Matching Networks ( GFMN ). pretrained neural networks USED-FOR approach. ConvNet classifiers HYPONYM-OF pretrained neural networks. autoencoders HYPONYM-OF pretrained neural networks. ImageNet HYPONYM-OF challenging datasets. CIFAR10 CONJUNCTION STL10. STL10 CONJUNCTION CIFAR10. first order statistics USED-FOR approach. pretrained ImageNet classifiers USED-FOR features. challenging benchmarks EVALUATE-FOR approach. CIFAR10 HYPONYM-OF challenging benchmarks. STL10 HYPONYM-OF challenging benchmarks. ,"This paper proposes a non-adversarial feature matching-based approach to improve the performance of generative models. The proposed method is based on the idea of GFMN, which is a generative feature matching network that is trained with a pre-trained classifier and an autoencoder. The authors show that the proposed method outperforms the state-of-the-art baselines on a number of benchmark datasets. ","This paper proposes a non-adversarial feature matching-based approach to improve the performance of generative models. The proposed method is based on the idea of GFMN, which is a generative feature matching network that is trained with a pre-trained classifier and an autoencoder. The authors show that the proposed method outperforms the state-of-the-art baselines on a number of benchmark datasets. "
8489,SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,Graph Neural Networks ( GNNs ) USED-FOR representation learning of graphs. representation vector USED-FOR node. neighborhood aggregation scheme USED-FOR GNNs. node and graph classification tasks EVALUATE-FOR GNN variants. GNNs USED-FOR graph representation learning. GNNs USED-FOR graph structures. theoretical framework USED-FOR graph structures. theoretical framework USED-FOR GNNs. Graph Convolutional Networks CONJUNCTION GraphSAGE. GraphSAGE CONJUNCTION Graph Convolutional Networks. Graph Convolutional Networks HYPONYM-OF GNN variants. GraphSAGE HYPONYM-OF GNN variants. architecture COMPARE WeisfeilerLehman graph isomorphism test. WeisfeilerLehman graph isomorphism test COMPARE architecture. architecture COMPARE GNNs. GNNs COMPARE architecture. graph classification benchmarks EVALUATE-FOR model. Generic is they. ,This paper proposes a neighborhood aggregation scheme for graph neural networks (GNNs). The proposed method is based on the Weisfeiler-Lehman graph isomorphism test. The authors show that the proposed method outperforms existing GNNs on both node and graph classification tasks. ,This paper proposes a neighborhood aggregation scheme for graph neural networks (GNNs). The proposed method is based on the Weisfeiler-Lehman graph isomorphism test. The authors show that the proposed method outperforms existing GNNs on both node and graph classification tasks. 
8498,SP:51126f2dd37ce57d2614c9044ede1e43627f0829,framework USED-FOR interpretable continual learning ( ICL ). this USED-FOR ICL. ICL idea USED-FOR continual learning approaches. saliency maps USED-FOR metric. average classification accuracy EVALUATE-FOR overall continual learning performance. metric EVALUATE-FOR ICL. overall continual learning performance EVALUATE-FOR ICL. average classification accuracy EVALUATE-FOR ICL. Method is variational continual learning framework. OtherScientificTerm is catastrophic forgetting. ,"This paper proposes a variational continual learning framework for interpretable continual learning (ICL). The framework is based on the idea of saliency maps, which can be used as a metric for continual learning. The proposed method is evaluated on a variety of continual learning benchmarks.","This paper proposes a variational continual learning framework for interpretable continual learning (ICL). The framework is based on the idea of saliency maps, which can be used as a metric for continual learning. The proposed method is evaluated on a variety of continual learning benchmarks."
8507,SP:27a565b3e5442b93d208652784051e640b0c1bfe,"perturbations USED-FOR model. evaluation framework USED-FOR adversarial attacks. adversarial attacks FEATURE-OF seq2seq models. constraints USED-FOR word - based MT systems. human and automatic evaluation EVALUATE-FOR they. adversarial training USED-FOR model. meaning - preserving attacks FEATURE-OF adversarial training. adversarial robustness EVALUATE-FOR model. Material is Adversarial examples. Metric is robustness. OtherScientificTerm are semantics, and meaning preservation. Task is machine translation ( MT ). Generic is methods. ",This paper studies the problem of adversarial robustness of machine translation (MT) systems. The authors propose a new evaluation framework for adversarial attacks on seq2seq models. The evaluation framework is based on the observation that adversarial examples can be used to improve the performance of a machine translation system. The paper also proposes a new adversarial training method for MT systems. ,This paper studies the problem of adversarial robustness of machine translation (MT) systems. The authors propose a new evaluation framework for adversarial attacks on seq2seq models. The evaluation framework is based on the observation that adversarial examples can be used to improve the performance of a machine translation system. The paper also proposes a new adversarial training method for MT systems. 
8516,SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,rewards CONJUNCTION inverse ( negative ) rewards. inverse ( negative ) rewards CONJUNCTION rewards. rewards USED-FOR policies. inverse ( negative ) rewards USED-FOR policies. policies COMPARE policies. policies COMPARE policies. policies USED-FOR mis - actions. inverse rewards USED-FOR policies. deep Q - learning CONJUNCTION double Q - learning. double Q - learning CONJUNCTION deep Q - learning. double Q - learning CONJUNCTION on - policy actor - critic. on - policy actor - critic CONJUNCTION double Q - learning. hybrid polices COMPARE algorithms. algorithms COMPARE hybrid polices. rewards FEATURE-OF hybrid polices. on - policy actor - critic USED-FOR hybrid polices. deep Q - learning USED-FOR hybrid polices. double Q - learning USED-FOR hybrid polices. polices COMPARE policies. policies COMPARE polices. Method is reinforcement learning algorithms. OtherScientificTerm is inverse policies. Material is OpenAI gym. ,"This paper studies the problem of learning policies with inverse (negative) and inverse (positive) rewards in reinforcement learning. The authors propose a hybrid policy, which is a combination of two existing RL algorithms: double Q-learning and deep Q-Learning. They show that the proposed hybrid policy outperforms the state-of-the-art in the OpenAI gym. ","This paper studies the problem of learning policies with inverse (negative) and inverse (positive) rewards in reinforcement learning. The authors propose a hybrid policy, which is a combination of two existing RL algorithms: double Q-learning and deep Q-Learning. They show that the proposed hybrid policy outperforms the state-of-the-art in the OpenAI gym. "
8525,SP:89a732b57934d08b937c93560f391b7758e54f8a,"object parts CONJUNCTION hierarchical structure. hierarchical structure CONJUNCTION object parts. dynamics model USED-FOR object parts. hierarchical, disentangled object representation CONJUNCTION dynamics model. dynamics model CONJUNCTION hierarchical, disentangled object representation. formulation USED-FOR hierarchical, disentangled object representation. formulation USED-FOR dynamics model. unlabeled videos USED-FOR dynamics model. structural descriptor USED-FOR low - level concepts. structural descriptor USED-FOR hierarchical structure. layered image representation USED-FOR object parts. structural descriptor USED-FOR hierarchy. PSD model USED-FOR segmenting object parts. real and synthetic datasets EVALUATE-FOR PSD model. PSD model USED-FOR tasks. PSD model USED-FOR motion distributions. motion distributions HYPONYM-OF tasks. segmenting object parts HYPONYM-OF tasks. OtherScientificTerm is system dynamics. ","This paper proposes a new method for learning object-based dynamics models from unlabeled videos. The key idea is to use a hierarchical, disentangled object representation and a dynamics model to model the dynamics of a video. The proposed method is evaluated on two tasks: segmenting object parts and segmenting motion distributions. ","This paper proposes a new method for learning object-based dynamics models from unlabeled videos. The key idea is to use a hierarchical, disentangled object representation and a dynamics model to model the dynamics of a video. The proposed method is evaluated on two tasks: segmenting object parts and segmenting motion distributions. "
8534,SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,noisy training datasets USED-FOR deep neural networks. noisy ( incorrect ) class labels PART-OF Large - scale datasets. noisy datasets USED-FOR softmax neural classifier. Deep Determinantal Generative Classifier ( DDGC ) HYPONYM-OF inference method. softmax neural classifier USED-FOR decision boundary. hidden feature spaces PART-OF discriminative deep model. discriminative deep model USED-FOR generative classifier. hidden feature spaces USED-FOR generative classifier. minimum covariance determinant estimator USED-FOR generative classifier. DDGC USED-FOR adversarial perturbations. noisy labels USED-FOR DDGC. noisy labels CONJUNCTION adversarial samples. adversarial samples CONJUNCTION noisy labels. training techniques USED-FOR noisy labels. training techniques USED-FOR adversarial samples. learning models USED-FOR noisy labels. learning models USED-FOR adversarial samples. learning models USED-FOR DDGC. training techniques USED-FOR DDGC. training techniques USED-FOR learning models. test accuracy EVALUATE-FOR deep model. CIFAR10 dataset EVALUATE-FOR deep model. noisy training labels FEATURE-OF CIFAR10 dataset. noise - handling training method USED-FOR deep model. Metric is classification accuracy. OtherScientificTerm is large margin property. ,"This paper studies the problem of training deep neural networks with noisy labels. The authors propose a new classifier called Deep Determinantal Generative Classifier (DDGC), which is based on the idea that the decision boundary of a deep classifier is a function of the covariance determinant of the classifier. The proposed method is evaluated on CIFAR-10 and ImageNet, and is shown to outperform existing methods in terms of classification accuracy. ","This paper studies the problem of training deep neural networks with noisy labels. The authors propose a new classifier called Deep Determinantal Generative Classifier (DDGC), which is based on the idea that the decision boundary of a deep classifier is a function of the covariance determinant of the classifier. The proposed method is evaluated on CIFAR-10 and ImageNet, and is shown to outperform existing methods in terms of classification accuracy. "
8543,SP:0fa525cc708470b757a60117cb608bb2feaa2c50,approaches USED-FOR Reinforcement Learning ( RL ). huge state spaces CONJUNCTION sparse delayed reward feedback. sparse delayed reward feedback CONJUNCTION huge state spaces. approaches USED-FOR large - scale applications. huge state spaces FEATURE-OF large - scale applications. sparse delayed reward feedback USED-FOR large - scale applications. action selection policies USED-FOR Hierarchical Reinforcement Learning ( HRL ) methods. temporal abstraction FEATURE-OF action selection policies. skill policies USED-FOR subgoals. approaches USED-FOR subgoal discovery. subgoal discovery USED-FOR HRL. approaches USED-FOR HRL. internal reward signal USED-FOR subgoal attainment. internal reward signal USED-FOR skills. intrinsic motivation USED-FOR skills. model - free method USED-FOR subgoal discovery. incremental unsupervised learning USED-FOR model - free method. method USED-FOR subgoals. intrinsic motivation learning mechanism CONJUNCTION method. method CONJUNCTION intrinsic motivation learning mechanism. approach USED-FOR HRL. rooms environment CONJUNCTION ATARI 2600 game. ATARI 2600 game CONJUNCTION rooms environment. sparse delayed feedback FEATURE-OF RL problems. Montezuma ’s Revenge HYPONYM-OF ATARI 2600 game. RL problems EVALUATE-FOR method. ATARI 2600 game HYPONYM-OF RL problems. Montezuma ’s Revenge HYPONYM-OF RL problems. rooms environment HYPONYM-OF RL problems. OtherScientificTerm is Abstraction. Generic is model. ,"This paper proposes a Hierarchical Reinforcement Learning (HRL) method for learning subgoals. The authors propose a model-free method to learn a set of skills that can be used for subgoal discovery in HRL. The proposed method is based on the idea of temporal abstraction, which is an extension of prior work on the problem of learning subgoal-aware action selection policies. The method is evaluated on the Montezuma’s Revenge game and the rooms environment. ","This paper proposes a Hierarchical Reinforcement Learning (HRL) method for learning subgoals. The authors propose a model-free method to learn a set of skills that can be used for subgoal discovery in HRL. The proposed method is based on the idea of temporal abstraction, which is an extension of prior work on the problem of learning subgoal-aware action selection policies. The method is evaluated on the Montezuma’s Revenge game and the rooms environment. "
8552,SP:e5861538bc8bb9165cb33299bbf12dd875abf976,Representation Learning CONJUNCTION Formal Methods. Formal Methods CONJUNCTION Representation Learning. Neuro - Symbolic Methods HYPONYM-OF Formal Methods. neural framework USED-FOR Circuit Satisfiability problem. model USED-FOR SAT problem. rich embedding architecture USED-FOR problem structure. end - to - end differentiable training procedure USED-FOR Reinforcement Learning. rich embedding architecture CONJUNCTION end - to - end differentiable training procedure. end - to - end differentiable training procedure CONJUNCTION rich embedding architecture. rich embedding architecture USED-FOR framework. end - to - end differentiable training procedure USED-FOR framework. framework COMPARE NeuroSAT method. NeuroSAT method COMPARE framework. out - of - sample generalization EVALUATE-FOR framework. out - of - sample generalization EVALUATE-FOR NeuroSAT method. Method is rich neural architectures. ,This paper proposes a neural framework for Circuit Satisfiability (SAT) problems. The proposed framework is based on Neuro-Symbolic Methods (NSM) and is able to solve the SAT problem in a differentiable way. The authors also propose an end-to-end differentiable training procedure to improve the generalization performance of the proposed method.,This paper proposes a neural framework for Circuit Satisfiability (SAT) problems. The proposed framework is based on Neuro-Symbolic Methods (NSM) and is able to solve the SAT problem in a differentiable way. The authors also propose an end-to-end differentiable training procedure to improve the generalization performance of the proposed method.
8561,SP:ff3e5d44619df3825632b0b1a943add081364861,"Deep neuroevolution CONJUNCTION deep reinforcement learning ( deep RL ) algorithms. deep reinforcement learning ( deep RL ) algorithms CONJUNCTION Deep neuroevolution. approaches USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms HYPONYM-OF approaches. Deep neuroevolution HYPONYM-OF approaches. Deep neuroevolution USED-FOR policy search. them PART-OF approach. ad hoc evolutionary algorithm CONJUNCTION goal exploration process. goal exploration process CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm HYPONYM-OF sample efficient off - policy deep RL algorithm. goal exploration process CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION goal exploration process. ad hoc evolutionary algorithm CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm USED-FOR combinations. ad hoc evolutionary algorithm USED-FOR combinations. goal exploration process USED-FOR combinations. cross - entropy method ( CEM ) USED-FOR combination scheme. CEM - RL HYPONYM-OF method. sample efficiency EVALUATE-FOR CEM - RL. Generic are former, latter, and methods. OtherScientificTerm is hyper - parameter setting. Method are off - policy deep RL algorithm, and DDPG. Task is deep RL. ",This paper proposes a sample efficient off-policy deep RL algorithm based on the cross-entropy method (CEM-RL) for policy search in hyper-parameter setting. The proposed algorithm is a combination of the DDPG algorithm and the ad hoc evolutionary algorithm. The authors show that the proposed algorithm achieves better sample efficiency compared to the baselines. ,This paper proposes a sample efficient off-policy deep RL algorithm based on the cross-entropy method (CEM-RL) for policy search in hyper-parameter setting. The proposed algorithm is a combination of the DDPG algorithm and the ad hoc evolutionary algorithm. The authors show that the proposed algorithm achieves better sample efficiency compared to the baselines. 
8570,SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,forecasting EVALUATE-FOR model. multivariate time series USED-FOR predictive model. forecasting CONJUNCTION temporal and variable level importance interpretation. temporal and variable level importance interpretation CONJUNCTION forecasting. hidden state matrix CONJUNCTION update process. update process CONJUNCTION hidden state matrix. IMV - LSTM USED-FOR variableswise hidden states. hidden state matrix USED-FOR IMV - LSTM. update process USED-FOR IMV - LSTM. summarization methods USED-FOR temporal and variable importance. mixture attention mechanism CONJUNCTION summarization methods. summarization methods CONJUNCTION mixture attention mechanism. mixture attention mechanism USED-FOR temporal and variable importance. real datasets EVALUATE-FOR IMV - LSTM. IMV - LSTM COMPARE baselines. baselines COMPARE IMV - LSTM. real datasets EVALUATE-FOR baselines. end - to - end framework USED-FOR forecasting. end - to - end framework USED-FOR knowledge extraction. forecasting CONJUNCTION knowledge extraction. knowledge extraction CONJUNCTION forecasting. It USED-FOR knowledge extraction. It USED-FOR forecasting. It USED-FOR end - to - end framework. multi - variate data USED-FOR knowledge extraction. OtherScientificTerm is target and exogenous variables. Generic is it. ,This paper proposes a new model for multivariate time series forecasting. The proposed model is based on a mixture attention mechanism and an update process. The authors show that the proposed model outperforms baselines on a number of real-world datasets. ,This paper proposes a new model for multivariate time series forecasting. The proposed model is based on a mixture attention mechanism and an update process. The authors show that the proposed model outperforms baselines on a number of real-world datasets. 
8579,SP:1c26660569b579f060f7b4a31e321c6d2356b928,"adversarial examples USED-FOR defenses. defenses USED-FOR adversarial attacks. feature smoothing HYPONYM-OF data augmentation method. feature smoothing USED-FOR neural network. virtual training data USED-FOR neural network. interpolation of features USED-FOR neural network. feature smoothing USED-FOR virtual data points. logit squeezing USED-FOR feature smoothing. adversarial and clean accuracy EVALUATE-FOR feature smoothing. weight decay CONJUNCTION mix up. mix up CONJUNCTION weight decay. logit squeezing CONJUNCTION weight decay. weight decay CONJUNCTION logit squeezing. label smoothing CONJUNCTION logit squeezing. logit squeezing CONJUNCTION label smoothing. mix up CONJUNCTION feature smoothing. feature smoothing CONJUNCTION mix up. weight decay CONJUNCTION feature smoothing. feature smoothing CONJUNCTION weight decay. feature smoothing USED-FOR unbiased estimation of the decision boundary. label smoothing CONJUNCTION weight decay. weight decay CONJUNCTION label smoothing. symmetrical assumptions CONJUNCTION label smoothing. label smoothing CONJUNCTION symmetrical assumptions. estimated variance FEATURE-OF unbiased estimation of the decision boundary. weight decay HYPONYM-OF methods. OtherScientificTerm are computational overhead, computational burden, and decision boundary. Material is MNIST and CIFAR10 datasets. Method is data augmentation methods. Generic is unified framework. ","This paper proposes a unified framework for data augmentation methods for adversarial defense. The proposed framework is based on the idea of feature smoothing, which is used to improve the robustness of neural networks against adversarial attacks. The authors show that the proposed framework can be applied to several existing data augmentations, including weight decay, label smoothing and logit squeezing. The paper also provides a theoretical analysis of the performance of the proposed method. ","This paper proposes a unified framework for data augmentation methods for adversarial defense. The proposed framework is based on the idea of feature smoothing, which is used to improve the robustness of neural networks against adversarial attacks. The authors show that the proposed framework can be applied to several existing data augmentations, including weight decay, label smoothing and logit squeezing. The paper also provides a theoretical analysis of the performance of the proposed method. "
8588,SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"deep convolutional neural network ( DCNN ) HYPONYM-OF deep and locally connected nonlinear network. theoretical framework USED-FOR networks. ReLU nonlinearity FEATURE-OF networks. framework USED-FOR disentangled representations. framework USED-FOR data distribution. gradient descent rules USED-FOR data distribution. Batch Norm HYPONYM-OF regularization techniques. teacher - student setting USED-FOR framework. Gaussian inputs CONJUNCTION independence of activation. independence of activation CONJUNCTION Gaussian inputs. independence of activation HYPONYM-OF unrealistic assumptions. Gaussian inputs HYPONYM-OF unrealistic assumptions. disentangled representations PART-OF deep networks. OtherScientificTerm are projection nature, and teacher ’s computational graph. ","This paper studies the problem of learning disentangled representations of deep convolutional neural networks (DCNNs). The authors propose a theoretical framework for disentangling representations of DCNNs based on the ReLU nonlinearity of the data distribution. The authors show that under certain assumptions (i.e., Gaussian inputs and independence of activation), the disentanglement can be achieved in a teacher-student setting. They also show that this can be extended to the case where the teacher has a computational graph and the student does not. ","This paper studies the problem of learning disentangled representations of deep convolutional neural networks (DCNNs). The authors propose a theoretical framework for disentangling representations of DCNNs based on the ReLU nonlinearity of the data distribution. The authors show that under certain assumptions (i.e., Gaussian inputs and independence of activation), the disentanglement can be achieved in a teacher-student setting. They also show that this can be extended to the case where the teacher has a computational graph and the student does not. "
8597,SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"Prefrontal cortex ( PFC ) USED-FOR behavior repertoire. connectivity CONJUNCTION human behavior formation process. human behavior formation process CONJUNCTION connectivity. Behavioral Module ( BM ) CONJUNCTION end - to - end training strategy. end - to - end training strategy CONJUNCTION Behavioral Module ( BM ). Behavioral Module ( BM ) PART-OF modular architecture of neural networks. end - to - end training strategy PART-OF modular architecture of neural networks. approach USED-FOR learning of behaviors. learning of behaviors CONJUNCTION preferences representation. preferences representation CONJUNCTION learning of behaviors. approach USED-FOR preferences representation. property USED-FOR user modeling. property USED-FOR recommendation tasks. user modeling CONJUNCTION recommendation tasks. recommendation tasks CONJUNCTION user modeling. video games playing EVALUATE-FOR method. independent learning of new behavior patterns USED-FOR network extendability. strategy USED-FOR transfer of newly learned BMs. Task are dialog agents, and personalized representations of different user states. OtherScientificTerm is BMs. ","This paper proposes a method for learning behavioral modules (BM) for dialog agents. The proposed method is based on an end-to-end training strategy, where the BMs are learned in a modular fashion. The authors show that the proposed method can be applied to a variety of tasks, including recommendation tasks and user modeling. The method is evaluated on video games and recommendation tasks. ","This paper proposes a method for learning behavioral modules (BM) for dialog agents. The proposed method is based on an end-to-end training strategy, where the BMs are learned in a modular fashion. The authors show that the proposed method can be applied to a variety of tasks, including recommendation tasks and user modeling. The method is evaluated on video games and recommendation tasks. "
8606,SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,plastic changes in synaptic connectivity USED-FOR lifelong learning. neuromodulation USED-FOR changes. learning CONJUNCTION adaptation. adaptation CONJUNCTION learning. self - modifying abilities USED-FOR biological reinforcement learning. self - modifying abilities USED-FOR learning. self - modifying abilities USED-FOR adaptation. self - modifying abilities FEATURE-OF brain. brain USED-FOR learning. brain USED-FOR adaptation. neuromodulated plasticity USED-FOR artificial neural networks. gradient descent USED-FOR artificial neural networks. differentiable formulation USED-FOR neuromodulation of plasticity. neuromodulated plasticity USED-FOR neural networks. neuromodulated plasticity USED-FOR reinforcement learning and supervised learning tasks. neural networks USED-FOR reinforcement learning and supervised learning tasks. neuromodulated plastic LSTMs COMPARE LSTMs. LSTMs COMPARE neuromodulated plastic LSTMs. task EVALUATE-FOR LSTMs. task EVALUATE-FOR neuromodulated plastic LSTMs. task EVALUATE-FOR benchmark language modeling task. benchmark language modeling task EVALUATE-FOR neuromodulated plastic LSTMs. benchmark language modeling task EVALUATE-FOR LSTMs. differentiable neuromodulation of plasticity USED-FOR neural networks. Method is differentiable Hebbian plasticity. ,"This paper proposes a differentiable formulation of Hebbian plasticity, which can be used to improve the performance of neural networks in reinforcement learning and supervised learning tasks. The authors show that the proposed formulation is differentiable and can be applied to differentiable neural networks. They also show that this formulation is applicable to neural networks trained with gradient descent. ","This paper proposes a differentiable formulation of Hebbian plasticity, which can be used to improve the performance of neural networks in reinforcement learning and supervised learning tasks. The authors show that the proposed formulation is differentiable and can be applied to differentiable neural networks. They also show that this formulation is applicable to neural networks trained with gradient descent. "
8615,SP:1ab5d94d31e99351433436c026799c8aa597bf73,"quantization techniques USED-FOR inference latency / memory consumption. full precision model USED-FOR non - intrusive quantization technique. quantization training process COMPARE training process. training process COMPARE quantization training process. loss function USED-FOR reduced quantization error. binary quantization USED-FOR full precision accuracy. 2 bit quantization USED-FOR full precision accuracy. 1.5 bits hybrid model COMPARE TWN LSTM model. TWN LSTM model COMPARE 1.5 bits hybrid model. WikiText-2 EVALUATE-FOR TWN LSTM model. Method are Deep Neural Networks, and binary model. Generic is techniques. Material are CIFAR dataset, and ImageNet. ","This paper proposes a new quantization method for deep neural networks. The proposed method is based on the idea of binary quantization, which is a non-intrusive quantization technique that can be used to reduce the quantization error. The authors show that the proposed method outperforms existing quantization methods on WikiText-2, CIFAR-10, and ImageNet. ","This paper proposes a new quantization method for deep neural networks. The proposed method is based on the idea of binary quantization, which is a non-intrusive quantization technique that can be used to reduce the quantization error. The authors show that the proposed method outperforms existing quantization methods on WikiText-2, CIFAR-10, and ImageNet. "
8624,SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"class - irrelevant properties USED-FOR style. method USED-FOR content embedding. deep metric - learning technique USED-FOR method. deep metric - learning technique USED-FOR content embedding. content encoder PART-OF variational autoencoder ( VAE ). content encoder CONJUNCTION to - be - trained style encoder. to - be - trained style encoder CONJUNCTION content encoder. to - be - trained style encoder PART-OF variational autoencoder ( VAE ). auxiliary loss CONJUNCTION leakage filtering. leakage filtering CONJUNCTION auxiliary loss. style information USED-FOR reconstruction. style information PART-OF content representation. auxiliary loss PART-OF method. leakage filtering PART-OF method. content representation USED-FOR style representation. method USED-FOR data - set augmentation. pose CONJUNCTION expression. expression CONJUNCTION pose. expression CONJUNCTION hairstyle. hairstyle CONJUNCTION expression. lighting CONJUNCTION pose. pose CONJUNCTION lighting. decompositions USED-FOR classification. Recombinations USED-FOR creative exercise. Recombinations USED-FOR data set augmentation. approach USED-FOR content - style decomposition and recombination. specific domain knowledge USED-FOR approaches. human body pose HYPONYM-OF specific domain knowledge. leakage filtering USED-FOR STOC. objective PART-OF STOC. leakage filtering HYPONYM-OF objective. supervised training USED-FOR style and content representations. STOC USED-FOR content - style recombination. Material are visual domains, masterworks of art, and Open - Ended Content. Method are domain - independent method, and Decompositions. OtherScientificTerm are VAE reconstruction loss, content, within - class variation, betweenand within - class variation, and musical composition. Task are few - shot learning tasks, face - recognition task, and emotion - recognition task. ",This paper proposes a method for content-style decomposition and recombination for few-shot learning tasks. The method is based on a VAE-based method for learning a style encoder and a content encoder. The authors propose to use a deep metric-learning technique to learn the content embedding and the style embedding. The style embeddings are learned using a deep learning approach. The proposed method is evaluated on the face recognition task and emotion recognition task.,This paper proposes a method for content-style decomposition and recombination for few-shot learning tasks. The method is based on a VAE-based method for learning a style encoder and a content encoder. The authors propose to use a deep metric-learning technique to learn the content embedding and the style embedding. The style embeddings are learned using a deep learning approach. The proposed method is evaluated on the face recognition task and emotion recognition task.
8633,SP:d37e15cde7765fca87595a242f0a4511b3346d46,method USED-FOR deep reinforcement learning ( deep RL ) training. deep reinforcement learning ( deep RL ) training USED-FOR problems. state - action permissibility ( SAP ) FEATURE-OF problems. permissibility PART-OF SAP. deep RL algorithms USED-FOR state - action exploration. SAP property PART-OF deep RL algorithms. SAP property USED-FOR state - action exploration. SAP guidance USED-FOR training. ,This paper studies the problem of state-action permissibility (SAP) in reinforcement learning. The authors propose a new method for training deep RL algorithms that is based on the idea that the state-actions are permissible. They show that this property can be used to guide the training of deep RL models. They also show that the proposed method can be applied to a variety of RL problems.,This paper studies the problem of state-action permissibility (SAP) in reinforcement learning. The authors propose a new method for training deep RL algorithms that is based on the idea that the state-actions are permissible. They show that this property can be used to guide the training of deep RL models. They also show that the proposed method can be applied to a variety of RL problems.
8642,SP:20015d8b60e13300586b67c281858cbe28825c48,"random weights USED-FOR weight - tied multilayer vanilla autoencoders. random deep weight - tied autoencoder model USED-FOR approximate inference. deep autoencoders COMPARE shallow counterparts. shallow counterparts COMPARE deep autoencoders. layer - wise pre - training CONJUNCTION batch normalization. batch normalization CONJUNCTION layer - wise pre - training. batch normalization HYPONYM-OF techniques. layer - wise pre - training HYPONYM-OF techniques. tanh activation USED-FOR deep autoencoder. OtherScientificTerm are large dimensions, phase transition phenomena, reversibility, and Lipschitz activations. Task is training initialization practice. Method is analytical techniques. ",This paper proposes a random deep weight-tied multilayer autoencoder model for training deep neural networks. The proposed model is based on the idea of Lipschitz activations. The authors show that the proposed model outperforms the baselines in terms of accuracy and reversibility. They also show that their model is more robust to the phase transition phenomenon. ,This paper proposes a random deep weight-tied multilayer autoencoder model for training deep neural networks. The proposed model is based on the idea of Lipschitz activations. The authors show that the proposed model outperforms the baselines in terms of accuracy and reversibility. They also show that their model is more robust to the phase transition phenomenon. 
8651,SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"search problem USED-FOR construction of adversarial images. model evaluations USED-FOR sporadic feedback. low frequency component PART-OF discrete cosine transform ( DCT ). iterative principle USED-FOR search strategy. iterative principle USED-FOR algorithm. method USED-FOR targeted and untargeted attacks. query efficiency EVALUATE-FOR method. median queries USED-FOR Google Cloud Vision. algorithm USED-FOR adversarial black - box attacks. PyTorch code USED-FOR it. Generic is model. Task is Model evaluations. Metric is adversarial loss. Material are ResNet-50, and adversarial ImageNet image. ",This paper proposes a new method for generating adversarial black-box images. The proposed method is based on the discrete cosine transform (DCT) algorithm. The key idea is to use the low-frequency component of discrete cosines transform (DST) as a search strategy. The authors show that the proposed method can be applied to both targeted and untargeted attacks. ,This paper proposes a new method for generating adversarial black-box images. The proposed method is based on the discrete cosine transform (DCT) algorithm. The key idea is to use the low-frequency component of discrete cosines transform (DST) as a search strategy. The authors show that the proposed method can be applied to both targeted and untargeted attacks. 
8660,SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"temporal abstractions USED-FOR curse of dimensionality. Hierarchical Reinforcement Learning USED-FOR temporal abstractions. method USED-FOR temporal abstractions. options framework HYPONYM-OF hierarchical framework. heuristics USED-FOR Option discovery. method COMPARE discovering bottlenecks. discovering bottlenecks COMPARE method. method USED-FOR bottlenecks. Successor options HYPONYM-OF model. Successor representations USED-FOR Successor options. Successor representations USED-FOR model. pseudo - reward USED-FOR intra - option policies. primitive actions USED-FOR Successor representations. Incremental Successor options model USED-FOR options. grid worlds CONJUNCTION complex high dimensional environments. complex high dimensional environments CONJUNCTION grid worlds. complex high dimensional environments EVALUATE-FOR approach. Deepmind - Lab HYPONYM-OF complex high dimensional environments. grid worlds EVALUATE-FOR approach. OtherScientificTerm are task - agnostic transferable skills, bottleneck states, landmark ” sub - goals, well connected regions, and sub - goals. Task is discovering bottleneck states. ",This paper proposes a Hierarchical Reinforcement Learning (HRL) method for option discovery. The method is based on the idea that the bottleneck states of a task can be decomposed into sub-goals and sub-goal sub-problems. The key idea is to use a hierarchical approach to learn a set of options that can be used to discover bottlenecks in a given task. The approach is evaluated on a number of gridworlds and Deepmind-Lab environments. ,This paper proposes a Hierarchical Reinforcement Learning (HRL) method for option discovery. The method is based on the idea that the bottleneck states of a task can be decomposed into sub-goals and sub-goal sub-problems. The key idea is to use a hierarchical approach to learn a set of options that can be used to discover bottlenecks in a given task. The approach is evaluated on a number of gridworlds and Deepmind-Lab environments. 
8669,SP:12a172c1e2892d016b37932acfc48dcb56874a89,"probabilistic distributions USED-FOR domain division. problem USED-FOR recognition tasks. Open Set Learning ( OSL ) HYPONYM-OF recognition tasks. probabilistic way USED-FOR decision boundary. domain division algorithm USED-FOR recognition tasks. domain USED-FOR recognition tasks. bootstrapping CONJUNCTION KolmogorovSmirnov ( K - S ) Test. KolmogorovSmirnov ( K - S ) Test CONJUNCTION bootstrapping. statistical tools USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test HYPONYM-OF statistical tools. bootstrapping HYPONYM-OF statistical tools. uncertain domain PART-OF framework. OSL and G - ZSL benchmarks EVALUATE-FOR approach. Method are classifiers, and WSVM. OtherScientificTerm is known, unknown and uncertain domains. ",This paper proposes a domain division algorithm for open set learning (OSL) and G-ZSL. The proposed method is based on the KolmogorovSmirnov (K-S) Test and bootstrapping. The authors show that the proposed method outperforms state-of-the-art methods on both OSL and GZSL benchmarks.,This paper proposes a domain division algorithm for open set learning (OSL) and G-ZSL. The proposed method is based on the KolmogorovSmirnov (K-S) Test and bootstrapping. The authors show that the proposed method outperforms state-of-the-art methods on both OSL and GZSL benchmarks.
8678,SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"neural network USED-FOR classification and regression. softmax cross - entropy CONJUNCTION mean squared error. mean squared error CONJUNCTION softmax cross - entropy. maximum margin separation CONJUNCTION simplicity ( Occam ’s Razor ). simplicity ( Occam ’s Razor ) CONJUNCTION maximum margin separation. mean squared error HYPONYM-OF solutions. softmax cross - entropy HYPONYM-OF solutions. simplicity ( Occam ’s Razor ) HYPONYM-OF inductive structures. maximum margin separation HYPONYM-OF inductive structures. polar prototype networks HYPONYM-OF networks. polar prototypes USED-FOR structure. maximal separation FEATURE-OF they. angular distances USED-FOR training. training USED-FOR regression. higher - dimensional outputs USED-FOR regression. polar interpolation USED-FOR training. large margin separation CONJUNCTION semantic class structure. semantic class structure CONJUNCTION large margin separation. semantic class structure USED-FOR polar prototype networks. large margin separation USED-FOR polar prototype networks. classification COMPARE network methods. network methods COMPARE classification. regression CONJUNCTION classification. classification CONJUNCTION regression. OtherScientificTerm are layout structures, layout, polar prototype, hypersphere, semantic priors, class prototypes, prototypes, and output dimensions. Task is minimizing angular distances. ","This paper studies the problem of minimizing angular distances between two dimensions of a neural network. The authors propose a new class of networks called polar prototype networks, which are based on the idea of polar interpolation. The proposed network is based on a polar prototype network, which is a linear combination of a linear classifier and a polar interpolator. They show that the proposed network outperforms the state-of-the-art methods on classification and regression tasks. They also show that polar prototypes can be used to train a classifier on higher-dimensional outputs. ","This paper studies the problem of minimizing angular distances between two dimensions of a neural network. The authors propose a new class of networks called polar prototype networks, which are based on the idea of polar interpolation. The proposed network is based on a polar prototype network, which is a linear combination of a linear classifier and a polar interpolator. They show that the proposed network outperforms the state-of-the-art methods on classification and regression tasks. They also show that polar prototypes can be used to train a classifier on higher-dimensional outputs. "
8687,SP:d1034342785d133cf8372b8624897963cc2ee83a,"Reinforcement learning ( RL ) agents USED-FOR features. reward function USED-FOR features. it EVALUATE-FOR idea. it USED-FOR proof - of - concept environments. proof - of - concept environments EVALUATE-FOR idea. Maximum Causal Entropy IRL USED-FOR algorithm. Generic are preferences, and robot. OtherScientificTerm are implicit preference information, and side effects. ","This paper proposes a new algorithm for reinforcement learning (RL) based on the Maximum Causal Entropy (MCE) algorithm. The main idea of the algorithm is to learn a reward function that maximizes the mutual information between the reward function and the preferences of the agent. The authors show that the proposed algorithm can be applied to a variety of RL tasks, including reinforcement learning, reinforcement learning in a proof-of-concept setting, and reinforcement learning on a toy environment. ","This paper proposes a new algorithm for reinforcement learning (RL) based on the Maximum Causal Entropy (MCE) algorithm. The main idea of the algorithm is to learn a reward function that maximizes the mutual information between the reward function and the preferences of the agent. The authors show that the proposed algorithm can be applied to a variety of RL tasks, including reinforcement learning, reinforcement learning in a proof-of-concept setting, and reinforcement learning on a toy environment. "
8696,SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,method USED-FOR dependency structure between latent variables. deep generative models CONJUNCTION probabilistic graphical models. probabilistic graphical models CONJUNCTION deep generative models. deep generative models PART-OF modeling and inference framework. probabilistic graphical models PART-OF modeling and inference framework. latent variable space FEATURE-OF variational autoencoder ( VAE ). flexible dependency structure FEATURE-OF Bayesian network. Bayesian network USED-FOR variational autoencoder ( VAE ). Bayesian network USED-FOR latent variable space. network parameters CONJUNCTION variational parameters. variational parameters CONJUNCTION network parameters. variational parameters CONJUNCTION latent topology. latent topology CONJUNCTION variational parameters. single objective USED-FOR latent topology. single objective USED-FOR variational parameters. single objective USED-FOR network parameters. latent variable values FEATURE-OF top - down and bottom - up reasoning. top - down and bottom - up reasoning USED-FOR Inference. sampling procedure USED-FOR Inference. MNIST CONJUNCTION Omniglot. Omniglot CONJUNCTION MNIST. Omniglot CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Omniglot. Omniglot EVALUATE-FOR framework. CIFAR-10 EVALUATE-FOR framework. MNIST EVALUATE-FOR framework. structured variational autoencoder baselines COMPARE model. model COMPARE structured variational autoencoder baselines. Method is deep latent variable models. OtherScientificTerm is latent variable structures. ,"This paper proposes a new model for deep generative models. The proposed model is based on a variational autoencoder (VAE) and a Bayesian network. The main idea is to learn the dependency structure between the latent variables and the variational parameters of the VAE. The model is evaluated on MNIST, Omniglot, and CIFAR-10 datasets. ","This paper proposes a new model for deep generative models. The proposed model is based on a variational autoencoder (VAE) and a Bayesian network. The main idea is to learn the dependency structure between the latent variables and the variational parameters of the VAE. The model is evaluated on MNIST, Omniglot, and CIFAR-10 datasets. "
8705,SP:976dedab53e69610692a563382ada1dbb82c1e9d,"interconnected neurons PART-OF dynamical neural network. numerical solutions USED-FOR mathematical optimization or learning problems. computational properties FEATURE-OF It. it CONJUNCTION massively parallel computer architecture. massively parallel computer architecture CONJUNCTION it. massively parallel computer architecture USED-FOR power and throughput efficiency. local memory HYPONYM-OF local information. dynamical network USED-FOR gradients. top - down feedback CONJUNCTION contrastive learning. contrastive learning CONJUNCTION top - down feedback. dynamical network USED-FOR ` 1 - minimizing dictionary learning problem. top - down feedback USED-FOR dynamical network. contrastive learning USED-FOR dynamical network. gradients USED-FOR learning. spiking neurons USED-FOR dynamical network. OtherScientificTerm is state space. Method are computational system, and learning process. Task is dictionary learning problems. ","This paper proposes a dynamical neural network (DNN) for solving dictionary learning problems. The proposed method is based on the idea of spiking neurons. The authors show that the dynamical network can be used to solve the 1-minimizing dictionary learning problem, which is a well-studied problem in the literature. They also show that it can be combined with top-down feedback and contrastive learning to improve the performance of DNNs. ","This paper proposes a dynamical neural network (DNN) for solving dictionary learning problems. The proposed method is based on the idea of spiking neurons. The authors show that the dynamical network can be used to solve the 1-minimizing dictionary learning problem, which is a well-studied problem in the literature. They also show that it can be combined with top-down feedback and contrastive learning to improve the performance of DNNs. "
8714,SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"spatial pyramid structure CONJUNCTION encoder - decoder structure. encoder - decoder structure CONJUNCTION spatial pyramid structure. semantic image segmentation CONJUNCTION lane detection. lane detection CONJUNCTION semantic image segmentation. spatial pyramid structure USED-FOR nets. encoder - decoder structure FEATURE-OF nets. nets USED-FOR lane detection. nets USED-FOR semantic image segmentation. weak visual appearance CONJUNCTION prior information. prior information CONJUNCTION weak visual appearance. multi - scale context CONJUNCTION pixel - level accuracy. pixel - level accuracy CONJUNCTION multi - scale context. network USED-FOR lane detection. encoder - decoders module USED-FOR lane detection. evaluation methods EVALUATE-FOR lane detection. Method are Convolutional neural networks ( CNNs ), and encoder - decoders nets. Task are lane detection task, and model - based lane detection. Generic is methods. ","This paper proposes a new method for lane detection based on the spatial pyramid structure of CNNs. The proposed method is based on an encoder-decoder architecture and is able to achieve state-of-the-art performance on the task of lane detection. The method is evaluated on a variety of tasks, including semantic image segmentation, lane detection, and semantic image classification. ","This paper proposes a new method for lane detection based on the spatial pyramid structure of CNNs. The proposed method is based on an encoder-decoder architecture and is able to achieve state-of-the-art performance on the task of lane detection. The method is evaluated on a variety of tasks, including semantic image segmentation, lane detection, and semantic image classification. "
8723,SP:68b0a10ca06df74612d0753cc3f3ddddde806035,policy COMPARE off - policy training data. off - policy training data COMPARE policy. supervised learning and online learning settings COMPARE batch contextual bandit learning. batch contextual bandit learning COMPARE supervised learning and online learning settings. ad platforms CONJUNCTION recommendation systems. recommendation systems CONJUNCTION ad platforms. batch learning setting USED-FOR online and interactive systems. ad platforms HYPONYM-OF online and interactive systems. recommendation systems HYPONYM-OF online and interactive systems. Policy Optimizer USED-FOR Exponential Models ( POEM ). Inverse Propensity Scoring ( IPS ) CONJUNCTION Policy Optimizer. Policy Optimizer CONJUNCTION Inverse Propensity Scoring ( IPS ). Policy Optimizer HYPONYM-OF approaches. Inverse Propensity Scoring ( IPS ) HYPONYM-OF approaches. inverse propensity weights USED-FOR approaches. Maximum Likelihood Inverse Propensity Scoring ( MLIPS ) USED-FOR batch learning. approach USED-FOR batch learning. logged bandit feedback USED-FOR Maximum Likelihood Inverse Propensity Scoring ( MLIPS ). logged bandit feedback USED-FOR batch learning. historical policy USED-FOR inverse propensity weights. logged action - context pairs USED-FOR maximum likelihood surrogate policy. MLIPS COMPARE IPS. IPS COMPARE MLIPS. nonasymptotic mean squared error EVALUATE-FOR IPS. nonasymptotic mean squared error EVALUATE-FOR MLIPS. surrogate policy COMPARE historical policy. historical policy COMPARE surrogate policy. large - scale ad placement dataset EVALUATE-FOR MLIPS. multi - label classification problems CONJUNCTION large - scale ad placement dataset. large - scale ad placement dataset CONJUNCTION multi - label classification problems. multi - label classification problems EVALUATE-FOR MLIPS. surrogate policy technique COMPARE error reduction techniques. error reduction techniques COMPARE surrogate policy technique. surrogate policy technique USED-FOR approaches. OtherScientificTerm is logged feedback. Metric is mean squared error. ,"This paper proposes a new method for batch contextual bandit learning. The proposed method, Maximum Likelihood Inverse Propensity Scoring (MLIPS), is based on the notion of logged bandit feedback. The authors show that the proposed method outperforms existing methods in terms of mean squared error. They also show that their method is able to achieve better performance than existing methods. ","This paper proposes a new method for batch contextual bandit learning. The proposed method, Maximum Likelihood Inverse Propensity Scoring (MLIPS), is based on the notion of logged bandit feedback. The authors show that the proposed method outperforms existing methods in terms of mean squared error. They also show that their method is able to achieve better performance than existing methods. "
8732,SP:8e0ed65c5dded23b34798499b2436b24422fd729,learning framework USED-FOR few - shot classification tasks. Meta - learning USED-FOR learning framework. Meta - learning USED-FOR few - shot classification tasks. meta - learner USED-FOR model optimization. parameter initialization CONJUNCTION similarity metric. similarity metric CONJUNCTION parameter initialization. model optimization CONJUNCTION parameter initialization. parameter initialization CONJUNCTION model optimization. meta - learner PART-OF meta - learning methods. individualized feature embedding USED-FOR classifying. feature embedding USED-FOR individualized feature space. kernel generator USED-FOR feature embedding. feature embedding USED-FOR query images. kernel generator USED-FOR meta - learner. meta - knowledge USED-FOR convolutional kernels. kernel generator USED-FOR convolutional kernels. training USED-FOR convolutional kernels. meta - knowledge USED-FOR kernel generator. few - shot classification data sets EVALUATE-FOR method. Omniglot CONJUNCTION miniImageNet. miniImageNet CONJUNCTION Omniglot. miniImageNet EVALUATE-FOR method. miniImageNet HYPONYM-OF few - shot classification data sets. Omniglot HYPONYM-OF few - shot classification data sets. Method is fine - tuning. ,"This paper proposes a meta-learning framework for few-shot classification tasks. The proposed method is based on the idea of meta-learner, which is an extension of the meta-learners framework. The key idea is to use a kernel generator to learn a feature embedding for each query image. The kernel generator is then used to train a meta learner that learns the feature embeddings for the query images. The authors show that the proposed method outperforms the baselines on Omniglot, miniImageNet, and mini-ImageNet.","This paper proposes a meta-learning framework for few-shot classification tasks. The proposed method is based on the idea of meta-learner, which is an extension of the meta-learners framework. The key idea is to use a kernel generator to learn a feature embedding for each query image. The kernel generator is then used to train a meta learner that learns the feature embeddings for the query images. The authors show that the proposed method outperforms the baselines on Omniglot, miniImageNet, and mini-ImageNet."
8741,SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,backpropagation HYPONYM-OF gradient - based learning algorithms. gradient - based learning algorithms USED-FOR Deep artificial neural networks ( DNNs ). Q - learning CONJUNCTION policy gradients. policy gradients CONJUNCTION Q - learning. Evolution strategies ( ES ) COMPARE backprop - based algorithms. backprop - based algorithms COMPARE Evolution strategies ( ES ). policy gradients USED-FOR deep reinforcement learning ( RL ) problems. backprop - based algorithms USED-FOR deep reinforcement learning ( RL ) problems. policy gradients HYPONYM-OF backprop - based algorithms. Q - learning HYPONYM-OF backprop - based algorithms. it USED-FOR stochastic gradient descent. ES HYPONYM-OF gradient - based algorithm. finite - difference approximation of the gradient HYPONYM-OF operation. operation USED-FOR it. operation USED-FOR stochastic gradient descent. non - gradient - based evolutionary algorithms USED-FOR DNN scales. Atari CONJUNCTION humanoid locomotion. humanoid locomotion CONJUNCTION Atari. it USED-FOR hard deep RL problems. humanoid locomotion HYPONYM-OF hard deep RL problems. Atari HYPONYM-OF hard deep RL problems. Deep GA USED-FOR networks. free parameters FEATURE-OF networks. evolutionary algorithm USED-FOR neural networks. ES CONJUNCTION GA. GA CONJUNCTION ES. DNNs CONJUNCTION novelty search. novelty search CONJUNCTION DNNs. A3C CONJUNCTION ES. ES CONJUNCTION A3C. novelty search USED-FOR exploration. DQN CONJUNCTION A3C. A3C CONJUNCTION DQN. DNNs USED-FOR high - dimensional problem. reward - maximizing algorithms USED-FOR high - dimensional problem. GA HYPONYM-OF reward - maximizing algorithms. ES HYPONYM-OF reward - maximizing algorithms. DQN HYPONYM-OF reward - maximizing algorithms. A3C HYPONYM-OF reward - maximizing algorithms. A3C CONJUNCTION DQN. DQN CONJUNCTION A3C. ES CONJUNCTION A3C. A3C CONJUNCTION ES. Deep GA,"This paper studies the evolution strategies (ES) of backprop-based algorithms for deep reinforcement learning (RL) problems. In particular, the authors propose a new evolutionary algorithm, Deep Evolution Strategies (Deep GA), which is based on a finite-difference approximation of the gradient of the policy gradients. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on Atari, DQN, and A3C. ","This paper studies the evolution strategies (ES) of backprop-based algorithms for deep reinforcement learning (RL) problems. In particular, the authors propose a new evolutionary algorithm, Deep Evolution Strategies (Deep GA), which is based on a finite-difference approximation of the gradient of the policy gradients. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on Atari, DQN, and A3C. "
8750,SP:dfdbe3267a8160f24746884cdf5297993e424231,"rewards USED-FOR learning. episodic memory USED-FOR novelty bonus. episodic memory USED-FOR curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. DMLab CONJUNCTION MuJoCo. MuJoCo CONJUNCTION DMLab. VizDoom FEATURE-OF visually rich 3D environments. visually rich 3D environments EVALUATE-FOR approach. agent COMPARE curiosity method. curiosity method COMPARE agent. agent COMPARE ICM. ICM COMPARE agent. navigational tasks EVALUATE-FOR agent. curiosity method COMPARE ICM. ICM COMPARE curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. ant USED-FOR MuJoCo. curiosity module PART-OF ant. OtherScientificTerm are Rewards, sparsity, curious behaviour, real task reward, environment dynamics, and first - person - view curiosity. Method are reinforcement learning algorithms, and RL algorithms. ","This paper proposes a curiosity module for reinforcement learning. The novelty bonus is used to encourage the agent to explore the environment. Experiments are conducted on MuJoCo, DMLab and VizDoom environments. ","This paper proposes a curiosity module for reinforcement learning. The novelty bonus is used to encourage the agent to explore the environment. Experiments are conducted on MuJoCo, DMLab and VizDoom environments. "
8759,SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,representation USED-FOR transition models. complex uncertain domains FEATURE-OF transition models. relational rules USED-FOR representation. iterative greedy algorithm USED-FOR deictic references. Feed - forward neural networks USED-FOR transition distribution. strategy COMPARE monolithic transition model. monolithic transition model COMPARE strategy. simulated domain EVALUATE-FOR monolithic transition model. OtherScientificTerm is rule. ,This paper studies the problem of representation learning in the context of transition models. The authors propose an iterative greedy algorithm to learn the deictic references of a transition model. The proposed algorithm is based on feedforward neural networks. The paper shows that the proposed algorithm outperforms a monolithic transition model on a simulated domain. ,This paper studies the problem of representation learning in the context of transition models. The authors propose an iterative greedy algorithm to learn the deictic references of a transition model. The proposed algorithm is based on feedforward neural networks. The paper shows that the proposed algorithm outperforms a monolithic transition model on a simulated domain. 
8768,SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"INVASE HYPONYM-OF instance - wise feature selection method. selector network CONJUNCTION predictor network. predictor network CONJUNCTION selector network. predictor network CONJUNCTION baseline network. baseline network CONJUNCTION predictor network. neural networks CONJUNCTION selector network. selector network CONJUNCTION neural networks. baseline network USED-FOR selector network. actor - critic methodology USED-FOR baseline network. actor - critic methodology USED-FOR INVASE. neural networks PART-OF INVASE. predictor network PART-OF INVASE. selector network PART-OF INVASE. baseline network PART-OF INVASE. methodology USED-FOR INVASE. INVASE COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE INVASE. synthetic and real data experiments EVALUATE-FOR INVASE. Material is big data. OtherScientificTerm is features. Task are global feature selection, and instance - wise feature selection. Generic is state - of - the - art methods. ","This paper proposes a new instance-wise feature selection method called INVASE. The proposed method is based on the actor-critic methodology and is able to achieve state-of-the-art performance on synthetic and real-world datasets. The method consists of three components: a baseline network, a selector network, and a predictor network. The selector network is trained using the Actor-Critic method, while the predictor network is learned using a neural network. Experiments show that the proposed method outperforms the baselines.","This paper proposes a new instance-wise feature selection method called INVASE. The proposed method is based on the actor-critic methodology and is able to achieve state-of-the-art performance on synthetic and real-world datasets. The method consists of three components: a baseline network, a selector network, and a predictor network. The selector network is trained using the Actor-Critic method, while the predictor network is learned using a neural network. Experiments show that the proposed method outperforms the baselines."
8777,SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"per - pixel annotations USED-FOR supervised models. semantic segmentation HYPONYM-OF Predicting structured outputs. convolutional neural networks HYPONYM-OF supervised models. per - pixel annotations USED-FOR Predicting structured outputs. annotations USED-FOR model finetuning. disentangled space USED-FOR discriminative feature representations of patches. label histograms USED-FOR discriminative feature representations of patches. adversarial learning scheme USED-FOR feature representations. representations USED-FOR guidance. global alignment process CONJUNCTION patch - level alignment. patch - level alignment CONJUNCTION global alignment process. global alignment process USED-FOR framework. semantic segmentation EVALUATE-FOR framework. patch - level alignment USED-FOR framework. Generic are models, and benchmark datasets. Task is annotation. Method is domain adaptation method. ","This paper proposes a framework for learning per-pixel annotations for semantic segmentation. The proposed method is based on the adversarial learning framework. The authors propose to learn a disentangled space for each patch of the dataset, which is then used to learn the discriminative feature representations of patches. They also propose a global alignment process for patch-level alignment. Experiments show that the proposed method outperforms baselines on several benchmark datasets. ","This paper proposes a framework for learning per-pixel annotations for semantic segmentation. The proposed method is based on the adversarial learning framework. The authors propose to learn a disentangled space for each patch of the dataset, which is then used to learn the discriminative feature representations of patches. They also propose a global alignment process for patch-level alignment. Experiments show that the proposed method outperforms baselines on several benchmark datasets. "
8786,SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"optimistic algorithms USED-FOR AMSGrad. AMSGrad CONJUNCTION Adam. Adam CONJUNCTION AMSGrad. optimistic algorithms USED-FOR Adam. predictability of gradients USED-FOR optimistic algorithms. momentum method CONJUNCTION adaptive gradient method. adaptive gradient method CONJUNCTION momentum method. algorithms USED-FOR OPTIMISTIC ONLINE LEARNING. adaptive gradient method CONJUNCTION algorithms. algorithms CONJUNCTION adaptive gradient method. algorithms USED-FOR algorithms. adaptive gradient method USED-FOR algorithms. momentum method USED-FOR algorithms. Method are optimization algorithms, and deep neural nets. OtherScientificTerm is mini - batch of stochastic gradients. Task is online learning literature. ","This paper studies the problem of online learning with stochastic gradient descent. The authors propose two algorithms for online learning, AMSGrad and Adam. The main contribution of the paper is to show that the adaptive gradient method and momentum method can be used to improve the predictability of gradient descent algorithms. They also show that adaptive gradient methods can improve the performance of Adam. ","This paper studies the problem of online learning with stochastic gradient descent. The authors propose two algorithms for online learning, AMSGrad and Adam. The main contribution of the paper is to show that the adaptive gradient method and momentum method can be used to improve the predictability of gradient descent algorithms. They also show that adaptive gradient methods can improve the performance of Adam. "
8795,SP:52228b48f2776d57dd422edb33b82e247f056b75,benchmarks USED-FOR image classifier robustness. classifiers USED-FOR safety - critical applications. benchmark USED-FOR corruption robustness topic. IMAGENET - C USED-FOR corruption robustness topic. IMAGENET - C HYPONYM-OF benchmark. dataset EVALUATE-FOR classifier. common perturbations FEATURE-OF classifier ’s robustness. IMAGENET - P HYPONYM-OF dataset. common corruptions CONJUNCTION perturbations. perturbations CONJUNCTION common corruptions. benchmark USED-FOR perturbations. perturbations COMPARE worst - case adversarial perturbations. worst - case adversarial perturbations COMPARE perturbations. common corruptions FEATURE-OF benchmark. AlexNet classifiers CONJUNCTION ResNet classifiers. ResNet classifiers CONJUNCTION AlexNet classifiers. relative corruption robustness EVALUATE-FOR ResNet classifiers. relative corruption robustness EVALUATE-FOR AlexNet classifiers. common perturbation robustness EVALUATE-FOR bypassed adversarial defense. Generic is networks. ,This paper presents a new benchmark for image classifier robustness. The benchmark is based on the ImageNet-C dataset and is designed to evaluate the robustness of a classifier to common perturbations and adversarial attacks. The authors also propose a bypassed adversarial defense method to improve the adversarial robustness performance of the classifier. ,This paper presents a new benchmark for image classifier robustness. The benchmark is based on the ImageNet-C dataset and is designed to evaluate the robustness of a classifier to common perturbations and adversarial attacks. The authors also propose a bypassed adversarial defense method to improve the adversarial robustness performance of the classifier. 
8804,SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"MAP estimation USED-FOR dropout training. model PART-OF family. models USED-FOR power mean. lower bounds FEATURE-OF stochastic subvariants. sampled dropout masks USED-FOR power mean. models PART-OF family. deterministic dropout USED-FOR MC averaging. Task is dropout. Method are conditional models, and regularisation - heavy language modelling. OtherScientificTerm are dropout objective, and deterministic subvariant ’s bound. Generic is It. ","This paper studies the problem of dropout training for conditional models. The authors consider the case of stochastic subvariants, where the dropout objective is deterministic. They show that the power mean of the target model is bounded by a deterministic subvariant, and they prove a lower bound for the deterministic lower bound. They also show that deterministic dropout can be used for MC averaging. ","This paper studies the problem of dropout training for conditional models. The authors consider the case of stochastic subvariants, where the dropout objective is deterministic. They show that the power mean of the target model is bounded by a deterministic subvariant, and they prove a lower bound for the deterministic lower bound. They also show that deterministic dropout can be used for MC averaging. "
8840,SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"redundant filters PART-OF Convolutional Neural Networks ( CNNs ). robust pruning method USED-FOR GSFP. soft pruning strategy USED-FOR GSFP. cumulative saliency strategy USED-FOR pruning. accuracy EVALUATE-FOR pruning. cumulative saliency strategy USED-FOR accuracy. pruning USED-FOR model recovery process. saliency FEATURE-OF filter. saliency FEATURE-OF filter. saliency USED-FOR pruning. pruning COMPARE local pruning. local pruning COMPARE pruning. normalization formula USED-FOR layers of filters. layers of filters PART-OF network. CNN architectures CONJUNCTION data sets. data sets CONJUNCTION CNN architectures. CNN architectures EVALUATE-FOR GSFP. data sets EVALUATE-FOR GSFP. GSFP USED-FOR global and soft pruning strategies. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 EVALUATE-FOR it. MNIST EVALUATE-FOR it. test accuracy EVALUATE-FOR it. compression ratio EVALUATE-FOR it. OtherScientificTerm are global redundancy, and excessive pruning rate. Generic is model. Task is pruning guidance. Method is pre - trained CNN model. ",This paper proposes a robust pruning method for convolutional neural networks (CNNs). The proposed method is based on the cumulative saliency strategy (GSFP). The authors show that the proposed method outperforms local pruning and soft pruning. The authors also show that GSFP outperforms global pruning on MNIST and CIFAR-10.,This paper proposes a robust pruning method for convolutional neural networks (CNNs). The proposed method is based on the cumulative saliency strategy (GSFP). The authors show that the proposed method outperforms local pruning and soft pruning. The authors also show that GSFP outperforms global pruning on MNIST and CIFAR-10.
8849,SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,character - based embedder CONJUNCTION word - based classifier. word - based classifier CONJUNCTION character - based embedder. transfer learning scheme USED-FOR cross - lingual subword similarity. limited training data USED-FOR transfer learning scheme. character - based embedder USED-FOR transfer learning scheme. embedder USED-FOR vector representations. written forms USED-FOR vector representations. word vectors USED-FOR classifier. multi - task objective USED-FOR model. CACO models COMPARE cross - lingual word embedding models. cross - lingual word embedding models COMPARE CACO models. low - resource settings USED-FOR CACO models. related language pairs USED-FOR cross - lingual word embedding models. high - resource settings USED-FOR cross - lingual word embedding models. Task is Text classification. Method is joint character representation. Material is cross - lingual or monolingual resources. ,This paper proposes a novel transfer learning scheme for cross-lingual word embedding models. The key idea is to use a character-based embedder and a word-based classifier to learn the cross-linking subword similarity between two word pairs. The authors show that the proposed method is able to achieve better transfer learning performance compared to existing cross-language word embeddings. ,This paper proposes a novel transfer learning scheme for cross-lingual word embedding models. The key idea is to use a character-based embedder and a word-based classifier to learn the cross-linking subword similarity between two word pairs. The authors show that the proposed method is able to achieve better transfer learning performance compared to existing cross-language word embeddings. 
8858,SP:544e421f9c747640d949f433e3091763508b7237,"marginalized average aggregation ( MAA ) module USED-FOR MAAN. latent discriminative probabilities USED-FOR MAA. latent discriminative probabilities USED-FOR MAA module. MAAN USED-FOR dense and integral action regions. MAAN USED-FOR class activation sequences. algorithm USED-FOR MAA. algorithm USED-FOR complexity. large - scale video datasets EVALUATE-FOR MAAN. MAAN USED-FOR weakly - supervised temporal action localization. large - scale video datasets EVALUATE-FOR weakly - supervised temporal action localization. OtherScientificTerm are dense and integral regions, overestimation of the most salient regions, video snippet features, averaged subset features, and O(T ). Method is marginalized average attentional network ( MAAN ). ","This paper proposes a new method for improving the performance of the marginalized average attentional network (MAAN) for video segmentation tasks. The proposed method is based on the marginalized average aggregation (MAA) module, which is an extension of the MAA module. The main contribution of the paper is to propose a new algorithm for the marginalised average aggregation. The authors show that the proposed algorithm is able to achieve better performance than the existing methods in terms of O(T) and O(O(T^2) complexity. They also show that their method can be used to improve the performance in the context of weakly supervised temporal action localization.","This paper proposes a new method for improving the performance of the marginalized average attentional network (MAAN) for video segmentation tasks. The proposed method is based on the marginalized average aggregation (MAA) module, which is an extension of the MAA module. The main contribution of the paper is to propose a new algorithm for the marginalised average aggregation. The authors show that the proposed algorithm is able to achieve better performance than the existing methods in terms of O(T) and O(O(T^2) complexity. They also show that their method can be used to improve the performance in the context of weakly supervised temporal action localization."
8867,SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"neural models USED-FOR Natural Language Processing. structureless distributed representations USED-FOR neural models. models COMPARE representational form. representational form COMPARE models. structures PART-OF wordlevel and chunk - level representations. HRR USED-FOR models. models USED-FOR crude linguistic roles. HRR USED-FOR structured compositional representation. OtherScientificTerm are linguistic structures, and syntax. Method are language models, and Holographic Reduced Representation ( HRR ). ","This paper proposes a new representation for natural language processing, called Holographic Reduced Representation (HRR). HRR is an approach to reduce the number of word-level and chunk-level representations in a language model. The authors show that HRR can be used to improve the performance of language models on a variety of tasks. They also show that the proposed method can be applied to language models trained on synthetic data. ","This paper proposes a new representation for natural language processing, called Holographic Reduced Representation (HRR). HRR is an approach to reduce the number of word-level and chunk-level representations in a language model. The authors show that HRR can be used to improve the performance of language models on a variety of tasks. They also show that the proposed method can be applied to language models trained on synthetic data. "
8876,SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"Partially observable Markov decision processes ( POMDPs ) USED-FOR decision - making. perception decision CONJUNCTION planning decision. planning decision CONJUNCTION perception decision. greedy strategy USED-FOR observation selection. point - based value iteration algorithm USED-FOR near - optimal uncertainty reduction. greedy strategy USED-FOR near - optimal uncertainty reduction. sampled belief points USED-FOR near - optimal uncertainty reduction. greedy strategy PART-OF point - based value iteration algorithm. solver USED-FOR reachable subspace of belief simplex. computations USED-FOR perception. planning HYPONYM-OF computations. active perception CONJUNCTION planning. planning CONJUNCTION active perception. OtherScientificTerm are stochastic outcome, known distribution, real - world scenarios, and action space. Method are POMDP models, and selection process. Material is robotic scenarios. ","This paper studies the problem of learning a POMDP model for partially observable Markov decision processes (POMDPs). In particular, the authors propose a greedy strategy for observation selection, which is based on a point-based value iteration algorithm. The authors show that the greedy strategy is able to achieve near-optimal uncertainty reduction in the action space. They also show that it is possible to find a reachable subspace of belief simplex that can be used to solve the problem. ","This paper studies the problem of learning a POMDP model for partially observable Markov decision processes (POMDPs). In particular, the authors propose a greedy strategy for observation selection, which is based on a point-based value iteration algorithm. The authors show that the greedy strategy is able to achieve near-optimal uncertainty reduction in the action space. They also show that it is possible to find a reachable subspace of belief simplex that can be used to solve the problem. "
8885,SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"Curriculum learning USED-FOR network. data complexity CONJUNCTION network training. network training CONJUNCTION data complexity. internal covariate shift PART-OF network forward pass. representation loss USED-FOR low weighted samples. adaptive weight CONJUNCTION representation loss. representation loss CONJUNCTION adaptive weight. adaptive weight PART-OF curriculum loss. representation loss PART-OF curriculum loss. random sampling USED-FOR curriculum learning. curriculum loss CONJUNCTION stochastic algorithms. stochastic algorithms CONJUNCTION curriculum loss. curriculum loss COMPARE SGD. SGD COMPARE curriculum loss. SGD HYPONYM-OF stochastic algorithms. Method are Deep neural networks, top layers, and learning of top layers. OtherScientificTerm are distribution changes in weight of top layers, backward pass, hard examples, noisy gradients, embedding space, fluctuation of top layers, and hard samples. Task are distribution shifting, and training. Material are Low - weighted data, and benchmark datasets. ",This paper studies the problem of curriculum learning for low-weighted data. The authors propose a new curriculum loss based on the adaptive weight and the representation loss. They show that the proposed curriculum loss outperforms SGD and stochastic algorithms. ,This paper studies the problem of curriculum learning for low-weighted data. The authors propose a new curriculum loss based on the adaptive weight and the representation loss. They show that the proposed curriculum loss outperforms SGD and stochastic algorithms. 
8894,SP:8b555b9f24044bc68c204169d6a37e262361d706,"heuristics USED-FOR combinatorial optimization problems. REINFORCE USED-FOR baseline. value function USED-FOR baseline. deterministic greedy rollout USED-FOR baseline. attention layers USED-FOR model. REINFORCE USED-FOR model. heuristics USED-FOR Travelling Salesman Problem ( TSP ). heuristics USED-FOR Vehicle Routing Problem ( VRP ). hyperparameters USED-FOR heuristics. Orienteering Problem ( OP ) HYPONYM-OF Vehicle Routing Problem ( VRP ). Generic are it, models, problems, and baselines. Method are Pointer Network, and Prize Collecting TSP ( PCTSP ). ","This paper studies the problem of Travelling Salesman Problem (TSP) and Prize Collecting TSP (PCTSP), which is a combinatorial optimization problem where the goal is to find a solution that maximizes the number of tickets collected by a travelling salesman. The authors propose a new heuristic for solving the TSP problem, called REINFORCE, which is based on a deterministic greedy rollout method. The proposed method is evaluated on a number of tasks, and shows that it outperforms existing heuristics and baselines.","This paper studies the problem of Travelling Salesman Problem (TSP) and Prize Collecting TSP (PCTSP), which is a combinatorial optimization problem where the goal is to find a solution that maximizes the number of tickets collected by a travelling salesman. The authors propose a new heuristic for solving the TSP problem, called REINFORCE, which is based on a deterministic greedy rollout method. The proposed method is evaluated on a number of tasks, and shows that it outperforms existing heuristics and baselines."
8903,SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"time and space complexity EVALUATE-FOR neural network inference. network quantization USED-FOR neural network inference. limited computational and memory resources FEATURE-OF embedded and mobile devices. differentiable neural architecture search ( DNAS ) framework USED-FOR exponential search space. gradient - based optimization USED-FOR differentiable neural architecture search ( DNAS ) framework. neural architecture search problem USED-FOR problem. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR ResNet. ImageNet EVALUATE-FOR ResNet. quantized models COMPARE full precision models. full precision models COMPARE quantized models. model size CONJUNCTION computational cost. computational cost CONJUNCTION model size. computational cost EVALUATE-FOR full precision models. model size EVALUATE-FOR quantized models. computational cost EVALUATE-FOR quantized models. Method is quantization methods. OtherScientificTerm are design space, and bit - widths. ","This paper proposes a differentiable neural architecture search (DNAS) framework for neural network inference. The proposed method is based on gradient-based optimization. The authors show that the proposed method outperforms existing quantization methods on CIFAR-10, ImageNet, and ResNet. ","This paper proposes a differentiable neural architecture search (DNAS) framework for neural network inference. The proposed method is based on gradient-based optimization. The authors show that the proposed method outperforms existing quantization methods on CIFAR-10, ImageNet, and ResNet. "
8912,SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"attention USED-FOR neural architectures. attention USED-FOR decoding stage. posterior attention distribution USED-FOR attention. posterior attention models COMPARE attention models. attention models COMPARE posterior attention models. morphological inflection tasks EVALUATE-FOR posterior attention models. alignment accuracy EVALUATE-FOR attention models. translation EVALUATE-FOR posterior attention models. translation CONJUNCTION morphological inflection tasks. morphological inflection tasks CONJUNCTION translation. BLEU score EVALUATE-FOR attention models. BLEU score CONJUNCTION alignment accuracy. alignment accuracy CONJUNCTION BLEU score. morphological inflection tasks EVALUATE-FOR attention models. translation EVALUATE-FOR attention models. alignment accuracy EVALUATE-FOR posterior attention models. BLEU score EVALUATE-FOR posterior attention models. Method are attention architectures, and Posterior Attention Models. Generic is architecture. ","This paper proposes a new architecture for posterior attention, which is based on the posterior attention distribution. The authors show that posterior attention models are able to achieve better alignment and alignment accuracy than prior attention models. They also show that the BLEU score of posterior attention can be used to evaluate the performance of attention models on a variety of tasks, including translation and morphological inflection. ","This paper proposes a new architecture for posterior attention, which is based on the posterior attention distribution. The authors show that posterior attention models are able to achieve better alignment and alignment accuracy than prior attention models. They also show that the BLEU score of posterior attention can be used to evaluate the performance of attention models on a variety of tasks, including translation and morphological inflection. "
8921,SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"artifacts CONJUNCTION degenerated transformations. degenerated transformations CONJUNCTION artifacts. smoothness term USED-FOR harmonic functions. harmonic functions USED-FOR consistent mappings. smoothness term PART-OF sample graph. HarmonicGAN USED-FOR bi - directional translations. similarity - consistency USED-FOR inherent selfconsistency property. histogram CONJUNCTION CNN. CNN CONJUNCTION histogram. features FEATURE-OF Distance metrics. CNN HYPONYM-OF Distance metrics. histogram HYPONYM-OF Distance metrics. CNN HYPONYM-OF features. histogram HYPONYM-OF features. HarmonicGAN COMPARE state of the art. state of the art COMPARE HarmonicGAN. CycleGAN COMPARE HarmonicGAN. HarmonicGAN COMPARE CycleGAN. interpretability EVALUATE-FOR HarmonicGAN. object transfiguration CONJUNCTION semantic labeling. semantic labeling CONJUNCTION object transfiguration. medical imaging CONJUNCTION object transfiguration. object transfiguration CONJUNCTION medical imaging. medical imaging HYPONYM-OF applications. semantic labeling HYPONYM-OF applications. object transfiguration HYPONYM-OF applications. tasks EVALUATE-FOR methods. method USED-FOR medical imaging task. Task are unpaired image - to - image translation, manifold view of the problem, and translation. Generic is it. OtherScientificTerm are pixel - to - pixel supervision, manual inputs, and mean - squared error. Metric is training - time cost. ","This paper proposes a method for bi-directional image-to-image translation. The proposed method is based on the idea that the smoothness term of the sample graph can be used as a regularizer to ensure that the mapping of two images to the same manifold is consistent. The authors show that the proposed method outperforms the state-of-the-art in terms of training time and interpretability. The method is evaluated on a variety of tasks, including object transfiguration, semantic labeling, and medical imaging.","This paper proposes a method for bi-directional image-to-image translation. The proposed method is based on the idea that the smoothness term of the sample graph can be used as a regularizer to ensure that the mapping of two images to the same manifold is consistent. The authors show that the proposed method outperforms the state-of-the-art in terms of training time and interpretability. The method is evaluated on a variety of tasks, including object transfiguration, semantic labeling, and medical imaging."
8930,SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"EVGP USED-FOR gradient components. stochastic algorithm ( h - detach ) USED-FOR LSTM optimization. stochastic algorithm ( h - detach ) USED-FOR problem. linear path ( cell state ) PART-OF LSTM computational graph. long term dependencies FEATURE-OF components. LSTM USED-FOR dependencies. seed CONJUNCTION learning rate. learning rate CONJUNCTION seed. convergence speed CONJUNCTION robustness. robustness CONJUNCTION convergence speed. robustness CONJUNCTION learning rate. learning rate CONJUNCTION robustness. robustness CONJUNCTION seed. seed CONJUNCTION robustness. benchmark datasets EVALUATE-FOR generalization. benchmark datasets EVALUATE-FOR LSTM gradient. convergence speed EVALUATE-FOR vanilla LSTM gradient based training. LSTM gradient USED-FOR generalization. Method are Recurrent neural networks, and LSTMs. Task is exploding and vanishing gradient problem ( EVGP ). OtherScientificTerm are LSTM weights, and gradients. Generic is path. ","This paper studies the exploding and vanishing gradient problem (EVGP) in recurrent neural networks. EVGP is an important problem for LSTM optimization. The authors propose a stochastic algorithm (h-divergences) to solve EVGP. The proposed algorithm is based on the notion of long-term dependencies between components of LSTMs. The main contribution of the paper is to show that EVGP can be decomposed into three components: (1) the learning rate, (2) the robustness, and (3) the convergence speed. ","This paper studies the exploding and vanishing gradient problem (EVGP) in recurrent neural networks. EVGP is an important problem for LSTM optimization. The authors propose a stochastic algorithm (h-divergences) to solve EVGP. The proposed algorithm is based on the notion of long-term dependencies between components of LSTMs. The main contribution of the paper is to show that EVGP can be decomposed into three components: (1) the learning rate, (2) the robustness, and (3) the convergence speed. "
8939,SP:9aaff3777321347d1194884af5690b0b5185eff9,posterior distribution FEATURE-OF binary weights. Bayesian deep learning perspective USED-FOR real binary weight networks. reinforcement learning scheme USED-FOR policy network. policy network USED-FOR posterior distribution. binary weights USED-FOR burn - after - reading style. binary weight instances USED-FOR recognition architecture. policy network USED-FOR binary weight instances. policy network USED-FOR recognition architecture. policy network USED-FOR neural network architecture. nested parameter structure FEATURE-OF policy network. nested parameterization USED-FOR joint posterior distribution of binary weights. ImageNet HYPONYM-OF visual recognition tasks. visual recognition tasks EVALUATE-FOR SnapQuant. ImageNet EVALUATE-FOR SnapQuant. Task is point estimation. Generic is method. ,This paper proposes a method for learning binary weight networks from scratch. The method is based on Bayesian deep learning and uses a reinforcement learning scheme to learn the posterior distribution of binary weights. The authors show that the proposed method can be used to learn binary weight instances for point estimation tasks. The paper also shows that the method is able to achieve better performance than baselines. ,This paper proposes a method for learning binary weight networks from scratch. The method is based on Bayesian deep learning and uses a reinforcement learning scheme to learn the posterior distribution of binary weights. The authors show that the proposed method can be used to learn binary weight instances for point estimation tasks. The paper also shows that the method is able to achieve better performance than baselines. 
8948,SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,Bayesian nonparametric framework USED-FOR federated learning with neural networks. inference approach USED-FOR global network. supervision CONJUNCTION data pooling. data pooling CONJUNCTION supervision. federated learning problems EVALUATE-FOR approach. image classification datasets USED-FOR federated learning problems. OtherScientificTerm is local neural network weights. Generic is framework. ,This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The proposed framework is based on the idea that the weights of the local neural network weights are shared across all clients. The authors show that the proposed framework can be applied to both supervised and unsupervised learning problems. They also show that their framework is able to achieve state-of-the-art performance on image classification tasks. ,This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The proposed framework is based on the idea that the weights of the local neural network weights are shared across all clients. The authors show that the proposed framework can be applied to both supervised and unsupervised learning problems. They also show that their framework is able to achieve state-of-the-art performance on image classification tasks. 
8957,SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"GANs CONJUNCTION intrinsic curiosity. intrinsic curiosity CONJUNCTION GANs. GANs CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION GANs. intrinsic curiosity CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION intrinsic curiosity. differentiable games USED-FOR learning methods. approach USED-FOR learning dynamics. Opponent shaping USED-FOR learning dynamics. Opponent shaping HYPONYM-OF approach. learning dynamics FEATURE-OF games. approach USED-FOR games. theoretical guarantees FEATURE-OF algorithms. LOLA CONJUNCTION stable variant. stable variant CONJUNCTION LOLA. method USED-FOR LOLA. Stable Opponent Shaping ( SOS ) HYPONYM-OF method. LookAhead HYPONYM-OF stable variant. LookAhead USED-FOR equilibria. strict saddles PART-OF differentiable games. Method are Opponent - Learning Awareness ( LOLA ), LOLA agents, and SOS. Generic is algorithm. OtherScientificTerm are cooperation, Iterated Prisoner ’s Dilemma, ‘ arrogant ’ behaviour, and learning of opponents. ","This paper studies the problem of learning dynamics in differentiable games. The authors propose a new algorithm, SOS, to learn the learning dynamics of opponents in the Iterated Prisoner’s Dilemma (IPD). SOS is a stable variant of LOLA, and the authors show that SOS is able to achieve equilibria in the strict saddles of the game.","This paper studies the problem of learning dynamics in differentiable games. The authors propose a new algorithm, SOS, to learn the learning dynamics of opponents in the Iterated Prisoner’s Dilemma (IPD). SOS is a stable variant of LOLA, and the authors show that SOS is able to achieve equilibria in the strict saddles of the game."
8966,SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"learning system USED-FOR rare events. feature space FEATURE-OF classifiers / regressors. shape feature HYPONYM-OF prior information. segmentation algorithms USED-FOR it. shape feature USED-FOR feature space. Variational Auto - Encoder(VAE ) USED-FOR segmentation result. loss function USED-FOR shape feature. ground truth masks USED-FOR VAE. VAE USED-FOR shapes. representation USED-FOR qualities of segmentation results. one - dimensional feature space FEATURE-OF representation. segmentation algorithms USED-FOR medical segmentation task. medical segmentation task EVALUATE-FOR alarm system. segmentation algorithms EVALUATE-FOR alarm system. OtherScientificTerm are low dimensional feature space, bad shapes, and loss value. Generic is system. ","This paper proposes a VAE-based method for segmentation based on shape feature learning. The proposed method is based on the VAE loss function, which is used to learn the shape feature of the feature space. The method is evaluated on a medical segmentation task and shows that the proposed method outperforms existing segmentation algorithms. ","This paper proposes a VAE-based method for segmentation based on shape feature learning. The proposed method is based on the VAE loss function, which is used to learn the shape feature of the feature space. The method is evaluated on a medical segmentation task and shows that the proposed method outperforms existing segmentation algorithms. "
8975,SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"denoising CONJUNCTION inpainting. inpainting CONJUNCTION denoising. inpainting CONJUNCTION reconstruction. reconstruction CONJUNCTION inpainting. Deep neural networks USED-FOR compressing images. Deep neural networks USED-FOR inverse problems. compressing images USED-FOR inverse problems. reconstruction HYPONYM-OF inverse problems. convolutional neural networks HYPONYM-OF Deep neural networks. denoising HYPONYM-OF inverse problems. few and noisy measurements USED-FOR reconstruction. inpainting HYPONYM-OF inverse problems. tools COMPARE imagegenerating deep neural networks. imagegenerating deep neural networks COMPARE tools. wavelets HYPONYM-OF tools. deep neural network USED-FOR natural images. deep decoder HYPONYM-OF untrained simple image model. deep decoder USED-FOR images. network weights COMPARE wavelet - based thresholding. wavelet - based thresholding COMPARE network weights. underparameterization USED-FOR deep decoder. deep decoder USED-FOR denoising. underparameterization USED-FOR overfitting. ReLU activation CONJUNCTION channelwise normalization. channelwise normalization CONJUNCTION ReLU activation. pixel - wise linear combination of channels CONJUNCTION ReLU activation. ReLU activation CONJUNCTION pixel - wise linear combination of channels. upsampling unit CONJUNCTION pixel - wise linear combination of channels. pixel - wise linear combination of channels CONJUNCTION upsampling unit. them USED-FOR signal representations. neural networks USED-FOR signal representations. it USED-FOR neural networks. neural networks USED-FOR them. theoretical analysis USED-FOR network. OtherScientificTerm are output dimension, weight parameters, convolutions, and output dimensionality. Material is large datasets. ",This paper studies the problem of denoising and inpainting in convolutional neural networks (CNNs). The authors propose a novel deep decoder that can be used to solve these inverse problems. The authors show that the proposed method outperforms wavelet-based thresholding and underparametrized deep decoders. ,This paper studies the problem of denoising and inpainting in convolutional neural networks (CNNs). The authors propose a novel deep decoder that can be used to solve these inverse problems. The authors show that the proposed method outperforms wavelet-based thresholding and underparametrized deep decoders. 
8984,SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"natural language ( NL ) USED-FOR Program synthesis. SAPS HYPONYM-OF end - to - end neural network. end - to - end neural network USED-FOR multi - sentence NL specifications. pretrained word embedding CONJUNCTION bi - directional multi - layer LSTM. bi - directional multi - layer LSTM CONJUNCTION pretrained word embedding. bi - directional multi - layer LSTM USED-FOR processing of word sequences. abstract syntax trees CONJUNCTION pretrained word embedding. pretrained word embedding CONJUNCTION abstract syntax trees. pretrained word embedding USED-FOR processing of word sequences. pretrained word embedding USED-FOR architecture. bi - directional multi - layer LSTM USED-FOR architecture. abstract syntax trees USED-FOR architecture. neural components USED-FOR architecture. signal propagation schemes CONJUNCTION soft attention mechanism. soft attention mechanism CONJUNCTION signal propagation schemes. doubly - recurrent LSTM CONJUNCTION signal propagation schemes. signal propagation schemes CONJUNCTION doubly - recurrent LSTM. signal propagation schemes USED-FOR decoder. soft attention mechanism USED-FOR decoder. doubly - recurrent LSTM USED-FOR decoder. SAPS COMPARE method. method COMPARE SAPS. NL analyzer CONJUNCTION source code generator. source code generator CONJUNCTION NL analyzer. methods COMPARE it. it COMPARE methods. fixed - dimensional latent representation USED-FOR NL analyzer. post - processing USED-FOR it. fixed - dimensional latent representation USED-FOR it. Task are software development, and end - user programming. ",This paper proposes a neural network architecture for program synthesis. The proposed architecture is based on a bi-directional multi-layer LSTM and a pretrained word embedding. The paper also proposes a soft attention mechanism for the decoder and a signal propagation scheme for the signal propagation schemes. Experiments show that the proposed architecture outperforms the baselines.,This paper proposes a neural network architecture for program synthesis. The proposed architecture is based on a bi-directional multi-layer LSTM and a pretrained word embedding. The paper also proposes a soft attention mechanism for the decoder and a signal propagation scheme for the signal propagation schemes. Experiments show that the proposed architecture outperforms the baselines.
8993,SP:d2ec231bb6153a303e5110e671dea14c2721e636,"deep neural networks USED-FOR tiny input perturbations. MNIST HYPONYM-OF computer vision. deep neural networks USED-FOR MNIST. L0 robustness EVALUATE-FOR undefended networks. adversarial robustness EVALUATE-FOR MNIST. class - conditional data distributions USED-FOR robust classification model. adversarial attacks USED-FOR model. robustness EVALUATE-FOR MNIST. MNIST EVALUATE-FOR approach. robustness EVALUATE-FOR approach. Method are neural network model, L∞ defense, input binarization, and decision - based attack. OtherScientificTerm are adversarial perturbations, L2 perturbations, Lp norms, and L0, L2 and L∞ perturbations. Material are unrecognizable images, and adversarial examples. Generic is attack. ","This paper studies the problem of adversarial robustness of deep neural networks against small input perturbations. The authors propose a decision-based adversarial defense method to improve the robustness against L2 and L∞ attacks. The proposed method is based on the idea of class-conditional data distributions, where the classifier is trained on a class-conditioned data distribution and the adversarial examples are generated from a classifier trained on the class distribution. The method is evaluated on MNIST and CIFAR-10 datasets.","This paper studies the problem of adversarial robustness of deep neural networks against small input perturbations. The authors propose a decision-based adversarial defense method to improve the robustness against L2 and L∞ attacks. The proposed method is based on the idea of class-conditional data distributions, where the classifier is trained on a class-conditioned data distribution and the adversarial examples are generated from a classifier trained on the class distribution. The method is evaluated on MNIST and CIFAR-10 datasets."
9002,SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"spectra of weight matrices PART-OF discriminator. spectra of weight matrices USED-FOR GANs. framework USED-FOR GANs. weight matrices PART-OF discriminator. slow singular value decays FEATURE-OF weight matrices. regularizers CONJUNCTION constraints. constraints CONJUNCTION regularizers. reparameterization approach USED-FOR GANs. reparameterization approach USED-FOR weight matrices. regularizers USED-FOR spectra of the weight matrices. constraints USED-FOR spectra of the weight matrices. spectrum control USED-FOR GANs. methods COMPARE method. method COMPARE methods. CIFAR-10, STL-10, and ImgaeNet datasets EVALUATE-FOR method. spectral normalization USED-FOR method. Method are Generative Adversarial Networks ( GANs ), and singular value decompositions. OtherScientificTerm is slow singular value decay. ","This paper proposes a new method to improve the performance of GANs. The authors propose to use spectral normalization to reduce the singular value decay of the weight matrices of the discriminator, which is an important problem in GAN training. The method is based on the idea of reparameterization of the spectra of the weights of a discriminator. The proposed method is evaluated on CIFAR-10, STL-10 and ImgaeNet datasets.","This paper proposes a new method to improve the performance of GANs. The authors propose to use spectral normalization to reduce the singular value decay of the weight matrices of the discriminator, which is an important problem in GAN training. The method is based on the idea of reparameterization of the spectra of the weights of a discriminator. The proposed method is evaluated on CIFAR-10, STL-10 and ImgaeNet datasets."
9011,SP:8115fd9b681198d62100c36794926fb57dc0a4f5,Acceleration USED-FOR reinforcement learning methods. Anderson acceleration technique USED-FOR value iteration. Anderson Accelerated Value Iteration ( A2VI ) HYPONYM-OF accelerated value iteration algorithm. method USED-FOR Deep Q - learning algorithm. approach USED-FOR approximation of the policy evaluation. approximate method USED-FOR policy evaluation. A2VI COMPARE policy iteration. policy iteration COMPARE A2VI. policy iteration HYPONYM-OF approximate method. toy problems CONJUNCTION Atari games. Atari games CONJUNCTION toy problems. Material is historical data. Generic is algorithm. ,"This paper proposes a new method for value iteration in reinforcement learning. The main idea is to use the Anderson accelerated value iteration (A2VI) technique to accelerate the value iteration of the policy evaluation. A2VI is based on the Anderson acceleration technique. The authors show that the proposed method outperforms existing value iteration methods on toy problems, Atari games, and Deep Q-learning tasks.","This paper proposes a new method for value iteration in reinforcement learning. The main idea is to use the Anderson accelerated value iteration (A2VI) technique to accelerate the value iteration of the policy evaluation. A2VI is based on the Anderson acceleration technique. The authors show that the proposed method outperforms existing value iteration methods on toy problems, Atari games, and Deep Q-learning tasks."
9020,SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,method USED-FOR catastrophic forgetting problem. SupportNet USED-FOR catastrophic forgetting problem. class incremental learning scenario FEATURE-OF catastrophic forgetting problem. deep learning CONJUNCTION support vector machine ( SVM ). support vector machine ( SVM ) CONJUNCTION deep learning. support vector machine ( SVM ) PART-OF SupportNet. deep learning PART-OF SupportNet. SVM PART-OF SupportNet. consolidation regularizers USED-FOR model. SupportNet COMPARE deep learning model. deep learning model COMPARE SupportNet. SupportNet COMPARE incremental learning methods. incremental learning methods COMPARE SupportNet. tasks EVALUATE-FOR SupportNet. tasks EVALUATE-FOR method. OtherScientificTerm is catastrophic forgetting. ,"This paper studies the catastrophic forgetting problem in the class incremental learning scenario. The authors propose a new method, called SupportNet, that combines deep learning and support vector machine (SVM). The authors show that the proposed method outperforms existing incremental learning methods on a variety of tasks.","This paper studies the catastrophic forgetting problem in the class incremental learning scenario. The authors propose a new method, called SupportNet, that combines deep learning and support vector machine (SVM). The authors show that the proposed method outperforms existing incremental learning methods on a variety of tasks."
9029,SP:d228d213f79716774043cea253305fecece659ec,"methods USED-FOR representations. methods USED-FOR unit selectivity. unit selectivity USED-FOR representations. neural networks ( NNs ) USED-FOR representations. measures PART-OF AlexNet. localist selectivity HYPONYM-OF measures. precision CONJUNCTION class - conditional mean activity selectivity CCMAS. class - conditional mean activity selectivity CCMAS CONJUNCTION precision. precision and CCMAS measures USED-FOR selectivity. fc6 CONJUNCTION conv5. conv5 CONJUNCTION fc6. units PART-OF conv5. units PART-OF fc6. RNNs COMPARE AlexNet. AlexNet COMPARE RNNs. AlexNet USED-FOR localist representations. RNNs USED-FOR localist representations. Generic is measure. Metric are top - class selectivity, and selectivity measures. Method are recurrent neural networks ( RNNs ), activation maximization ( AM ) images, fc8, and NNs. OtherScientificTerm are hidden layers, and selective units. ","This paper proposes a new measure of unit selectivity for RNNs, called AlexNet. The proposed measure is based on the notion of top-class selectivity, which is defined as the difference between the top- and bottom-class performance of the hidden layers of an RNN. The paper shows that AlexNet is able to achieve better performance than the state-of-the-art on fc8, fc6, conv5, and conv5. ","This paper proposes a new measure of unit selectivity for RNNs, called AlexNet. The proposed measure is based on the notion of top-class selectivity, which is defined as the difference between the top- and bottom-class performance of the hidden layers of an RNN. The paper shows that AlexNet is able to achieve better performance than the state-of-the-art on fc8, fc6, conv5, and conv5. "
9038,SP:b9deae0392e0160b400d76c549d382e235196f8c,"spectral methods CONJUNCTION posterior inference. posterior inference CONJUNCTION spectral methods. probabilistic graphical models USED-FOR posterior inference. graphs USED-FOR Community detection. spectral methods USED-FOR Community detection. posterior inference USED-FOR Community detection. signal - to - noise ratio FEATURE-OF statistical and computational detection thresholds. stochastic block model HYPONYM-OF random graph families. graphs USED-FOR node - wise classification problem. node - wise classification problem USED-FOR community detection. learning perspective USED-FOR it. Graph Neural Networks ( GNNs ) USED-FOR community detection problems. supervised learning setting USED-FOR community detection problems. belief propagation algorithm USED-FOR binary and multiclass stochastic block models. they COMPARE belief propagation algorithm. belief propagation algorithm COMPARE they. line graph of edge adjacencies FEATURE-OF non - backtracking operator. non - backtracking operator USED-FOR GNNs. real - world datasets EVALUATE-FOR GNNs. linear ) GNNs USED-FOR community detection problems. linear ) GNNs USED-FOR optimization landscape. Generic is approaches. Method is generative models. OtherScientificTerm are computational threshold, local minimum, and global minimum / minima. ","This paper studies the problem of community detection in graph neural networks (GNNs). In particular, the authors focus on the stochastic block model (SBM), which is a probabilistic graphical model that can be used for community detection. The authors propose a belief propagation algorithm for binary and multiclass SBM, which is based on the line graph of edge adjacencies. They show that the proposed method is able to achieve state-of-the-art performance on a number of real-world datasets. ","This paper studies the problem of community detection in graph neural networks (GNNs). In particular, the authors focus on the stochastic block model (SBM), which is a probabilistic graphical model that can be used for community detection. The authors propose a belief propagation algorithm for binary and multiclass SBM, which is based on the line graph of edge adjacencies. They show that the proposed method is able to achieve state-of-the-art performance on a number of real-world datasets. "
9047,SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"sparse weights PART-OF linear combination. provable algorithms USED-FOR dictionary learning. provable dictionary learning methods USED-FOR coefficient recovery. linear and non - linear operations PART-OF it. algorithm COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE algorithm. Task are dictionary learning problem, optimization, and recovery of the dictionary. OtherScientificTerm are dictionary, coefficients, and geometric rate. Method are linear model, NOODL, and neural architectures. ",This paper studies the problem of dictionary learning for linear combination of linear and non-linear operations. The authors propose a novel algorithm for solving the problem. The algorithm is based on the idea that the coefficients of a linear combination can be decomposed into two parts. The first part is the linear part and the second part is a nonlinear part. The main contribution of the paper is to show that the proposed algorithm can be used to solve the dictionary learning problem. ,This paper studies the problem of dictionary learning for linear combination of linear and non-linear operations. The authors propose a novel algorithm for solving the problem. The algorithm is based on the idea that the coefficients of a linear combination can be decomposed into two parts. The first part is the linear part and the second part is a nonlinear part. The main contribution of the paper is to show that the proposed algorithm can be used to solve the dictionary learning problem. 
9056,SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"differentiable model CONJUNCTION similarity function. similarity function CONJUNCTION differentiable model. loss function USED-FOR binary hash codes. differentiable model USED-FOR binary hash codes. loss function COMPARE prior methods. prior methods COMPARE loss function. log likelihood loss USED-FOR prior methods. log likelihood loss USED-FOR loss function. multi - indexing USED-FOR hashes. techniques USED-FOR similarity search tasks. ImageNet CONJUNCTION SIFT 1 M. SIFT 1 M CONJUNCTION ImageNet. information retrieval tasks EVALUATE-FOR SIFT 1 M. ImageNet USED-FOR information retrieval tasks. OtherScientificTerm are Hamming distance target, loss terms, and minibatch. Method is training scheme. Metric are MAP, and query cost. ",This paper proposes a new loss function for binary hash codes. The proposed loss function is based on the log likelihood loss. The authors show that the proposed loss can be used to reduce the query cost and improve the performance of the model. The paper also provides a theoretical analysis of the proposed method. ,This paper proposes a new loss function for binary hash codes. The proposed loss function is based on the log likelihood loss. The authors show that the proposed loss can be used to reduce the query cost and improve the performance of the model. The paper also provides a theoretical analysis of the proposed method. 
9065,SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,Neural architecture search ( NAS ) USED-FOR task - specific neural network topology. networks USED-FOR search. Graph HyperNetwork ( GHN ) USED-FOR search cost. graph neural network USED-FOR inference. regular hypernetworks CONJUNCTION premature early stopping. premature early stopping CONJUNCTION regular hypernetworks. GHNs USED-FOR architecture. GHNs USED-FOR network. GHNs COMPARE regular hypernetworks. regular hypernetworks COMPARE GHNs. GHNs COMPARE premature early stopping. premature early stopping COMPARE GHNs. validation accuracy EVALUATE-FOR networks. validation accuracy EVALUATE-FOR surrogate search signal. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. they COMPARE random search methods. random search methods COMPARE they. GHNs COMPARE random search methods. random search methods COMPARE GHNs. ImageNet EVALUATE-FOR random search methods. CIFAR-10 EVALUATE-FOR random search methods. networks COMPARE manual designs. manual designs COMPARE networks. GHNs USED-FOR anytime prediction setting. speed - accuracy tradeoff EVALUATE-FOR networks. Method is manual architecture designs. Generic is it. Task is NAS. ,"This paper proposes a new method for neural architecture search (NAS) called Graph HyperNetwork (GHN). The main idea of GHN is to use a graph neural network (GNN) as a surrogate search signal to reduce the search cost and speed-accuracy trade-off. The authors show that GHNs are able to achieve better performance than random search methods on CIFAR-10, ImageNet, and ImageNet-10. ","This paper proposes a new method for neural architecture search (NAS) called Graph HyperNetwork (GHN). The main idea of GHN is to use a graph neural network (GNN) as a surrogate search signal to reduce the search cost and speed-accuracy trade-off. The authors show that GHNs are able to achieve better performance than random search methods on CIFAR-10, ImageNet, and ImageNet-10. "
9074,SP:65ccf43cd4e033d22239069057f5200d49f33724,Imitation learning USED-FOR optimal policy. expert demonstrations USED-FOR Imitation learning. expert demonstrations USED-FOR optimal policy. expert demonstrations USED-FOR deep learning. method USED-FOR generative adversarial imitation learning. multiclass classification USED-FOR discriminator functions. method USED-FOR multiclass classification. method USED-FOR discriminator functions. method COMPARE generative adversarial imitation learning baseline. generative adversarial imitation learning baseline COMPARE method. continuous control tasks EVALUATE-FOR method. method USED-FOR policies. continuous control tasks EVALUATE-FOR generative adversarial imitation learning baseline. OtherScientificTerm is non - expert demonstrations. ,"This paper proposes a generative adversarial imitation learning (GANL) method for imitation learning. The proposed method is based on the idea that the discriminator function of a GANL model can be decomposed into two parts. The first part is a discriminator that is trained on a set of expert demonstrations, and the second part is an ensemble of discriminator functions that are trained on the same set of demonstrations. Experiments show that the proposed method outperforms the baselines on several continuous control tasks.","This paper proposes a generative adversarial imitation learning (GANL) method for imitation learning. The proposed method is based on the idea that the discriminator function of a GANL model can be decomposed into two parts. The first part is a discriminator that is trained on a set of expert demonstrations, and the second part is an ensemble of discriminator functions that are trained on the same set of demonstrations. Experiments show that the proposed method outperforms the baselines on several continuous control tasks."
9083,SP:e8427949a98effbd37ce7604fa11f240e2342196,"natural science HYPONYM-OF applications. neural networks USED-FOR task. Invertible Neural Networks ( INNs ) HYPONYM-OF neural networks. neural networks USED-FOR ambiguous inverse problem. INNs USED-FOR forward process. neural networks COMPARE INNs. INNs COMPARE neural networks. latent output variables USED-FOR INNs. model USED-FOR inverse process. invertibility USED-FOR model. medicine CONJUNCTION astrophysics. astrophysics CONJUNCTION medicine. INNs USED-FOR unrecoverable parameters. INNs USED-FOR multi - modalities. INNs USED-FOR parameter correlations. parameter space FEATURE-OF multi - modalities. OtherScientificTerm are hidden system parameters, posterior parameter distribution, observed measurement, and distribution of the latent variables. Task is inverse problem. Generic is ambiguity. Method is INN. Material is artificial data. ","This paper studies the problem of inverse problem, which is an important problem in natural science. The authors propose a novel approach to solve this problem, called Invertible Neural Networks (INNs), which can be seen as an extension of neural networks (NNs). The main idea of INNs is to learn a model that is invertible to unseen parameters. The model is then used to solve the inverse problem in a forward process. In particular, the authors show that INNs can be used to learn an inverse process that is invariant to unseen parameter correlations. ","This paper studies the problem of inverse problem, which is an important problem in natural science. The authors propose a novel approach to solve this problem, called Invertible Neural Networks (INNs), which can be seen as an extension of neural networks (NNs). The main idea of INNs is to learn a model that is invertible to unseen parameters. The model is then used to solve the inverse problem in a forward process. In particular, the authors show that INNs can be used to learn an inverse process that is invariant to unseen parameter correlations. "
9092,SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"ensemble of NNs COMPARE Bayesian NNs. Bayesian NNs COMPARE ensemble of NNs. scoring rule USED-FOR ensemble of NNs. finite mixture model USED-FOR ensemble method. uniform mixing weights USED-FOR finite mixture model. adaptive, input - dependent distribution USED-FOR fixed mixing weights. NN USED-FOR adaptive, input - dependent distribution. model COMPARE approaches. approaches COMPARE model. uncertainty estimates EVALUATE-FOR model. Method are deep neural networks ( NNs ), mixture model approach, mixture density networks, and compound density networks. OtherScientificTerm are prediction uncertainty, and mixture components. Material is adversarial examples. ","This paper proposes a new ensemble of NNs based on a finite mixture model. The authors propose a scoring rule for the ensemble method, which is based on the notion of scoring rule. The scoring rule is defined as the sum of the score of the ensemble members of a mixture model and the scoring rule of a uniform mixing weights. The proposed scoring rule can be applied to any mixture model, and the authors show that the proposed method outperforms other ensemble methods. ","This paper proposes a new ensemble of NNs based on a finite mixture model. The authors propose a scoring rule for the ensemble method, which is based on the notion of scoring rule. The scoring rule is defined as the sum of the score of the ensemble members of a mixture model and the scoring rule of a uniform mixing weights. The proposed scoring rule can be applied to any mixture model, and the authors show that the proposed method outperforms other ensemble methods. "
9101,SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"energy consumption CONJUNCTION communication bandwidth. communication bandwidth CONJUNCTION energy consumption. communication bandwidth CONJUNCTION storage requirements. storage requirements CONJUNCTION communication bandwidth. deep neural networks HYPONYM-OF model class. model size reduction PART-OF deep learning. pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. techniques USED-FOR Shannon - style coding schemes. Shannon - style coding schemes USED-FOR empirical weight distribution. quantization HYPONYM-OF techniques. pruning HYPONYM-OF techniques. full variational distribution USED-FOR coding schemes. compression rates EVALUATE-FOR coding schemes. KullbackLeibler divergence FEATURE-OF sampled variational distribution. random sample USED-FOR network weights. constraint USED-FOR compression rate. constraint FEATURE-OF Kullback - Leibler divergence. encoding scheme COMPARE information - theoretical lower bound. information - theoretical lower bound COMPARE encoding scheme. variational family USED-FOR information - theoretical lower bound. variational family USED-FOR encoding scheme. it COMPARE approaches. approaches COMPARE it. method USED-FOR neural network compression. VGG-16 / CIFAR-10 EVALUATE-FOR approach. fixed memory budget EVALUATE-FOR approach. compression rates EVALUATE-FOR approach. compression rates EVALUATE-FOR it. OtherScientificTerm are memory footprint, deterministic weights, weight determinism, encoding distribution, and expected loss. Method is bits - back argument. ","This paper proposes a new compression method for neural network compression based on the Kullback-Leibler divergence. The proposed method is based on a variational family of variational distributions, which can be used to compute the compression rate. The authors show that the proposed method can achieve better compression rates than existing compression methods. ","This paper proposes a new compression method for neural network compression based on the Kullback-Leibler divergence. The proposed method is based on a variational family of variational distributions, which can be used to compute the compression rate. The authors show that the proposed method can achieve better compression rates than existing compression methods. "
9110,SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,neural network architectures USED-FOR Neural architecture search ( NAS ). architectures USED-FOR large - scale tasks. ImageNet HYPONYM-OF large - scale tasks. GPU hours EVALUATE-FOR Differentiable NAS. continuous representation of network architecture USED-FOR Differentiable NAS. continuous representation of network architecture USED-FOR GPU hours. proxy tasks USED-FOR they. architectures USED-FOR proxy tasks. large - scale target tasks CONJUNCTION hardware platforms. hardware platforms CONJUNCTION large - scale target tasks. architectures USED-FOR large - scale target tasks. ProxylessNAS USED-FOR architectures. ProxylessNAS USED-FOR large - scale target tasks. GPU hours CONJUNCTION GPU memory. GPU memory CONJUNCTION GPU hours. computational cost EVALUATE-FOR regular training. GPU hours HYPONYM-OF computational cost. GPU memory HYPONYM-OF computational cost. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR model. test error EVALUATE-FOR model. ImageNet EVALUATE-FOR model. model COMPARE MobileNetV2. MobileNetV2 COMPARE model. ImageNet EVALUATE-FOR MobileNetV2. top-1 accuracy EVALUATE-FOR MobileNetV2. GPU latency EVALUATE-FOR model. top-1 accuracy EVALUATE-FOR model. ProxylessNAS USED-FOR neural architectures. Neural architecture search ( NAS ) USED-FOR neural network architecture design. neural network architecture design USED-FOR deep learning tasks. Neural architecture search ( NAS ) USED-FOR deep learning tasks. image recognition CONJUNCTION language modeling. language modeling CONJUNCTION image recognition. image recognition HYPONYM-OF deep learning tasks. language modeling HYPONYM-OF deep learning tasks. models USED-FOR task. NAS USED-FOR large - scale task. ImageNet HYPONYM-OF large - scale task. building blocks USED-FOR proxy tasks. top - performing blocks USED-FOR large - scale target task. paradigm USED-FOR NAS algorithms. blocks USED-FOR proxy tasks. latency HYPONYM-OF hardware metrics. methods USED-FOR transferability. Proxy,"This paper proposes a new method for Neural Architecture Search (NAS) for large-scale target tasks. The proposed method, called ProxylessNAS, uses a continuous representation of the network architecture as a proxy task, which can be used to compute the GPU hours and compute the memory. The method is evaluated on CIFAR-10, MobileNetV2, ImageNet, and language modeling tasks. ","This paper proposes a new method for Neural Architecture Search (NAS) for large-scale target tasks. The proposed method, called ProxylessNAS, uses a continuous representation of the network architecture as a proxy task, which can be used to compute the GPU hours and compute the memory. The method is evaluated on CIFAR-10, MobileNetV2, ImageNet, and language modeling tasks. "
9119,SP:e5b70d43d301d1980fae02623ea711976b429c14,"Lagrangian dual FEATURE-OF problem. additive linear penalties USED-FOR Lagrangian dual. non - convex settings FEATURE-OF problem. training procedure USED-FOR non - convex, large - data settings. second - order ones FEATURE-OF linear penalties. secondorder penalties USED-FOR penalized objective. penalty coefficient USED-FOR penalized objective. method USED-FOR gradients. second - order penalties FEATURE-OF gradients. algorithm USED-FOR classifier. Metric is fairness. OtherScientificTerm are linear constraints, constrained objective, Lagrangian, deterministic saddle - point equilibrium, instability, and stochastic mini - batch settings. Method is two - player min - max games. ",This paper studies the problem of minimizing the Lagrangian dual of the two-player min-max game in non-convex and large-data settings. The main contribution of the paper is a new method for penalizing the objective of the classifier. The proposed method is based on the second-order penalty coefficient of the penalized objective. The authors show that the second order penalty coefficient can be used as a regularizer to improve the fairness of the proposed method. ,This paper studies the problem of minimizing the Lagrangian dual of the two-player min-max game in non-convex and large-data settings. The main contribution of the paper is a new method for penalizing the objective of the classifier. The proposed method is based on the second-order penalty coefficient of the penalized objective. The authors show that the second order penalty coefficient can be used as a regularizer to improve the fairness of the proposed method. 
9128,SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"Sampling discrete latent variables USED-FOR highvariance gradient estimators. continuous - relaxation methods USED-FOR latter. control - variate schemes USED-FOR former. branch paths PART-OF model. control - variate schemes CONJUNCTION continuous - relaxation methods. continuous - relaxation methods CONJUNCTION control - variate schemes. control - variate schemes USED-FOR state - of - the - art methods. state - of - the - art methods USED-FOR discrete latent - variable models. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. models CONJUNCTION inference networks. inference networks CONJUNCTION models. importance weighted autoencoder COMPARE RWS. RWS COMPARE importance weighted autoencoder. RWS USED-FOR inference networks. RWS USED-FOR models. RWS USED-FOR deep generative models. Method are Discrete latent - variable models, and continuous latentvariable models. OtherScientificTerm is pathwise derivative. ","This paper studies the problem of learning discrete latent variable models with high-variance gradient estimators. In particular, the authors propose a new method called pathwise derivative sampling (RWS), which is a variant of the importance weighted autoencoder. The authors show that RWS can be used to learn discrete latent variables in a way that is similar to the state-of-the-art methods. They also show that the RWS method can be applied to deep generative models and inference networks.","This paper studies the problem of learning discrete latent variable models with high-variance gradient estimators. In particular, the authors propose a new method called pathwise derivative sampling (RWS), which is a variant of the importance weighted autoencoder. The authors show that RWS can be used to learn discrete latent variables in a way that is similar to the state-of-the-art methods. They also show that the RWS method can be applied to deep generative models and inference networks."
9137,SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"human knowledge CONJUNCTION non - differentiable pipelines. non - differentiable pipelines CONJUNCTION human knowledge. non - differentiable pipelines USED-FOR scalar reward function. human knowledge USED-FOR scalar reward function. scalar reward function USED-FOR tasks. truncated randomized search USED-FOR structured prediction energy networks ( SPENs ). truncated randomized search USED-FOR reward function. structured prediction energy networks ( SPENs ) USED-FOR test - time inference. gradient - based search USED-FOR structured prediction energy networks ( SPENs ). gradient - based search USED-FOR test - time inference. truncated randomized search USED-FOR unknown local improvements. supervision USED-FOR SPENs. truncated randomized search USED-FOR supervision. truncated randomized search USED-FOR reward function. Task are structured output prediction tasks, and structured prediction. OtherScientificTerm are output space, and score landscape. Material is labeled training data. ","This paper studies the use of truncated randomized search for structured prediction energy networks (SPENs) for test-time inference. In particular, the authors propose to use a truncated version of gradient-based search for the reward function of SPENs. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in terms of test time inference. ","This paper studies the use of truncated randomized search for structured prediction energy networks (SPENs) for test-time inference. In particular, the authors propose to use a truncated version of gradient-based search for the reward function of SPENs. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in terms of test time inference. "
9146,SP:638c1bc09992029b78bd83f0127594dcccb96c06,"It USED-FOR transferring policies. simulation environment FEATURE-OF transferring policies. these USED-FOR robust policies. active learning based framework USED-FOR model parameters. EffAcTS HYPONYM-OF active learning based framework. framework USED-FOR method. sample efficiency EVALUATE-FOR approach. EPOpt HYPONYM-OF method. continuous control tasks EVALUATE-FOR approach. Multi - Task Learning perspective USED-FOR Robust Policy Search. framework COMPARE Multi - Task Learning. Multi - Task Learning COMPARE framework. Task is learning policies. OtherScientificTerm are environment model parameters, and policies. Generic is approaches. ","This paper proposes a method for learning robust policies in a multi-task learning setting. The proposed method is based on an active learning framework, where the goal is to learn a set of robust policies that can be transferred to new tasks. The method is evaluated on a number of continuous control tasks and shows promising results.","This paper proposes a method for learning robust policies in a multi-task learning setting. The proposed method is based on an active learning framework, where the goal is to learn a set of robust policies that can be transferred to new tasks. The method is evaluated on a number of continuous control tasks and shows promising results."
9155,SP:491c239713a6489f0b1790ca26db54a1813c67ae,"policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. value function USED-FOR policy evaluation. value function USED-FOR control. fixed basis CONJUNCTION fixed representation. fixed representation CONJUNCTION fixed basis. algorithms USED-FOR linear function approximation. fixed basis USED-FOR linear function approximation. fixed representation USED-FOR linear function approximation. temporal difference learning CONJUNCTION Q - learning. Q - learning CONJUNCTION temporal difference learning. extensions USED-FOR nonlinear function approximation. Q - learning HYPONYM-OF methods. temporal difference learning HYPONYM-OF methods. nonlinear gradient temporal difference learning HYPONYM-OF nonlinear function approximation. two - timescale network ( TTN ) architecture USED-FOR linear methods. algorithms USED-FOR nonlinear value estimates. algorithms USED-FOR linear setting. data - efficient least - squares methods CONJUNCTION eligibility traces. eligibility traces CONJUNCTION data - efficient least - squares methods. linear policy evaluation algorithms USED-FOR nonlinear value estimates. eligibility traces CONJUNCTION linear policy evaluation algorithms. linear policy evaluation algorithms CONJUNCTION eligibility traces. algorithms USED-FOR approach. linear policy evaluation algorithms HYPONYM-OF algorithms. data - efficient least - squares methods HYPONYM-OF algorithms. eligibility traces HYPONYM-OF algorithms. dependent features FEATURE-OF linear component. nonlinear value function approximation algorithms USED-FOR policy evaluation and control. TTNs COMPARE nonlinear value function approximation algorithms. nonlinear value function approximation algorithms COMPARE TTNs. TTNs USED-FOR policy evaluation and control. Method are reinforcement learning agents, and nonlinear representation. ",This paper proposes a two-timescale network (TTN) architecture for non-linear value function approximation for reinforcement learning. The proposed method is based on the two-time-scales network (TNN) framework. The authors show that the proposed method outperforms existing algorithms for linear function approximation in the setting of policy evaluation and control. The main contribution of the paper is the introduction of a novel extension of the TTN framework for nonlinear function approximation. ,This paper proposes a two-timescale network (TTN) architecture for non-linear value function approximation for reinforcement learning. The proposed method is based on the two-time-scales network (TNN) framework. The authors show that the proposed method outperforms existing algorithms for linear function approximation in the setting of policy evaluation and control. The main contribution of the paper is the introduction of a novel extension of the TTN framework for nonlinear function approximation. 
9164,SP:327d606cf3813b00a009a7785e08ef9e11f89493,"intrinsic semantic regularities PART-OF man - made environments. multi - target sub - policy CONJUNCTION Bayesian model. Bayesian model CONJUNCTION multi - target sub - policy. visual inputs USED-FOR multi - target sub - policy. semantic structures USED-FOR Bayesian model. Bayesian model PART-OF LEArning and Planning with Semantics ( LEAPS ). multi - target sub - policy PART-OF LEArning and Planning with Semantics ( LEAPS ). House3D HYPONYM-OF 3D environment. real - world objects FEATURE-OF human - designed indoor scenes. human - designed indoor scenes PART-OF 3D environment. House3D USED-FOR visual navigation tasks. LEAPS COMPARE baselines. baselines COMPARE LEAPS. semantic content USED-FOR baselines. Method are deep reinforcement learning agents, semantic model, and sub - policy. Task is AI. ","This paper proposes a method for learning a multi-target sub-policy for visual navigation in a 3D environment. The proposed method is based on a Bayesian model and a semantic model. The model is trained to predict the semantic structure of the environment, and the semantic model is used to train the sub-problems. Experiments show that the proposed method outperforms baselines on a number of visual navigation tasks.","This paper proposes a method for learning a multi-target sub-policy for visual navigation in a 3D environment. The proposed method is based on a Bayesian model and a semantic model. The model is trained to predict the semantic structure of the environment, and the semantic model is used to train the sub-problems. Experiments show that the proposed method outperforms baselines on a number of visual navigation tasks."
9173,SP:d7c26f43bc68d160095b1f50447528843d79edbd,"multi - task perception - related basic knowledge CONJUNCTION driving knowledge. driving knowledge CONJUNCTION multi - task perception - related basic knowledge. perception module CONJUNCTION driving module. driving module CONJUNCTION perception module. perception module PART-OF driving model. driving module PART-OF driving model. driving knowledge USED-FOR it. multi - task perception - related basic knowledge USED-FOR it. segmentation map CONJUNCTION depth map. depth map CONJUNCTION segmentation map. control commands USED-FOR difficult driving task. depth map USED-FOR easier drivingrelated perception problems. depth map CONJUNCTION pixel level understanding of images. pixel level understanding of images CONJUNCTION depth map. generalization CONJUNCTION accident explanation ability. accident explanation ability CONJUNCTION generalization. multitask perception knowledge USED-FOR accident explanation ability. multitask perception knowledge USED-FOR generalization. method COMPARE benchmark method. benchmark method COMPARE method. average sucess rate EVALUATE-FOR navigation tasks. average sucess rate EVALUATE-FOR benchmark method. trained weather CONJUNCTION untrained weathers. untrained weathers CONJUNCTION trained weather. method USED-FOR navigation tasks. average sucess rate EVALUATE-FOR method. Method are deep learning driving models, and driving models. OtherScientificTerm is unobserved driving environment. Material is diversity of training driving dataset. ",This paper proposes a new method for training deep learning driving models. The method is based on multi-task perception-related basic knowledge and driving knowledge. The authors show that the proposed method is able to generalize well to unseen driving environments. They also show that their method can be used to improve the generalization ability of the driving model. ,This paper proposes a new method for training deep learning driving models. The method is based on multi-task perception-related basic knowledge and driving knowledge. The authors show that the proposed method is able to generalize well to unseen driving environments. They also show that their method can be used to improve the generalization ability of the driving model. 
9182,SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,adversarial robustness CONJUNCTION generalization. generalization CONJUNCTION adversarial robustness. accuracy EVALUATE-FOR model. adversarial perturbations FEATURE-OF robustness. robustness EVALUATE-FOR model. robust classifiers COMPARE classifiers. classifiers COMPARE robust classifiers. robust classifiers USED-FOR feature representations. feature representations COMPARE classifiers. classifiers COMPARE feature representations. salient data characteristics CONJUNCTION human perception. human perception CONJUNCTION salient data characteristics. robust models USED-FOR features. ,This paper studies the robustness of classifiers against adversarial perturbations. The authors show that robust classifiers are more robust to adversarial attacks than classifiers that are not robust to them. They also show that classifiers trained with robustness outperform robust models trained without robustness. ,This paper studies the robustness of classifiers against adversarial perturbations. The authors show that robust classifiers are more robust to adversarial attacks than classifiers that are not robust to them. They also show that classifiers trained with robustness outperform robust models trained without robustness. 
9191,SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"backpropagation HYPONYM-OF reverse - mode automatic differentiation. reverse - mode automatic differentiation USED-FOR Deep neural networks. method USED-FOR gradient - based training of neural networks. Equilibrium Propagation USED-FOR gradient - based training of neural networks. local learning rules USED-FOR gradient - based training of neural networks. local learning rules USED-FOR method. iterative optimization of neural activations USED-FOR inference. iterative inference procedure USED-FOR Equilibrium propagation. feedforward network USED-FOR iterative inference procedure. feedforward network USED-FOR Initialized Equilibrium Propagation. local learning rule USED-FOR feed - forward network. initializing network USED-FOR inference. initializing network USED-FOR feedforward network. network COMPARE Equilibrium propagation. Equilibrium propagation COMPARE network. backpropagation USED-FOR deep networks. Method is Biological networks. OtherScientificTerm are gradients, neurons, and error gradient. ","This paper proposes a new method for gradient-based training of neural networks. The proposed method, Initialized Equilibrium Propagation (IEP), is based on the idea of local learning rules. The key idea is to learn a local learning rule for the initializing network and the feedforward network. The authors show that the proposed method outperforms backpropagation and Equilibrium propagation in terms of accuracy and convergence. ","This paper proposes a new method for gradient-based training of neural networks. The proposed method, Initialized Equilibrium Propagation (IEP), is based on the idea of local learning rules. The key idea is to learn a local learning rule for the initializing network and the feedforward network. The authors show that the proposed method outperforms backpropagation and Equilibrium propagation in terms of accuracy and convergence. "
9200,SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,gradient - free operations CONJUNCTION signSGD. signSGD CONJUNCTION gradient - free operations. gradient - free operations PART-OF ZO - signSGD. signSGD PART-OF ZO - signSGD. latter COMPARE SGD - type algorithms. SGD - type algorithms COMPARE latter. convergence speed EVALUATE-FOR SGD - type algorithms. convergence speed EVALUATE-FOR latter. sign information of gradient estimates USED-FOR latter. ZO - signSGD COMPARE signSGD. signSGD COMPARE ZO - signSGD. gradient estimators USED-FOR ZO - signSGD. gradient estimators USED-FOR convergence. ZO - signSGD CONJUNCTION black - box adversarial attacks. black - box adversarial attacks CONJUNCTION ZO - signSGD. ZO - signSGD USED-FOR robust deep learning. black - box adversarial attacks USED-FOR robust deep learning. ZO - signSGD USED-FOR generation of adversarial examples. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. image classification datasets EVALUATE-FOR ZO - signSGD. CIFAR-10 HYPONYM-OF image classification datasets. black - box neural networks USED-FOR generation of adversarial examples. Metric is convergence rate. OtherScientificTerm is optimization variables. ,"This paper studies the convergence of ZO-signSGD and signSGD for black-box adversarial attacks. In particular, the authors show that the convergence rate of the proposed method is faster than that of SGD-type algorithms. The main contribution of the paper is the use of sign information of gradient estimates to improve the convergence speed of the method. The authors also provide a theoretical analysis of the convergence properties of the algorithm. ","This paper studies the convergence of ZO-signSGD and signSGD for black-box adversarial attacks. In particular, the authors show that the convergence rate of the proposed method is faster than that of SGD-type algorithms. The main contribution of the paper is the use of sign information of gradient estimates to improve the convergence speed of the method. The authors also provide a theoretical analysis of the convergence properties of the algorithm. "
9209,SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"Deep learning USED-FOR artificial intelligence applications. energy - limited edge device USED-FOR complex neural network model. optimization method USED-FOR convolutional neural networks. multiply - accumulate ( MAC ) operations PART-OF convolutional filter. checkpoint USED-FOR MAC process. fine - tuning process USED-FOR accuracy drop. CIFAR-10 example model CONJUNCTION Network in Network. Network in Network CONJUNCTION CIFAR-10 example model. MAC operations EVALUATE-FOR method. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR Network in Network. accuracy drop EVALUATE-FOR method. method COMPARE method. method COMPARE method. CIFAR-100 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. Method is convolutional operations. OtherScientificTerm are activation or pooling layers, filter, and checkpoints. ","This paper proposes a new optimization method for energy-limited edge devices. The proposed method is based on the multiply-accumulate (MAC) operations, which are commonly used in deep learning. The authors show that the proposed method can be applied to any convolutional filter, and that it can be combined with a fine-tuning process to improve the performance of the network. The method is evaluated on CIFAR-10 and Cifar-100 datasets.","This paper proposes a new optimization method for energy-limited edge devices. The proposed method is based on the multiply-accumulate (MAC) operations, which are commonly used in deep learning. The authors show that the proposed method can be applied to any convolutional filter, and that it can be combined with a fine-tuning process to improve the performance of the network. The method is evaluated on CIFAR-10 and Cifar-100 datasets."
9218,SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"adversarial examples USED-FOR neural network models. unique data properties USED-FOR learning principles. temporal dependency USED-FOR adversarial examples. temporal dependency USED-FOR discriminate power. temporal dependency FEATURE-OF audio data. automatic speech recognition ( ASR ) tasks CONJUNCTION audio adversarial attacks. audio adversarial attacks CONJUNCTION automatic speech recognition ( ASR ) tasks. temporal dependency USED-FOR discriminative power. image adversarial defense USED-FOR input transformation. robustness EVALUATE-FOR ASR systems. domain - specific data properties USED-FOR adversarial examples. OtherScientificTerm are adversarial inputs, audio adversarial examples, and adaptive attacks. ","This paper studies the problem of adversarial robustness in the context of audio adversarial attacks. The authors propose a new adversarial defense framework that leverages the temporal dependency between the input and the target domain. The proposed framework is based on the observation that adversarial examples are more likely to be generated from domain-specific data than from the source domain, and the authors show that this is the case for both audio and image-based attacks. They also show that the proposed framework can be used to improve the robustness of audio-based ASR systems.","This paper studies the problem of adversarial robustness in the context of audio adversarial attacks. The authors propose a new adversarial defense framework that leverages the temporal dependency between the input and the target domain. The proposed framework is based on the observation that adversarial examples are more likely to be generated from domain-specific data than from the source domain, and the authors show that this is the case for both audio and image-based attacks. They also show that the proposed framework can be used to improve the robustness of audio-based ASR systems."
9227,SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"They USED-FOR representations. images USED-FOR approaches. core inductive biases USED-FOR approaches. generator USED-FOR GAN. composition USED-FOR images. real - world images USED-FOR generative model. multi - object image datasets EVALUATE-FOR approach. generative model USED-FOR images. reference distribution FEATURE-OF images. Method are Deep generative models, and object representations. OtherScientificTerm is representational level. ","This paper proposes a generative model for multi-object image generation. The key idea is to use a generator to generate a set of images from a reference distribution, and then use the generated images to train a GAN. The generator is trained on the reference distribution and the generator is then used to generate the final image. The proposed method is evaluated on a number of image datasets and shows that the proposed method outperforms the baselines. ","This paper proposes a generative model for multi-object image generation. The key idea is to use a generator to generate a set of images from a reference distribution, and then use the generated images to train a GAN. The generator is trained on the reference distribution and the generator is then used to generate the final image. The proposed method is evaluated on a number of image datasets and shows that the proposed method outperforms the baselines. "
9236,SP:fb59990b8da0e95d8202383478a456667de60449,"Learning disentangled representations USED-FOR computer vision tasks. visual data USED-FOR Learning disentangled representations. referencebased disentangling HYPONYM-OF learning setting. deep generative model USED-FOR weak supervisory signal. reference - based variational autoencoders HYPONYM-OF deep generative model. reference set USED-FOR weak supervisory signal. adversarial learning USED-FOR objective function. adversarial learning USED-FOR variational inference framework. variational inference framework USED-FOR training. model USED-FOR disentangled representations. feature learning CONJUNCTION conditional image generation. conditional image generation CONJUNCTION feature learning. conditional image generation CONJUNCTION attribute transfer. attribute transfer CONJUNCTION conditional image generation. tasks EVALUATE-FOR model. minimal supervision USED-FOR model. attribute transfer HYPONYM-OF tasks. feature learning HYPONYM-OF tasks. conditional image generation HYPONYM-OF tasks. minimal supervision USED-FOR disentangled representations. OtherScientificTerm are high - level generative factors, target factors, supervision, and factors of interest. Method is Supervised approaches. Generic is representation. ","This paper proposes a method for learning disentangled representations from reference data. The method is based on a variational autoencoder (VAE) model, which is trained with a weak supervisory signal. The proposed method is evaluated on a number of tasks, including feature learning, conditional image generation, and attribute transfer. ","This paper proposes a method for learning disentangled representations from reference data. The method is based on a variational autoencoder (VAE) model, which is trained with a weak supervisory signal. The proposed method is evaluated on a number of tasks, including feature learning, conditional image generation, and attribute transfer. "
9245,SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"Deep neural network models USED-FOR rapid online adaptation. method USED-FOR continual online learning. deep neural network models USED-FOR method. mixture of models USED-FOR non - stationary task distributions. expectation maximization algorithm USED-FOR mixture of models. stochastic gradient descent USED-FOR model parameters. expectation maximization algorithm USED-FOR non - stationary task distributions. Chinese restaurant process USED-FOR expectation maximization algorithm. stochastic gradient descent USED-FOR online learning procedure. models COMPARE models. models COMPARE models. meta - learning USED-FOR model. SGD USED-FOR online adaptation. meta - learning USED-FOR online learning ( MOLe ) approach. motor failures CONJUNCTION unexpected disturbances. unexpected disturbances CONJUNCTION motor failures. varying terrains CONJUNCTION motor failures. motor failures CONJUNCTION varying terrains. MOLe COMPARE prior methods. prior methods COMPARE MOLe. MOLe USED-FOR continuous adaptation. continuous adaptation USED-FOR non - stationary task distributions. predictive model USED-FOR control. meta - learning USED-FOR model - based reinforcement learning. online learning ( MOLe ) approach USED-FOR model - based reinforcement learning. unexpected disturbances HYPONYM-OF non - stationary task distributions. varying terrains HYPONYM-OF non - stationary task distributions. motor failures HYPONYM-OF non - stationary task distributions. Method are predictive models, and large function approximators. OtherScientificTerm is real - world phenomena. ","This paper proposes a meta-learning approach for continual online learning (MOLe) for non-stationary tasks. The authors propose to use a mixture of models to learn a model that can adapt to new tasks in a continual learning setting. The proposed method is based on the Chinese restaurant process (CDP) framework, and the authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks. ","This paper proposes a meta-learning approach for continual online learning (MOLe) for non-stationary tasks. The authors propose to use a mixture of models to learn a model that can adapt to new tasks in a continual learning setting. The proposed method is based on the Chinese restaurant process (CDP) framework, and the authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks. "
9254,SP:5665e5f006f84927beb0440e145f476e02538077,"distributed prioritized experience replay USED-FOR RNN - based RL agents. representational drift CONJUNCTION recurrent state staleness. recurrent state staleness CONJUNCTION representational drift. parameter lag USED-FOR representational drift. single network architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION single network architecture. single network architecture USED-FOR agent. Recurrent Replay Distributed DQN HYPONYM-OF agent. It HYPONYM-OF agent. human - level performance EVALUATE-FOR It. human - level performance EVALUATE-FOR agent. Atari games EVALUATE-FOR It. Method are distributed training of RL agents, and training strategy. Material are Atari-57, and DMLab-30. ","This paper proposes a distributed prioritized experience replay (DQN) method for RL agents. The proposed method is motivated by the observation that RNN-based RL agents suffer from representational drift and recurrent state staleness. To address this issue, the authors propose to use a single network architecture and hyperparameters to reduce the parameter lag. Experiments on Atari games show that the proposed method outperforms the baselines.","This paper proposes a distributed prioritized experience replay (DQN) method for RL agents. The proposed method is motivated by the observation that RNN-based RL agents suffer from representational drift and recurrent state staleness. To address this issue, the authors propose to use a single network architecture and hyperparameters to reduce the parameter lag. Experiments on Atari games show that the proposed method outperforms the baselines."
9263,SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,sequential generative models USED-FOR coordinated multi - agent trajectory behavior. offensive basketball gameplay HYPONYM-OF coordinated multi - agent trajectory behavior. hierarchical models USED-FOR long - term coordination. hierarchical models USED-FOR settings. intermediate variables USED-FOR hierarchical models. intermediate variables USED-FOR high - level behavioral semantics. hierarchical framework USED-FOR sequential generative models. programmatically produced weak labels USED-FOR spatiotemporal regime. programmatically produced weak labels USED-FOR approach. framework USED-FOR complex interactions between basketball players. framework USED-FOR realistic multi - agent trajectories of basketball gameplay. quantitative and qualitative evaluations EVALUATE-FOR approach. OtherScientificTerm is synthetic settings. ,This paper proposes a hierarchical generative model for multi-agent trajectories of basketball games. The model is based on a programmatically produced weak labels for the spatiotemporal regime and a hierarchical model for long-term coordination. Experiments show that the proposed model is able to achieve state-of-the-art performance in both quantitative and qualitative evaluations.,This paper proposes a hierarchical generative model for multi-agent trajectories of basketball games. The model is based on a programmatically produced weak labels for the spatiotemporal regime and a hierarchical model for long-term coordination. Experiments show that the proposed model is able to achieve state-of-the-art performance in both quantitative and qualitative evaluations.
9272,SP:1a90cdf028068528b0559e7d44bf26dda20310bd,vision model USED-FOR interacting agents. method USED-FOR temporal information. ambiguous visual information USED-FOR dynamics model. dynamics model USED-FOR method. method COMPARE baselines. baselines COMPARE method. one CONJUNCTION one. one CONJUNCTION one. one EVALUATE-FOR method. one EVALUATE-FOR method. soccer game engine USED-FOR one. real basketball trajectories USED-FOR one. one HYPONYM-OF sports datasets. one HYPONYM-OF sports datasets. sports datasets EVALUATE-FOR method. sports datasets EVALUATE-FOR baselines. ,This paper proposes a method for learning a dynamic dynamics model for interacting agents. The proposed method is based on a vision model and a dynamics model. The dynamics model is trained using a soccer game engine. The authors show that the proposed method outperforms baselines on a number of sports datasets. ,This paper proposes a method for learning a dynamic dynamics model for interacting agents. The proposed method is based on a vision model and a dynamics model. The dynamics model is trained using a soccer game engine. The authors show that the proposed method outperforms baselines on a number of sports datasets. 
9281,SP:8392f04b7265f665ba6d44d297bca245d44b4708,"Deep neural networks USED-FOR approximating complicated functions. gradient descent methods USED-FOR Deep neural networks. neural network USED-FOR functionality. method USED-FOR end - to - end training. base neural network USED-FOR end - to - end training. method USED-FOR base neural network. differentiable neural network USED-FOR black - box functionality. differentiable estimator CONJUNCTION external blackbox non - differentiable counterpart. external blackbox non - differentiable counterpart CONJUNCTION differentiable estimator. neural network USED-FOR input to blackbox functionality. Estimate and Replace ” paradigm USED-FOR neural network. black - box function USED-FOR integrated model. integrated model COMPARE fully differentiable model. fully differentiable model COMPARE integrated model. black - box function USED-FOR inference. black - box function USED-FOR fully differentiable model. integrated model COMPARE RL - based methods. RL - based methods COMPARE integrated model. Task are training, and end - to - end optimization process. Generic is task. OtherScientificTerm are black - box functions, blackbox functions, black - box function interface, and intermediate labels. Method is base network. ","This paper proposes a new method for end-to-end training of deep neural networks. The proposed method is based on the “Estimate and Replace” paradigm. The authors propose to use a differentiable neural network to learn the black-box function interface between the input and the output of a neural network, and then use an external blackbox non-differentiable estimator to replace the input to the blackbox function. The paper shows that the proposed method outperforms a number of baselines in terms of accuracy and efficiency. ","This paper proposes a new method for end-to-end training of deep neural networks. The proposed method is based on the “Estimate and Replace” paradigm. The authors propose to use a differentiable neural network to learn the black-box function interface between the input and the output of a neural network, and then use an external blackbox non-differentiable estimator to replace the input to the blackbox function. The paper shows that the proposed method outperforms a number of baselines in terms of accuracy and efficiency. "
9290,SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,learning USED-FOR task. data - driven inductive bias USED-FOR learning. gradient - based meta - learning CONJUNCTION hierarchical Bayes. hierarchical Bayes CONJUNCTION gradient - based meta - learning. function approximator USED-FOR mixture of hierarchical Bayesian models. neural network HYPONYM-OF function approximator. stochastic expectation maximization procedure USED-FOR parameter initializations. parameter initializations USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR latent assignment of tasks. initializations USED-FOR latent assignment of tasks. approach USED-FOR diversity of training tasks. inductive biases PART-OF hyperparameters. miniImageNet benchmark EVALUATE-FOR 1 - shot classification. miniImageNet benchmark EVALUATE-FOR generalization. method USED-FOR task distribution. non - parametric variant USED-FOR task distribution. method USED-FOR non - parametric variant. few - shot regression tasks EVALUATE-FOR non - parametric variant. OtherScientificTerm is transfer. ,This paper proposes a method for learning a mixture of hierarchical Bayesian models with a data-driven inductive bias. The main idea is to use a function approximator to approximate the function of a hierarchical Bayes model. The authors show that the proposed method is able to generalize well to a wide range of training tasks. The method is evaluated on miniImageNet and few-shot regression tasks.,This paper proposes a method for learning a mixture of hierarchical Bayesian models with a data-driven inductive bias. The main idea is to use a function approximator to approximate the function of a hierarchical Bayes model. The authors show that the proposed method is able to generalize well to a wide range of training tasks. The method is evaluated on miniImageNet and few-shot regression tasks.
9299,SP:a410144dbe19713a06c63da87d9fb58b999a7492,Auxiliary learning USED-FOR principal task. domain knowledge USED-FOR manually - defined auxiliary tasks. auxiliary tasks USED-FOR auxiliary tasks. Meta Auxiliary Learning ( MAXL ) USED-FOR image classification. hierarchical sub - class image classification HYPONYM-OF auxiliary task. meta learner USED-FOR sub - class target labels. meta learner USED-FOR multi - task evaluator. MAXL COMPARE baseline auxiliary learning methods. baseline auxiliary learning methods COMPARE MAXL. CIFAR datasets EVALUATE-FOR MAXL. MAXL COMPARE method. method COMPARE MAXL. CIFAR datasets EVALUATE-FOR baseline auxiliary learning methods. human - defined sub - class hierarchies USED-FOR method. MAXL USED-FOR automated generalisation. OtherScientificTerm is human knowledge. ,"This paper proposes Meta Auxiliary Learning (MAXL), a meta-learning method for image classification. The proposed method is based on the meta learner model, which is a multi-task evaluator that learns a set of sub-class target labels for each task. The meta-learner is trained on the sub-classes of the target task, and the meta-learner is used to evaluate the performance of the task on the target tasks. MAXL is evaluated on CIFAR-10 and ImageNet, and shows that it outperforms the baselines in terms of generalization. ","This paper proposes Meta Auxiliary Learning (MAXL), a meta-learning method for image classification. The proposed method is based on the meta learner model, which is a multi-task evaluator that learns a set of sub-class target labels for each task. The meta-learner is trained on the sub-classes of the target task, and the meta-learner is used to evaluate the performance of the task on the target tasks. MAXL is evaluated on CIFAR-10 and ImageNet, and shows that it outperforms the baselines in terms of generalization. "
9308,SP:76248e1c914c60ce69de244fe7ec62488d01e161,neural network based representation USED-FOR open set recognition problem. datasets EVALUATE-FOR approaches. Generic is representation. ,This paper proposes a neural network based representation for open set recognition problem. The proposed method is based on the idea of neural network-based representation learning. The authors show that the proposed method can be applied to a wide range of open set classification problems. The method is evaluated on a variety of datasets.,This paper proposes a neural network based representation for open set recognition problem. The proposed method is based on the idea of neural network-based representation learning. The authors show that the proposed method can be applied to a wide range of open set classification problems. The method is evaluated on a variety of datasets.
9317,SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"ResNet-34 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-34. ResNet-50 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION ResNet-50. Inception - v3 CONJUNCTION densenet161. densenet161 CONJUNCTION Inception - v3. densenet161 CONJUNCTION VGG-16bn networks. VGG-16bn networks CONJUNCTION densenet161. ResNet-152 CONJUNCTION Inception - v3. Inception - v3 CONJUNCTION ResNet-152. accuracy EVALUATE-FOR full - precision baseline networks. finetuning USED-FOR full - precision baseline networks. ImageNet classification benchmark EVALUATE-FOR VGG-16bn networks. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. accuracy EVALUATE-FOR full - precision baseline networks. stochastic gradient descent USED-FOR training error. pretrained fp32 precision baseline networks CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pretrained fp32 precision baseline networks. pretrained fp32 precision baseline networks USED-FOR solution distance. matched learning rate annealing USED-FOR combat noise. techniques USED-FOR low - precision networks. techniques CONJUNCTION activation function range calibration. activation function range calibration CONJUNCTION techniques. activation function range calibration USED-FOR low - precision networks. Task is embedded deep network inference. Metric are energy and area efficiency, and energy. Method are pretrained models, and fp32 precision baseline networks. Generic are 4 - bit models, baseline networks, noise, and they. OtherScientificTerm are cosine similarity, gradient noise, quantization, and maximum variance of the gradient estimates. ","This paper studies the problem of training low-precision deep neural networks in the presence of cosine similarity and gradient noise. The authors propose two techniques to mitigate the problem. First, the authors propose to use stochastic gradient descent to reduce the training error. Second, they propose a matched learning rate annealing method to mitigate cosine noise. Experiments show that the proposed method outperforms the baselines in terms of energy and area efficiency.","This paper studies the problem of training low-precision deep neural networks in the presence of cosine similarity and gradient noise. The authors propose two techniques to mitigate the problem. First, the authors propose to use stochastic gradient descent to reduce the training error. Second, they propose a matched learning rate annealing method to mitigate cosine noise. Experiments show that the proposed method outperforms the baselines in terms of energy and area efficiency."
9326,SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,approach USED-FOR surface properties. model USED-FOR post - bounce trajectories. bouncing restitution CONJUNCTION effective collision normals. effective collision normals CONJUNCTION bouncing restitution. model USED-FOR physical properties. sensor inputs USED-FOR post - bounce trajectories. physical properties USED-FOR bouncing restitution. physical properties USED-FOR effective collision normals. sensor inputs USED-FOR model. Physics Inference Module ( PIM ) CONJUNCTION Visual Inference Module ( VIM ). Visual Inference Module ( VIM ) CONJUNCTION Physics Inference Module ( PIM ). modules PART-OF model. Bounce PART-OF model. Visual Inference Module ( VIM ) HYPONYM-OF modules. Physics Inference Module ( PIM ) HYPONYM-OF modules. Visual Inference Module ( VIM ) PART-OF model. PIM USED-FOR physical interactions. PIM USED-FOR prediction task. VIM USED-FOR physical parameters. physical parameters CONJUNCTION observed pre - collision 3D trajectories. observed pre - collision 3D trajectories CONJUNCTION physical parameters. physical interactions USED-FOR prediction task. physical parameters USED-FOR PIM. observed pre - collision 3D trajectories USED-FOR PIM. dataset EVALUATE-FOR model. dataset EVALUATE-FOR baselines. predicting post - bounce trajectories CONJUNCTION physical properties. physical properties CONJUNCTION predicting post - bounce trajectories. model COMPARE baselines. baselines COMPARE model. model USED-FOR predicting post - bounce trajectories. model USED-FOR physical properties. trajectory fitting USED-FOR post - bounce trajectories. Newtonian physics USED-FOR trajectory fitting. trajectory fitting HYPONYM-OF baselines. Material is Bounce Dataset. OtherScientificTerm is physics simulations. ,"This paper proposes a new method for predicting post-bounce trajectories based on physics simulations. The method is based on the Bounce Dataset, which is a collection of 3D trajectories from physics simulations and is used to train a model that can predict post-boom trajectories. The model is built on top of the PIM and Visual Inference Module (VIM) models, which are used to learn the physical properties of the trajectories and the physical interactions between physical parameters. The authors show that the proposed method outperforms the baselines in terms of trajectory fitting and physical properties.","This paper proposes a new method for predicting post-bounce trajectories based on physics simulations. The method is based on the Bounce Dataset, which is a collection of 3D trajectories from physics simulations and is used to train a model that can predict post-boom trajectories. The model is built on top of the PIM and Visual Inference Module (VIM) models, which are used to learn the physical properties of the trajectories and the physical interactions between physical parameters. The authors show that the proposed method outperforms the baselines in terms of trajectory fitting and physical properties."
9335,SP:010bd055310c363d3cb0fbe0e11546de58220e15,"neural networks USED-FOR adversarial images. gradients USED-FOR adversarial vulnerability. ` 1 - norm FEATURE-OF gradients. OtherScientificTerm are targeted but imperceptible image perturbations, image size, and network ’s weight distribution. Method is network architectures. Generic is nets. ","This paper studies the adversarial vulnerability of neural networks. The authors show that the gradients of the weight distribution of a neural network can be used as a measure of the vulnerability of the network to adversarial perturbations. They show that if the weights of a network are large enough, then the adversarially vulnerable network will not be able to detect the perturbation. They also show that when the weights are small enough, the network will be vulnerable to the perturbed image. ","This paper studies the adversarial vulnerability of neural networks. The authors show that the gradients of the weight distribution of a neural network can be used as a measure of the vulnerability of the network to adversarial perturbations. They show that if the weights of a network are large enough, then the adversarially vulnerable network will not be able to detect the perturbation. They also show that when the weights are small enough, the network will be vulnerable to the perturbed image. "
9351,SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"agent modeling USED-FOR mind model. probing USED-FOR agent modeling. pure curiosity - driven reinforcement learning USED-FOR probing policy. imitation learning USED-FOR approximated agent model. pure curiosity - driven reinforcement learning HYPONYM-OF learning processes. learning processes PART-OF framework. pure curiosity - driven reinforcement learning PART-OF framework. imitation learning PART-OF framework. imitation learning HYPONYM-OF learning processes. tasks EVALUATE-FOR approach. collaboration CONJUNCTION competition. competition CONJUNCTION collaboration. passive observation CONJUNCTION random probing. random probing CONJUNCTION passive observation. agent model COMPARE ones. ones COMPARE agent model. distilling optimal planning CONJUNCTION collaboration. collaboration CONJUNCTION distilling optimal planning. random probing CONJUNCTION curiositydriven approaches. curiositydriven approaches CONJUNCTION random probing. distilling optimal planning CONJUNCTION policy net. policy net CONJUNCTION distilling optimal planning. curiositydriven approaches USED-FOR ones. random probing USED-FOR agent model. approach USED-FOR agent model. competition HYPONYM-OF applications. distilling optimal planning HYPONYM-OF applications. passive observation USED-FOR agent model. passive observation USED-FOR ones. random probing USED-FOR ones. collaboration HYPONYM-OF applications. Method are interactive agent modeling scheme, and probing agent. ","This paper proposes an interactive agent modeling framework that combines curiosity-driven reinforcement learning with imitation learning to improve the performance of the probing agent. The proposed method is evaluated on a number of tasks and shows that the proposed method outperforms existing approaches. The authors also show that the method can be applied to a variety of tasks such as distilling optimal planning, collaboration, and competition. ","This paper proposes an interactive agent modeling framework that combines curiosity-driven reinforcement learning with imitation learning to improve the performance of the probing agent. The proposed method is evaluated on a number of tasks and shows that the proposed method outperforms existing approaches. The authors also show that the method can be applied to a variety of tasks such as distilling optimal planning, collaboration, and competition. "
9367,SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"modification USED-FOR Artificial Neural Networks ( ANNs ). Artificial Neural Networks ( ANNs ) USED-FOR ANNs. firing modes FEATURE-OF biological neuron. peripheral factors FEATURE-OF biological neuron. neuromodulators HYPONYM-OF peripheral factors. modification USED-FOR ANN nodes. ANN nodes USED-FOR activation sensitivities. Convolutional Neural Networks CONJUNCTION Long Short - Term Memory networks. Long Short - Term Memory networks CONJUNCTION Convolutional Neural Networks. modification COMPARE ANN nodes. ANN nodes COMPARE modification. ANN nodes USED-FOR Convolutional Neural Networks. ANN nodes USED-FOR Long Short - Term Memory networks. OtherScientificTerm are biological neurons, Biological neurons, biological neuromodulators, modulators, and slope of the activation function. ","This paper studies the effect of modulators on the activation sensitivity of neural networks. Specifically, the authors show that modulators can affect the activation sensitivities of the neural network. They show that the sensitivity of the activations of biological neurons can be modulated by a number of factors, including the number of neurons, the size of the neuron, and the size and depth of the neurons. The authors also show that this modification can be used to improve the performance of ANNs. ","This paper studies the effect of modulators on the activation sensitivity of neural networks. Specifically, the authors show that modulators can affect the activation sensitivities of the neural network. They show that the sensitivity of the activations of biological neurons can be modulated by a number of factors, including the number of neurons, the size of the neuron, and the size and depth of the neurons. The authors also show that this modification can be used to improve the performance of ANNs. "
9383,SP:287a577834fd2820a939a1113b39146a22727491,voice CONJUNCTION pitch. pitch CONJUNCTION voice. neural analysis and synthesis ( NANSY ) framework USED-FOR voice. neural analysis and synthesis ( NANSY ) framework USED-FOR pitch. information bottleneck USED-FOR analysis features. analysis features USED-FOR controllable synthesis. information perturbation USED-FOR training strategy. formant CONJUNCTION pitch. pitch CONJUNCTION formant. pitch CONJUNCTION frequency response. frequency response CONJUNCTION pitch. reconstruction quality CONJUNCTION controllability. controllability CONJUNCTION reconstruction quality. controllability EVALUATE-FOR it. reconstruction quality EVALUATE-FOR it. wav2vec feature CONJUNCTION pitch feature. pitch feature CONJUNCTION wav2vec feature. Yingram USED-FOR self - supervised training. pitch feature CONJUNCTION Yingram. Yingram CONJUNCTION pitch feature. analysis features USED-FOR NANSY. pitch feature HYPONYM-OF analysis features. wav2vec feature HYPONYM-OF analysis features. Yingram HYPONYM-OF analysis features. selfsupervised training USED-FOR NANSY. NANSY USED-FOR multilingual setting. multilingual dataset USED-FOR NANSY. multilingual dataset USED-FOR it. zero - shot voice conversion CONJUNCTION pitch shift. pitch shift CONJUNCTION zero - shot voice conversion. pitch shift CONJUNCTION time - scale modification. time - scale modification CONJUNCTION pitch shift. NANSY USED-FOR zero - shot voice conversion. NANSY USED-FOR applications. NANSY USED-FOR pitch shift. NANSY USED-FOR time - scale modification. time - scale modification HYPONYM-OF applications. zero - shot voice conversion HYPONYM-OF applications. pitch shift HYPONYM-OF applications. Method is synthesis networks. OtherScientificTerm is bottleneck structures. Material is speech data. ,This paper proposes a neural analysis and synthesis (NANSY) framework for voice synthesis. NANSY is based on self-supervised training with information perturbation to improve the controllability and reconstruction quality of voice synthesis networks. The proposed method is evaluated on a multilingual dataset and is able to achieve state-of-the-art performance on zero-shot voice conversion and pitch shift.,This paper proposes a neural analysis and synthesis (NANSY) framework for voice synthesis. NANSY is based on self-supervised training with information perturbation to improve the controllability and reconstruction quality of voice synthesis networks. The proposed method is evaluated on a multilingual dataset and is able to achieve state-of-the-art performance on zero-shot voice conversion and pitch shift.
9408,SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"gradient - based ) bilevel programming framework USED-FOR hyperparameter optimization. overfitting FEATURE-OF validation set. expectation bound USED-FOR cross - validation algorithm. gradient - based algorithms COMPARE cross - validation. cross - validation COMPARE gradient - based algorithms. regularization terms USED-FOR overfitting problem. regularization terms USED-FOR gradient - based algorithms. overfitting problem PART-OF gradient - based algorithms. outer and inner levels FEATURE-OF regularization terms. feature learning CONJUNCTION data reweighting. data reweighting CONJUNCTION feature learning. data reweighting USED-FOR noisy labels. OtherScientificTerm are optimization properties, and uniform stability. Task is generalization. Method is bilevel programming. ",This paper proposes a bilevel programming framework for hyperparameter optimization. The main idea is to use regularization terms to reduce the overfitting problem in cross-validation. The authors show that the regularization term can be applied to both the outer and inner levels of the validation set. The paper also provides an upper bound on the variance of the upper bound. ,This paper proposes a bilevel programming framework for hyperparameter optimization. The main idea is to use regularization terms to reduce the overfitting problem in cross-validation. The authors show that the regularization term can be applied to both the outer and inner levels of the validation set. The paper also provides an upper bound on the variance of the upper bound. 
9433,SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"knowledge distillation approach USED-FOR transfer of dark knowledge. student models USED-FOR methods. algorithm USED-FOR student - friendly representations. algorithm USED-FOR student branches. knowledge distillation methods USED-FOR student models. accuracy CONJUNCTION convergence speed. convergence speed CONJUNCTION accuracy. approach USED-FOR teacher models. technique USED-FOR student models. technique USED-FOR knowledge distillation methods. convergence speed EVALUATE-FOR student models. knowledge distillation techniques EVALUATE-FOR algorithm. teacher and student models USED-FOR knowledge distillation techniques. accuracy EVALUATE-FOR algorithm. Task are knowledge transfer, and knowledge distillation procedure. Method are teacher model, and teacher networks. ","This paper proposes a knowledge distillation approach for transfer of dark knowledge from teacher networks to student networks. The key idea is to learn a student-friendly representation of the teacher network, which can then be used to improve the performance of the student models. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in terms of accuracy and convergence speed.","This paper proposes a knowledge distillation approach for transfer of dark knowledge from teacher networks to student networks. The key idea is to learn a student-friendly representation of the teacher network, which can then be used to improve the performance of the student models. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in terms of accuracy and convergence speed."
9458,SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"Generalization PART-OF machine learning. invariant features USED-FOR algorithms. invariance USED-FOR OOD generalization. generalization USED-FOR out - of - distribution. expansion function USED-FOR OOD generalization. model selection module PART-OF OOD learning algorithm. model selection criterion FEATURE-OF theory. model selection criterion COMPARE baselines. baselines COMPARE model selection criterion. benchmark OOD datasets EVALUATE-FOR model selection criterion. benchmark OOD datasets EVALUATE-FOR baselines. Task are extracting invariant features, OOD, and OOD problem. OtherScientificTerm is OOD generalization error bounds. ","This paper studies the problem of out-of-distribution (OOD) generalization, which is an important problem in machine learning. The main contribution of this paper is to provide a new generalization bound for OOD generalization. The bound is based on the theory of invariance, which shows that the generalization error of an OOD learning algorithm is bounded by the expansion function of the model selection module. The paper also provides a theoretical analysis of model selection criterion and provides theoretical guarantees for the bounds. ","This paper studies the problem of out-of-distribution (OOD) generalization, which is an important problem in machine learning. The main contribution of this paper is to provide a new generalization bound for OOD generalization. The bound is based on the theory of invariance, which shows that the generalization error of an OOD learning algorithm is bounded by the expansion function of the model selection module. The paper also provides a theoretical analysis of model selection criterion and provides theoretical guarantees for the bounds. "
9483,SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"stationary distribution USED-FOR meta - learning. Dynamic Gaussian Mixture Model USED-FOR meta - parameters. Dynamic Gaussian Mixture Model USED-FOR VC - BML. Chinese Restaurant Process USED-FOR number of component distributions. Dynamic mixtures USED-FOR meta - parameter level. Dynamic mixtures USED-FOR negative knowledge transfer problem. Dynamic mixtures USED-FOR diverse and dissimilar tasks. structured variational inference USED-FOR avoiding forgetting knowledge. posterior approximation method USED-FOR avoiding forgetting knowledge. point estimation method COMPARE posterior approximation method. posterior approximation method COMPARE point estimation method. structured variational inference HYPONYM-OF posterior approximation method. point estimation method USED-FOR posteriors of model parameters. VC - BML USED-FOR catastrophic forgetting. tasks EVALUATE-FOR VC - BML. non - stationary distributions FEATURE-OF tasks. Task is online setting. OtherScientificTerm are non - stationary distribution, and parameter space. ","This paper proposes a new method for meta-learning, called Dynamic Gaussian Mixture Model (VC-BML), which is based on the Chinese Restaurant Process (CDP) framework. The authors propose to use Dynamic Mixture Models (DMMs) to learn the meta-parameters of the model, which are then used to estimate the posteriors of model parameters. They show that the proposed method is able to avoid catastrophic forgetting in the online setting. ","This paper proposes a new method for meta-learning, called Dynamic Gaussian Mixture Model (VC-BML), which is based on the Chinese Restaurant Process (CDP) framework. The authors propose to use Dynamic Mixture Models (DMMs) to learn the meta-parameters of the model, which are then used to estimate the posteriors of model parameters. They show that the proposed method is able to avoid catastrophic forgetting in the online setting. "
9508,SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"boundary conditions FEATURE-OF ordinary differential equations. it USED-FOR BVPs. Gauss – Markov prior CONJUNCTION it. it CONJUNCTION Gauss – Markov prior. linear time FEATURE-OF posterior distribution. mesh refinement CONJUNCTION hyperparameter adaptation. hyperparameter adaptation CONJUNCTION mesh refinement. uncertainty quantification CONJUNCTION mesh refinement. mesh refinement CONJUNCTION uncertainty quantification. model USED-FOR uncertainty quantification. model USED-FOR mesh refinement. model USED-FOR hyperparameter adaptation. probabilistic BVP solver COMPARE non - probabilistic algorithms. non - probabilistic algorithms COMPARE probabilistic BVP solver. algorithms USED-FOR ODE boundary value problems. first - order problems USED-FOR higher - order problems. manifold learning USED-FOR BVPs. numerical simulation CONJUNCTION probabilistic inference. probabilistic inference CONJUNCTION numerical simulation. lineartime complexity CONJUNCTION adaptive step - size selection. adaptive step - size selection CONJUNCTION lineartime complexity. adaptive step - size selection CONJUNCTION polynomial convergence rates. polynomial convergence rates CONJUNCTION adaptive step - size selection. adaptive step - size selection FEATURE-OF probabilistic solvers. lineartime complexity FEATURE-OF probabilistic solvers. probabilistic solvers USED-FOR initial value problems. Generic are algorithm, and scheme. Method are non - probabilistic methods, statistical modelling tool - chain, and Probabilistic numerical algorithms. Task are Boundary value problems, first - order boundary value problem, machine learning, and Neural Information Processing Systems. OtherScientificTerm are computational pipelines, leftand right - hand side boundary conditions, vector field, ODE knowledge, infectious disease, integration domain, and structured output uncertainty. ","This paper proposes a probabilistic BVP solver for the first-order boundary value problem. The proposed method is based on the manifold learning framework. The authors show that the proposed method outperforms non-probabilistic methods in terms of line-time complexity, adaptive step-size selection, and polynomial convergence rates. They also show that their method can be applied to the integration domain.","This paper proposes a probabilistic BVP solver for the first-order boundary value problem. The proposed method is based on the manifold learning framework. The authors show that the proposed method outperforms non-probabilistic methods in terms of line-time complexity, adaptive step-size selection, and polynomial convergence rates. They also show that their method can be applied to the integration domain."
9533,SP:86aac0c6b75fdc12f84bba342934865616f866d4,"partially observable system FEATURE-OF near optimal policy. episodic reinforcement learning USED-FOR reward - mixingMarkov decision process ( MDP ). reward models USED-FOR reward function. near optimal policy USED-FOR reward - mixing MDPs. algorithmic and analysis techniques USED-FOR problem. polynomial - time algorithm USED-FOR -optimal policy. observation space COMPARE latent state space. latent state space COMPARE observation space. Task is reinforcement learning. Method are reward model, and switching reward - models. OtherScientificTerm are dynamics, time - horizon, and partially observed environments. Generic are approaches, assumptions, and algorithm. ",This paper studies the problem of reward-mixing in episodic reinforcement learning. The authors propose a polynomial-time algorithm to find the near optimal policy for reward mixing in partially observable systems. The algorithm is based on the analysis of the dynamics of the reward model and the reward function in the latent state space. Experiments show that the proposed algorithm outperforms existing approaches.,This paper studies the problem of reward-mixing in episodic reinforcement learning. The authors propose a polynomial-time algorithm to find the near optimal policy for reward mixing in partially observable systems. The algorithm is based on the analysis of the dynamics of the reward model and the reward function in the latent state space. Experiments show that the proposed algorithm outperforms existing approaches.
9558,SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"methods USED-FOR conditional average treatment effect estimation. two - step procedure USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) HYPONYM-OF two - step procedure. covariate adjustment USED-FOR estimator. covariate adjustment USED-FOR augmented dataset. It USED-FOR estimator. covariate adjustment USED-FOR It. synthetic and semi - synthetic experiments EVALUATE-FOR SCP. Generic are applications, problem, and procedure. Task are multi - cause treatment effect problems, multi - cause problem, and causal inference. OtherScientificTerm are confounding bias, cause combination, and single - cause interventions. Material is observational dataset. ",This paper proposes a two-step procedure for multi-cause treatment effect estimation. The first step is to estimate the conditional average treatment effect (CATE) of a single-causal treatment effect. The second step is a covariate adjustment to adjust the covariate of the augmented dataset. The proposed method is evaluated on synthetic and semi-synthetic experiments.,This paper proposes a two-step procedure for multi-cause treatment effect estimation. The first step is to estimate the conditional average treatment effect (CATE) of a single-causal treatment effect. The second step is a covariate adjustment to adjust the covariate of the augmented dataset. The proposed method is evaluated on synthetic and semi-synthetic experiments.
9583,SP:247bc6675cce89d51558537daf63dadb0c4307f8,"multiwavelet - based neural operator learning scheme USED-FOR operator ’s kernel. fine - grained wavelets USED-FOR multiwavelet - based neural operator learning scheme. multiwavelet polynomial bases USED-FOR projection of the kernel. Burgers ’ equation CONJUNCTION Darcy Flow. Darcy Flow CONJUNCTION Burgers ’ equation. Darcy Flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy Flow. Korteweg - de Vries ( KdV ) equation CONJUNCTION Burgers ’ equation. Burgers ’ equation CONJUNCTION Korteweg - de Vries ( KdV ) equation. neural operator approaches COMPARE model. model COMPARE neural operator approaches. state - of - the - art EVALUATE-FOR model. accuracy EVALUATE-FOR neural operator approaches. accuracy EVALUATE-FOR model. relative L2 error EVALUATE-FOR Burgers ’ ( KdV ) equation. method USED-FOR time - varying equations. relative L2 error EVALUATE-FOR method. mappings between function spaces USED-FOR method. lower - resolution data USED-FOR method. OtherScientificTerm are inverse operator map, and complex dependencies. Method are inverse multiwavelet filters, projected kernel, and resolution - independent scheme. ","This paper proposes a multi-wavelet-based neural operator learning scheme for time-varying equations. The proposed method is based on the idea of inverse multiwavelet filters, which can be viewed as a projection of the operator’s kernel. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and relative L2 error. The method is evaluated on the Burgers’ equation, Darcy Flow and Navier-Stokes equation.","This paper proposes a multi-wavelet-based neural operator learning scheme for time-varying equations. The proposed method is based on the idea of inverse multiwavelet filters, which can be viewed as a projection of the operator’s kernel. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and relative L2 error. The method is evaluated on the Burgers’ equation, Darcy Flow and Navier-Stokes equation."
9608,SP:1153785e6a016cfee2644952a772aa08927299b6,"Binary neural networks ( BNNs ) USED-FOR full - precision weights. 1 - bit with sign function USED-FOR Binary neural networks ( BNNs ). gradient FEATURE-OF sign function. approximate gradient USED-FOR optimization difficulty. sine functions USED-FOR BNNs. frequency domain approximation ( FDA ) HYPONYM-OF BNNs. Fourier frequency domain FEATURE-OF gradient of sign function. frequency domain approximation ( FDA ) HYPONYM-OF sine functions. low - frequency information FEATURE-OF sign function. noise adaptation module USED-FOR approximation error. benchmark datasets CONJUNCTION neural architectures. neural architectures CONJUNCTION benchmark datasets. method USED-FOR binary network. Method is back - propagation. Generic are approximations, and approach. OtherScientificTerm are factual gradient, and high - frequency coefficients. ",This paper studies the problem of learning binary neural networks (BNNs) with 1-bit with sign function. The authors propose a novel method to approximate the gradient of the sign function in the Fourier frequency domain (FDA) using a noise adaptation module. The proposed method is based on the idea of back-propagation. The method is evaluated on a number of benchmark datasets and shows promising results. ,This paper studies the problem of learning binary neural networks (BNNs) with 1-bit with sign function. The authors propose a novel method to approximate the gradient of the sign function in the Fourier frequency domain (FDA) using a noise adaptation module. The proposed method is based on the idea of back-propagation. The method is evaluated on a number of benchmark datasets and shows promising results. 
9633,SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,cortical areas USED-FOR tasks. Recurrent neural networks ( RNNs ) USED-FOR cortical areas. Recurrent neural networks ( RNNs ) USED-FOR neuroscience - based tasks. cortical area USED-FOR tasks. multi - area RNNs USED-FOR multi - area computation. neuroscience - inspired architecture constraints FEATURE-OF multi - area RNNs. Dale ’s Law USED-FOR networks. full observability FEATURE-OF RNNs. full observability USED-FOR output - relevant information. modular computation USED-FOR minimal sufficient representations of task information. cortex USED-FOR minimal sufficient representations of task information. modular computation USED-FOR cortex. constrained multi - area RNNs USED-FOR computations. distributed computation PART-OF neural systems. OtherScientificTerm is coordination of multiple brain areas. Generic is computation. ,"This paper studies the problem of multi-area computation in recurrent neural networks (RNNs). The authors propose a new architecture for RNNs based on neuroscience-inspired architecture constraints. The proposed architecture is based on the Dale’s Law. The authors show that the proposed architecture can be applied to a wide range of tasks, and that it is able to achieve state-of-the-art performance. ","This paper studies the problem of multi-area computation in recurrent neural networks (RNNs). The authors propose a new architecture for RNNs based on neuroscience-inspired architecture constraints. The proposed architecture is based on the Dale’s Law. The authors show that the proposed architecture can be applied to a wide range of tasks, and that it is able to achieve state-of-the-art performance. "
9658,SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,Saliency maps USED-FOR convolutional neural networks ( CNNs ). convolutional neural networks ( CNNs ) USED-FOR image classification. saliency map USED-FOR image of interest. maps USED-FOR classification. confidence EVALUATE-FOR classifier. structured attention graphs ( SAGs ) USED-FOR attention maps. compact and representative SAG USED-FOR visualization. approach USED-FOR compact and representative SAG. diverse sampling USED-FOR approach. diverse sampling USED-FOR compact and representative SAG. diverse sampling USED-FOR visualization. SAGs COMPARE saliency maps. saliency maps COMPARE SAGs. SAGs USED-FOR comparative counterfactual questions. saliency maps USED-FOR comparative counterfactual questions. user study USED-FOR comparative counterfactual questions. user study EVALUATE-FOR SAGs. image classifications FEATURE-OF comparative counterfactual questions. SAGs COMPARE saliency map baselines. saliency map baselines COMPARE SAGs. user accuracy EVALUATE-FOR SAGs. user accuracy EVALUATE-FOR saliency map baselines. Method is beam search algorithm. OtherScientificTerm is image regions. ,"This paper proposes a novel approach to learn saliency maps for image classification. The proposed approach is based on the idea of structured attention graphs (SAGs), which is an extension of the attention graph (AG) framework. The key idea is to learn a compact and representative SAG, which is then used to learn the saliency map of an image of interest. The authors show that the proposed approach outperforms SAGs on a number of benchmark datasets.","This paper proposes a novel approach to learn saliency maps for image classification. The proposed approach is based on the idea of structured attention graphs (SAGs), which is an extension of the attention graph (AG) framework. The key idea is to learn a compact and representative SAG, which is then used to learn the saliency map of an image of interest. The authors show that the proposed approach outperforms SAGs on a number of benchmark datasets."
9683,SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,loss functions CONJUNCTION regularizers. regularizers CONJUNCTION loss functions. loss functions USED-FOR image classification tasks. image classification tasks EVALUATE-FOR regularizers. test accuracy EVALUATE-FOR loss functions. test accuracy EVALUATE-FOR regularizers. loss functions USED-FOR representations. representations USED-FOR downstream tasks. loss functions USED-FOR downstream tasks. transferability EVALUATE-FOR hidden representations of convolutional neural networks. ImageNet USED-FOR hidden representations of convolutional neural networks. fixed feature extractors USED-FOR downstream tasks. networks USED-FOR tasks. objectives COMPARE vanilla softmax cross - entropy. vanilla softmax cross - entropy COMPARE objectives. ImageNet accuracy EVALUATE-FOR vanilla softmax cross - entropy. ImageNet accuracy EVALUATE-FOR objectives. centered kernel alignment USED-FOR hidden representations of networks. objectives CONJUNCTION hyperparameter combinations. hyperparameter combinations CONJUNCTION objectives. hyperparameter combinations USED-FOR class separation. objectives USED-FOR class separation. features USED-FOR downstream tasks. accuracy EVALUATE-FOR task. class separation FEATURE-OF Representations. task EVALUATE-FOR Representations. accuracy EVALUATE-FOR Representations. learning invariant features CONJUNCTION features. features CONJUNCTION learning invariant features. features USED-FOR transfer tasks. learning invariant features USED-FOR task. OtherScientificTerm is loss. Generic is network. ,"This paper studies the transferability of hidden representations of convolutional neural networks. The authors propose two objectives for learning invariant features and hyperparameter combinations for class separation. The main contribution of the paper is to propose a new objective for class separability, which is based on the idea of centered kernel alignment. The proposed objective is evaluated on ImageNet classification tasks and compared to the vanilla softmax cross-entropy objective. ","This paper studies the transferability of hidden representations of convolutional neural networks. The authors propose two objectives for learning invariant features and hyperparameter combinations for class separation. The main contribution of the paper is to propose a new objective for class separability, which is based on the idea of centered kernel alignment. The proposed objective is evaluated on ImageNet classification tasks and compared to the vanilla softmax cross-entropy objective. "
9708,SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"spatial sampling CONJUNCTION temporal frequency of sampling. temporal frequency of sampling CONJUNCTION spatial sampling. neural network training strategy USED-FOR deep generative models of latent dynamics. selective backpropagation through time ( SBTT ) HYPONYM-OF neural network training strategy. SBTT USED-FOR sequential autoencoders. electrophysiological and calcium imaging data USED-FOR neural population dynamics. SBTT USED-FOR inference of neuronal population dynamics. electrophysiology USED-FOR inference of neuronal population dynamics. SBTT USED-FOR electrophysiology. interface bandwidths FEATURE-OF inference of neuronal population dynamics. SBTT USED-FOR high - frequency temporal structure. high - frequency temporal structure FEATURE-OF neural population activity. SBTT USED-FOR neural population activity. SBTT USED-FOR two - photon calcium imaging. limited, highbandwidth sampling USED-FOR pretrain dynamics models. SBTT USED-FOR models. models USED-FOR sparsely - sampled data. OtherScientificTerm are neural interfaces, brain circuits, bandwidth limits, latent low - dimensional population dynamics, latent dynamics, neuronal population dynamics, and implanted neuroelectronic interfaces. Task is Neural Information Processing Systems. ","This paper proposes selective backpropagation through time (SBTT) as a neural network training strategy to improve the performance of deep generative models of latent dynamics models of neural population dynamics. SBTT is based on the observation that the bandwidth of neural networks is limited due to the high-frequency temporal structure of the input data. To address this issue, the authors propose to use a sequential autoencoder (SAC) model to learn the latent dynamics of the neural network. The proposed method is evaluated on two-photon calcium imaging and electrophysiological data.","This paper proposes selective backpropagation through time (SBTT) as a neural network training strategy to improve the performance of deep generative models of latent dynamics models of neural population dynamics. SBTT is based on the observation that the bandwidth of neural networks is limited due to the high-frequency temporal structure of the input data. To address this issue, the authors propose to use a sequential autoencoder (SAC) model to learn the latent dynamics of the neural network. The proposed method is evaluated on two-photon calcium imaging and electrophysiological data."
9733,SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"Sequence - to - sequence learning USED-FOR sequence prediction tasks. neural networks USED-FOR Sequence - to - sequence learning. approach USED-FOR local distribution. neural network USED-FOR approach. neural network USED-FOR local distribution. hierarchical approach USED-FOR sequence - to - sequence learning. quasi - synchronous grammars USED-FOR hierarchical approach. style transfer CONJUNCTION small - scale machine translation. small - scale machine translation CONJUNCTION style transfer. compositional generalization ( SCAN ) CONJUNCTION style transfer. style transfer CONJUNCTION compositional generalization ( SCAN ). it COMPARE baselines. baselines COMPARE it. latent neural grammar USED-FOR domains. latent neural grammar USED-FOR diagnostic language navigation task. diagnostic language navigation task EVALUATE-FOR compositional generalization ( SCAN ). diagnostic language navigation task EVALUATE-FOR small - scale machine translation. diagnostic language navigation task HYPONYM-OF domains. style transfer HYPONYM-OF diagnostic language navigation task. compositional generalization ( SCAN ) HYPONYM-OF domains. small - scale machine translation HYPONYM-OF domains. style transfer HYPONYM-OF domains. Generic is models. Task is compositional generalization. Method are neural parameterization of the grammar, and manual feature engineering. OtherScientificTerm is combinatorial space of derivation rules. ","This paper proposes a hierarchical approach for sequence-to-sequence learning. The proposed approach is based on the notion of quasi-synchronous grammars. The authors show that the proposed approach outperforms baselines on a number of tasks, including style transfer, compositional generalization, and small-scale machine translation. ","This paper proposes a hierarchical approach for sequence-to-sequence learning. The proposed approach is based on the notion of quasi-synchronous grammars. The authors show that the proposed approach outperforms baselines on a number of tasks, including style transfer, compositional generalization, and small-scale machine translation. "
9769,SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,Feature Selection CONJUNCTION Functional Data Analysis. Functional Data Analysis CONJUNCTION Feature Selection. algorithm USED-FOR function - on - scalar feature selection. algorithm USED-FOR Group Elastic Net. Group Elastic Net USED-FOR function - on - scalar feature selection. scalar predictors USED-FOR functional response. algorithm USED-FOR Group Elastic Net. sparsity structure FEATURE-OF Augmented Lagrangian. algorithm USED-FOR ultrahigh dimensional settings. algorithm USED-FOR sparsity structure. ultrahigh dimensional settings FEATURE-OF Group Elastic Net. algorithm USED-FOR function - on - scalar regression framework. Functional Principal Components USED-FOR algorithm. approach COMPARE competitors. competitors COMPARE approach. simulations EVALUATE-FOR approach. Genome Wide Association Study USED-FOR application. Task is analysis of large and complex data sets. OtherScientificTerm is computational burden. ,"This paper proposes a new algorithm for function-on-scalar feature selection. The proposed algorithm is based on the Augmented Lagrangian (AGL) framework. The main idea is to learn a group of scalar predictors that can be used to predict the functional response of a function. The authors show that the proposed algorithm can be applied to a variety of applications, including feature selection, functional data analysis, and regression. ","This paper proposes a new algorithm for function-on-scalar feature selection. The proposed algorithm is based on the Augmented Lagrangian (AGL) framework. The main idea is to learn a group of scalar predictors that can be used to predict the functional response of a function. The authors show that the proposed algorithm can be applied to a variety of applications, including feature selection, functional data analysis, and regression. "
9805,SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,functional principal component analysis ( FPCA ) USED-FOR model estimation. real data analyses EVALUATE-FOR framework. Material is Structured point process data. Generic is matrix. OtherScientificTerm is log - Gaussian Cox processes. ,"This paper proposes a new framework for model estimation based on functional principal component analysis (FPCA). The framework is based on the notion of functional principal components (FPC) and is motivated by the observation that FPCA can be used to estimate the log-Gaussian Cox processes of a point process. The authors propose to use FPCAs to estimate a matrix of the principal components of the point process, which is then used as the basis for the model estimation. The proposed framework is evaluated on a number of real-world datasets and shows that the proposed method outperforms existing methods.","This paper proposes a new framework for model estimation based on functional principal component analysis (FPCA). The framework is based on the notion of functional principal components (FPC) and is motivated by the observation that FPCA can be used to estimate the log-Gaussian Cox processes of a point process. The authors propose to use FPCAs to estimate a matrix of the principal components of the point process, which is then used as the basis for the model estimation. The proposed framework is evaluated on a number of real-world datasets and shows that the proposed method outperforms existing methods."
9841,SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"online multi - task learning approach USED-FOR adaptive nonlinear control. adversarial disturbance CONJUNCTION unknown environmentdependent nonlinear dynamics. unknown environmentdependent nonlinear dynamics CONJUNCTION adversarial disturbance. unknown environmentdependent nonlinear dynamics FEATURE-OF nonlinear system. adversarial disturbance FEATURE-OF nonlinear system. shared representation USED-FOR environmentdependent dynamics. approach USED-FOR robot control. unified framework USED-FOR control - theoretic and learning - theoretic guarantees. non - asymptotic endto - end convergence guarantee USED-FOR multi - task nonlinear control. OMAC CONJUNCTION deep representation learning. deep representation learning CONJUNCTION OMAC. OMAC COMPARE adaptive control approaches. adaptive control approaches COMPARE OMAC. Method are Online Meta - Adaptive Control ( OMAC ), online representation learning, and control theory. Task is robotic system. ",This paper studies the problem of online multi-task learning for adaptive nonlinear control. The authors propose an online meta-adaptive control approach that learns a shared representation of the environment-dependent nonlinear dynamics and the adversarial dynamics of the system. The proposed approach is based on a unified framework of control theory and learning-theoretic guarantees. The paper shows that the proposed approach converges to a non-asymptotic end-to-end convergence guarantee. The method is evaluated on a variety of tasks and shows that it outperforms the baselines.,This paper studies the problem of online multi-task learning for adaptive nonlinear control. The authors propose an online meta-adaptive control approach that learns a shared representation of the environment-dependent nonlinear dynamics and the adversarial dynamics of the system. The proposed approach is based on a unified framework of control theory and learning-theoretic guarantees. The paper shows that the proposed approach converges to a non-asymptotic end-to-end convergence guarantee. The method is evaluated on a variety of tasks and shows that it outperforms the baselines.
9877,SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"bound propagation based certified robust training methods USED-FOR neural networks. certifiable robustness guarantees FEATURE-OF neural networks. interval bound propagation ( IBP ) CONJUNCTION CROWN - IBP. CROWN - IBP CONJUNCTION interval bound propagation ( IBP ). interval bound propagation ( IBP ) HYPONYM-OF SOTA ) methods. CROWN - IBP PART-OF SOTA ) methods. weight initialization method USED-FOR IBP training. regularization USED-FOR ReLU activation states. regularization USED-FOR certified bounds. BN USED-FOR ReLU activation states. Batch Normalization ( BN ) USED-FOR model. regularization USED-FOR certified training. verified error CONJUNCTION verified error. verified error CONJUNCTION verified error. verified error FEATURE-OF TinyImageNet. network architecture USED-FOR SOTA. Metric is per - batch training complexity. Method are neural network training, and Fast - Certified - Robust - Training. Generic are they, and methods. OtherScientificTerm are exploded bounds, long warmup schedules, and training schedules. Task is wamrup. Material is CIFAR-10. ","This paper studies the problem of certified robust training for deep neural networks. The authors propose two methods, CROWN-IBP and IBP-BCN, which are based on interval bound propagation (IBP) and Batch Normalization (BN). The authors show that these two methods are able to achieve better certified error bounds than existing methods. They also show that BN can be used as a regularizer to improve the robustness of the network. ","This paper studies the problem of certified robust training for deep neural networks. The authors propose two methods, CROWN-IBP and IBP-BCN, which are based on interval bound propagation (IBP) and Batch Normalization (BN). The authors show that these two methods are able to achieve better certified error bounds than existing methods. They also show that BN can be used as a regularizer to improve the robustness of the network. "
9913,SP:18ffeb199a670fb2b1f4417b8653479001944dab,"change point detection method USED-FOR adversaries. Huber ε - contamination framework USED-FOR adversarial attacks. phase transition phenomenon FEATURE-OF change point detection. minimax lower bound USED-FOR computationally - feasible method. Task are Change point detection, and univariate mean change point detection problem. Method are theoretically - justified methods, and robust change point detection methods. OtherScientificTerm are model violations, heavy - tailed noise distribution, isolate outliers, systematic contamination, spurious change points, contamination distributions, detection boundary, contamination proportion ε, contamination proportion, and logarithmic factors. Metric is minimax - rate optimal localisation error rate. ","This paper studies the problem of change point detection. The authors propose a Huber ε-contamination framework for the univariate mean-change point detection problem. The proposed method is based on the phase transition phenomenon of the detection boundary, and is computationally-easier than existing methods. The paper also provides a minimax lower bound for the localisation error rate of the proposed method. ","This paper studies the problem of change point detection. The authors propose a Huber ε-contamination framework for the univariate mean-change point detection problem. The proposed method is based on the phase transition phenomenon of the detection boundary, and is computationally-easier than existing methods. The paper also provides a minimax lower bound for the localisation error rate of the proposed method. "
9949,SP:d03617b5fc446768809cf015c9234b0c9386a690,"differentiable model CONJUNCTION neural network. neural network CONJUNCTION differentiable model. batch Gradient Descent ( GD ) USED-FOR empirical loss. batch Gradient Descent ( GD ) USED-FOR learning. paradigms USED-FOR learning problems. GD USED-FOR learning. SGD USED-FOR learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. precision ρ FEATURE-OF gradient calculations. statistical queries ( SQ ) USED-FOR learning. SGD USED-FOR sample - based learning algorithm. learning power EVALUATE-FOR PAC learning. SGD USED-FOR SQ learning. fine enough precision COMPARE minibatch size. minibatch size COMPARE fine enough precision. GD USED-FOR sample - based learning algorithm. fine enough precision USED-FOR GD. GD USED-FOR PAC learning. SGD USED-FOR PAC learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. SGD COMPARE SQ learning. SQ learning COMPARE SGD. OtherScientificTerm are population loss, bρ, ρ, and mini - batch size. ","This paper studies the problem of sample-based learning in the context of statistical queries (SQ). In particular, the authors consider the case of PAC learning, where the sample size is limited to a minibatch size. The authors propose to use batch Gradient Descent (GD) as the empirical loss for learning, which is a generalization of SGD. They show that GD can be used to improve the sample efficiency of SQ learning. They also show that fine-tuning SGD with fine-to-fine precision is sufficient to improve sample efficiency in PAC learning.","This paper studies the problem of sample-based learning in the context of statistical queries (SQ). In particular, the authors consider the case of PAC learning, where the sample size is limited to a minibatch size. The authors propose to use batch Gradient Descent (GD) as the empirical loss for learning, which is a generalization of SGD. They show that GD can be used to improve the sample efficiency of SQ learning. They also show that fine-tuning SGD with fine-to-fine precision is sufficient to improve sample efficiency in PAC learning."
9985,SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"machine learning CONJUNCTION inverse problems. inverse problems CONJUNCTION machine learning. model probability distribution USED-FOR discrete data. discrete data USED-FOR inverse problems. discrete data USED-FOR machine learning. Wasserstein distance FEATURE-OF model distribution. uniform probability distribution USED-FOR Wasserstein distance. convergence FEATURE-OF Lloyd - type algorithm. ambient space FEATURE-OF point cloud. point cloud USED-FOR algorithm. Poliak - Łojasiewicz inequality USED-FOR Wasserstein distance cost. Task is minimization problem. Method are Lloyd ’s algorithm, and gradient descent. OtherScientificTerm are Voronoi cells, Power cells, spurious critical points, error term, and discrete distribution. Metric is Wasserstein error. Generic are problem, and bounds. ","This paper studies the problem of minimizing the Wasserstein distance between two points in a point cloud. The authors consider the case where the point cloud is discrete and the data distribution is uniform. They show that under certain assumptions, they can converge to the optimal solution of the minimization problem under the assumption that the distribution of the data is uniform and that the error term is bounded by the Poliak-Łojasiewicz inequality. They also show that the convergence of the proposed algorithm is guaranteed under certain conditions. ","This paper studies the problem of minimizing the Wasserstein distance between two points in a point cloud. The authors consider the case where the point cloud is discrete and the data distribution is uniform. They show that under certain assumptions, they can converge to the optimal solution of the minimization problem under the assumption that the distribution of the data is uniform and that the error term is bounded by the Poliak-Łojasiewicz inequality. They also show that the convergence of the proposed algorithm is guaranteed under certain conditions. "
10021,SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"Convolution HYPONYM-OF feature transform. Convolution HYPONYM-OF neural networks. feature transform PART-OF neural networks. convolution layers CONJUNCTION self - attention blocks. self - attention blocks CONJUNCTION convolution layers. convolution layers PART-OF Transformer networks. dynamic transforms USED-FOR video understanding. correspondence relations USED-FOR representation. motion information HYPONYM-OF correspondence relations. self - attention HYPONYM-OF dynamic transforms. relational kernels CONJUNCTION relational contexts. relational contexts CONJUNCTION relational kernels. rich structures of spatio - temporal relations USED-FOR relational feature transform. relational kernels USED-FOR rich structures of spatio - temporal relations. relational self - attention ( RSA ) HYPONYM-OF relational feature transform. Diving48 CONJUNCTION FineGym. FineGym CONJUNCTION Diving48. Something - Something - V1&V2 CONJUNCTION Diving48. Diving48 CONJUNCTION Something - Something - V1&V2. RSA network COMPARE convolution and self - attention counterparts. convolution and self - attention counterparts COMPARE RSA network. motion - centric benchmarks USED-FOR video action recognition. Something - Something - V1&V2 HYPONYM-OF video action recognition. Diving48 HYPONYM-OF video action recognition. motion - centric benchmarks EVALUATE-FOR RSA network. FineGym HYPONYM-OF motion - centric benchmarks. Something - Something - V1&V2 HYPONYM-OF motion - centric benchmarks. Diving48 HYPONYM-OF motion - centric benchmarks. Method are deep learning, stationary convolution kernels, and dynamic feature transforms. ",This paper proposes a relational self-attention (RSA) model for video understanding. The proposed model is based on the relational kernel (RK) framework. The key idea of RSA is to use relational kernels to learn a relational representation of the input video. The authors show that RSA can be used to improve the performance of motion-based video recognition tasks. ,This paper proposes a relational self-attention (RSA) model for video understanding. The proposed model is based on the relational kernel (RK) framework. The key idea of RSA is to use relational kernels to learn a relational representation of the input video. The authors show that RSA can be used to improve the performance of motion-based video recognition tasks. 
10057,SP:2c2530069d5cab485629090243da464d107feadd,"mean field theory FEATURE-OF multilayer neural networks. mean field limit USED-FOR learning dynamics. infinite - width limit FEATURE-OF random fluctuation. large - width expansion USED-FOR random fluctuation. formulation USED-FOR stochastic dependency. fluctuation FEATURE-OF multilayer networks. system of dynamical equations USED-FOR limiting fluctuation distribution. second - order mean field limit HYPONYM-OF system of dynamical equations. stochasticity CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION stochasticity. nonlinear time evolution FEATURE-OF limiting fluctuation. cross - layer dependency CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION cross - layer dependency. cross - layer dependency FEATURE-OF stochasticity. large - width networks USED-FOR fluctuation. vanishing fluctuation FEATURE-OF output function. squared loss FEATURE-OF empirical risk minimization setting. empirical risk minimization setting FEATURE-OF shallow networks. loss function FEATURE-OF multilayer networks. squared loss FEATURE-OF shallow networks. OtherScientificTerm are infinite - width scaling, network depth, complex interaction, limit theorem, large - width regime, training trajectory, and global optimum. Material is multilayer case. Method are neuronal embedding framework, and gradient descent mean field training. Generic are it, and network. ","This paper studies the mean field theory of multilayer neural networks. The authors derive the limit theorem for the infinite-width limit of the learning dynamics of the network. They show that the limiting fluctuation distribution of the training trajectory is a system of dynamical equations. They also show that in the large-width regime, the limit is a second-order mean field limit. ","This paper studies the mean field theory of multilayer neural networks. The authors derive the limit theorem for the infinite-width limit of the learning dynamics of the network. They show that the limiting fluctuation distribution of the training trajectory is a system of dynamical equations. They also show that in the large-width regime, the limit is a second-order mean field limit. "
10093,SP:a3d927854d9d7fd39b8d05a79666810d585d5062,inductive biases USED-FOR predictive extrapolation. Hamiltonian / Lagrangian form USED-FOR structure. inductive biases USED-FOR Forecasting of time - series data. dissipative brackets PART-OF metriplectic dynamical systems. metriplectic dynamical systems USED-FOR parameterization of dissipative brackets. process USED-FOR generalized Casimirs. generalized Casimirs USED-FOR entropy. dynamics COMPARE penalty - based approaches. penalty - based approaches COMPARE dynamics. time - series data USED-FOR dynamical system. data - driven modeling USED-FOR physical systems. data - driven modeling CONJUNCTION machine learning ( ML ) tasks. machine learning ( ML ) tasks CONJUNCTION data - driven modeling. learnable dynamics FEATURE-OF dynamical system. physics - based structure USED-FOR architectures. minimal bias FEATURE-OF black - box model form. approaches USED-FOR structure preserving models of reversible dynamics. structure preserving models of reversible dynamics USED-FOR inductive bias. approaches USED-FOR inductive bias. algebraic structure of Hamiltonian / Lagrangian dynamics USED-FOR flow map. energy FEATURE-OF flow map. symplectic structure FEATURE-OF flow map. approaches USED-FOR reversible systems. entropy HYPONYM-OF generalized Casimirs. framework USED-FOR Physical systems. thermodynamic consistency FEATURE-OF mimetic properties. first and second laws of thermodynamics HYPONYM-OF mimetic properties. fluctuation dissipation theorem ( FDT ) USED-FOR closed stochastic systems. model USED-FOR metriplectic systems. algebraic structure FEATURE-OF system. first principles modeling USED-FOR system. system USED-FOR multiscale problems. time history USED-FOR multiscale problems. training strategy USED-FOR NODEs. metriplectic system USED-FOR time - series data. training strategy USED-FOR algebraic objects. internal entropy CONJUNCTION temperature. temperature CONJUNCTION internal entropy. non - observable states FEATURE-OF dissipative systems. internal entropy HYPONYM-OF non - observable states. temperature HYPONYM-OF non - observable states. null - spaces USED-FOR reversible and irreversible components of the dynamics. dissipative chaotic systems USED-FOR science and engineering problems. latent dimension FEATURE-OF,"This paper studies the problem of predictive extrapolation of time-series data from a metriplectic dynamical system. The authors propose a new framework for learning the dynamics of a metrizlectic system, which is based on the generalized Casimirs framework. The framework is motivated by the notion of dissipative brackets in the Hamiltonian/Lagrangian form of the system, and is able to learn the dynamics in a black-box model form. The proposed framework is evaluated on a number of time series data sets, and shows that the proposed framework outperforms the baselines in terms of predictive accuracy.","This paper studies the problem of predictive extrapolation of time-series data from a metriplectic dynamical system. The authors propose a new framework for learning the dynamics of a metrizlectic system, which is based on the generalized Casimirs framework. The framework is motivated by the notion of dissipative brackets in the Hamiltonian/Lagrangian form of the system, and is able to learn the dynamics in a black-box model form. The proposed framework is evaluated on a number of time series data sets, and shows that the proposed framework outperforms the baselines in terms of predictive accuracy."
10129,SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. robustness PART-OF Trustworthy AI. Fairness PART-OF Trustworthy AI. Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. sample selection - based algorithm USED-FOR fair and robust training. combinatorial optimization problem USED-FOR unbiased selection of samples. greedy algorithm USED-FOR optimization problem. algorithm COMPARE state - of - the - art technique. state - of - the - art technique COMPARE algorithm. fairness CONJUNCTION robustness. robustness CONJUNCTION fairness. robustness EVALUATE-FOR state - of - the - art technique. fairness EVALUATE-FOR state - of - the - art technique. robustness EVALUATE-FOR algorithm. fairness EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR state - of - the - art technique. fair and robust training baselines COMPARE algorithm. algorithm COMPARE fair and robust training baselines. sampling step USED-FOR batch selection. sampling step USED-FOR algorithm. clean data USED-FOR algorithm. Method are unbiased model, and training algorithm. OtherScientificTerm is data corruption. ",This paper proposes a sample selection-based algorithm for fair and robust training. The proposed algorithm is based on the combinatorial optimization problem. The authors show that the proposed algorithm outperforms the state-of-the-art in both fairness and robustness.,This paper proposes a sample selection-based algorithm for fair and robust training. The proposed algorithm is based on the combinatorial optimization problem. The authors show that the proposed algorithm outperforms the state-of-the-art in both fairness and robustness.
10165,SP:991127729bf067fe27fdd7ed360aab39e4df5921,"Neural network models USED-FOR hidden data biases. function space FEATURE-OF inductive biases. inductive biases USED-FOR models. periodic activation functions USED-FOR Bayesian neural networks. triangular wave CONJUNCTION periodic ReLU activation functions. periodic ReLU activation functions CONJUNCTION triangular wave. deep neural networks USED-FOR out - of - domain detection. periodic activation functions USED-FOR deep neural networks. in - domain data EVALUATE-FOR periodic activation functions. Generic is them. OtherScientificTerm are network weights, translation - invariant, stationary Gaussian process priors, sinusoidal ( Fourier ) activations, and perturbed inputs. ","This paper studies the inductive bias of Bayesian neural networks in the function space. In particular, the authors consider the case of periodic activation functions, which is a special case of the triangular wave and periodic ReLU activation functions. The authors show that these functions are invariant to translation-invariant, stationary Gaussian process priors, and can be used for out-of-domain detection. They also show that in-domain data can be perturbed by perturbed inputs.","This paper studies the inductive bias of Bayesian neural networks in the function space. In particular, the authors consider the case of periodic activation functions, which is a special case of the triangular wave and periodic ReLU activation functions. The authors show that these functions are invariant to translation-invariant, stationary Gaussian process priors, and can be used for out-of-domain detection. They also show that in-domain data can be perturbed by perturbed inputs."
10201,SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,user interaction CONJUNCTION complex dynamic systems. complex dynamic systems CONJUNCTION user interaction. complex dynamic systems FEATURE-OF programs. user interaction FEATURE-OF programs. mouse based games HYPONYM-OF complex dynamic systems. autonomous methods USED-FOR feedback. unit tests USED-FOR interactive programs. feedback USED-FOR interactive programs. classifying Markov Decision Processes ( MDPs ) USED-FOR feedback. dynamics and reward model USED-FOR MDP. agent USED-FOR differential trajectories. agent CONJUNCTION autoregressive model. autoregressive model CONJUNCTION agent. differential trajectories PART-OF MDP. agent USED-FOR cooperative objective. autoregressive model USED-FOR cooperative objective. method USED-FOR automatic feedback system. automatic feedback system USED-FOR interactive code assignments. anonymized student submissions FEATURE-OF dataset. Task is coding education. Method is classifier. Material is hand - coded bug labels. ,This paper proposes a new method for automatic feedback for interactive programs. The proposed method is based on the idea of classifying Markov Decision Processes (MDPs) and learning an autoregressive model that is able to learn differential trajectories of the dynamics and reward model. The method is evaluated on a dataset of interactive code assignments and is shown to outperform the baselines. ,This paper proposes a new method for automatic feedback for interactive programs. The proposed method is based on the idea of classifying Markov Decision Processes (MDPs) and learning an autoregressive model that is able to learn differential trajectories of the dynamics and reward model. The method is evaluated on a dataset of interactive code assignments and is shown to outperform the baselines. 
10237,SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"superpixels CONJUNCTION attentions. attentions CONJUNCTION superpixels. attentions CONJUNCTION saliency maps. saliency maps CONJUNCTION attentions. superpixels USED-FOR low - level input features. high - level latent object features USED-FOR approach. disentangled representation USED-FOR high - level latent object features. identifiable latent representation USED-FOR independent factors of variation. mimic tree USED-FOR DRL action values. identifiable latent representation USED-FOR Represent And Mimic ( RAMi ) framework. fidelity EVALUATE-FOR mimic tree. Minimum Description Length ( MDL ) objective EVALUATE-FOR mimic tree. Information Bottleneck ( IB ) principle USED-FOR Minimum Description Length ( MDL ) objective. mimic tree COMPARE baseline models. baseline models COMPARE mimic tree. decision rules CONJUNCTION causal impacts. causal impacts CONJUNCTION decision rules. latent traversals CONJUNCTION decision rules. decision rules CONJUNCTION latent traversals. causal impacts CONJUNCTION human evaluation results. human evaluation results CONJUNCTION causal impacts. latent traversals FEATURE-OF mimic tree. decision rules PART-OF mimic tree. Task is Interpreting Deep Reinforcement Learning ( DRL ) models. OtherScientificTerm are transparency regulations, latent features, IB - optimal mimic tree, and nodes. Method is DRL model. ","This paper proposes Represent And Mimic (RAMi), a method for learning representations of DRL action values that are disentangled from low-level input features. The method is based on the Information Bottleneck (IB) principle, which is used to learn an IB-optimal mimic tree that is able to capture the influence of different factors of variation in DRL actions. The paper shows that the proposed method outperforms baselines in terms of fidelity and performance. ","This paper proposes Represent And Mimic (RAMi), a method for learning representations of DRL action values that are disentangled from low-level input features. The method is based on the Information Bottleneck (IB) principle, which is used to learn an IB-optimal mimic tree that is able to capture the influence of different factors of variation in DRL actions. The paper shows that the proposed method outperforms baselines in terms of fidelity and performance. "
10273,SP:84560de78af979354fff83d1370d8675c1e9191f,"weather forecasts CONJUNCTION political prognostications. political prognostications CONJUNCTION weather forecasts. political prognostications CONJUNCTION financial projections. financial projections CONJUNCTION political prognostications. Bayesian framework USED-FOR structure of dynamic predictions. GLIM HYPONYM-OF Bayesian framework. Gaussian latent information martingale HYPONYM-OF Bayesian framework. historical data USED-FOR latent process of information flow. martingale structure CONJUNCTION volatility. volatility CONJUNCTION martingale structure. approach USED-FOR probability paths. volatility HYPONYM-OF probability paths. martingale structure HYPONYM-OF probability paths. GLIM COMPARE baseline methods. baseline methods COMPARE GLIM. metrics USED-FOR estimated posterior probability path distributions. estimated posterior probability path distributions EVALUATE-FOR GLIM. estimated posterior probability path distributions EVALUATE-FOR baseline methods. metrics EVALUATE-FOR baseline methods. metrics EVALUATE-FOR GLIM. Task are probability estimates of future binary outcomes, and time series analysis. Generic are first, second, former, and trajectories. OtherScientificTerm is dynamic structure of predictions. ","This paper proposes a Bayesian framework for estimating the probability distribution of future binary outcomes. The proposed method is based on the Gaussian latent information martingale (GLIM) framework, which is used to model the latent process of information flow in the latent space. The authors propose a new method to estimate the distribution of probability paths in a time series, and show that the proposed method outperforms the baselines on a number of metrics. ","This paper proposes a Bayesian framework for estimating the probability distribution of future binary outcomes. The proposed method is based on the Gaussian latent information martingale (GLIM) framework, which is used to model the latent process of information flow in the latent space. The authors propose a new method to estimate the distribution of probability paths in a time series, and show that the proposed method outperforms the baselines on a number of metrics. "
10309,SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"fixed confidence FEATURE-OF active pure exploration. generic stochastic bandit environments USED-FOR active pure exploration. instance - specific lower bounds FEATURE-OF expected sample complexity. instance - specific lower bounds USED-FOR problem. proportions USED-FOR optimization problem. tractability FEATURE-OF optimization problem. algorithm USED-FOR pure exploration problems. Frank - Wolfe algorithm USED-FOR lower - bound optimization problem. Frank - Wolfe algorithm USED-FOR it. FWS USED-FOR pure exploration tasks. arm identification HYPONYM-OF pure exploration tasks. FWS COMPARE state - of - art algorithms. state - of - art algorithms COMPARE FWS. OtherScientificTerm are sampling budget, structural properties of the environment, and lower bounds. Method are Oracle algorithm, and learning algorithms. Metric is sample complexity. ","This paper studies the problem of active pure exploration in stochastic bandit environments, where the goal is to find an arm that can be used for arm identification. The main contribution of this paper is the Frank-Wolfe algorithm (FWS), which is a variant of the Oracle algorithm. FWS is based on the idea of instance-specific lower bounds for the optimization problem. The authors show that the FWS algorithm is tractable in the presence of structural properties of the environment. They also provide a lower bound for the sample complexity of FWS. ","This paper studies the problem of active pure exploration in stochastic bandit environments, where the goal is to find an arm that can be used for arm identification. The main contribution of this paper is the Frank-Wolfe algorithm (FWS), which is a variant of the Oracle algorithm. FWS is based on the idea of instance-specific lower bounds for the optimization problem. The authors show that the FWS algorithm is tractable in the presence of structural properties of the environment. They also provide a lower bound for the sample complexity of FWS. "
10345,SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"sequences CONJUNCTION trees. trees CONJUNCTION sequences. trees CONJUNCTION graphs. graphs CONJUNCTION trees. graphs HYPONYM-OF optimizing combinatorial spaces. sequences HYPONYM-OF optimizing combinatorial spaces. trees HYPONYM-OF optimizing combinatorial spaces. black - box function evaluations USED-FOR combinatorial spaces. Bayesian optimization ( BO ) USED-FOR problems. framework USED-FOR problems. BO approach USED-FOR combinatorial spaces. deep generative models ( DGMs ) USED-FOR latent representation of structures. discrete structure USED-FOR function evaluation. latent space USED-FOR surrogate model. DGM USED-FOR surrogate model. LADDER HYPONYM-OF approach. latent space representation USED-FOR surrogate modeling. structural information PART-OF decoded structures. structural information PART-OF structure - coupled kernel. real - world benchmarks EVALUATE-FOR LADDER. LADDER COMPARE BO. BO COMPARE LADDER. LADDER COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LADDER. LADDER COMPARE latent space method. latent space method COMPARE LADDER. real - world benchmarks EVALUATE-FOR BO. BO COMPARE latent space method. latent space method COMPARE BO. real - world benchmarks EVALUATE-FOR state - of - the - art methods. Task is drug design. OtherScientificTerm are physical lab experiments, continuous spaces, continuous space, inductive bias, and black - box function. ",This paper proposes a new method for black-box optimization for combinatorial optimization. The method is based on deep generative models (DGMs) and uses the structure-coupled kernel to learn the latent representation of the function evaluation. The authors show that the proposed method outperforms existing methods on a number of real-world benchmarks. ,This paper proposes a new method for black-box optimization for combinatorial optimization. The method is based on deep generative models (DGMs) and uses the structure-coupled kernel to learn the latent representation of the function evaluation. The authors show that the proposed method outperforms existing methods on a number of real-world benchmarks. 
10381,SP:37adabdc6615c5199a481553c8ccc06d57363614,"representation of state - action value functions USED-FOR regret minimization. constant regret FEATURE-OF MDP. linear reward function FEATURE-OF MDP. low - rank MDPs CONJUNCTION zero inherent Bellman error. zero inherent Bellman error CONJUNCTION low - rank MDPs. condition USED-FOR problems. LSVI - UCB CONJUNCTION ELEANOR. ELEANOR CONJUNCTION LSVI - UCB. constant regret bound USED-FOR optimistic algorithms. LSVI - UCB HYPONYM-OF optimistic algorithms. ELEANOR HYPONYM-OF optimistic algorithms. algorithm USED-FOR representation selection. representations CONJUNCTION them. them CONJUNCTION representations. constant regret EVALUATE-FOR it. representations USED-FOR it. OtherScientificTerm are linear structure, universally spanning optimal features ( UNISOFT ), Bellman closure assumption, and UNISOFT condition. Generic is representation. ","This paper studies the problem of regret minimization for linear MDPs with linear reward function. The authors consider the case where the reward function is linear in the form of a linear function, and the goal is to minimize the regret of the MDP under the Bellman closure assumption. In particular, the authors show that under certain conditions, the regret bound of the optimal MDP can be bounded by a constant regret bound. They also show that this bound holds for low-rank MDP and zero inherent Bellman error. ","This paper studies the problem of regret minimization for linear MDPs with linear reward function. The authors consider the case where the reward function is linear in the form of a linear function, and the goal is to minimize the regret of the MDP under the Bellman closure assumption. In particular, the authors show that under certain conditions, the regret bound of the optimal MDP can be bounded by a constant regret bound. They also show that this bound holds for low-rank MDP and zero inherent Bellman error. "
10417,SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"energy conservation FEATURE-OF dynamics. Lagrangian or Hamiltonian dynamics PART-OF neural network architecture. differential equations USED-FOR approaches. legged robots CONJUNCTION robotic manipulators. robotic manipulators CONJUNCTION legged robots. contacts CONJUNCTION collisions. collisions CONJUNCTION contacts. contacts PART-OF physical systems. collisions PART-OF physical systems. robotic manipulators HYPONYM-OF physical systems. legged robots HYPONYM-OF physical systems. differentiable contact model USED-FOR contact mechanics. frictionless / frictional CONJUNCTION elastic / inelastic. elastic / inelastic CONJUNCTION frictionless / frictional. frictionless / frictional HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF contact mechanics. frictionless / frictional HYPONYM-OF contact mechanics. model USED-FOR inequality constraints. contact model USED-FOR Lagrangian and Hamiltonian neural networks. simultaneous learning of contact and system properties USED-FOR contact model. coefficients of restitution FEATURE-OF 2D and 3D physical systems. 2D and 3D physical systems EVALUATE-FOR framework. differentiable physics simulator USED-FOR downstream gradient - based optimization tasks. dynamics USED-FOR differentiable physics simulator. dynamics USED-FOR downstream gradient - based optimization tasks. planning and control HYPONYM-OF downstream gradient - based optimization tasks. OtherScientificTerm are inductive bias, and joint angles. ","This paper proposes a differentiable contact model for learning the dynamics of physical systems. The proposed method is based on the Lagrangian and Hamiltonian neural networks. The authors show that the proposed method can be used to learn the dynamics for both frictionless and inelastic contact models. The method is evaluated on a number of simulation tasks, including planning and control. ","This paper proposes a differentiable contact model for learning the dynamics of physical systems. The proposed method is based on the Lagrangian and Hamiltonian neural networks. The authors show that the proposed method can be used to learn the dynamics for both frictionless and inelastic contact models. The method is evaluated on a number of simulation tasks, including planning and control. "
10453,SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"Lipschitz constant USED-FOR parameter trajectory. 1st layer bias FEATURE-OF NNs. bounded complexity EVALUATE-FOR NNs. Task is Benevolent Training Hypothesis ( BTH ). Metric is complexity. Method are deep neural network ( NN ), and stochastic training procedure. OtherScientificTerm are training dynamics, BTH, NN ’s Lipschitz constant, input space, Dropout, and trainingand datadependent generalization bound. ","This paper studies the Benevolent Training Hypothesis (BTH) for deep neural networks. The authors show that the Lipschitz constant of a deep neural network (NN) is bounded in the input space, and that the training dynamics of a stochastic training procedure (e.g., Dropout) can be viewed as a function of the parameter trajectory of the NN. They also provide a generalization bound for the BTH.","This paper studies the Benevolent Training Hypothesis (BTH) for deep neural networks. The authors show that the Lipschitz constant of a deep neural network (NN) is bounded in the input space, and that the training dynamics of a stochastic training procedure (e.g., Dropout) can be viewed as a function of the parameter trajectory of the NN. They also provide a generalization bound for the BTH."
10489,SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"operation USED-FOR distribution. Forster transform HYPONYM-OF operation. disjoint mixture of few distributions USED-FOR distribution. polynomial - time algorithm USED-FOR distribution - independent PAC learning of halfspaces. distribution - independent PAC learning of halfspaces PART-OF Massart noise model. polynomial sample complexity FEATURE-OF polynomial - time algorithm. algorithms USED-FOR learning problem. sample complexity EVALUATE-FOR algorithms. OtherScientificTerm are anticoncentration properties, and bit complexity. ",This paper studies the problem of distribution-independent PAC learning of half-spaces. The authors propose a polynomial-time algorithm for learning the distribution of halfspaces in the Massart noise model. They show that the sample complexity of the proposed algorithm is polynomially smaller than the bit complexity of existing algorithms. They also provide a theoretical analysis of the anticoncentration properties of the algorithm.,This paper studies the problem of distribution-independent PAC learning of half-spaces. The authors propose a polynomial-time algorithm for learning the distribution of halfspaces in the Massart noise model. They show that the sample complexity of the proposed algorithm is polynomially smaller than the bit complexity of existing algorithms. They also provide a theoretical analysis of the anticoncentration properties of the algorithm.
10525,SP:e5229305af00067ae2dbabd903e585964aec8928,"models USED-FOR graph - based learning tasks. Graph neural networks USED-FOR graph - based learning tasks. Graph neural networks HYPONYM-OF models. adversarial attacks USED-FOR graph - level classification. biochemistry and social network analysis HYPONYM-OF real - life applications. Bayesian optimisation - based attack method USED-FOR graph classification models. graph properties CONJUNCTION constraints. constraints CONJUNCTION graph properties. constraints CONJUNCTION modes of attack. modes of attack CONJUNCTION constraints. graph properties FEATURE-OF graph classification tasks. graph classification tasks EVALUATE-FOR method. adversarial robustness EVALUATE-FOR graph classification models. Task is node - level classification tasks. OtherScientificTerm are unrealistic setups, perturbation, and adversarial samples. ","This paper proposes a Bayesian optimisation-based attack method to improve the robustness of graph-level classification models against adversarial attacks. The proposed method is based on Bayesian optimization, and is able to exploit the fact that adversarial samples can be perturbed in different ways. The method is evaluated on several graph classification tasks and shows that the proposed method outperforms baselines. ","This paper proposes a Bayesian optimisation-based attack method to improve the robustness of graph-level classification models against adversarial attacks. The proposed method is based on Bayesian optimization, and is able to exploit the fact that adversarial samples can be perturbed in different ways. The method is evaluated on several graph classification tasks and shows that the proposed method outperforms baselines. "
10561,SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"distribution shifts FEATURE-OF Machine learning models. adaptation USED-FOR label distribution shift. adaptation USED-FOR online setting. online learning USED-FOR online label shift adaptation. Follow The Leader ( FTL ) CONJUNCTION Online Gradient Descent ( OGD ). Online Gradient Descent ( OGD ) CONJUNCTION Follow The Leader ( FTL ). Online Gradient Descent ( OGD ) USED-FOR adaptation algorithms. online learning techniques USED-FOR adaptation algorithms. Online Gradient Descent ( OGD ) HYPONYM-OF online learning techniques. Follow The Leader ( FTL ) HYPONYM-OF online learning techniques. OGD USED-FOR label shift scenarios. OtherScientificTerm are test - time label distribution, regret bounds, and simulated and real world label distribution shifts. Generic is model. Task is estimation of the expected test loss. ",This paper studies the problem of online label shift adaptation in the context of online learning. The authors consider the case where the test-time label distribution of a model is shifted during training. They show that the expected test loss of an online learning algorithm is bounded by the regret bound. They also show that online learning algorithms can be used to adapt to label shift in the online setting. ,This paper studies the problem of online label shift adaptation in the context of online learning. The authors consider the case where the test-time label distribution of a model is shifted during training. They show that the expected test loss of an online learning algorithm is bounded by the regret bound. They also show that online learning algorithms can be used to adapt to label shift in the online setting. 
10597,SP:806515ae07fb1c9d02773592005d53d4158ef102,distribution FEATURE-OF detection and localization of gradual changes. time - ordered observations USED-FOR distribution. time - ordered observations USED-FOR detection and localization of gradual changes. discontinuity jump in distribution USED-FOR abrupt setting. method USED-FOR detecting and localizing gradual changes. features FEATURE-OF distribution. prior knowledge FEATURE-OF distribution. prior knowledge FEATURE-OF features. detection CONJUNCTION localization. localization CONJUNCTION detection. method USED-FOR detection. method USED-FOR localization. Method is data generating model. ,"This paper proposes a method for detecting and localization of gradual changes in the data generating model. The proposed method is based on the idea that the distribution of time-ordered observations can be used to detect and localize gradual changes. The method is evaluated on a variety of datasets, including MNIST, CIFAR-10, and ImageNet.","This paper proposes a method for detecting and localization of gradual changes in the data generating model. The proposed method is based on the idea that the distribution of time-ordered observations can be used to detect and localize gradual changes. The method is evaluated on a variety of datasets, including MNIST, CIFAR-10, and ImageNet."
10633,SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"brain USED-FOR blind source separation ( BSS ) problems. linear BSS problems PART-OF signal processing. Independent Component Analysis ( ICA ) USED-FOR linear BSS problems. neural architecture CONJUNCTION synaptic learning rules. synaptic learning rules CONJUNCTION neural architecture. objective function USED-FOR biologically plausible NN. objective function USED-FOR ICA. neural architecture PART-OF biologically plausible NN. synaptic learning rules PART-OF biologically plausible NN. synaptic plasticity USED-FOR algorithm. extracellular calcium CONJUNCTION local field potential. local field potential CONJUNCTION extracellular calcium. local field potential CONJUNCTION nitric oxide. nitric oxide CONJUNCTION local field potential. neuromodulators CONJUNCTION extracellular calcium. extracellular calcium CONJUNCTION neuromodulators. OtherScientificTerm are biological circuit, biophysical variables, and synapse. Method are ICA neural network ( NN ), and NN. Task is synaptic weight update. ",This paper studies the problem of blind source separation (BSS) in the biological circuit. The authors propose a neural network architecture that is biologically plausible for linear BSS problems. The proposed method is based on the ICA neural network (NCA) framework. The main contribution of the paper is to propose a biologically plausible NN that can be used to solve the BSS problem. The method is evaluated on a number of tasks and shows that the proposed method outperforms the baselines.,This paper studies the problem of blind source separation (BSS) in the biological circuit. The authors propose a neural network architecture that is biologically plausible for linear BSS problems. The proposed method is based on the ICA neural network (NCA) framework. The main contribution of the paper is to propose a biologically plausible NN that can be used to solve the BSS problem. The method is evaluated on a number of tasks and shows that the proposed method outperforms the baselines.
10669,SP:22f8b517a3df65144412938f5891c463d7bae0ab,"neural activity USED-FOR task - related behavior. Recurrent Neural Networks ( RNNs ) USED-FOR neural activity. Recurrent Neural Networks ( RNNs ) USED-FOR task - related behavior. neuroscience CONJUNCTION machine learning. machine learning CONJUNCTION neuroscience. space of solutions FEATURE-OF task. RNNs COMPARE neural data. neural data COMPARE RNNs. space of solutions USED-FOR tasks. two - neuron network USED-FOR task. discrete dynamical regimes USED-FOR diversity. Delayed discrimination CONJUNCTION Interval discrimination. Interval discrimination CONJUNCTION Delayed discrimination. Interval discrimination CONJUNCTION Time reproduction. Time reproduction CONJUNCTION Interval discrimination. Delayed discrimination HYPONYM-OF neuroscience - inspired tasks. Time reproduction HYPONYM-OF neuroscience - inspired tasks. Interval discrimination HYPONYM-OF neuroscience - inspired tasks. neural activity FEATURE-OF networks. extrapolation patterns USED-FOR dynamical objects. tool USED-FOR reduced dynamics of networks. compact directed graph USED-FOR tool. compact directed graph USED-FOR reduced dynamics of networks. Machine learning CONJUNCTION Neuroscience. Neuroscience CONJUNCTION Machine learning. Method is machine learning algorithms. OtherScientificTerm are underspecification, hidden structure, and neural features. Generic is representation. ","This paper studies the problem of learning representations for discrete dynamical regimes. The authors propose a new method for learning representations of dynamical systems. The proposed method is based on a two-neuron RNN, which is trained to learn representations of the dynamical system in discrete regimes. They show that the proposed method outperforms existing methods on a number of tasks, including delayed discrimination, Interval discrimination, and Time Reproduction. ","This paper studies the problem of learning representations for discrete dynamical regimes. The authors propose a new method for learning representations of dynamical systems. The proposed method is based on a two-neuron RNN, which is trained to learn representations of the dynamical system in discrete regimes. They show that the proposed method outperforms existing methods on a number of tasks, including delayed discrimination, Interval discrimination, and Time Reproduction. "
10705,SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"Modeling distributions of covariates PART-OF unsupervised learning. density estimation PART-OF unsupervised learning. density estimation HYPONYM-OF Modeling distributions of covariates. arbitrary conditional density estimation USED-FOR conditional distribution. covariates FEATURE-OF conditional distribution. arbitrary conditional density estimation HYPONYM-OF problem. prior knowledge USED-FOR inference. unobserved features CONJUNCTION observed features xo. observed features xo CONJUNCTION unobserved features. ACE USED-FOR complexity. learning one - dimensional conditionals USED-FOR problem. energy function USED-FOR densities. approach COMPARE prior methods. prior methods COMPARE approach. arbitrary conditional likelihood estimation CONJUNCTION data imputation. data imputation CONJUNCTION arbitrary conditional likelihood estimation. ACE USED-FOR arbitrary conditional likelihood estimation. ACE USED-FOR data imputation. state - of - the - art USED-FOR arbitrary conditional likelihood estimation. state - of - the - art EVALUATE-FOR ACE. benchmarks EVALUATE-FOR state - of - the - art. benchmarks EVALUATE-FOR data imputation. benchmarks EVALUATE-FOR ACE. OtherScientificTerm are distributions of covariates, joint distribution, and one - dimensional conditionals. Generic is method. Method is Arbitrary Conditioning with Energy ( ACE ). ","This paper proposes a new method for learning one-dimensional conditional distributions of covariates. The proposed method is based on the notion of Arbitrary Conditioning with Energy (ACE), which is an extension of the previous work on conditional density estimation (CDE). The main idea of ACE is to learn a conditional distribution over the covariates of the joint distribution, and then use the energy function to estimate the conditional distribution. The authors show that ACE is able to achieve state-of-the-art performance on a number of benchmarks.","This paper proposes a new method for learning one-dimensional conditional distributions of covariates. The proposed method is based on the notion of Arbitrary Conditioning with Energy (ACE), which is an extension of the previous work on conditional density estimation (CDE). The main idea of ACE is to learn a conditional distribution over the covariates of the joint distribution, and then use the energy function to estimate the conditional distribution. The authors show that ACE is able to achieve state-of-the-art performance on a number of benchmarks."
10741,SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"MSE or L1 loss function USED-FOR low - level vision. single image super - resolution ( SISR ) HYPONYM-OF low - level vision. texture and edge areas COMPARE smooth areas. smooth areas COMPARE texture and edge areas. smooth areas PART-OF photographic images. adaptive weighted loss USED-FOR deep networks. adaptive weighted loss USED-FOR SISR. adaptive weighted loss USED-FOR situations. SISR USED-FOR deep networks. textured and edge pixels HYPONYM-OF situations. variance estimation USED-FOR SISR solutions. sparsity prior USED-FOR regularizing SISR solutions. uncertainty estimation USED-FOR regularizing SISR solutions. visual quality EVALUATE-FOR SISR. uncertainty - driven loss COMPARE MSE or L1 loss. MSE or L1 loss COMPARE uncertainty - driven loss. uncertainty - driven loss COMPARE loss functions. loss functions COMPARE uncertainty - driven loss. SISR networks EVALUATE-FOR uncertainty - driven loss. computation EVALUATE-FOR loss functions. OtherScientificTerm are visual information, pixel - by - pixel basis, high - resolution image ( mean ), and uncertainty ( variance ). Task is spatial adaptation. Method is network architectures. ",This paper proposes a new loss function for single image super-resolution (SISR). The proposed loss is based on the variance estimation of the high-resolution image (mean) and the low-res image (variance). The variance estimation is used to regularize the SISR solutions. Experiments show that the proposed loss function outperforms the MSE or L1 loss.,This paper proposes a new loss function for single image super-resolution (SISR). The proposed loss is based on the variance estimation of the high-resolution image (mean) and the low-res image (variance). The variance estimation is used to regularize the SISR solutions. Experiments show that the proposed loss function outperforms the MSE or L1 loss.
10777,SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"PAC - Bayesian generalization bounds USED-FOR adversarial robustness. PACBayesian framework USED-FOR averaged risk. perturbations FEATURE-OF averaged risk. majority votes FEATURE-OF perturbations. robust model USED-FOR attacks. adversarial attacks HYPONYM-OF attacks. Generic is model. Method are worst - case analysis, theoretically founded analysis, and PAC - Bayesian framework. Task is learning phase. ","This paper studies the PAC-Bayesian generalization bounds for adversarial robustness. The authors consider the worst-case analysis of adversarial attacks, which is a well-studied problem in the literature. The main contribution of this paper is to provide a generalization bound for PACBayesian robustness under the assumption that the average risk of the adversarial perturbations is a function of the number of majority votes. The paper also provides a theoretical analysis of the generalization properties of PACBayes. ","This paper studies the PAC-Bayesian generalization bounds for adversarial robustness. The authors consider the worst-case analysis of adversarial attacks, which is a well-studied problem in the literature. The main contribution of this paper is to provide a generalization bound for PACBayesian robustness under the assumption that the average risk of the adversarial perturbations is a function of the number of majority votes. The paper also provides a theoretical analysis of the generalization properties of PACBayes. "
10813,SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,Logical reasoning USED-FOR querying mechanism. large and incomplete databases USED-FOR querying mechanism. Knowledge Graphs ( KGs ) USED-FOR Logical reasoning. spatial geometries USED-FOR query representations. spatial geometries USED-FOR approaches. boxes HYPONYM-OF spatial geometries. transformation tricks USED-FOR unions. Probabilistic Entity Representation Model ( PERM ) USED-FOR entities. semantic position CONJUNCTION smooth decision boundary. smooth decision boundary CONJUNCTION semantic position. Multivariate Gaussian density USED-FOR semantic position. Multivariate Gaussian density USED-FOR smooth decision boundary. mean and covariance parameters USED-FOR semantic position. Multivariate Gaussian density USED-FOR Probabilistic Entity Representation Model ( PERM ). mean and covariance parameters FEATURE-OF Multivariate Gaussian density. Multivariate Gaussian density USED-FOR entities. intersection CONJUNCTION union. union CONJUNCTION intersection. projection CONJUNCTION intersection. intersection CONJUNCTION projection. projection HYPONYM-OF closed logical operations. union HYPONYM-OF closed logical operations. intersection HYPONYM-OF closed logical operations. end - to - end objective function USED-FOR union. end - to - end objective function USED-FOR closed logical operations. PERM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PERM. logical query reasoning problem EVALUATE-FOR PERM. logical query reasoning problem EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR public benchmark KG datasets. public benchmark KG datasets EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR state - of - the - art methods. public benchmark KG datasets EVALUATE-FOR PERM. evaluation metrics EVALUATE-FOR PERM. work COMPARE methods. methods COMPARE work. F1 EVALUATE-FOR methods. F1 EVALUATE-FOR work. COVID-19 drugrepurposing case study EVALUATE-FOR PERM ’s competence. low - dimensional visualization of the Gaussian representations USED-FOR query answering process. Task is logical operations of projection and intersection. OtherScientific,"This paper proposes a probabilistic entity representation model (PERM) for knowledge graph querying. PERM is based on the Probabilistic Entity Representation Model (PEM) framework, which is used to represent entities in a low-dimensional space. The proposed method is evaluated on a number of public benchmark KG datasets and is shown to outperform state-of-the-art methods.","This paper proposes a probabilistic entity representation model (PERM) for knowledge graph querying. PERM is based on the Probabilistic Entity Representation Model (PEM) framework, which is used to represent entities in a low-dimensional space. The proposed method is evaluated on a number of public benchmark KG datasets and is shown to outperform state-of-the-art methods."
10849,SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"memory scaling CONJUNCTION gradient degradation issues. gradient degradation issues CONJUNCTION memory scaling. Gradient - based hyperparameter optimization USED-FOR few - shot meta - learning. algorithm USED-FOR memory scaling issues. forward - mode differentiation USED-FOR memory scaling issues. noise reduction properties EVALUATE-FOR algorithm. theoretical guarantees FEATURE-OF algorithm. noise reduction properties FEATURE-OF theoretical guarantees. greedy gradientbased alternatives COMPARE black - box methods. black - box methods COMPARE greedy gradientbased alternatives. hyperparameter search ranges FEATURE-OF CIFAR-10. Generic is tasks. OtherScientificTerm are hyperparameters, and greediness. Method is unrolled optimization. ",This paper studies the problem of hyperparameter optimization for few-shot meta-learning. The authors propose an unrolled optimization method for hyperparameters. The main contribution of the paper is to provide theoretical guarantees on the noise reduction properties of the proposed algorithm. The paper also provides a theoretical analysis of the performance of the algorithm. ,This paper studies the problem of hyperparameter optimization for few-shot meta-learning. The authors propose an unrolled optimization method for hyperparameters. The main contribution of the paper is to provide theoretical guarantees on the noise reduction properties of the proposed algorithm. The paper also provides a theoretical analysis of the performance of the algorithm. 
10885,SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"systems PART-OF Human reasoning. Neural sequence models USED-FOR structured tasks. neural sequence model USED-FOR candidate generations. symbolic reasoning module USED-FOR logical consistency. neural System 1 CONJUNCTION logical System 2. logical System 2 CONJUNCTION neural System 1. neural inference USED-FOR neural System 1. neural inference USED-FOR approach. story generation CONJUNCTION grounded instruction - following. grounded instruction - following CONJUNCTION story generation. accuracy EVALUATE-FOR neurally - based generations. coherence CONJUNCTION accuracy. accuracy CONJUNCTION coherence. coherence EVALUATE-FOR neurally - based generations. approach USED-FOR neurally - based generations. coherence EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Method are System 1, System 1 - like sequence models, and System 2 - inspired logical reasoning. Generic is they. ",This paper proposes a neural sequence model based on symbolic reasoning module for learning a sequence of candidate generations. The proposed method is based on the idea of neural system 1 and logical system 2. The method is evaluated on two tasks: story generation and grounded instruction-following. The results show that the proposed method achieves state-of-the-art performance on both tasks.,This paper proposes a neural sequence model based on symbolic reasoning module for learning a sequence of candidate generations. The proposed method is based on the idea of neural system 1 and logical system 2. The method is evaluated on two tasks: story generation and grounded instruction-following. The results show that the proposed method achieves state-of-the-art performance on both tasks.
10921,SP:d77d046095e4c8336c0c76ac48cb046923230753,"off - policy evaluation ( OPE ) USED-FOR continuous treatment settings. personalized dose - finding HYPONYM-OF continuous treatment settings. decision rule USED-FOR historical data. discrete treatment settings FEATURE-OF OPE. estimation method USED-FOR OPE. estimation method USED-FOR continuous treatments. deep jump learning USED-FOR estimation method. deep learning CONJUNCTION multiscale change point detection. multiscale change point detection CONJUNCTION deep learning. OPE methods USED-FOR continuous treatments. OPE methods USED-FOR discrete treatments. OtherScientificTerm are treatment decision rule, and treatment space. Generic is method. Method is deep discretization. Task is Warfarin Dosing. ",This paper proposes a new method for off-policy evaluation (OPE) for continuous treatment settings. The proposed method is based on deep discretization. The method is evaluated on the Warfarin Dosing task and shows promising results. ,This paper proposes a new method for off-policy evaluation (OPE) for continuous treatment settings. The proposed method is based on deep discretization. The method is evaluated on the Warfarin Dosing task and shows promising results. 
10957,SP:4d085e57286fdd36143108a002d16914222c239a,natural sciences CONJUNCTION engineering applications. engineering applications CONJUNCTION natural sciences. modeling framework USED-FOR inference in time - series data. Switching dynamical systems USED-FOR modeling framework. inference in time - series data USED-FOR engineering applications. inference in time - series data USED-FOR natural sciences. biology CONJUNCTION discrete - event systems. discrete - event systems CONJUNCTION biology. subordinated diffusion process FEATURE-OF Markov jump process. continuous time FEATURE-OF areas. Markov jump process USED-FOR model. biology HYPONYM-OF areas. discrete - event systems HYPONYM-OF areas. evolution equations USED-FOR prior and posterior marginal densities. Gaussian process approximation CONJUNCTION posterior inference. posterior inference CONJUNCTION Gaussian process approximation. posterior inference USED-FOR Markov jump processes. Gaussian process approximation USED-FOR diffusion level. posterior inference PART-OF continuous - time variational inference algorithm. Gaussian process approximation PART-OF continuous - time variational inference algorithm. path - wise Kullback - Leibler divergence USED-FOR Bayesian latent state estimates. variational expectation maximization USED-FOR point estimates of unknown system parameters. real - world examples EVALUATE-FOR algorithm. Material is time - series data. OtherScientificTerm is real axis. ,"This paper proposes a variational inference framework for inference in time-series data. The proposed method is based on the Markov jump process (MPS) model, which is an extension of the diffusion process. The authors show that the MPS model can be used to approximate the prior and posterior marginal densities of a discrete-event system, and that the posterior can be approximated by a Gaussian process approximation. They also show that their method can be applied to Bayesian latent state estimation. ","This paper proposes a variational inference framework for inference in time-series data. The proposed method is based on the Markov jump process (MPS) model, which is an extension of the diffusion process. The authors show that the MPS model can be used to approximate the prior and posterior marginal densities of a discrete-event system, and that the posterior can be approximated by a Gaussian process approximation. They also show that their method can be applied to Bayesian latent state estimation. "
10993,SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"A HYPONYM-OF linear mapping. compressed sensing CONJUNCTION phase retrieval. phase retrieval CONJUNCTION compressed sensing. model USED-FOR signal processing problems. nonlinear processing function USED-FOR model. phase retrieval HYPONYM-OF signal processing problems. compressed sensing HYPONYM-OF signal processing problems. spectrum of sensing matrices HYPONYM-OF sensing matrices. expectation propagation algorithm ( EP ) HYPONYM-OF recovery methods. spikiness FEATURE-OF spectrum. measure USED-FOR EP. EP USED-FOR recovery. spikiness of the spectrum USED-FOR EP recovery. Task are nonlinear inverse problem, phase - retrieval problems, and 1 - bit compressed sensing problems. Method are componentwise nonlinear transformation, and optimal sensing systems. OtherScientificTerm are f, spikier spectrums, and sub - Gaussian and orthogonal matrices. Generic is framework. ","This paper proposes an expectation propagation algorithm (EP) based on the spikiness of the spectrum of sensing matrices. The authors show that EP can be used to recover the spectral properties of a sensing matrix from a nonlinear inverse problem. They show that the EP recovers the spectral property of the sensing matrix f, which is a measure of the spiking of a spectrum. They also provide a theoretical analysis of EP recovery. ","This paper proposes an expectation propagation algorithm (EP) based on the spikiness of the spectrum of sensing matrices. The authors show that EP can be used to recover the spectral properties of a sensing matrix from a nonlinear inverse problem. They show that the EP recovers the spectral property of the sensing matrix f, which is a measure of the spiking of a spectrum. They also provide a theoretical analysis of EP recovery. "
11029,SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,auxiliary semantic information USED-FOR Generalized Zero - Shot Learning ( GZSL ). category attributes HYPONYM-OF auxiliary semantic information. cross - domain transferability CONJUNCTION category discriminability. category discriminability CONJUNCTION cross - domain transferability. category discriminability EVALUATE-FOR visual representations. cross - domain transferability EVALUATE-FOR visual representations. prototypes USED-FOR prototypical visual patterns. attribute prototypes USED-FOR DPPN. DPPN USED-FOR attribute - related local regions. attribute prototypes USED-FOR attribute - region correspondence. DPPN USED-FOR attribute - region correspondence. attribute prototypes USED-FOR DPPN. DPPN USED-FOR visual representations. semantic - visual alignment CONJUNCTION representation transferability. representation transferability CONJUNCTION semantic - visual alignment. attribute localization ability FEATURE-OF visual representations. DPPN USED-FOR visual representations. progressive attribute localization CONJUNCTION DPPN. DPPN CONJUNCTION progressive attribute localization. category prototypes USED-FOR DPPN. unifed framework USED-FOR visual representations. DPPN USED-FOR visual representations. unifed framework USED-FOR attribute and category prototypes. DPPN USED-FOR domain shift problem. DPPN USED-FOR GZSL. domain shift problem FEATURE-OF GZSL. Generic is approach. Method is Dual Progressive Prototype Network ( DPPN ). ,"This paper proposes a new method for zero-shot learning based on the Dual Progressive Prototype Network (DPPN) framework. DPPN is an unifed framework that combines attribute and category prototypes to improve the representation transferability and cross-domain transferability of GZSL. The proposed method is evaluated on a variety of tasks, including semantic-visual alignment, category discriminability, and attribute-region correspondence. The results show that the proposed method achieves state-of-the-art performance.","This paper proposes a new method for zero-shot learning based on the Dual Progressive Prototype Network (DPPN) framework. DPPN is an unifed framework that combines attribute and category prototypes to improve the representation transferability and cross-domain transferability of GZSL. The proposed method is evaluated on a variety of tasks, including semantic-visual alignment, category discriminability, and attribute-region correspondence. The results show that the proposed method achieves state-of-the-art performance."
11065,SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"Defocus blur HYPONYM-OF blur effects. blur effects PART-OF images. end - to - end deep learning approach USED-FOR removing defocus blur. all - in - focus image USED-FOR consequent vision tasks. end - to - end deep learning approach USED-FOR all - in - focus image. accuracy EVALUATE-FOR models. linear parametric form FEATURE-OF spatially variant defocus blur kernels. fixed - point iteration USED-FOR GKM - based deblurring. fixed - point iteration USED-FOR deep neural network. GKMNet HYPONYM-OF deep neural network. scale - recurrent attention module USED-FOR mixing coefficients. GKM USED-FOR defocus deblurring. mixing coefficients PART-OF GKM. lightweight scale - recurrent architecture CONJUNCTION scale - recurrent attention module. scale - recurrent attention module CONJUNCTION lightweight scale - recurrent architecture. mixing coefficients USED-FOR defocus deblurring. scale - recurrent attention module USED-FOR GKMNet. lightweight scale - recurrent architecture USED-FOR GKMNet. model complexity CONJUNCTION computational efficiency. computational efficiency CONJUNCTION model complexity. GKMNet COMPARE defocus deblurring methods. defocus deblurring methods COMPARE GKMNet. computational efficiency EVALUATE-FOR GKMNet. model complexity EVALUATE-FOR GKMNet. OtherScientificTerm are spatially variant amount, and defocus blur. ","This paper proposes a deep learning approach for removing defocus blur from an image. The proposed method is based on the GKM-based deblurring method, which is an end-to-end deep learning method. The authors show that the proposed method can achieve better performance than the state-of-the-art methods in terms of accuracy, model complexity, and computational efficiency. ","This paper proposes a deep learning approach for removing defocus blur from an image. The proposed method is based on the GKM-based deblurring method, which is an end-to-end deep learning method. The authors show that the proposed method can achieve better performance than the state-of-the-art methods in terms of accuracy, model complexity, and computational efficiency. "
11101,SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"models USED-FOR SSVRL. visual content PART-OF videos. RGB frames CONJUNCTION motion vectors. motion vectors CONJUNCTION RGB frames. motion vectors USED-FOR low - resolution optical flows. compressed videos USED-FOR motion vectors. supervision signals FEATURE-OF motion vectors. multi - instance InfoNCE loss USED-FOR cross guidance contrastive learning algorithm. downstream tasks EVALUATE-FOR MVCGC. MVCGC COMPARE competitors. competitors COMPARE MVCGC. Generic are methods, and method. OtherScientificTerm is mutual information. Metric is storage and computation efficiency. ",This paper proposes a cross guidance contrastive learning algorithm for SSVRL. The proposed method is based on the InfoNCE loss. The authors show that the proposed method outperforms the state-of-the-art methods on several downstream tasks.,This paper proposes a cross guidance contrastive learning algorithm for SSVRL. The proposed method is based on the InfoNCE loss. The authors show that the proposed method outperforms the state-of-the-art methods on several downstream tasks.
11137,SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"Bayesian treatment USED-FOR overconfidence. Bayesian treatment USED-FOR ReLU nets. overconfidence FEATURE-OF ReLU nets. features FEATURE-OF BNN. ReLU features USED-FOR Bayesian linear models. it USED-FOR BNNs. model COMPARE BNNs. BNNs COMPARE model. infinite ReLU features FEATURE-OF finite ReLU BNNs. GP USED-FOR finite ReLU BNNs. model USED-FOR GP posterior. it USED-FOR ReLU BNN. Method are ReLU Bayesian neural networks ( BNNs ), and Gaussian process ( GP ). OtherScientificTerm are infinite - width limit, and asymptotic overconfidence. ","This paper studies the problem of overconfidence in ReLU Bayesian neural networks (BNNs). In particular, the authors consider the infinite-width limit of ReLU BNNs and show that under certain assumptions, the overconfidence can be reduced to the asymptotic limit of the GP posterior. The authors then propose a Bayesian linear model to solve this problem.","This paper studies the problem of overconfidence in ReLU Bayesian neural networks (BNNs). In particular, the authors consider the infinite-width limit of ReLU BNNs and show that under certain assumptions, the overconfidence can be reduced to the asymptotic limit of the GP posterior. The authors then propose a Bayesian linear model to solve this problem."
11173,SP:e77276f61626e896f6a985296f1d832129242cdf,"tools USED-FOR finite - sample confidence bounds. LUCB CONJUNCTION Successive Elimination. Successive Elimination CONJUNCTION LUCB. tools USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR asymptotic variance. Successive Elimination USED-FOR best - arm - identification algorithms. LUCB USED-FOR best - arm - identification algorithms. bounds USED-FOR best - arm - identification algorithms. sample complexity EVALUATE-FOR upper bounds. upper bounds EVALUATE-FOR method. sample complexity EVALUATE-FOR method. OtherScientificTerm are data collection mechanism, and arm. Method is bestarm - identification bandit framework. Material is artificially generated data. ","This paper studies the problem of best-arm-identification bandit, where the goal is to identify the best arm from a given set of samples. The authors propose a new method to estimate the asymptotic variance of the nuisance function of the best-armed bandit. The method is based on LUCB and Successive Elimination, and the authors show that the upper bound of the variance can be derived for both the sample complexity and sample complexity of the bandit algorithm. ","This paper studies the problem of best-arm-identification bandit, where the goal is to identify the best arm from a given set of samples. The authors propose a new method to estimate the asymptotic variance of the nuisance function of the best-armed bandit. The method is based on LUCB and Successive Elimination, and the authors show that the upper bound of the variance can be derived for both the sample complexity and sample complexity of the bandit algorithm. "
11209,SP:471361588bfc6c6033631509d1e43e77fd9721ce,"scalability EVALUATE-FOR distributed learning. communication FEATURE-OF gradient. algorithm USED-FOR biased compression. variance FEATURE-OF stochastic gradient. moving average USED-FOR history gradients. moving average USED-FOR variance. compression error USED-FOR ErrorCompensatedX. asymptotic convergence rate EVALUATE-FOR ErrorCompensatedX. unified theoretical analysis framework USED-FOR variance reduced algorithms. Metric are Communication cost, communication cost, and convergence speed. Method are stochastic gradient descent, training without compression, and error compensation. ","This paper studies the variance reduction problem in stochastic gradient descent. The authors propose a new algorithm called ErrorCompensatedX, which reduces the variance of the gradient descent algorithm to the moving average of the history gradients. The algorithm is based on a unified theoretical analysis of variance reduced algorithms. The paper shows that the variance reduced algorithm converges to the asymptotic convergence rate. ","This paper studies the variance reduction problem in stochastic gradient descent. The authors propose a new algorithm called ErrorCompensatedX, which reduces the variance of the gradient descent algorithm to the moving average of the history gradients. The algorithm is based on a unified theoretical analysis of variance reduced algorithms. The paper shows that the variance reduced algorithm converges to the asymptotic convergence rate. "
11245,SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"graph USED-FOR model. local explainability CONJUNCTION global explainability. global explainability CONJUNCTION local explainability. performant paradigm USED-FOR multi - grained explainability. pre - training and fine - tuning idea USED-FOR explainer. pre - training and fine - tuning idea USED-FOR multi - grained explanations. explainer USED-FOR multi - grained explanations. synthetic and real - world datasets EVALUATE-FOR explainer. AUC EVALUATE-FOR baselines. explainer COMPARE baselines. baselines COMPARE explainer. explaining graph classification EVALUATE-FOR baselines. AUC EVALUATE-FOR explaining graph classification. synthetic and real - world datasets EVALUATE-FOR baselines. explaining graph classification EVALUATE-FOR explainer. AUC EVALUATE-FOR explainer. Method are graph neural network ( GNN ), explainers, pre - training phase, and fine - tuning phase. Metric is explainability. Generic is approaches. OtherScientificTerm are class - wise patterns, local context, and class - wise characteristics. ","This paper proposes a performant paradigm for explaining graph neural networks (GNNs). The authors propose a pre-training and fine-tuning approach to improve the explainability of GNNs. The proposed approach is based on the idea of explainer, which is an extension of the explainer framework. The authors show that the proposed method is able to achieve better explainability on synthetic and real-world datasets. ","This paper proposes a performant paradigm for explaining graph neural networks (GNNs). The authors propose a pre-training and fine-tuning approach to improve the explainability of GNNs. The proposed approach is based on the idea of explainer, which is an extension of the explainer framework. The authors show that the proposed method is able to achieve better explainability on synthetic and real-world datasets. "
11281,SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"subgraph USED-FOR methods. method USED-FOR counterfactual explanations. GNNs USED-FOR counterfactual explanations. GNNs USED-FOR common decision logic. common decision boundaries USED-FOR GNN. GNN USED-FOR they. common decision boundaries USED-FOR they. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are noise, human intuition, explanations, and edges. ",This paper proposes a new method for generating counterfactual explanations for counterfactually explained graphs. The method is based on the idea that the decision boundary of a GNN can be represented as a subgraph. The authors show that the subgraph can be decomposed into two parts: (1) common decision boundaries and (2) edges. The main contribution of the paper is that it shows that the common decision boundary can be expressed as a function of the number of edges in the sub-graph. ,This paper proposes a new method for generating counterfactual explanations for counterfactually explained graphs. The method is based on the idea that the decision boundary of a GNN can be represented as a subgraph. The authors show that the subgraph can be decomposed into two parts: (1) common decision boundaries and (2) edges. The main contribution of the paper is that it shows that the common decision boundary can be expressed as a function of the number of edges in the sub-graph. 
11317,SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"information bottleneck CONJUNCTION adversarial feedback. adversarial feedback CONJUNCTION information bottleneck. information bottleneck USED-FOR VoiceMixer. adversarial feedback USED-FOR VoiceMixer. self - supervised representation learning USED-FOR information bottleneck. self - supervision USED-FOR model. adversarial feedback USED-FOR discriminator. voice style FEATURE-OF generalization. content and style discriminator PART-OF discriminator. generalization EVALUATE-FOR model. self - supervision USED-FOR content and style discriminator. transfer EVALUATE-FOR model. content information USED-FOR audio quality. audio quality EVALUATE-FOR model. Task is voice conversion. Material is converted voice. OtherScientificTerm are converted speech containing source speech style, and source speech content. ",This paper proposes a self-supervised representation learning method for voice conversion. The method is based on the idea of self supervision and self-distribution. The authors propose to use adversarial feedback to improve the performance of the discriminator and content and style discriminator. The proposed method is evaluated on the task of voice conversion and is shown to outperform the baselines.,This paper proposes a self-supervised representation learning method for voice conversion. The method is based on the idea of self supervision and self-distribution. The authors propose to use adversarial feedback to improve the performance of the discriminator and content and style discriminator. The proposed method is evaluated on the task of voice conversion and is shown to outperform the baselines.
11353,SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"Siamese voxel - to - BEV tracker USED-FOR tracking. sparse 3D point clouds FEATURE-OF tracking. Siamese shape - aware feature learning network CONJUNCTION voxel - to - BEV target localization network. voxel - to - BEV target localization network CONJUNCTION Siamese shape - aware feature learning network. Siamese shape - aware feature learning network PART-OF it. voxel - to - BEV target localization network PART-OF it. Siamese shape - aware feature learning network USED-FOR discriminative features. Siamese shape - aware feature learning network USED-FOR 3D shape information. dense 3D shape USED-FOR shape information. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. voxelized point cloud USED-FOR dense BEV feature map. max pooling USED-FOR dense BEV feature map. max pooling USED-FOR voxelized point cloud. KITTI and nuScenes datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. Task is 3D object tracking in point clouds. Material is point clouds. OtherScientificTerm are dynamic environments, sparse point clouds, and template ’s feature. Method are template feature embedding, and voxel - toBEV target localization network. ","This paper proposes a Siamese voxel-to-BEV tracker for tracking in sparse 3D point clouds. The proposed method is based on the Siameses shape-aware feature learning network, which is trained to learn the discriminative features of the target localization network and the voxels. The method is evaluated on the KITTI and nuScenes datasets. ","This paper proposes a Siamese voxel-to-BEV tracker for tracking in sparse 3D point clouds. The proposed method is based on the Siameses shape-aware feature learning network, which is trained to learn the discriminative features of the target localization network and the voxels. The method is evaluated on the KITTI and nuScenes datasets. "
11389,SP:8b788c78680a54c453a04f4551436763ee57585e,Positional encoding USED-FOR attention - based deep model architectures. Transformer HYPONYM-OF attention - based deep model architectures. learnable Fourier features USED-FOR positional encoding method. multi - layer perceptron USED-FOR trainable encoding. learnable Fourier feature mapping USED-FOR trainable encoding. representation USED-FOR spatial multi - dimensional position. L2 distances CONJUNCTION positional relationships. positional relationships CONJUNCTION L2 distances. image FEATURE-OF pixel positions. image HYPONYM-OF spatial multi - dimensional position. pixel positions HYPONYM-OF spatial multi - dimensional position. learnable Fourier feature representation USED-FOR multi - dimensional positional encoding. learnable Fourier feature representation COMPARE methods. methods COMPARE learnable Fourier feature representation. faster convergence EVALUATE-FOR methods. accuracy EVALUATE-FOR learnable Fourier feature representation. accuracy EVALUATE-FOR methods. Method is Attentional mechanisms. ,"This paper proposes a multi-layer positional encoding method for attention-based deep model architectures. The proposed method is based on learnable Fourier feature mapping, which can be used to encode the spatial multi-dimensional position of pixels in an image. The authors show that the proposed method converges faster than the state-of-the-art methods in terms of accuracy and convergence rate. ","This paper proposes a multi-layer positional encoding method for attention-based deep model architectures. The proposed method is based on learnable Fourier feature mapping, which can be used to encode the spatial multi-dimensional position of pixels in an image. The authors show that the proposed method converges faster than the state-of-the-art methods in terms of accuracy and convergence rate. "
11425,SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"latent variables CONJUNCTION selection bias. selection bias CONJUNCTION latent variables. causal MAG FEATURE-OF system. observational data USED-FOR system. observational data USED-FOR causal MAG. Constraint - based methods USED-FOR problem. latter USED-FOR CI tests. computational complexity EVALUATE-FOR former. lower bound USED-FOR constraint - based method. lower bound USED-FOR CI tests. CI tests USED-FOR constraint - based method. upper bound CONJUNCTION approach. approach CONJUNCTION upper bound. approach COMPARE state of the art. state of the art COMPARE approach. synthetic and real - world structures EVALUATE-FOR state of the art. synthetic and real - world structures EVALUATE-FOR approach. Generic are methods, and technique. OtherScientificTerm are large graphs, completeness guarantees, structure, and conditional independence ( CI ) tests. Method is recursive constraint - based method. ",This paper proposes a recursive constraint-based method for conditional independence (CI) tests for large graphs. The authors show that the proposed method is computationally efficient and can be applied to both synthetic and real-world data. They also provide a lower bound for the computational complexity of the proposed approach. ,This paper proposes a recursive constraint-based method for conditional independence (CI) tests for large graphs. The authors show that the proposed method is computationally efficient and can be applied to both synthetic and real-world data. They also provide a lower bound for the computational complexity of the proposed approach. 
11461,SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,information parallelism USED-FOR online decision making problems. stochastic multi - arm bandit CONJUNCTION linear contextual bandit. linear contextual bandit CONJUNCTION stochastic multi - arm bandit. batch Thompson Sampling framework USED-FOR canonical online decision making problems. linear contextual bandit HYPONYM-OF canonical online decision making problems. stochastic multi - arm bandit HYPONYM-OF canonical online decision making problems. asymptotic ) regret bound EVALUATE-FOR batch Thompson Sampling policy. batch policy USED-FOR exploration - exploitation trade - off. batch policy USED-FOR exponential reduction. dynamic batch allocation COMPARE natural baselines. natural baselines COMPARE dynamic batch allocation. static batch allocations HYPONYM-OF natural baselines. ,"This paper studies the problem of online decision making in the context of Thompson Sampling. The authors propose a new algorithm for batch Thompson sampling, which is a variant of the Thompson sampling algorithm. The algorithm is based on the idea of exploration-exploitation trade-off, and the authors show that the proposed algorithm is able to achieve exponential reduction in the regret bound. The paper also shows that the algorithm can be used to solve the linear contextual bandit and stochastic multi-arm bandit problems.","This paper studies the problem of online decision making in the context of Thompson Sampling. The authors propose a new algorithm for batch Thompson sampling, which is a variant of the Thompson sampling algorithm. The algorithm is based on the idea of exploration-exploitation trade-off, and the authors show that the proposed algorithm is able to achieve exponential reduction in the regret bound. The paper also shows that the algorithm can be used to solve the linear contextual bandit and stochastic multi-arm bandit problems."
11497,SP:653a519e3c799c25e0d0b4240322642040b121a3,"multiple source DA CONJUNCTION domain generalization ( DG ) settings. domain generalization ( DG ) settings CONJUNCTION multiple source DA. upper - bounds USED-FOR domain - invariant representations. upper - bounds USED-FOR target general loss. Task is Domain adaptation ( DA ). Method is domain - invariant representation. Generic are representations, them, and theory. ","This paper studies the problem of domain adaptation (DA) in the context of multiple source DA and domain generalization (DG) settings. In particular, the authors consider the case where the target generalization loss is a function of the source domain and the target domain is a domain-invariant representation. The authors show that under certain assumptions on the source and target domain, they can derive upper bounds on the generalization error of the target loss. The upper bounds are based on the assumption that the target target loss is invariant to domain changes. ","This paper studies the problem of domain adaptation (DA) in the context of multiple source DA and domain generalization (DG) settings. In particular, the authors consider the case where the target generalization loss is a function of the source domain and the target domain is a domain-invariant representation. The authors show that under certain assumptions on the source and target domain, they can derive upper bounds on the generalization error of the target loss. The upper bounds are based on the assumption that the target target loss is invariant to domain changes. "
11533,SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"lightweight architectures USED-FOR SR methods. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. memory and computation resources USED-FOR model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. neural architecture search HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. L2 regularization USED-FOR sparsity. L2 regularization USED-FOR scale parameters. L2 regularization USED-FOR aligned structured sparsity learning ( ASSL ). weight normalization layer PART-OF aligned structured sparsity learning ( ASSL ). sparsity structure alignment penalty term USED-FOR norm of soft mask gram matrix. layers FEATURE-OF pruned filter locations. sparsity structure alignment penalty term USED-FOR pruned filter locations. aligned structured sparsity learning strategy USED-FOR image SR network. model size CONJUNCTION computation. computation CONJUNCTION model size. ASSLN HYPONYM-OF image SR network. ASSLN COMPARE methods. methods COMPARE ASSLN. OtherScientificTerm are moderate model size, and network parameters. Generic is state - of - the - art methods. Method is lightweight SR networks. ","This paper proposes an aligned structured sparsity learning (ASSL) method to reduce the size of SR networks. The proposed method is based on the L2 regularization, which is applied to the weight normalization layer. The authors also propose a sparsity structure alignment penalty term to penalize the pruned filter locations. Experiments show that the proposed method outperforms the state-of-the-art.","This paper proposes an aligned structured sparsity learning (ASSL) method to reduce the size of SR networks. The proposed method is based on the L2 regularization, which is applied to the weight normalization layer. The authors also propose a sparsity structure alignment penalty term to penalize the pruned filter locations. Experiments show that the proposed method outperforms the state-of-the-art."
11569,SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"exploration USED-FOR complex coordination problems. EMC HYPONYM-OF Episodic Multi - agent reinforcement learning. reward backpropagation USED-FOR centralized training. individual utility functions USED-FOR local execution. individual utility functions HYPONYM-OF induced "" individual Q - values. episodic memory USED-FOR policy training. episodic memory USED-FOR explored informative experience. intrinsic rewards USED-FOR coordinated exploration. intrinsic reward USED-FOR coordinated exploration. method COMPARE MARL baselines. MARL baselines COMPARE method. StarCraft II micromanagement benchmark FEATURE-OF tasks. didactic examples USED-FOR method. tasks EVALUATE-FOR MARL baselines. tasks EVALUATE-FOR method. StarCraft II micromanagement benchmark EVALUATE-FOR MARL baselines. Method is factorized MARL algorithms. OtherScientificTerm are embeddings of local actionobservation histories, and individual Q - value function. ","This paper proposes a method for episodic multi-agent reinforcement learning (MARL) based on episodic memory. The proposed method is based on the idea of EMC, which is an extension of the EMC framework for reinforcement learning. The key idea is to learn an intrinsic reward function for each agent, which can be used to guide exploration. The authors show that the proposed method outperforms existing MARL baselines on the StarCraft II micromanagement task.","This paper proposes a method for episodic multi-agent reinforcement learning (MARL) based on episodic memory. The proposed method is based on the idea of EMC, which is an extension of the EMC framework for reinforcement learning. The key idea is to learn an intrinsic reward function for each agent, which can be used to guide exploration. The authors show that the proposed method outperforms existing MARL baselines on the StarCraft II micromanagement task."
11605,SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"Gaussian covariates USED-FOR linear regression model. Statistical Query ( SQ ) lower bound USED-FOR problem. upper bounds USED-FOR task. SQ lower bound COMPARE algorithms. algorithms COMPARE SQ lower bound. Task is list - decodable linear regression. OtherScientificTerm are noise distribution, hypothesis vectors, and regression vector. ","This paper studies the problem of list-decodable linear regression with Gaussian covariates. The authors propose a new lower bound for the problem, which is based on the Statistical Query (SQ) lower bound. The lower bound is derived for the case where the covariates are Gaussian and the noise distribution is Gaussian. They show that the lower bound can be used to derive a lower bound on the number of queries required to solve the problem. The upper bound is also derived.","This paper studies the problem of list-decodable linear regression with Gaussian covariates. The authors propose a new lower bound for the problem, which is based on the Statistical Query (SQ) lower bound. The lower bound is derived for the case where the covariates are Gaussian and the noise distribution is Gaussian. They show that the lower bound can be used to derive a lower bound on the number of queries required to solve the problem. The upper bound is also derived."
11641,SP:7b258252a9063514348f5fa8d9c85afd85748747,"expert domain knowledge USED-FOR ML models. patient health status CONJUNCTION disease progression. disease progression CONJUNCTION patient health status. pharmacology USED-FOR domain knowledge. systems of Ordinary Differential Equations ( ODEs ) USED-FOR Pharmacological models. expert - designed ODEs CONJUNCTION machine - learned Neural ODEs. machine - learned Neural ODEs CONJUNCTION expert - designed ODEs. expert and latent variables USED-FOR observable quantities. synthetic data EVALUATE-FOR LHM. Task is Modeling a system ’s temporal behaviour. Method are Machine Learning ( ML ) approaches, and latent hybridisation model ( LHM ). OtherScientificTerm is small sample regime. Generic are application, models, variables, and system. ",This paper proposes a method for learning a system of ODEs from expert domain knowledge. The authors propose a latent hybridisation model (LHM) that combines expert and latent variables in order to improve the performance of the model. The proposed method is evaluated on synthetic data and real-world data. The results show that the proposed method outperforms the state-of-the-art methods. ,This paper proposes a method for learning a system of ODEs from expert domain knowledge. The authors propose a latent hybridisation model (LHM) that combines expert and latent variables in order to improve the performance of the model. The proposed method is evaluated on synthetic data and real-world data. The results show that the proposed method outperforms the state-of-the-art methods. 
11677,SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,Representation learning USED-FOR meta - learning. Representation learning USED-FOR rapid learning of new tasks. meta - learning USED-FOR rapid learning of new tasks. works USED-FOR task - specific representations. MAML USED-FOR task - specific representations. MAML HYPONYM-OF works. fine - tuning - based objective HYPONYM-OF per - task adaptation. per - task adaptation USED-FOR representation. representation USED-FOR task - specific representations. theoretical framework USED-FOR MAML - like algorithm. risk bounds FEATURE-OF predictors. shared structure USED-FOR method. finetuning USED-FOR risk bounds. finetuning USED-FOR predictors. gradient descent USED-FOR finetuning. gradient descent USED-FOR predictors. logistic regression and neural network settings EVALUATE-FOR bounds. OtherScientificTerm is frozen representation ” objective. Generic is algorithm. Method is few - shot learning. ,"This paper studies the problem of few-shot learning in the context of meta-learning. The authors propose a new algorithm called MAML-like algorithm for learning a task-specific representation. The main contribution of this paper is to provide a theoretical framework for this problem and to provide risk bounds for the proposed algorithm. The proposed algorithm is based on the idea of fine-tuning the per-task adaptation objective, which is an extension of the fine-tune-based objective for meta-training. The paper also provides a theoretical analysis of the risk bounds of the proposed method. ","This paper studies the problem of few-shot learning in the context of meta-learning. The authors propose a new algorithm called MAML-like algorithm for learning a task-specific representation. The main contribution of this paper is to provide a theoretical framework for this problem and to provide risk bounds for the proposed algorithm. The proposed algorithm is based on the idea of fine-tuning the per-task adaptation objective, which is an extension of the fine-tune-based objective for meta-training. The paper also provides a theoretical analysis of the risk bounds of the proposed method. "
11713,SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"paired images CONJUNCTION texts. texts CONJUNCTION paired images. lexicalist approach USED-FOR compositional and grounded meaning representation of language. grounded data USED-FOR compositional and grounded meaning representation of language. paired images HYPONYM-OF grounded data. texts HYPONYM-OF grounded data. neural network embedding USED-FOR shiny objects. symbolic form FEATURE-OF neuro - symbolic semantic program. lexical meanings USED-FOR neuro - symbolic program. syntax USED-FOR lexical meanings. joint parsing CONJUNCTION expected execution algorithm. expected execution algorithm CONJUNCTION joint parsing. exponentiallygrowing compositional space FEATURE-OF learning. expected execution algorithm USED-FOR learning. joint parsing USED-FOR learning. visual reasoning CONJUNCTION language - driven navigation. language - driven navigation CONJUNCTION visual reasoning. language - driven navigation EVALUATE-FOR G2L2. visual reasoning EVALUATE-FOR G2L2. domains EVALUATE-FOR G2L2. language - driven navigation HYPONYM-OF domains. visual reasoning HYPONYM-OF domains. G2L2 USED-FOR compositions of words. OtherScientificTerm are syntactic type, syntactic type of adjective, and local marginalization. Method is meaning programs. Metric is training time. ","This paper proposes a neuro-symbolic semantic program (G2L2) for learning compositional and grounded meaning representation of language. The authors propose to use a neural network embedding to learn compositional embeddings of words in a symbolic form, which is then used to learn a compositional representation of the language. They show that the learned compositional space is exponentially growing as the number of words increases. They also show that joint parsing and expected execution algorithms can be used to improve the performance of the learning process. ","This paper proposes a neuro-symbolic semantic program (G2L2) for learning compositional and grounded meaning representation of language. The authors propose to use a neural network embedding to learn compositional embeddings of words in a symbolic form, which is then used to learn a compositional representation of the language. They show that the learned compositional space is exponentially growing as the number of words increases. They also show that joint parsing and expected execution algorithms can be used to improve the performance of the learning process. "
11749,SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,stochastic Newton algorithm USED-FOR homogeneous distributed stochastic convex optimization. stochastic gradients CONJUNCTION stochastic Hessian - vector products. stochastic Hessian - vector products CONJUNCTION stochastic gradients. stochastic gradients FEATURE-OF population objective. method COMPARE methods. methods COMPARE method. convergence guarantees FEATURE-OF quasi - self - concordant objectives. method USED-FOR communication rounds. communication rounds COMPARE methods. methods COMPARE communication rounds. logistic regression HYPONYM-OF quasi - self - concordant objectives. OtherScientificTerm is stochastic computations. ,This paper studies the problem of homogeneous distributed stochastic convex optimization. The authors propose a new method to solve the problem. The main contribution of the paper is to provide convergence guarantees for quasi-self-concordant objectives for the population objective and the Hessian-vector product objective. The convergence guarantees are based on the assumption that the population is homogeneous. The paper also shows that the proposed method can be used for communication rounds. ,This paper studies the problem of homogeneous distributed stochastic convex optimization. The authors propose a new method to solve the problem. The main contribution of the paper is to provide convergence guarantees for quasi-self-concordant objectives for the population objective and the Hessian-vector product objective. The convergence guarantees are based on the assumption that the population is homogeneous. The paper also shows that the proposed method can be used for communication rounds. 
11785,SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"Chamfer Distance ( CD ) CONJUNCTION Earth Mover ’s Distance ( EMD ). Earth Mover ’s Distance ( EMD ) CONJUNCTION Chamfer Distance ( CD ). Chamfer Distance ( CD ) HYPONYM-OF metrics. Earth Mover ’s Distance ( EMD ) HYPONYM-OF metrics. global distribution USED-FOR EMD. them USED-FOR consistent evaluation. Density - aware Chamfer Distance ( DCD ) HYPONYM-OF similarity measure. it USED-FOR disparity of density distributions. it COMPARE EMD. EMD COMPARE it. it COMPARE CD. CD COMPARE it. it USED-FOR detailed structures. CD USED-FOR It. DCD USED-FOR point cloud completion task. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR local geometric details. DCD USED-FOR training loss. metrics EVALUATE-FOR model. CD loss USED-FOR model. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR it. OtherScientificTerm are mismatched local density, fidelity of detailed structures, unbounded value range, outliers, and bounded value range. Method is point discriminator module. Task are guided downsampling step, and point cloud similarity evaluation. ","This paper proposes a new metric for point cloud similarity evaluation based on the Chamfer Distance (CD) and Earth Mover’s Distance (EMD) metrics. The proposed metric, called Density-aware Chamfer distance (DDCD), is an extension of the existing CD and EMD metrics, which are used to measure the disparity of density distributions between two points in a point cloud. The authors show that DCD can be used to improve the performance of point cloud completion tasks by reducing the number of outliers. The paper also shows that the proposed metric can be applied to the task of guided downsampling.","This paper proposes a new metric for point cloud similarity evaluation based on the Chamfer Distance (CD) and Earth Mover’s Distance (EMD) metrics. The proposed metric, called Density-aware Chamfer distance (DDCD), is an extension of the existing CD and EMD metrics, which are used to measure the disparity of density distributions between two points in a point cloud. The authors show that DCD can be used to improve the performance of point cloud completion tasks by reducing the number of outliers. The paper also shows that the proposed metric can be applied to the task of guided downsampling."
11821,SP:e4b302009520770814ff2c096020b779a9fc38fe,Knowledge distillation USED-FOR small student network. small student network USED-FOR teacher model. ensemble of networks HYPONYM-OF teacher model. knowledge distillation USED-FOR student generalization. dataset USED-FOR distillation. Generic is it. OtherScientificTerm is predictive distributions. Task is optimization. ,"This paper studies the problem of knowledge distillation, which is an important problem in machine learning. The authors propose to use a teacher model to distill the knowledge of a small student network to a larger teacher model. The teacher model is trained on a large dataset, and the student model is used to train the teacher model on a small subset of the dataset. Experiments are conducted to show the effectiveness of the proposed method. ","This paper studies the problem of knowledge distillation, which is an important problem in machine learning. The authors propose to use a teacher model to distill the knowledge of a small student network to a larger teacher model. The teacher model is trained on a large dataset, and the student model is used to train the teacher model on a small subset of the dataset. Experiments are conducted to show the effectiveness of the proposed method. "
11857,SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"algorithm USED-FOR ( k, ε)-coreset. decision trees PART-OF machine learning. decision trees CONJUNCTION partition trees. partition trees CONJUNCTION decision trees. computational geometry FEATURE-OF partition trees. sklearn CONJUNCTION lightGBM. lightGBM CONJUNCTION sklearn. coresets USED-FOR random forests. computation time EVALUATE-FOR random forests. random forests CONJUNCTION parameter tuning. parameter tuning CONJUNCTION random forests. lightGBM EVALUATE-FOR coresets. sklearn EVALUATE-FOR coresets. computation time EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR random forests. coresets USED-FOR parameter tuning. computation time EVALUATE-FOR coresets. accuracy EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR coresets. accuracy EVALUATE-FOR coresets. Method is k - tree. OtherScientificTerm are axis - parallel rectangles, error parameter, tree, optimal k - tree, and coreset. Metric is regression or classification loss. Generic is loss. ","This paper studies the problem of learning a (k, ε)-coreset for decision trees. The coreset is defined as the optimal k-tree of the decision tree. The authors propose a new algorithm for learning the k-coreset. The algorithm is based on the idea of partition trees, which is a generalization of the partition tree to the case of decision trees, and the authors show that the proposed algorithm can be used to learn a coreset for any decision tree, and that it is computationally efficient. The paper also shows that the coreset can also be used for parameter tuning.","This paper studies the problem of learning a (k, ε)-coreset for decision trees. The coreset is defined as the optimal k-tree of the decision tree. The authors propose a new algorithm for learning the k-coreset. The algorithm is based on the idea of partition trees, which is a generalization of the partition tree to the case of decision trees, and the authors show that the proposed algorithm can be used to learn a coreset for any decision tree, and that it is computationally efficient. The paper also shows that the coreset can also be used for parameter tuning."
11893,SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"δ - correct algorithm USED-FOR Top - m identification problem. sample complexity EVALUATE-FOR δ - correct algorithm. tractable lower bound USED-FOR δ - correct algorithm. sample complexity EVALUATE-FOR tractable lower bound. algorithm USED-FOR setting. sample complexity FEATURE-OF upper bound. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic and real - world data EVALUATE-FOR algorithm. Method are fixed - confidence Top - m identification ), misspecified linear bandit models, and linear models. Generic are problem, and algorithms. Task is medicine and recommendation systems. OtherScientificTerm are linearity, structure of the problem, misspecification, and lower bound. ",This paper studies the problem of Top-m identification in the setting of misspecified linear bandit models. The authors propose a new lower bound for the sample complexity of the problem. The upper bound is based on a tractable lower bound on the linearity of the problems. The lower bound is proved for both synthetic and real-world data. ,This paper studies the problem of Top-m identification in the setting of misspecified linear bandit models. The authors propose a new lower bound for the sample complexity of the problem. The upper bound is based on a tractable lower bound on the linearity of the problems. The lower bound is proved for both synthetic and real-world data. 
11929,SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"self - supervised learning USED-FOR graph neural networks ( GNNs ). self - supervised learning USED-FOR representation of graph - structure data. self - supervised learning methods USED-FOR GNNs. self - supervised learning USED-FOR disentangled graph representations. Disentangled Graph Contrastive Learning ( DGCL ) method USED-FOR disentangled graph - level representations. self - supervision USED-FOR disentangled graph - level representations. factorized representations USED-FOR latent and disentangled aspect. latent factor FEATURE-OF graph. factorized representations USED-FOR expressive information. contrastive learning manner FEATURE-OF factor - wise discrimination objective. latent factors USED-FOR expressive information. synthetic and real - world datasets EVALUATE-FOR method. method COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE method. synthetic and real - world datasets EVALUATE-FOR state - of - the - art baselines. OtherScientificTerm are real - world graph, and entanglement of the latent factors. Generic is learned representations. Task is Learning disentangled graph representations. ","This paper proposes a self-supervised learning method for disentangled graph representation learning. The proposed method is based on contrastive learning, which aims to disentangle the latent factors of a given graph. The authors propose to use a factor-wise discrimination objective to learn the disentanglement between the latent factor and the graph-level representation. The method is evaluated on synthetic and real-world datasets.","This paper proposes a self-supervised learning method for disentangled graph representation learning. The proposed method is based on contrastive learning, which aims to disentangle the latent factors of a given graph. The authors propose to use a factor-wise discrimination objective to learn the disentanglement between the latent factor and the graph-level representation. The method is evaluated on synthetic and real-world datasets."
11965,SP:0a7edbbdabab11273689c40c517001eb46491113,"robustness EVALUATE-FOR network. Statistical Reliability Engineering FEATURE-OF stochastic simulation. stochastic simulation USED-FOR network. stochastic simulation USED-FOR robustness. statistical hypothesis test USED-FOR robustness assessment. Importance Splitting simulation USED-FOR procedure. OtherScientificTerm are theoretical guarantees, sample size, and network function. Method is large scale networks. Generic is method. ","This paper studies the problem of estimating the robustness of a neural network to perturbations in the input data. The authors propose to use stochastic simulation as a statistical hypothesis test to evaluate the robustity of the network. The paper also proposes a method to estimate the sample size of the test set. The method is based on the idea of Importance Splitting Simulation (IS), which is an extension of SimCLR.   ","This paper studies the problem of estimating the robustness of a neural network to perturbations in the input data. The authors propose to use stochastic simulation as a statistical hypothesis test to evaluate the robustity of the network. The paper also proposes a method to estimate the sample size of the test set. The method is based on the idea of Importance Splitting Simulation (IS), which is an extension of SimCLR.   "
12001,SP:c1db485ff1ff9573daa421e167225654babb55ac,Generative modeling USED-FOR machine learning. Deep polynomial neural networks ( PNNs ) USED-FOR unsupervised image generation. PNNs USED-FOR conditional generation tasks. super - resolution HYPONYM-OF conditional generation tasks. noise variable CONJUNCTION conditional variable. conditional variable CONJUNCTION noise variable. single - variable polynomial expansions USED-FOR PNNs. conditional variable HYPONYM-OF two - variable inputs. noise variable HYPONYM-OF two - variable inputs. framework USED-FOR autoand cross - correlations. framework USED-FOR polynomial expansion. input variables USED-FOR CoPE. edges - to - image translation CONJUNCTION image - to - image translation. image - to - image translation CONJUNCTION edges - to - image translation. inverse problems CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION inverse problems. class - conditional generation CONJUNCTION inverse problems. inverse problems CONJUNCTION class - conditional generation. image - to - image translation CONJUNCTION attributeguided generation. attributeguided generation CONJUNCTION image - to - image translation. class - conditional generation CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION class - conditional generation. tasks EVALUATE-FOR CoPE. class - conditional generation EVALUATE-FOR CoPE. attributeguided generation HYPONYM-OF tasks. class - conditional generation HYPONYM-OF tasks. inverse problems HYPONYM-OF tasks. image - to - image translation HYPONYM-OF tasks. edges - to - image translation HYPONYM-OF tasks. CoPE USED-FOR conditional generation tasks. CoPE USED-FOR polynomial_nets_for_conditional_generation. OtherScientificTerm is noise. Material is synthesized image. ,"This paper proposes CoPE, a framework for unsupervised image generation using deep polynomial neural networks (PNNs). CoPE is based on the notion of auto-correlation and cross-correlations. The authors propose to use a single-variable polynomials to represent the input variables and the noise variable as two-variable inputs. CoPE can be applied to a wide range of conditional generation tasks such as image-to-image translation, class-conditional generation, and attribute-guided generation. The proposed method is evaluated on a number of tasks and shows promising results.","This paper proposes CoPE, a framework for unsupervised image generation using deep polynomial neural networks (PNNs). CoPE is based on the notion of auto-correlation and cross-correlations. The authors propose to use a single-variable polynomials to represent the input variables and the noise variable as two-variable inputs. CoPE can be applied to a wide range of conditional generation tasks such as image-to-image translation, class-conditional generation, and attribute-guided generation. The proposed method is evaluated on a number of tasks and shows promising results."
12037,SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,neural tangent kernel ( NTK ) CONJUNCTION MMD. MMD CONJUNCTION neural tangent kernel ( NTK ). approach USED-FOR MMD statistic. connection USED-FOR approach. memory and computational complexity EVALUATE-FOR MMD statistic. MMD statistic USED-FOR online implementation. approach USED-FOR NTK based two - sample tests. connection USED-FOR NTK based two - sample tests. theories USED-FOR kernel MMD. connection USED-FOR NTK test statistic properties. Type - I error CONJUNCTION testing power. testing power CONJUNCTION Type - I error. testing power HYPONYM-OF NTK test statistic properties. Type - I error HYPONYM-OF NTK test statistic properties. synthetic and real - world datasets EVALUATE-FOR theory. synthetic and real - world datasets EVALUATE-FOR NTK - MMD statistic. OtherScientificTerm is two - sample test. ,This paper proposes a new two-sample test method based on neural tangent kernel (NTK) and kernel MMD. The proposed method is based on the connection between MMD and NTK. The connection between NTK and MMD is defined in terms of the Type-I error and Type-II error. The authors show that the proposed method outperforms existing methods on synthetic and real-world datasets. ,This paper proposes a new two-sample test method based on neural tangent kernel (NTK) and kernel MMD. The proposed method is based on the connection between MMD and NTK. The connection between NTK and MMD is defined in terms of the Type-I error and Type-II error. The authors show that the proposed method outperforms existing methods on synthetic and real-world datasets. 
12073,SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"minimum necessary information USED-FOR neural net D ( · ). class - disentanglement USED-FOR variational autoencoder G ( · ). former COMPARE latter. latter COMPARE former. variational autoencoder G ( · ) USED-FOR class - dependent information. class - disentanglement USED-FOR class - dependent information. classification PART-OF x − G(x ). latter USED-FOR classification. clean images CONJUNCTION adversarial images. adversarial images CONJUNCTION clean images. it USED-FOR adversarial images. it USED-FOR clean images. adversarial attacks USED-FOR perturbations. class - dependent part USED-FOR perturbations. adversarial detection CONJUNCTION adversarial defense. adversarial defense CONJUNCTION adversarial detection. adversarial defense USED-FOR G(x ). detection CONJUNCTION defense. defense CONJUNCTION detection. approach USED-FOR adversarial attacks. defense USED-FOR adversarial attacks. approach USED-FOR defense. detection EVALUATE-FOR approach. Task are detection and defense of adversarial attacks, and classification and attack models. ",This paper proposes a new adversarial defense method for classification and detection. The proposed method is based on a variational autoencoder (VAE) and class-disentanglement (class-dependent information). The authors show that the class-dependent part of the VAE can be used to detect and defend against adversarial attacks. They also show that adversarial detection and defense can be combined to improve the performance of the classifier.,This paper proposes a new adversarial defense method for classification and detection. The proposed method is based on a variational autoencoder (VAE) and class-disentanglement (class-dependent information). The authors show that the class-dependent part of the VAE can be used to detect and defend against adversarial attacks. They also show that adversarial detection and defense can be combined to improve the performance of the classifier.
12109,SP:2789874561620ba7894c4672f935056bb911e919,Bayesian optimization ( BO ) USED-FOR federated learning ( FL ) setting. federated Thompson sampling ( FTS ) algorithm USED-FOR applications. federated hyperparameter tuning HYPONYM-OF applications. federated Thompson sampling ( FTS ) algorithm USED-FOR Bayesian optimization ( BO ). federated Thompson sampling ( FTS ) algorithm USED-FOR federated learning ( FL ) setting. privacy guarantee FEATURE-OF FL. privacy guarantee FEATURE-OF FTS. differential privacy ( DP ) USED-FOR deep neural networks. DP USED-FOR iterative algorithms. DP USED-FOR user - level privacy. FTS USED-FOR user - level privacy. DP USED-FOR FTS. local modeling USED-FOR BO. DP framework USED-FOR parameter vectors. utility EVALUATE-FOR algorithm. local modeling USED-FOR algorithm. distributed exploration ( DE ) USED-FOR utility. distributed exploration ( DE ) USED-FOR algorithm. privacy CONJUNCTION utility. utility CONJUNCTION privacy. theoretical guarantees FEATURE-OF privacy. theoretical guarantees FEATURE-OF utility. theoretical guarantees FEATURE-OF differentially private FTS. privacy CONJUNCTION utility. utility CONJUNCTION privacy. utility CONJUNCTION privacy guarantee. privacy guarantee CONJUNCTION utility. real - world experiments EVALUATE-FOR DP - FTS - DE. utility EVALUATE-FOR DP - FTS - DE. OtherScientificTerm is privacy - utility trade - off. ,This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) and federated hyperparameter tuning (FL). The proposed algorithm is based on the differential privacy (DP) framework. The authors show that the FTS algorithm is differentially private and can be used for both user-level privacy and utility trade-off. The paper also provides theoretical guarantees on the utility of the algorithm. ,This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) and federated hyperparameter tuning (FL). The proposed algorithm is based on the differential privacy (DP) framework. The authors show that the FTS algorithm is differentially private and can be used for both user-level privacy and utility trade-off. The paper also provides theoretical guarantees on the utility of the algorithm. 
12145,SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"it USED-FOR real - world problems. data annotation USED-FOR MLC models. informative samples USED-FOR cost - effective annotation. BM USED-FOR label correlations. mixture component USED-FOR global pattern of label correlations. Bayesian Bernoulli mixture of label clusters USED-FOR BM. Bayesian Bernoulli mixture of label clusters USED-FOR label correlations. predictive GP USED-FOR feature - component - label mapping. BM CONJUNCTION predictive GP. predictive GP CONJUNCTION BM. BM USED-FOR feature - component - label mapping. predictive GP USED-FOR data features. AL USED-FOR sparse labels. BM USED-FOR sparse labels. GP USED-FOR mixture components. auxiliary variable based variational inference algorithm USED-FOR non - conjugacy. mapping process USED-FOR end - to - end posterior inference. predictive distribution USED-FOR label prediction. model USED-FOR predictive distribution. feature uncertainty CONJUNCTION label covariance. label covariance CONJUNCTION feature uncertainty. label covariance USED-FOR data sampling. BM ) USED-FOR data sampling. label covariance CONJUNCTION BM ). BM ) CONJUNCTION label covariance. GP USED-FOR feature uncertainty. real - world multi - label datasets EVALUATE-FOR model. AL EVALUATE-FOR model. real - world multi - label datasets EVALUATE-FOR AL. Task is Multi - label classification ( MLC ). OtherScientificTerm are correlated ( hence non - exclusive ) labels, sparse label space, correlated label space, inductive bias, and label covariance matrix. ",This paper proposes a new model for multi-label classification (MLC) based on Bayesian Bernoulli mixture of label clusters (BM) and predictive GP (GP). The proposed model is based on an auxiliary variable-based variational inference algorithm. The authors show that the proposed model outperforms the baselines in terms of both feature uncertainty and label covariance. ,This paper proposes a new model for multi-label classification (MLC) based on Bayesian Bernoulli mixture of label clusters (BM) and predictive GP (GP). The proposed model is based on an auxiliary variable-based variational inference algorithm. The authors show that the proposed model outperforms the baselines in terms of both feature uncertainty and label covariance. 
12181,SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"wedge - shaped point cloud sectors COMPARE point cloud. point cloud COMPARE wedge - shaped point cloud sectors. end - to - end latency EVALUATE-FOR lidar perception models. lidars HYPONYM-OF streaming data source. cartesian coordinate systems USED-FOR methods. multi - scale padding USED-FOR spatial context. feature undistortion CONJUNCTION range stratified convolutions. range stratified convolutions CONJUNCTION feature undistortion. feature undistortion USED-FOR core polar convolutional architecture. range stratified convolutions USED-FOR core polar convolutional architecture. nuScenes dataset EVALUATE-FOR streaming based methods. OtherScientificTerm are sectors, and rectangular regions. Method are polar coordinate system, and non - streaming methods. ","This paper proposes a new method to improve the performance of lidar perception models. The proposed method is based on the polar coordinate system, which is a cartesian coordinate system. The authors propose to use a range stratified convolutional architecture to reduce feature undistortion and feature padding. The method is evaluated on the nuScenes dataset and shows that the proposed method outperforms the baselines.","This paper proposes a new method to improve the performance of lidar perception models. The proposed method is based on the polar coordinate system, which is a cartesian coordinate system. The authors propose to use a range stratified convolutional architecture to reduce feature undistortion and feature padding. The method is evaluated on the nuScenes dataset and shows that the proposed method outperforms the baselines."
12217,SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"Structured latent variables USED-FOR deep learning models. prior knowledge PART-OF deep learning models. variables USED-FOR learning. differentiable surrogate USED-FOR training. learning approach USED-FOR latent variable. Gumbel - Max trick USED-FOR distributions. structured domains FEATURE-OF distributions. score function estimators USED-FOR optimization. score function estimators USED-FOR differentiable surrogates. stochastic invariant HYPONYM-OF recursive algorithms. gradient estimates CONJUNCTION control variates. control variates CONJUNCTION gradient estimates. feature USED-FOR gradient estimates. feature USED-FOR control variates. structured latent variable models COMPARE relaxation - based counterparts. relaxation - based counterparts COMPARE structured latent variable models. OtherScientificTerm are surrogate, and biased gradients. Generic is model. ","This paper studies the problem of learning a differentiable surrogate for structured latent variables in deep learning models. The authors propose a novel learning approach to learn a surrogate for a structured latent variable. The surrogate is learned using a Gumbel-Max trick. The paper shows that the surrogate can be learned in a recursive manner, and that the gradient of the surrogate is stochastic invariant.  The authors also show that the proposed method can be used to learn differentiable surrogates for differentiable control variates. ","This paper studies the problem of learning a differentiable surrogate for structured latent variables in deep learning models. The authors propose a novel learning approach to learn a surrogate for a structured latent variable. The surrogate is learned using a Gumbel-Max trick. The paper shows that the surrogate can be learned in a recursive manner, and that the gradient of the surrogate is stochastic invariant.  The authors also show that the proposed method can be used to learn differentiable surrogates for differentiable control variates. "
12253,SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"Deep convolutional neural networks ( CNNs ) USED-FOR image denoising. large datasets USED-FOR Deep convolutional neural networks ( CNNs ). noisy image USED-FOR denoisers. models USED-FOR features. large datasets USED-FOR CNN models. convolutional layers PART-OF CNN. multiplicative scaling parameter USED-FOR GainTuning. GainTuning COMPARE CNNs. CNNs COMPARE GainTuning. denoising EVALUATE-FOR GainTuning. image - denoising benchmarks EVALUATE-FOR CNNs. image - denoising benchmarks EVALUATE-FOR GainTuning. noise level CONJUNCTION image type. image type CONJUNCTION noise level. adaptive GainTuning USED-FOR transmission - electronmicroscope images. synthetic data USED-FOR CNN. CNN USED-FOR adaptive GainTuning. GainTuning USED-FOR structure of catalytic nanoparticles. methodology COMPARE GainTuning. GainTuning COMPARE methodology. low signal - to - noise ratios FEATURE-OF data. data USED-FOR GainTuning. data USED-FOR structure of catalytic nanoparticles. Generic are they, and them. OtherScientificTerm is overfitting. ","This paper proposes a method for denoising CNNs. The method is based on adaptive GainTuning, which is a multiplicative scaling parameter that is applied to the convolutional layers of the CNN. The proposed method is evaluated on synthetic and real-world datasets.","This paper proposes a method for denoising CNNs. The method is based on adaptive GainTuning, which is a multiplicative scaling parameter that is applied to the convolutional layers of the CNN. The proposed method is evaluated on synthetic and real-world datasets."
12289,SP:90afa1102683b456bc72a54abef466326827546a,convolutional neural network CONJUNCTION asymmetric multiway cut problem solver. asymmetric multiway cut problem solver CONJUNCTION convolutional neural network. fully differentiable architecture USED-FOR simultaneous semantic and instance segmentation. panoptic segmentation PART-OF fully differentiable architecture. convolutional neural network PART-OF fully differentiable architecture. asymmetric multiway cut problem solver PART-OF fully differentiable architecture. latter USED-FOR combinatorial optimization problem. combinatorial optimization problem USED-FOR panoptic labeling. semantic and boundary predictions USED-FOR panoptic labeling. semantic and boundary predictions PART-OF combinatorial optimization problem. formulation USED-FOR smooth surrogate of the panoptic quality metric. gradient USED-FOR optimization problem. Cityscapes and COCO datasets EVALUATE-FOR approaches. combinatorial optimization USED-FOR panoptic segmentation ( COPS ). optimization USED-FOR large scale real - world problem. optimization CONJUNCTION deep learning. deep learning CONJUNCTION optimization. deep learning USED-FOR large scale real - world problem. approach USED-FOR combinatorial optimization. optimization USED-FOR approach. Generic is architecture. ,This paper proposes a new architecture for panoptic segmentation (COPS). The proposed architecture is based on an asymmetric multi-way cut problem solver and a convolutional neural network. The proposed method is evaluated on Cityscapes and COCO datasets.,This paper proposes a new architecture for panoptic segmentation (COPS). The proposed architecture is based on an asymmetric multi-way cut problem solver and a convolutional neural network. The proposed method is evaluated on Cityscapes and COCO datasets.
12325,SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"Probabilistic context - free grammars ( PCFGs ) CONJUNCTION dynamic Bayesian networks ( DBNs ). dynamic Bayesian networks ( DBNs ) CONJUNCTION Probabilistic context - free grammars ( PCFGs ). dynamic Bayesian networks ( DBNs ) HYPONYM-OF sequence models. Probabilistic context - free grammars ( PCFGs ) HYPONYM-OF sequence models. PCFGs USED-FOR nested hierarchical dependencies ( tree structures ). continuous latent variables USED-FOR DBNs. PCFGs CONJUNCTION DBNs. DBNs CONJUNCTION PCFGs. PCFGs USED-FOR Recursive Bayesian Networks ( RBNs ). RBNs USED-FOR joint distribution. discrete or continuous latent variables FEATURE-OF tree - structured Bayesian networks. tree - structured Bayesian networks USED-FOR joint distribution. exponential number of possible structures CONJUNCTION continuous variables. continuous variables CONJUNCTION exponential number of possible structures. exponential number of possible structures USED-FOR joint inference. maximum posterior estimates USED-FOR continuous latent variables. PCFGs USED-FOR inside and outside probabilities. inside and outside probabilities USED-FOR RBNs. gradient descent USED-FOR maximum posterior estimates. robust parameter optimisation CONJUNCTION Bayesian inference. Bayesian inference CONJUNCTION robust parameter optimisation. marginal data likelihood ( evidence ) CONJUNCTION marginal posterior distribution. marginal posterior distribution CONJUNCTION marginal data likelihood ( evidence ). change point detection CONJUNCTION hierarchical clustering. hierarchical clustering CONJUNCTION change point detection. RBNs USED-FOR segmentation. RBNs COMPARE change point detection. change point detection COMPARE RBNs. RBNs COMPARE hierarchical clustering. hierarchical clustering COMPARE RBNs. noisy sequences USED-FOR RBNs. examples EVALUATE-FOR RBNs. musical data USED-FOR hierarchical music analysis. raw note level USED-FOR hierarchical music analysis. OtherScientificTerm are dependencies, latent variables, nested hierarchical dependency structure, mixed discrete - continuous case, network structures, and expert annotations. Generic is neither. Method is Gaussian RBNs. Material is synthetic data. ","This paper studies the use of Recursive Bayesian Networks (RBNs) for hierarchical music analysis. RBNs are a variant of dynamic Bayesian networks (DBNs), where the latent variables are discrete and continuous. The authors show that the RBN is able to learn the joint distribution of the discrete-continuous case and the continuous case. They also show that RBN can learn the marginal data likelihood (evidence) and the marginal posterior distribution (probabilistic evidence) of the continuous latent variables. They further show that for discrete-convex case, RBN-based models can learn both the inside and outside probabilities. ","This paper studies the use of Recursive Bayesian Networks (RBNs) for hierarchical music analysis. RBNs are a variant of dynamic Bayesian networks (DBNs), where the latent variables are discrete and continuous. The authors show that the RBN is able to learn the joint distribution of the discrete-continuous case and the continuous case. They also show that RBN can learn the marginal data likelihood (evidence) and the marginal posterior distribution (probabilistic evidence) of the continuous latent variables. They further show that for discrete-convex case, RBN-based models can learn both the inside and outside probabilities. "
12361,SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"objective functions FEATURE-OF deep neural networks. Backward propagation of errors ( backpropagation ) USED-FOR objective functions. loss functions HYPONYM-OF objective functions. pseudo - Lagrange multiplier method USED-FOR constrained backpropagation ( CBP ) algorithm. two - bit shift weight constraints HYPONYM-OF constraints. binary HYPONYM-OF constraints. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet-50. AlexNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION AlexNet. CBP USED-FOR AlexNet. CBP USED-FOR GoogLeNet. CBP USED-FOR ResNet-18. posttraining method USED-FOR CBP. GoogLeNet PART-OF ImageNet. ImageNet USED-FOR CBP. backpropagation USED-FOR CBP. ResNet-18 CONJUNCTION ResNet50. ResNet50 CONJUNCTION ResNet-18. ResNet50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet50. algorithm COMPARE methods. methods COMPARE algorithm. top-1 accuracy EVALUATE-FOR ResNet-18. binary weights USED-FOR GoogLeNet. ImageNet EVALUATE-FOR methods. ImageNet EVALUATE-FOR algorithm. top-1 accuracy EVALUATE-FOR algorithm. CBP USED-FOR learning algorithm. constraint functions USED-FOR learning algorithm. constraint functions USED-FOR CBP. OtherScientificTerm are weight precision, objective function, and minimal performance loss. Generic is algorithms. Method is CBP algorithm. ","This paper proposes a new constrained backpropagation (CBP) algorithm for deep neural networks. The proposed method is based on the pseudo-Lagrange multiplier method. The authors propose to use two-bit shift weight constraints to enforce the constraint on the objective function. The paper also proposes a post-training method to improve the performance of the proposed method. Experiments are conducted on ImageNet, ResNet-18, and GoogLeNet.","This paper proposes a new constrained backpropagation (CBP) algorithm for deep neural networks. The proposed method is based on the pseudo-Lagrange multiplier method. The authors propose to use two-bit shift weight constraints to enforce the constraint on the objective function. The paper also proposes a post-training method to improve the performance of the proposed method. Experiments are conducted on ImageNet, ResNet-18, and GoogLeNet."
12397,SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"acquisition function USED-FOR data / label efficiency. acquisition function USED-FOR Active learning. discrete instance set ( pool - based scenario CONJUNCTION continuous instance space ( query synthesis scenario ). continuous instance space ( query synthesis scenario ) CONJUNCTION discrete instance set ( pool - based scenario. active learning scenarios USED-FOR Gaussian Process Classification ( GPC ). active learning strategies USED-FOR classification error. active learning strategies USED-FOR Estimated Error Reduction ( EER ). gradient - based optimization techniques USED-FOR continuous instance space. continuous instance space USED-FOR query synthesis. it COMPARE gradient - based optimization techniques. gradient - based optimization techniques COMPARE it. gradient - based optimization techniques USED-FOR query synthesis. algorithms USED-FOR EER - based active learning. GPC USED-FOR EER - based active learning. one - dimensional integral USED-FOR joint predictive distribution of label pairs. gradient chain rule USED-FOR gradient of the acquisition function. query synthesis active learning algorithm USED-FOR EER - based strategies. algorithms COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithms. sampling efficiency EVALUATE-FOR state - of - the - art algorithms. synthetic and real - world datasets EVALUATE-FOR state - of - the - art algorithms. sampling efficiency EVALUATE-FOR algorithms. synthetic and real - world datasets EVALUATE-FOR algorithms. Task is labeling. Method is onestep - look - ahead manner. OtherScientificTerm are EER - based acquisition functions, and EER. Metric is computational overhead. ","This paper studies the problem of active learning for Gaussian process classification. The authors propose two active learning scenarios: discrete instance set (pool-based) and continuous instance space (query synthesis) active learning. In the pool-based setting, the authors propose a query synthesis active learning algorithm, which is based on a gradient-based optimization technique. The proposed algorithm is evaluated on synthetic and real-world datasets and shows better performance than existing active learning algorithms.","This paper studies the problem of active learning for Gaussian process classification. The authors propose two active learning scenarios: discrete instance set (pool-based) and continuous instance space (query synthesis) active learning. In the pool-based setting, the authors propose a query synthesis active learning algorithm, which is based on a gradient-based optimization technique. The proposed algorithm is evaluated on synthetic and real-world datasets and shows better performance than existing active learning algorithms."
12433,SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"energy functions USED-FOR bounded gradients. gradients USED-FOR numerical instabilities. regularization FEATURE-OF autoencoder - based architectures. low - dimensional manifold FEATURE-OF data. natural images HYPONYM-OF data. natural images HYPONYM-OF low - dimensional manifold. VAE models HYPONYM-OF autoencoder - based architectures. over - regularization CONJUNCTION underregularization. underregularization CONJUNCTION over - regularization. infinite gradients FEATURE-OF autoencoder - based energy function. Method are continuous variational autoencoder ( VAE ) models, and VAE energy function. OtherScientificTerm are parameter gradients, unbounded gradients, posterior collapse, overand under - regularization, and large gradients. Generic is model. Task is suboptimal feature selection. ","This paper studies the problem of under-regularization in continuous variational autoencoder (VAE) models. The authors consider the case where the parameter gradients are unbounded, which is the case in the case of infinite-gradient VAE models. They show that the posterior collapse of a VAE model is due to the unbounded gradients of the energy function of the model. They also show that this phenomenon is also the case when the parameter gradient is bounded. They provide a theoretical analysis of this phenomenon. ","This paper studies the problem of under-regularization in continuous variational autoencoder (VAE) models. The authors consider the case where the parameter gradients are unbounded, which is the case in the case of infinite-gradient VAE models. They show that the posterior collapse of a VAE model is due to the unbounded gradients of the energy function of the model. They also show that this phenomenon is also the case when the parameter gradient is bounded. They provide a theoretical analysis of this phenomenon. "
12469,SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"graph feedback USED-FOR bandit problem. directed graph USED-FOR bandit problem. min - max regret EVALUATE-FOR graph. fractional weak domination number CONJUNCTION k - packing independence number. k - packing independence number CONJUNCTION fractional weak domination number. linear program USED-FOR them. fractional vertex packing set HYPONYM-OF linear program. strong duality theorem USED-FOR regret upper bound. integrality gap FEATURE-OF dual linear program. trees CONJUNCTION graphs. graphs CONJUNCTION trees. bounded integrality gap FEATURE-OF vertex packing problem. graphs FEATURE-OF vertex packing problem. trees FEATURE-OF vertex packing problem. bounded degree FEATURE-OF graphs. OtherScientificTerm are bandit arms, lower bound, and optimal regret. Metric is regret. Generic are notions, and bounds. ","This paper studies the regret upper bound of the bandit problem for directed graphs. The authors show that the regret of a directed graph is bounded by the bounded integrality gap of the dual linear program of the vertex packing set, which is a linear program with bounded degree. They also prove the strong duality theorem, which gives a lower bound for the regret. ","This paper studies the regret upper bound of the bandit problem for directed graphs. The authors show that the regret of a directed graph is bounded by the bounded integrality gap of the dual linear program of the vertex packing set, which is a linear program with bounded degree. They also prove the strong duality theorem, which gives a lower bound for the regret. "
12505,SP:e50dec57af337839cbde4b65fb7b431785fda44d,"Shapley values USED-FOR model agnostic feature attributions. global population distribution USED-FOR feature absence. neighbourhood reference distributions USED-FOR Shapley values. Nadaraya - Watson estimator HYPONYM-OF kernel regressor. self - normalised importance sampling estimator USED-FOR Nadaraya - Watson estimator. Neighbourhood Shapley values USED-FOR sparse feature relevance attributions. on - manifold explainability CONJUNCTION robustness. robustness CONJUNCTION on - manifold explainability. robustness EVALUATE-FOR adversarial classifiers. They USED-FOR adversarial classifiers. robustness EVALUATE-FOR They. on - manifold explainability EVALUATE-FOR They. OtherScientificTerm are global population, and local model behaviour. Method is Shapley analysis. ","This paper proposes a new Shapley analysis method for model agnostic feature attributions. The method is based on the Nadaraya-Watson estimator (Nadaraya et al., 2017), which is a kernel regressor. The authors show that the proposed method is robust to adversarial attacks and on-manifold explainability.","This paper proposes a new Shapley analysis method for model agnostic feature attributions. The method is based on the Nadaraya-Watson estimator (Nadaraya et al., 2017), which is a kernel regressor. The authors show that the proposed method is robust to adversarial attacks and on-manifold explainability."
12541,SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"feature representations USED-FOR deep reinforcement learning ( RL ). them USED-FOR feature learning. state - action sequences HYPONYM-OF un - experienced or less - experienced trajectories. data efficiency EVALUATE-FOR RL feature representation learning. backward dynamics model USED-FOR trajectory cycle. dynamics model USED-FOR PlayVirtual. actions USED-FOR virtual state - action trajectories. cycle consistency constraint FEATURE-OF trajectory. Atari and DeepMind Control Suite benchmarks EVALUATE-FOR designs. benchmarks EVALUATE-FOR method. Method is RL. OtherScientificTerm are data inefficiency, cycle - consistent virtual trajectories, latent space, and groudtruth state supervision. ","This paper proposes PlayVirtual, a method for learning state-action trajectories in deep reinforcement learning (RL). The authors propose to use a backward dynamics model to learn a trajectory cycle, which is then used to learn the dynamics of the latent space. The proposed method is evaluated on Atari and DeepMind Control Suite benchmarks. ","This paper proposes PlayVirtual, a method for learning state-action trajectories in deep reinforcement learning (RL). The authors propose to use a backward dynamics model to learn a trajectory cycle, which is then used to learn the dynamics of the latent space. The proposed method is evaluated on Atari and DeepMind Control Suite benchmarks. "
12577,SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,Noisy labels FEATURE-OF large real - world datasets. noisy labels FEATURE-OF robustness. robustness EVALUATE-FOR network ’s architecture. framework USED-FOR robustness. robustness EVALUATE-FOR network. architecture CONJUNCTION target / noise functions. target / noise functions CONJUNCTION architecture. predictive power EVALUATE-FOR representations. predictive power EVALUATE-FOR network ’s robustness. representations USED-FOR linear model. clean labels USED-FOR linear model. architecture COMPARE noise. noise COMPARE architecture. architecture USED-FOR network. predictive power COMPARE methods. methods COMPARE predictive power. predictive power USED-FOR representations. predictive power COMPARE noisy - label - training methods. noisy - label - training methods COMPARE predictive power. representations COMPARE noisy - label - training methods. noisy - label - training methods COMPARE representations. test accuracy EVALUATE-FOR noisy - label - training methods. test accuracy EVALUATE-FOR predictive power. clean labels USED-FOR methods. Method is neural network architectures. ,This paper studies the robustness of neural networks to noisy labels. The authors propose a new framework for robustness that combines the idea of target/noise functions and the architecture of a neural network. The proposed framework is tested on a variety of real-world datasets and shows that the proposed framework outperforms existing methods. ,This paper studies the robustness of neural networks to noisy labels. The authors propose a new framework for robustness that combines the idea of target/noise functions and the architecture of a neural network. The proposed framework is tested on a variety of real-world datasets and shows that the proposed framework outperforms existing methods. 
12613,SP:903727fe028684623a8ccadec210e641ecffc685,"RL algorithm USED-FOR reward function. method USED-FOR value function. transitions USED-FOR value function. hyperparameters USED-FOR method. data - driven Bellman equation USED-FOR method. approach COMPARE prior methods. prior methods COMPARE approach. Method are Reinforcement learning ( RL ) algorithms, RL algorithms, and control algorithm. Generic are process, and two - stage process. OtherScientificTerm are intermediate reward function, and reward function term. ",This paper proposes a new method for learning the value function of a reward function in reinforcement learning (RL). The authors propose to use a data-driven Bellman equation to compute the value of the reward function. The authors show that the proposed method is able to learn a value function in a two-stage process. They also show that their method can be applied to any RL algorithm. ,This paper proposes a new method for learning the value function of a reward function in reinforcement learning (RL). The authors propose to use a data-driven Bellman equation to compute the value of the reward function. The authors show that the proposed method is able to learn a value function in a two-stage process. They also show that their method can be applied to any RL algorithm. 
12649,SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"convex and non - convex settings FEATURE-OF differentially private stochastic optimization. algorithm USED-FOR l2 setting. differentially private algorithms USED-FOR general convex losses. algorithm USED-FOR optimal excess population risk. near - linear time FEATURE-OF algorithm. super - linear time FEATURE-OF differentially private algorithms. algorithm USED-FOR l1 setting. algorithm USED-FOR dimension dependent lower bound. nearly - optimal excess population risk FEATURE-OF algorithm. algorithms USED-FOR differentially private non - convex setting. smooth losses CONJUNCTION polyhedral constraint. polyhedral constraint CONJUNCTION smooth losses. smooth losses FEATURE-OF l1 - case. polyhedral constraint FEATURE-OF l1 - case. linear time FEATURE-OF nearly dimension independent rate. smooth losses FEATURE-OF constrained l2 - case. linear - time algorithm USED-FOR constrained l2 - case. method USED-FOR non - smooth weakly convex stochastic optimization. method COMPARE non - private algorithm. non - private algorithm COMPARE method. method USED-FOR l2 - case. non - convex l2 setting CONJUNCTION lp setting. lp setting CONJUNCTION non - convex l2 setting. Material is convex case. OtherScientificTerm are general non - smooth convex losses, and polylogarithmic ( in the dimension ) overhead. ",This paper studies the problem of differentially private stochastic optimization in convex and non-convex settings. The authors propose a new algorithm for the non-smooth weakly convex setting. The main contribution of the paper is to prove a new lower bound on the excess population risk of the algorithm in the convex case. The lower bound is based on the dimension-dependent lower bound of the loss in the l2-case. The algorithm is shown to converge to the optimal risk in near-linear time. ,This paper studies the problem of differentially private stochastic optimization in convex and non-convex settings. The authors propose a new algorithm for the non-smooth weakly convex setting. The main contribution of the paper is to prove a new lower bound on the excess population risk of the algorithm in the convex case. The lower bound is based on the dimension-dependent lower bound of the loss in the l2-case. The algorithm is shown to converge to the optimal risk in near-linear time. 
12685,SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"cooperative bandit problem USED-FOR large - scale decision - making. arbitrary corruptions CONJUNCTION delays. delays CONJUNCTION arbitrary corruptions. arbitrary corruptions FEATURE-OF stochastic networks. stochastic networks USED-FOR communication. cooperative bandit learning HYPONYM-OF real - world communication scenarios. adversarially corrupted rewards FEATURE-OF message - passing. random delays FEATURE-OF network. byzantine communication PART-OF message - passing. network USED-FOR instantaneous rewardsharing. stochastic time - varying networks USED-FOR message - passing. message - passing HYPONYM-OF real - world communication scenarios. instantaneous rewardsharing HYPONYM-OF real - world communication scenarios. message - passing HYPONYM-OF real - world communication scenarios. near - optimal guarantees FEATURE-OF incurred group regret. decentralized algorithms CONJUNCTION near - optimal guarantees. near - optimal guarantees CONJUNCTION decentralized algorithms. decentralized algorithms USED-FOR environments. delayed - update algorithm COMPARE state - of - the - art. state - of - the - art COMPARE delayed - update algorithm. network topologies EVALUATE-FOR delayed - update algorithm. network topologies EVALUATE-FOR state - of - the - art. tight network - dependent minimax lower bounds FEATURE-OF group regret. Generic are problem, and algorithms. OtherScientificTerm is perfect communication. Task is real - world distributed settings. ","This paper studies the problem of cooperative bandit learning with stochastic time-varying networks. In particular, the authors consider the case of message-passing with adversarially corrupted rewards, where the goal is to minimize the group regret. In this setting, the proposed algorithm is based on the delayed-update algorithm. The authors show that the delayed update algorithm achieves a tight network-dependent minimax lower bound for group regret and a near-optimal guarantee for decentralized algorithms. ","This paper studies the problem of cooperative bandit learning with stochastic time-varying networks. In particular, the authors consider the case of message-passing with adversarially corrupted rewards, where the goal is to minimize the group regret. In this setting, the proposed algorithm is based on the delayed-update algorithm. The authors show that the delayed update algorithm achieves a tight network-dependent minimax lower bound for group regret and a near-optimal guarantee for decentralized algorithms. "
12721,SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"transformer USED-FOR computer vision applications. architectures USED-FOR feature representations. vision transformers USED-FOR feature representations. convolutional neural networks COMPARE vision transformers. vision transformers COMPARE convolutional neural networks. architectures FEATURE-OF vision transformers. mobile devices USED-FOR feature representations. post - training quantization algorithm USED-FOR vision transformers. optimal low - bit quantization intervals USED-FOR quantization task. ranking loss USED-FOR quantization objective. quantization loss CONJUNCTION feature diversity. feature diversity CONJUNCTION quantization loss. nuclear norm FEATURE-OF attention map. nuclear norm USED-FOR mixedprecision quantization scheme. method COMPARE posttraining quantization algorithms. posttraining quantization algorithms COMPARE method. benchmark models EVALUATE-FOR method. top-1 accuracy EVALUATE-FOR DeiT - B model. ImageNet dataset EVALUATE-FOR DeiT - B model. Method is attention mechanism. OtherScientificTerm are self - attention, and quantization. ",This paper proposes a post-training quantization algorithm for vision transformers. The proposed method is based on the nuclear norm of self-attention and quantization. The authors show that the proposed method can be applied to any low-bit quantization intervals. They also show that their method is able to achieve state-of-the-art performance on the ImageNet dataset.,This paper proposes a post-training quantization algorithm for vision transformers. The proposed method is based on the nuclear norm of self-attention and quantization. The authors show that the proposed method can be applied to any low-bit quantization intervals. They also show that their method is able to achieve state-of-the-art performance on the ImageNet dataset.
12757,SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"Double Q - learning USED-FOR overestimation issue of Q - learning. polynomial learning rate USED-FOR analysis. polynomial learning rate USED-FOR slower convergence rate. analytical tools USED-FOR convergence rate. sampling strategy USED-FOR asynchronous double Q - learning. synchronous double Q - learning USED-FOR global optimum. time complexity EVALUATE-FOR asynchronous algorithm. fast convergence FEATURE-OF double - Q learning. Method are Q - learning, double Q - learning, and finite - time analysis. OtherScientificTerm are constant learning rate, state - action space, and discount factor. ","This paper studies the problem of double-Q-learning, which is an extension of the double Q-learning problem. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of double Q learning under the assumption that the learning rate is polynomial in the number of steps and the discount factor is constant. In particular, the authors show that under certain assumptions, the convergence rates converge to a global optimum in finite-time. The authors also provide a sampling strategy for synchronous double-q-learning and show that it converges to the global optimum. ","This paper studies the problem of double-Q-learning, which is an extension of the double Q-learning problem. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of double Q learning under the assumption that the learning rate is polynomial in the number of steps and the discount factor is constant. In particular, the authors show that under certain assumptions, the convergence rates converge to a global optimum in finite-time. The authors also provide a sampling strategy for synchronous double-q-learning and show that it converges to the global optimum. "
12793,SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"unlabeled and test data COMPARE labeled data. labeled data COMPARE unlabeled and test data. SSL algorithms USED-FOR real - world applications. semi - supervised OOD detection HYPONYM-OF setting. labeled data CONJUNCTION in - distribution data. in - distribution data CONJUNCTION labeled data. approach STEP USED-FOR OOD detection. technique USED-FOR approach STEP. Structure - Keep Unzipping HYPONYM-OF technique. It USED-FOR representation space. representation space USED-FOR OOD samples. STEP approach COMPARE methods. methods COMPARE STEP approach. OOD detection benchmarks EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR detection. detection EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR STEP approach. Task are semi - supervised learning ( SSL ) studies, OOD detection settings, and training. OtherScientificTerm are distribution of labeled data, and unknown distribution. Method is optimization algorithm. ","This paper proposes a semi-supervised OOD detection method based on the Structure-Keep Unzipping (STEP) technique. The proposed method is motivated by the observation that SSL algorithms often fail to distinguish between labeled and in-distribution data in the presence of OOD samples. To address this issue, the authors propose to use the structure-keep-unzipping technique to learn a representation space for OOD data. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark datasets.","This paper proposes a semi-supervised OOD detection method based on the Structure-Keep Unzipping (STEP) technique. The proposed method is motivated by the observation that SSL algorithms often fail to distinguish between labeled and in-distribution data in the presence of OOD samples. To address this issue, the authors propose to use the structure-keep-unzipping technique to learn a representation space for OOD data. The authors show that the proposed method outperforms the state-of-the-art methods on several benchmark datasets."
12829,SP:6bf8b94483b26033795b0eda9649518027f5e1c2,phrase localization CONJUNCTION referring expression comprehension / segmentation. referring expression comprehension / segmentation CONJUNCTION phrase localization. visual grounding USED-FOR visual reasoning. referring expression comprehension / segmentation HYPONYM-OF visual grounding. phrase localization HYPONYM-OF visual grounding. approaches USED-FOR referring expression comprehension ( REC ). approaches USED-FOR segmentation ( RES ). referring expression comprehension ( REC ) CONJUNCTION segmentation ( RES ). segmentation ( RES ) CONJUNCTION referring expression comprehension ( REC ). one - stage multi - task framework USED-FOR visual grounding tasks. modalities PART-OF visual - lingual encoder. modalities PART-OF transformer architecture. model USED-FOR contextualized lingual queries. segmentation mask USED-FOR referred regions. model USED-FOR decoder. contextualized model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE contextualized model. REC and RES tasks EVALUATE-FOR contextualized model. REC and RES tasks EVALUATE-FOR state - of - the - art methods. external dataset EVALUATE-FOR pre - training schedule. contextualized information CONJUNCTION multi - task training. multi - task training CONJUNCTION contextualized information. contextualized information USED-FOR model. multi - task training USED-FOR model. Generic is two - stage setup. Method is complex task - specific one - stage architectures. OtherScientificTerm is bounding box. ,This paper proposes a multi-task framework for visual grounding tasks. The framework is based on the transformer architecture. The proposed framework is evaluated on two tasks: referring expression comprehension (REC) and segmentation (RES).,This paper proposes a multi-task framework for visual grounding tasks. The framework is based on the transformer architecture. The proposed framework is evaluated on two tasks: referring expression comprehension (REC) and segmentation (RES).
12865,SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"Boosting HYPONYM-OF algorithmic approach. weak learner HYPONYM-OF agnostic PAC learner. classification loss EVALUATE-FOR agnostic PAC learner. boosting algorithm USED-FOR weak hypotheses. booster CONJUNCTION weak learner. weak learner CONJUNCTION booster. OtherScientificTerm are weak and moderately inaccurate hypotheses, and weak - learner calls. Method are multiclass boosting, Multiclass boosting, boosting, and AdaBoost. Metric is weak learner ’s accuracy parameter. ","This paper studies the problem of learning weak and moderately inaccurate hypotheses in the context of multiclass boosting. The authors propose a new algorithm called AdaBoost, which is based on the idea that the weak learner can be considered as an agnostic PAC learner and the booster can be seen as a weak-learner. They show that the proposed algorithm outperforms existing methods in terms of the classification loss. They also show that AdaBoost can be used to improve the performance of weak learners.","This paper studies the problem of learning weak and moderately inaccurate hypotheses in the context of multiclass boosting. The authors propose a new algorithm called AdaBoost, which is based on the idea that the weak learner can be considered as an agnostic PAC learner and the booster can be seen as a weak-learner. They show that the proposed algorithm outperforms existing methods in terms of the classification loss. They also show that AdaBoost can be used to improve the performance of weak learners."
12901,SP:f63b050773871338c48b778c362172e4b72477a4,"methods USED-FOR unsupervised object segmentation. methods USED-FOR interpretable object - centric scene generation. unsupervised object segmentation CONJUNCTION interpretable object - centric scene generation. interpretable object - centric scene generation CONJUNCTION unsupervised object segmentation. limited visual complexity FEATURE-OF simulated and real - world datasets. simulated and real - world datasets EVALUATE-FOR methods. RNNs USED-FOR object representations. paradigms COMPARE embedding - based approach. embedding - based approach COMPARE paradigms. clustering procedure USED-FOR randomly ordered object representations. iterative refinement COMPARE clustering procedure. clustering procedure COMPARE iterative refinement. RNNs CONJUNCTION iterative refinement. iterative refinement CONJUNCTION RNNs. GENESIS - V2 USED-FOR variable number of object representations. GENESIS - V2 HYPONYM-OF model. RNNs USED-FOR variable number of object representations. iterative refinement USED-FOR variable number of object representations. GENESIS - V2 COMPARE baselines. baselines COMPARE GENESIS - V2. unsupervised image segmentation CONJUNCTION object - centric scene generation. object - centric scene generation CONJUNCTION unsupervised image segmentation. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. GENESIS - V2 USED-FOR unsupervised image segmentation. GENESIS - V2 USED-FOR object - centric scene generation. baselines USED-FOR unsupervised image segmentation. synthetic datasets USED-FOR unsupervised image segmentation. object - centric scene generation EVALUATE-FOR baselines. real - world datasets USED-FOR object - centric scene generation. synthetic datasets USED-FOR object - centric scene generation. synthetic datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR baselines. Method are unsupervised learning of object - representations, and stochastic stick - breaking process. OtherScientificTerm is unnatural ordering. ",This paper proposes a novel unsupervised learning method for object segmentation. The proposed method is based on a clustering procedure and an iterative refinement procedure. The method is evaluated on synthetic and real-world datasets. The results show that the proposed method outperforms baselines on both synthetic and synthetic datasets. ,This paper proposes a novel unsupervised learning method for object segmentation. The proposed method is based on a clustering procedure and an iterative refinement procedure. The method is evaluated on synthetic and real-world datasets. The results show that the proposed method outperforms baselines on both synthetic and synthetic datasets. 
12937,SP:408deb9e5577ee7118b836fee77135df641fe545,"black box method USED-FOR point predictions. conformal inference USED-FOR framework. conformal inference methods COMPARE adaptive approach. adaptive approach COMPARE conformal inference methods. coverage frequency EVALUATE-FOR adaptive approach. learning problem USED-FOR distribution shift. adaptive conformal inference HYPONYM-OF method. real world datasets EVALUATE-FOR adaptive conformal inference. real world datasets EVALUATE-FOR method. Generic is methods. OtherScientificTerm are data generating distribution, data generating process, and distribution shifts. ",This paper proposes an adaptive conformal inference method for point predictions. The proposed method is based on the idea that the distribution shift in the data generating process is due to distribution shift. The authors propose to learn a learning problem that is adaptive to distribution shifts. The method is evaluated on a number of real-world datasets and shows that the proposed method outperforms existing methods.,This paper proposes an adaptive conformal inference method for point predictions. The proposed method is based on the idea that the distribution shift in the data generating process is due to distribution shift. The authors propose to learn a learning problem that is adaptive to distribution shifts. The method is evaluated on a number of real-world datasets and shows that the proposed method outperforms existing methods.
12973,SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"crowded scenes USED-FOR Multi - person pose estimation. bounding box detection CONJUNCTION keypoint grouping. keypoint grouping CONJUNCTION bounding box detection. bounding box detection PART-OF direct pose - level inference strategy. keypoint grouping PART-OF direct pose - level inference strategy. Pose - level Inference Network ( PINet ) USED-FOR complete pose cues. visible body parts USED-FOR complete pose cues. Part - based Pose Generation ( PPG ) USED-FOR coarse poses. PINet USED-FOR coarse poses. Part - based Pose Generation ( PPG ) USED-FOR PINet. pose priors USED-FOR Pose Refinement module. Pose Refinement module USED-FOR coarse poses. visual body cues USED-FOR global pose cues. visual body cues USED-FOR PINet. discriminative body parts USED-FOR PINet. crowded scenes pose estimation benchmarks EVALUATE-FOR PINet. AP EVALUATE-FOR it. OCHuman dataset EVALUATE-FOR it. OtherScientificTerm are overlapping and occlusions, person bounding boxes, and pose cues. Method is Pose Fusion module. ",This paper proposes a Pose-level Inference Network (PINet) for multi-person pose estimation. PINet is based on the Pose Fusion module and Pose Refinement module. The PINet model is trained on the OCHuman dataset and is able to achieve state-of-the-art results on the AP and OCHUMAN datasets. The proposed method is evaluated on a variety of pose estimation benchmarks.,This paper proposes a Pose-level Inference Network (PINet) for multi-person pose estimation. PINet is based on the Pose Fusion module and Pose Refinement module. The PINet model is trained on the OCHuman dataset and is able to achieve state-of-the-art results on the AP and OCHUMAN datasets. The proposed method is evaluated on a variety of pose estimation benchmarks.
13022,SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,Robust Markov decision processes ( RMDPs ) PART-OF robust reinforcement learning algorithms. algorithm USED-FOR Bellman operator. Bellman operator USED-FOR S - rectangular robust Markov decision processes. L∞-constrained rectangular ambiguity sets FEATURE-OF S - rectangular robust Markov decision processes. homotopy continuation method CONJUNCTION bisection method. bisection method CONJUNCTION homotopy continuation method. algorithm USED-FOR S - rectangular ambiguity. bisection method USED-FOR S - rectangular ambiguity. homotopy continuation method USED-FOR S - rectangular ambiguity. quasi - linear time FEATURE-OF S - rectangular ambiguity. bisection method PART-OF algorithm. homotopy continuation method PART-OF algorithm. cubic time FEATURE-OF leading general linear programming methods. leading general linear programming methods USED-FOR algorithm. cubic time EVALUATE-FOR algorithm. it COMPARE leading commercial optimization package. leading commercial optimization package COMPARE it. Generic is method. ,This paper studies the problem of learning robust Markov decision processes (RMDPs) with L∞-constrained rectangular ambiguity sets. The authors propose a novel algorithm to solve the problem. The main idea is to use the Bellman operator as a regularization term for the decision process. The algorithm is based on the homotopy continuation method and bisection method. The proposed algorithm is shown to achieve quasi-linear time and cubic time in the case of S-rectangular ambiguity. ,This paper studies the problem of learning robust Markov decision processes (RMDPs) with L∞-constrained rectangular ambiguity sets. The authors propose a novel algorithm to solve the problem. The main idea is to use the Bellman operator as a regularization term for the decision process. The algorithm is based on the homotopy continuation method and bisection method. The proposed algorithm is shown to achieve quasi-linear time and cubic time in the case of S-rectangular ambiguity. 
13071,SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,machine - learned predictions USED-FOR online algorithms. generalized one - way trading CONJUNCTION two - stage online knapsack. two - stage online knapsack CONJUNCTION generalized one - way trading. competitive ratio EVALUATE-FOR online algorithms. Task is online knapsack problem. OtherScientificTerm is upper and lower bound. ,This paper studies the online knapsack problem with machine-learned predictions for generalized one-way trading and two-stage online trading. The paper provides an upper and lower bound on the competitive ratio between two online algorithms for the online k-means trading problem. The upper bound is based on the upper bound of the upper-bound of the lower bound of a previous work. The lower bound is a lower bound based on a prior work. ,This paper studies the online knapsack problem with machine-learned predictions for generalized one-way trading and two-stage online trading. The paper provides an upper and lower bound on the competitive ratio between two online algorithms for the online k-means trading problem. The upper bound is based on the upper bound of the upper-bound of the lower bound of a previous work. The lower bound is a lower bound based on a prior work. 
13120,SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"Episodic control USED-FOR reinforcement learning. model - based episodic memory of trajectories USED-FOR episodic control. memory USED-FOR agent. memory USED-FOR complementary learning model. model - based, episodic and habitual learning PART-OF architecture. dynamic hybrid control CONJUNCTION model - based, episodic and habitual learning. model - based, episodic and habitual learning CONJUNCTION dynamic hybrid control. dynamic hybrid control USED-FOR complementary learning model. model - based, episodic and habitual learning USED-FOR complementary learning model. model COMPARE reinforcement learning agents. reinforcement learning agents COMPARE model. OtherScientificTerm are episodic memory, and stochastic and non - Markovian settings. ","This paper proposes a model-based episodic memory of trajectories for reinforcement learning. The proposed method is based on the idea of dynamic hybrid control, which is an extension of the model based episodic learning framework. The authors show that the proposed method outperforms the baselines in both stochastic and non-Markovian settings. ","This paper proposes a model-based episodic memory of trajectories for reinforcement learning. The proposed method is based on the idea of dynamic hybrid control, which is an extension of the model based episodic learning framework. The authors show that the proposed method outperforms the baselines in both stochastic and non-Markovian settings. "
13169,SP:551174c1266b5f4b6aaf5432a4c713386f90898c,labeled data USED-FOR deep learning. Semi - supervised learning ( SSL ) USED-FOR unlabeled data. pseudo labels USED-FOR unlabeled data. data programming ( DP ) scheme USED-FOR probabilistic labels. probabilistic labels USED-FOR unlabeled data. DP - SSL HYPONYM-OF SSL method. data programming ( DP ) scheme USED-FOR SSL method. LFs PART-OF SSL style. DP methods USED-FOR initial labeling functions ( LFs ). human experts USED-FOR DP methods. noisy labels USED-FOR label model. probabilistic labels USED-FOR unlabeled samples. LFs USED-FOR noisy labels. DP - SSL COMPARE SSL methods. SSL methods COMPARE DP - SSL. classification EVALUATE-FOR SSL methods. SSL benchmarks EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR SSL methods. DP - SSL USED-FOR unlabeled data. test sets EVALUATE-FOR classification. classification EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR DP - SSL. CIFAR-10 EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR test data. unlabeled data EVALUATE-FOR DP - SSL. test data EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR DP - SSL. annotation accuracy EVALUATE-FOR DP - SSL. Method is SSL. Material is labeled samples. ,"This paper proposes DP-SSL, a semi-supervised learning method for learning pseudo-labels for unlabeled data. The proposed method is based on the data programming (DP) scheme, which is used to learn a probabilistic label model that can be applied to noisy labels. The method is evaluated on CIFAR-10 and ImageNet.","This paper proposes DP-SSL, a semi-supervised learning method for learning pseudo-labels for unlabeled data. The proposed method is based on the data programming (DP) scheme, which is used to learn a probabilistic label model that can be applied to noisy labels. The method is evaluated on CIFAR-10 and ImageNet."
13218,SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"Multi - view Pose transformer ( MvP ) USED-FOR estimating multi - person 3D poses. multi - view images USED-FOR Multi - view Pose transformer ( MvP ). multi - view images USED-FOR estimating multi - person 3D poses. MvP USED-FOR multi - person 3D poses. volumetric representation USED-FOR per - person 3D pose. volumetric representation USED-FOR estimating 3D joint locations. detected 2D poses USED-FOR per - person 3D pose. query embeddings USED-FOR MvP. query embeddings USED-FOR skeleton joints. accuracy EVALUATE-FOR pipeline. hierarchical scheme USED-FOR query embeddings of multi - person skeleton joints. accuracy EVALUATE-FOR MvP. hierarchical scheme USED-FOR MvP. geometrically guided attention mechanism USED-FOR cross - view information. MvP USED-FOR geometrically guided attention mechanism. projective attention HYPONYM-OF geometrically guided attention mechanism. feature representations USED-FOR projective attention. view - dependent camera geometry PART-OF feature representations. RayConv operation USED-FOR MvP. MvP model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MvP model. it COMPARE approach. approach COMPARE it. Panoptic dataset EVALUATE-FOR AP25. AP25 EVALUATE-FOR it. Panoptic dataset EVALUATE-FOR it. MvP USED-FOR recovering human mesh. MvP USED-FOR modeling multi - person body shapes. SMPL model USED-FOR modeling multi - person body shapes. SMPL model USED-FOR recovering human mesh. Generic is intermediate tasks. OtherScientificTerm are multi - view information, 3D joint locations, human mesh, and multi - person body shapes. Method is inputdependent query adaptation approach. ","This paper proposes a multi-view Pose Transformer (MvP) model for estimating multi-person 3D poses. The proposed model is based on the idea of projective attention, which is a geometrically guided attention mechanism that maps the view-dependent camera geometry to the cross-view information. The paper also proposes a RayConv operation to improve the performance of the model. The method is evaluated on the Panoptic dataset and shows competitive performance with state-of-the-art methods.","This paper proposes a multi-view Pose Transformer (MvP) model for estimating multi-person 3D poses. The proposed model is based on the idea of projective attention, which is a geometrically guided attention mechanism that maps the view-dependent camera geometry to the cross-view information. The paper also proposes a RayConv operation to improve the performance of the model. The method is evaluated on the Panoptic dataset and shows competitive performance with state-of-the-art methods."
13267,SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"sparse vectors PART-OF family. error - free responses USED-FOR sparse vectors. support recovery CONJUNCTION approximate recovery problems. approximate recovery problems CONJUNCTION support recovery. approximate recovery problems USED-FOR problems. support recovery USED-FOR problems. 1 - bit compressed sensing USED-FOR approximate recovery problems. 1 - bit compressed sensing USED-FOR problems. learning algorithms USED-FOR problem. learning algorithms USED-FOR problem. Task is learning problems. OtherScientificTerm are noisy responses, and unknown vectors. Method is learning model. Metric is query complexity. ","This paper studies the problem of learning sparse vectors with noisy responses. The authors propose a new family of learning algorithms for this problem. The main idea is to learn a learning model that can learn sparse vectors that are error-free. They show that this learning model can be used to solve the following problems: support recovery, approximate recovery, and 1-bit compressed sensing. ","This paper studies the problem of learning sparse vectors with noisy responses. The authors propose a new family of learning algorithms for this problem. The main idea is to learn a learning model that can learn sparse vectors that are error-free. They show that this learning model can be used to solve the following problems: support recovery, approximate recovery, and 1-bit compressed sensing. "
13316,SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"sensors USED-FOR detecting abrupt changes in temporal behavior patterns. detecting abrupt changes in temporal behavior patterns FEATURE-OF industrial and security applications. sensors USED-FOR industrial and security applications. information - theoretic lower bound USED-FOR finitely parameterized probability distributions. information - theoretic lower bound FEATURE-OF detection delay. bounds COMPARE information - theoretic lower bounds. information - theoretic lower bounds COMPARE bounds. expected delay bounds USED-FOR scheme. synthetic and real datasets EVALUATE-FOR method. OtherScientificTerm are abrupt changes in temporal behavior patterns, abrupt changes, sensing actions, and exploitation of querying informative actions. Task is bandit quickest changepoint detection problem. Method is online sensing scheme. Metric is false alarm rates. ",This paper studies the problem of detecting abrupt changes in temporal behavior patterns. The authors propose an online sensing scheme to solve the problem. The proposed method is based on the information-theoretic lower bound of the detection delay for finitely parameterized probability distributions. The method is evaluated on synthetic and real datasets.,This paper studies the problem of detecting abrupt changes in temporal behavior patterns. The authors propose an online sensing scheme to solve the problem. The proposed method is based on the information-theoretic lower bound of the detection delay for finitely parameterized probability distributions. The method is evaluated on synthetic and real datasets.
13365,SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"stochastic bilevel CONJUNCTION min - max. min - max CONJUNCTION stochastic bilevel. min - max CONJUNCTION compositional optimization. compositional optimization CONJUNCTION min - max. Stochastic nested optimization USED-FOR machine learning applications. compositional optimization HYPONYM-OF Stochastic nested optimization. stochastic bilevel HYPONYM-OF Stochastic nested optimization. min - max HYPONYM-OF Stochastic nested optimization. nested structure FEATURE-OF problems. SGD - type updates USED-FOR nested problems. they COMPARE non - nested problems. non - nested problems COMPARE they. convergence rate EVALUATE-FOR they. SGD - type updates USED-FOR stochastic nested problems. ALternating Stochastic gradient dEscenT ( ALSET ) method HYPONYM-OF SGD approach. ALSET USED-FOR stochastic nested problems. hidden smoothness FEATURE-OF problem. SGD - type algorithms USED-FOR stochastic nested problems. Method is problem - specific algorithms and analyses. Generic are analysis, and it. OtherScientificTerm are nested problem, and regularity conditions. Metric is sample complexity. ","This paper studies the problem of stochastic nested optimization, which is a special case of the bilevel optimization problem. In particular, the authors consider the case where the problem has a hidden smoothness condition. The authors propose an algorithm called ALternating Stochastic Gradient dEscenT (ALSET) to solve this problem. The algorithm is based on SGD-type updates, and the authors show that it converges to the optimal solution of the nested problem. They also show that the algorithm can be applied to non-nested problems as well.","This paper studies the problem of stochastic nested optimization, which is a special case of the bilevel optimization problem. In particular, the authors consider the case where the problem has a hidden smoothness condition. The authors propose an algorithm called ALternating Stochastic Gradient dEscenT (ALSET) to solve this problem. The algorithm is based on SGD-type updates, and the authors show that it converges to the optimal solution of the nested problem. They also show that the algorithm can be applied to non-nested problems as well."
13414,SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"supervised learning USED-FOR transformer - based model. siamese sampling mechanism USED-FOR sparse and similar clips. interdependent knowledge PART-OF network. reasoning strategy USED-FOR interdependent knowledge. interdependent knowledge PART-OF network inference. siamese clips HYPONYM-OF sparse and similar clips. Siamese Sampling and Reasoning ( SiaSamRea ) approach USED-FOR interdependent knowledge. reasoning strategy PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. reasoning strategy USED-FOR network inference. siamese sampling mechanism PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. siamese knowledge reasoning USED-FOR soft label. siamese knowledge reasoning HYPONYM-OF modules. siamese knowledge generation HYPONYM-OF modules. modules PART-OF reasoning strategy. siamese knowledge reasoning PART-OF reasoning strategy. siamese knowledge generation PART-OF reasoning strategy. SiaSamRea USED-FOR multimodal reasoning paradigm. ActivityNet - QA CONJUNCTION How2QA. How2QA CONJUNCTION ActivityNet - QA. How2QA CONJUNCTION TGIF - QA. TGIF - QA CONJUNCTION How2QA. MSVD - QA CONJUNCTION ActivityNet - QA. ActivityNet - QA CONJUNCTION MSVD - QA. MSRVTT - QA CONJUNCTION MSVD - QA. MSVD - QA CONJUNCTION MSRVTT - QA. VideoQA benchmarks EVALUATE-FOR SiaSamRea. Task is VideoQA ) task. OtherScientificTerm are inter - relationship, and soft labels. ","This paper proposes a new method for multi-modal reasoning in video QA. The method is based on siamese sampling and reasoning, which is an extension of the Siamese Sampling and Reasoning (SiaSamRea) framework. The key idea is to learn a soft label for each video clip, and then use the soft label as a sample from the network. The authors show that the proposed method is able to achieve better performance than previous methods on a variety of video classification tasks. ","This paper proposes a new method for multi-modal reasoning in video QA. The method is based on siamese sampling and reasoning, which is an extension of the Siamese Sampling and Reasoning (SiaSamRea) framework. The key idea is to learn a soft label for each video clip, and then use the soft label as a sample from the network. The authors show that the proposed method is able to achieve better performance than previous methods on a variety of video classification tasks. "
13463,SP:160022e2cd61159da92f92e85520b7062a337a8d,"Structured distributions USED-FOR latent probabilistic representations. observed data USED-FOR latent probabilistic representations. computational and memory complexity EVALUATE-FOR latent representations. Hidden Markov Models ( HMMs ) CONJUNCTION Probabilistic Context - Free Grammars ( PCFGs ). Probabilistic Context - Free Grammars ( PCFGs ) CONJUNCTION Hidden Markov Models ( HMMs ). Probabilistic Context - Free Grammars ( PCFGs ) HYPONYM-OF models. Hidden Markov Models ( HMMs ) HYPONYM-OF models. computational and memory complexity EVALUATE-FOR structured models. approach USED-FOR structured models. computational and memory complexity EVALUATE-FOR approach. rank USED-FOR speed. matrix - vector product USED-FOR central inference step. polyphonic music modeling CONJUNCTION unsupervised grammar induction. unsupervised grammar induction CONJUNCTION polyphonic music modeling. language modeling CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION language modeling. neural parameterized structured models USED-FOR language modeling. unsupervised grammar induction CONJUNCTION video modeling. video modeling CONJUNCTION unsupervised grammar induction. neural parameterized structured models USED-FOR polyphonic music modeling. accuracy EVALUATE-FOR models. approach COMPARE models. models COMPARE approach. neural parameterized structured models EVALUATE-FOR approach. neural parameterized structured models USED-FOR unsupervised grammar induction. unsupervised grammar induction EVALUATE-FOR approach. large state spaces FEATURE-OF models. accuracy EVALUATE-FOR approach. OtherScientificTerm are combinatorial spaces, hidden states, and low - rank constraint. ","This paper proposes a new method for learning latent representations for structured models. The proposed method is based on a low-rank constraint on the matrix-vector product of the central inference step. The authors show that the proposed method outperforms existing methods in terms of computational and memory complexity. The method is evaluated on unsupervised grammar induction, video modeling, and language modeling.","This paper proposes a new method for learning latent representations for structured models. The proposed method is based on a low-rank constraint on the matrix-vector product of the central inference step. The authors show that the proposed method outperforms existing methods in terms of computational and memory complexity. The method is evaluated on unsupervised grammar induction, video modeling, and language modeling."
13512,SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"exploration USED-FOR Reinforcement Learning. Thompson Sampling HYPONYM-OF Bayesian exploration strategies. technique USED-FOR complex environments. computational intractability FEATURE-OF probability distributions. deep neural network models CONJUNCTION approximate posterior methods. approximate posterior methods CONJUNCTION deep neural network models. approximation techniques USED-FOR exploration - exploitation trade - offs. Sample Average Uncertainty ( SAU ) HYPONYM-OF uncertainty measure. uncertainty measure USED-FOR contextual bandits. SAU HYPONYM-OF frequentist approach. Bayesian approaches USED-FOR outcomes uncertainty. Thompson Sampling HYPONYM-OF Bayesian approaches. SAU USED-FOR uncertainty measure. SAU USED-FOR deep contextual bandits. drop - in replacement USED-FOR epsilongreedy exploration. drop - in replacement USED-FOR deep contextual bandits. SAU - based exploration COMPARE deep Bayesian bandit methods. deep Bayesian bandit methods COMPARE SAU - based exploration. modest computation cost EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR SAU - based exploration. Task is exploration - exploitation dilemma. OtherScientificTerm are action - value function, value predictions, and regret bounds. Method are outcome models, and outcome model. Metric is complexity. Material is deep bandit scenario. ","This paper proposes a new uncertainty measure, Sample Average Uncertainty (SAU), for Bayesian exploration in the context of contextual bandits. SAU is a measure of the uncertainty of an action-value function, which can be used to measure the complexity of the exploration-exploitation trade-off. The authors show that SAU-based exploration is computationally efficient compared to Thompson Sampling and Bayesian bandit methods. They also show that the proposed SAU can be applied to epsilongreedy exploration.","This paper proposes a new uncertainty measure, Sample Average Uncertainty (SAU), for Bayesian exploration in the context of contextual bandits. SAU is a measure of the uncertainty of an action-value function, which can be used to measure the complexity of the exploration-exploitation trade-off. The authors show that SAU-based exploration is computationally efficient compared to Thompson Sampling and Bayesian bandit methods. They also show that the proposed SAU can be applied to epsilongreedy exploration."
13561,SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"behavior CONJUNCTION neural activity. neural activity CONJUNCTION behavior. deep learning USED-FOR automated analysis of behavior. computer vision CONJUNCTION deep learning. deep learning CONJUNCTION computer vision. computer vision USED-FOR automated analysis of behavior. Disentangled Behavior Embedding ( DBE ) USED-FOR robust behavioral embeddings. DBE CONJUNCTION stochastic temporal model. stochastic temporal model CONJUNCTION DBE. end - to - end approach USED-FOR discrete behavior representations. models USED-FOR consistent behavior representations. dynamic behavioral factors ( pose ) PART-OF deep autoencoder. temporal structures of pose dynamics USED-FOR models. fine - grained behavioral motif generation CONJUNCTION behavior decoding. behavior decoding CONJUNCTION fine - grained behavioral motif generation. approaches COMPARE DBE. DBE COMPARE approaches. approaches COMPARE VDBE. VDBE COMPARE approaches. DBE CONJUNCTION VDBE. VDBE CONJUNCTION DBE. DBE USED-FOR tasks. tasks EVALUATE-FOR VDBE. tasks EVALUATE-FOR approaches. behavior decoding HYPONYM-OF tasks. fine - grained behavioral motif generation HYPONYM-OF tasks. Material are neuroscience, large and high - quality video datasets, and interpretable behavioral videos. Task is motor task. ","This paper proposes a method for learning representations of dynamic behavioral factors (pose dynamics) from videos. The method is based on the Disentangled Behavior Embedding (DBE) framework, which is an end-to-end approach for learning discrete behavior representations. The proposed method is evaluated on a number of tasks, including fine-grained behavioral motif generation, behavior decoding, and behavior recognition. The results show that the proposed method outperforms existing methods on these tasks.","This paper proposes a method for learning representations of dynamic behavioral factors (pose dynamics) from videos. The method is based on the Disentangled Behavior Embedding (DBE) framework, which is an end-to-end approach for learning discrete behavior representations. The proposed method is evaluated on a number of tasks, including fine-grained behavioral motif generation, behavior decoding, and behavior recognition. The results show that the proposed method outperforms existing methods on these tasks."
13610,SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"DMTET HYPONYM-OF deep 3D conditional generative model. user guides USED-FOR DMTET. coarse voxels HYPONYM-OF user guides. hybrid 3D representation USED-FOR implicit and explicit 3D representations. DMTET USED-FOR reconstructed surface. implicit approaches COMPARE DMTET. DMTET COMPARE implicit approaches. deep 3D generative models USED-FOR explicit representations. deep 3D generative models COMPARE model. model COMPARE deep 3D generative models. meshes HYPONYM-OF explicit representations. deformable tetrahedral grid USED-FOR discretized signed distance function. implicit signed distance representation CONJUNCTION explicit surface mesh representation. explicit surface mesh representation CONJUNCTION implicit signed distance representation. discretized signed distance function CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION discretized signed distance function. differentiable marching tetrahedra layer USED-FOR implicit signed distance representation. deformable tetrahedral grid CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION deformable tetrahedral grid. deformable tetrahedral grid PART-OF DMTET. differentiable marching tetrahedra layer PART-OF DMTET. reconstruction CONJUNCTION adversarial losses. adversarial losses CONJUNCTION reconstruction. surface mesh USED-FOR adversarial losses. adversarial losses USED-FOR generation of the hierarchy of subdivisions. reconstruction USED-FOR generation of the hierarchy of subdivisions. approach USED-FOR conditional shape synthesis. coarse voxel inputs USED-FOR conditional shape synthesis. OtherScientificTerm are signed distance values, finer geometric details, arbitrary topology, and hierarchy of subdivisions. Material is complex 3D animal shapes. ","This paper proposes a deep 3D conditional generative model that combines implicit and explicit 3D representations for 3D shape synthesis. The proposed method is based on a hybrid 3D representation that combines explicit and implicit representations. The implicit representation is composed of a discretized signed distance function, a differentiable marching tetrahedra layer, and a deformable tetrahedral grid. The explicit representation consists of a surface mesh representation and an adversarial loss. The adversarial losses are used to generate the hierarchy of subdivisions and the reconstruction of the reconstructed surface. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy and robustness to adversarial attacks.","This paper proposes a deep 3D conditional generative model that combines implicit and explicit 3D representations for 3D shape synthesis. The proposed method is based on a hybrid 3D representation that combines explicit and implicit representations. The implicit representation is composed of a discretized signed distance function, a differentiable marching tetrahedra layer, and a deformable tetrahedral grid. The explicit representation consists of a surface mesh representation and an adversarial loss. The adversarial losses are used to generate the hierarchy of subdivisions and the reconstruction of the reconstructed surface. Experiments show that the proposed method outperforms the state-of-the-art in terms of accuracy and robustness to adversarial attacks."
13659,SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"information theory CONJUNCTION statistics. statistics CONJUNCTION information theory. statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. statistical dependence FEATURE-OF Mutual information ( MI ). structural properties FEATURE-OF it. sliced MI ( SMI ) USED-FOR surrogate measure of dependence. it USED-FOR structural properties. scalable computation CONJUNCTION estimation. estimation CONJUNCTION scalable computation. structural properties FEATURE-OF MI. estimation EVALUATE-FOR it. scalable computation EVALUATE-FOR it. MI COMPARE SMI. SMI COMPARE MI. deterministic transformations USED-FOR SMI. SMI USED-FOR feature extraction. processing functions of raw data USED-FOR it. independence testing CONJUNCTION feature extraction. feature extraction CONJUNCTION independence testing. MI USED-FOR high - dimensional inference. SMI COMPARE MI. MI COMPARE SMI. SMI USED-FOR high - dimensional inference. independence testing USED-FOR theory. feature extraction USED-FOR theory. Task is estimation of highdimensional MI. OtherScientificTerm are statistical scalability, and one - dimensional random projections. ","This paper proposes a surrogate measure of Mutual Information (MI) called sliced MI (SMI), which is a measure of the statistical dependence between two data points. The paper shows that sliced MI is a surrogate of mutual information (MI). The paper also shows that SMI can be used to estimate the mutual information between two points in a high-dimensional data space.  The paper provides theoretical analysis of SMI and shows that it can be applied to the estimation of high dimensional MI. ","This paper proposes a surrogate measure of Mutual Information (MI) called sliced MI (SMI), which is a measure of the statistical dependence between two data points. The paper shows that sliced MI is a surrogate of mutual information (MI). The paper also shows that SMI can be used to estimate the mutual information between two points in a high-dimensional data space.  The paper provides theoretical analysis of SMI and shows that it can be applied to the estimation of high dimensional MI. "
13708,SP:e220b348901b476c2afd95f97630fb5400582f40,"query efficiency EVALUATE-FOR myopic methods. non - myopic Bayesian optimization COMPARE myopic methods. myopic methods COMPARE non - myopic Bayesian optimization. expected improvement HYPONYM-OF myopic methods. query efficiency EVALUATE-FOR non - myopic Bayesian optimization. unreliable bruteforce derivative - free optimization USED-FOR Monte Carlo rollout acquisition function. unreliable bruteforce derivative - free optimization USED-FOR multi - step lookahead constrained BO method. sample average approximation CONJUNCTION infinitesimal perturbation analysis. infinitesimal perturbation analysis CONJUNCTION sample average approximation. reparameterization trick USED-FOR Methods. likelihoodratio - based unbiased estimator USED-FOR acquisition function optimization. 2 - OPT - C COMPARE methods. methods COMPARE 2 - OPT - C. query efficiency EVALUATE-FOR methods. query efficiency EVALUATE-FOR 2 - OPT - C. Metric is computational cost. Method is unconstrained BO methods. Material is unconstrained setting. OtherScientificTerm are constraints, sampled acquisition function surface, feasible and infeasible regions, and tight constraints. Task are constrained problems, and sequential and batch settings. ",This paper proposes a multi-step lookahead constrained BO method for Monte Carlo rollout acquisition function optimization. The proposed method is based on the use of a likelihood-ratio-based unbiased estimator to estimate the acquisition function. The authors show that the proposed method outperforms the state-of-the-art methods in both sequential and batch settings. ,This paper proposes a multi-step lookahead constrained BO method for Monte Carlo rollout acquisition function optimization. The proposed method is based on the use of a likelihood-ratio-based unbiased estimator to estimate the acquisition function. The authors show that the proposed method outperforms the state-of-the-art methods in both sequential and batch settings. 
13757,SP:51fbd861422647912f275b48861ea3c4812afdc8,scalar value functions PART-OF value network. distributional RL USED-FOR return distribution. return distribution COMPARE scalar value. scalar value COMPARE return distribution. hybrid reward architectures ( HRA ) USED-FOR source - specific value functions. hybrid reward architectures ( HRA ) USED-FOR RL. source - specific value functions USED-FOR reward. distributional RL CONJUNCTION hybrid reward architectures. hybrid reward architectures CONJUNCTION distributional RL. Multi - Dimensional Distributional DQN ( MD3QN ) USED-FOR joint return distribution. distributional RL USED-FOR joint return distribution. distributional RL USED-FOR Multi - Dimensional Distributional DQN ( MD3QN ). MD3QN USED-FOR randomness in returns. MD3QN USED-FOR rich reward correlation. joint return distribution CONJUNCTION Bellman target. Bellman target CONJUNCTION joint return distribution. Maximum Mean Discrepancy FEATURE-OF joint return distribution. Maximum Mean Discrepancy USED-FOR empirical algorithm. method USED-FOR joint return distribution. method COMPARE RL methods. RL methods COMPARE method. multi - dimensional reward functions USED-FOR control setting. richly correlated reward functions FEATURE-OF joint return distribution. control setting EVALUATE-FOR RL methods. multi - dimensional reward functions USED-FOR method. multi - dimensional reward functions USED-FOR RL methods. Method is joint distribution modeling. OtherScientificTerm is joint distributional Bellman operator. ,This paper proposes a new method for joint distributional Bellman operator (MD3QN) based on distributional RL. The authors show that the proposed method is able to capture the rich reward correlation between the joint return distribution and the Bellman target. They also show that their method can be extended to multi-dimensional reward functions. They provide empirical results to show the effectiveness of their method. ,This paper proposes a new method for joint distributional Bellman operator (MD3QN) based on distributional RL. The authors show that the proposed method is able to capture the rich reward correlation between the joint return distribution and the Bellman target. They also show that their method can be extended to multi-dimensional reward functions. They provide empirical results to show the effectiveness of their method. 
13806,SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"CorticalFlow HYPONYM-OF geometric deep - learning model. diffeomorphic transformations USED-FOR model. numeric conditions USED-FOR manifoldness. discrete resolution USED-FOR topological errors. numeric conditions USED-FOR topological errors. CorticalFlow USED-FOR brain cortical surface reconstruction. its USED-FOR brain cortical surface reconstruction. its EVALUATE-FOR CorticalFlow. computation time EVALUATE-FOR CorticalFlow. CorticalFlow USED-FOR generation of anatomically plausible surfaces. Material is 3 - dimensional image. OtherScientificTerm are template mesh ’s topological properties, and GPU memory footprint. Method are flow Ordinary Differential Equation ( ODE ) framework, and surface reconstruction methods. Task is generation of surfaces. ","This paper proposes a geometric deep learning model for cortical surface reconstruction. The proposed method is based on the flow Ordinary Differential Equation (ODE) framework, which is used to model the topological properties of topological errors. The method is evaluated on a number of tasks, and is shown to outperform state-of-the-art methods. ","This paper proposes a geometric deep learning model for cortical surface reconstruction. The proposed method is based on the flow Ordinary Differential Equation (ODE) framework, which is used to model the topological properties of topological errors. The method is evaluated on a number of tasks, and is shown to outperform state-of-the-art methods. "
13855,SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"computational cost EVALUATE-FOR models. differential privacy CONJUNCTION max information. max information CONJUNCTION differential privacy. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees FEATURE-OF non - adaptive sequences. deletion guarantees FEATURE-OF adaptive sequences. provable deletion guarantees FEATURE-OF adaptive deletion sequences. attack USED-FOR SISA algorithm. non - convex models USED-FOR adaptive deletion sequences. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. Method is Data deletion algorithms. Task is non - convex setting. OtherScientificTerm are update sequence, non - adaptive deletion sequences, and training methodologies. ","This paper studies the problem of data deletion in non-convex settings. The authors propose a new algorithm, SISA, which is based on the SISA algorithm. They show that SISA is provable for adaptive deletion sequences and non-adaptive deletion sequences. They also show that the algorithm can be used to attack the deletion guarantees of the adaptive deletion sequence. They provide a theoretical analysis of the proposed algorithm. ","This paper studies the problem of data deletion in non-convex settings. The authors propose a new algorithm, SISA, which is based on the SISA algorithm. They show that SISA is provable for adaptive deletion sequences and non-adaptive deletion sequences. They also show that the algorithm can be used to attack the deletion guarantees of the adaptive deletion sequence. They provide a theoretical analysis of the proposed algorithm. "
13904,SP:7150006590e268ab732c9be6c9048f67a377f956,epistemic uncertainty CONJUNCTION aleatoric uncertainty. aleatoric uncertainty CONJUNCTION epistemic uncertainty. prior distribution FEATURE-OF MDPs. policy optimising CVaR USED-FOR setting. aleatoric uncertainty CONJUNCTION inherent stochasticity of MDPs. inherent stochasticity of MDPs CONJUNCTION aleatoric uncertainty. prior distribution FEATURE-OF epistemic uncertainty. Monte Carlo tree search CONJUNCTION Bayesian optimisation. Bayesian optimisation CONJUNCTION Monte Carlo tree search. two - player stochastic game USED-FOR problem. Monte Carlo tree search USED-FOR approximate algorithm. Bayesian optimisation USED-FOR approximate algorithm. approach COMPARE baseline approaches. baseline approaches COMPARE approach. baseline approaches USED-FOR problem. approach USED-FOR problem. Task is risk - averse Bayes - adaptive reinforcement learning. OtherScientificTerm is conditional value at risk ( CVaR ). ,"This paper studies the problem of risk-averse Bayes-adaptive reinforcement learning (RARL) in the setting of two-player stochastic games. The authors consider the setting where each player has a conditional value at risk (CVaR) and the goal is to learn a policy that maximizes the CVaR under the assumption that the prior distribution of the MDP is the same as that of the epistemic distribution. The paper proposes a new algorithm for this setting, which is based on Monte Carlo tree search and Bayesian optimisation. Experiments show that the proposed algorithm outperforms the baselines in terms of performance.","This paper studies the problem of risk-averse Bayes-adaptive reinforcement learning (RARL) in the setting of two-player stochastic games. The authors consider the setting where each player has a conditional value at risk (CVaR) and the goal is to learn a policy that maximizes the CVaR under the assumption that the prior distribution of the MDP is the same as that of the epistemic distribution. The paper proposes a new algorithm for this setting, which is based on Monte Carlo tree search and Bayesian optimisation. Experiments show that the proposed algorithm outperforms the baselines in terms of performance."
13953,SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"binary classification data USED-FOR shallow ReLU networks. logistic loss USED-FOR shallow ReLU networks. gradient descent USED-FOR logistic loss. gradient descent USED-FOR shallow ReLU networks. sigmoid mapping USED-FOR conditional distribution. gradient descent USED-FOR population risk. early stopping USED-FOR gradient descent. complexity measure EVALUATE-FOR conditional model. local interpolation property FEATURE-OF univariate classifier. Deep networks USED-FOR arbitrary prediction problems. gradient descent USED-FOR Deep networks. constant step size FEATURE-OF vanilla gradient descent. vanilla gradient descent USED-FOR inner ( inputfacing ) weights. shallow ReLU networks HYPONYM-OF networks. induced conditional model COMPARE model. model COMPARE induced conditional model. optimality FEATURE-OF population misclassification rate. sigmoid mapping USED-FOR induced conditional model. gradient descent USED-FOR restricted conditional models. network nodes CONJUNCTION gradient descent iterations. gradient descent iterations CONJUNCTION network nodes. optimal test error FEATURE-OF noisy distributions. optimal test error EVALUATE-FOR univariate predictor. local interpolation property FEATURE-OF univariate predictor. multiplicative error property FEATURE-OF logistic loss. technique USED-FOR large network width. empirical logistic risk CONJUNCTION logistic risk. logistic risk CONJUNCTION empirical logistic risk. logistic loss CONJUNCTION empirical logistic risk. empirical logistic risk CONJUNCTION logistic loss. compactly - supported marginal FEATURE-OF Borel measure. gradient descent USED-FOR optimal test error. it USED-FOR learning task. universal approximation properties FEATURE-OF neural networks. Bayes ( convex ) risk USED-FOR conditional model. agnostic learning setting HYPONYM-OF predictors. shallow ReLU networks USED-FOR predictors. gradient descent USED-FOR shallow ReLU networks. OtherScientificTerm are data distribution, joint distribution, training time, measurable functions, training risk, data simplicity, distribution, finite sample, sphere, function f, R, and computational and statistical obstructions. Metric are Bayes risk, logistic and misclassification losses, calibration, Bayes risk R, and misclassification loss. Method are stalwart methods, infinite - width random feature model, classification calibration, and universal","This paper studies the problem of estimating the population risk of deep ReLU networks. The authors show that the Bayes risk of the classifier is bounded by the logistic loss and the empirical logistic risk. They show that under certain conditions, the population misclassification rate can be reduced to zero. They also show that for a certain classifier, the optimal test error can be obtained by minimizing the Bayesian risk. ","This paper studies the problem of estimating the population risk of deep ReLU networks. The authors show that the Bayes risk of the classifier is bounded by the logistic loss and the empirical logistic risk. They show that under certain conditions, the population misclassification rate can be reduced to zero. They also show that for a certain classifier, the optimal test error can be obtained by minimizing the Bayesian risk. "
14002,SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"misinformation campaigns USED-FOR social outcomes. misinformation campaigns USED-FOR coordinated accounts. social media FEATURE-OF coordinated accounts. coordinated group detection USED-FOR misinformation. methodology USED-FOR misinformation. methodology USED-FOR coordinated group detection. social media USED-FOR misinformation. social media FEATURE-OF sparsity of account activities. limited expressive power EVALUATE-FOR detectors. prior knowledge USED-FOR detectors. temporal logic CONJUNCTION pre - defined filtering functions. pre - defined filtering functions CONJUNCTION temporal logic. prior knowledge FEATURE-OF neural temporal point process. neural temporal point process PART-OF coordination detection framework. pre - defined filtering functions HYPONYM-OF prior knowledge. temporal logic HYPONYM-OF prior knowledge. account embedding space CONJUNCTION prior knowledge. prior knowledge CONJUNCTION account embedding space. theoretically guaranteed variational inference approach USED-FOR mean - field approximation. mean - field approximation USED-FOR it. theoretically guaranteed variational inference approach USED-FOR it. real - world dataset EVALUATE-FOR method. method COMPARE model. model COMPARE method. real - world dataset EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR method. COVID-19 Vaccine Tweets dataset EVALUATE-FOR model. COVID-19 vaccines FEATURE-OF spreading misinformation. Method is deep learning based coordination detectors. Generic are they, and distribution. OtherScientificTerm is Gibbs distribution of group assignment. Task is detection. ",This paper proposes a method for group detection of misinformation in social media. The proposed method is based on neural temporal point process (NTP) and prior knowledge. The authors show that the proposed method can be applied to both unsupervised and semi-supervised settings. Experiments are conducted on the COVID-19 Vaccine Tweets dataset to demonstrate the effectiveness of the method. ,This paper proposes a method for group detection of misinformation in social media. The proposed method is based on neural temporal point process (NTP) and prior knowledge. The authors show that the proposed method can be applied to both unsupervised and semi-supervised settings. Experiments are conducted on the COVID-19 Vaccine Tweets dataset to demonstrate the effectiveness of the method. 
14051,SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"binary classification task HYPONYM-OF model problem. unit sphere FEATURE-OF smooth curves. structure USED-FOR model problem. deep fully - connected neural network USED-FOR binary classification task. network depth COMPARE geometric properties. geometric properties COMPARE network depth. generalization guarantee EVALUATE-FOR deep networks. nonlinear data USED-FOR deep networks. fitting resource USED-FOR classification problem. network depth HYPONYM-OF fitting resource. neural tangent kernel ( NTK ) regime FEATURE-OF reduction to dynamics. convergence CONJUNCTION generalization. generalization CONJUNCTION convergence. decay properties FEATURE-OF NTK. fine - grained control USED-FOR decay properties. fine - grained control USED-FOR NTK. manifolds FEATURE-OF translationally invariant operator. smooth functions USED-FOR NTK. translationally invariant operator USED-FOR NTK. OtherScientificTerm are low - dimensional nonlinear structure, mild regularity conditions, network width, intrinsic data properties, and network. Task is engineering and scientific problems. Method is randomly - initialized gradient descent. ","This paper studies the generalization properties of neural tangent kernels (NTK) in the low-dimensional nonlinear setting. The authors show that NTK is a generalization guarantee for deep neural networks. They show that under certain regularity conditions, NTK converges to a unit sphere with a smooth function. They also show that the NTK can be decomposed into two parts. The first part is based on a translation invariant operator and the second part on a fine-grained control. ","This paper studies the generalization properties of neural tangent kernels (NTK) in the low-dimensional nonlinear setting. The authors show that NTK is a generalization guarantee for deep neural networks. They show that under certain regularity conditions, NTK converges to a unit sphere with a smooth function. They also show that the NTK can be decomposed into two parts. The first part is based on a translation invariant operator and the second part on a fine-grained control. "
14100,SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"class information PART-OF GAN. auxiliary classifier GAN HYPONYM-OF cGANs. softmax cross - entropy loss ( ACGAN ) FEATURE-OF auxiliary classifier GAN. relational information FEATURE-OF class - labeled dataset. Tiny - ImageNet CONJUNCTION CUB200. CUB200 CONJUNCTION Tiny - ImageNet. CIFAR10 CONJUNCTION Tiny - ImageNet. Tiny - ImageNet CONJUNCTION CIFAR10. CUB200 CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION CUB200. Tiny - ImageNet EVALUATE-FOR ReACGAN. CUB200 EVALUATE-FOR ReACGAN. CIFAR10 EVALUATE-FOR ReACGAN. ImageNet datasets EVALUATE-FOR ReACGAN. D2D - CE CONJUNCTION StyleGAN2 architecture. StyleGAN2 architecture CONJUNCTION D2D - CE. differentiable augmentations USED-FOR ReACGAN. software package USED-FOR representative cGANs. Model weights CONJUNCTION software package. software package CONJUNCTION Model weights. Method are Conditional Generative Adversarial Networks ( cGAN ), ACGAN, and classifier. OtherScientificTerm are diversity, and unit hypersphere. ","This paper proposes a new classifier for Conditional Generative Adversarial Networks (cGANs). The proposed method, ReACGAN, is based on softmax cross-entropy loss (ACGAN), which is an auxiliary classifier GAN. The authors show that the proposed method outperforms the state-of-the-art cGANs on CIFAR-10, CUB200, and Tiny-ImageNet datasets. ","This paper proposes a new classifier for Conditional Generative Adversarial Networks (cGANs). The proposed method, ReACGAN, is based on softmax cross-entropy loss (ACGAN), which is an auxiliary classifier GAN. The authors show that the proposed method outperforms the state-of-the-art cGANs on CIFAR-10, CUB200, and Tiny-ImageNet datasets. "
14149,SP:080e80746a87228b156408ff649ab7a17f44e92d,Policy Space Response Oracles ( PSRO ) HYPONYM-OF reinforcement learning ( RL ) algorithm. reinforcement learning ( RL ) algorithm USED-FOR two - player zero - sum games. large games FEATURE-OF approximate Nash equilibria. PSRO USED-FOR continuous actions. PSRO USED-FOR approximate Nash equilibrium. Extensive - Form Double Oracle ( XDO ) HYPONYM-OF extensive - form double oracle algorithm. extensive - form double oracle algorithm USED-FOR two - player zero - sum games. PSRO COMPARE XDO. XDO COMPARE PSRO. best responses USED-FOR XDO. deep RL USED-FOR Neural XDO ( NXDO ). deep RL USED-FOR best response. XDO COMPARE PSRO. PSRO COMPARE XDO. XDO USED-FOR approximate Nash equilibrium. XDO COMPARE CFR. CFR COMPARE XDO. Leduc poker game CONJUNCTION Oshi - Zumo. Oshi - Zumo CONJUNCTION Leduc poker game. exploitability EVALUATE-FOR CFR. exploitability EVALUATE-FOR XDO. NXDO COMPARE PSRO. PSRO COMPARE NXDO. NXDO COMPARE NFSP. NFSP COMPARE NXDO. PSRO CONJUNCTION NFSP. NFSP CONJUNCTION PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NFSP. sequential multidimensional continuous - action game EVALUATE-FOR PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NXDO. NXDO HYPONYM-OF deep RL method. deep RL method USED-FOR approximate Nash equilibrium. NXDO USED-FOR approximate Nash equilibrium. high - dimensional continuous - action sequential games FEATURE-OF approximate Nash equilibrium. OtherScientificTerm is infostates. Material is Leduc poker. ,"This paper proposes Neural XDO (NXDO), a deep RL algorithm for two-player zero-sum games. The main idea is to use deep RL to learn the best response for each player in the game, which is then used as a policy space response oracle. The authors show that the proposed method outperforms PSRO and NFSP in a number of games. ","This paper proposes Neural XDO (NXDO), a deep RL algorithm for two-player zero-sum games. The main idea is to use deep RL to learn the best response for each player in the game, which is then used as a policy space response oracle. The authors show that the proposed method outperforms PSRO and NFSP in a number of games. "
14198,SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"graph structured data USED-FOR deep neural networks. node - level unsupervised learning HYPONYM-OF nodeor graph - level supervised learning. node clustering HYPONYM-OF node - level unsupervised learning. representation complexity EVALUATE-FOR graphs. adjacency matrices USED-FOR graphs. permutation - invariant variational autoencoder USED-FOR graph structured data. model USED-FOR node order. model USED-FOR graph reconstruction. extracted representations USED-FOR downstream graph - level classification and regression. Method are graph - level unsupervised representation learning, and graph matching. ",This paper proposes a graph-level unsupervised learning method for node-level representation learning. The proposed method is based on a permutation-invariant variational autoencoder (PVAE) that learns the adjacency matrices between nodes in a graph. The authors show that the proposed method outperforms the state-of-the-art methods for node clustering and graph matching. They also show that their method can be applied to downstream graph classification and regression tasks.,This paper proposes a graph-level unsupervised learning method for node-level representation learning. The proposed method is based on a permutation-invariant variational autoencoder (PVAE) that learns the adjacency matrices between nodes in a graph. The authors show that the proposed method outperforms the state-of-the-art methods for node clustering and graph matching. They also show that their method can be applied to downstream graph classification and regression tasks.
14247,SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"limited scalability EVALUATE-FOR Graph Neural Networks ( GNNs ). subgraph USED-FOR GNN. bounded - size scope FEATURE-OF localized subgraph. critical neighbors PART-OF subgraph. GNN USED-FOR informative representation. GNN USED-FOR local neighborhood. function approximation ( GraphSAGE ) CONJUNCTION topological learning ( GIN ). topological learning ( GIN ) CONJUNCTION function approximation ( GraphSAGE ). graph signal processing ( GCN ) CONJUNCTION function approximation ( GraphSAGE ). function approximation ( GraphSAGE ) CONJUNCTION graph signal processing ( GCN ). decoupling USED-FOR GNN expressive power. graphs CONJUNCTION backbone GNN architectures. backbone GNN architectures CONJUNCTION graphs. backbone GNN architectures EVALUATE-FOR design. graphs EVALUATE-FOR design. OtherScientificTerm are graph and model sizes, model depth, receptive field, degraded expressivity, oversmoothing, neighborhood explosion, node, and global graph. Task is expensive computation. Method is GNNs. ","This paper studies the problem of oversmoothing in graph neural networks (GNNs). In particular, the authors consider the case where the number of nodes in a subgraph is bounded by the size of the subgraph. The authors propose to decouple the local neighborhood of a GNN from the global neighborhood of the same subgraph, in order to improve the expressivity of GNNs. They show that this decoupling improves the expressive power of the global GNN. They also show that the proposed method can be applied to graph signal processing (GCN) and function approximation (GraphSAGE).","This paper studies the problem of oversmoothing in graph neural networks (GNNs). In particular, the authors consider the case where the number of nodes in a subgraph is bounded by the size of the subgraph. The authors propose to decouple the local neighborhood of a GNN from the global neighborhood of the same subgraph, in order to improve the expressivity of GNNs. They show that this decoupling improves the expressive power of the global GNN. They also show that the proposed method can be applied to graph signal processing (GCN) and function approximation (GraphSAGE)."
14296,SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"Normalizing flows HYPONYM-OF latent - variable generative models. tractable likelihood FEATURE-OF latent - variable generative models. Jacobian FEATURE-OF latent - to - observable - variable transformation. linear time FEATURE-OF likelihood. nearly - singular Jacobian FEATURE-OF networks. affine couplings USED-FOR regular distributions. well - conditioned affine - coupling flows USED-FOR log - concave distribution. underdamped Langevin dynamics CONJUNCTION Hénon maps. Hénon maps CONJUNCTION underdamped Langevin dynamics. Hénon maps HYPONYM-OF structured dynamical system. underdamped Langevin dynamics HYPONYM-OF stochastic differential equation. affine coupling architectures CONJUNCTION underdamped Langevin dynamics. underdamped Langevin dynamics CONJUNCTION affine coupling architectures. symplectic diffeomorphisms FEATURE-OF structured dynamical system. iid Gaussians USED-FOR padded version of the input distribution. Gaussian padding USED-FOR normalizing flows. Method is Affine - coupling models. Generic is architecture. OtherScientificTerm are representational power, ill - conditioned Jacobians, well - conditioned affine coupling flows, and Gibbs measures. Task are universal approximation, likelihood - based training, and training affine couplings. ","This paper studies the problem of learning affine-coupling flows for latent variable generative models. The authors show that the Jacobian of the latent variable is ill-conditioned, which is a common problem in generative modeling. To address this problem, the authors propose a new normalizing flow that is well-convex and can be used to train a generative model. The proposed normalizing flows are shown to be universal approximators of underdamped Langevin dynamics, Hénon maps, and symplectic diffeomorphisms. ","This paper studies the problem of learning affine-coupling flows for latent variable generative models. The authors show that the Jacobian of the latent variable is ill-conditioned, which is a common problem in generative modeling. To address this problem, the authors propose a new normalizing flow that is well-convex and can be used to train a generative model. The proposed normalizing flows are shown to be universal approximators of underdamped Langevin dynamics, Hénon maps, and symplectic diffeomorphisms. "
14345,SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"Lagrangian problem USED-FOR coupons allocation. method USED-FOR coupons allocation policy. λ - generalization method USED-FOR policy learning process. λ USED-FOR policy learning process. offline reinforcement learning method CONJUNCTION off - policy evaluation algorithm. off - policy evaluation algorithm CONJUNCTION offline reinforcement learning method. policy learning CONJUNCTION policy evaluation. policy evaluation CONJUNCTION policy learning. offline reinforcement learning method USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy evaluation. offline reinforcement learning method USED-FOR policy evaluation. simulation platform CONJUNCTION real - world e - commerce market. real - world e - commerce market CONJUNCTION simulation platform. real - world e - commerce market EVALUATE-FOR approach. simulation platform EVALUATE-FOR approach. Task are Coupons allocation, and online e - commerce environment. Material are e - commerce market, and e - commerce platform. OtherScientificTerm are coupons, Lagrangian multiplier variable λ, and policy space. Method are coupons allocation policy learning, and λ - generalization ( BCORLE(λ ) ) framework. Metric are computation overhead, and users ’ retention rate. Generic are policy, and problem. ","This paper proposes a new algorithm for coupons allocation policy learning. The algorithm is based on BCORLE(λ) framework, which is a generalization of the Lagrangian problem to the online e-commerce setting. The main idea is to learn a coupon allocation policy that maximizes the return of the users and minimizes the cost of the policy. The paper also proposes an off-policy evaluation algorithm for policy learning and policy evaluation. Experiments show that the proposed algorithm outperforms the baselines in terms of user retention rate and computation overhead.","This paper proposes a new algorithm for coupons allocation policy learning. The algorithm is based on BCORLE(λ) framework, which is a generalization of the Lagrangian problem to the online e-commerce setting. The main idea is to learn a coupon allocation policy that maximizes the return of the users and minimizes the cost of the policy. The paper also proposes an off-policy evaluation algorithm for policy learning and policy evaluation. Experiments show that the proposed algorithm outperforms the baselines in terms of user retention rate and computation overhead."
14394,SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"local affinity FEATURE-OF label consistency. local affinity USED-FOR intrinsic structure. self regularization loss USED-FOR noisy neighbors. inherent structure USED-FOR domain adaptation. local neighbors CONJUNCTION reciprocal neighbors. reciprocal neighbors CONJUNCTION local neighbors. reciprocal neighbors CONJUNCTION expanded neighborhood. expanded neighborhood CONJUNCTION reciprocal neighbors. reciprocal neighbors USED-FOR local structure. local neighbors USED-FOR local structure. Task is Domain adaptation ( DA ). Method are DA methods, source pretrained model, and source domain classifier. Generic is method. OtherScientificTerm are affinity, and expanded neighborhoods. Material is 2D image and 3D point cloud recognition datasets. ","This paper studies the problem of Domain Adaptation (DA) in the context of label consistency. In particular, the authors propose a new method for domain adaptation based on the notion of local affinity, which is an intrinsic structure of the source domain classifier. The authors show that the proposed method can be used to improve the performance of existing DA methods in the presence of noisy neighbors. The proposed method is based on a self-regularization loss that is applied to the noisy neighbors of the target domain. The method is evaluated on two datasets, 2D image and 3D point cloud recognition.","This paper studies the problem of Domain Adaptation (DA) in the context of label consistency. In particular, the authors propose a new method for domain adaptation based on the notion of local affinity, which is an intrinsic structure of the source domain classifier. The authors show that the proposed method can be used to improve the performance of existing DA methods in the presence of noisy neighbors. The proposed method is based on a self-regularization loss that is applied to the noisy neighbors of the target domain. The method is evaluated on two datasets, 2D image and 3D point cloud recognition."
14443,SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,graph learning CONJUNCTION image / video recognition. image / video recognition CONJUNCTION graph learning. image / video recognition CONJUNCTION object detection. object detection CONJUNCTION image / video recognition. point cloud processing CONJUNCTION graph learning. graph learning CONJUNCTION point cloud processing. Learning representations from sets USED-FOR point cloud processing. point cloud processing CONJUNCTION image / video recognition. image / video recognition CONJUNCTION point cloud processing. geometrically - interpretable and generic pooling mechanism USED-FOR fixed - dimensional representation. geometrically - interpretable and generic pooling mechanism USED-FOR features. end - to - end trainable Euclidean embedding USED-FOR sliced - Wasserstein distance. end - to - end trainable Euclidean embedding USED-FOR set - structured data. method COMPARE set representation learning approaches. set representation learning approaches COMPARE method. pooling method COMPARE method. method COMPARE pooling method. point - cloud HYPONYM-OF set - structured data. set - structured data EVALUATE-FOR pooling method. OtherScientificTerm is probability distribution. ,"This paper proposes a pooling method for learning representations from set-structured data. The proposed method is based on a generalized pooling mechanism that can be applied to any fixed-dimensional representation. The authors show that the proposed method outperforms existing pooling methods on a variety of tasks, including graph learning, point cloud processing, and image/video recognition. ","This paper proposes a pooling method for learning representations from set-structured data. The proposed method is based on a generalized pooling mechanism that can be applied to any fixed-dimensional representation. The authors show that the proposed method outperforms existing pooling methods on a variety of tasks, including graph learning, point cloud processing, and image/video recognition. "
14492,SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"training stability FEATURE-OF recurrent neural networks ( RNNs ). SBO - RNN HYPONYM-OF RNNs. stochastic bilevel optimization ( SBO ) USED-FOR RNNs. feedforward and backpropagation USED-FOR lower and upper - level optimization. stochastic gradient descent ( SGD ) USED-FOR SBO problem. stochastic gradient descent ( SGD ) USED-FOR RNN. RNN USED-FOR SBO problem. benchmark datasets EVALUATE-FOR approach. OtherScientificTerm are hidden states, hyperparameters, and vanishing or exploding gradient. Material is training data. ","This paper studies the stability of RNNs in the setting of stochastic bilevel optimization (SBO). In particular, the authors consider the case where the hidden states of the RNN have vanishing or exploding gradients. The authors propose to use SGD to solve the SBO problem. The proposed method is evaluated on several benchmark datasets.","This paper studies the stability of RNNs in the setting of stochastic bilevel optimization (SBO). In particular, the authors consider the case where the hidden states of the RNN have vanishing or exploding gradients. The authors propose to use SGD to solve the SBO problem. The proposed method is evaluated on several benchmark datasets."
14541,SP:d3a4300e21ca215334f256f0467a428470548fe4,"online problem USED-FOR minimizing power consumption. algorithm USED-FOR power - saving states. energy consumption CONJUNCTION wake - up costs. wake - up costs CONJUNCTION energy consumption. wake - up costs FEATURE-OF power - saving states. energy consumption FEATURE-OF power - saving states. predicted lengths of the idle periods USED-FOR learning - augmented online algorithm. worst - case guarantee EVALUATE-FOR algorithm. algorithm USED-FOR online ski rental problem. learning augmented setting USED-FOR online ski rental problem. OtherScientificTerm is prediction error. Generic are problem, and approach. ",This paper proposes a learning-augmented online algorithm for online ski rental. The proposed algorithm is based on the prediction error of the idle periods. The authors show that the proposed algorithm can achieve the worst-case guarantee for the online skiing rental problem.,This paper proposes a learning-augmented online algorithm for online ski rental. The proposed algorithm is based on the prediction error of the idle periods. The authors show that the proposed algorithm can achieve the worst-case guarantee for the online skiing rental problem.
14590,SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,sample sizes FEATURE-OF tasks. task similarities CONJUNCTION sample complexity. sample complexity CONJUNCTION task similarities. mathematical framework USED-FOR transferability. sample complexity EVALUATE-FOR learning models. transferability FEATURE-OF multi - source transfer learning problems. optimal combining coefficients USED-FOR transferability. models USED-FOR tasks. models USED-FOR task. model complexity CONJUNCTION similarities. similarities CONJUNCTION model complexity. sample sizes CONJUNCTION model complexity. model complexity CONJUNCTION sample sizes. analytical expression USED-FOR transferability measure. sample sizes FEATURE-OF analytical expression. model complexity FEATURE-OF analytical expression. sample sizes FEATURE-OF transferability measure. model complexity FEATURE-OF transferability measure. analyses USED-FOR practical learning tasks. parameterized model USED-FOR quantifiable transferability measure. deep neural networks USED-FOR multi - source transfer learning tasks. alternating iterative algorithm USED-FOR deep neural networks. approach COMPARE transfer learning algorithms. transfer learning algorithms COMPARE approach. image classification tasks EVALUATE-FOR approach. image classification tasks EVALUATE-FOR transfer learning algorithms. transfer learning algorithms USED-FOR multi - source and few - shot scenarios. multi - source and few - shot scenarios EVALUATE-FOR approach. Method is transfer learning algorithm designs. Task is knowledge transferring mechanism. ,This paper studies the problem of multi-source transfer learning. The authors propose a mathematical framework for quantifying the transferability of learning models. The proposed method is based on the idea that the optimal combining coefficients of a learning model can be defined as a function of the sample complexity of the learning model and the similarity between the task and the target task. The method is evaluated on a number of transfer learning tasks and shows that the proposed method outperforms existing transfer learning algorithms.,This paper studies the problem of multi-source transfer learning. The authors propose a mathematical framework for quantifying the transferability of learning models. The proposed method is based on the idea that the optimal combining coefficients of a learning model can be defined as a function of the sample complexity of the learning model and the similarity between the task and the target task. The method is evaluated on a number of transfer learning tasks and shows that the proposed method outperforms existing transfer learning algorithms.
14639,SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"asymmetry FEATURE-OF search tasks. search image USED-FOR computational model. eccentricity - dependent visual recognition CONJUNCTION target - dependent top - down cues. target - dependent top - down cues CONJUNCTION eccentricity - dependent visual recognition. eccentricity - dependent visual recognition USED-FOR model. model COMPARE human behavior. human behavior COMPARE model. human behavior USED-FOR paradigmatic search tasks. asymmetry FEATURE-OF paradigmatic search tasks. paradigmatic search tasks EVALUATE-FOR model. model USED-FOR search asymmetry. ImageNet USED-FOR model. developmental diet USED-FOR model. classical perceptual properties FEATURE-OF neural network models. Task are Visual search, and visual search. OtherScientificTerm are eye movements, polarity of search asymmetry, and VisualSearchAsymmetry. Method is task - specific training. Material is natural images. ","This paper studies the problem of search asymmetry in visual search. The authors propose a method to learn a search model that is able to learn the polarity of search images. The model is trained using a developmental diet, and is evaluated on three paradigmatic search tasks: eccentricity-dependent visual recognition, target-dependent top-down cues, and visual search asynchrony. ","This paper studies the problem of search asymmetry in visual search. The authors propose a method to learn a search model that is able to learn the polarity of search images. The model is trained using a developmental diet, and is evaluated on three paradigmatic search tasks: eccentricity-dependent visual recognition, target-dependent top-down cues, and visual search asynchrony. "
14688,SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"adversarial examples USED-FOR certifiably robust models. tightness of the upper bound USED-FOR certifiably robust models. Interval Bound Propagation ( IBP ) training COMPARE models. models COMPARE Interval Bound Propagation ( IBP ) training. looser bounds USED-FOR Interval Bound Propagation ( IBP ) training. tighter bounds USED-FOR models. loss landscapes FEATURE-OF linear relaxation - based methods. tightness CONJUNCTION smoothness. smoothness CONJUNCTION tightness. tightness USED-FOR method. smoothness USED-FOR method. Method are Certifiable training, certifiable training, and certifiable training method. OtherScientificTerm are worst - case loss, and loss landscape. Generic is state - of - the - arts method. ",This paper studies the problem of certifiable training of robust models. The authors show that the tightness of the upper bound of the worst-case loss is tighter than that of the smoothness in linear relaxation-based methods. They also show that tighter bounds can be used to improve the robustness of models trained with IBP. ,This paper studies the problem of certifiable training of robust models. The authors show that the tightness of the upper bound of the worst-case loss is tighter than that of the smoothness in linear relaxation-based methods. They also show that tighter bounds can be used to improve the robustness of models trained with IBP. 
14737,SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"stochastic setting FEATURE-OF online linear regression. online ridge regression CONJUNCTION forward algorithm. forward algorithm CONJUNCTION online ridge regression. high probability regret bounds USED-FOR online ridge regression. high probability regret bounds USED-FOR forward algorithm. robustness FEATURE-OF regularization parameter. ridge USED-FOR forward algorithm. it PART-OF algorithms. linear function approximation PART-OF algorithms. it USED-FOR regret bounds. modification USED-FOR linear bandit settings. Method is online regression algorithms. OtherScientificTerm are bounded observations, boundedness assumption, and theoretical bounds. ","This paper studies the problem of online linear regression in the stochastic setting. In particular, the authors consider the online ridge regression setting, where the objective function is bounded by a bounded set of bounded observations. The authors show that the regret bounds of the forward algorithm and the ridge regression algorithm are bounded by the boundedness assumption. They also provide a theoretical bound for the regret of the ridge-based forward algorithm. ","This paper studies the problem of online linear regression in the stochastic setting. In particular, the authors consider the online ridge regression setting, where the objective function is bounded by a bounded set of bounded observations. The authors show that the regret bounds of the forward algorithm and the ridge regression algorithm are bounded by the boundedness assumption. They also provide a theoretical bound for the regret of the ridge-based forward algorithm. "
14786,SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"generative adversarial network CONJUNCTION adversarial training. adversarial training CONJUNCTION generative adversarial network. method USED-FOR setting. nonconvex - nonconcave setting FEATURE-OF minimax problems. adversarial training HYPONYM-OF minimax problems. generative adversarial network HYPONYM-OF minimax problems. two - time - scale variant PART-OF EG. slowO(1 / k ) rate FEATURE-OF squared gradient norm. smooth structured nonconvexnonconcave setting FEATURE-OF two - time - scale variant. EG+ HYPONYM-OF EG. EG+ HYPONYM-OF two - time - scale variant. slowO(1 / k ) rate EVALUATE-FOR two - time - scale variant. O(1 / k ) rate FEATURE-OF squared gradient norm. anchoring technique USED-FOR EG. EG+ CONJUNCTION EAG. EAG CONJUNCTION EG+. fast O(1 / k ) rate FEATURE-OF squared gradient norm. squared gradient norm FEATURE-OF smooth structured nonconvex - nonconcave problems. EG+ USED-FOR two - time - scale EG. negative comonotonicity condition FEATURE-OF saddle - gradient operator. fast extragradient ( FEG ) HYPONYM-OF two - time - scale EG. FEG - A HYPONYM-OF backtracking line - search version. Method are extragradient ( EG ) method, extra anchored gradient ( EAG ), and FEG. OtherScientificTerm are smooth convex - concave setting, and problem parameters. ",This paper proposes a two-time-scale variant of the extragradient (EG) method for nonconvex-nonconcave problems. The main idea is to use a saddle-gradient operator for the squared gradient norm of the problem. The authors show that the proposed method achieves O(1/k) rate for smooth convex convex nonconcaves and O(k/n) rate in smooth structured nonconformal problems. ,This paper proposes a two-time-scale variant of the extragradient (EG) method for nonconvex-nonconcave problems. The main idea is to use a saddle-gradient operator for the squared gradient norm of the problem. The authors show that the proposed method achieves O(1/k) rate for smooth convex convex nonconcaves and O(k/n) rate in smooth structured nonconformal problems. 
14835,SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"statistical data USED-FOR uniformity testing. rankings FEATURE-OF statistical data. uniform distribution COMPARE Mallows model. Mallows model COMPARE uniform distribution. pairwise statistics USED-FOR uniform distribution. pairwise statistics USED-FOR Mallows model. uniformity testing algorithm USED-FOR local DP scenario. ranking data USED-FOR binary statistics. binary statistics USED-FOR it. OtherScientificTerm are alternative class, large domain, uniformity, ✏ 0, and privacy budget parameter. Method are Mallows models, central DP algorithm, and uniformity testing algorithms. Task is Testing ranking data. ",This paper proposes a uniformity testing algorithm for ranking data. The algorithm is based on the Mallows model. The authors show that the proposed algorithm outperforms existing uniformity test algorithms in the local DP scenario. ,This paper proposes a uniformity testing algorithm for ranking data. The algorithm is based on the Mallows model. The authors show that the proposed algorithm outperforms existing uniformity test algorithms in the local DP scenario. 
14884,SP:99a835191a3ba8372e391b6d3316e9b68e543295,Greedy algorithms USED-FOR learning graphical models. worst - case exponential runtime EVALUATE-FOR greedy algorithms. greedy algorithms USED-FOR learning directed acyclic graphs. greedy scorebased algorithm USED-FOR learning DAGs. edge - greedy algorithms COMPARE approach. approach COMPARE edge - greedy algorithms. vertex - greedy USED-FOR approach. GES and hill - climbing algorithms HYPONYM-OF edge - greedy algorithms. score evaluations USED-FOR approach. polynomial - time algorithms USED-FOR learning DAG models. polynomial - time algorithms PART-OF algorithm. score - based algorithms USED-FOR order - based algorithms. Bregman divergences CONJUNCTION exponential families. exponential families CONJUNCTION Bregman divergences. score functions CONJUNCTION optimality conditions. optimality conditions CONJUNCTION score functions. algorithm USED-FOR score. Task is learning statistical models with sparse structure. Generic is they. OtherScientificTerm is DAGs. Method is DAG models. Metric is sample and computational complexity bounds. ,"This paper proposes a score-based algorithm for learning directed acyclic graphs (DAGs) with sparse structure. The proposed algorithm is based on edge-greedy algorithms. The main contribution of this paper is to introduce a score based algorithm for DAGs. The score is computed by computing the score function of a DAG with a sparse structure, which is then used to compute a score function for each edge of the DAG. The authors show that the proposed algorithm outperforms existing edge greedy algorithms in terms of sample complexity and worst-case exponential runtime.","This paper proposes a score-based algorithm for learning directed acyclic graphs (DAGs) with sparse structure. The proposed algorithm is based on edge-greedy algorithms. The main contribution of this paper is to introduce a score based algorithm for DAGs. The score is computed by computing the score function of a DAG with a sparse structure, which is then used to compute a score function for each edge of the DAG. The authors show that the proposed algorithm outperforms existing edge greedy algorithms in terms of sample complexity and worst-case exponential runtime."
14933,SP:b60989706296b963b6671c01f22384978a334be1,"accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. adversarial robustness EVALUATE-FOR CNNs. adversarial training USED-FOR CNNs. adversarial training USED-FOR adversarial robustness. adversarial robustness FEATURE-OF backbone CNNs. dilation architecture USED-FOR backbone CNN. accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. real - world datasets CONJUNCTION benchmark neural networks. benchmark neural networks CONJUNCTION real - world datasets. real - world datasets EVALUATE-FOR algorithm. benchmark neural networks EVALUATE-FOR algorithm. adversarial robustness EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. Method are convolutional neural networks ( CNNs ), and neural architecture dilation algorithm. Generic is they. OtherScientificTerm are adversarial attacks, and minimal computational overhead. ",This paper proposes a neural architecture dilation algorithm to improve the robustness of backbone CNNs against adversarial attacks. The authors show that the proposed method can achieve better adversarial robustness compared to existing methods. The proposed method is evaluated on a variety of benchmark datasets.,This paper proposes a neural architecture dilation algorithm to improve the robustness of backbone CNNs against adversarial attacks. The authors show that the proposed method can achieve better adversarial robustness compared to existing methods. The proposed method is evaluated on a variety of benchmark datasets.
14982,SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"linear function approximation USED-FOR episodic Markov decision processes ( MDPs ). model - based reward - free reinforcement learning USED-FOR episodic Markov decision processes ( MDPs ). linear function approximation USED-FOR model - based reward - free reinforcement learning. transition probability kernel PART-OF MDP. feature mappings FEATURE-OF linear function. Linear Mixture MDP assumption USED-FOR algorithm. linear function USED-FOR transition probability kernel. reward function USED-FOR ε - optimal policy. UCRL - RFE USED-FOR ε - optimal policy. Bernstein - type bonus USED-FOR UCRL - RFE. upper bound COMPARE lower bound. lower bound COMPARE upper bound. Task are planning phase, and exploration phase. Generic is policy. OtherScientificTerm is feature mapping. Method are linear Mixture MDPs, and reward - free algorithm. ","This paper studies the problem of reward-free reinforcement learning for linear Mixture MDPs. In particular, the authors consider the case where the transition probability kernel of the MDP is approximated by a linear function and the reward function is learned by a model-based reward-based RL algorithm. The authors provide a lower bound for the UCRL-RFE algorithm, which shows that the optimal policy can be obtained by maximizing the reward of the policy in the planning phase, and minimizing the reward in the exploration phase. ","This paper studies the problem of reward-free reinforcement learning for linear Mixture MDPs. In particular, the authors consider the case where the transition probability kernel of the MDP is approximated by a linear function and the reward function is learned by a model-based reward-based RL algorithm. The authors provide a lower bound for the UCRL-RFE algorithm, which shows that the optimal policy can be obtained by maximizing the reward of the policy in the planning phase, and minimizing the reward in the exploration phase. "
15031,SP:28563ba0975f56ddb662cd46e85de78bb6024d36,seasonal patterns FEATURE-OF data stream of events. Shifting Seasonal Matrix Factorization approach USED-FOR seasonal patterns. SSMF HYPONYM-OF Shifting Seasonal Matrix Factorization approach. it USED-FOR regime shifts. regime shifts PART-OF seasonal patterns. regime shifts USED-FOR it. lossless data compression scheme USED-FOR it. lossless data compression scheme USED-FOR method. algorithm COMPARE baseline methods. baseline methods COMPARE algorithm. real - world data streams EVALUATE-FOR baseline methods. real - world data streams EVALUATE-FOR algorithm. OtherScientificTerm is human intervention. ,This paper proposes a method for learning seasonal patterns in a data stream of events. The proposed method is based on the Shifting Seasonal Matrix Factorization (SSMF) framework. The authors show that SSMF can be used to learn seasonal patterns for a given data stream. They also show that the proposed method can be combined with a lossless data compression scheme to improve the performance of the method. ,This paper proposes a method for learning seasonal patterns in a data stream of events. The proposed method is based on the Shifting Seasonal Matrix Factorization (SSMF) framework. The authors show that SSMF can be used to learn seasonal patterns for a given data stream. They also show that the proposed method can be combined with a lossless data compression scheme to improve the performance of the method. 
15080,SP:e4bb07033001be4d04695ef058f426d49fe440be,"Assignment HYPONYM-OF informatics. exact solvers USED-FOR assignment problems. objective functions CONJUNCTION prior assumptions. prior assumptions CONJUNCTION objective functions. algorithms USED-FOR real problems. WeaveNet HYPONYM-OF neural network architecture. feature weaving layer PART-OF core module. strongly NP - hard settings USED-FOR stable matching. stable matching HYPONYM-OF non - linear assignment problems. OtherScientificTerm is NP - hardness or incomplete input. Method are approximation algorithms, learning - based 7 method, learning - based baselines, and algorithmic method. Task are real - world assignment problems, and combinatorial problem of assignment. Generic is model. ",This paper studies the problem of assignment in a combinatorial setting. The authors propose a learning-based 7-layer neural network architecture to solve the assignment problem. The proposed method is based on the WeaveNet architecture. The main contribution of the paper is to propose a new algorithm for stable matching in strongly NP-hard settings. The paper shows that the proposed algorithm can be used to solve non-linear assignment problems with incomplete input. ,This paper studies the problem of assignment in a combinatorial setting. The authors propose a learning-based 7-layer neural network architecture to solve the assignment problem. The proposed method is based on the WeaveNet architecture. The main contribution of the paper is to propose a new algorithm for stable matching in strongly NP-hard settings. The paper shows that the proposed algorithm can be used to solve non-linear assignment problems with incomplete input. 
15129,SP:8a559e21d45661eef427b310e5fe8488d5749137,3D point cloud data USED-FOR safety - critical applications. autonomous driving HYPONYM-OF safety - critical applications. robustness EVALUATE-FOR 3D deep learning models. adversarial attacks FEATURE-OF 3D deep learning models. threat models USED-FOR 3D point clouds. self - supervised learning proxy tasks USED-FOR threat models. adversarial training USED-FOR self - supervised learning proxy tasks. adversarial training USED-FOR threat models. MLP - based ( PointNet ) CONJUNCTION convolution - based ( DGCNN ). convolution - based ( DGCNN ) CONJUNCTION MLP - based ( PointNet ). convolution - based ( DGCNN ) CONJUNCTION transformer - based ( PCT ) 3D architectures. transformer - based ( PCT ) 3D architectures CONJUNCTION convolution - based ( DGCNN ). self - supervision USED-FOR 3D point cloud recognition. self - supervision COMPARE adversarial training baseline. adversarial training baseline COMPARE self - supervision. robustness EVALUATE-FOR 3D point cloud recognition. robustness EVALUATE-FOR self - supervision. it USED-FOR adversarial propagation. local feature learning USED-FOR adversarial robustness. local feature learning USED-FOR point clouds. adversarial robustness FEATURE-OF point clouds. DGCNN CONJUNCTION jigsaw proxy task. jigsaw proxy task CONJUNCTION DGCNN. jigsaw proxy task USED-FOR 3D adversarial robustness. 3D adversarial robustness EVALUATE-FOR DGCNN. OtherScientificTerm is point - level input perturbations. ,This paper studies the adversarial robustness of self-supervised learning proxy tasks for 3D point cloud data. The authors propose a new adversarial training baseline for self supervision and show that the proposed method is more robust to adversarial perturbations compared to the existing self supervision baseline. They also show that adversarial attacks can be used to improve the performance of 3D deep learning models. ,This paper studies the adversarial robustness of self-supervised learning proxy tasks for 3D point cloud data. The authors propose a new adversarial training baseline for self supervision and show that the proposed method is more robust to adversarial perturbations compared to the existing self supervision baseline. They also show that adversarial attacks can be used to improve the performance of 3D deep learning models. 
15178,SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"FISTA CONJUNCTION mirror descent. mirror descent CONJUNCTION FISTA. projected Newton ’s method CONJUNCTION FISTA. FISTA CONJUNCTION projected Newton ’s method. near - optimal regret bounds CONJUNCTION convergence rates. convergence rates CONJUNCTION near - optimal regret bounds. projected Newton ’s method CONJUNCTION mirror descent. mirror descent CONJUNCTION projected Newton ’s method. O(T ) regret EVALUATE-FOR online mirror descent. near - optimal regret bounds FEATURE-OF Optimization algorithms. mirror descent HYPONYM-OF Optimization algorithms. projected Newton ’s method HYPONYM-OF Optimization algorithms. FISTA HYPONYM-OF Optimization algorithms. conditional gradient variants USED-FOR linear optimization. O(T ) regret HYPONYM-OF suboptimal rates. toolkit USED-FOR projections. discrete and continuous perspectives USED-FOR toolkit. discrete and continuous perspectives USED-FOR projections. early termination USED-FOR away - step Frank - Wolfe algorithm. runtime EVALUATE-FOR Bregman projections. OtherScientificTerm are computational bottleneck, iterative projections, and cardinality based submodular polytopes. Metric is runtime v / s convergence rates. ","This paper studies the convergence of iterative optimization algorithms in the setting of cardinality-based submodular polytopes. The authors consider the case where the cardinality of the submodularity is bounded by the number of iterations. They show that the convergence rate is O(T) and the regret is O(\sqrt{T}) under certain assumptions. They also show that under certain conditions, the convergence rates are O(t) and O(1/T) in the case of online mirror descent and FISTA. ","This paper studies the convergence of iterative optimization algorithms in the setting of cardinality-based submodular polytopes. The authors consider the case where the cardinality of the submodularity is bounded by the number of iterations. They show that the convergence rate is O(T) and the regret is O(\sqrt{T}) under certain assumptions. They also show that under certain conditions, the convergence rates are O(t) and O(1/T) in the case of online mirror descent and FISTA. "
15227,SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"natural parameters PART-OF k - parameter minimal exponential family. i.i.d. samples USED-FOR natural parameters. maximum likelihood estimator USED-FOR exponential family. finite sample guarantees USED-FOR parameter estimation. maximum likelihood estimation USED-FOR re - parameterized distribution. exponential family FEATURE-OF re - parameterized distribution. maximum likelihood estimation USED-FOR method. re - parameterized distribution USED-FOR method. Generic are it, and estimator. Metric are sample complexity, and computational complexity. ","This paper studies the problem of parameter estimation in the minimal exponential family. The authors consider the case where the parameters of the exponential family are parameterized by a re-parameterized distribution, and they show that the sample complexity of the estimator is bounded by a finite sample guarantee. They also provide a theoretical analysis of the maximum likelihood estimator for this family. ","This paper studies the problem of parameter estimation in the minimal exponential family. The authors consider the case where the parameters of the exponential family are parameterized by a re-parameterized distribution, and they show that the sample complexity of the estimator is bounded by a finite sample guarantee. They also provide a theoretical analysis of the maximum likelihood estimator for this family. "
15276,SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"differentiable renderers USED-FOR predicting intrinsic object properties. learning - based approaches USED-FOR inverse graphics. rasterization - based renderers USED-FOR learning - based approaches. rasterization CONJUNCTION ray - tracing. ray - tracing CONJUNCTION rasterization. DIBR++ HYPONYM-OF hybrid differentiable renderer. speed CONJUNCTION realism. realism CONJUNCTION speed. hybrid differentiable renderer USED-FOR photorealistic effects. ray - tracing PART-OF hybrid differentiable renderer. direct estimation CONJUNCTION spherical basis functions. spherical basis functions CONJUNCTION direct estimation. renderer USED-FOR light transport. environmental lighting and spatially - varying material models PART-OF renderer. direct estimation USED-FOR light transport. spherical basis functions USED-FOR light transport. learning frameworks USED-FOR geometry, reflectance and lighting prediction. physics - based differentiable renderers COMPARE DIB - R++. DIB - R++ COMPARE physics - based differentiable renderers. compact and expressive shading model USED-FOR DIB - R++. path tracing USED-FOR physics - based differentiable renderers. approach COMPARE rasterization - based approaches. rasterization - based approaches COMPARE approach. material editing CONJUNCTION relighting. relighting CONJUNCTION material editing. approach USED-FOR artistic applications. material and lighting disentanglement FEATURE-OF synthetic and real data. rasterization - based approaches USED-FOR artistic applications. material and lighting disentanglement EVALUATE-FOR rasterization - based approaches. relighting HYPONYM-OF artistic applications. material editing HYPONYM-OF artistic applications. material and lighting disentanglement EVALUATE-FOR approach. synthetic and real data EVALUATE-FOR approach. Method is naive lighting and material models. OtherScientificTerm are non - Lambertian, specular reflections, and ground - truth. ","This paper proposes a hybrid differentiable renderer (DIB-R++) for inverse rendering. The authors propose to combine the physics-based renderer and ray-tracing based renderer in order to improve the performance of the renderer. The proposed renderer is based on a compact and expressive shading model, which is able to capture both lighting and lighting disentanglement. The method is evaluated on synthetic and real-world data, and is shown to achieve state-of-the-art performance. ","This paper proposes a hybrid differentiable renderer (DIB-R++) for inverse rendering. The authors propose to combine the physics-based renderer and ray-tracing based renderer in order to improve the performance of the renderer. The proposed renderer is based on a compact and expressive shading model, which is able to capture both lighting and lighting disentanglement. The method is evaluated on synthetic and real-world data, and is shown to achieve state-of-the-art performance. "
15325,SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"Soft - argmax operation USED-FOR detection - based methods. soft - argmax USED-FOR neural network. sampling - argmax HYPONYM-OF differentiable training method. continuous formulation USED-FOR output distribution. continuous formulation USED-FOR differentiable sampling process. continuous formulation USED-FOR expectation. sampling - argmax USED-FOR localization tasks. soft - argmax operation USED-FOR localization tasks. sampling - argmax COMPARE soft - argmax operation. soft - argmax operation COMPARE sampling - argmax. OtherScientificTerm are differentiable manner, probability map, pixel - wise supervision, map, implicit constraints, and expectation of the localization error. Generic are model, and method. Metric is average error. ","This paper proposes a differentiable training method for detection-based methods. The proposed method is based on sampling-argmax, which is a continuous formulation of the differentiable sampling process. The authors show that the proposed method can be used for localization tasks.","This paper proposes a differentiable training method for detection-based methods. The proposed method is based on sampling-argmax, which is a continuous formulation of the differentiable sampling process. The authors show that the proposed method can be used for localization tasks."
15374,SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"Graph Contrastive Learning ( GCL ) USED-FOR generalizable representations. contrastive views USED-FOR generalizable representations. data augmentation USED-FOR contrastive views. incomplete structure information USED-FOR models learning. directional structure FEATURE-OF directed graphs. hand - picking parameters FEATURE-OF predefined contrastive views. data augmentation USED-FOR contrastive information. directional structure HYPONYM-OF intrinsic graph structural information. data augmentation USED-FOR graph structure. predefined contrastive views USED-FOR GCL. hand - picking parameters USED-FOR GCL. it USED-FOR contrastive information. Laplacian perturbation HYPONYM-OF directed graph data augmentation method. contrastive views USED-FOR directed graph contrastive learning framework. Laplacian perturbation USED-FOR contrastive views. multi - task curriculum learning USED-FOR it. model COMPARE GCL models. GCL models COMPARE model. structural features of directed graphs EVALUATE-FOR model. benchmarks EVALUATE-FOR state - of - the - art approaches. Method is message passing scheme. OtherScientificTerm are graph changing action, directed graph structure, and easy - to - difficult contrastive views. ","This paper proposes a directed graph contrastive learning (DGCL) framework for learning graph structural information. The proposed method is based on the Laplacian perturbation (LAP) method, which augments the contrastive information of directed graphs with data augmentation. The authors show that the proposed method outperforms state-of-the-art methods on a number of benchmark datasets. ","This paper proposes a directed graph contrastive learning (DGCL) framework for learning graph structural information. The proposed method is based on the Laplacian perturbation (LAP) method, which augments the contrastive information of directed graphs with data augmentation. The authors show that the proposed method outperforms state-of-the-art methods on a number of benchmark datasets. "
15423,SP:85b383d2f722f7bff438840e423f5cb4c67d5980,common interface FEATURE-OF grounded language learning environments. RTFM CONJUNCTION Messenger. Messenger CONJUNCTION RTFM. grid - world environments CONJUNCTION symbolic counterparts of visual worlds. symbolic counterparts of visual worlds CONJUNCTION grid - world environments. interpreting rich natural language USED-FOR complex scenes. interpreting rich natural language USED-FOR symbolic counterparts of visual worlds. RTFM HYPONYM-OF grid - world environments. ALFWorld HYPONYM-OF complex scenes. Messenger HYPONYM-OF grid - world environments. grid - world environments PART-OF SILG. symbolic counterparts of visual worlds PART-OF SILG. action space CONJUNCTION language specification. language specification CONJUNCTION action space. richness of observation space CONJUNCTION action space. action space CONJUNCTION richness of observation space. language specification CONJUNCTION plan complexity. plan complexity CONJUNCTION language specification. shared model architecture USED-FOR RL. recurrent state - tracking CONJUNCTION entity - centric attention. entity - centric attention CONJUNCTION recurrent state - tracking. egocentric local convolution CONJUNCTION recurrent state - tracking. recurrent state - tracking CONJUNCTION egocentric local convolution. entity - centric attention CONJUNCTION pretrained LM. pretrained LM CONJUNCTION entity - centric attention. shared model architecture USED-FOR environments. SILG USED-FOR pretrained LM. shared architecture COMPARE environment - specific architectures. environment - specific architectures COMPARE shared architecture. SILG EVALUATE-FOR models. SILG USED-FOR language grounding. Method is unified models. OtherScientificTerm is entities. Material is multi - environment benchmark. ,"This paper proposes SILG (SILG), a framework for language grounding in multi-task RL. SILG is based on the idea of using a shared model architecture to learn a common interface between different environments. The proposed framework is tested on a number of environments, including RTFM, Messenger, and ALFWorld. The authors show that SILG outperforms the baselines on a variety of tasks. ","This paper proposes SILG (SILG), a framework for language grounding in multi-task RL. SILG is based on the idea of using a shared model architecture to learn a common interface between different environments. The proposed framework is tested on a number of environments, including RTFM, Messenger, and ALFWorld. The authors show that SILG outperforms the baselines on a variety of tasks. "
15472,SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"Vision MoE ( V - MoE ) COMPARE dense networks. dense networks COMPARE Vision MoE ( V - MoE ). Vision MoE ( V - MoE ) HYPONYM-OF Vision Transformer. V - MoE COMPARE networks. networks COMPARE V - MoE. V - MoE USED-FOR image recognition. extension USED-FOR adaptive per - image compute. routing algorithm USED-FOR extension. V - MoE USED-FOR scale vision models. V - MoE USED-FOR 15B parameter model. ImageNet EVALUATE-FOR 15B parameter model. Task are Natural Language Processing, and Computer Vision. Method is vision models. ",This paper proposes a novel extension of Vision Transformer (V-MoE) to the per-image compute problem. The main idea is to use a routing algorithm to reduce the number of images needed to compute the per image compute. The authors show that the proposed method is able to achieve state-of-the-art performance on ImageNet. ,This paper proposes a novel extension of Vision Transformer (V-MoE) to the per-image compute problem. The main idea is to use a routing algorithm to reduce the number of images needed to compute the per image compute. The authors show that the proposed method is able to achieve state-of-the-art performance on ImageNet. 
15521,SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"benign optimization landscape FEATURE-OF loss function. n ( sample size ) neurons FEATURE-OF 1 - hidden - layer networks. zero training loss FEATURE-OF global minimizer. local - min or saddle points FEATURE-OF nice local region. global minimizer FEATURE-OF KKT point. projected gradient methods USED-FOR KKT points. SGD USED-FOR narrow neural nets. projected gradient methods USED-FOR narrow neural nets. projected gradient methods COMPARE SGD. SGD COMPARE projected gradient methods. projected gradient methods USED-FOR constrained formulation. Method are neural networks, narrow networks, and gradient descent. Generic is network. OtherScientificTerm are activation, expressivity, and feasible region. Task is constrained optimization formulation. ","This paper studies the problem of constrained optimization in neural networks. The authors consider the case of 1-hidden-layer networks with n (sample size) neurons. They show that the global minimizer of the loss function can be reduced to a local minimizer, which is a function of the local min and saddle points of the network. They also show that a constrained optimization formulation of the problem can be formulated using projected gradient methods. ","This paper studies the problem of constrained optimization in neural networks. The authors consider the case of 1-hidden-layer networks with n (sample size) neurons. They show that the global minimizer of the loss function can be reduced to a local minimizer, which is a function of the local min and saddle points of the network. They also show that a constrained optimization formulation of the problem can be formulated using projected gradient methods. "
15570,SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"risk measures USED-FOR risk - aware multi - armed bandit models. variance HYPONYM-OF risk measures. correlated options FEATURE-OF real - world online decision making problems. learner PART-OF CMCB. full - bandit feedback HYPONYM-OF feedback settings. full - information HYPONYM-OF feedback settings. logarithmic factors FEATURE-OF optimal regrets. matching lower bounds USED-FOR algorithms. optimal regrets FEATURE-OF algorithms. option correlation FEATURE-OF risk - aware bandits. analytical techniques USED-FOR bandit analysis. analytical techniques USED-FOR concentration. estimated covariance USED-FOR concentration. analytical techniques USED-FOR risk of selected actions. analytical techniques USED-FOR estimated covariance. sampling strategy properties USED-FOR bandit analysis. sampling strategy properties USED-FOR analytical techniques. Generic is they. OtherScientificTerm are weight vectors, random feedback, option covariance, reward observation scenarios, and covariance structures. ","This paper studies the problem of risk-aware multi-armed bandit (CMCB) in the context of online decision making. The authors consider the setting where the learner has access to a large number of options, and the goal is to maximize the risk of selected actions. They propose a new risk measure, called option covariance, which is a measure of the covariance between the probability of a given action and the probability that the action will be taken. The paper shows that the proposed risk measure can be used to estimate the concentration of actions in a bandit model. They also provide a matching lower bound for the optimal regret of an algorithm. ","This paper studies the problem of risk-aware multi-armed bandit (CMCB) in the context of online decision making. The authors consider the setting where the learner has access to a large number of options, and the goal is to maximize the risk of selected actions. They propose a new risk measure, called option covariance, which is a measure of the covariance between the probability of a given action and the probability that the action will be taken. The paper shows that the proposed risk measure can be used to estimate the concentration of actions in a bandit model. They also provide a matching lower bound for the optimal regret of an algorithm. "
15619,SP:472a90bb175b0286765c5a47b040e1a58f594a05,"r × r - dimensional PSD matrices PART-OF Positive Semidefinite ( PSD ) factorization. PSD factorizations USED-FOR semidefinite programs. quantum resources USED-FOR information theory. Nonnegative Matrix Factorization ( NMF ) problem USED-FOR PSD factorization task. algorithm USED-FOR NMFs. Multiplicative Update algorithm HYPONYM-OF algorithm. positive diagonal matrices USED-FOR non - negativity. non - commutative extension USED-FOR PSD factorizations. Lee - Seung ’s algorithm USED-FOR non - commutative extension. Matrix Multiplicative Update ( MMU ) algorithm HYPONYM-OF non - commutative extension. multiplicative update algorithm USED-FOR NMF. squared loss objective EVALUATE-FOR update scheme. blockdiagonal PSD factorizations CONJUNCTION tensor PSD factorizations. tensor PSD factorizations CONJUNCTION blockdiagonal PSD factorizations. MMU algorithm USED-FOR blockdiagonal PSD factorizations. MMU algorithm USED-FOR tensor PSD factorizations. MMU algorithm USED-FOR primitive. primitive USED-FOR blockdiagonal PSD factorizations. primitive USED-FOR tensor PSD factorizations. real and synthetic data EVALUATE-FOR method. OtherScientificTerm are r - dimensional non - negative vectors, and matrix geometric mean of appropriate PSD matrices. Generic are problem, and it. Method are PSD factorization, MajorizationMinimization framework, and Lieb ’s Concavity Theorem. ",This paper proposes a non-negative matrix factorization (NMF) algorithm for PSD factorization. The main idea is to use the matrix geometric mean of the appropriate PSD matrices to compute the non-negativity of the PSD matrix. The authors show that the MMU algorithm can be used to solve the NMF problem. They also show that it can be applied to tensor PSD and blockdiagonal PSD. ,This paper proposes a non-negative matrix factorization (NMF) algorithm for PSD factorization. The main idea is to use the matrix geometric mean of the appropriate PSD matrices to compute the non-negativity of the PSD matrix. The authors show that the MMU algorithm can be used to solve the NMF problem. They also show that it can be applied to tensor PSD and blockdiagonal PSD. 
15668,SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"Domain Generalization ( DG ) USED-FOR model. DG approaches USED-FOR domaininvariant information. DG approaches USED-FOR generalization capability. features PART-OF latent space. domain - specific representation USED-FOR generalization. meta - learning framework USED-FOR domain - specific representation. mDSDI COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE mDSDI. state - of - the - art techniques USED-FOR DG. mDSDI USED-FOR DG. domain - specific COMPARE domain - invariant. domain - invariant COMPARE domain - specific. Background - Colored - MNIST HYPONYM-OF dataset. OtherScientificTerm are domain - specific information, invariance view, and domain - invariant and domainspecific features. Generic is framework. Method is unified framework. ","This paper proposes a meta-learning framework for domain generalization (DG) based on domain-invariant and domain-specific features. The proposed framework is based on the idea of domain invariance view, which is an extension of the Domain-Invariant Domain-Specific Information (DIA) framework. The authors show that the proposed method is able to achieve state-of-the-art performance on the Background-Colored-MNIST dataset. ","This paper proposes a meta-learning framework for domain generalization (DG) based on domain-invariant and domain-specific features. The proposed framework is based on the idea of domain invariance view, which is an extension of the Domain-Invariant Domain-Specific Information (DIA) framework. The authors show that the proposed method is able to achieve state-of-the-art performance on the Background-Colored-MNIST dataset. "
15717,SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"diffusion models COMPARE generative models. generative models COMPARE diffusion models. image sample quality EVALUATE-FOR generative models. image sample quality EVALUATE-FOR diffusion models. architecture USED-FOR unconditional image synthesis. diversity CONJUNCTION fidelity. fidelity CONJUNCTION diversity. method USED-FOR diversity. gradients USED-FOR classifier. sample quality USED-FOR conditional image synthesis. classifier guidance USED-FOR sample quality. classifier USED-FOR method. gradients USED-FOR method. classifier guidance USED-FOR conditional image synthesis. classifier guidance CONJUNCTION upsampling diffusion models. upsampling diffusion models CONJUNCTION classifier guidance. FID EVALUATE-FOR classifier guidance. Material are ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512. Method is BigGAN - deep. ","This paper proposes a new method for unconditional image synthesis based on classifier guidance. The proposed method is based on the idea that classifiers can be used to improve the sample quality and diversity of the generated images. The authors show that the proposed method outperforms existing methods in terms of sample quality, diversity, and fidelity. They also show that their method is able to outperform existing methods on the FID dataset. ","This paper proposes a new method for unconditional image synthesis based on classifier guidance. The proposed method is based on the idea that classifiers can be used to improve the sample quality and diversity of the generated images. The authors show that the proposed method outperforms existing methods in terms of sample quality, diversity, and fidelity. They also show that their method is able to outperform existing methods on the FID dataset. "
15766,SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"out - of - distribution samples USED-FOR few - shot learning. unlabeled samples HYPONYM-OF out - of - distribution samples. out - of - distribution samples USED-FOR classifier. query data HYPONYM-OF in - distribution samples. approach USED-FOR inductive and transductive settings. method COMPARE pretrained networks. pretrained networks COMPARE method. architectures FEATURE-OF pretrained networks. OtherScientificTerm are irrelevant features, prototypes, and feature extractors. Task is pre - training. ","This paper proposes a new method for few-shot learning based on out-of-distribution (OOD) samples. The idea is to learn a classifier from out of distribution samples, and then use the classifier to predict the class of the next query sample. The method is based on the idea of feature extractors, and is shown to outperform the baselines in both inductive and transductive settings. Experiments are conducted on MNIST and CIFAR-10.","This paper proposes a new method for few-shot learning based on out-of-distribution (OOD) samples. The idea is to learn a classifier from out of distribution samples, and then use the classifier to predict the class of the next query sample. The method is based on the idea of feature extractors, and is shown to outperform the baselines in both inductive and transductive settings. Experiments are conducted on MNIST and CIFAR-10."
15815,SP:b1f65724926f136979829b7a6c870bc31f38f591,experience replay USED-FOR reinforcement learning. Prioritized sampling USED-FOR samples. recentness CONJUNCTION corrective feedback. corrective feedback CONJUNCTION recentness. TD error CONJUNCTION recentness. recentness CONJUNCTION TD error. criteria PART-OF prioritization. TD error HYPONYM-OF criteria. recentness HYPONYM-OF prioritization. corrective feedback HYPONYM-OF criteria. corrective feedback PART-OF prioritization. recentness HYPONYM-OF criteria. optimal prioritization strategy USED-FOR Bellman update. on - policiness CONJUNCTION Q value. Q value CONJUNCTION on - policiness. methods USED-FOR prioritization weight. ReMERN CONJUNCTION ReMERT. ReMERT CONJUNCTION ReMERN. ReMERT HYPONYM-OF methods. ReMERN HYPONYM-OF methods. ReMERT USED-FOR temporal ordering of states. ReMERN HYPONYM-OF error network. MuJoCo CONJUNCTION Atari. Atari CONJUNCTION MuJoCo. Atari CONJUNCTION Meta - World. Meta - World CONJUNCTION Atari. methods COMPARE prioritized sampling algorithms. prioritized sampling algorithms COMPARE methods. RL benchmarks EVALUATE-FOR prioritized sampling algorithms. RL benchmarks EVALUATE-FOR methods. Meta - World HYPONYM-OF RL benchmarks. MuJoCo HYPONYM-OF RL benchmarks. Atari HYPONYM-OF RL benchmarks. Metric is regret minimization objective. OtherScientificTerm is hindsight TD error. Task is sampling. Generic is strategy. ,"This paper studies the problem of prioritized sampling in reinforcement learning. Prioritized sampling is used to improve the regret minimization. The authors propose two criteria for prioritization: recentness and corrective feedback. They also propose an optimal prioritization strategy for Bellman update. Experiments are conducted on MuJoCo, Atari, and Meta-World.","This paper studies the problem of prioritized sampling in reinforcement learning. Prioritized sampling is used to improve the regret minimization. The authors propose two criteria for prioritization: recentness and corrective feedback. They also propose an optimal prioritization strategy for Bellman update. Experiments are conducted on MuJoCo, Atari, and Meta-World."
15864,SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,nonstationary environment FEATURE-OF expert advice. nonstationary environment FEATURE-OF sequential prediction. expert advice USED-FOR sequential prediction. regret bounds USED-FOR linear - time algorithm. relative entropy projection step PART-OF algorithm. projection COMPARE weight - sharing approaches. weight - sharing approaches COMPARE projection. implicit costs FEATURE-OF weight updates. algorithm USED-FOR projection step. linear time FEATURE-OF projection step. OtherScientificTerm is long - term memory guarantees. Task is portfolio optimization. ,This paper considers the problem of portfolio optimization in the nonstationary setting. The authors propose a linear-time algorithm for weight-sharing based on the relative entropy projection step. They show that the regret bound of the proposed algorithm is linear in terms of the number of weight updates. They also provide a theoretical analysis of the implicit costs of weight sharing.,This paper considers the problem of portfolio optimization in the nonstationary setting. The authors propose a linear-time algorithm for weight-sharing based on the relative entropy projection step. They show that the regret bound of the proposed algorithm is linear in terms of the number of weight updates. They also provide a theoretical analysis of the implicit costs of weight sharing.
15913,SP:b2439973063e827b3cbe92306a2fdee3286b6b44,navigational engines CONJUNCTION recommendation systems. recommendation systems CONJUNCTION navigational engines. routing applications USED-FOR recommendation systems. routing applications USED-FOR navigational engines. contextual linear bandits USED-FOR routing applications. routing applications USED-FOR variant. contextual linear bandits USED-FOR variant. algorithms USED-FOR problem. O(d log d ) regret CONJUNCTION list size poly(d ). list size poly(d ) CONJUNCTION O(d log d ) regret. O(d log d ) regret FEATURE-OF algorithm. list size poly(d ) FEATURE-OF algorithm. nearly tight algorithms USED-FOR problem. Steiner ’s formula USED-FOR centroid of a convex set. algorithmic techniques USED-FOR convex geometry. Steiner ’s formula HYPONYM-OF algorithmic techniques. OtherScientificTerm is hidden d - dimensional value. Method is cutting - plane algorithms. Metric is regret. ,"This paper studies the problem of finding the centroid of a convex set with a hidden d-dimensional value. The authors propose a new algorithm for solving this problem, which is based on the Steiner's formula. They show that the algorithm is nearly tight and achieves O(d log d) regret. They also show that their algorithm can be used to solve the problem with list size poly(d) and list size d. ","This paper studies the problem of finding the centroid of a convex set with a hidden d-dimensional value. The authors propose a new algorithm for solving this problem, which is based on the Steiner's formula. They show that the algorithm is nearly tight and achieves O(d log d) regret. They also show that their algorithm can be used to solve the problem with list size poly(d) and list size d. "
15962,SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"Automated machine learning ( AutoML ) USED-FOR data scientists. combinators USED-FOR compositional code. orthogonal combinators USED-FOR machinelearning operators. orthogonal combinators USED-FOR pipelines. machinelearning operators PART-OF pipelines. search spaces USED-FOR AutoML optimizers. hyperparameter schemas CONJUNCTION search spaces. search spaces CONJUNCTION hyperparameter schemas. translation scheme USED-FOR search spaces. hyperparameter schemas USED-FOR translation scheme. pipelines USED-FOR translation scheme. Lale HYPONYM-OF sklearn - compatible AutoML library. user study EVALUATE-FOR it. Method are machine learning, AutoML, and functional programming. OtherScientificTerm is non - compositional code changes. ","This paper proposes a new method for learning compositional code for AutoML. The authors propose to use orthogonal combinators in the search space of AutoML optimizers. The main contribution of the paper is to introduce a new search space, called Lale, which is a sklearn-compatible AutoML library. The paper also proposes a translation scheme based on the hyperparameter schemas of the search spaces. Experiments show that the proposed method outperforms existing methods in terms of user study. ","This paper proposes a new method for learning compositional code for AutoML. The authors propose to use orthogonal combinators in the search space of AutoML optimizers. The main contribution of the paper is to introduce a new search space, called Lale, which is a sklearn-compatible AutoML library. The paper also proposes a translation scheme based on the hyperparameter schemas of the search spaces. Experiments show that the proposed method outperforms existing methods in terms of user study. "
16011,SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"small datasets USED-FOR neural network weights. problem - byproblem basis FEATURE-OF pattern of sparsity. generalization CONJUNCTION interference. interference CONJUNCTION generalization. interference FEATURE-OF few - shot and continual learning problems. generalization EVALUATE-FOR selective sparsity. meta - learning USED-FOR adaptable features. inductive bias USED-FOR meta - learning systems. Method are weight initialization, learning algorithm, sparse learning, and sparse gradient descent. Metric is generalization error. OtherScientificTerm are patterned sparsity, and learning rates. ","This paper studies the generalization error of meta-learning in the context of few-shot and continual learning problems. The authors consider the problem-by-problem basis of sparsity and show that the generalisation error is correlated with the number of samples in the training set and the learning rate of the network. They also show that in the case of sparse learning, there is a pattern of patterned sparsity in the learning rates. They show that this pattern is due to the inductive bias of the learning algorithm. They then propose a method to mitigate this issue by learning a weight initialization algorithm that minimizes the sparsity. ","This paper studies the generalization error of meta-learning in the context of few-shot and continual learning problems. The authors consider the problem-by-problem basis of sparsity and show that the generalisation error is correlated with the number of samples in the training set and the learning rate of the network. They also show that in the case of sparse learning, there is a pattern of patterned sparsity in the learning rates. They show that this pattern is due to the inductive bias of the learning algorithm. They then propose a method to mitigate this issue by learning a weight initialization algorithm that minimizes the sparsity. "
16060,SP:05037e1850003a725a466b64d3e32aa2aed458fb,"shared response modeling HYPONYM-OF multi - view learning problem. multi - set canonical correlation analysis USED-FOR unmixing matrices. sampling noise USED-FOR Multiset CCA. joint diagonalization USED-FOR approach. joint diagonalization USED-FOR Multiset CCA. ShICA - ML HYPONYM-OF maximum - likelihood method. non - Gaussianity USED-FOR ShICA - J. maximum - likelihood method USED-FOR non - Gaussianity. maximum - likelihood method USED-FOR ShICA - J. second - order statistics USED-FOR ShICA - J. method USED-FOR shared components estimation. ShICA USED-FOR shared components estimation. ShICA USED-FOR method. ShICA COMPARE alternatives. alternatives COMPARE ShICA. fMRI and MEG datasets EVALUATE-FOR ShICA. OtherScientificTerm are common components, shared independent components, additive Gaussian noise, and noise variances. Method is Shared Independent Component Analysis ( ShICA ). Generic is model. ","This paper proposes a new method for multi-view learning based on the multi-set canonical correlation analysis (CCA) framework. The proposed method, called Shared Independent Component Analysis (ShICA-J), is based on joint diagonalization (JDA) and maximum likelihood (MLL). The authors show that the proposed method outperforms existing methods on fMRI and MEG datasets.","This paper proposes a new method for multi-view learning based on the multi-set canonical correlation analysis (CCA) framework. The proposed method, called Shared Independent Component Analysis (ShICA-J), is based on joint diagonalization (JDA) and maximum likelihood (MLL). The authors show that the proposed method outperforms existing methods on fMRI and MEG datasets."
16109,SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"self - play ( SP ) CONJUNCTION population play ( PP ). population play ( PP ) CONJUNCTION self - play ( SP ). multi - agent reinforcement learning techniques USED-FOR agents. population play ( PP ) HYPONYM-OF multi - agent reinforcement learning techniques. self - play ( SP ) HYPONYM-OF multi - agent reinforcement learning techniques. model USED-FOR human - aware ” agents. behavioral cloning play ” CONJUNCTION BCP. BCP CONJUNCTION behavioral cloning play ”. behavioral cloning play ” HYPONYM-OF human - aware ” agents. BCP HYPONYM-OF human - aware ” agents. behavioral cloning USED-FOR human model. generalization EVALUATE-FOR agents. agents USED-FOR human co - players. approach USED-FOR agents. generalization EVALUATE-FOR approach. multi - agent approaches USED-FOR competitive domains. self - play agents USED-FOR agent partner. FCP agents COMPARE SP. SP COMPARE FCP agents. SP CONJUNCTION PP. PP CONJUNCTION SP. PP CONJUNCTION BCP. BCP CONJUNCTION PP. FCP agents COMPARE BCP. BCP COMPARE FCP agents. FCP agents COMPARE PP. PP COMPARE FCP agents. Material is human data. Generic are it, and method. Method are Fictitious Co - Play ( FCP ), and two - player collaborative cooking simulator. OtherScientificTerm is subjective preference. ","This paper proposes a new multi-agent reinforcement learning method for two-player collaborative cooking simulators. The proposed method is based on behavioral cloning play (BCP) and Fictitious Co-Play (FCP). BCP is an extension of behavioral cloning, and FCP is a variant of self-play (SP) and population play (PP). The authors show that the proposed method outperforms the state-of-the-art in terms of generalization and generalization to new environments. The authors also show that FCP agents are able to generalize better than BCP and SP agents.","This paper proposes a new multi-agent reinforcement learning method for two-player collaborative cooking simulators. The proposed method is based on behavioral cloning play (BCP) and Fictitious Co-Play (FCP). BCP is an extension of behavioral cloning, and FCP is a variant of self-play (SP) and population play (PP). The authors show that the proposed method outperforms the state-of-the-art in terms of generalization and generalization to new environments. The authors also show that FCP agents are able to generalize better than BCP and SP agents."
16158,SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"method USED-FOR cooperative multi - agent reinforcement learning. discrete and continuous action spaces FEATURE-OF cooperative multi - agent reinforcement learning. deep deterministic policy gradients USED-FOR policies. approach USED-FOR policies. MADDPG HYPONYM-OF multi - agent actor - critic method. deep deterministic policy gradients USED-FOR approach. QMIX HYPONYM-OF multi - agent Q - learning algorithm. FACMAC USED-FOR centralised but factored critic. per - agent utilities CONJUNCTION joint action - value function. joint action - value function CONJUNCTION per - agent utilities. joint action - value function FEATURE-OF centralised but factored critic. per - agent utilities PART-OF centralised but factored critic. non - linear monotonic function USED-FOR centralised but factored critic. non - linear monotonic function USED-FOR joint action - value function. it USED-FOR tasks. representational capacity USED-FOR it. monolithic, or monotonically factored critics USED-FOR tasks. joint action space FEATURE-OF centralised policy gradient estimator. centralised policy gradient estimator USED-FOR FACMAC. multi - agent MuJoCo benchmark CONJUNCTION StarCraft II micromanagement tasks. StarCraft II micromanagement tasks CONJUNCTION multi - agent MuJoCo benchmark. multi - agent particle environments CONJUNCTION multi - agent MuJoCo benchmark. multi - agent MuJoCo benchmark CONJUNCTION multi - agent particle environments. multi - agent MuJoCo benchmark EVALUATE-FOR FACMAC. multi - agent particle environments EVALUATE-FOR FACMAC. StarCraft II micromanagement tasks EVALUATE-FOR FACMAC. FACMAC COMPARE baselines. baselines COMPARE FACMAC. FACMAC COMPARE MADDPG. MADDPG COMPARE FACMAC. MADDPG COMPARE baselines. baselines COMPARE MADDPG. OtherScientificTerm are critic, and coordinated policy changes. Method are nonmonotonic factorisation, and centralised critic. ","This paper proposes a new method for cooperative multi-agent reinforcement learning. The proposed method, called FACMAC, uses a centralised but factored critic to estimate the policy gradient of each agent in the joint action space. The centralised policy gradient estimator is based on a non-linear monotonic function. The authors show that FACMAC outperforms MADDPG and QMIX on a number of tasks.","This paper proposes a new method for cooperative multi-agent reinforcement learning. The proposed method, called FACMAC, uses a centralised but factored critic to estimate the policy gradient of each agent in the joint action space. The centralised policy gradient estimator is based on a non-linear monotonic function. The authors show that FACMAC outperforms MADDPG and QMIX on a number of tasks."
16207,SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,Hebbian plasticity USED-FOR storage. Hebbian plasticity CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION Hebbian plasticity. storage CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION storage. Hopfield networks PART-OF neuroscience. memory - augmented neural networks USED-FOR machine learning. key - value mechanism USED-FOR memory - augmented neural networks. augmented networks COMPARE variants. variants COMPARE augmented networks. three - factor plasticity rules USED-FOR basic key - value memory. network parameters USED-FOR rules. heteroassociative memory CONJUNCTION sequence learning. sequence learning CONJUNCTION heteroassociative memory. continual recall CONJUNCTION heteroassociative memory. heteroassociative memory CONJUNCTION continual recall. network COMPARE Hopfield networks. Hopfield networks COMPARE network. network USED-FOR continual recall. network USED-FOR heteroassociative memory. Hopfield networks USED-FOR autoassociative memory tasks. network USED-FOR sequence learning. autoassociative memory tasks EVALUATE-FOR network. Hopfield network USED-FOR model of biological long - term memory. ,"This paper proposes a new memory-augmented neural network architecture that combines Hebbian plasticity, attractor dynamics, and storage. The key-value mechanism is based on the three-factor plasticity rules. The authors show that the proposed architecture is able to achieve state-of-the-art performance on a variety of tasks, including continual recall, hetero-associative memory, sequence learning, and sequence learning. ","This paper proposes a new memory-augmented neural network architecture that combines Hebbian plasticity, attractor dynamics, and storage. The key-value mechanism is based on the three-factor plasticity rules. The authors show that the proposed architecture is able to achieve state-of-the-art performance on a variety of tasks, including continual recall, hetero-associative memory, sequence learning, and sequence learning. "
16256,SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"Pairwise learning HYPONYM-OF learning tasks. bipartite ranking CONJUNCTION metric learning. metric learning CONJUNCTION bipartite ranking. It USED-FOR machine learning tasks. metric learning HYPONYM-OF machine learning tasks. bipartite ranking HYPONYM-OF machine learning tasks. approach USED-FOR streaming data. streaming data USED-FOR pairwise learning. online gradient descent ( OGD ) algorithm USED-FOR approach. stochastic and online gradient descent methods USED-FOR pairwise learning. optimization CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION optimization. stability results CONJUNCTION optimization. optimization CONJUNCTION stability results. generalization error bounds USED-FOR smooth and nonsmooth problems. generalization error bounds USED-FOR convex and nonconvex. convex and nonconvex CONJUNCTION smooth and nonsmooth problems. smooth and nonsmooth problems CONJUNCTION convex and nonconvex. optimization CONJUNCTION generalization analysis. generalization analysis CONJUNCTION optimization. techniques USED-FOR optimization. techniques USED-FOR generalization analysis. generalization bounds USED-FOR OGD. buffering set USED-FOR OGD. buffering set USED-FOR generalization bounds. algorithms USED-FOR differentially private SGD algorithms. differentially private SGD algorithms USED-FOR pairwise learning. stability analysis USED-FOR differentially private SGD algorithms. algorithms CONJUNCTION stability analysis. stability analysis CONJUNCTION algorithms. OtherScientificTerm are loss function, scalability issue, and gradient direction. Metric is storage and computational complexity. ",This paper proposes an online gradient descent (OGD) algorithm for pairwise learning with streaming data. The main idea is to use a buffering set to improve the generalization performance of the algorithm. The authors show that the proposed algorithm can be used for both convex and non-convex problems and generalization error bounds for smooth and nonsmooth problems. The paper also provides theoretical analysis of the stability of the algorithms.,This paper proposes an online gradient descent (OGD) algorithm for pairwise learning with streaming data. The main idea is to use a buffering set to improve the generalization performance of the algorithm. The authors show that the proposed algorithm can be used for both convex and non-convex problems and generalization error bounds for smooth and nonsmooth problems. The paper also provides theoretical analysis of the stability of the algorithms.
16305,SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"REDO HYPONYM-OF class - agnostic framework. class - agnostic framework USED-FOR Dynamic Objects. RGBD or calibrated videos USED-FOR REDO. RGBD or calibrated videos USED-FOR class - agnostic framework. rigid motion CONJUNCTION non - rigid motion. non - rigid motion CONJUNCTION rigid motion. non - rigid motion CONJUNCTION articulation. articulation CONJUNCTION non - rigid motion. occlusion CONJUNCTION camera settings. camera settings CONJUNCTION occlusion. unified framework USED-FOR problem setting. articulation HYPONYM-OF object dynamics. rigid motion HYPONYM-OF object dynamics. non - rigid motion HYPONYM-OF object dynamics. aggregated temporal visual cues USED-FOR canonical 4D implicit function. 4D transformation module USED-FOR object dynamics. REDO COMPARE dynamic reconstruction methods. dynamic reconstruction methods COMPARE REDO. Generic are modules, and component. Material is real - world video data 3DPW. ","This paper proposes a new framework for dynamic object reconstruction based on class-agnostic video data. The proposed method is based on a unified framework that can be applied to both RGBD and calibrated videos. The core idea is to learn a canonical 4D implicit function for object dynamics, which can be used to represent any object dynamics. The method is evaluated on three real-world video datasets (3DPW, RGBD, and calibrated video) and compared to a number of state-of-the-art methods.","This paper proposes a new framework for dynamic object reconstruction based on class-agnostic video data. The proposed method is based on a unified framework that can be applied to both RGBD and calibrated videos. The core idea is to learn a canonical 4D implicit function for object dynamics, which can be used to represent any object dynamics. The method is evaluated on three real-world video datasets (3DPW, RGBD, and calibrated video) and compared to a number of state-of-the-art methods."
16354,SP:8ae97752e74b4395774575009031abcb6ba5cea7,"fixed stepsize FEATURE-OF linear stochastic approximation ( LSA ) algorithms. methods USED-FOR machine learning tasks. high probability bounds USED-FOR LSA. covariance matrices PART-OF central limit theorems. Method are random estimates, polynomial concentration bounds, and Gaussian or exponential high probability bounds. OtherScientificTerm are products of matrices, stepsize, and random matrices. Generic is bounds. ",This paper studies the high probability bounds for linear stochastic approximation (LSA) algorithms with fixed stepsize. The main contribution of the paper is to prove the central limit theorems of the polynomial concentration bounds and Gaussian concentration bounds for LSA algorithms. These bounds are based on the assumption that the covariance matrices are products of the products of matrices and random matrices. The authors show that these bounds can be extended to the case where the stepsize is fixed. ,This paper studies the high probability bounds for linear stochastic approximation (LSA) algorithms with fixed stepsize. The main contribution of the paper is to prove the central limit theorems of the polynomial concentration bounds and Gaussian concentration bounds for LSA algorithms. These bounds are based on the assumption that the covariance matrices are products of the products of matrices and random matrices. The authors show that these bounds can be extended to the case where the stepsize is fixed. 
16403,SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"options framework USED-FOR temporal abstraction. options framework USED-FOR reinforcement learning. temporal abstraction USED-FOR reinforcement learning. discounted Markov decision processes ( MDPs ) CONJUNCTION average - reward MDPs. average - reward MDPs CONJUNCTION discounted Markov decision processes ( MDPs ). samplebased planning variants PART-OF learning algorithms. intra - option algorithms CONJUNCTION samplebased planning variants. samplebased planning variants CONJUNCTION intra - option algorithms. algorithms CONJUNCTION convergence proofs. convergence proofs CONJUNCTION algorithms. those USED-FOR algorithms. those USED-FOR convergence proofs. Four - Room domain EVALUATE-FOR algorithms. OtherScientificTerm is option - interrupting behavior. Method are discounted, and average - reward formulation. ","This paper studies the problem of option-interrupting behavior in reinforcement learning. The authors propose a new option-based option framework, which is based on discounted Markov decision processes (MDPs) and average-reward MDPs. They show that the proposed framework can be applied to a variety of RL algorithms, including intra-option algorithms and sample-based planning variants. They also provide convergence proofs for the proposed algorithms.","This paper studies the problem of option-interrupting behavior in reinforcement learning. The authors propose a new option-based option framework, which is based on discounted Markov decision processes (MDPs) and average-reward MDPs. They show that the proposed framework can be applied to a variety of RL algorithms, including intra-option algorithms and sample-based planning variants. They also provide convergence proofs for the proposed algorithms."
16452,SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"Visual Transformers ( VTs ) COMPARE Convolutional networks ( CNNs ). Convolutional networks ( CNNs ) COMPARE Visual Transformers ( VTs ). CNNs COMPARE VTs. VTs COMPARE CNNs. VTs USED-FOR global relations between image elements. representation capacity FEATURE-OF they. models COMPARE common CNNs. common CNNs COMPARE models. local properties FEATURE-OF visual domain. local properties USED-FOR VTs. local properties PART-OF CNN architectural design. small training set regime FEATURE-OF robustness. robustness EVALUATE-FOR VTs. images USED-FOR auxiliary selfsupervised task. VTs USED-FOR spatial relations. task USED-FOR VT training. task USED-FOR VTs. task CONJUNCTION ( supervised ) training. ( supervised ) training CONJUNCTION task. it PART-OF VTs. accuracy EVALUATE-FOR VTs. VTs EVALUATE-FOR method. accuracy EVALUATE-FOR method. OtherScientificTerm are convolutional inductive bias, and computational overhead. Material is ImageNet. Method is VTs - Drloc. ","This paper studies the problem of training visual transformers (VTs) in the presence of convolutional inductive bias. The authors propose a new task called Drloc, which is an auxiliary self-supervised task to improve the robustness of VTs. The proposed task is based on the observation that VTs are more robust to convolutions than CNNs, and that this is due to the local properties of the visual domain. To address this issue, the authors propose Drloc-Drloc, a method for training VTs in a small training set regime. The method is evaluated on ImageNet and CIFAR-10.","This paper studies the problem of training visual transformers (VTs) in the presence of convolutional inductive bias. The authors propose a new task called Drloc, which is an auxiliary self-supervised task to improve the robustness of VTs. The proposed task is based on the observation that VTs are more robust to convolutions than CNNs, and that this is due to the local properties of the visual domain. To address this issue, the authors propose Drloc-Drloc, a method for training VTs in a small training set regime. The method is evaluated on ImageNet and CIFAR-10."
16501,SP:0132ef17585e293b23e9dc45189c0989d829b52a,datasets USED-FOR Label - free alignment. Hyperbolic spaces USED-FOR informative representations of hierarchical data. geometric approach USED-FOR label - free alignment of hierarchical datasets. translation CONJUNCTION scaling. scaling CONJUNCTION translation. scaling CONJUNCTION rotation. rotation CONJUNCTION scaling. Riemannian geometry USED-FOR Lorentz model of hyperbolic space. Riemannian geometry USED-FOR HPA. theoretical properties CONJUNCTION stability. stability CONJUNCTION theoretical properties. stability CONJUNCTION computational efficiency. computational efficiency CONJUNCTION stability. theoretical properties EVALUATE-FOR HPA. gene expression and mass cytometry data FEATURE-OF batch correction tasks. batch correction tasks EVALUATE-FOR its. methods USED-FOR label - free alignment in hyperbolic spaces. data USED-FOR unsupervised batch effect removal. Method is hyperbolic Procrustes analysis ( HPA ). Generic is components. Task is alignment. OtherScientificTerm is hyperbolic spaces. ,This paper proposes a geometric approach for label-free alignment of hierarchical data in hyperbolic spaces. The proposed method is based on the Riemannian geometry of hyper-bolic Procrustes analysis (HPA). The authors show that the proposed method can be applied to unsupervised batch effect removal and batch correction tasks. The authors also provide theoretical properties of the proposed HPA. ,This paper proposes a geometric approach for label-free alignment of hierarchical data in hyperbolic spaces. The proposed method is based on the Riemannian geometry of hyper-bolic Procrustes analysis (HPA). The authors show that the proposed method can be applied to unsupervised batch effect removal and batch correction tasks. The authors also provide theoretical properties of the proposed HPA. 
16550,SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"Privacy - protected microdata USED-FOR differentially private algorithm. logarithmic factor FEATURE-OF accuracy. accuracy EVALUATE-FOR differentially private query answering systems. sum query CONJUNCTION point queries. point queries CONJUNCTION sum query. noisy answers USED-FOR sum query. noisy answers USED-FOR point queries. log(d ) factor COMPARE O(d ) factor. O(d ) factor COMPARE log(d ) factor. log(d ) factor USED-FOR point queries. O(d ) factor USED-FOR sum query. lower bounds USED-FOR pure, approximate, and concentrated differential privacy. Material are microdata, and benchmark datasets. Method are uncertainty principle, pure differential privacy, and mitigation strategies. OtherScientificTerm is microdata requirement. ","This paper studies the problem of differentially private query answering. The authors consider the case where the data is private, but the query answering system is not. In particular, they consider the setting where there is no privacy guarantee on the query, but there is a requirement that the answer is private. They show that for the case of pure differential privacy, there exists a lower bound on the error of the algorithm. They also provide lower bounds on the lower bound for pure, approximate, and concentrated differential privacy. ","This paper studies the problem of differentially private query answering. The authors consider the case where the data is private, but the query answering system is not. In particular, they consider the setting where there is no privacy guarantee on the query, but there is a requirement that the answer is private. They show that for the case of pure differential privacy, there exists a lower bound on the error of the algorithm. They also provide lower bounds on the lower bound for pure, approximate, and concentrated differential privacy. "
16599,SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"sparse reward CONJUNCTION inefficient exploration. inefficient exploration CONJUNCTION sparse reward. inefficient exploration FEATURE-OF long - horizon tasks. RL CONJUNCTION planning. planning CONJUNCTION RL. path - planner CONJUNCTION RL agent. RL agent CONJUNCTION path - planner. dense feedback USED-FOR curriculum of tree - structured sub - tasks. RL agent USED-FOR dense feedback. dense feedback USED-FOR path - planner. planner USED-FOR long - horizon task. easy - to - hard curriculum USED-FOR planner. bottom - up traversal of the tree USED-FOR RL agent. RL agent CONJUNCTION planner. planner CONJUNCTION RL agent. mutual training USED-FOR CO - PILOT. SAC CONJUNCTION PPO. PPO CONJUNCTION SAC. RL CONJUNCTION planning ( RRT *. planning ( RRT * CONJUNCTION RL. CO - PILOT COMPARE RL. RL COMPARE CO - PILOT. CO - PILOT COMPARE combination ( SoRB ). combination ( SoRB ) COMPARE CO - PILOT. navigation and continuous control tasks EVALUATE-FOR combination ( SoRB ). SAC HYPONYM-OF RL. navigation and continuous control tasks EVALUATE-FOR CO - PILOT. PPO HYPONYM-OF RL. success rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION success rate. sample efficiency EVALUATE-FOR CO - PILOT. success rate EVALUATE-FOR CO - PILOT. Method are Goal - conditioned reinforcement learning ( RL ), Planning, environment model, and planning policy. OtherScientificTerm are dense reward / guidance, tree of sub - tasks, sub - tasks, and RRT *. Generic is task. ","This paper proposes a method for goal-conditioned reinforcement learning (RL) and planning (RRT) that combines the benefits of both RL and planning. The main idea is to learn a curriculum of tree-structured sub-tasks, which are then used to guide the RL agent through the tree. The authors show that the proposed method outperforms the state-of-the-art methods on navigation and continuous control tasks.","This paper proposes a method for goal-conditioned reinforcement learning (RL) and planning (RRT) that combines the benefits of both RL and planning. The main idea is to learn a curriculum of tree-structured sub-tasks, which are then used to guide the RL agent through the tree. The authors show that the proposed method outperforms the state-of-the-art methods on navigation and continuous control tasks."
16648,SP:9911693a04a300b5a93634fb0267ef83e5489d77,"black box explanations USED-FOR model credibility. techniques USED-FOR explanations. hyper - parameter tuning USED-FOR methods. Bayesian framework USED-FOR generating local explanations. LIME CONJUNCTION KernelSHAP. KernelSHAP CONJUNCTION LIME. credible intervals USED-FOR feature importances. real world datasets CONJUNCTION user studies. user studies CONJUNCTION real world datasets. OtherScientificTerm are local explanations, and feature importance. Generic are framework, and uncertainty. ","This paper proposes a Bayesian framework for generating black box explanations for black box models. The proposed framework is based on the idea of local explanations, which can be seen as an extension of Bayesian Bayes. The authors show that the proposed framework can be used to generate local explanations that are robust to hyper-parameter tuning. They also show that their method can be applied to kernel-shaping and LIME. ","This paper proposes a Bayesian framework for generating black box explanations for black box models. The proposed framework is based on the idea of local explanations, which can be seen as an extension of Bayesian Bayes. The authors show that the proposed framework can be used to generate local explanations that are robust to hyper-parameter tuning. They also show that their method can be applied to kernel-shaping and LIME. "
16697,SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"energy - efficient neural networks CONJUNCTION hardware accelerations. hardware accelerations CONJUNCTION energy - efficient neural networks. multiplications PART-OF convolutional neural networks ( CNNs ). Adder neural networks ( ANNs ) USED-FOR low energy cost. ANNs COMPARE CNNs. CNNs COMPARE ANNs. accuracy EVALUATE-FOR ANNs. accuracy EVALUATE-FOR CNNs. ANNs CONJUNCTION CNNs. CNNs CONJUNCTION ANNs. knowledge distillation HYPONYM-OF training tricks. filters CONJUNCTION features. features CONJUNCTION filters. similarity measurement FEATURE-OF features. similarity measurement FEATURE-OF filters. similarity measurement USED-FOR property difference. unordered heavy tails PART-OF ANNs. classification EVALUATE-FOR ANNs. feature distributions PART-OF loss function. method USED-FOR heavy tails. angle - based constraint USED-FOR diversity of tails. method USED-FOR ANNs. heavy tails PART-OF ANNs. angle - based constraint USED-FOR distribution parameters. classifier USED-FOR method. approach USED-FOR ANNs. benchmarks EVALUATE-FOR approach. benchmarks EVALUATE-FOR distributions. OtherScientificTerm are fatter tails, feature space, Multivariate Skew Laplace distributions, and ANN features. ",This paper proposes a new method to reduce the number of heavy tails in deep neural networks (DNNs). The main idea is to use an angle-based constraint on the distribution parameters of the loss function to encourage the diversity of tails in the feature space. The authors show that the proposed method can be applied to both DNNs and CNNs. They also show that their method is able to achieve better performance than existing methods. ,This paper proposes a new method to reduce the number of heavy tails in deep neural networks (DNNs). The main idea is to use an angle-based constraint on the distribution parameters of the loss function to encourage the diversity of tails in the feature space. The authors show that the proposed method can be applied to both DNNs and CNNs. They also show that their method is able to achieve better performance than existing methods. 
16746,SP:cbccb65457564992d534504c0d060da44cafce8c,"gradient descent phenomenon USED-FOR learning proclivity. learning proclivity FEATURE-OF over - parameterized neural networks. features USED-FOR task. feature imbalances FEATURE-OF neural networks. learning dynamics USED-FOR imbalance. learning dynamics USED-FOR gradient descent. guarantees USED-FOR regularization method. formalism USED-FOR regularization method. regularization method USED-FOR feature learning dynamics. formalism USED-FOR guarantees. accuracy EVALUATE-FOR regularization method. robustness EVALUATE-FOR regularization method. OtherScientificTerm are Gradient Starvation, predictive features, statistical structure, and gradient starvation. Metric is cross - entropy loss. Method is Dynamical Systems theory. ","This paper studies the phenomenon of gradient starvation in neural networks. The authors propose a regularization method based on the Dynamical Systems theory to improve the robustness of neural networks against the gradient starvation phenomenon. In particular, the authors consider the case of over-parameterized neural networks, where the learning dynamics of the feature learning dynamics can be seen as a function of the learning proclivity of the network. They show that under certain assumptions, the regularization is guaranteed to be robust to gradient starvation. ","This paper studies the phenomenon of gradient starvation in neural networks. The authors propose a regularization method based on the Dynamical Systems theory to improve the robustness of neural networks against the gradient starvation phenomenon. In particular, the authors consider the case of over-parameterized neural networks, where the learning dynamics of the feature learning dynamics can be seen as a function of the learning proclivity of the network. They show that under certain assumptions, the regularization is guaranteed to be robust to gradient starvation. "
16795,SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"Deep reinforcement learning USED-FOR superhuman AI. superhuman AI USED-FOR competitive games. Go and StarCraft HYPONYM-OF competitive games. learning techniques USED-FOR AI teammate. AI teammate USED-FOR human - machine collaborative games. AI teammates COMPARE those. those COMPARE AI teammates. subjective metrics of trust EVALUATE-FOR those. objective team performance EVALUATE-FOR AI teammates. AI agents PART-OF cooperative card game Hanabi. interpretability CONJUNCTION trust. trust CONJUNCTION interpretability. teamwork CONJUNCTION interpretability. interpretability CONJUNCTION teamwork. interpretability HYPONYM-OF subjective measures. trust HYPONYM-OF subjective measures. teamwork HYPONYM-OF subjective measures. AI design CONJUNCTION reinforcement learning benchmarking. reinforcement learning benchmarking CONJUNCTION AI design. subjective metrics of human - AI teaming CONJUNCTION objective task performance. objective task performance CONJUNCTION subjective metrics of human - AI teaming. Method are rule - based and learning - based agents, rule - based AI teammate ( SmartBot ), and learning - based agent. OtherScientificTerm are game score, and human - AI team performance. Metric is subjective metrics. ","This paper presents a method for learning a rule-based AI teammate for cooperative multi-agent cooperative games. The proposed method is based on reinforcement learning and is able to achieve state-of-the-art performance on the Hanabi cooperative card game. The method is evaluated on a number of tasks and is shown to outperform human-AI teammates in terms of trust, interpretability, and teamwork. ","This paper presents a method for learning a rule-based AI teammate for cooperative multi-agent cooperative games. The proposed method is based on reinforcement learning and is able to achieve state-of-the-art performance on the Hanabi cooperative card game. The method is evaluated on a number of tasks and is shown to outperform human-AI teammates in terms of trust, interpretability, and teamwork. "
16844,SP:2a05e333fc1a14057515ef3addde9a40152373db,"visual question generation ( VQG ) USED-FOR human - like neural questions. image CONJUNCTION side information. side information CONJUNCTION image. side information USED-FOR human - like neural questions. image USED-FOR human - like neural questions. double visual and answer hints USED-FOR model. rule - based similarity matching method USED-FOR candidate visual hints. learning approach USED-FOR double - hints based VQG. weakly supervised learning problem USED-FOR learning approach. benchmark datasets EVALUATE-FOR method. automatic machine metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic machine metrics. method COMPARE approaches. approaches COMPARE method. benchmark datasets EVALUATE-FOR approaches. automatic machine metrics HYPONYM-OF metrics. human evaluation HYPONYM-OF metrics. human evaluation EVALUATE-FOR method. metrics EVALUATE-FOR approaches. metrics EVALUATE-FOR method. automatic machine metrics EVALUATE-FOR method. OtherScientificTerm are uninformative and non - referential questions, visual hints, salient visual regions of interest, predicted salient visual regions of interest, and quality of predicted visual hints. Generic is they. Method is generation procedure. ",This paper proposes a method for visual question generation (VQG) based on double-hints. The proposed method is based on a rule-based similarity matching method to match candidate visual hints and answer hints. The method is evaluated on a number of benchmark datasets and compared to several baselines. ,This paper proposes a method for visual question generation (VQG) based on double-hints. The proposed method is based on a rule-based similarity matching method to match candidate visual hints and answer hints. The method is evaluated on a number of benchmark datasets and compared to several baselines. 
16908,SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"Label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION Label noise. label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION label noise. Generalized Data Weighting ( GDW ) USED-FOR class imbalance. Generalized Data Weighting ( GDW ) USED-FOR label noise. class level FEATURE-OF gradients. gradients USED-FOR Generalized Data Weighting ( GDW ). GDW USED-FOR loss gradient. chain rule USED-FOR GDW. GDW COMPARE instance weighting methods. instance weighting methods COMPARE GDW. GDW USED-FOR class - level weights. computational cost EVALUATE-FOR instance weighting methods. gradient descent step USED-FOR class - level weights. class - level weights USED-FOR GDW. gradient descent step USED-FOR GDW. GDW COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GDW. uniform noise setting FEATURE-OF CIFAR10. uniform noise setting EVALUATE-FOR GDW. CIFAR10 EVALUATE-FOR GDW. Material are real - world datasets, and clean and unbiased data. Generic is methods. OtherScientificTerm are class - level information, class - level gradients, and intermediate gradients. ","This paper studies the problem of class imbalance and label noise in the context of generalized data weighting (GDW). The authors propose a new method for class-level weighting based on the chain rule. The main contribution of the paper is to propose a method that uses gradient descent to learn the class level weights for instance weighting. The proposed method is evaluated on CIFAR-10, Cifar-100, and ImageNet. The authors show that the proposed method outperforms the state-of-the-art methods.","This paper studies the problem of class imbalance and label noise in the context of generalized data weighting (GDW). The authors propose a new method for class-level weighting based on the chain rule. The main contribution of the paper is to propose a method that uses gradient descent to learn the class level weights for instance weighting. The proposed method is evaluated on CIFAR-10, Cifar-100, and ImageNet. The authors show that the proposed method outperforms the state-of-the-art methods."
16972,SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"it USED-FOR embodied agents. language USED-FOR embodied agents. sensorimotor modalities USED-FOR language. embodied agent FEATURE-OF spatio - temporal descriptions of behavioral traces. time - extended predicates CONJUNCTION spatio - temporal references. spatio - temporal references CONJUNCTION time - extended predicates. time - extended predicates PART-OF descriptions. spatio - temporal references PART-OF descriptions. architectural biases USED-FOR task. attention computations USED-FOR latter. multimodal Transformer architectures HYPONYM-OF models. generalization CONJUNCTION generalization. generalization CONJUNCTION generalization. generalization EVALUATE-FOR models. randomly held - out sentences USED-FOR generalization. generalization EVALUATE-FOR models. grammar primitives USED-FOR generalization. generalization HYPONYM-OF generalization. generalization HYPONYM-OF generalization. object identity FEATURE-OF attention computation. attention computation PART-OF Transformers. object identity USED-FOR generalization. code CONJUNCTION pretrained models. pretrained models CONJUNCTION code. OtherScientificTerm are Language, grounded language, spatio - temporal linguistic concepts, and truth function. Task are spatio - temporal language grounding task, and language - guided autonomous embodied agents. ","This paper proposes a new task for language-guided autonomous embodied agents. The task is to learn a language grounding task that is grounded in spatio-temporal descriptions of behavioral traces. The grounding task is formulated as a multi-modal Transformer task, where the agent is asked to describe a sequence of objects in a language-based way, and the object is represented as a set of time-extended predicates. The agent is then asked to use these predicates to describe the objects in the sequence. The authors show that the proposed task is able to generalize better than baselines in terms of generalization and generalization to unseen objects. ","This paper proposes a new task for language-guided autonomous embodied agents. The task is to learn a language grounding task that is grounded in spatio-temporal descriptions of behavioral traces. The grounding task is formulated as a multi-modal Transformer task, where the agent is asked to describe a sequence of objects in a language-based way, and the object is represented as a set of time-extended predicates. The agent is then asked to use these predicates to describe the objects in the sequence. The authors show that the proposed task is able to generalize better than baselines in terms of generalization and generalization to unseen objects. "
17036,SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"detecting CONJUNCTION tracking. tracking CONJUNCTION detecting. tracking USED-FOR Multiple object tracking and segmentation. detecting USED-FOR Multiple object tracking and segmentation. single frame predictions USED-FOR segmentation mask. temporal dimension USED-FOR association problem. approaches USED-FOR association problem. temporal dimension USED-FOR approaches. Prototypical Cross - Attention Network ( PCAN ) USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR Prototypical Cross - Attention Network ( PCAN ). cross - attention USED-FOR rich information. cross - attention USED-FOR PCAN. prototypical appearance module USED-FOR contrastive foreground and background prototypes. PCAN USED-FOR contrastive foreground and background prototypes. prototypical appearance module USED-FOR PCAN. PCAN COMPARE video instance tracking and segmentation competition winners. video instance tracking and segmentation competition winners COMPARE PCAN. Youtube - VIS and BDD100 K datasets EVALUATE-FOR video instance tracking and segmentation competition winners. Youtube - VIS and BDD100 K datasets EVALUATE-FOR PCAN. OtherScientificTerm are space - time memory, and prototypes. ","This paper proposes a new method for online multiple object tracking and segmentation. The proposed method, Prototypical Cross-Attention Network (PCAN), is a cross-attention network that is able to capture rich spatio-temporal information for multi-object tracking. PCAN is based on the idea of prototypical appearance module, which is used to capture contrastive foreground and background prototypes. The authors show that PCAN outperforms the state-of-the-art methods on Youtube-VIS and BDD100K.","This paper proposes a new method for online multiple object tracking and segmentation. The proposed method, Prototypical Cross-Attention Network (PCAN), is a cross-attention network that is able to capture rich spatio-temporal information for multi-object tracking. PCAN is based on the idea of prototypical appearance module, which is used to capture contrastive foreground and background prototypes. The authors show that PCAN outperforms the state-of-the-art methods on Youtube-VIS and BDD100K."
17100,SP:1175ad16382b349ab1a39895150172d266abe571,optimization USED-FOR deep learning. it USED-FOR gradient descent. approximate numerical solution USED-FOR initial value problem of gradient flow. curvature FEATURE-OF gradient flow trajectory. gradient descent USED-FOR initial value problem of gradient flow. gradient descent USED-FOR approximate numerical solution. homogeneous activations FEATURE-OF deep neural networks. favorable curvature FEATURE-OF gradient flow trajectories. gradient descent USED-FOR they. gradient descent USED-FOR global minimum. deep linear neural networks USED-FOR gradient flow. random initialization USED-FOR gradient descent. gradient descent COMPARE gradient flow. gradient flow COMPARE gradient descent. deep neural networks USED-FOR gradient descent. step size FEATURE-OF gradient descent. OtherScientificTerm is Gradient flow. Metric is computational efficiency. Method is gradient flows. ,This paper studies the problem of gradient descent in deep neural networks. The authors show that gradient descent converges to the global minimum of the initial value problem of the gradient flow. They also show that the gradient descent can be viewed as an approximate numerical solution of gradient flow with favorable curvature. ,This paper studies the problem of gradient descent in deep neural networks. The authors show that gradient descent converges to the global minimum of the initial value problem of the gradient flow. They also show that the gradient descent can be viewed as an approximate numerical solution of gradient flow with favorable curvature. 
17164,SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"multi - armed bandits USED-FOR delayed and longterm impact of actions. action history USED-FOR learning. regret EVALUATE-FOR algorithm. OtherScientificTerm are delayed impact of actions, arm rewards, feedback loop, and delayed impacts of historical actions. Generic are setting, and techniques. Task is bandit setting. Metric is matching regret lower bound. Method are bandit literature, and fair algorithms. ","This paper studies the regret of multi-armed bandits in the setting of delayed and long-term impact of actions. In particular, the authors consider the setting where the action history is not available and the agent has access to a feedback loop. The authors propose an algorithm that learns a regret lower bound for a given action history and then uses this lower bound to learn a regret upper bound for the current action history. They show that the proposed algorithm achieves better regret upper bounds than existing algorithms.","This paper studies the regret of multi-armed bandits in the setting of delayed and long-term impact of actions. In particular, the authors consider the setting where the action history is not available and the agent has access to a feedback loop. The authors propose an algorithm that learns a regret lower bound for a given action history and then uses this lower bound to learn a regret upper bound for the current action history. They show that the proposed algorithm achieves better regret upper bounds than existing algorithms."
17228,SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"end - to - end solution USED-FOR video instance segmentation ( VIS ). transformers USED-FOR end - to - end solution. per - clip pipeline COMPARE per - frame methods. per - frame methods COMPARE per - clip pipeline. per - clip models USED-FOR frame - to - frame communications. benchmark sets EVALUATE-FOR method. method USED-FOR near - online inference. Method are Inter - frame Communication Transformers ( IFC ), and offline inference. OtherScientificTerm are overhead, concise memory tokens, and features. Material is YouTube - VIS 2019 val set. ","This paper proposes a new method for video instance segmentation (VIS). The proposed method, called Inter-Frame Communication Transformers (IFC), is based on a per-clip model. The authors show that the proposed method is able to achieve better performance than per-frame models on YouTube-VIS 2019. ","This paper proposes a new method for video instance segmentation (VIS). The proposed method, called Inter-Frame Communication Transformers (IFC), is based on a per-clip model. The authors show that the proposed method is able to achieve better performance than per-frame models on YouTube-VIS 2019. "
17292,SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"vector - space representation USED-FOR machine learning applications. vector - space representation USED-FOR graph analysis. graph analysis CONJUNCTION machine learning applications. machine learning applications CONJUNCTION graph analysis. Graph embedding USED-FOR vector - space representation. Graph embedding USED-FOR graph. sampling of context nodes USED-FOR graph embedding methods. random walks USED-FOR sampling of context nodes. random walks HYPONYM-OF biased sampler. degree FEATURE-OF random walks. residual2vec HYPONYM-OF graph embedding method. random walks ’ bias USED-FOR graph embedding. random graphs USED-FOR residual2vec. link prediction CONJUNCTION clustering. clustering CONJUNCTION link prediction. debiasing USED-FOR structural properties. structural properties PART-OF graph embedding. clustering EVALUATE-FOR debiasing. link prediction EVALUATE-FOR debiasing. OtherScientificTerm are structural properties of graphs, node, and structural biases in graphs. Task is graph representation learning. ",This paper studies the problem of learning embeddings of graphs from random walks. The authors propose to use random walks as a sampling of context nodes for graph embedding. They show that random walks can be used as a biased sampler for graph representation learning. They also show that the bias of random walks is related to the degree of structural biases in the graph. ,This paper studies the problem of learning embeddings of graphs from random walks. The authors propose to use random walks as a sampling of context nodes for graph embedding. They show that random walks can be used as a biased sampler for graph representation learning. They also show that the bias of random walks is related to the degree of structural biases in the graph. 
17356,SP:851eac96135b577a5014166edcb43db6a190cf4b,"local differential privacy FEATURE-OF estimating non - linear functionals of discrete distributions. quadratic risk USED-FOR power sum functional. plug - in type estimators COMPARE MLE. MLE COMPARE plug - in type estimators. two - step procedure USED-FOR sequentially interactive case. α - LDP mechanisms CONJUNCTION estimators. estimators CONJUNCTION α - LDP mechanisms. private samples USED-FOR estimators. OtherScientificTerm are discrete distribution, non - interactive case, and privacy constraint. Method are privacy mechanisms ( PM ), multinomial model, and Gaussian model. ","This paper studies the problem of estimating non-linear functionals of discrete distributions under local differential privacy (LDP) constraints. The authors consider the case where the discrete distribution is non-interactive and the data distribution is discrete. In this setting, the authors consider a multinomial model and a Gaussian model. They show that the power sum functional of discrete functions can be approximated by a quadratic risk minimization problem, which is a two-step procedure. They also show that this problem can be solved by a two step procedure. ","This paper studies the problem of estimating non-linear functionals of discrete distributions under local differential privacy (LDP) constraints. The authors consider the case where the discrete distribution is non-interactive and the data distribution is discrete. In this setting, the authors consider a multinomial model and a Gaussian model. They show that the power sum functional of discrete functions can be approximated by a quadratic risk minimization problem, which is a two-step procedure. They also show that this problem can be solved by a two step procedure. "
17420,SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"directed graph USED-FOR learner ’s feedback. filtering CONJUNCTION label efficient classification. label efficient classification CONJUNCTION filtering. feedback graphs USED-FOR applications. label efficient classification HYPONYM-OF applications. filtering HYPONYM-OF applications. GAPPLETRON HYPONYM-OF online multiclass algorithm. arbitrary feedback graphs USED-FOR online multiclass algorithm. surrogate regret bounds USED-FOR algorithm. domination number HYPONYM-OF graph - theoretic parameter. full information case FEATURE-OF GAPPLETRON. surrogate regret EVALUATE-FOR GAPPLETRON. synthetic data EVALUATE-FOR algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic data EVALUATE-FOR baselines. feedback graphs EVALUATE-FOR algorithm. Task is online multiclass classification. OtherScientificTerm are bandit feedback, surrogate losses, prediction space, and time horizon. Generic are bounds, lower bound, and upper bounds. ","This paper studies the problem of online multiclass classification with bandit feedback. The authors propose a new algorithm called GAPPLETRON, which uses a directed graph as a surrogate loss function for the learner’s feedback. They show that the proposed algorithm achieves a surrogate regret bound of $\mathcal{O}(\sqrt{T})$ for the full information case. They also provide an upper bound of $O(T)$ and a lower bound of \sqrt{\log T}(\log T)$. They also show that their algorithm can be applied to a variety of applications such as label efficient classification and filtering. ","This paper studies the problem of online multiclass classification with bandit feedback. The authors propose a new algorithm called GAPPLETRON, which uses a directed graph as a surrogate loss function for the learner’s feedback. They show that the proposed algorithm achieves a surrogate regret bound of $\mathcal{O}(\sqrt{T})$ for the full information case. They also provide an upper bound of $O(T)$ and a lower bound of \sqrt{\log T}(\log T)$. They also show that their algorithm can be applied to a variety of applications such as label efficient classification and filtering. "
17484,SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"threshold cut USED-FOR single dimension ( feature ). decision tree USED-FOR it. decision tree USED-FOR k - clustering. algorithm USED-FOR explainable clustering. O(k ) CONJUNCTION O(k ). O(k ) CONJUNCTION O(k ). Ω(k ) lower bound USED-FOR k - means. Ω(log k ) lower bound USED-FOR k - medians. Ω(log k ) lower bound CONJUNCTION Ω(k ) lower bound. Ω(k ) lower bound CONJUNCTION Ω(log k ) lower bound. upper bounds COMPARE O(k ). O(k ) COMPARE upper bounds. O(k ) HYPONYM-OF upper bounds. OtherScientificTerm are cluster, k - means objective, upper and lower bounds, and ` p - norms. Metric is k - medians objective. ","This paper studies the problem of explainable clustering in the context of k-means clustering. The authors consider the case where a single dimension (feature) is represented by a decision tree, and the goal is to find a cluster that is explainable in terms of the k-medians. The main contribution of this paper is to provide upper and lower bounds for k-mean clustering under the assumption that the decision tree is a closed-form decision tree. The upper bound is based on the lower bound of O(k). The lower bound is derived based on a lower bound on the log-likelihood of the clustering objective. ","This paper studies the problem of explainable clustering in the context of k-means clustering. The authors consider the case where a single dimension (feature) is represented by a decision tree, and the goal is to find a cluster that is explainable in terms of the k-medians. The main contribution of this paper is to provide upper and lower bounds for k-mean clustering under the assumption that the decision tree is a closed-form decision tree. The upper bound is based on the lower bound of O(k). The lower bound is derived based on a lower bound on the log-likelihood of the clustering objective. "
17548,SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"pre - trained language model ( PrLM ) USED-FOR downstream natural language processing tasks. multilingual PrLM USED-FOR limited resources. language universality USED-FOR limited resources. limited resources USED-FOR low - resource languages. multilingual PrLM USED-FOR downstream natural language processing tasks. language universality USED-FOR multilingual PrLM. plain text USED-FOR multilingual PrLMs. monolingual linguistic structure knowledge USED-FOR PrLMs. explicit universal dependency parsing CONJUNCTION implicit language modeling. implicit language modeling CONJUNCTION explicit universal dependency parsing. multilingual PrLM USED-FOR explicit universal dependency parsing. multilingual PrLM USED-FOR implicit language modeling. learned representation USED-FOR model. universal dependency parse FEATURE-OF Syntax. model COMPARE multilingual PrLM. multilingual PrLM COMPARE model. model COMPARE multilingual - BERT. multilingual - BERT COMPARE model. model COMPARE approach. approach COMPARE model. linguistic structure parsing datasets EVALUATE-FOR multilingual PrLM. linguistic structure parsing datasets EVALUATE-FOR model. multilingual - BERT HYPONYM-OF multilingual PrLM. OtherScientificTerm are universal linguistic structure clues, and PrLM interpretability. ",This paper proposes a multilingual pre-trained language model (PrLM) for downstream natural language processing tasks. The multilingual PrLM is based on the monolingual linguistic structure knowledge and is able to handle low-resource languages with limited resources. Experiments show that the multilingual prLM outperforms multilingual BERT on a variety of downstream tasks.,This paper proposes a multilingual pre-trained language model (PrLM) for downstream natural language processing tasks. The multilingual PrLM is based on the monolingual linguistic structure knowledge and is able to handle low-resource languages with limited resources. Experiments show that the multilingual prLM outperforms multilingual BERT on a variety of downstream tasks.
17612,SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"deep architecture USED-FOR vehicle routing problems ( VRPs ). Transformer HYPONYM-OF deep architecture. Transformer USED-FOR vehicle routing problems ( VRPs ). positional encoding ( PE ) method USED-FOR representing VRP solutions. learning improvement models USED-FOR VRP. it USED-FOR learning improvement models. Dual - Aspect Collaborative Transformer ( DACT ) USED-FOR embeddings. embeddings USED-FOR node and positional features. Transformer USED-FOR symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR Transformer. cyclic sequences HYPONYM-OF symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR positional features. curriculum learning strategy USED-FOR sample efficiency. Proximal Policy Optimization USED-FOR DACT. traveling salesman problem ( TSP ) CONJUNCTION capacitated vehicle routing problem ( CVRP ). capacitated vehicle routing problem ( CVRP ) CONJUNCTION traveling salesman problem ( TSP ). DACT USED-FOR capacitated vehicle routing problem ( CVRP ). DACT USED-FOR traveling salesman problem ( TSP ). DACT COMPARE Transformer based improvement models. Transformer based improvement models COMPARE DACT. synthetic and benchmark instances EVALUATE-FOR DACT. OtherScientificTerm are VRP solutions, and incompatible correlations. Generic are them, and ones. ","This paper proposes a new transformer architecture for vehicle routing problems (VRPs). The proposed architecture is based on the Dual-Aspect Collaborative Transformer (DACT) framework. The authors propose a curriculum learning strategy to improve the sample efficiency of the proposed model. The proposed method is evaluated on a variety of VRP problems, including the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP).","This paper proposes a new transformer architecture for vehicle routing problems (VRPs). The proposed architecture is based on the Dual-Aspect Collaborative Transformer (DACT) framework. The authors propose a curriculum learning strategy to improve the sample efficiency of the proposed model. The proposed method is evaluated on a variety of VRP problems, including the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP)."
17676,SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"Bayes error FEATURE-OF generative models. normalizing flows USED-FOR generative models. invertible transformation USED-FOR Bayes error. it USED-FOR Gaussian base distributions. Bayes error EVALUATE-FOR flow models. Holmes - Diaconis - Ross integration USED-FOR it. it USED-FOR Bayes error. synthetic datasets COMPARE benchmark datasets. benchmark datasets COMPARE synthetic datasets. Bayes error FEATURE-OF synthetic datasets. approach USED-FOR classification models. method USED-FOR benchmark datasets. Task is data - driven classification problem. Metric is classification error. OtherScientificTerm are data distribution, and intractable quantity. Generic are technique, and models. ",This paper studies the problem of Bayes error estimation for generative models. The authors propose to use normalizing flows as a regularizer to improve the performance of the model. They show that the proposed normalizing flow can be used to reduce the problem to a data-driven classification problem. They also show that their method can be applied to a variety of synthetic datasets. ,This paper studies the problem of Bayes error estimation for generative models. The authors propose to use normalizing flows as a regularizer to improve the performance of the model. They show that the proposed normalizing flow can be used to reduce the problem to a data-driven classification problem. They also show that their method can be applied to a variety of synthetic datasets. 
17740,SP:2896679f0472522bc3334178cd7574494cf12b7b,"language modeling CONJUNCTION computer vision. computer vision CONJUNCTION language modeling. neural architectures USED-FOR language modeling. neural architectures USED-FOR computer vision. hyper - parameter choices CONJUNCTION training instability. training instability CONJUNCTION hyper - parameter choices. automated and architecture agnostic method USED-FOR initializing neural networks. GradInit HYPONYM-OF automated and architecture agnostic method. GradInit USED-FOR initializing neural networks. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. norm FEATURE-OF network layer. heuristic USED-FOR GradInit. numerical scheme USED-FOR variables. GradInit USED-FOR convolutional architectures. skip connections USED-FOR convolutional architectures. learning rates CONJUNCTION momentum coefficients. momentum coefficients CONJUNCTION learning rates. Adam CONJUNCTION SGD. SGD CONJUNCTION Adam. Transformer architecture USED-FOR machine translation. It USED-FOR Transformer architecture. learning rate warmup FEATURE-OF it. Adam USED-FOR learning rate warmup. SGD USED-FOR learning rate warmup. SGD USED-FOR it. Adam USED-FOR it. Generic are architectures, and schemes. OtherScientificTerm are network parameters, hyperparameters, scalar multiplier variable, and normalization layers. Method are architecture - specific initialization schemes, and neural networks. ","This paper proposes a new method for initialization of neural networks. The method is based on a heuristic called GradInit, which is a generalization of Adam and SGD. The main idea of GradInit is to use a scalar multiplier to compute the norm of a neural network layer. The authors show that GradInit can be applied to a wide range of neural network architectures, including neural networks with skip connections and convolutional networks with normalization layers. ","This paper proposes a new method for initialization of neural networks. The method is based on a heuristic called GradInit, which is a generalization of Adam and SGD. The main idea of GradInit is to use a scalar multiplier to compute the norm of a neural network layer. The authors show that GradInit can be applied to a wide range of neural network architectures, including neural networks with skip connections and convolutional networks with normalization layers. "
17804,SP:f69731403592fa5bdd4ca327708582d615aa131c,"Linear mixed - effect models USED-FOR disease progression. interpretable parameters USED-FOR subject trajectories. diffeomorphism USED-FOR Euclidean metric. diffeomorphism USED-FOR metric. reproducible kernel Hilbert space FEATURE-OF radial basis functions. radial basis functions USED-FOR diffeomorphism. metric update USED-FOR forecasting of imaging and clinical biomarkers. TADPOLE challenge EVALUATE-FOR methods. Material is longitudinal data. Method are interpretable models, and ADNI. OtherScientificTerm are progression profiles, Riemannian manifold, patient - specific trajectories, and central geodesic. Generic is approach. Metric is interpretability. Task is Neural Information Processing Systems. ",This paper proposes a new approach for predicting disease progression in longitudinal data. The proposed approach is based on diffeomorphism of the Euclidean metric. The authors show that the proposed approach can be used to improve the interpretability of the model. The approach is evaluated on the TADPOLE dataset.,This paper proposes a new approach for predicting disease progression in longitudinal data. The proposed approach is based on diffeomorphism of the Euclidean metric. The authors show that the proposed approach can be used to improve the interpretability of the model. The approach is evaluated on the TADPOLE dataset.
17868,SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"routing - by - memory mechanism USED-FOR CNN architectures. memory head CONJUNCTION procedure. procedure CONJUNCTION memory head. memory head PART-OF PU. procedure PART-OF PU. procedures USED-FOR features. mechanism USED-FOR Networks. four - step training strategy USED-FOR mechanism. four - step training strategy USED-FOR Networks. VGGNet CONJUNCTION ResNet. ResNet CONJUNCTION VGGNet. ResNet CONJUNCTION EfficientNet ’s accuracies. EfficientNet ’s accuracies CONJUNCTION ResNet. Tiny ImageNet CONJUNCTION ImageNet. ImageNet CONJUNCTION Tiny ImageNet. ImageNet CONJUNCTION CIFAR-100 benchmarks. CIFAR-100 benchmarks CONJUNCTION ImageNet. Tiny ImageNet EVALUATE-FOR EfficientNet ’s accuracies. VGGNet EVALUATE-FOR method. ResNet EVALUATE-FOR method. EfficientNet ’s accuracies EVALUATE-FOR method. ImageNet EVALUATE-FOR method. CIFAR-100 benchmarks EVALUATE-FOR method. Tiny ImageNet EVALUATE-FOR method. Method are Convolutional Neural Networks ( CNNs ), parallel procedures, and parallel Procedural Units ( PUs ). OtherScientificTerm are semantic features, procedure sequence, intermediate features, and intermediate feature. Generic are specialized procedures, It, and network. ","This paper proposes a new routing-by-memory mechanism for CNN architectures. The proposed method is based on the idea of parallel procedures and parallel PUs. The authors propose a four-step training strategy to train parallel procedures. The method is evaluated on CIFAR-10, Tiny ImageNet, ResNet, and VGGNet. ","This paper proposes a new routing-by-memory mechanism for CNN architectures. The proposed method is based on the idea of parallel procedures and parallel PUs. The authors propose a four-step training strategy to train parallel procedures. The method is evaluated on CIFAR-10, Tiny ImageNet, ResNet, and VGGNet. "
17932,SP:d240173080cd3647dbaa5173a6422396f226775b,"fundamental symmetries CONJUNCTION coordinate freedoms of physical law. coordinate freedoms of physical law CONJUNCTION fundamental symmetries. coordinate freedoms of physical law FEATURE-OF neural networks. fundamental symmetries FEATURE-OF neural networks. irreducible representations USED-FOR frameworks. rotation CONJUNCTION reflection ( parity ). reflection ( parity ) CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. fundamental symmetries USED-FOR physical laws. scalar products CONJUNCTION scalar contractions. scalar contractions CONJUNCTION scalar products. scalar contractions HYPONYM-OF scalars. scalar products HYPONYM-OF scalars. OtherScientificTerm are high - order tensor objects, symmetry - enforcing constraints, classical physics, permutations, symmetries, and Euclidean, Lorentz, and Poincaré groups. Method are polynomial functions, and scalar - based method. Generic is theory. ","This paper studies the problem of learning invariant representations of high-order tensor objects. The authors propose a scalar-based method to learn invariant representation of high order tensors. The main contribution of the paper is a theoretical analysis of scalar products and scalar contractions, which is based on the symmetry-preserving constraints of classical physics.  The authors show that the invariance of the scalar product can be understood as a generalization of the symmetries of Euclidean, Lorentz, and Poincaré groups. ","This paper studies the problem of learning invariant representations of high-order tensor objects. The authors propose a scalar-based method to learn invariant representation of high order tensors. The main contribution of the paper is a theoretical analysis of scalar products and scalar contractions, which is based on the symmetry-preserving constraints of classical physics.  The authors show that the invariance of the scalar product can be understood as a generalization of the symmetries of Euclidean, Lorentz, and Poincaré groups. "
17996,SP:72c0f47566904deb27d8157da30807ec1d6b5685,Bounding box ( bbox ) regression HYPONYM-OF computer vision. loss functions USED-FOR bbox regression. Intersection over Union ( IoU ) loss HYPONYM-OF loss functions. IoUbased losses USED-FOR power IoU losses. power IoU term CONJUNCTION power regularization term. power regularization term CONJUNCTION power IoU term. power parameter α FEATURE-OF power regularization term. power regularization term FEATURE-OF power IoU losses. power IoU term FEATURE-OF power IoU losses. order preservingness CONJUNCTION loss / gradient reweighting. loss / gradient reweighting CONJUNCTION order preservingness. α - IoU losses HYPONYM-OF losses. loss / gradient reweighting HYPONYM-OF properties. order preservingness HYPONYM-OF properties. α - IoU losses COMPARE IoU - based losses. IoU - based losses COMPARE α - IoU losses. small datasets CONJUNCTION noisy bboxes. noisy bboxes CONJUNCTION small datasets. object detection benchmarks CONJUNCTION models. models CONJUNCTION object detection benchmarks. bbox regression accuracy EVALUATE-FOR detectors. performance margin EVALUATE-FOR α - IoU losses. performance margin EVALUATE-FOR IoU - based losses. OtherScientificTerm is α. ,"This paper proposes a new loss function for bbox regression, called power IoU loss, which is a generalization of the power-IoU loss. The main idea is to use the power parameter of the IoU term as a regularization term to improve the performance of the bbox-based loss function. The authors show that the power term can be used to reduce the variance of the loss function and improve the order-preserving properties of the proposed loss. ","This paper proposes a new loss function for bbox regression, called power IoU loss, which is a generalization of the power-IoU loss. The main idea is to use the power parameter of the IoU term as a regularization term to improve the performance of the bbox-based loss function. The authors show that the power term can be used to reduce the variance of the loss function and improve the order-preserving properties of the proposed loss. "
18060,SP:397125177d7007316d67194ec00d5dc57b44ac79,"imitation learning problem USED-FOR policy. Markov Decision Process ( MDP ) setting FEATURE-OF policy. imitation learning USED-FOR policy. policy USED-FOR problem. adversarial construction USED-FOR policy. DROIL CONJUNCTION Maximum Entropy Inverse Reinforcement Learning. Maximum Entropy Inverse Reinforcement Learning CONJUNCTION DROIL. framework USED-FOR generalized concept of entropy. generalized concept of entropy USED-FOR DROIL. framework USED-FOR DROIL. approach USED-FOR objective function. approach USED-FOR convex optimization problem. state and action spaces FEATURE-OF loss functions. convex optimization problem USED-FOR objective function. polynomial number of variables FEATURE-OF convex optimization problem. approach USED-FOR stationary and non - stationary policies. methods COMPARE it. it COMPARE methods. inner reinforcement learning problem USED-FOR it. synthetic data CONJUNCTION highway driving environment. highway driving environment CONJUNCTION synthetic data. optimization method USED-FOR DROIL. synthetic data EVALUATE-FOR DROIL. highway driving environment EVALUATE-FOR DROIL. synthetic data EVALUATE-FOR optimization method. highway driving environment EVALUATE-FOR optimization method. OtherScientificTerm are reward function, demonstrated behaviors, noisy demonstrations, and optimistic generalizations. Generic is task. Method is Distributionally Robust Imitation Learning ( DROIL ). ",This paper proposes a new method for imitation learning in Markov Decision Process (MDP) settings. The main idea is to use the distributionally robust imitation learning (DROIL) framework to learn a policy that maximizes the entropy of the state and action spaces. The authors show that DROIL can be applied to both stationary and non-stationary MDPs. The method is evaluated on synthetic and real-world datasets.,This paper proposes a new method for imitation learning in Markov Decision Process (MDP) settings. The main idea is to use the distributionally robust imitation learning (DROIL) framework to learn a policy that maximizes the entropy of the state and action spaces. The authors show that DROIL can be applied to both stationary and non-stationary MDPs. The method is evaluated on synthetic and real-world datasets.
18124,SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"Post - processing USED-FOR algorithmic fairness. approach USED-FOR ML systems. Post - processing HYPONYM-OF approach. retraining USED-FOR it. post - processing algorithms USED-FOR individual fairness ( IF ). similarity graph USED-FOR fairness constraints. graph Laplacian regularization USED-FOR graph smoothing problem. graph smoothing problem USED-FOR IF post - processing problem. post - processing algorithms USED-FOR individual biases. post - processing algorithms USED-FOR large - scale NLP models. individual biases FEATURE-OF large - scale NLP models. accuracy EVALUATE-FOR post - processing algorithms. BERT HYPONYM-OF large - scale NLP models. Method is postprocessing. Generic is model. OtherScientificTerm are objective function, and individual fairness. ",This paper studies the problem of post-processing for algorithmic fairness in ML systems. The authors propose a graph smoothing algorithm for individual fairness (IF) based on the graph Laplacian regularization. They show that the proposed algorithm is able to improve the performance of individual fairness algorithms on BERT and BERT-based NLP models. ,This paper studies the problem of post-processing for algorithmic fairness in ML systems. The authors propose a graph smoothing algorithm for individual fairness (IF) based on the graph Laplacian regularization. They show that the proposed algorithm is able to improve the performance of individual fairness algorithms on BERT and BERT-based NLP models. 
18188,SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"Text - to - SQL task USED-FOR SQL queries. model USED-FOR database schemas. graph structure USED-FOR unified encoding model. unified encoding model USED-FOR natural language question and database schema. graph structure USED-FOR SADGA. question - graph CONJUNCTION schema - graph. schema - graph CONJUNCTION question - graph. unified modeling USED-FOR structure - aware aggregation method. Global Graph Linking CONJUNCTION Local Graph Linking. Local Graph Linking CONJUNCTION Global Graph Linking. Local Graph Linking CONJUNCTION DualGraph Aggregation Mechanism. DualGraph Aggregation Mechanism CONJUNCTION Local Graph Linking. Global Graph Linking USED-FOR structure - aware aggregation method. Local Graph Linking PART-OF structure - aware aggregation method. DualGraph Aggregation Mechanism PART-OF structure - aware aggregation method. Text - to - SQL benchmark Spider EVALUATE-FOR proposal. Task are Text - to - SQL, and cross - domain Text - to - SQL. Method are encoding method, and question - schema linking method. OtherScientificTerm is database schema. ","This paper proposes a unified encoding model for the text-to-SQL task. The proposed model is based on the idea of SADGA, which is a graph-based model that is able to encode both the question and the database schema. The model is trained on both natural language question and database schema and can be applied to both cross-domain and cross-dataset tasks. The authors show that the proposed model outperforms the baselines on the Spider benchmark. ","This paper proposes a unified encoding model for the text-to-SQL task. The proposed model is based on the idea of SADGA, which is a graph-based model that is able to encode both the question and the database schema. The model is trained on both natural language question and database schema and can be applied to both cross-domain and cross-dataset tasks. The authors show that the proposed model outperforms the baselines on the Spider benchmark. "
18252,SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"models USED-FOR supervised and reinforcement learning. discrete and continuous model components USED-FOR models. approach USED-FOR discrete - continuous computation graphs. discrete probability distributions USED-FOR neural networks. stochastic softmax tricks USED-FOR neural networks. discrete component USED-FOR graph ’s execution paths. discrete component USED-FOR computation graphs. sequential discrete components USED-FOR stochastic computations graphs. small gradients CONJUNCTION local minima. local minima CONJUNCTION small gradients. scale parameter FEATURE-OF Gumbel noise perturbations. scale parameter USED-FOR learning behavior. dropout residual connections USED-FOR stochastic, discrete - continuous computation graphs. complex discrete - stochastic models COMPARE continuous counterparts. continuous counterparts COMPARE complex discrete - stochastic models. benchmark datasets EVALUATE-FOR complex discrete - stochastic models. benchmark datasets EVALUATE-FOR continuous counterparts. Method are discrete - continuous models, and complex discrete - continuous models. Generic is strategies. ",This paper proposes a novel approach for learning discrete-continuous computation graphs. The authors propose to use dropout residual connections as a discrete component of the computation graph. They show that the proposed approach is able to learn discrete computation graphs with small gradients and local minima. They also show that their approach can be used to learn stochastic softmax tricks.,This paper proposes a novel approach for learning discrete-continuous computation graphs. The authors propose to use dropout residual connections as a discrete component of the computation graph. They show that the proposed approach is able to learn discrete computation graphs with small gradients and local minima. They also show that their approach can be used to learn stochastic softmax tricks.
18316,SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"Approximate Bayesian inference USED-FOR neural networks. Approximate Bayesian inference COMPARE training. training COMPARE Approximate Bayesian inference. high - fidelity approximate inference FEATURE-OF Bayesian neural networks ( BNNs ). full - batch Hamiltonian Monte Carlo USED-FOR high - fidelity approximate inference. covariate shift FEATURE-OF Bayesian model average. approximate inference procedures CONJUNCTION maximum a - posteriori ( MAP ) training. maximum a - posteriori ( MAP ) training CONJUNCTION approximate inference procedures. priors USED-FOR BNNs. robustness EVALUATE-FOR BNNs. robustness EVALUATE-FOR priors. Material is out - of - distribution data. Method is classical estimation. OtherScientificTerm are linear dependencies, features, and posterior contraction. ","This paper studies the problem of high-fidelity approximate inference for Bayesian neural networks (BNNs). In particular, the authors consider the case where the covariate shift of the Bayesian model average is a function of the out-of-distribution (OOD) data distribution. The authors show that the posterior contraction of BNNs is a result of the covariance shift in the data distribution, and propose a Hamiltonian Monte Carlo (HMC) method to solve this problem. They also show that HMC can be used to improve the robustness of a BNN.","This paper studies the problem of high-fidelity approximate inference for Bayesian neural networks (BNNs). In particular, the authors consider the case where the covariate shift of the Bayesian model average is a function of the out-of-distribution (OOD) data distribution. The authors show that the posterior contraction of BNNs is a result of the covariance shift in the data distribution, and propose a Hamiltonian Monte Carlo (HMC) method to solve this problem. They also show that HMC can be used to improve the robustness of a BNN."
18380,SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"settings PART-OF meta - learning evaluation. in - distribution [ ID ] HYPONYM-OF settings. out - of - distribution [ OOD ] HYPONYM-OF settings. task distribution USED-FOR train and test tasks. metalearning theory CONJUNCTION FSL applications. FSL applications CONJUNCTION metalearning theory. they USED-FOR task generation. few - shot classification benchmarks EVALUATE-FOR OOD evaluation. ID setting USED-FOR metalearning theory. meta - learning methods USED-FOR ID setting. OOD datasets EVALUATE-FOR meta - learning methods. meta - learning method USED-FOR model selection. ID evaluation CONJUNCTION OOD evaluation. OOD evaluation CONJUNCTION ID evaluation. FSL benchmarks USED-FOR ID evaluation. FSL benchmarks USED-FOR OOD evaluation. benchmarks USED-FOR OOD evaluation. Task are OOD setting, and ID vs. OOD evaluation. Generic is methods. ","This paper studies the problem of meta-learning in the out-of-distribution (OOD) setting. In the ID setting, the goal is to find the best model for a given task. The authors propose a meta-training method for OOD evaluation. The proposed method is based on the metalearning theory and is evaluated on several few-shot classification benchmarks. ","This paper studies the problem of meta-learning in the out-of-distribution (OOD) setting. In the ID setting, the goal is to find the best model for a given task. The authors propose a meta-training method for OOD evaluation. The proposed method is based on the metalearning theory and is evaluated on several few-shot classification benchmarks. "
18444,SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"rules PART-OF knowledge base ( KB ). language model ( LM)-based rule generation USED-FOR rules. KB - based rule induction CONJUNCTION LM - based rule generation. LM - based rule generation CONJUNCTION KB - based rule induction. data commonalities USED-FOR KB - based methods. LMs USED-FOR free text. rich expressive power FEATURE-OF LMs. methods USED-FOR canned ” rules. open rule induction problem USED-FOR open rules. Orion ( open rule induction ) system USED-FOR open rules. LMs USED-FOR open rules. automatically inducted rules COMPARE manually annotated rules. manually annotated rules COMPARE automatically inducted rules. open rules USED-FOR relation extraction. OtherScientificTerm are Rules, annotated rules, and supervision of annotated rules. Method are inference systems, rule induction systems, and LM - based methods. Generic is they. ",This paper studies the problem of open rule induction in the context of knowledge base (KB). The authors propose a novel approach to learn open rules that can be used in conjunction with existing methods for KB-based rule induction. The approach is based on the idea that open rules can be learned from the knowledge base. The authors show that the proposed approach is able to generate rules that are more expressive than existing methods. They also show that this approach can be combined with other methods for open rules. ,This paper studies the problem of open rule induction in the context of knowledge base (KB). The authors propose a novel approach to learn open rules that can be used in conjunction with existing methods for KB-based rule induction. The approach is based on the idea that open rules can be learned from the knowledge base. The authors show that the proposed approach is able to generate rules that are more expressive than existing methods. They also show that this approach can be combined with other methods for open rules. 
18508,SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,Reinforcement Learning ( RL ) algorithms USED-FOR real - world scenarios. single - agent counterpart COMPARE offline multiagent RL. offline multiagent RL COMPARE single - agent counterpart. state and action space FEATURE-OF agents. agents PART-OF offline multiagent RL. offline RL algorithms USED-FOR multi - agent systems. offline RL algorithm USED-FOR extrapolation error. state - action pairs USED-FOR value estimation. Implicit Constraint Q - learning ( ICQ ) HYPONYM-OF offline RL algorithm. ICQ USED-FOR multi - agent tasks. implicit constraint USED-FOR joint - policy. OtherScientificTerm is accumulated extrapolation error. ,This paper proposes an implicit constraint Q-learning (ICQ) algorithm for offline multi-agent reinforcement learning. The main idea is to learn a joint-policy that maximizes the cumulative extrapolation error between state-action pairs in the action space and the joint policy in the state and action space. The proposed method is evaluated on a number of tasks and shows that it outperforms the state-of-the-art. ,This paper proposes an implicit constraint Q-learning (ICQ) algorithm for offline multi-agent reinforcement learning. The main idea is to learn a joint-policy that maximizes the cumulative extrapolation error between state-action pairs in the action space and the joint policy in the state and action space. The proposed method is evaluated on a number of tasks and shows that it outperforms the state-of-the-art. 
18572,SP:1939b24b68970c33ca16ce238deed257f76d009e,"machine learning models USED-FOR security related applications. real - world adversaries USED-FOR neural network based detectors. uniform norm - bounded perturbations USED-FOR adversarial examples ( AEs ). finance CONJUNCTION social networks. social networks CONJUNCTION finance. malware CONJUNCTION finance. finance CONJUNCTION malware. AEs USED-FOR domains. social networks HYPONYM-OF domains. malware HYPONYM-OF domains. finance HYPONYM-OF domains. semantically meaningful dependencies FEATURE-OF features. features USED-FOR applications. non - uniform perturbations USED-FOR feature dependencies. non - uniform perturbations USED-FOR adversarial training. malware classification CONJUNCTION credit risk prediction. credit risk prediction CONJUNCTION malware classification. credit risk prediction CONJUNCTION spam detection. spam detection CONJUNCTION credit risk prediction. approach USED-FOR real - world attacks. certification EVALUATE-FOR non - uniform bounds. non - uniform perturbation bounds USED-FOR robustness certification. Metric is imperceptibility. OtherScientificTerm are uniform perturbations, and empirical data distribution. ","This paper considers the problem of robustness certification of neural networks against adversarial examples (AEs). The authors propose a non-uniform norm-bounded perturbation bound for adversarial training, which is based on the observation that uniform perturbations are imperceptible in the empirical data distribution. The authors show that this bound can be used to certify the robustness of a neural network against AEs. They also provide a theoretical analysis of this bound.","This paper considers the problem of robustness certification of neural networks against adversarial examples (AEs). The authors propose a non-uniform norm-bounded perturbation bound for adversarial training, which is based on the observation that uniform perturbations are imperceptible in the empirical data distribution. The authors show that this bound can be used to certify the robustness of a neural network against AEs. They also provide a theoretical analysis of this bound."
18636,SP:417b30930b245667d777e5d90ee80dd41546760e,spectral filtering USED-FOR statistical properties. spectral filtering USED-FOR learning with kernels. learning with kernels USED-FOR statistical properties. regularization schemes COMPARE Tikhonov regularization. Tikhonov regularization COMPARE regularization schemes. faster convergence rates FEATURE-OF excess risk. regularization schemes USED-FOR excess risk. regularization schemes USED-FOR least squares. faster convergence rates EVALUATE-FOR regularization schemes. loss functions USED-FOR estimators. Tikhonov regularization USED-FOR generalized self concordant loss functions ( GSC ). logistic loss HYPONYM-OF generalized self concordant loss functions ( GSC ). proximal point method USED-FOR optimization. iterated Tikhonov regularization scheme CONJUNCTION proximal point method. proximal point method CONJUNCTION iterated Tikhonov regularization scheme. fast and optimal rates USED-FOR GSC. iterated Tikhonov regularization scheme USED-FOR fast and optimal rates. iterated Tikhonov regularization scheme USED-FOR GSC. OtherScientificTerm is source and capacity conditions. Task is learning task. ,"This paper studies the problem of minimizing excess risk in learning with kernels. The authors propose two regularization schemes for minimizing the excess risk. The first regularization scheme, iterated Tikhonov regularization, is based on the idea of proximal point method. The second regularisation scheme, called iterated self-concordant loss (i.e., iterated logistic loss), is based upon the iterated regularization of the kernel. Experiments show that the proposed regularization can achieve faster convergence rates compared to the baselines.","This paper studies the problem of minimizing excess risk in learning with kernels. The authors propose two regularization schemes for minimizing the excess risk. The first regularization scheme, iterated Tikhonov regularization, is based on the idea of proximal point method. The second regularisation scheme, called iterated self-concordant loss (i.e., iterated logistic loss), is based upon the iterated regularization of the kernel. Experiments show that the proposed regularization can achieve faster convergence rates compared to the baselines."
18700,SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"linear transform USED-FOR input - output dimensions. butterfly matrices USED-FOR linear transform. Deformable Butterfly ( DeBut ) HYPONYM-OF linear transform. It USED-FOR neural networks. sparsity FEATURE-OF DeBut layer. sparsity USED-FOR network compression. light weight CONJUNCTION inference complexity. inference complexity CONJUNCTION light weight. DeBut COMPARE fully connected and convolutional layers. fully connected and convolutional layers COMPARE DeBut. OtherScientificTerm are butterflies, and natural complexity - accuracy tradeoff. Method is neural network. Generic is it. Metric is accuracy. ","This paper proposes a new variant of the Deformable Butterfly (DeBut) layer for neural networks. The proposed DeBut layer is based on the butterfly matrices. The paper shows that the sparsity of DeBut layers is proportional to the size of the input-output dimension, which is a natural complexity-accuracy tradeoff. This paper also shows that DeBut can be used for network compression. ","This paper proposes a new variant of the Deformable Butterfly (DeBut) layer for neural networks. The proposed DeBut layer is based on the butterfly matrices. The paper shows that the sparsity of DeBut layers is proportional to the size of the input-output dimension, which is a natural complexity-accuracy tradeoff. This paper also shows that DeBut can be used for network compression. "
18764,SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"weights PART-OF network. method USED-FOR weight reusability. shared weights USED-FOR MARK. common Knowledge Base ( KB ) USED-FOR shared weights. metalearning approach USED-FOR weight reusability. metalearning approach USED-FOR KB. benchmarks EVALUATE-FOR MARK. average accuracy EVALUATE-FOR methods. MARK USED-FOR reusable knowledge. Method are artificial neural networks, and MetA Reusable Knowledge. Task are Catastrophic Forgetting ( CF ), and overwriting. OtherScientificTerm are forgetting of old information, trainable masks, and KB relevant weights. Generic are task, and model. Material is 20 - Split - MiniImageNet dataset. Metric is forgetfulness. ","This paper proposes a method for reusing the knowledge base (KB) of a neural network to prevent catastrophic forgetting. The authors propose a method called MetA Reusable Knowledge (MARK), which is based on the concept of common Knowledge Base (KB). The idea is that the KB of a network can be used as a mask to prevent forgetting of old information. The proposed method is evaluated on the 20-Split MiniImageNet dataset. ","This paper proposes a method for reusing the knowledge base (KB) of a neural network to prevent catastrophic forgetting. The authors propose a method called MetA Reusable Knowledge (MARK), which is based on the concept of common Knowledge Base (KB). The idea is that the KB of a network can be used as a mask to prevent forgetting of old information. The proposed method is evaluated on the 20-Split MiniImageNet dataset. "
18828,SP:722c52467e384058f8fdffa254d0e8db47440a64,"exact solvers USED-FOR Mixed Integer Programming ( MIP ). Primal heuristics USED-FOR exact solvers. MIP heuristics PART-OF solver. hard - coded rules USED-FOR solvers. rules USED-FOR problem. rules USED-FOR heuristics. data - driven framework USED-FOR scheduling heuristics. scheduling heuristics PART-OF exact MIP solver. data - driven framework USED-FOR exact MIP solver. algorithm USED-FOR schedule. Task are real - world applications, and learning task. Method are primal heuristics, problem - specific schedule of heuristics, and academic MIP solver. Metric is average primal integral. ","This paper proposes a data-driven framework for learning exact MIP solvers. The main idea is to learn a set of primal heuristics for solving mixed integer programming (MIP) problems, and then use them to learn an exact solver. The proposed method is based on the idea of learning a schedule of heuristic rules that can be used to guide the learning of a solver to solve a given problem. The authors show that the proposed method can be applied to a number of real-world MIP problems.","This paper proposes a data-driven framework for learning exact MIP solvers. The main idea is to learn a set of primal heuristics for solving mixed integer programming (MIP) problems, and then use them to learn an exact solver. The proposed method is based on the idea of learning a schedule of heuristic rules that can be used to guide the learning of a solver to solve a given problem. The authors show that the proposed method can be applied to a number of real-world MIP problems."
18892,SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"it COMPARE real - world applications. real - world applications COMPARE it. self - driving cars CONJUNCTION robotics. robotics CONJUNCTION self - driving cars. real - world applications PART-OF reinforcement learning. robotics HYPONYM-OF reinforcement learning. self - driving cars HYPONYM-OF reinforcement learning. robotics HYPONYM-OF real - world applications. self - driving cars HYPONYM-OF real - world applications. sublinear regret EVALUATE-FOR algorithm. unknown parametric model USED-FOR trajectory labels. Task are reinforcement learning ( RL ), RL practice, and learning. OtherScientificTerm are binary feedback, and reward signal. Generic is this. ","This paper studies the problem of learning trajectories for reinforcement learning with binary feedback. The authors propose an algorithm that learns a parametric model that predicts the trajectory labels of trajectories, and then uses this model to learn a reward function. They show that the proposed algorithm achieves sublinear regret and outperforms the baselines. They also show that their algorithm can be applied to a number of real-world applications. ","This paper studies the problem of learning trajectories for reinforcement learning with binary feedback. The authors propose an algorithm that learns a parametric model that predicts the trajectory labels of trajectories, and then uses this model to learn a reward function. They show that the proposed algorithm achieves sublinear regret and outperforms the baselines. They also show that their algorithm can be applied to a number of real-world applications. "
18956,SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"node embedding CONJUNCTION graph pooling methods. graph pooling methods CONJUNCTION node embedding. Graph neural networks USED-FOR representing graph - structured data. edges PART-OF graph. edges PART-OF graph. edges USED-FOR discrimination. graph reconstruction and generation CONJUNCTION graph classification tasks. graph classification tasks CONJUNCTION graph reconstruction and generation. graph classification tasks HYPONYM-OF tasks. graph reconstruction and generation HYPONYM-OF tasks. nodes PART-OF hypergraph. edges PART-OF graph. Dual Hypergraph Transformation ( DHT ) USED-FOR edge representation learning framework. message - passing techniques USED-FOR node representations. message - passing techniques USED-FOR edges. dual hypergraph construction USED-FOR message - passing techniques. hypergraphs USED-FOR edge representations. method COMPARE graph representation learning methods. graph representation learning methods COMPARE method. edge representation learning method USED-FOR graph representation and generation. graph datasets USED-FOR graph representation and generation. hypergraphs USED-FOR edge representation learning method. graph datasets EVALUATE-FOR hypergraphs. graph datasets EVALUATE-FOR edge representation learning method. lossless compression of the nodes CONJUNCTION removal of irrelevant edges. removal of irrelevant edges CONJUNCTION lossless compression of the nodes. edge representation learning and pooling method COMPARE graph pooling methods. graph pooling methods COMPARE edge representation learning and pooling method. graph pooling methods USED-FOR graph classification. edge representation learning and pooling method USED-FOR graph classification. Material is graph - structured data. Generic is they. OtherScientificTerm is connectivity. Method are graph representation learning, holistic graph - level edge representations, and edge representation learning. ",This paper proposes a dual hypergraph transformation (DHT) method for edge representation learning and pooling for graph classification tasks. The proposed method is based on the dual Hypergraph construction and uses message-passing techniques to learn edge representations. Experiments show that the proposed method outperforms existing graph representation learning methods and graph pooling methods.,This paper proposes a dual hypergraph transformation (DHT) method for edge representation learning and pooling for graph classification tasks. The proposed method is based on the dual Hypergraph construction and uses message-passing techniques to learn edge representations. Experiments show that the proposed method outperforms existing graph representation learning methods and graph pooling methods.
19020,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ","This paper studies the problem of learning representations of data in reinforcement learning. The authors propose a new objective called mutual information maximization (MI) that maximizes the mutual information between the state representation of the policy and the representations of high-dimensional observations. The proposed objective is motivated by the observation that the representation of high dimensional observations can be used to learn representations of low dimensional observations, which can then be used as the basis for learning representations for high dimensional data. The paper shows that the proposed objective can be applied to a variety of RL tasks, including reinforcement learning, reinforcement learning with reinforcement learning (RL), and reinforcement learning in a simulated game environment. Experiments are conducted to demonstrate the effectiveness of the proposed MI objective.","This paper studies the problem of learning representations of data in reinforcement learning. The authors propose a new objective called mutual information maximization (MI) that maximizes the mutual information between the state representation of the policy and the representations of high-dimensional observations. The proposed objective is motivated by the observation that the representation of high dimensional observations can be used to learn representations of low dimensional observations, which can then be used as the basis for learning representations for high dimensional data. The paper shows that the proposed objective can be applied to a variety of RL tasks, including reinforcement learning, reinforcement learning with reinforcement learning (RL), and reinforcement learning in a simulated game environment. Experiments are conducted to demonstrate the effectiveness of the proposed MI objective."
19084,SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"steerable convolution USED-FOR 3D semantic analysis. SS - Conv USED-FOR steerable convolution. sparse tensors USED-FOR steerable convolution. pipeline USED-FOR precise estimation of object poses. SS - Conv USED-FOR pipeline. Feature - Steering module USED-FOR pose refinement. SE(3)-equivariance USED-FOR Feature - Steering module. instance - level 6D pose estimation CONJUNCTION category - level 6D pose and size estimation. category - level 6D pose and size estimation CONJUNCTION instance - level 6D pose estimation. category - level 6D pose and size estimation CONJUNCTION categorylevel 6D pose tracking. categorylevel 6D pose tracking CONJUNCTION category - level 6D pose and size estimation. categorylevel 6D pose tracking HYPONYM-OF 3D object semantic analysis. instance - level 6D pose estimation HYPONYM-OF 3D object semantic analysis. category - level 6D pose and size estimation HYPONYM-OF 3D object semantic analysis. pipeline COMPARE methods. methods COMPARE pipeline. metrics EVALUATE-FOR tasks. tasks EVALUATE-FOR pipeline. tasks EVALUATE-FOR methods. metrics EVALUATE-FOR pipeline. metrics EVALUATE-FOR methods. SS - Conv USED-FOR pipeline. SS - Conv COMPARE convolutions. convolutions COMPARE SS - Conv. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR SS - Conv. accuracy EVALUATE-FOR SS - Conv. efficiency EVALUATE-FOR convolutions. accuracy EVALUATE-FOR convolutions. Method are SE(3)-equivariant deep feature learning, and Sparse Steerable Convolution ( SS - Conv ). Material is dense, volumetric data. Task is processing of 3D data. Generic are designs, and code. ",This paper proposes a steerable convolution (SS-conv) pipeline for 3D semantic analysis. The proposed pipeline is based on the SE(3)-equivariant deep feature learning (SE(3-equivariance) module and the feature-steering module for pose refinement. Experiments show that the proposed SS-conv pipeline outperforms the state-of-the-art methods on several tasks.,This paper proposes a steerable convolution (SS-conv) pipeline for 3D semantic analysis. The proposed pipeline is based on the SE(3)-equivariant deep feature learning (SE(3-equivariance) module and the feature-steering module for pose refinement. Experiments show that the proposed SS-conv pipeline outperforms the state-of-the-art methods on several tasks.
19148,SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"Attention USED-FOR vision transformers. informative tokens USED-FOR image recognition. dynamic token sparsification framework USED-FOR redundant tokens. lightweight prediction module USED-FOR importance score. features USED-FOR importance score. module USED-FOR redundant tokens. module PART-OF layers. layers USED-FOR redundant tokens. attention masking strategy USED-FOR prediction module. hierarchically pruning USED-FOR method. accuracy EVALUATE-FOR vision transformers. FLOPs EVALUATE-FOR method. accuracy EVALUATE-FOR method. throughput EVALUATE-FOR method. DynamicViT models COMPARE CNNs. CNNs COMPARE DynamicViT models. CNNs CONJUNCTION vision transformers. vision transformers CONJUNCTION CNNs. DynamicViT models COMPARE vision transformers. vision transformers COMPARE DynamicViT models. dynamic token sparsification framework USED-FOR DynamicViT models. ImageNet EVALUATE-FOR CNNs. ImageNet EVALUATE-FOR vision transformers. complexity / accuracy trade - offs EVALUATE-FOR DynamicViT models. Method are self - attention, and raoyongming. OtherScientificTerm is unstructured sparse tokens. Generic is framework. ",This paper proposes a dynamic token sparsification framework for self-attention in vision transformers. The key idea is to use a lightweight prediction module to reduce redundant tokens in the layers. The proposed method is evaluated on ImageNet and compares favorably to existing methods.,This paper proposes a dynamic token sparsification framework for self-attention in vision transformers. The key idea is to use a lightweight prediction module to reduce redundant tokens in the layers. The proposed method is evaluated on ImageNet and compares favorably to existing methods.
19212,SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"cross - validation methods CONJUNCTION conformal prediction. conformal prediction CONJUNCTION cross - validation methods. holdout methods CONJUNCTION cross - validation methods. cross - validation methods CONJUNCTION holdout methods. inference USED-FOR regression function. methods USED-FOR predictive inference. distribution - free guarantees USED-FOR predictive inference. methods USED-FOR distribution - free guarantees. inference USED-FOR inference. inference USED-FOR conditional mean. inference HYPONYM-OF regression function. conformal prediction HYPONYM-OF methods. holdout methods HYPONYM-OF methods. cross - validation methods HYPONYM-OF methods. non - vanishing width FEATURE-OF confidence interval. inference USED-FOR E [ Y |X ]. finite setting CONJUNCTION continuous setting. continuous setting CONJUNCTION finite setting. Task is data analysis problems. OtherScientificTerm are distributional assumptions, inference guarantees, sample size, and vanishing - width confidence intervals. ","This paper studies the problem of distribution-free predictive inference, which is an important problem in the data analysis community. The authors consider the case where the sample size is small and the confidence interval is non-vanishing. They show that under certain distributional assumptions, the confidence intervals of the conditional mean of the regression function are not vanishing. They also show that for a certain class of distributions, the vanishing-width confidence intervals can be obtained for any distributional assumption. ","This paper studies the problem of distribution-free predictive inference, which is an important problem in the data analysis community. The authors consider the case where the sample size is small and the confidence interval is non-vanishing. They show that under certain distributional assumptions, the confidence intervals of the conditional mean of the regression function are not vanishing. They also show that for a certain class of distributions, the vanishing-width confidence intervals can be obtained for any distributional assumption. "
19276,SP:123952325765c040c3078fc7dca2b6d370e55590,bias mitigation methods USED-FOR DNN models. learning debiased encoders USED-FOR bias mitigation methods. instance - level annotations USED-FOR sensitive attributes. fairness sensitive information PART-OF encoder. discrimination EVALUATE-FOR DNN models. task - specific classification head PART-OF DNN models. Representation Neutralization for Fairness ( RNF ) HYPONYM-OF mitigation technique. neutralized representations USED-FOR classification head. ground - truth label CONJUNCTION sensitive attributes. sensitive attributes CONJUNCTION ground - truth label. classification head PART-OF DNN model. neutralized representations USED-FOR DNN model. RNF USED-FOR classification head. fairness sensitive information PART-OF encoder representations. bias - amplified model USED-FOR proxy annotations. proxy annotations USED-FOR sensitive attributes. bias - amplified model USED-FOR low - resource settings. benchmark datasets EVALUATE-FOR RNF framework. RNF framework USED-FOR DNN models. benchmark datasets EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR RNF framework. Method is biased representations. Task is fairness. OtherScientificTerm is sensitive attribute annotations. ,"This paper proposes Representation Neutralization for Fairness (RNF), a method for learning debiased encoders that can be used to improve the fairness of DNN models. The proposed method is based on a bias-amplified model, which is used to augment the classification head of a DNN model with sensitive attribute annotations. The authors show that the proposed method can improve the performance of the model on several benchmark datasets. ","This paper proposes Representation Neutralization for Fairness (RNF), a method for learning debiased encoders that can be used to improve the fairness of DNN models. The proposed method is based on a bias-amplified model, which is used to augment the classification head of a DNN model with sensitive attribute annotations. The authors show that the proposed method can improve the performance of the model on several benchmark datasets. "
19340,SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,translations CONJUNCTION rotations. rotations CONJUNCTION translations. rotations FEATURE-OF learning models. translations FEATURE-OF learning models. learning models USED-FOR image analysis. Convolutional Neural Networks ( CNN ) USED-FOR image analysis. convolutions USED-FOR They. physics FEATURE-OF Bessel functions. Bessel functions USED-FOR convolutional layer. Task is medical imaging. Method is Bessel - CNNs ( B - CNNs ). OtherScientificTerm is rotation angles. ,"This paper studies the Bessel functions of convolutional neural networks (CNNs) for medical imaging. The authors propose a new convolution layer for Bessel-CNNs, which is based on the idea of Bessel function. They show that the convolutions of a Bessel layer are invariant to rotations, translations, and rotation angles. They also show that this invariance can be extended to the case where the rotation angles are not invariant. They further show that Bessel convolutions can be used for image analysis. ","This paper studies the Bessel functions of convolutional neural networks (CNNs) for medical imaging. The authors propose a new convolution layer for Bessel-CNNs, which is based on the idea of Bessel function. They show that the convolutions of a Bessel layer are invariant to rotations, translations, and rotation angles. They also show that this invariance can be extended to the case where the rotation angles are not invariant. They further show that Bessel convolutions can be used for image analysis. "
19404,SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,large - scale solver USED-FOR kernel ridge regression. ParK HYPONYM-OF large - scale solver. ParK HYPONYM-OF kernel ridge regression. random projections CONJUNCTION iterative optimization. iterative optimization CONJUNCTION random projections. partitioning CONJUNCTION random projections. random projections CONJUNCTION partitioning. partitioning CONJUNCTION iterative optimization. iterative optimization CONJUNCTION partitioning. partitioning PART-OF approach. iterative optimization PART-OF approach. space and time complexity EVALUATE-FOR approach. random projections PART-OF approach. statistical accuracy EVALUATE-FOR approach. local effective dimension CONJUNCTION bias. bias CONJUNCTION local effective dimension. orthogonality FEATURE-OF local estimators. feature space COMPARE input space. input space COMPARE feature space. feature space FEATURE-OF partitions. statistical - computational tradeoff EVALUATE-FOR model. large - scale datasets EVALUATE-FOR method. ,This paper proposes a new method for solving kernel ridge regression. The proposed method is based on the idea of orthogonality of local estimators. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of both space and time complexity. They also show that their method can be used to solve large-scale datasets.,This paper proposes a new method for solving kernel ridge regression. The proposed method is based on the idea of orthogonality of local estimators. The authors show that the proposed method outperforms the state-of-the-art baselines in terms of both space and time complexity. They also show that their method can be used to solve large-scale datasets.
19468,SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"reinforcement learning settings USED-FOR Neural agents. discrete tokens USED-FOR Neural agents. one - hot vectors USED-FOR discrete communication tokens. zero - shot understanding HYPONYM-OF communication. natural language processing USED-FOR word embedding techniques. discrete tokens USED-FOR neural agent architectures. continuous space USED-FOR discrete tokens. technique USED-FOR communication. technique COMPARE one - hot tokens. one - hot tokens COMPARE technique. decision theoretic framework EVALUATE-FOR technique. Generic are techniques, and method. OtherScientificTerm are human communication, unlabeled emergent agent communication, and one - hot communication. ",This paper proposes a new method for learning discrete communication tokens for zero-shot understanding in reinforcement learning. The key idea is to use discrete tokens in the continuous space to learn a discrete representation of the agent’s actions. The authors show that the proposed method outperforms one-hot communication in terms of zero shot understanding. They also provide a decision theoretic framework to explain the effectiveness of their method. ,This paper proposes a new method for learning discrete communication tokens for zero-shot understanding in reinforcement learning. The key idea is to use discrete tokens in the continuous space to learn a discrete representation of the agent’s actions. The authors show that the proposed method outperforms one-hot communication in terms of zero shot understanding. They also provide a decision theoretic framework to explain the effectiveness of their method. 
19532,SP:8630ccc627534f9033bced04e2137a897ffef701,they COMPARE convolutional networks. convolutional networks COMPARE they. Transformers USED-FOR computer vision. generalization COMPARE convolutional networks. convolutional networks COMPARE generalization. model capacity FEATURE-OF Transformers. depthwise Convolution CONJUNCTION self - Attention. self - Attention CONJUNCTION depthwise Convolution. convolution layers CONJUNCTION attention layers. attention layers CONJUNCTION convolution layers. CoAtNets HYPONYM-OF hybrid models. capacity CONJUNCTION efficiency. efficiency CONJUNCTION capacity. generalization CONJUNCTION capacity. capacity CONJUNCTION generalization. coat ” nets HYPONYM-OF CoAtNets. relative attention USED-FOR hybrid models. relative attention USED-FOR depthwise Convolution. relative attention USED-FOR self - Attention. CoAtNets COMPARE CoAtNet. CoAtNet COMPARE CoAtNets. resource constraints FEATURE-OF CoAtNets. JFT-3B USED-FOR CoAtNet. top-1 accuracy EVALUATE-FOR CoAtNet. top-1 accuracy EVALUATE-FOR it. ImageNet EVALUATE-FOR it. OtherScientificTerm is inductive bias. Generic is architectures. Metric is ImageNet top-1 accuracy. Material is ImageNet-21 K. ,"This paper proposes CoAtNets, a hybrid model that combines self-attention and convolutional layers. The proposed CoAtNet is based on the idea of relative attention and self-convolution. The authors show that the proposed model outperforms the state-of-the-art in terms of top-1 accuracy and generalization on ImageNet-21K. The paper also shows that the performance of the proposed method is comparable to that of the state of the art on JFT-3B.","This paper proposes CoAtNets, a hybrid model that combines self-attention and convolutional layers. The proposed CoAtNet is based on the idea of relative attention and self-convolution. The authors show that the proposed model outperforms the state-of-the-art in terms of top-1 accuracy and generalization on ImageNet-21K. The paper also shows that the performance of the proposed method is comparable to that of the state of the art on JFT-3B."
19596,SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,second - order oracle bound USED-FOR expected risk. expected risk FEATURE-OF weighted majority vote. second - order oracle bound USED-FOR weighted majority vote. one - sided Chebyshev ’s ) HYPONYM-OF parametric form of the ChebyshevCantelli inequality. parametric form of the ChebyshevCantelli inequality USED-FOR bound. form USED-FOR optimization challenge. Chebyshev - Cantelli inequality CONJUNCTION C - bounds. C - bounds CONJUNCTION Chebyshev - Cantelli inequality. optimization challenge USED-FOR prior oracle bounds. Chebyshev - Cantelli inequality USED-FOR prior oracle bounds. it USED-FOR oracle bound. second order Markov ’s inequality USED-FOR it. second order Markov ’s inequality USED-FOR oracle bound. PAC - Bayesian bounding CONJUNCTION Bennett ’s inequality. Bennett ’s inequality CONJUNCTION PAC - Bayesian bounding. PAC - Bayes - Bennett HYPONYM-OF concentration of measure inequality. PAC - Bayesian bounding PART-OF it. Bennett ’s inequality PART-OF it. it USED-FOR empirical estimation of the oracle bound. PAC - Bayes - Bennett inequality COMPARE PAC - Bayes - Bernstein inequality. PAC - Bayes - Bernstein inequality COMPARE PAC - Bayes - Bennett inequality. ChebyshevCantelli inequality CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION ChebyshevCantelli inequality. parametric form USED-FOR concentration of measure. PAC - Bayes - Bennett inequality USED-FOR concentration of measure. parametric form CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION parametric form. parametric form FEATURE-OF ChebyshevCantelli inequality. Task is minimization. Generic is bounds. ,"This paper studies the oracle bound of the weighted majority vote for the one-sided Chebyshev-Cantelli inequality. The authors propose a parametric form of the inequality, which they call PAC-Bayes-Bennett (PAC-BENNETT). They show that it is equivalent to the PAC Bayes-Bernstein inequality, and that it can be approximated by the second-order Markov's inequality. They also show that the PAC- Bayesian bounding and PAC-C-Bayesian bounds are equivalent to these two prior oracle bounds. ","This paper studies the oracle bound of the weighted majority vote for the one-sided Chebyshev-Cantelli inequality. The authors propose a parametric form of the inequality, which they call PAC-Bayes-Bennett (PAC-BENNETT). They show that it is equivalent to the PAC Bayes-Bernstein inequality, and that it can be approximated by the second-order Markov's inequality. They also show that the PAC- Bayesian bounding and PAC-C-Bayesian bounds are equivalent to these two prior oracle bounds. "
19660,SP:5bac542a6532d43cf100e085398b4a4783719814,"audio - visual video parsing task USED-FOR audio or visual event categories. method USED-FOR audio or visual events. common and diverse event semantics USED-FOR audio or visual events. common and diverse event semantics USED-FOR method. method USED-FOR event co - occurrence. method COMPARE methods. methods COMPARE method. weakly - supervised audio - visual video parsing EVALUATE-FOR method. weakly - supervised audio - visual video parsing EVALUATE-FOR methods. OtherScientificTerm are audio and visual events, cross - modality co - occurrence, supervisory signals, and video - level annotations. Method is parsing model. ","This paper proposes a method for weakly supervised audio-visual video parsing. The proposed method is based on the idea of cross-modality co-occurrence, which is a common problem in video-to-audio video parsing tasks. The authors propose a method that is able to capture both audio and visual events in the context of video-level annotations. They show that the proposed method outperforms existing methods in terms of performance on the weakly-supervised task.","This paper proposes a method for weakly supervised audio-visual video parsing. The proposed method is based on the idea of cross-modality co-occurrence, which is a common problem in video-to-audio video parsing tasks. The authors propose a method that is able to capture both audio and visual events in the context of video-level annotations. They show that the proposed method outperforms existing methods in terms of performance on the weakly-supervised task."
19724,SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"federated learning ( FL ) USED-FOR global model. quantized and personalized FL algorithm USED-FOR collective ( personalized model compression ) training. knowledge distillation ( KD ) USED-FOR collective ( personalized model compression ) training. quantization parameters CONJUNCTION model dimensions / structures. model dimensions / structures CONJUNCTION quantization parameters. model dimensions / structures FEATURE-OF compressed personalized models. quantization parameters FEATURE-OF compressed personalized models. algorithm USED-FOR quantized models. relaxed optimization problem USED-FOR algorithm. knowledge distillation loss USED-FOR local client objectives. global model USED-FOR knowledge distillation loss. model dimension EVALUATE-FOR compressed model. knowledge distillation loss USED-FOR compressed personalization framework. alternating proximal gradient update USED-FOR compressed personalization problem. FedAvg CONJUNCTION local training of clients. local training of clients CONJUNCTION FedAvg. QuPeD COMPARE personalized FL methods. personalized FL methods COMPARE QuPeD. personalized FL methods CONJUNCTION FedAvg. FedAvg CONJUNCTION personalized FL methods. QuPeD COMPARE FedAvg. FedAvg COMPARE QuPeD. QuPeD COMPARE local training of clients. local training of clients COMPARE QuPeD. local training of clients HYPONYM-OF personalized FL methods. Method are FL algorithms, and ( federated ) learning process. Material is heterogeneous data. Task is personalization. OtherScientificTerm is quantization values. ","This paper proposes a new personalized model compression method for federated learning. The proposed method is based on the knowledge distillation (KD) loss for quantized and personalized models. The authors show that the KD loss can be decomposed into two components: (1) the global KD loss and (2) the local KD loss, which is used for local client objectives. Experiments are conducted to demonstrate the effectiveness of the proposed method.","This paper proposes a new personalized model compression method for federated learning. The proposed method is based on the knowledge distillation (KD) loss for quantized and personalized models. The authors show that the KD loss can be decomposed into two components: (1) the global KD loss and (2) the local KD loss, which is used for local client objectives. Experiments are conducted to demonstrate the effectiveness of the proposed method."
19788,SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"Constrained clustering PART-OF machine learning. partially labeled data USED-FOR prior information. prior information USED-FOR it. partially labeled data USED-FOR it. framework USED-FOR constrained clustering. deep generative models USED-FOR framework. stochastic gradient variational inference USED-FOR framework. domain knowledge USED-FOR model ( DC - GMM ). probabilistic relations FEATURE-OF domain knowledge. DC - GMM COMPARE deep constrained clustering methods. deep constrained clustering methods COMPARE DC - GMM. robustness EVALUATE-FOR deep constrained clustering methods. data sets EVALUATE-FOR DC - GMM. data sets EVALUATE-FOR deep constrained clustering methods. robustness EVALUATE-FOR DC - GMM. real - world applications EVALUATE-FOR approach. Generic are model, and constraints. OtherScientificTerm are prior clustering preferences, and pairwise constraints. Task is clustering process. ",This paper proposes a framework for constrained clustering based on domain knowledge (DC-GMM). The proposed framework is based on a deep generative model that is trained on a set of partially labeled data. The authors propose to use a stochastic gradient variational inference approach to learn the domain knowledge of the model. The proposed method is evaluated on a number of real-world datasets.,This paper proposes a framework for constrained clustering based on domain knowledge (DC-GMM). The proposed framework is based on a deep generative model that is trained on a set of partially labeled data. The authors propose to use a stochastic gradient variational inference approach to learn the domain knowledge of the model. The proposed method is evaluated on a number of real-world datasets.
19852,SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,Neural Tangent Kernel ( NTK ) USED-FOR infinitely - wide neural networks. least squares loss USED-FOR infinitely - wide neural networks. gradient descent USED-FOR least squares loss. gradient descent USED-FOR infinitely - wide neural networks. NTK regression COMPARE finitely - wide neural networks. finitely - wide neural networks COMPARE NTK regression. small - scale datasets USED-FOR finitely - wide neural networks. kernel methods USED-FOR large - scale learning tasks. computational complexity EVALUATE-FOR kernel methods. near input - sparsity time approximation algorithm USED-FOR NTK. near input - sparsity time approximation algorithm USED-FOR learning. polynomial expansions of arc - cosine kernels USED-FOR near input - sparsity time approximation algorithm. NTK USED-FOR learning. spectral approximation guarantee USED-FOR NTK matrix. random features CONJUNCTION sketching algorithm. sketching algorithm CONJUNCTION random features. random features PART-OF arc - cosine kernels. leverage score sampling USED-FOR random features. accuracy EVALUATE-FOR CNTK. linear regressor COMPARE CNTK. CNTK COMPARE linear regressor. accuracy EVALUATE-FOR linear regressor. speedup EVALUATE-FOR linear regressor. CNTK features USED-FOR linear regressor. speedup EVALUATE-FOR CNTK. large - scale regression and classification tasks EVALUATE-FOR methods. CIFAR-10 dataset EVALUATE-FOR CNTK. Method is convolutional counterpart of NTK ( CNTK ). OtherScientificTerm is linear runtime. ,"This paper proposes a convolutional version of Neural Tangent Kernel (NTK) for learning infinitely-wide neural networks. The authors propose a near-input-sparsity time approximation algorithm for learning the NTK matrix, which is based on polynomial expansions of arc-cosine kernels. The proposed method is evaluated on CIFAR-10 and MNIST datasets, and is shown to outperform existing methods. ","This paper proposes a convolutional version of Neural Tangent Kernel (NTK) for learning infinitely-wide neural networks. The authors propose a near-input-sparsity time approximation algorithm for learning the NTK matrix, which is based on polynomial expansions of arc-cosine kernels. The proposed method is evaluated on CIFAR-10 and MNIST datasets, and is shown to outperform existing methods. "
19916,SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"framework USED-FOR multi - person 3D motion trajectory prediction. local - range encoder CONJUNCTION global - range encoder. global - range encoder CONJUNCTION local - range encoder. global - range encoder USED-FOR social interactions. local - range encoder USED-FOR individual motion. local - range encoder PART-OF Multi - Range Transformers model. global - range encoder PART-OF Multi - Range Transformers model. Transformer decoder USED-FOR prediction. model COMPARE methods. methods COMPARE model. long - term 3D motion prediction EVALUATE-FOR methods. long - term 3D motion prediction EVALUATE-FOR model. OtherScientificTerm are human pose trajectory, and local and global - range encoder features. ","This paper proposes a multi-person 3D motion trajectory prediction framework. The proposed framework is based on the Transformer architecture. The model is trained with a global-range encoder and a local-range decoder. The global encoder is used to predict the trajectory of each person, while the local encoder predicts the trajectory for each individual person. Experiments show that the proposed model outperforms the baselines.","This paper proposes a multi-person 3D motion trajectory prediction framework. The proposed framework is based on the Transformer architecture. The model is trained with a global-range encoder and a local-range decoder. The global encoder is used to predict the trajectory of each person, while the local encoder predicts the trajectory for each individual person. Experiments show that the proposed model outperforms the baselines."
19997,SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"reinforcement learning USED-FOR long - horizon planning problems. programs USED-FOR reinforcement learning. programs USED-FOR settings. strategy USED-FOR program. program synthesis USED-FOR guiding programs. generative model USED-FOR It. It USED-FOR program. model USED-FOR program. approach COMPARE non - program - guided approaches. non - program - guided approaches COMPARE approach. benchmarks EVALUATE-FOR non - program - guided approaches. 2D Minecraft - inspired environment HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR approach. program - guided reinforcement learning USED-FOR approach. Generic is approaches. Method are guiding program, model predictive program synthesis ( MPPS ), and handcrafted programs. Task is programming task. ","This paper proposes a new approach for program-guided reinforcement learning. The approach is based on a generative model to generate a program that can be used to guide the learning of a reinforcement learning agent to solve a given task. The model is trained on a set of handcrafted programs, which are then used to train the agent to execute the program. The proposed approach is evaluated on a number of tasks, including a Minecraft-inspired environment, a 2D Minecraft-based environment, and a 3D Minecraft environment.","This paper proposes a new approach for program-guided reinforcement learning. The approach is based on a generative model to generate a program that can be used to guide the learning of a reinforcement learning agent to solve a given task. The model is trained on a set of handcrafted programs, which are then used to train the agent to execute the program. The proposed approach is evaluated on a number of tasks, including a Minecraft-inspired environment, a 2D Minecraft-based environment, and a 3D Minecraft environment."
20078,SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"causal imitation learning USED-FOR Imitation learning. sequential settings USED-FOR causal imitation learning. graphical criterion USED-FOR causal imitation. algorithm USED-FOR imitability. Task are naïve imitation, and single - stage decision - making. OtherScientificTerm are sensors, imitator, demonstrator ’s behavior ( DO ), and demonstrator. Generic is theory. ","This paper studies the problem of causal imitation learning in sequential settings, where the goal is to learn to imitate the behavior of a demonstrator in a single-stage decision-making task. The authors propose an algorithm to learn a graphical criterion for causal imitation. The criterion is based on the observation that the demonstrator’s behavior (DO) can be used as a surrogate for the imitator's behavior (i.e., the difference between the two actions) in a sequential setting. The algorithm is evaluated on a number of synthetic and real-world datasets, and is shown to outperform baselines.","This paper studies the problem of causal imitation learning in sequential settings, where the goal is to learn to imitate the behavior of a demonstrator in a single-stage decision-making task. The authors propose an algorithm to learn a graphical criterion for causal imitation. The criterion is based on the observation that the demonstrator’s behavior (DO) can be used as a surrogate for the imitator's behavior (i.e., the difference between the two actions) in a sequential setting. The algorithm is evaluated on a number of synthetic and real-world datasets, and is shown to outperform baselines."
20159,SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,transition losses USED-FOR object - structured representation. object - structured representation COMPARE pixels. pixels COMPARE object - structured representation. transition losses COMPARE pixels. pixels COMPARE transition losses. transition losses USED-FOR model. object persistence CONJUNCTION object identity. object identity CONJUNCTION object persistence. alignment module USED-FOR model. object identity HYPONYM-OF transition models. object persistence HYPONYM-OF transition models. objectlevel loss CONJUNCTION object alignment. object alignment CONJUNCTION objectlevel loss. object occlusion CONJUNCTION re - appearance. re - appearance CONJUNCTION object occlusion. model COMPARE baseline. baseline COMPARE model. it USED-FOR object occlusion. it USED-FOR re - appearance. partially observable environments FEATURE-OF re - appearance. partially observable environments FEATURE-OF object occlusion. OtherScientificTerm is slot - wise object memory. ,"This paper proposes a new model for object memory that combines object alignment, object persistence, and object occlusion. The proposed model is based on the idea of object-level loss and object alignment module. The authors show that the proposed model outperforms the state-of-the-art in object memory and re-appearance. ","This paper proposes a new model for object memory that combines object alignment, object persistence, and object occlusion. The proposed model is based on the idea of object-level loss and object alignment module. The authors show that the proposed model outperforms the state-of-the-art in object memory and re-appearance. "
20240,SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"classification CONJUNCTION regression. regression CONJUNCTION classification. regression CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION regression. classification CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION classification. Empirical risk minimization ( ERM ) PART-OF machine learning. classification HYPONYM-OF machine learning. adaptively collected data USED-FOR generic importance sampling weighted ERM algorithm. maximal inequality USED-FOR rates. exploration rate FEATURE-OF rates. fast rates USED-FOR regression. convexity of squared - error loss USED-FOR fast rates. regret guarantees USED-FOR policy learning. OtherScientificTerm are modelagnostic guarantees, hypothesis class, importance sampling structure, and exploration. Method is contextual bandit algorithm. Metric is fast convergence rates. Material is bandit - collected data. Generic is theory. ","This paper studies the problem of empirical risk minimization (ERM) in the context of contextual bandit algorithms. The authors consider the case where the data is adaptively collected from a bandit algorithm. They show that the exploration rate of an ERM algorithm is bounded by the maximal inequality of the squared-error loss. They also show that under certain assumptions on the hypothesis class and the importance sampling structure of the algorithm, the exploration rates converge to fast convergence rates. ","This paper studies the problem of empirical risk minimization (ERM) in the context of contextual bandit algorithms. The authors consider the case where the data is adaptively collected from a bandit algorithm. They show that the exploration rate of an ERM algorithm is bounded by the maximal inequality of the squared-error loss. They also show that under certain assumptions on the hypothesis class and the importance sampling structure of the algorithm, the exploration rates converge to fast convergence rates. "
20321,SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,weighted regression model USED-FOR machine learning tasks. predictive power EVALUATE-FOR models. low sample sizes CONJUNCTION covariate perturbations. covariate perturbations CONJUNCTION low sample sizes. mitigation strategy USED-FOR problems. doubly non - negative matrix USED-FOR sample weights. log - determinant divergence CONJUNCTION Bures - Wasserstein distance. Bures - Wasserstein distance CONJUNCTION log - determinant divergence. uncertainty set FEATURE-OF weighting matrix. Bures - Wasserstein distance USED-FOR weighting matrix. log - determinant divergence USED-FOR weighting matrix. first - order methods USED-FOR adversarially reweighted estimate. Task is kernel - reweighted regression. Method is reweighting strategy. ,This paper proposes an adversarial reweighting strategy for kernel-reweighted regression. The proposed method is based on the duality of the weighting matrix and the Bures-Wasserstein distance between the model and the uncertainty set. The authors show that the proposed method can be used to mitigate low sample sizes and covariate perturbations. The paper also shows that adversarially reweighted estimators are robust to adversarial attacks. ,This paper proposes an adversarial reweighting strategy for kernel-reweighted regression. The proposed method is based on the duality of the weighting matrix and the Bures-Wasserstein distance between the model and the uncertainty set. The authors show that the proposed method can be used to mitigate low sample sizes and covariate perturbations. The paper also shows that adversarially reweighted estimators are robust to adversarial attacks. 
20402,SP:fe12e13602925b9400fd596a987755beb10aa3d1,"discrete latent variables USED-FOR models. continuous relaxation USED-FOR low - variance reparameterization gradients. binary random variables USED-FOR it. importance sampling CONJUNCTION statistical couplings. statistical couplings CONJUNCTION importance sampling. importance sampling USED-FOR estimator. statistical couplings USED-FOR estimator. sequences of binary variables CONJUNCTION Rao - Blackwellization. Rao - Blackwellization CONJUNCTION sequences of binary variables. Rao - Blackwellization USED-FOR reparameterizing categorical variables. sequences of binary variables USED-FOR reparameterizing categorical variables. reparameterizing categorical variables USED-FOR gradient estimators. Rao - Blackwellization USED-FOR gradient estimators. estimators COMPARE REINFORCE. REINFORCE COMPARE estimators. leave - one - out - baseline estimator USED-FOR estimators. leave - one - out - baseline estimator USED-FOR REINFORCE. Method are unbiased gradient estimators, performant estimator, continuous relaxations, and categorical gradient estimators. Material is categorical setting. OtherScientificTerm is stick - breaking coupling. ","This paper proposes REINFORCE, an unbiased estimator for categorical reparameterization gradients, which is based on continuous relaxation. The authors show that the estimator is performant in the presence of stick-breaking coupling and importance sampling. They also show that it is also performant when the covariance matrix is a binary random variable. ","This paper proposes REINFORCE, an unbiased estimator for categorical reparameterization gradients, which is based on continuous relaxation. The authors show that the estimator is performant in the presence of stick-breaking coupling and importance sampling. They also show that it is also performant when the covariance matrix is a binary random variable. "
20483,SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"predictors USED-FOR top architectures. search path USED-FOR high - performance sub - space. weaker predictors USED-FOR search path. strong predictor USED-FOR architecture space. predictor USED-FOR well - performed architectures. coarse - to - fine iteration USED-FOR ranking of sampling space. NAS - Bench-101 CONJUNCTION NAS - Bench-201. NAS - Bench-201 CONJUNCTION NAS - Bench-101. WeakNAS USED-FOR top - performance architectures. ImageNet MobileNet Search Space EVALUATE-FOR SOTA. SOTA EVALUATE-FOR WeakNAS. ImageNet MobileNet Search Space EVALUATE-FOR WeakNAS. Method are Neural Architecture Search ( NAS ), predictor - based NAS approaches, proxy accuracy predictor, and weak predictor. OtherScientificTerm are architecture - performance pairs, and weak predictors. Generic are architecture, and framework. ","This paper proposes a new method for neural architecture search (NAS) called WeakNAS. WeakNAS is based on a proxy accuracy predictor, which can be used to find the top-performing architectures in the sub-space of the search space. The proposed method is evaluated on NAS-Bench-101, NAS-201, and NAS-bench-201. The paper shows that the proposed method outperforms existing methods in terms of performance.","This paper proposes a new method for neural architecture search (NAS) called WeakNAS. WeakNAS is based on a proxy accuracy predictor, which can be used to find the top-performing architectures in the sub-space of the search space. The proposed method is evaluated on NAS-Bench-101, NAS-201, and NAS-bench-201. The paper shows that the proposed method outperforms existing methods in terms of performance."
20564,SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"latent codes PART-OF globally consistent coordinate system. Entropic Desired Dynamics USED-FOR Intrinsic ConTrol ( EDDICT ). tractable learning CONJUNCTION interpretable latent space. interpretable latent space CONJUNCTION tractable learning. EDDICT ’s globally consistent codes USED-FOR it. prior methods COMPARE EDDICT ’s globally consistent codes. EDDICT ’s globally consistent codes COMPARE prior methods. state coverage CONJUNCTION unsupervised. unsupervised CONJUNCTION state coverage. hard exploration games EVALUATE-FOR unsupervised. Montezuma ’s Revenge HYPONYM-OF hard exploration games. OtherScientificTerm are local objective, and fixed additive latent dynamics. ","This paper proposes Intrinsic ConTrol (EDDICT), a method for learning globally consistent coordinate systems. EDDICT is based on the Entropic Desired Dynamics framework. The key idea is to learn a global coordinate system that is invariant to changes in the local objective, which is then used to learn globally consistent codes. The proposed method is evaluated on Montezuma’s Revenge, a hard exploration game. ","This paper proposes Intrinsic ConTrol (EDDICT), a method for learning globally consistent coordinate systems. EDDICT is based on the Entropic Desired Dynamics framework. The key idea is to learn a global coordinate system that is invariant to changes in the local objective, which is then used to learn globally consistent codes. The proposed method is evaluated on Montezuma’s Revenge, a hard exploration game. "
20645,SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"reinforcement learning ( RL ) USED-FOR drug design. reward scoring function USED-FOR RL. molecular docking program USED-FOR RL. molecular docking program HYPONYM-OF physical simulation. molecular docking program HYPONYM-OF reward scoring function. physical simulation USED-FOR protein - small molecule binding affinity. models USED-FOR chemically realistic and pharmacochemically acceptable molecules. local optima CONJUNCTION smooth surfaces. smooth surfaces CONJUNCTION local optima. docking score optimization HYPONYM-OF exploration problem. RL framework USED-FOR pharmacochemically acceptable molecules. docking scores FEATURE-OF pharmacochemically acceptable molecules. fragment - based generation method CONJUNCTION error - prioritized experience replay ( PER ). error - prioritized experience replay ( PER ) CONJUNCTION fragment - based generation method. Explorative Experience replay USED-FOR Drug design ( FREED ). Explorative Experience replay USED-FOR Fragment - based generative RL. de novo and scaffold - based schemes EVALUATE-FOR model. model COMPARE methods. methods COMPARE model. method USED-FOR model. predictive error - PER ( FREED(PE ) ) USED-FOR model. predictive error - PER ( FREED(PE ) ) HYPONYM-OF method. OtherScientificTerm are molecular structure, realistic and qualified chemical space, and drugs. ","This paper proposes a new framework for drug design based on a molecular docking program. The proposed framework is based on the idea of reward-priorized experience replay (PER), which is an extension of the fragment-based generation method (FREED) and Explorative Experience Replay (EXR). The authors show that the proposed method is able to generate molecules that are pharmacochemically acceptable and can be used to generate new drugs. The authors also show that their method can be applied to de-novo and scaffold-based drug design.","This paper proposes a new framework for drug design based on a molecular docking program. The proposed framework is based on the idea of reward-priorized experience replay (PER), which is an extension of the fragment-based generation method (FREED) and Explorative Experience Replay (EXR). The authors show that the proposed method is able to generate molecules that are pharmacochemically acceptable and can be used to generate new drugs. The authors also show that their method can be applied to de-novo and scaffold-based drug design."
20726,SP:b938bca513e7de1231212064caf8877a78d8b612,"complexity EVALUATE-FOR directed acyclic graphical models. observational data USED-FOR directed acyclic graphical models. local Markov boundary search procedure USED-FOR ancestral sets. ancestral sets PART-OF graphical model. local Markov boundary search procedure USED-FOR approach. forward greedy search algorithm USED-FOR Markov boundary. backward pruning phase PART-OF forward greedy search algorithm. forward greedy search algorithm USED-FOR graph ensembles. identifiability condition FEATURE-OF graph. finite - sample guarantees USED-FOR recovering Markov boundaries. results USED-FOR polytrees. polynomial time FEATURE-OF polytrees. minimal assumptions USED-FOR structure of directed graphical models. approach USED-FOR discrete or continuous distributions. OtherScientificTerm are distributional assumptions, nodes, and Markov boundaries. Metric is sample complexity. Generic is algorithm. ",This paper studies the problem of learning directed acyclic graphical models from observational data. The authors propose a local Markov boundary search procedure to recover the ancestral sets of the graph ensembles. The proposed method is based on the forward greedy search algorithm and the backward pruning phase. The method is shown to recover Markov boundaries in polynomial time for discrete and continuous distributions. The paper also provides a finite sample guarantee for the algorithm. ,This paper studies the problem of learning directed acyclic graphical models from observational data. The authors propose a local Markov boundary search procedure to recover the ancestral sets of the graph ensembles. The proposed method is based on the forward greedy search algorithm and the backward pruning phase. The method is shown to recover Markov boundaries in polynomial time for discrete and continuous distributions. The paper also provides a finite sample guarantee for the algorithm. 
20807,SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"( "", ) DP algorithm USED-FOR privately learnable class. public randomness USED-FOR global stability. Task is learning with differential privacy ( DP ). OtherScientificTerm are privacy protection, probabilistic representation dimension, and nearly - matching lower bound. Method are "" -DP algorithms, local model, and correlated sampling strategy. ","This paper studies the problem of learning with differential privacy (DP) in the presence of public randomness. In particular, the authors consider the setting where the data distribution is private, and the goal is to learn a privately learnable class. In this setting, they show that the global stability of the learned class is guaranteed by a nearly-matching lower bound on the probabilistic representation dimension of the private class. They also show that this lower bound can be extended to the case where the public distribution is not private. ","This paper studies the problem of learning with differential privacy (DP) in the presence of public randomness. In particular, the authors consider the setting where the data distribution is private, and the goal is to learn a privately learnable class. In this setting, they show that the global stability of the learned class is guaranteed by a nearly-matching lower bound on the probabilistic representation dimension of the private class. They also show that this lower bound can be extended to the case where the public distribution is not private. "
20888,SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"Estimating the per - state expected cumulative rewards PART-OF reinforcement learning approaches. latent Markov decision process USED-FOR transition and reward models. gradient descent USED-FOR global optima. gradient descent USED-FOR linear parametrization. convergence rates USED-FOR cases. stochastic gradient descent ( SGD ) COMPARE explicit counterpart. explicit counterpart COMPARE stochastic gradient descent ( SGD ). implicit representation COMPARE explicit counterpart. explicit counterpart COMPARE implicit representation. implicit representation USED-FOR stochastic gradient descent ( SGD ). OtherScientificTerm are per - state expected cumulative rewards, and value predictions. Method are deep neural - network function - approximation methods, value iteration networks, and implicit representations of value functions. Generic is approach. ","This paper studies the problem of learning the per-state expected cumulative rewards of a reward model. The authors propose a new implicit representation of the value function, which can be used to approximate the global optima of the transition and reward models. The paper shows that the implicit representation is equivalent to the explicit representation of SGD in the case of linear parametrization, and that the convergence rate of the implicit value function is bounded by a constant factor. ","This paper studies the problem of learning the per-state expected cumulative rewards of a reward model. The authors propose a new implicit representation of the value function, which can be used to approximate the global optima of the transition and reward models. The paper shows that the implicit representation is equivalent to the explicit representation of SGD in the case of linear parametrization, and that the convergence rate of the implicit value function is bounded by a constant factor. "
20988,SP:992aa07d4f815d1c81f967374590eece933833b1,text sources USED-FOR Knowledge Graphs ( KGs ). embeddings USED-FOR inferring new facts. KG refinement task USED-FOR KGs. embeddings USED-FOR KGs. techniques USED-FOR KG refinement. inference rules USED-FOR techniques. ontological information USED-FOR embeddings. ontological information CONJUNCTION inferences rules. inferences rules CONJUNCTION ontological information. IterefinE HYPONYM-OF KG refinement framework. inferences rules USED-FOR one. ontological information USED-FOR one. ComplEx HYPONYM-OF KG embeddings. KG embeddings USED-FOR IterefinE. ontological information USED-FOR IterefinE. type - supervised embeddings USED-FOR KG. KG benchmarks EVALUATE-FOR embeddings. embeddings USED-FOR KG. PSL - KGI USED-FOR KG. overall weighted F1 score EVALUATE-FOR embeddings. Task is KG - based question answering. OtherScientificTerm is ontologies. Method is IterefinE framework. ,"This paper proposes a method for learning knowledge graph embeddings for KG-based question answering. The proposed method is based on the idea of type-supervised KG refinement. The authors propose a new method called IterefinE, which combines the knowledge graph refinement (KG refinement) task with the question answering task. The method is evaluated on a number of KG benchmarks and is shown to outperform the baselines. ","This paper proposes a method for learning knowledge graph embeddings for KG-based question answering. The proposed method is based on the idea of type-supervised KG refinement. The authors propose a new method called IterefinE, which combines the knowledge graph refinement (KG refinement) task with the question answering task. The method is evaluated on a number of KG benchmarks and is shown to outperform the baselines. "
20997,SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"binary predictions USED-FOR KBC quality. evaluation paradigm USED-FOR model selection criteria. real - world entities PART-OF KB. model COMPARE KB. KB COMPARE model. benchmark EVALUATE-FOR KB embeddings models. completion task EVALUATE-FOR ranking task. prediction separability FEATURE-OF KB embedding models. thresholding PART-OF TransE. classification F1 score EVALUATE-FOR TransE. Method is Knowledge base completion ( KBC ) methods. Material are knowledge base ( KB ), and FB14k - QAQ. Generic are method, and models. OtherScientificTerm are likelihood ranking, evaluation data structure, and KB queries. Task are ranking setting, and ranking - based and classification - based evaluation. ","This paper proposes a new evaluation framework for knowledge base completion (KBC) based on binary predictions. The authors propose a new benchmark, FB14k-QAQ, to evaluate the performance of KB embeddings models on the ranking and classification tasks. The benchmark consists of a ranking task and a classification task, and the authors show that the proposed method outperforms existing KB embedding models on both tasks. ","This paper proposes a new evaluation framework for knowledge base completion (KBC) based on binary predictions. The authors propose a new benchmark, FB14k-QAQ, to evaluate the performance of KB embeddings models on the ranking and classification tasks. The benchmark consists of a ranking task and a classification task, and the authors show that the proposed method outperforms existing KB embedding models on both tasks. "
21006,SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,dialog system models USED-FOR tasks. human annotations USED-FOR dialog system models. language priors USED-FOR down - stream NLP tasks. BERT CONJUNCTION GPT-2. GPT-2 CONJUNCTION BERT. GPT-2 HYPONYM-OF large pre - trained language models. BERT HYPONYM-OF large pre - trained language models. pre - trained language models USED-FOR dialog response generation. Alternating Roles Dialog Model ( ARDM ) HYPONYM-OF framework. large pretrained language model USED-FOR ARDM. belief states CONJUNCTION dialog acts. dialog acts CONJUNCTION belief states. It USED-FOR conversations. supervision USED-FOR It. human annotations USED-FOR It. belief states HYPONYM-OF human annotations. dialog acts HYPONYM-OF human annotations. ARDM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ARDM. CamRest676 CONJUNCTION MultiWOZ. MultiWOZ CONJUNCTION CamRest676. task - oriented dialog datasets EVALUATE-FOR state - of - the - art methods. task - oriented dialog datasets EVALUATE-FOR ARDM. MultiWOZ HYPONYM-OF task - oriented dialog datasets. CamRest676 HYPONYM-OF task - oriented dialog datasets. ARDM USED-FOR non - collaborative tasks. persuasion HYPONYM-OF non - collaborative tasks. ARDM USED-FOR human - like responses. ARDM USED-FOR persuasion tasks. ,"This paper proposes a new method for dialog generation, called Alternating Roles Dialog Model (ARDM), which uses a pre-trained language model to generate human-like dialog responses. The method is based on the idea that human annotations can be used to improve the performance of dialog system models. The proposed method is evaluated on a number of task-oriented dialog datasets, including MultiWOZ, CamRest676, and Multi-WOWZ. The results show that the proposed method outperforms state-of-the-art methods.","This paper proposes a new method for dialog generation, called Alternating Roles Dialog Model (ARDM), which uses a pre-trained language model to generate human-like dialog responses. The method is based on the idea that human annotations can be used to improve the performance of dialog system models. The proposed method is evaluated on a number of task-oriented dialog datasets, including MultiWOZ, CamRest676, and Multi-WOWZ. The results show that the proposed method outperforms state-of-the-art methods."
21015,SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,deep neural network USED-FOR classification. softmax values FEATURE-OF network. implied loss FEATURE-OF uncertainty measure. confidence measures USED-FOR Top k. networks USED-FOR method. binning values USED-FOR confidence measures. Generic is values. ,This paper considers the problem of learning confidence measures for deep neural networks. The authors propose to use binning values for confidence measures and show that binning can be used to reduce the uncertainty of confidence measures. They also provide a theoretical analysis of the binning value for the Top k confidence measure. The paper also shows that the confidence measures can be improved by using binning. ,This paper considers the problem of learning confidence measures for deep neural networks. The authors propose to use binning values for confidence measures and show that binning can be used to reduce the uncertainty of confidence measures. They also provide a theoretical analysis of the binning value for the Top k confidence measure. The paper also shows that the confidence measures can be improved by using binning. 
21024,SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION architecture. generalization FEATURE-OF neural networks. wide neural networks USED-FOR gradient descent. spectrum FEATURE-OF NNGP kernel. kernel USED-FOR gradient descent. kernel USED-FOR Gaussian Processes. Neural Network Gaussian Process ( NNGP ) kernel HYPONYM-OF kernel. Neural Tangent Kernel ( NTK ) HYPONYM-OF kernel. NTK COMPARE NNGP kernel. NNGP kernel COMPARE NTK. spectrum COMPARE NNGP kernel. NNGP kernel COMPARE spectrum. spectrum FEATURE-OF NTK. Fully Connected Networks ( FCNs ) CONJUNCTION Convolutional Neural Networks ( CNNs ). Convolutional Neural Networks ( CNNs ) CONJUNCTION Fully Connected Networks ( FCNs ). Convolutional Neural Networks ( CNNs ) HYPONYM-OF architectures. Fully Connected Networks ( FCNs ) HYPONYM-OF architectures. CNNs COMPARE FCNs. FCNs COMPARE CNNs. learning dynamics FEATURE-OF CNNs. average pooling FEATURE-OF CNNs. pooling FEATURE-OF CNNs. Task is deep learning. OtherScientificTerm are wide network limit, large depth limit, and hyperparameter space. Method are random networks, and NNGP. Metric are trainability, and training accuracy. Material is real datasets. ","This paper studies the generalization properties of neural networks. The authors propose a new kernel, the Neural Tangent Kernel (NTK), which is a generalization of the Neural Network Gaussian Process (NNGP). The authors show that the NTK kernel is more general than the NNGP kernel, and that the spectrum of NTK is larger than that of NNGPs. In addition, the authors provide a theoretical analysis of the learning dynamics of CNNs and FCNs. ","This paper studies the generalization properties of neural networks. The authors propose a new kernel, the Neural Tangent Kernel (NTK), which is a generalization of the Neural Network Gaussian Process (NNGP). The authors show that the NTK kernel is more general than the NNGP kernel, and that the spectrum of NTK is larger than that of NNGPs. In addition, the authors provide a theoretical analysis of the learning dynamics of CNNs and FCNs. "
21033,SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"computational methods USED-FOR protein folding. geometric invariance CONJUNCTION computational efficiency. computational efficiency CONJUNCTION geometric invariance. graph - based method USED-FOR protein models. GRAPHQA HYPONYM-OF graph - based method. GRAPHQA USED-FOR protein models. geometric invariance HYPONYM-OF favorable properties. representation learning HYPONYM-OF favorable properties. state - ofthe - art EVALUATE-FOR hand - engineered and representation - learning approaches. Material is Proteins. Task is biological processes. OtherScientificTerm are 3D structure, protein ’s structure, and sequential and 3D structure. Method is GRAPHQA components. ","This paper proposes a graph-based method for protein folding. The proposed method is based on Graph-based Graph-QA (GRAPHQA) and is able to learn the 3D structure of a protein in a sequential and 3D manner. The authors show that the proposed method can achieve state-of-the-art performance in terms of geometric invariance, computational efficiency, and representation learning. ","This paper proposes a graph-based method for protein folding. The proposed method is based on Graph-based Graph-QA (GRAPHQA) and is able to learn the 3D structure of a protein in a sequential and 3D manner. The authors show that the proposed method can achieve state-of-the-art performance in terms of geometric invariance, computational efficiency, and representation learning. "
21042,SP:5188280131b58a35d3deda126a0754aea8fa6e58,"loss function FEATURE-OF neural network. geometry of the functional space CONJUNCTION parameterization of this space. parameterization of this space CONJUNCTION geometry of the functional space. network ’s weights USED-FOR parameterization of this space. pure critical points COMPARE spurious critical points. spurious critical points COMPARE pure critical points. loss function FEATURE-OF linear neural networks. determinantal variety HYPONYM-OF functional space. linear maps HYPONYM-OF determinantal variety. bounded rank FEATURE-OF linear maps. functional space USED-FOR network. loss functions CONJUNCTION parameterizations. parameterizations CONJUNCTION loss functions. loss functions FEATURE-OF linear networks. architectures USED-FOR linear maps. loss landscape FEATURE-OF linear networks. determinantal variety FEATURE-OF functional space. Generic is space. OtherScientificTerm are parameterization, determinantal varieties, smooth convex losses, filling architectures, quadratic loss, non - filling architectures, and architecture. Task is landscape of linear networks. ","This paper studies the landscape of linear neural networks. The authors consider the determinantal variety of the loss function of a linear network, which is defined as a function of the weights of the network. They show that the loss landscape is bounded by the bounded rank of the linear maps of the functional space. They also provide a theoretical analysis of the properties of non-filling linear networks. ","This paper studies the landscape of linear neural networks. The authors consider the determinantal variety of the loss function of a linear network, which is defined as a function of the weights of the network. They show that the loss landscape is bounded by the bounded rank of the linear maps of the functional space. They also provide a theoretical analysis of the properties of non-filling linear networks. "
21051,SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"Inductive and unsupervised graph learning USED-FOR predictive or information retrieval tasks. graph similarity evaluation USED-FOR learning processes. reconstruction error based loss functions USED-FOR learning processes. embedding of the subgraph vector distribution USED-FOR output vector representation. output vector representation USED-FOR graph. SEED framework USED-FOR subgraphs. reconstruction errors FEATURE-OF subgraphs. SEED CONJUNCTION graph isomorphism. graph isomorphism CONJUNCTION SEED. SEED framework COMPARE competitive baseline methods. competitive baseline methods COMPARE SEED framework. public benchmark datasets EVALUATE-FOR SEED framework. OtherScientificTerm are label information, subgraph samples, subgraph vectors, and subgraph vector distribution. Method is graph learning. Material is graph structured objects. ",This paper proposes a method for unsupervised graph learning based on reconstruction error-based loss functions. The proposed method is based on the idea of embedding of the subgraph vector distribution into the output vector representation of the graph. The method is evaluated on a number of public benchmark datasets and shows that the proposed method outperforms several baselines.,This paper proposes a method for unsupervised graph learning based on reconstruction error-based loss functions. The proposed method is based on the idea of embedding of the subgraph vector distribution into the output vector representation of the graph. The method is evaluated on a number of public benchmark datasets and shows that the proposed method outperforms several baselines.
21060,SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,Counterfactual regret minimization ( CFR ) methods USED-FOR twoplayer zero - sum extensive games. imperfect information FEATURE-OF twoplayer zero - sum extensive games. vanilla CFR USED-FOR large - scale games. game tree USED-FOR vanilla CFR. Lazy - CFR HYPONYM-OF CFR algorithm. lazy update strategy USED-FOR CFR algorithm. Lazy - CFR COMPARE vanilla CFR. vanilla CFR COMPARE Lazy - CFR. regret EVALUATE-FOR Lazy - CFR. regret COMPARE regret. regret COMPARE regret. Lazy - CFR COMPARE regret. regret COMPARE Lazy - CFR. regret EVALUATE-FOR vanilla CFR. regret EVALUATE-FOR vanilla CFR. Lazy - CFR COMPARE CFR. CFR COMPARE Lazy - CFR. ,"This paper proposes a new counterfactual regret minimization (CFR) algorithm, Lazy-CFR, for zero-sum extensive games with imperfect information. The proposed algorithm is based on the idea of lazy update strategy. The authors show that the proposed algorithm outperforms the vanilla CFR algorithm in terms of regret. They also show that their algorithm can be applied to large-scale games.","This paper proposes a new counterfactual regret minimization (CFR) algorithm, Lazy-CFR, for zero-sum extensive games with imperfect information. The proposed algorithm is based on the idea of lazy update strategy. The authors show that the proposed algorithm outperforms the vanilla CFR algorithm in terms of regret. They also show that their algorithm can be applied to large-scale games."
21069,SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"Unsupervised Domain Adaptation ( UDA ) methods USED-FOR transferable features. explicit feature distribution modeling USED-FOR UDA. Distribution Matching Prototypical Network ( DMPN ) USED-FOR deep features. Gaussian mixture distributions USED-FOR Distribution Matching Prototypical Network ( DMPN ). Gaussian mixture distributions USED-FOR deep features. domain discrepancy losses PART-OF DMPN. probabilistic interpretations FEATURE-OF domain discrepancy losses. one USED-FOR pseudo negative log likelihood. classification loss CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION classification loss. labeled source data CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION labeled source data. classification loss USED-FOR DMPN. labeled source data USED-FOR classification loss. DMPN USED-FOR discriminative and domain invariant features. domain discrepancy losses USED-FOR DMPN. Digits Image transfer task EVALUATE-FOR state - of - the - art approaches. Digits Image transfer task EVALUATE-FOR approach. mean accuracy EVALUATE-FOR DMPN. VisDA 2017 dataset EVALUATE-FOR DMPN. hyper - parameter sensitivity analysis EVALUATE-FOR approach. hyper - parameter changes FEATURE-OF approach. OtherScientificTerm are feature distribution discrepancy, feature distributions, Gaussian component means, and source feature distribution. Generic is methods. Task is UDA tasks. ","This paper proposes a new method for unsupervised domain adaptation (UDA) based on the distribution matching network (DMPN). DMPN is an extension of the DDPN framework, where the source and target features are modeled as Gaussian mixture distributions, and the source features are learned from the target features. The proposed method is evaluated on the Digits Image Transfer task, and shows better performance than the state-of-the-art methods. ","This paper proposes a new method for unsupervised domain adaptation (UDA) based on the distribution matching network (DMPN). DMPN is an extension of the DDPN framework, where the source and target features are modeled as Gaussian mixture distributions, and the source features are learned from the target features. The proposed method is evaluated on the Digits Image Transfer task, and shows better performance than the state-of-the-art methods. "
21078,SP:40be996e8bb86e887077b762b87c7c34a786ac98,"deep generative models USED-FOR tasks. Continuous Normalizing Flows ( CNFs ) HYPONYM-OF deep generative models. conditional image generation CONJUNCTION downstream predictive tasks. downstream predictive tasks CONJUNCTION conditional image generation. CNFs USED-FOR conditional image generation. CNFs USED-FOR downstream predictive tasks. model USED-FOR highdimensional latent code. class - specific supervised code CONJUNCTION unsupervised code. unsupervised code CONJUNCTION class - specific supervised code. InfoCNF HYPONYM-OF conditional CNF. unsupervised code PART-OF conditional CNF. class - specific supervised code PART-OF conditional CNF. gating networks USED-FOR error tolerances. gating networks USED-FOR ordinary differential equation ( ODE ) solvers. partitioning strategy USED-FOR InfoCNF. error tolerances FEATURE-OF ordinary differential equation ( ODE ) solvers. InfoCNF USED-FOR error tolerances. gating networks USED-FOR InfoCNF. test accuracy EVALUATE-FOR baseline. InfoCNF COMPARE baseline. baseline COMPARE InfoCNF. NFEs FEATURE-OF CIFAR10. likelihood scores EVALUATE-FOR InfoCNF. CIFAR10 EVALUATE-FOR InfoCNF. test accuracy EVALUATE-FOR InfoCNF. partitioning strategy USED-FOR extrapolation. partitioning strategy USED-FOR InfoCNF. time - series data USED-FOR InfoCNF. time - series data EVALUATE-FOR partitioning strategy. Task is exact likelihood estimation. OtherScientificTerm are latent space, and labeled information. ","This paper proposes InfoCNF, a continuous normalizing flow (CNF) model for conditional conditional image generation. The proposed model is based on the idea of gating networks, which can be seen as an extension of the gating network used in ODE solvers. The authors propose a partitioning strategy to improve the error tolerances of conditional CNFs, and show that the proposed method outperforms the baseline on CIFAR-10. The method is evaluated on a number of downstream predictive tasks, and is shown to outperform the baseline.","This paper proposes InfoCNF, a continuous normalizing flow (CNF) model for conditional conditional image generation. The proposed model is based on the idea of gating networks, which can be seen as an extension of the gating network used in ODE solvers. The authors propose a partitioning strategy to improve the error tolerances of conditional CNFs, and show that the proposed method outperforms the baseline on CIFAR-10. The method is evaluated on a number of downstream predictive tasks, and is shown to outperform the baseline."
21087,SP:97764e3393216106ff2ac3f550845acf4636119f,"nonlinear functions USED-FOR approximation of the value function. Temporal - Difference ( TD ) learning algorithm USED-FOR nonlinear functions. lazy training regime FEATURE-OF algorithm. non - lazy TD learning USED-FOR models. Generic are problem, regime, model, and convergence. OtherScientificTerm are approximating function, learning process, scaling, and exponential convergence. Method are lazy training, neural networks, and underand over - parametrized frameworks. ","This paper studies the problem of temporal-difference (TD) learning for non-linear functions. The authors consider the case where the learning process is non-asymptotically linear and the model is under-parametrized. In this setting, the authors show that the convergence rate of TD learning is exponential in the lazy training regime. They also show that in the over-parameterized regime, the rate of convergence converges to zero. ","This paper studies the problem of temporal-difference (TD) learning for non-linear functions. The authors consider the case where the learning process is non-asymptotically linear and the model is under-parametrized. In this setting, the authors show that the convergence rate of TD learning is exponential in the lazy training regime. They also show that in the over-parameterized regime, the rate of convergence converges to zero. "
21096,SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"reinforcement learning problem USED-FOR hypothesis verification. agents USED-FOR problem. action sequence CONJUNCTION post - condition. post - condition CONJUNCTION action sequence. pre - condition CONJUNCTION action sequence. action sequence CONJUNCTION pre - condition. Generic are agent, and they. ","This paper studies the problem of hypothesis verification in reinforcement learning, where the goal is to verify whether a hypothesis is true or false. The authors propose a new problem called hypothesis verification, which is an extension of the hypothesis verification problem. The key idea is to learn a pre-condition and a post-condition for a given hypothesis, and then use these two conditions to train an agent to verify the hypothesis. The proposed method is evaluated on a variety of tasks, and the authors show that the proposed method outperforms the baselines.","This paper studies the problem of hypothesis verification in reinforcement learning, where the goal is to verify whether a hypothesis is true or false. The authors propose a new problem called hypothesis verification, which is an extension of the hypothesis verification problem. The key idea is to learn a pre-condition and a post-condition for a given hypothesis, and then use these two conditions to train an agent to verify the hypothesis. The proposed method is evaluated on a variety of tasks, and the authors show that the proposed method outperforms the baselines."
21105,SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"neural networks USED-FOR approximate reasoning. fixed dimensional latent space USED-FOR approximate reasoning. latent space FEATURE-OF approximate reasoning. formula space CONJUNCTION latent space. latent space CONJUNCTION formula space. embeddings COMPARE predicted embeddings. predicted embeddings COMPARE embeddings. formula space FEATURE-OF rewrite steps. latent space FEATURE-OF rewrite steps. graph neural networks USED-FOR rewrite - success of statements. mathematical disciplines PART-OF corpus of mathematical formulas. OtherScientificTerm are transformations, semantic features, vector space, rewrite rule, and predicted latent representations. Generic are reasoning, and they. ","This paper studies the problem of rewrite-success of statements in a fixed dimensional latent space. The authors propose to use graph neural networks (GNNs) to learn a rewrite rule for a set of statements. The proposed rewrite rule is based on the fact that the latent space of a statement can be represented as a vector space, which is then used to learn the rewrite rule.  The authors show that the proposed rewrite rules can be used to improve the performance of GNNs on a corpus of mathematical formulas. ","This paper studies the problem of rewrite-success of statements in a fixed dimensional latent space. The authors propose to use graph neural networks (GNNs) to learn a rewrite rule for a set of statements. The proposed rewrite rule is based on the fact that the latent space of a statement can be represented as a vector space, which is then used to learn the rewrite rule.  The authors show that the proposed rewrite rules can be used to improve the performance of GNNs on a corpus of mathematical formulas. "
21114,SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"multi - view geometry USED-FOR methods. approach USED-FOR depth. images CONJUNCTION sparse depth measurements. sparse depth measurements CONJUNCTION images. images USED-FOR approach. sparse depth measurements USED-FOR depth. images USED-FOR depth. global - local network architecture USED-FOR inductive bias. model USED-FOR monocular dense depth estimation. sparse ground truth USED-FOR model. sparse ground truth USED-FOR monocular dense depth estimation. global parameters USED-FOR metric agent motion. network USED-FOR global parameters. Method are Natural intelligent agents, artificial systems, and natural agents. OtherScientificTerm are equations of projective geometry, visual and haptic feedback, and sparse supervision. ",This paper proposes a method for monocular dense depth estimation based on a global-local network architecture. The proposed method is based on the idea that the global parameters of the global network can be used to estimate the depth of an agent in a multi-view geometry. The authors show that the proposed method can be applied to the problem of monocular depth estimation. The method is evaluated on a number of datasets and shows promising results. ,This paper proposes a method for monocular dense depth estimation based on a global-local network architecture. The proposed method is based on the idea that the global parameters of the global network can be used to estimate the depth of an agent in a multi-view geometry. The authors show that the proposed method can be applied to the problem of monocular depth estimation. The method is evaluated on a number of datasets and shows promising results. 
21123,SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,word pieces USED-FOR machine learning tasks. word pieces USED-FOR natural language models. natural language models USED-FOR machine learning tasks. machine learning tasks USED-FOR opaque ids. hash functions USED-FOR hash tokens. multi - layer Transformer USED-FOR Bloom filter digests. multi - layer Transformer USED-FOR models. accuracy EVALUATE-FOR models. They COMPARE models. models COMPARE They. computational budget FEATURE-OF sampled softmax. sampled softmax USED-FOR models. multi - layer Transformer USED-FOR Bloom filter digests. method USED-FOR problems. this USED-FOR problems. this USED-FOR method. large vocabulary size FEATURE-OF problems. Method is Bloom filter. OtherScientificTerm is hashing. ,"This paper proposes a novel Bloom filter digest method for word embeddings. The proposed method is based on the idea of multi-layer Transformer, which is a Transformer-based model that can be used to compute the Bloom filter. The authors show that the proposed method outperforms the baselines in terms of accuracy and computational budget. They also show that their method can be applied to large vocabulary size problems. ","This paper proposes a novel Bloom filter digest method for word embeddings. The proposed method is based on the idea of multi-layer Transformer, which is a Transformer-based model that can be used to compute the Bloom filter. The authors show that the proposed method outperforms the baselines in terms of accuracy and computational budget. They also show that their method can be applied to large vocabulary size problems. "
21132,SP:745dd86d7f7bba79a02d27922003b764b620f83e,"grouping policy USED-FOR small part proposals. grouping policy USED-FOR learningbased agglomerative clustering framework. local context USED-FOR part - level features. largescale fine - grained 3D part dataset EVALUATE-FOR method. method USED-FOR knowledge of parts. PartNet HYPONYM-OF largescale fine - grained 3D part dataset. shape segmentation baselines COMPARE approach. approach COMPARE shape segmentation baselines. Task are discovering 3D parts, and contextual bandit problem. Generic is prior. Method is data - driven shape segmentation approaches. ",This paper proposes a learning-based agglomerative clustering framework for learning 3D part proposals. The proposed method is based on the contextual bandit problem. The method is evaluated on a large-scale fine-grained part dataset.,This paper proposes a learning-based agglomerative clustering framework for learning 3D part proposals. The proposed method is based on the contextual bandit problem. The method is evaluated on a large-scale fine-grained part dataset.
21141,SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"input / output datasets USED-FOR they. gender - related characteristics CONJUNCTION hair color. hair color CONJUNCTION gender - related characteristics. generative adversarial network ( GAN ) USED-FOR images of black - haired men. edit USED-FOR transformation. latent space FEATURE-OF transformation. autoencoder USED-FOR transformed data. editing transformation USED-FOR transformed data. transformation USED-FOR complex and non - linear transformations. latent trained space USED-FOR transformation. data domains CONJUNCTION modalities. modalities CONJUNCTION data domains. modalities CONJUNCTION applications. applications CONJUNCTION modalities. technique USED-FOR data domains. applications PART-OF biology. image transformations USED-FOR it. removal of batch artifacts HYPONYM-OF biology. removal of batch artifacts HYPONYM-OF applications. Method are generative neural networks, discriminator, generative models, and neuron editing. OtherScientificTerm are blond - haired men, source distribution, target distribution, manifold, distribution shifts, neuron ’s activations, unwanted noise, and drug treatments. Material is images of black - haired women. ","This paper proposes a method for generating images of black-haired women using a generative adversarial network (GAN). The method is based on an autoencoder, which is trained on the latent space of a GAN. The authors show that the proposed method is able to generate images of blond-haired women that are more similar to the target distribution than the original image. They also show that their method can be applied to image transformations. ","This paper proposes a method for generating images of black-haired women using a generative adversarial network (GAN). The method is based on an autoencoder, which is trained on the latent space of a GAN. The authors show that the proposed method is able to generate images of blond-haired women that are more similar to the target distribution than the original image. They also show that their method can be applied to image transformations. "
21150,SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"training algorithms CONJUNCTION model architectures. model architectures CONJUNCTION training algorithms. reinforcement learning CONJUNCTION image semantic segmentation. image semantic segmentation CONJUNCTION reinforcement learning. few - shot image classification CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION few - shot image classification. model architectures USED-FOR few - shot domain. meta - learning approaches USED-FOR few - shot image classification. meta - learning approaches USED-FOR reinforcement learning. training algorithms USED-FOR few - shot domain. neural network representations USED-FOR meta - learning approaches. learning systems USED-FOR few - shot to many - shot settings. first - order meta - learning of initializations USED-FOR deep neural networks. first - order meta - learning of initializations USED-FOR dense, structured predictions. FOMAML CONJUNCTION Reptile. Reptile CONJUNCTION FOMAML. neural network architecture USED-FOR fast learning. generalization error EVALUATE-FOR meta - learning algorithms. small benchmark dataset EVALUATE-FOR meta - learning systems. meta - learning systems USED-FOR fewand many - shot settings. EfficientLab HYPONYM-OF neural network architecture. FP - k HYPONYM-OF small benchmark dataset. meta - learned initializations USED-FOR image segmentation. meta - learned initializations USED-FOR canonical few - shot learning problems. meta - learned initializations COMPARE random and ImageNet - trained initializations. random and ImageNet - trained initializations COMPARE meta - learned initializations. update routine USED-FOR tasks. FSS-1000 dataset EVALUATE-FOR network. Method are ensembling many models, relation networks, and MAML - type algorithms. OtherScientificTerm is initialization. Generic are task, and model. ","This paper presents a meta-learning approach for few-shot learning. The authors propose a method to learn a set of initializations for each task, which can be used to improve the generalization performance of the model. The proposed method is based on the idea of first-order meta learning, which is an extension of the meta learning of neural networks.  The authors show that the proposed method outperforms the baselines on the FSS-1000 and Reptile datasets. ","This paper presents a meta-learning approach for few-shot learning. The authors propose a method to learn a set of initializations for each task, which can be used to improve the generalization performance of the model. The proposed method is based on the idea of first-order meta learning, which is an extension of the meta learning of neural networks.  The authors show that the proposed method outperforms the baselines on the FSS-1000 and Reptile datasets. "
21159,SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"unlabelled data USED-FOR few - shot learning. Prototypical Random Walk Networks(PRWN ) HYPONYM-OF SS - FSL approach. Prototypical Networks ( PN ) USED-FOR SS - FSL approach. Prototypical Networks ( PN ) USED-FOR Prototypical Random Walk Networks(PRWN ). random walk semi - supervised loss USED-FOR representations. random walk semi - supervised loss USED-FOR network. network USED-FOR representations. graph - based approaches USED-FOR few - shot learning. prototypical random walk notion USED-FOR compact and well - separated class representations. model COMPARE art. art COMPARE model. model COMPARE fully supervised prototypical networks. fully supervised prototypical networks COMPARE model. 1 - shot mini - Imagenet case EVALUATE-FOR it. accuracy EVALUATE-FOR it. robustness FEATURE-OF labelled / unlabelled class distribution mismatch. discriminative power test EVALUATE-FOR baseline. Task are human intelligence, transductive setting, and mini - Imagenet 5 - shot classification task. Method is AI models. OtherScientificTerm are graph - NN parameters, distractors, and unlabeled data. Material are collective test set, mini - Imagenet, and Omniglot. ","This paper proposes a novel method for few-shot learning based on Prototypical Random Walk Networks (PRWN). The proposed method is based on the prototypical random walk (PN) framework, which is used to learn representations of unlabeled data. The authors show that the proposed method outperforms the state-of-the-art in the 1-shot mini-imagenet task and outperforms a baseline method in the 5-shot classification task. ","This paper proposes a novel method for few-shot learning based on Prototypical Random Walk Networks (PRWN). The proposed method is based on the prototypical random walk (PN) framework, which is used to learn representations of unlabeled data. The authors show that the proposed method outperforms the state-of-the-art in the 1-shot mini-imagenet task and outperforms a baseline method in the 5-shot classification task. "
21168,SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"machine learning USED-FOR remote sensing. labeled data USED-FOR machine learning. deep convolutional neural networks HYPONYM-OF models. selfsupervised learning approaches USED-FOR remote sensing domain. multi - sensor, multi - channel information USED-FOR remote sensing applications. Contrastive Sensor Fusion USED-FOR representations. Contrastive Sensor Fusion HYPONYM-OF self - supervised training objective. model USED-FOR representation. encoder USED-FOR representations. dataset USED-FOR encoder. unlabeled coterminous image triplets PART-OF dataset. remote sensing classification task EVALUATE-FOR representations. Material are unlabeled data, and coterminous data. Method is fused multi - sensor representations. Generic is method. ","This paper proposes a self-supervised learning approach for multi-channel multi-sensor representation learning. The proposed method is based on contrastive sensor fusion, which is an existing method for learning multi-spectral representations from unlabeled image triplets. The key idea is to use contrastive fusion to learn a representation for each sensor. The method is evaluated on the remote sensing classification task and shows promising results. ","This paper proposes a self-supervised learning approach for multi-channel multi-sensor representation learning. The proposed method is based on contrastive sensor fusion, which is an existing method for learning multi-spectral representations from unlabeled image triplets. The key idea is to use contrastive fusion to learn a representation for each sensor. The method is evaluated on the remote sensing classification task and shows promising results. "
21177,SP:4d8e054f07006b4f896721b5c24da805727d2c22,"fine - tuning HYPONYM-OF retraining technique. fine - tuning COMPARE retraining techniques. retraining techniques COMPARE fine - tuning. Learning rate rewinding USED-FOR unpruned weights. learning rate schedule USED-FOR weight rewinding. Learning rate rewinding COMPARE weight rewinding. weight rewinding COMPARE Learning rate rewinding. learning rate schedule USED-FOR Learning rate rewinding. rewinding techniques COMPARE fine - tuning. fine - tuning COMPARE rewinding techniques. accuracy CONJUNCTION compression ratios. compression ratios CONJUNCTION accuracy. rewinding techniques USED-FOR network - agnostic pruning algorithm. compression ratios EVALUATE-FOR network - agnostic pruning algorithm. accuracy EVALUATE-FOR network - agnostic pruning algorithm. Method are neural network pruning algorithms, and Weight rewinding. Generic is network. OtherScientificTerm are learning rate, and training schedule. ","This paper studies the problem of weight rewinding in the context of fine-tuning and pruning. The authors propose a new method for weight re-winding based on the learning rate schedule. The main idea is to learn a learning rate for each layer of the network, which is then used to re-weight the weights of the unpruned layer. The method is evaluated on a number of benchmark datasets and shows that the proposed method is able to achieve better accuracy and compression ratios compared to other re-weighing methods.","This paper studies the problem of weight rewinding in the context of fine-tuning and pruning. The authors propose a new method for weight re-winding based on the learning rate schedule. The main idea is to learn a learning rate for each layer of the network, which is then used to re-weight the weights of the unpruned layer. The method is evaluated on a number of benchmark datasets and shows that the proposed method is able to achieve better accuracy and compression ratios compared to other re-weighing methods."
21186,SP:3bb1c79f9482e09828eda45fbb2e654f37219365,normalized ) output margin CONJUNCTION generalization. generalization CONJUNCTION normalized ) output margin. output margin FEATURE-OF generalization. all - layer margin HYPONYM-OF margin. generalization EVALUATE-FOR deep models. theoretically inspired training algorithm USED-FOR all - layer margin. neural net USED-FOR adversarially robust setting. robust test error FEATURE-OF deep networks. Jacobian and hidden layer norms USED-FOR neural nets. Method is linear classifiers. Generic is algorithm. ,"This paper studies the all-layer margin of deep neural networks for adversarially robust training. The authors show that the output margin of a neural network is a function of the Jacobian and the hidden layer norms of the network. They also show that under certain assumptions, the robustness of the neural network can be reduced to a lower bound on the all layer margin. They show that this lower bound can be obtained by training the network on a set of adversarial examples, and show that it can be used to reduce the robust test error of a deep neural network. ","This paper studies the all-layer margin of deep neural networks for adversarially robust training. The authors show that the output margin of a neural network is a function of the Jacobian and the hidden layer norms of the network. They also show that under certain assumptions, the robustness of the neural network can be reduced to a lower bound on the all layer margin. They show that this lower bound can be obtained by training the network on a set of adversarial examples, and show that it can be used to reduce the robust test error of a deep neural network. "
21195,SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"ungrounded dialogues CONJUNCTION unstructured documents. unstructured documents CONJUNCTION ungrounded dialogues. unstructured documents USED-FOR model. ungrounded dialogues USED-FOR model. limited training examples USED-FOR small parameters. benchmarks EVALUATE-FOR model. out - of - domain knowledge EVALUATE-FOR model. Task are intelligent conversational agent, and knowledge - grounded dialogue generation. Material is knowledge - grounded dialogues. Method are response generation model, disentangled response decoder, and generation model. ","This paper proposes a framework for knowledge-grounded dialogue generation. The framework is based on a disentangled response decoder, which is used to generate dialogues from unstructured documents and ungrounded dialogues. The proposed framework is evaluated on a number of benchmark datasets. The model is shown to outperform the baselines on a variety of tasks. ","This paper proposes a framework for knowledge-grounded dialogue generation. The framework is based on a disentangled response decoder, which is used to generate dialogues from unstructured documents and ungrounded dialogues. The proposed framework is evaluated on a number of benchmark datasets. The model is shown to outperform the baselines on a variety of tasks. "
21204,SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"parallel corpus USED-FOR neural machine translation models ( NMT ). non - parallel bilingual data USED-FOR decoding. non - parallel bilingual data USED-FOR training. training CONJUNCTION decoding. decoding CONJUNCTION training. non - parallel bilingual data USED-FOR Existing approaches. source to target translation model CONJUNCTION target to source translation model. target to source translation model CONJUNCTION source to target translation model. target to source translation model CONJUNCTION language models. language models CONJUNCTION target to source translation model. mirror - generative NMT ( MGNMT ) HYPONYM-OF single unified architecture. source to target translation model PART-OF single unified architecture. target to source translation model PART-OF single unified architecture. language models PART-OF single unified architecture. translation models CONJUNCTION language models. language models CONJUNCTION translation models. latent semantic space FEATURE-OF language models. non - parallel data USED-FOR translation directions. translation models CONJUNCTION language models. language models CONJUNCTION translation models. language models USED-FOR decoding. translation models USED-FOR decoding. MGNMT COMPARE approaches. approaches COMPARE MGNMT. Material are non - parallel corpora, and resource - rich and low - resource situations. ","This paper proposes a mirror-generative NMT (MGNMT) model for non-parallel bilingual data. The main idea is to combine the source-to-target translation model and the target to target translation model in a single unified architecture. The authors show that the proposed model is able to achieve state-of-the-art performance on a variety of tasks, including translation, decoding, and training. ","This paper proposes a mirror-generative NMT (MGNMT) model for non-parallel bilingual data. The main idea is to combine the source-to-target translation model and the target to target translation model in a single unified architecture. The authors show that the proposed model is able to achieve state-of-the-art performance on a variety of tasks, including translation, decoding, and training. "
21213,SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,maximum entropy reinforcement learning algorithms USED-FOR Deep Reinforcement Learning ( DRL ). benchmarks EVALUATE-FOR sample efficiency. entropy term USED-FOR maximum entropy algorithms. entropy term USED-FOR bounded nature of the action spaces. entropy term PART-OF Soft Actor Critic ( SAC ). entropy term USED-FOR Mujoco benchmark. streamlined algorithms USED-FOR SAC. entropy maximization USED-FOR streamlined algorithms. non - uniform sampling method USED-FOR transitions. transitions PART-OF replay buffer. streamlined algorithm COMPARE SAC. SAC COMPARE streamlined algorithm. continuous control tasks EVALUATE-FOR streamlined algorithm. non - uniform sampling scheme USED-FOR streamlined algorithm. OtherScientificTerm is maximum entropy objective. Task is training. ,This paper proposes a new algorithm for maximum entropy reinforcement learning (Mujoco) based on the Soft Actor Critic (SAC) algorithm. The main idea is to use a non-uniform sampling scheme for the transitions in the replay buffer to improve sample efficiency. The proposed algorithm is evaluated on the Mujoco benchmark and shows that it outperforms the state-of-the-art SAC algorithm.,This paper proposes a new algorithm for maximum entropy reinforcement learning (Mujoco) based on the Soft Actor Critic (SAC) algorithm. The main idea is to use a non-uniform sampling scheme for the transitions in the replay buffer to improve sample efficiency. The proposed algorithm is evaluated on the Mujoco benchmark and shows that it outperforms the state-of-the-art SAC algorithm.
21222,SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"machine learning models USED-FOR adversarial attacks. industrial copyright detection tools USED-FOR web. industrial copyright detection tools USED-FOR adversarial attacks. neural net USED-FOR system. gradient methods USED-FOR system. AudioTag copyright detector CONJUNCTION YouTube ’s Content ID system. YouTube ’s Content ID system CONJUNCTION AudioTag copyright detector. Adversarial music USED-FOR industrial systems. YouTube ’s Content ID system HYPONYM-OF industrial systems. AudioTag copyright detector HYPONYM-OF industrial systems. Method are classifier, copyright detection systems, neural network based systems, and music identification method. Generic is they. OtherScientificTerm is attacks. Material is adversarial examples. ","This paper studies the problem of adversarial attacks against industrial copyright detection systems. The authors propose a new method for detecting adversarial examples in a neural network-based system. The method is based on the idea that the classifier should be able to distinguish between the source and target samples, and the target samples should be different from the source samples. The proposed method is evaluated on a number of industrial systems, including YouTube and AudioTag. ","This paper studies the problem of adversarial attacks against industrial copyright detection systems. The authors propose a new method for detecting adversarial examples in a neural network-based system. The method is based on the idea that the classifier should be able to distinguish between the source and target samples, and the target samples should be different from the source samples. The proposed method is evaluated on a number of industrial systems, including YouTube and AudioTag. "
21231,SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"visual explanation USED-FOR deep metric learning. model COMPARE classification. classification COMPARE model. framework USED-FOR metric learning applications. framework USED-FOR model. cross - view pattern discovery CONJUNCTION interactive retrieval. interactive retrieval CONJUNCTION cross - view pattern discovery. interactive retrieval HYPONYM-OF applications. cross - view pattern discovery HYPONYM-OF applications. Method are learning representation, and metric learning. OtherScientificTerm are overall activation map, and point - to - point activation intensity. ","This paper proposes a new framework for deep metric learning based on visual explanation. The framework is based on the idea that the overall activation map of a point-to-point neural network can be viewed as a visual explanation of the activation intensity of a given point. The authors show that the proposed framework can be applied to a variety of metric learning applications such as cross-view pattern discovery, interactive retrieval, and interactive retrieval. ","This paper proposes a new framework for deep metric learning based on visual explanation. The framework is based on the idea that the overall activation map of a point-to-point neural network can be viewed as a visual explanation of the activation intensity of a given point. The authors show that the proposed framework can be applied to a variety of metric learning applications such as cross-view pattern discovery, interactive retrieval, and interactive retrieval. "
21240,SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"learning control USED-FOR online lifelong learning scenario. they USED-FOR failure modes. computational resources USED-FOR model - based planning methods. model - based planning CONJUNCTION model - free learning. model - free learning CONJUNCTION model - based planning. planner CONJUNCTION model - free components. model - free components CONJUNCTION planner. Method are model - free policy learning methods, Adaptive Online Planning ( AOP ), AOP, continual learning agent, and reinforcement learning methods. OtherScientificTerm are compact networks, performance degradation, dynamics, constrained computation limits, and unpredictable changes in the world. Generic are setting, and algorithm. Task is planning. ","This paper studies the problem of adaptive online planning (AOP) in the lifelong learning setting, where the goal is to learn a continual learning agent that is able to adapt to changes in the environment. The authors propose a new algorithm for this setting, called Adaptive Online Planning (ATP), which combines model-based planning and model-free learning. The main contribution of the paper is to show that AOP can be applied to the case where the learning agent is constrained to learn in a finite number of time steps. The paper also shows that ATP can be used as a learning agent for continual learning. ","This paper studies the problem of adaptive online planning (AOP) in the lifelong learning setting, where the goal is to learn a continual learning agent that is able to adapt to changes in the environment. The authors propose a new algorithm for this setting, called Adaptive Online Planning (ATP), which combines model-based planning and model-free learning. The main contribution of the paper is to show that AOP can be applied to the case where the learning agent is constrained to learn in a finite number of time steps. The paper also shows that ATP can be used as a learning agent for continual learning. "
21249,SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"Visual attention mechanisms USED-FOR image captioning models. sparsemax CONJUNCTION Total - Variation Sparse Attention ( TVMAX ). Total - Variation Sparse Attention ( TVMAX ) CONJUNCTION sparsemax. sparsity - promoting transformations USED-FOR softmax attention mechanism. Total - Variation Sparse Attention ( TVMAX ) HYPONYM-OF sparsity - promoting transformations. sparsemax HYPONYM-OF sparsity - promoting transformations. sparsemax USED-FOR sparse attention weights. interpretability EVALUATE-FOR TVMAX transformation. humanrated caption quality CONJUNCTION attention relevance. attention relevance CONJUNCTION humanrated caption quality. TVMAX COMPARE attention mechanisms. attention mechanisms COMPARE TVMAX. attention relevance EVALUATE-FOR attention mechanisms. attention relevance EVALUATE-FOR TVMAX. humanrated caption quality EVALUATE-FOR attention mechanisms. humanrated caption quality EVALUATE-FOR TVMAX. OtherScientificTerm are image structure, relevant features, and sparsity. Material is Microsoft COCO and Flickr30k datasets. Method is softmax. ","This paper proposes a new attention mechanism for image captioning, called TVMAX. TVMAX is based on the idea of sparsity-promoting transformations. The authors show that TVMAX outperforms softmax and sparsemax in terms of human-rated caption quality and attention relevance. They also show that the proposed method is able to improve the interpretability of TVMAX compared to softmax.","This paper proposes a new attention mechanism for image captioning, called TVMAX. TVMAX is based on the idea of sparsity-promoting transformations. The authors show that TVMAX outperforms softmax and sparsemax in terms of human-rated caption quality and attention relevance. They also show that the proposed method is able to improve the interpretability of TVMAX compared to softmax."
21258,SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,Neural networks USED-FOR structured data. Neural networks USED-FOR graphs. graphs HYPONYM-OF structured data. Predicting the evolution of dynamic graphs PART-OF graph mining. model USED-FOR evolution of dynamic graphs. graph neural network USED-FOR temporal evolution patterns of dynamic graphs. graph neural network CONJUNCTION recurrent architecture. recurrent architecture CONJUNCTION graph neural network. recurrent architecture USED-FOR temporal evolution patterns of dynamic graphs. generative model USED-FOR graph instance. graph instance USED-FOR topology. common network evolving dynamics FEATURE-OF artificial datasets. real - world datasets EVALUATE-FOR model. artificial datasets EVALUATE-FOR model. OtherScientificTerm is static graphs. Material is real - world networks. Generic is task. ,This paper proposes a generative model for predicting the evolution of dynamic graphs. The model is based on a graph neural network and a recurrent architecture. The authors show that the model is able to predict the evolution patterns of the dynamic graphs in the context of graph mining. The proposed model is evaluated on a variety of synthetic and real-world datasets. ,This paper proposes a generative model for predicting the evolution of dynamic graphs. The model is based on a graph neural network and a recurrent architecture. The authors show that the model is able to predict the evolution patterns of the dynamic graphs in the context of graph mining. The proposed model is evaluated on a variety of synthetic and real-world datasets. 
21267,SP:ff722957a1765c0568426ed88dd910a6b74054ef,"incomplete datasets USED-FOR machine learning applications. missing data imputation techniques USED-FOR filling missing values. method USED-FOR imputing missing features. method USED-FOR distribution of target assignments. incomplete data USED-FOR distribution of target assignments. generator network USED-FOR imputations. generator network USED-FOR imputations. predictor network USED-FOR classification uncertainties. generator network USED-FOR predictor network. imputed samples USED-FOR predictor network. CIFAR-10 image dataset CONJUNCTION real - world tabular classification datasets. real - world tabular classification datasets CONJUNCTION CIFAR-10 image dataset. real - world tabular classification datasets EVALUATE-FOR method. CIFAR-10 image dataset EVALUATE-FOR method. method USED-FOR generating imputations. class uncertainties FEATURE-OF classification task. OtherScientificTerm are missing values, distribution of missing values, missing features, and missingness rates. Method is discriminator network. ",This paper proposes a method for imputing missing features from incomplete data. The method is based on the idea that missing data imputation can be used to generate imputations of missing features. The imputations are generated by a generator network and a predictor network. The generator network is trained to predict the missingness rates of the imputed samples and the predictor network is used to estimate the uncertainty of the classifier. The proposed method is evaluated on CIFAR-10 and real-world tabular classification datasets.,This paper proposes a method for imputing missing features from incomplete data. The method is based on the idea that missing data imputation can be used to generate imputations of missing features. The imputations are generated by a generator network and a predictor network. The generator network is trained to predict the missingness rates of the imputed samples and the predictor network is used to estimate the uncertainty of the classifier. The proposed method is evaluated on CIFAR-10 and real-world tabular classification datasets.
21276,SP:c051b0fe779d9e4131016970b7ba469b596f3009,"Off - policy estimation USED-FOR long - horizon problems. healthcare CONJUNCTION robotics. robotics CONJUNCTION healthcare. Off - policy estimation USED-FOR real - life applications. robotics HYPONYM-OF real - life applications. healthcare HYPONYM-OF real - life applications. curse of horizon FEATURE-OF importance - sampling - based methods. stationary distribution FEATURE-OF known behavior policy. estimator USED-FOR importance ratios of stationary distributions. Reproducing Kernel Hilbert Spaces ( RKHSs ) USED-FOR estimator. asymptotic consistency CONJUNCTION finite - sample generalization. finite - sample generalization CONJUNCTION asymptotic consistency. Method is high - fidelity simulators. Task is on - policy evaluation. Generic are approach, it, problem, and operator. Material is off - policy data. ",This paper proposes a new method for off-policy estimation based on Reproducing Kernel Hilbert Spaces (RKHSs). The main idea is to estimate the importance ratios of stationary distributions of the known behavior policy. The authors show that the proposed method is able to achieve asymptotic consistency and finite-sample generalization. ,This paper proposes a new method for off-policy estimation based on Reproducing Kernel Hilbert Spaces (RKHSs). The main idea is to estimate the importance ratios of stationary distributions of the known behavior policy. The authors show that the proposed method is able to achieve asymptotic consistency and finite-sample generalization. 
21285,SP:065c900843011a71b70ed35357a2f71fe83872a7,"probabilistic framework USED-FOR dataset. Mixture Model ( MM ) HYPONYM-OF probabilistic framework. modes PART-OF dataset. Gaussian distribution FEATURE-OF modes. paintings dataset CONJUNCTION fashion images. fashion images CONJUNCTION paintings dataset. unlabelled modes PART-OF large datasets. fashion images HYPONYM-OF large datasets. paintings dataset HYPONYM-OF large datasets. plausible method USED-FOR probabilities. Generative Adversarial Network ( GAN ) framework USED-FOR plausible method. GAN USED-FOR distribution. GAN USED-FOR classification network. techniques USED-FOR unsupervised dataset. smooth linear interpolation USED-FOR outdistribution ” data. Method are Gaussian MM, GMM, and GMM paradigm. OtherScientificTerm are conditional likelihood, distribution index, Euclidean distances, responsibility distribution, latent representation of x, and dataset segments. Generic is responsibility. ",This paper proposes a new probabilistic framework for unsupervised data classification based on the Gaussian Mixture Model (MM) framework. The proposed framework is based on a GAN-based approach to learn the distribution of the latent representation of the data. The authors show that the proposed framework can be applied to the problem of out-of-distribution (OOD) data. They also show that it can be used to improve the performance of the model. ,This paper proposes a new probabilistic framework for unsupervised data classification based on the Gaussian Mixture Model (MM) framework. The proposed framework is based on a GAN-based approach to learn the distribution of the latent representation of the data. The authors show that the proposed framework can be applied to the problem of out-of-distribution (OOD) data. They also show that it can be used to improve the performance of the model. 
21294,SP:2da1608209058d214f8671062cc9eb0833ba4831,method USED-FOR large capacity neural networks. accuracy CONJUNCTION dynamic computational cost. dynamic computational cost CONJUNCTION accuracy. accuracy EVALUATE-FOR method. fine - grained - level FEATURE-OF deep - learning architecture. residual block architecture USED-FOR convolutional channels. fine - grained manner FEATURE-OF residual block architecture. fine - grained manner FEATURE-OF convolutional channels. marginal aggregate posteriors of features PART-OF neural network. pre - specified prior distribution FEATURE-OF marginal aggregate posteriors of features. technique USED-FOR gates. CIFAR-10 and ImageNet datasets USED-FOR image classification. Cityscapes USED-FOR semantic segmentation. image classification CONJUNCTION Cityscapes. Cityscapes CONJUNCTION image classification. average computational cost COMPARE architecture. architecture COMPARE average computational cost. method USED-FOR architectures. method COMPARE architecture. architecture COMPARE method. ImageNet EVALUATE-FOR ResNet34 gated networks. accuracy EVALUATE-FOR ResNet18 model. top-1 accuracy EVALUATE-FOR ResNet34 gated networks. complexity EVALUATE-FOR ResNet18 model. features CONJUNCTION features. features CONJUNCTION features. features USED-FOR networks. OtherScientificTerm is convolutional maps. Generic is network. Method is batch - shaping. ,"This paper proposes a method for batch-shaping convolutional neural networks. The proposed method is based on the residual block architecture, which is an extension of residual block (RBC) to the convolution layer. The authors show that the proposed method can be used to improve the performance of existing deep learning models on CIFAR-10, ImageNet, and Cityscapes datasets. ","This paper proposes a method for batch-shaping convolutional neural networks. The proposed method is based on the residual block architecture, which is an extension of residual block (RBC) to the convolution layer. The authors show that the proposed method can be used to improve the performance of existing deep learning models on CIFAR-10, ImageNet, and Cityscapes datasets. "
21303,SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"probabilistic importance inference approach USED-FOR pruning DNNs. approach COMPARE techniques. techniques COMPARE approach. lossless compression rates EVALUATE-FOR techniques. lossless compression rates EVALUATE-FOR approach. Method are Deep neural networks ( DNNs ), DNNs, DNN, and nonparemetric scoring test. OtherScientificTerm are energy and computational resources, and DNN ’s outputs. ",This paper proposes a probabilistic importance inference approach for pruning DNNs. The main idea is to use a non-paremetric scoring test to measure the importance of the outputs of a given DNN. The authors show that the proposed method is able to find the optimal pruning rate for a given pruning task. They also show that their method can be applied to a variety of pruning tasks.,This paper proposes a probabilistic importance inference approach for pruning DNNs. The main idea is to use a non-paremetric scoring test to measure the importance of the outputs of a given DNN. The authors show that the proposed method is able to find the optimal pruning rate for a given pruning task. They also show that their method can be applied to a variety of pruning tasks.
21312,SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"approaches USED-FOR hierarchical reinforcement learning. approaches USED-FOR sub - goal structure. method USED-FOR iteratively compressing action trajectories. iteratively compressing action trajectories USED-FOR nested behavioral hierarchies. method USED-FOR nested behavioral hierarchies. action primitives USED-FOR deeper hierarchies. approach USED-FOR learning. transfer USED-FOR approach. Generic is perspective. OtherScientificTerm are compact code of action trajectories, and non - trivial hierarchical structure. ",This paper proposes a method for hierarchical reinforcement learning that compresses action trajectories into a compact code of sub-goal trajectories that can be used to learn a hierarchical structure. The method is based on iteratively compressing the action primitives of the subgoal space and then applying a transfer learning algorithm to learn the sub-goals. The proposed method is evaluated on a number of tasks and shows promising results. ,This paper proposes a method for hierarchical reinforcement learning that compresses action trajectories into a compact code of sub-goal trajectories that can be used to learn a hierarchical structure. The method is based on iteratively compressing the action primitives of the subgoal space and then applying a transfer learning algorithm to learn the sub-goals. The proposed method is evaluated on a number of tasks and shows promising results. 
21321,SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"generative models USED-FOR complex data. Autoencoders HYPONYM-OF generative models. images HYPONYM-OF complex data. variational autoencoder ( VAE ) HYPONYM-OF models. unimodal Gaussian decoders USED-FOR models. Hierarchical Bayes Autoencoder ( HBAE ) HYPONYM-OF probabilistic generative model. energybased model ( EBM ) USED-FOR multimodal decoder. multimodal decoder PART-OF HBAE. variational inference USED-FOR VAE. variational inference USED-FOR HBAE. conditional generator USED-FOR EBM distribution. adversarial approximation USED-FOR decoder. conditional generator USED-FOR stochastic reconstruction. code USED-FOR stochastic reconstruction. sampling steps PART-OF HBAE. HBAE USED-FOR sets. latent code USED-FOR HBAE. decoder USED-FOR realistic unconditional samples. single image and set cases EVALUATE-FOR decoder. model USED-FOR complex image sets. Set - HBAE USED-FOR complex image sets. Set - HBAE HYPONYM-OF model. OtherScientificTerm are semantic variations, and latent codes. Method is unimodal Gaussian distribution. ","This paper proposes Hierarchical Bayes Autoencoder (HBAE), a probabilistic generative model based on the energy-based model (EBM) for multi-modal decoders. HBAE is an extension of the VAE family of generative models, where the decoder is based on an energy based model (EBM) and the generator is a conditional generator. The authors show that the proposed model is able to achieve state-of-the-art performance on both single image and set cases. ","This paper proposes Hierarchical Bayes Autoencoder (HBAE), a probabilistic generative model based on the energy-based model (EBM) for multi-modal decoders. HBAE is an extension of the VAE family of generative models, where the decoder is based on an energy based model (EBM) and the generator is a conditional generator. The authors show that the proposed model is able to achieve state-of-the-art performance on both single image and set cases. "
21330,SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"deep off - policy TD algorithms CONJUNCTION feature normalization techniques. feature normalization techniques CONJUNCTION deep off - policy TD algorithms. normalization USED-FOR target networks. normalization USED-FOR optimization stability. mixture of onand off - policy transitions USED-FOR normalization. batch normalization USED-FOR It. DDPG CONJUNCTION TD3. TD3 CONJUNCTION DDPG. cross - normalization USED-FOR TD3. cross - normalization USED-FOR DDPG. MuJoCo benchmark tasks EVALUATE-FOR cross - normalization. Method are reinforcement learning ( RL ) algorithms, normalization techniques, and off - policy learning. ",This paper proposes a new normalization technique for off-policy reinforcement learning (off-policy learning). The idea is to use a mixture of on-policy transitions to improve the optimization stability of the target network. The authors show that the proposed normalization can be applied to both on-and-off policy learning. They also show that cross-normalization can improve the performance of TD3 and DDPG.,This paper proposes a new normalization technique for off-policy reinforcement learning (off-policy learning). The idea is to use a mixture of on-policy transitions to improve the optimization stability of the target network. The authors show that the proposed normalization can be applied to both on-and-off policy learning. They also show that cross-normalization can improve the performance of TD3 and DDPG.
21339,SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"bias CONJUNCTION confounding effects. confounding effects CONJUNCTION bias. spurious associations of confounding variables HYPONYM-OF challenges. residualization CONJUNCTION stratification. stratification CONJUNCTION residualization. precomputed features USED-FOR confounding variables. techniques USED-FOR precomputed features. stratification HYPONYM-OF techniques. residualization HYPONYM-OF techniques. techniques USED-FOR statistical methods. techniques USED-FOR end - to - end deep learning methods. method USED-FOR discriminative features. adversarial training strategy USED-FOR discriminative features. adversarial training strategy USED-FOR method. synthetic data CONJUNCTION medical images. medical images CONJUNCTION synthetic data. method USED-FOR synthetic data. medical images EVALUATE-FOR method. Task are machine learning applications, and face recognition systems. Material is medical studies. Generic are datasets, and models. OtherScientificTerm are biases, confounder(s ), and bias or confounder variables. Method is adversarial loss function. ",This paper proposes a novel adversarial training method to improve the performance of deep learning models in the presence of confounder variables. The authors propose a new adversarial loss function for the discriminative features. The proposed method is evaluated on synthetic and real-world datasets and is shown to outperform the baselines. ,This paper proposes a novel adversarial training method to improve the performance of deep learning models in the presence of confounder variables. The authors propose a new adversarial loss function for the discriminative features. The proposed method is evaluated on synthetic and real-world datasets and is shown to outperform the baselines. 
21348,SP:783049ff463edd1283c058c6106a3e1f9a033df4,Transformer USED-FOR Character - level language modeling. limited resources USED-FOR character - level language models. computational resources USED-FOR Transformer - based models. lightweight model USED-FOR calculation paths. GroupTransformer HYPONYM-OF lightweight model. grouped embedding operators USED-FOR lightweight model. grouped embedding operators USED-FOR calculation paths. inter - group linear operators USED-FOR Group - Transformer. enwik8 CONJUNCTION text8. text8 CONJUNCTION enwik8. LSTM - based models COMPARE Transformer - based models. Transformer - based models COMPARE LSTM - based models. benchmark tasks EVALUATE-FOR GroupTransformer. text8 EVALUATE-FOR GroupTransformer. enwik8 EVALUATE-FOR GroupTransformer. enwik8 HYPONYM-OF benchmark tasks. text8 HYPONYM-OF benchmark tasks. OtherScientificTerm is limitation of recursive operation. Method is group strategy. Task is qualitative analysis. ,"This paper proposes a new transformer-based model for character-level language modeling, called GroupTransformer. The proposed model is based on inter-group linear operators. The authors show that the proposed model outperforms LSTM-based models on several benchmark tasks. ","This paper proposes a new transformer-based model for character-level language modeling, called GroupTransformer. The proposed model is based on inter-group linear operators. The authors show that the proposed model outperforms LSTM-based models on several benchmark tasks. "
21357,SP:946c26d371297c88d0ac246257104099b4585edc,hierarchical - latent - variable structures FEATURE-OF Probabilistic models. approach USED-FOR models. Variational Autoencoders USED-FOR approach. Variational Autoencoders USED-FOR models. inference and optimisation schemes USED-FOR approaches. non - likelihood - based framework USED-FOR generative models. bespoke models CONJUNCTION inference networks. inference networks CONJUNCTION bespoke models. approach USED-FOR models. deep - latent hierarchies USED-FOR models. Optimal Transport USED-FOR approach. Optimal Transport USED-FOR models. it COMPARE Wasserstein Autoencoder. Wasserstein Autoencoder COMPARE it. method USED-FOR generative model. deep - latent hierarchy USED-FOR generative model. Maximum Mean Discrepancy divergence FEATURE-OF Wasserstein Autoencoder. ,This paper proposes a generative model based on Hierarchical-latent-variable structures for probabilistic models. The proposed method is based on the Variational Autoencoders (VAE) framework. The authors show that the proposed method outperforms the Wasserstein Autoencoder (WAE) and the Maximum Mean Discrepancy Divergence (MDSD) for generative models. ,This paper proposes a generative model based on Hierarchical-latent-variable structures for probabilistic models. The proposed method is based on the Variational Autoencoders (VAE) framework. The authors show that the proposed method outperforms the Wasserstein Autoencoder (WAE) and the Maximum Mean Discrepancy Divergence (MDSD) for generative models. 
21366,SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"latent variable models CONJUNCTION adversarial training. adversarial training CONJUNCTION latent variable models. video - specific neural network architectures CONJUNCTION latent variable models. latent variable models CONJUNCTION video - specific neural network architectures. adversarial training CONJUNCTION methods. methods CONJUNCTION adversarial training. methods PART-OF video generation models. they USED-FOR continuations. realism FEATURE-OF continuations. benchmark datasets EVALUATE-FOR autoregressive video generation models. three - dimensional self - attention mechanism USED-FOR autoregressive video generation models. camera movement CONJUNCTION complex object interactions. complex object interactions CONJUNCTION camera movement. complex object interactions CONJUNCTION human movement. human movement CONJUNCTION complex object interactions. Kinetics HYPONYM-OF large scale action recognition dataset. Kinetics HYPONYM-OF phenomena. YouTube videos FEATURE-OF large scale action recognition dataset. Kinetics USED-FOR models. human movement HYPONYM-OF phenomena. camera movement HYPONYM-OF phenomena. complex object interactions HYPONYM-OF phenomena. Metric are statistical complexity, complexity, and fidelity. OtherScientificTerm are inherent stochasticity, and narrow domains. Task is generating natural video. Material is natural video. Generic is approaches. ","This paper proposes a three-dimensional self-attention mechanism for video generation models. The proposed method is based on the three-dimension self attention mechanism, which can be applied to both video-specific neural network architectures and latent variable models. It is shown that the proposed method can achieve state-of-the-art performance on Kinetics and action recognition datasets. ","This paper proposes a three-dimensional self-attention mechanism for video generation models. The proposed method is based on the three-dimension self attention mechanism, which can be applied to both video-specific neural network architectures and latent variable models. It is shown that the proposed method can achieve state-of-the-art performance on Kinetics and action recognition datasets. "
21375,SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"International Classification of Diseases ( ICD ) HYPONYM-OF classification codes. noisy clinical document inputs CONJUNCTION long - tailed label distribution. long - tailed label distribution CONJUNCTION noisy clinical document inputs. Automatic ICD coding HYPONYM-OF multi - label text classification task. frequent and zeroshot codes USED-FOR fine - grained classification. long - tailed label distribution FEATURE-OF multi - label text classification task. noisy clinical document inputs FEATURE-OF multi - label text classification task. latent feature generation framework USED-FOR generalized zero - shot ICD coding. codes USED-FOR prediction. ICD code hierarchical structure CONJUNCTION cycle architecture. cycle architecture CONJUNCTION ICD code hierarchical structure. cycle architecture USED-FOR keywords. framework USED-FOR semantically meaningful features. semantically meaningful features USED-FOR zero - shot codes. cycle architecture USED-FOR framework. ICD code hierarchical structure USED-FOR framework. adversarial generative model USED-FOR generalized zero - shot learning. generalized zero - shot learning USED-FOR multi - label text classification. public MIMIC - III dataset EVALUATE-FOR methods. methods USED-FOR zero - shot codes. AUC score EVALUATE-FOR methods. F1 score EVALUATE-FOR methods. OtherScientificTerm are labeled data, and seen codes. Generic is approach. ",This paper proposes a latent feature generation framework for zero-shot ICD coding for multi-label text classification. The proposed framework is based on the cycle architecture and the ICD code hierarchical structure. The authors show that the proposed method is able to achieve better performance on the public MIMIC-III dataset compared to the baselines. ,This paper proposes a latent feature generation framework for zero-shot ICD coding for multi-label text classification. The proposed framework is based on the cycle architecture and the ICD code hierarchical structure. The authors show that the proposed method is able to achieve better performance on the public MIMIC-III dataset compared to the baselines. 
21384,SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,self - supervised representation learning USED-FOR reinforcement learning ( RL ). self - supervised representation learning USED-FOR sample efficiency. sample efficiency EVALUATE-FOR reinforcement learning ( RL ). forward prediction objective USED-FOR embeddings of states and action sequences. embeddings USED-FOR policy learning. embeddings USED-FOR structure of the environment ’s dynamics. action embeddings USED-FOR model - free RL. sample efficiency EVALUATE-FOR model - free RL. low - dimensional states USED-FOR model - free RL. sample efficiency EVALUATE-FOR action embeddings. state and action embeddings USED-FOR learning of high - quality policies. goal - conditioned continuous control USED-FOR learning of high - quality policies. pixel observations USED-FOR learning of high - quality policies. ,"This paper studies the sample efficiency of self-supervised representation learning in reinforcement learning (RL). In particular, the authors consider the problem of learning the embeddings of states and action sequences in a goal-conditioned continuous control setting, where the goal is to learn high-quality policies. The authors propose to use a forward prediction objective to learn the embedding of the state and the action sequences. They show that the proposed method achieves better sample efficiency compared to the baselines. ","This paper studies the sample efficiency of self-supervised representation learning in reinforcement learning (RL). In particular, the authors consider the problem of learning the embeddings of states and action sequences in a goal-conditioned continuous control setting, where the goal is to learn high-quality policies. The authors propose to use a forward prediction objective to learn the embedding of the state and the action sequences. They show that the proposed method achieves better sample efficiency compared to the baselines. "
21393,SP:11ce1616e721340eea9e80dad7460c77355ac7d1,meta - learning USED-FOR tasks. hand - crafted structure design USED-FOR task - specific meta - learning methods. knowledge bases USED-FOR knowledge organization. structure knowledge USED-FOR meta - learner. framework USED-FOR task heterogeneity. model interpretability EVALUATE-FOR framework. meta - knowledge graph USED-FOR framework. meta - knowledge graph USED-FOR task heterogeneity. 2D toy regression CONJUNCTION few - shot image classification. few - shot image classification CONJUNCTION 2D toy regression. ARML COMPARE baselines. baselines COMPARE ARML. Generic is ones. Method is globally shared meta - learning methods. OtherScientificTerm is cross - task relations. ,"This paper proposes a meta-learning framework for task-specific meta learning. The proposed framework is based on the meta-knowledge graph (meta-KG) framework. The meta-kG is composed of knowledge bases for each task, and a meta learner is trained on the knowledge bases. The authors show that the proposed framework can be applied to a wide range of tasks, including few-shot image classification, toy regression, and 2D toy regression. ","This paper proposes a meta-learning framework for task-specific meta learning. The proposed framework is based on the meta-knowledge graph (meta-KG) framework. The meta-kG is composed of knowledge bases for each task, and a meta learner is trained on the knowledge bases. The authors show that the proposed framework can be applied to a wide range of tasks, including few-shot image classification, toy regression, and 2D toy regression. "
21402,SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"model architecture CONJUNCTION fine - tuning. fine - tuning CONJUNCTION model architecture. attribute - specific data USED-FOR fine - tuning. Plug and Play Language Model ( PPLM ) USED-FOR controllable language generation. pretrained LM CONJUNCTION attribute classifiers. attribute classifiers CONJUNCTION pretrained LM. attribute classifiers USED-FOR text generation. pretrained LM PART-OF Plug and Play Language Model ( PPLM ). attribute models HYPONYM-OF classifiers. attribute models COMPARE LM. LM COMPARE attribute models. attribute alignment CONJUNCTION fluency. fluency CONJUNCTION attribute alignment. automated and human annotated evaluations EVALUATE-FOR attribute alignment. automated and human annotated evaluations EVALUATE-FOR fluency. differentiable attribute models USED-FOR text generation. Material are huge text corpora, and Model samples. Method are retraining, attribute model, and PPLMs. Task are Sampling, and generation. OtherScientificTerm are gradients, and hidden activations. ","This paper proposes a new approach for controllable language generation based on attribute-specific data. The key idea is to use a pre-trained Plug and Play Language Model (PPLM) to generate a set of attributes for each attribute, and then fine-tune the PPLM on these attributes. The authors show that the proposed approach is able to achieve better performance than the state-of-the-art in terms of attribute alignment, fluency, and fine-tuning. ","This paper proposes a new approach for controllable language generation based on attribute-specific data. The key idea is to use a pre-trained Plug and Play Language Model (PPLM) to generate a set of attributes for each attribute, and then fine-tune the PPLM on these attributes. The authors show that the proposed approach is able to achieve better performance than the state-of-the-art in terms of attribute alignment, fluency, and fine-tuning. "
21411,SP:12d0980bfea2de880905a0b87b40856969bb1c58,"deep neural networks USED-FOR machine learning tasks. unlabeled data USED-FOR learning robust representations. unsupervised and self - supervised learning approaches USED-FOR visual data. domain knowledge USED-FOR unsupervised and self - supervised learning approaches. gradient domain FEATURE-OF clean data. clean data USED-FOR noisy input data. denoising autoencoder USED-FOR data representations. visual benchmarks EVALUATE-FOR representations. approach USED-FOR representations. representations USED-FOR vision tasks. Material is supervised data. Method is unsupervised learning framework. Generic is agent. OtherScientificTerm are data structures, and single - scale corruption. ",This paper proposes a novel unsupervised learning framework for learning robust representations from unlabeled data. The authors propose a denoising autoencoder to learn robust representations for clean data and noisy input data. They show that the proposed method is able to learn representations that are robust to single-scale corruption in the gradient domain. They also show that their method can be used to learn a robust representation for noisy data. ,This paper proposes a novel unsupervised learning framework for learning robust representations from unlabeled data. The authors propose a denoising autoencoder to learn robust representations for clean data and noisy input data. They show that the proposed method is able to learn representations that are robust to single-scale corruption in the gradient domain. They also show that their method can be used to learn a robust representation for noisy data. 
21420,SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"Neural networks USED-FOR Natural Language Processing. under - sensitivity FEATURE-OF natural language inference. technique USED-FOR formal verification of this specification. technique USED-FOR models. interval bound propagation ( IBP ) approach USED-FOR technique. training methods USED-FOR under - sensitivity. SNLI and MNLI datasets EVALUATE-FOR IBP training. verified accuracy EVALUATE-FOR IBP training. Generic are they, specification, method, model, and metrics. Method are decomposable attention mechanism, and training. OtherScientificTerm is under - sensitivity problem. Material is SNLI test set. ","This paper studies the problem of under-sensitivity in natural language inference (NLP). The authors propose a new method to verify the accuracy of a neural network trained on the SNLI and MNLI datasets. The proposed method is based on the interval bound propagation (IBP) approach, which is a decomposable attention mechanism. The authors show that the proposed method can be used to train neural networks on the under-sensitive SNLI/MNLI datasets and show that it is able to achieve better performance than existing methods. ","This paper studies the problem of under-sensitivity in natural language inference (NLP). The authors propose a new method to verify the accuracy of a neural network trained on the SNLI and MNLI datasets. The proposed method is based on the interval bound propagation (IBP) approach, which is a decomposable attention mechanism. The authors show that the proposed method can be used to train neural networks on the under-sensitive SNLI/MNLI datasets and show that it is able to achieve better performance than existing methods. "
21429,SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"replay memory USED-FOR network updates. soft divergence FEATURE-OF structure. data graph USED-FOR transitions. Q - values FEATURE-OF Markov Decision Process ( MDP ). favorable structure FEATURE-OF subgraph. transition PART-OF MDP. transition PART-OF continuous Q - learning problem. Q - value FEATURE-OF transition. Q - value USED-FOR continuous Q - learning problem. Q - value FEATURE-OF transition. lower bounds USED-FOR method. lower bounds USED-FOR TD learning. TD learning USED-FOR method. soft divergence FEATURE-OF method. sample efficiency EVALUATE-FOR method. replay memory capacity FEATURE-OF algorithm. OtherScientificTerm are state and action spaces, QGRAPH, hyperparameters, and QGRAPHs. ","This paper studies the problem of continuous Q-learning in the Markov Decision Process (MDP) setting. The authors propose a new algorithm for learning the Q-value of a transition in the MDP. The transition is defined as a subgraph of the subgraphs of the state and action spaces, and the transition is represented as a soft divergence between the Q value and the soft divergence of the transition. They show that the proposed algorithm can learn the transition in a continuous MDP setting. They also provide lower bounds on the sample efficiency of the algorithm. ","This paper studies the problem of continuous Q-learning in the Markov Decision Process (MDP) setting. The authors propose a new algorithm for learning the Q-value of a transition in the MDP. The transition is defined as a subgraph of the subgraphs of the state and action spaces, and the transition is represented as a soft divergence between the Q value and the soft divergence of the transition. They show that the proposed algorithm can learn the transition in a continuous MDP setting. They also provide lower bounds on the sample efficiency of the algorithm. "
21438,SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,approach USED-FOR problem. domain - invariant embeddings USED-FOR approach. domain - invariant embeddings USED-FOR problem. embedding complexity FEATURE-OF generalization. theoretical framework USED-FOR multilayer neural networks. strategy COMPARE layer - dependent complexity tradeoff. layer - dependent complexity tradeoff COMPARE strategy. Task is Unsupervised domain adaptation. Generic is complexity. ,This paper studies the problem of unsupervised domain adaptation in multilayer neural networks. The authors propose a theoretical framework for learning domain-invariant embeddings for the problem. They show that the embedding complexity tradeoff between layer-dependent and layer-independent complexity tradeoffs can be reduced to a single-layer complexity trade-off. They also show that this tradeoff can be achieved by learning a domain invariant embedding for each layer of the network. ,This paper studies the problem of unsupervised domain adaptation in multilayer neural networks. The authors propose a theoretical framework for learning domain-invariant embeddings for the problem. They show that the embedding complexity tradeoff between layer-dependent and layer-independent complexity tradeoffs can be reduced to a single-layer complexity trade-off. They also show that this tradeoff can be achieved by learning a domain invariant embedding for each layer of the network. 
21447,SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"framework USED-FOR algorithm - dependent generalization error bounds. PAC - Bayesian theory CONJUNCTION algorithmic stability. algorithmic stability CONJUNCTION PAC - Bayesian theory. PAC - Bayesian theory USED-FOR framework. mini - batch and acceleration CONJUNCTION Entropy - SGD. Entropy - SGD CONJUNCTION mini - batch and acceleration. momentum CONJUNCTION mini - batch and acceleration. mini - batch and acceleration CONJUNCTION momentum. Bayes - Stability method USED-FOR data - dependent generalization bounds. data - dependent generalization bounds USED-FOR stochastic gradient Langevin dynamics ( SGLD ). momentum HYPONYM-OF noisy gradient methods. Entropy - SGD HYPONYM-OF noisy gradient methods. mini - batch and acceleration HYPONYM-OF noisy gradient methods. data - dependent bounds USED-FOR randomly labelled data. randomly labelled data COMPARE normal data. normal data COMPARE randomly labelled data. bounded loss CONJUNCTION ` 2 regularization term. ` 2 regularization term CONJUNCTION bounded loss. bounded loss PART-OF total loss. ` 2 regularization term PART-OF total loss. Log - Sobolev inequality USED-FOR parameter distribution. generalization bounds USED-FOR continuous Langevin dynamic. Log - Sobolev inequality USED-FOR generalization bounds. Metric is Generalization error. OtherScientificTerm are out - of - sample error, tight generalization error bounds, generalization error bounds, and noise level. Task is statistical learning theory. Method is Bayes - Stability. Generic is bounds. ","This paper studies the generalization error bounds for stochastic gradient Langevin dynamics (SGLD). The authors propose a new generalization bound based on PAC-Bayesian theory. The generalization bounds are based on the Log-Sobolev inequality of the parameter distribution of the Langevin dynamic. The authors show that under certain assumptions, the bound is tight. They also show that the bound can be extended to the data-dependent case. ","This paper studies the generalization error bounds for stochastic gradient Langevin dynamics (SGLD). The authors propose a new generalization bound based on PAC-Bayesian theory. The generalization bounds are based on the Log-Sobolev inequality of the parameter distribution of the Langevin dynamic. The authors show that under certain assumptions, the bound is tight. They also show that the bound can be extended to the data-dependent case. "
21456,SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"spatial memory CONJUNCTION goal - directed spatial navigation. goal - directed spatial navigation CONJUNCTION spatial memory. hippocampus USED-FOR goal - directed spatial navigation. hippocampus USED-FOR spatial memory. hippocampal CA1 neurons USED-FOR continual learning. populationlevel activity FEATURE-OF hippocampal CA1 neurons. populationlevel activity USED-FOR continual learning. continual learning USED-FOR spatial navigation strategies. navigational strategies CONJUNCTION reward location. reward location CONJUNCTION navigational strategies. hippocampal neurons USED-FOR task variables. decisions CONJUNCTION navigational strategies. navigational strategies CONJUNCTION decisions. firing activity USED-FOR dPCA. decisions HYPONYM-OF task variables. reward location HYPONYM-OF task variables. navigational strategies HYPONYM-OF task variables. dPCA USED-FOR components. hippocampal features COMPARE reinforcement learning algorithms. reinforcement learning algorithms COMPARE hippocampal features. deep reinforcement learning model COMPARE animal learning. animal learning COMPARE deep reinforcement learning model. hippocampus USED-FOR reinforced spatial continual learning. biological and machine learning USED-FOR spatial continual learning. Task are continual learning of navigational strategies, and allocentric and egocentric spatial tasks. Method is Demixed Principal Component Analysis ( dPCA ). OtherScientificTerm is task switching. ",This paper proposes a method for continual learning in the hippocampus. The method is based on the Demixed Principal Component Analysis (dPCA) framework. The authors show that dPCA is able to capture the dynamics of the hippocampal neurons in the continual learning setting. They also show that the proposed method outperforms baselines on both allocentric and egocentric tasks.,This paper proposes a method for continual learning in the hippocampus. The method is based on the Demixed Principal Component Analysis (dPCA) framework. The authors show that dPCA is able to capture the dynamics of the hippocampal neurons in the continual learning setting. They also show that the proposed method outperforms baselines on both allocentric and egocentric tasks.
21465,SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"Monte Carlo Tree Search ( MCTS ) USED-FOR discrete environments. it USED-FOR continuous domains. Go HYPONYM-OF discrete environments. tree search based policy optimization method USED-FOR continuous environments. TPO HYPONYM-OF tree search based policy optimization method. hybrid approach USED-FOR policy optimization. hybrid approach USED-FOR TPO. continuous action space FEATURE-OF MCTS tree. off - policy MCTS trajectories USED-FOR policy gradient. pre - trained policy USED-FOR bootstrapping tree search. branching factor CONJUNCTION simulation count. simulation count CONJUNCTION branching factor. policy bootstrapping USED-FOR continuous MCTS. branching factor USED-FOR continuous MCTS. simulation count USED-FOR continuous MCTS. PPO USED-FOR baseline policy optimization algorithm. TPO USED-FOR policy. Humanoid HYPONYM-OF complex environments. Method are limiting tree search branching factor, and MCTS training. OtherScientificTerm are tree search branching factor, policy distribution, and loss function. Generic are approach, and baseline algorithm. Metric is MCTS branching factor. ",This paper proposes a hybrid approach to bootstrapping tree search based policy optimization for continuous MCTS. The main idea is to use a policy bootstrapped from a pre-trained policy to bootstrap the policy from the off-policy trajectories. The authors show that this approach is able to achieve better performance than the baseline policy optimization algorithm PPO. They also show that their approach can be applied to continuous environments. ,This paper proposes a hybrid approach to bootstrapping tree search based policy optimization for continuous MCTS. The main idea is to use a policy bootstrapped from a pre-trained policy to bootstrap the policy from the off-policy trajectories. The authors show that this approach is able to achieve better performance than the baseline policy optimization algorithm PPO. They also show that their approach can be applied to continuous environments. 
21474,SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,accuracy EVALUATE-FOR network. sparse subnetworks PART-OF neural networks. supervision USED-FOR generating process. winning tickets COMPARE winning tickets. winning tickets COMPARE winning tickets. ImageNet dataset EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. Material is lottery ticket hypothesis. Method is neural network optimization. OtherScientificTerm is initializations. ,This paper studies the lottery ticket hypothesis for neural network optimization. The authors show that winning tickets can be generated from sparse subnetworks of a neural network. They also show that the winning tickets are more likely to be generated in the presence of supervision than in the absence of supervision. They further show that this phenomenon is also observed in the case where the number of training samples is large. The paper also shows that the lottery tickets are generated in a way that is similar to winning tickets. ,This paper studies the lottery ticket hypothesis for neural network optimization. The authors show that winning tickets can be generated from sparse subnetworks of a neural network. They also show that the winning tickets are more likely to be generated in the presence of supervision than in the absence of supervision. They further show that this phenomenon is also observed in the case where the number of training samples is large. The paper also shows that the lottery tickets are generated in a way that is similar to winning tickets. 
21483,SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"excessive prediction undersensitivity HYPONYM-OF complementary problem. spurious surface patterns USED-FOR models. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial training USED-FOR defence strategies. data augmentation USED-FOR defence strategies. undersensitivity attacks FEATURE-OF model. held out evaluation data EVALUATE-FOR undersensitivity attacks. held out evaluation data EVALUATE-FOR model. they COMPARE model. model COMPARE they. train / evaluation distribution mismatch FEATURE-OF biased data setting. biased data setting EVALUATE-FOR model. predictive cues USED-FOR they. biased data setting EVALUATE-FOR adversarially robust models. F1 EVALUATE-FOR they. F1 EVALUATE-FOR model. Method are Neural reading comprehension models, noisy adversarial attack, and NewsQA models. OtherScientificTerm are adversarially selected input, semantically invariant text perturbations, and semantic variations of comprehension questions. Generic is attack. Material is adversarially generated questions. ","This paper studies the problem of excessive prediction undersensitivity in adversarial training, which is an important problem in the context of adversarial learning. The authors propose a novel approach to mitigate this problem by training adversarial models that are robust to noisy adversarial attacks. The proposed approach is based on the idea that the adversarial model is sensitive to spurious surface patterns in the training data, which can be used to improve the robustness of the model to noisy attacks. They show that the proposed approach can be applied to adversarially robust models in a biased data setting. They also show that their approach is able to outperform the state-of-the-art adversarial robust models on the F1 dataset.","This paper studies the problem of excessive prediction undersensitivity in adversarial training, which is an important problem in the context of adversarial learning. The authors propose a novel approach to mitigate this problem by training adversarial models that are robust to noisy adversarial attacks. The proposed approach is based on the idea that the adversarial model is sensitive to spurious surface patterns in the training data, which can be used to improve the robustness of the model to noisy attacks. They show that the proposed approach can be applied to adversarially robust models in a biased data setting. They also show that their approach is able to outperform the state-of-the-art adversarial robust models on the F1 dataset."
21492,SP:5da870060778de460c1abe91562d6f3e707efef4,"reinforcement learning ( RL ) agents USED-FOR real - world tasks. reinforcement learning ( RL ) agents USED-FOR safety. approaches USED-FOR problem. safety penalty PART-OF reward function. complex domains EVALUATE-FOR approaches. model - based approach USED-FOR safety. imaginative module HYPONYM-OF directed graph. it CONJUNCTION RL algorithm. RL algorithm CONJUNCTION it. gridworld environments CONJUNCTION self - driving car simulator. self - driving car simulator CONJUNCTION gridworld environments. approach COMPARE baseline. baseline COMPARE approach. self - driving car simulator EVALUATE-FOR approach. gridworld environments EVALUATE-FOR approach. self - driving car simulator EVALUATE-FOR proposal. gridworld environments EVALUATE-FOR proposal. OtherScientificTerm are bad incentives, unsafe scenarios, transition dynamics of the environment, baseline state, and discrete action space. Generic are they, graph, method, and task. ",This paper proposes an approach to improve the safety of reinforcement learning agents in the presence of unsafe scenarios. The authors propose a novel model-based approach that uses a directed graph to model the transition dynamics of the environment. The proposed approach is evaluated on a gridworld environment and a self-driving car simulator. ,This paper proposes an approach to improve the safety of reinforcement learning agents in the presence of unsafe scenarios. The authors propose a novel model-based approach that uses a directed graph to model the transition dynamics of the environment. The proposed approach is evaluated on a gridworld environment and a self-driving car simulator. 
21501,SP:c2796f28fb067138303df8d424d646f4ada31558,"numerical error FEATURE-OF finite differences. deep learning models USED-FOR physics - governing observations. unstructured grid USED-FOR physics - governing observations. neighboring information USED-FOR finite differences. physics equations USED-FOR finite differences. PA - DGN USED-FOR dynamical relations. synthetic data CONJUNCTION real - world climate observations. real - world climate observations CONJUNCTION synthetic data. PA - DGN USED-FOR approximation of directional derivatives. PA - DGN USED-FOR prediction of graph signals. approximation of directional derivatives CONJUNCTION prediction of graph signals. prediction of graph signals CONJUNCTION approximation of directional derivatives. real - world climate observations USED-FOR prediction of graph signals. synthetic data USED-FOR prediction of graph signals. weather stations USED-FOR real - world climate observations. Task is dynamics of physical systems. OtherScientificTerm are discretization error, spatial and temporal differences, and sequential observations. Material is sparse data. Generic is architecture. ","This paper proposes an unstructured grid-based deep learning model for physics-governing observations. The proposed method is based on the notion of discretization error, which is defined as the difference between the discretized error of a pair of observations and their nearest neighbors. The authors show that discretisation error is a function of the number of neighboring information in the graph, and that it can be understood as a measure of the distance between two points in a graph.  The authors also show that their method can be used for prediction of directional derivatives and prediction of graph signals. ","This paper proposes an unstructured grid-based deep learning model for physics-governing observations. The proposed method is based on the notion of discretization error, which is defined as the difference between the discretized error of a pair of observations and their nearest neighbors. The authors show that discretisation error is a function of the number of neighboring information in the graph, and that it can be understood as a measure of the distance between two points in a graph.  The authors also show that their method can be used for prediction of directional derivatives and prediction of graph signals. "
21510,SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"nonsmooth regularization CONJUNCTION constraints. constraints CONJUNCTION nonsmooth regularization. nonsmooth regularization FEATURE-OF structured neural networks ( NN ). constraints FEATURE-OF structured neural networks ( NN ). interval constraints HYPONYM-OF constraints. ` 1 - norm HYPONYM-OF nonsmooth regularization. constrained nonsmooth nonconvex optimization problem USED-FOR training. ProxSGD USED-FOR sparse or binary neural networks. regularization function CONJUNCTION constraint set. constraint set CONJUNCTION regularization function. regularization function USED-FOR ProxSGD. constraint set USED-FOR ProxSGD. OtherScientificTerm are learning rates, and stationary point. Method is ProxSGD algorithm. ","This paper studies the problem of training neural networks in a constrained nonsmooth nonconvex optimization problem. The authors propose a new algorithm called ProxSGD for solving this problem, which is based on the notion of interval constraints. The main contribution of this paper is to provide a new regularization function for the training of neural networks. The paper also provides a theoretical analysis of the convergence of the proposed algorithm. ","This paper studies the problem of training neural networks in a constrained nonsmooth nonconvex optimization problem. The authors propose a new algorithm called ProxSGD for solving this problem, which is based on the notion of interval constraints. The main contribution of this paper is to provide a new regularization function for the training of neural networks. The paper also provides a theoretical analysis of the convergence of the proposed algorithm. "
21519,SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"encoder USED-FOR compression algorithms. approximate methods USED-FOR encoder. approximate methods USED-FOR algorithms. framework USED-FOR lossy image compression. non - deterministic compression codec USED-FOR framework. expected code length CONJUNCTION relative entropy. relative entropy CONJUNCTION expected code length. gradient - based optimizers USED-FOR end - to - end differentiable compression framework. it USED-FOR lossy image compression. rate - distortion curves COMPARE state - of - the - art. state - of - the - art COMPARE rate - distortion curves. low bitrates FEATURE-OF rate - distortion curves. it USED-FOR method. Kodak dataset EVALUATE-FOR rate - distortion curves. Probabilistic Ladder Networks ( PLNs ) USED-FOR it. low bitrates EVALUATE-FOR state - of - the - art. CLIC 2018 dataset EVALUATE-FOR Probabilistic Ladder Networks ( PLNs ). Material is image. OtherScientificTerm are discrete code, quantization step, continuous space, and encoding distribution. Method is decoder. Generic is process. ","This paper proposes an end-to-end differentiable compression framework for lossy image compression. The main idea is to use a non-deterministic compression codec for the encoder and decoder, and then use gradient-based optimizers for the decoder. The authors show that the proposed method outperforms state-of-the-art compression algorithms on Kodak and CLIC 2018. ","This paper proposes an end-to-end differentiable compression framework for lossy image compression. The main idea is to use a non-deterministic compression codec for the encoder and decoder, and then use gradient-based optimizers for the decoder. The authors show that the proposed method outperforms state-of-the-art compression algorithms on Kodak and CLIC 2018. "
21528,SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"compressed JPG ( C - JPG ) image USED-FOR SR. SR HYPONYM-OF image processing operations. components CONJUNCTION cycle loss. cycle loss CONJUNCTION components. components PART-OF SR structure. cycle loss PART-OF SR structure. high - qualified SR images USED-FOR prevalent C - JPG images. hybrid loss function USED-FOR SR generation. cycle loss PART-OF SR solver. cycle loss USED-FOR hybrid loss function. SR solver USED-FOR hybrid loss function. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Task are Super Resolution ( SR ), and SR issue. Method are SR models, C - JPG, functional sub - model, and SR approaches. OtherScientificTerm is storage space. Material is C - JPG images. ",This paper proposes a hybrid loss function for Super Resolution (SR) generation. The proposed method is based on the idea of hybrid loss and cycle loss. The hybrid loss is a combination of cycle loss and SR solver. The authors show that the proposed method outperforms state-of-the-art methods on C-JPG image generation tasks. ,This paper proposes a hybrid loss function for Super Resolution (SR) generation. The proposed method is based on the idea of hybrid loss and cycle loss. The hybrid loss is a combination of cycle loss and SR solver. The authors show that the proposed method outperforms state-of-the-art methods on C-JPG image generation tasks. 
21537,SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"fully convolutional network architecture USED-FOR surface of pass probabilities. professional soccer matches USED-FOR single - location labels. single - location labels USED-FOR fully convolutional network architecture. single - location labels USED-FOR surface of pass probabilities. feature hierarchy USED-FOR network. low - level inputs USED-FOR network. approach USED-FOR weakly supervised learning. approach USED-FOR spatiotemporal decision - making analysis. spatiotemporal decision - making analysis HYPONYM-OF sports. network USED-FOR pass - selection likelihood. deep learning architecture USED-FOR sports analytics. OtherScientificTerm are sampling levels, coarse and fine detail, single pixel correspondence, and predicted probability map. ","This paper proposes a deep learning approach for learning the surface of pass probabilities for soccer matches. The proposed approach is based on the idea of learning a low-level feature hierarchy for each player, and then using the feature hierarchy to predict the probability map of the next pass. The authors show that the proposed approach can be applied to a wide range of tasks, and is able to achieve state-of-the-art performance. ","This paper proposes a deep learning approach for learning the surface of pass probabilities for soccer matches. The proposed approach is based on the idea of learning a low-level feature hierarchy for each player, and then using the feature hierarchy to predict the probability map of the next pass. The authors show that the proposed approach can be applied to a wide range of tasks, and is able to achieve state-of-the-art performance. "
21546,SP:1ae31baf383fc520687b255d9cac14c3b040e253,"side information USED-FOR inductive matrix completion model. user ’s age CONJUNCTION movie ’s genre. movie ’s genre CONJUNCTION user ’s age. movie ’s genre HYPONYM-OF content ( side information ). user ’s age HYPONYM-OF content ( side information ). IGMC USED-FOR graph neural network ( GNN ). It COMPARE transductive baselines. transductive baselines COMPARE It. model USED-FOR Douban movie ratings. MovieLens dataset USED-FOR model. Long - range dependencies USED-FOR modeling recommender systems. side information USED-FOR inductive matrix completion models. OtherScientificTerm are ( rating ) matrix, low - dimensional latent embeddings, embeddings, rating matrix, subgraphs, and local graph patterns. Method are matrix completion methods, and transductive methods. Task is matrix completion. Generic is it. ",This paper proposes an inductive matrix completion model that learns a graph neural network (GNN) to learn a low-dimensional latent embedding of the rating matrix. The proposed method is based on the idea of graph neural networks (GANs) that can be used to model long-range dependencies between subgraphs. The authors show that the proposed method outperforms existing matrix completion methods on the MovieLens dataset. ,This paper proposes an inductive matrix completion model that learns a graph neural network (GNN) to learn a low-dimensional latent embedding of the rating matrix. The proposed method is based on the idea of graph neural networks (GANs) that can be used to model long-range dependencies between subgraphs. The authors show that the proposed method outperforms existing matrix completion methods on the MovieLens dataset. 
21555,SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,smooth objective function USED-FOR unconstrained minimization. heavy ball momentum USED-FOR stochastic zeroth - order method. learning to continuous control tasks EVALUATE-FOR method. STP COMPARE policy gradient methods. policy gradient methods COMPARE STP. STP COMPARE derivative - free optimization algorithms. derivative - free optimization algorithms COMPARE STP. derivative - free optimization algorithms CONJUNCTION policy gradient methods. policy gradient methods CONJUNCTION derivative - free optimization algorithms. SMTP COMPARE STP. STP COMPARE SMTP. SMTP COMPARE methods. methods COMPARE SMTP. STP CONJUNCTION methods. methods CONJUNCTION STP. importance sampling USED-FOR SMTP. OtherScientificTerm is function evaluations. Metric is complexity. Method is SMTP_IS. ,This paper proposes a stochastic zeroth-order method for unconstrained minimization of smooth objective functions. The main idea is to use heavy ball momentum to approximate the momentum of the smooth objective function. The authors show that the proposed method outperforms existing methods on a number of continuous control tasks.,This paper proposes a stochastic zeroth-order method for unconstrained minimization of smooth objective functions. The main idea is to use heavy ball momentum to approximate the momentum of the smooth objective function. The authors show that the proposed method outperforms existing methods on a number of continuous control tasks.
21564,SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"environmental stochasticity CONJUNCTION uncertainties. uncertainties CONJUNCTION environmental stochasticity. deep learning architecture USED-FOR multiagent coordination. multiagent coordination mechanisms USED-FOR deep learning architecture. Action Semantics Network ( ASN ) HYPONYM-OF network architecture. action semantics USED-FOR neural networks. neural networks USED-FOR ASN. action semantics USED-FOR ASN. ASN CONJUNCTION deep reinforcement learning ( DRL ) algorithms. deep reinforcement learning ( DRL ) algorithms CONJUNCTION ASN. StarCraft II micromanagement CONJUNCTION Neural MMO. Neural MMO CONJUNCTION StarCraft II micromanagement. DRL approaches COMPARE network architectures. network architectures COMPARE DRL approaches. Neural MMO EVALUATE-FOR ASN. StarCraft II micromanagement EVALUATE-FOR ASN. ASN COMPARE DRL approaches. DRL approaches COMPARE ASN. ASN COMPARE network architectures. network architectures COMPARE ASN. Task are multiagent systems ( MASs ), and system evolution. Material is MASs. OtherScientificTerm is co - learning agents. ","This paper proposes an action semantics network (ASN) for multi-agent systems (MAS). The proposed ASN is based on the Action Semantics Network (ASNS) framework, which is an extension of the action semantics framework for reinforcement learning (RL). The main difference between ASN and ASNS is that ASN uses a neural network to learn action semantics, while ASNS uses a deep RL algorithm to learn actions. The authors show that the proposed ASNs outperform existing DRL algorithms on a variety of tasks, including StarCraft II micromanagement and Neural MMO.","This paper proposes an action semantics network (ASN) for multi-agent systems (MAS). The proposed ASN is based on the Action Semantics Network (ASNS) framework, which is an extension of the action semantics framework for reinforcement learning (RL). The main difference between ASN and ASNS is that ASN uses a neural network to learn action semantics, while ASNS uses a deep RL algorithm to learn actions. The authors show that the proposed ASNs outperform existing DRL algorithms on a variety of tasks, including StarCraft II micromanagement and Neural MMO."
21573,SP:efaf3a440dc17e05177832083ffbc23760ed7c97,Value - based methods USED-FOR planning and deep reinforcement learning ( RL ). Q function HYPONYM-OF state - action value function. system dynamics USED-FOR global structures of the Q function. lowrank structure USED-FOR big data matrices. low - rank Q functions USED-FOR control and deep RL tasks. Matrix Estimation ( ME ) techniques USED-FOR framework. low - rank structure FEATURE-OF Q functions. low - rank structure USED-FOR framework. planning procedure USED-FOR classical control. scheme USED-FOR value - based RL techniques. scheme USED-FOR “ low - rank ” tasks. control tasks CONJUNCTION Atari games. Atari games CONJUNCTION control tasks. control tasks EVALUATE-FOR approach. Atari games EVALUATE-FOR approach. Task is planning and deep RL. Generic is structures. ,This paper proposes a method for learning low-rank Q functions for planning and deep RL. The proposed method is based on Matrix Estimation (ME) techniques. The authors show that the proposed method can be applied to both classical control and deep reinforcement learning tasks. The method is evaluated on a number of Atari games.,This paper proposes a method for learning low-rank Q functions for planning and deep RL. The proposed method is based on Matrix Estimation (ME) techniques. The authors show that the proposed method can be applied to both classical control and deep reinforcement learning tasks. The method is evaluated on a number of Atari games.
21582,SP:430336893b247b7bd45687d78b0d0511a7369e87,"batch reinforcement learning USED-FOR sample - efficient learning. batch reinforcement learning USED-FOR Deep Reinforcement Learning ( DRL ). off - policy DRL algorithms USED-FOR batch DRL setting. action space FEATURE-OF maximizing Q functions. state - action pairs USED-FOR policy network. it USED-FOR policy network. state - action pairs USED-FOR it. imitation learning USED-FOR it. imitation learning USED-FOR policy network. Mujoco benchmark EVALUATE-FOR BAIL. Generic is algorithm. Method are Best - Action Imitation Learning ( BAIL ), and offpolicy DRL algorithms. OtherScientificTerm is Q functions. ","This paper proposes an off-policy DRL algorithm called Best-action imitation learning (BAIL) for batch reinforcement learning (BRL). BAIL is based on best-action imitation learning, which is an extension of the Best-Action Imitation Learning (BIL) algorithm. The main difference between BAIL and BIL is that BAIL considers the action space instead of the state space. BAIL also considers the Q function instead of maximizing Q functions. The authors show that the proposed algorithm outperforms BIL on the Mujoco benchmark.","This paper proposes an off-policy DRL algorithm called Best-action imitation learning (BAIL) for batch reinforcement learning (BRL). BAIL is based on best-action imitation learning, which is an extension of the Best-Action Imitation Learning (BIL) algorithm. The main difference between BAIL and BIL is that BAIL considers the action space instead of the state space. BAIL also considers the Q function instead of maximizing Q functions. The authors show that the proposed algorithm outperforms BIL on the Mujoco benchmark."
21591,SP:94078964876667e8a5d9ae7728d779d5b91a576e,"feature representations CONJUNCTION classifiers. classifiers CONJUNCTION feature representations. classifiers PART-OF deep extreme multi - label learning. feature representations PART-OF deep extreme multi - label learning. deep extreme classifiers USED-FOR short text documents. word embeddings USED-FOR DeepXML. negative sub - sampling techniques USED-FOR negative training data. accuracy EVALUATE-FOR DeepXML. residual connection USED-FOR them. Slice algorithm USED-FOR DeepXML architecture. Slice algorithm USED-FOR pretrained embeddings. pretrained embeddings USED-FOR DeepXML architecture. it COMPARE XML - CNN. XML - CNN COMPARE it. XML - CNN CONJUNCTION AttentionXML. AttentionXML CONJUNCTION XML - CNN. it COMPARE AttentionXML. AttentionXML COMPARE it. DeepXML COMPARE leading techniques. leading techniques COMPARE DeepXML. leading techniques USED-FOR search engine queries. search engine queries CONJUNCTION advertiser bid phrases. advertiser bid phrases CONJUNCTION search engine queries. DeepXML USED-FOR search engine queries. Method are DeepXML algorithm, and classifier. Generic is architecture. ","This paper proposes a novel deep extreme multi-label learning method for short text documents. The proposed method is based on the Slice algorithm, which uses negative sub-sampling techniques for negative training data. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and performance on search engine queries. ","This paper proposes a novel deep extreme multi-label learning method for short text documents. The proposed method is based on the Slice algorithm, which uses negative sub-sampling techniques for negative training data. The authors show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and performance on search engine queries. "
21600,SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"binary vector representations ( hash codes ) USED-FOR Hashing - based collaborative filtering. Hamming distance USED-FOR recommendations. Hamming distance USED-FOR hashing - based collaborative filtering. user hash code USED-FOR mask. Boolean AND operation USED-FOR user hash code. Boolean AND operation USED-FOR mask. approach COMPARE baselines. baselines COMPARE approach. NDCG EVALUATE-FOR approach. runtime overhead EVALUATE-FOR Hamming distance. self - masking COMPARE Hamming distance. Hamming distance COMPARE self - masking. OtherScientificTerm are hash codes, and binary user - level importance weighting. Task is distance computation. Generic is it. ",This paper proposes a new method for hashing-based collaborative filtering. The proposed method is based on the Hamming distance between user hash codes and user-level importance weighting. The authors show that the proposed method can be used to reduce the computational cost of the hashing algorithm. The method is evaluated on the NDCG benchmark.,This paper proposes a new method for hashing-based collaborative filtering. The proposed method is based on the Hamming distance between user hash codes and user-level importance weighting. The authors show that the proposed method can be used to reduce the computational cost of the hashing algorithm. The method is evaluated on the NDCG benchmark.
21609,SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"Generative adversarial networks ( GANs ) USED-FOR images. mode collapse FEATURE-OF GAN ’s learned distribution. evaluation metrics EVALUATE-FOR image synthesis. low - level perceptual quality EVALUATE-FOR evaluation metrics. statistical tools USED-FOR mode collapse. mode collapse FEATURE-OF GANs. mode collapse FEATURE-OF GANs. toolset USED-FOR GANs. toolset USED-FOR mode collapse. OtherScientificTerm are GAN learned distribution, and model parameters. ",This paper studies the phenomenon of mode collapse in GANs. The authors show that mode collapse occurs when the learned distribution of a GAN’s learned distribution converges to a low-level perceptual quality. They then propose a new statistical tool to measure mode collapse. They also provide a theoretical analysis of the phenomenon.,This paper studies the phenomenon of mode collapse in GANs. The authors show that mode collapse occurs when the learned distribution of a GAN’s learned distribution converges to a low-level perceptual quality. They then propose a new statistical tool to measure mode collapse. They also provide a theoretical analysis of the phenomenon.
21618,SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"over - parametrized neural networks CONJUNCTION linearized models. linearized models CONJUNCTION over - parametrized neural networks. Neural Tangent Kernels ( NTKs ) USED-FOR linearized models. neural networks COMPARE linearized models. linearized models COMPARE neural networks. Taylor expansion FEATURE-OF network. optimization landscape FEATURE-OF randomized two - layer networks. escaping - saddle algorithms USED-FOR optimization landscape. mild distributional assumptions FEATURE-OF dimension factor. it USED-FOR networks. higher - order terms PART-OF Taylor series. networks CONJUNCTION higher - order terms. higher - order terms CONJUNCTION networks. it USED-FOR randomization technique. Method are NTK theory, Taylor expansion of the network, quadratic models, and randomized networks. Generic are theory, and them. OtherScientificTerm are NTK regime, NTK, and sample complexity bounds. Material is quadratic case. ","This paper studies the Taylor expansion of neural tangent kernels (NTKs) in randomized two-layer neural networks. The authors show that the sample complexity of NTKs is bounded in the quadratic case, which is a special case of the NTK regime. They also show that NTK theory can be extended to the case where the dimension factor is larger than the dimension of the neural network. They show that this is the case for any NTK with higher-order terms. ","This paper studies the Taylor expansion of neural tangent kernels (NTKs) in randomized two-layer neural networks. The authors show that the sample complexity of NTKs is bounded in the quadratic case, which is a special case of the NTK regime. They also show that NTK theory can be extended to the case where the dimension factor is larger than the dimension of the neural network. They show that this is the case for any NTK with higher-order terms. "
21627,SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"graph data USED-FOR downstream tasks. Graph Neural Networks ( GNNs ) USED-FOR graph data. graph filter design PART-OF GNN models. filter USED-FOR graph data. graph properties USED-FOR graph filter. filters USED-FOR graph. assessment tool EVALUATE-FOR graph convolutional filters. graph convolutional filters USED-FOR graph. node classification EVALUATE-FOR graph convolutional filters. graphs USED-FOR graph convolutional filters. model USED-FOR data - specific filters. Adaptive Filter Graph Neural Network ( AFGNN ) HYPONYM-OF model. AFGNN USED-FOR graph. base filters PART-OF AFGNN. graph filter assessment USED-FOR AFGNN. synthetic and real - world benchmark datasets EVALUATE-FOR model. model USED-FOR filter. Method are optimal filter, and Graph Filter Discriminant Score ( GFD ). Task is semi - supervised node classification task. OtherScientificTerm is loss term. ","This paper proposes a novel adaptive filter graph neural network (AFGNN) for graph convolutional filters. The proposed method is based on the idea that the optimal filter for a given graph is the one that maximizes the graph filter discriminant score (GFD), which is a measure of the similarity between the input and the target graph. The authors show that the proposed method outperforms the baselines on several benchmark datasets.","This paper proposes a novel adaptive filter graph neural network (AFGNN) for graph convolutional filters. The proposed method is based on the idea that the optimal filter for a given graph is the one that maximizes the graph filter discriminant score (GFD), which is a measure of the similarity between the input and the target graph. The authors show that the proposed method outperforms the baselines on several benchmark datasets."
21636,SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"i.i.d. test set EVALUATE-FOR Overparameterized neural networks. Distributionally robust optimization ( DRO ) USED-FOR models. group DRO USED-FOR overparameterized neural networks. average training loss FEATURE-OF model. vanishing worst - case training loss FEATURE-OF model. natural language inference task CONJUNCTION image tasks. image tasks CONJUNCTION natural language inference task. early stopping HYPONYM-OF regularization. regularization USED-FOR group DRO models. regularization USED-FOR worst - group generalization. it USED-FOR average generalization. overparameterized regime FEATURE-OF worst - group generalization. stochastic optimization algorithm USED-FOR group DRO models. convergence guarantees FEATURE-OF stochastic optimization algorithm. OtherScientificTerm is spurious correlations. Metric are worst - case training loss, worst - group accuracies, and average accuracies. ","This paper studies the problem of distributionally robust optimization (DRO) for overparameterized neural networks. The authors show that the worst-case training loss of a group DRO model is vanishing in the overparametrized regime, and that the average generalization of the model is also vanishing. They then propose a stochastic optimization algorithm to solve this problem, and show that it converges to the optimal worst-group generalization. They also provide convergence guarantees for the proposed algorithm.","This paper studies the problem of distributionally robust optimization (DRO) for overparameterized neural networks. The authors show that the worst-case training loss of a group DRO model is vanishing in the overparametrized regime, and that the average generalization of the model is also vanishing. They then propose a stochastic optimization algorithm to solve this problem, and show that it converges to the optimal worst-group generalization. They also provide convergence guarantees for the proposed algorithm."
21645,SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"local explanation methods USED-FOR decision of black - box classifiers. ad hoc constraints FEATURE-OF classification loss. ad hoc constraints USED-FOR relevance scores. neural network USED-FOR distribution of relevance scores. it CONJUNCTION neural network. neural network CONJUNCTION it. it USED-FOR distribution of relevance scores. classification loss USED-FOR predictor. strategy USED-FOR discriminative scores. features USED-FOR discriminative scores. faithfulness CONJUNCTION explainability. explainability CONJUNCTION faithfulness. method COMPARE others. others COMPARE method. faithfulness EVALUATE-FOR others. explainability EVALUATE-FOR others. faithfulness EVALUATE-FOR method. explainability EVALUATE-FOR method. Generic is methods. Method are mask predictor, and distribution controllers. OtherScientificTerm is hyperparameters. ","This paper proposes a new local explanation method for black-box classifiers. The proposed method is based on the idea of using a mask predictor to predict the distribution of relevance scores for a given classifier, and then using a neural network to compute the classification loss for the mask predictor. The paper shows that the proposed method outperforms other local explanation methods in terms of faithfulness, explainability, and faithfulness to hyperparameters. ","This paper proposes a new local explanation method for black-box classifiers. The proposed method is based on the idea of using a mask predictor to predict the distribution of relevance scores for a given classifier, and then using a neural network to compute the classification loss for the mask predictor. The paper shows that the proposed method outperforms other local explanation methods in terms of faithfulness, explainability, and faithfulness to hyperparameters. "
21654,SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"deep network USED-FOR image reconstruction and classification problems. task - specific network USED-FOR domain specific problem. patches USED-FOR task - specific network. auto - encoder or classifier HYPONYM-OF task - specific network. slack variable USED-FOR top - K selection. method USED-FOR recurring structures. it COMPARE state - of - the - art. state - of - the - art COMPARE it. Task are detection of multiple object instances, and training optimization problem. OtherScientificTerm is supervision. Generic are network, and It. Method are non - differentiable top - K selection process, and multi - stage training. ","This paper proposes a new method for training a task-specific neural network for image classification and reconstruction tasks. The proposed method is based on top-k selection, which is a non-differentiable top-K selection process. The key idea of the method is to learn a slack variable that is used to select a set of patches for each task. The method is evaluated on a number of image classification tasks and is shown to outperform existing methods.","This paper proposes a new method for training a task-specific neural network for image classification and reconstruction tasks. The proposed method is based on top-k selection, which is a non-differentiable top-K selection process. The key idea of the method is to learn a slack variable that is used to select a set of patches for each task. The method is evaluated on a number of image classification tasks and is shown to outperform existing methods."
21663,SP:da1c5f6351d531482e90b86c3cceb52850c520de,assembly code USED-FOR state change. CPU FEATURE-OF state change. RAM FEATURE-OF state change. self - learning reinforcement learning USED-FOR large code space. AutoAssemblet HYPONYM-OF neural program synthesis algorithm. self - learning reinforcement learning USED-FOR AutoAssemblet. Policy networks CONJUNCTION value networks. value networks CONJUNCTION Policy networks. value networks USED-FOR Monte Carlo Tree Search. Policy networks USED-FOR synthesis. value networks USED-FOR synthesis. multi - entropy policy sampling technique USED-FOR online update correlations. AutoAssemblet USED-FOR basic programming tasks. success rates EVALUATE-FOR baselines. AutoAssemblet COMPARE baselines. baselines COMPARE AutoAssemblet. success rates EVALUATE-FOR AutoAssemblet. Method is Neural inductive program synthesis. Task is task generating instructions. ,"This paper proposes AutoAssemblet, a neural program synthesis algorithm that uses self-learning reinforcement learning to solve the problem of program synthesis. The authors propose a multi-entropy policy sampling technique to improve the online update correlations between the policy and the value network. They also propose a Monte Carlo Tree Search algorithm to find the optimal policy for each task. The proposed method is evaluated on a number of tasks and shows better performance compared to baselines.","This paper proposes AutoAssemblet, a neural program synthesis algorithm that uses self-learning reinforcement learning to solve the problem of program synthesis. The authors propose a multi-entropy policy sampling technique to improve the online update correlations between the policy and the value network. They also propose a Monte Carlo Tree Search algorithm to find the optimal policy for each task. The proposed method is evaluated on a number of tasks and shows better performance compared to baselines."
21672,SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"speed of training CONJUNCTION resource requirements. resource requirements CONJUNCTION speed of training. accuracy CONJUNCTION speed of training. speed of training CONJUNCTION accuracy. accuracy EVALUATE-FOR model architecture. model architecture USED-FOR gradient descent optimization. speed of training EVALUATE-FOR gradient descent optimization. speed of training EVALUATE-FOR model architecture. ODE ’s coefficient matrix H USED-FOR convergence rate. first - order ODE USED-FOR gradient descent. analysis technique USED-FOR H. analysis technique USED-FOR model architecture modifications. OtherScientificTerm are neural network architecture design space, network, and model architecture parameters. Generic is architecture. Metric is speed of convergence. ","This paper studies the convergence rate of gradient descent in neural networks. The authors consider the case where the network is trained with a fixed number of parameters, and the optimization problem is formulated as a first-order ODE. They show that the speed of convergence of the gradient descent problem is bounded by the coefficient matrix H, which is defined as the sum of the first order ODE coefficients of the parameters of the network. They then propose a new analysis technique to study the convergence rates of the ODEs in the neural network architecture design space. They also provide a theoretical analysis of the effect of model architecture modifications on the rate of convergence.","This paper studies the convergence rate of gradient descent in neural networks. The authors consider the case where the network is trained with a fixed number of parameters, and the optimization problem is formulated as a first-order ODE. They show that the speed of convergence of the gradient descent problem is bounded by the coefficient matrix H, which is defined as the sum of the first order ODE coefficients of the parameters of the network. They then propose a new analysis technique to study the convergence rates of the ODEs in the neural network architecture design space. They also provide a theoretical analysis of the effect of model architecture modifications on the rate of convergence."
21681,SP:3e3bc8f617df742a395e7d315ec3810a42071294,"overparametrized neural networks ( NNs ) CONJUNCTION kernel methods. kernel methods CONJUNCTION overparametrized neural networks ( NNs ). initialization USED-FOR overparametrized NNs. gradient descent USED-FOR overparametrized NNs. minimum complexity solution USED-FOR interpolating kernel method. squared loss USED-FOR fully - connected wide ReLU - NNs. test error EVALUATE-FOR wide NNs. minimum complexity interpolating kernel methods CONJUNCTION NNs. NNs CONJUNCTION minimum complexity interpolating kernel methods. generalization EVALUATE-FOR initialization scheme. Generic are first, and second. OtherScientificTerm are initialization variance, and generalization bounds. Method is initialization strategies. ","This paper studies the generalization of overparametrized neural networks (NNs) and interpolating kernel methods (Kernels). The authors consider the case of wide ReLU-NNs, where the initialization variance is bounded by the squared loss of the network. The authors show that under certain assumptions, the initialization of wide NNs can be approximated by a kernel method with a minimum complexity of $O(\sqrt{n})$. The authors also show that the initialization error of a wide NN can be bounded by $O(n^{-1/n})$ if the network is fully connected. ","This paper studies the generalization of overparametrized neural networks (NNs) and interpolating kernel methods (Kernels). The authors consider the case of wide ReLU-NNs, where the initialization variance is bounded by the squared loss of the network. The authors show that under certain assumptions, the initialization of wide NNs can be approximated by a kernel method with a minimum complexity of $O(\sqrt{n})$. The authors also show that the initialization error of a wide NN can be bounded by $O(n^{-1/n})$ if the network is fully connected. "
21690,SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"cars CONJUNCTION pedestrians. pedestrians CONJUNCTION cars. Detecting objects USED-FOR autonomous driving. 3D FEATURE-OF pedestrians. 3D FEATURE-OF cars. 3D FEATURE-OF Detecting objects. pedestrians HYPONYM-OF Detecting objects. cars HYPONYM-OF Detecting objects. LiDAR sensors USED-FOR accurate depth information. LiDAR sensors USED-FOR approaches. stereo images USED-FOR pseudo - LiDAR. stereo depth estimation USED-FOR pseudo - LiDAR framework. loss function USED-FOR depth estimation of faraway objects. stereo network architecture CONJUNCTION loss function. loss function CONJUNCTION stereo network architecture. depth estimates USED-FOR depthpropagation algorithm. depth estimation CONJUNCTION stereo - based 3D object detection. stereo - based 3D object detection CONJUNCTION depth estimation. KITTI object detection benchmark EVALUATE-FOR approach. approach USED-FOR depth estimation. detection accuracy USED-FOR faraway objects. approach USED-FOR stereo - based 3D object detection. approach COMPARE detection accuracy. detection accuracy COMPARE approach. OtherScientificTerm are insufficient information, and depth map. Task is 3D detection. ","This paper proposes a pseudo-LiDAR framework for stereo-based 3D object detection. The proposed method is based on the idea of stereo depth estimation, which is an extension of the depthpropagation algorithm for depth estimation of faraway objects. The authors show that the proposed method outperforms the state-of-the-art on the KITTI object detection benchmark. ","This paper proposes a pseudo-LiDAR framework for stereo-based 3D object detection. The proposed method is based on the idea of stereo depth estimation, which is an extension of the depthpropagation algorithm for depth estimation of faraway objects. The authors show that the proposed method outperforms the state-of-the-art on the KITTI object detection benchmark. "
21699,SP:983d84502264633f3385d426c1d4601a0744ea9a,"models USED-FOR sensitive domains. deep neural networks USED-FOR adversarial examples. defense USED-FOR attacks. detecting adversarial samples USED-FOR methods. adversarial example detection method USED-FOR norm - constrained white - box attacks. detector USED-FOR natural data. base detectors USED-FOR K class classification problem. generative approach USED-FOR detecting / classifying adversarial examples. classconditional data USED-FOR unnormalized density model. unnormalized density model USED-FOR base detector. classconditional data USED-FOR base detector. Method are detection mechanism, one - versus - the - rest classification, k - th detector, adversarial example detection / classification methods, and GAT - Generative - Adversarial - Training. OtherScientificTerm is adversarial example. ",This paper proposes a generative adversarial example detection method for white-box attacks. The proposed method is based on the GAT-Generative Adversarial Training (GAT) framework. GAT is an unnormalized density model that is trained on class-conditional data. The authors show that the proposed method can detect adversarial examples in the presence of adversarial samples. They also show that GAT can be used to train a base detector that can be applied to any classifier. ,This paper proposes a generative adversarial example detection method for white-box attacks. The proposed method is based on the GAT-Generative Adversarial Training (GAT) framework. GAT is an unnormalized density model that is trained on class-conditional data. The authors show that the proposed method can detect adversarial examples in the presence of adversarial samples. They also show that GAT can be used to train a base detector that can be applied to any classifier. 
21708,SP:461e9308d050bc3dc7b35233452668bb31f5d491,"Exploration PART-OF model - free reinforcement learning. sparse reward environments FEATURE-OF Exploration. intrinsic rewards USED-FOR exploration. MiniGrid FEATURE-OF procedurally - generated tasks. high - dimensional observations USED-FOR tasks. tasks EVALUATE-FOR method. procedurally - generated tasks EVALUATE-FOR method. approach COMPARE exploration methods. exploration methods COMPARE approach. exploration methods USED-FOR procedurally - generated MiniGrid environments. intrinsic reward FEATURE-OF agent. approaches COMPARE intrinsic reward. intrinsic reward COMPARE approaches. OtherScientificTerm are extrinsic rewards, and procedurally - generated environments. Method is learned state representation. Generic is it. ","This paper proposes a method for model-free reinforcement learning in sparse reward environments. The method is based on the notion of intrinsic reward, which is defined as the difference between the state representation of the agent and the learned state representation. The authors show that the intrinsic reward is a function of the number of observations in the state space, and that it can be used as a reward for exploration. They also show that this intrinsic reward can be combined with other intrinsic rewards to improve the performance of model-based exploration methods. ","This paper proposes a method for model-free reinforcement learning in sparse reward environments. The method is based on the notion of intrinsic reward, which is defined as the difference between the state representation of the agent and the learned state representation. The authors show that the intrinsic reward is a function of the number of observations in the state space, and that it can be used as a reward for exploration. They also show that this intrinsic reward can be combined with other intrinsic rewards to improve the performance of model-based exploration methods. "
21717,SP:c002c20b5e8696588e029c0f65e88860418826c4,"recall EVALUATE-FOR retrieval algorithm. scoring phase COMPARE retrieval phase. retrieval phase COMPARE scoring phase. cross - attention models USED-FOR BERT - style pre - training tasks. sparse handcrafted features USED-FOR models. pre - training tasks USED-FOR embedding - based Transformer model. Transformer models COMPARE BM-25. BM-25 COMPARE Transformer models. Transformer models COMPARE embedding models. embedding models COMPARE Transformer models. BM-25 CONJUNCTION embedding models. embedding models CONJUNCTION BM-25. paragraph - level pre - training tasks EVALUATE-FOR Transformer models. Body First Selection ( BFS ) CONJUNCTION Wiki Link Prediction ( WLP ). Wiki Link Prediction ( WLP ) CONJUNCTION Body First Selection ( BFS ). Inverse Cloze Task ( ICT ) CONJUNCTION Body First Selection ( BFS ). Body First Selection ( BFS ) CONJUNCTION Inverse Cloze Task ( ICT ). Inverse Cloze Task ( ICT ) HYPONYM-OF paragraph - level pre - training tasks. Wiki Link Prediction ( WLP ) HYPONYM-OF paragraph - level pre - training tasks. Body First Selection ( BFS ) HYPONYM-OF paragraph - level pre - training tasks. Task is large - scale query - document retrieval problem. Material is large document corpus. Generic is problem. OtherScientificTerm are solution space, and TF - IDF weights. Method are Information Retrieval ( IR ) methods, and embedding - based retrieval models. ","This paper proposes a Transformer-based retrieval model for BERT-style pre-training tasks. The proposed model is based on a cross-attention model and is trained on a large-scale query-document retrieval problem. The model is evaluated on Wiki Link Prediction (WLP), Inverse Cloze Task (ICT) and Body First Selection (BFS). The model outperforms baselines on all three tasks.","This paper proposes a Transformer-based retrieval model for BERT-style pre-training tasks. The proposed model is based on a cross-attention model and is trained on a large-scale query-document retrieval problem. The model is evaluated on Wiki Link Prediction (WLP), Inverse Cloze Task (ICT) and Body First Selection (BFS). The model outperforms baselines on all three tasks."
21726,SP:4e161e08a624f87633dfb49dfd46bd1665e15189,social graphs CONJUNCTION molecular structures. molecular structures CONJUNCTION social graphs. point clouds CONJUNCTION social graphs. social graphs CONJUNCTION point clouds. Graph neural networks USED-FOR applications. Graph neural networks USED-FOR learning relational representations. learning relational representations CONJUNCTION modeling data on irregular domains. modeling data on irregular domains CONJUNCTION learning relational representations. modeling data on irregular domains HYPONYM-OF applications. molecular structures HYPONYM-OF modeling data on irregular domains. point clouds HYPONYM-OF modeling data on irregular domains. social graphs HYPONYM-OF modeling data on irregular domains. learning relational representations HYPONYM-OF applications. graph convolution operator USED-FOR graph neural network architectures. graph convolution operations CONJUNCTION non - parameterized pooling or expansion layers. non - parameterized pooling or expansion layers CONJUNCTION graph convolution operations. non - parameterized pooling or expansion layers USED-FOR representational hierarchy. graph convolution operations USED-FOR representational hierarchy. parameterized strided and transpose convolution operations CONJUNCTION skip connections. skip connections CONJUNCTION parameterized strided and transpose convolution operations. convolutional network architectures CONJUNCTION parameterized strided and transpose convolution operations. parameterized strided and transpose convolution operations CONJUNCTION convolutional network architectures. bipartite graph convolution operation HYPONYM-OF parameterized transformation. framework USED-FOR multi - graph aggregation. framework COMPARE graph convolution and pooling. graph convolution and pooling COMPARE framework. framework USED-FOR flexible and adaptable network architectures. BiGraphNet HYPONYM-OF flexible and adaptable network architectures. memory requirements FEATURE-OF hierarchical networks. graph convolution USED-FOR hierarchical architectures. graph convolution CONJUNCTION single parametric bipartite graph convolution. single parametric bipartite graph convolution CONJUNCTION graph convolution. graph skip connections CONJUNCTION graph autoencoders. graph autoencoders CONJUNCTION graph skip connections. BiGraphNet formalism ( iii ) USED-FOR architectures. modeling flexibility USED-FOR architectures. BiGraphNet formalism ( iii ) USED-FOR modeling flexibility. graph autoen,"This paper proposes a new graph convolution operator, BiGraphNet, for graph neural networks. The proposed method is based on the idea of multi-graph aggregation, which can be applied to graph convolutions and pooling. The method is evaluated on graph autoencoders, graph skip connections, and graph pooling and expansion layers. The authors show that the proposed method outperforms baselines in terms of performance and memory.","This paper proposes a new graph convolution operator, BiGraphNet, for graph neural networks. The proposed method is based on the idea of multi-graph aggregation, which can be applied to graph convolutions and pooling. The method is evaluated on graph autoencoders, graph skip connections, and graph pooling and expansion layers. The authors show that the proposed method outperforms baselines in terms of performance and memory."
21735,SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"metric function USED-FOR metric - based few - shot classification algorithms. metric function USED-FOR feature embeddings. metric - based methods USED-FOR few - shot classification. domain shifts FEATURE-OF few - shot classification. feature - wise transformation layers USED-FOR image features. affine transforms USED-FOR feature distributions. feature - wise transformation layers USED-FOR feature distributions. affine transforms USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR hyper - parameters. hyper - parameters FEATURE-OF feature - wise transformation layers. learning - to - learn approach USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR feature distributions. Cars CONJUNCTION Places. Places CONJUNCTION Cars. CUB CONJUNCTION Cars. Cars CONJUNCTION CUB. Places CONJUNCTION Plantae. Plantae CONJUNCTION Places. mini - ImageNet CONJUNCTION CUB. CUB CONJUNCTION mini - ImageNet. Plantae HYPONYM-OF few - shot classification datasets. mini - ImageNet HYPONYM-OF few - shot classification datasets. CUB HYPONYM-OF few - shot classification datasets. Places HYPONYM-OF few - shot classification datasets. Cars HYPONYM-OF few - shot classification datasets. feature - wise transformation layer USED-FOR metric - based models. feature - wise transformation layer USED-FOR few - shot classification. domain shift FEATURE-OF few - shot classification. Task are Few - shot classification, and domain generalization setting. Generic is methods. OtherScientificTerm is feature distribution. ",This paper proposes a new metric-based few-shot classification method based on feature-wise transformation layers. The proposed method is based on the affine transformation of the feature embeddings. The key idea is to learn the hyper-parameters of the transformation layers by learning a learning-to-learn approach. The authors show that the proposed method outperforms existing methods on a number of benchmark datasets. ,This paper proposes a new metric-based few-shot classification method based on feature-wise transformation layers. The proposed method is based on the affine transformation of the feature embeddings. The key idea is to learn the hyper-parameters of the transformation layers by learning a learning-to-learn approach. The authors show that the proposed method outperforms existing methods on a number of benchmark datasets. 
21744,SP:df46627cb984a56bba36d510bfc52e00751e9107,approach USED-FOR Lagrangian fluid simulation. convolutional network USED-FOR approach. moving particles USED-FOR fluids. networks USED-FOR moving particles. graph structure USED-FOR particles. N - D convolutions USED-FOR continuous domain. network architecture USED-FOR inverse problems. network architecture USED-FOR arbitrary collision geometries. continuous convolutions COMPARE prior formulations. prior formulations COMPARE continuous convolutions. accuracy CONJUNCTION speed. speed CONJUNCTION accuracy. speed EVALUATE-FOR prior formulations. speed EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR prior formulations. Generic is approaches. Method is spatial convolutions. ,This paper proposes a method for Lagrangian fluid simulation based on convolutional neural networks. The proposed method is based on the N-D convolution and is able to solve the inverse problems in the continuous domain. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and speed. ,This paper proposes a method for Lagrangian fluid simulation based on convolutional neural networks. The proposed method is based on the N-D convolution and is able to solve the inverse problems in the continuous domain. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and speed. 
21753,SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"accuracy CONJUNCTION predictive uncertainty. predictive uncertainty CONJUNCTION accuracy. predictive uncertainty EVALUATE-FOR single neural networks. accuracy EVALUATE-FOR single neural networks. BatchEnsemble1 HYPONYM-OF ensemble method. Hadamard product USED-FOR weight matrix. ensembles COMPARE BatchEnsemble. BatchEnsemble COMPARE ensembles. BatchEnsemble COMPARE ensembles. ensembles COMPARE BatchEnsemble. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. speedup CONJUNCTION memory reduction. memory reduction CONJUNCTION speedup. CIFAR-10 EVALUATE-FOR BatchEnsemble. out - of - distribution tasks EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - CIFAR-100 EVALUATE-FOR BatchEnsemble. BatchEnsemble COMPARE progressive neural networks. progressive neural networks COMPARE BatchEnsemble. computational and memory costs EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - ImageNet USED-FOR lifelong learning. sequential learning tasks PART-OF lifelong learning. Method are Ensembles, and neural networks. Metric is ensemble ’s cost. OtherScientificTerm are mini - batch, and ensemble. ","This paper proposes BatchEnsemble, a new ensemble method for learning multi-class neural networks. The proposed method is based on the Hadamard product of the weight matrix of the ensemble. The authors show that the proposed method outperforms existing ensembles on out-of-distribution (OOD) tasks and lifelong learning tasks. They also show that their method can reduce the computational and memory costs compared to existing ensemble methods. ","This paper proposes BatchEnsemble, a new ensemble method for learning multi-class neural networks. The proposed method is based on the Hadamard product of the weight matrix of the ensemble. The authors show that the proposed method outperforms existing ensembles on out-of-distribution (OOD) tasks and lifelong learning tasks. They also show that their method can reduce the computational and memory costs compared to existing ensemble methods. "
21762,SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"neural network - based partial differential equations solver USED-FOR forward and inverse problems. mesh free and shape free FEATURE-OF solver. neural network USED-FOR solution. explicit smooth differentiable function USED-FOR solution. analytical form FEATURE-OF explicit smooth differentiable function. finite differences CONJUNCTION finite elements. finite elements CONJUNCTION finite differences. finite differences HYPONYM-OF numerical methods. finite elements HYPONYM-OF numerical methods. algorithm USED-FOR forward and inverse problems. Robust boundary conditions constraints CONJUNCTION regularizers. regularizers CONJUNCTION Robust boundary conditions constraints. Electrical Impedance Tomography ( EIT ) CONJUNCTION diffusion and wave equations. diffusion and wave equations CONJUNCTION Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR Electrical Impedance Tomography ( EIT ). method USED-FOR diffusion and wave equations. method USED-FOR Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR diffusion and wave equations. method USED-FOR free shape 2D second order systems. Method is unsupervised approach. Generic are network, framework, and methods. OtherScientificTerm are strong PDE solution, boundary conditions, derivatives of the desired function, and loss function. ","This paper proposes a neural network-based partial differential equations solver for mesh-free and shape-free problems. The proposed method is based on the idea that the solution of a PDE problem can be represented as an explicit smooth differentiable function, which can be expressed as a function of the desired function. The authors show that the proposed method can be used to solve the forward and inverse PDE solvers for mesh free and shape free problems. They also show that their method is able to find a strong PDE solution under boundary conditions and regularizers.","This paper proposes a neural network-based partial differential equations solver for mesh-free and shape-free problems. The proposed method is based on the idea that the solution of a PDE problem can be represented as an explicit smooth differentiable function, which can be expressed as a function of the desired function. The authors show that the proposed method can be used to solve the forward and inverse PDE solvers for mesh free and shape free problems. They also show that their method is able to find a strong PDE solution under boundary conditions and regularizers."
21771,SP:973d0ad0faadcf7298300f2758de9154205e7113,neural networks PART-OF deep learning. Binarized Neural Networks HYPONYM-OF networks. SAT solvers HYPONYM-OF logic - based reasoning tools. tools USED-FOR existential and probabilistic queries. tools USED-FOR explanation generation. existential and probabilistic queries FEATURE-OF network. they USED-FOR logic - based reasoners. training procedure USED-FOR network. network USED-FOR SAT solvers. BNN architecture CONJUNCTION training procedure. training procedure CONJUNCTION BNN architecture. approach COMPARE work. work COMPARE approach. work USED-FOR existential and probabilistic queries. approach USED-FOR existential and probabilistic queries. deep neural networks COMPARE work. work COMPARE deep neural networks. approach USED-FOR deep neural networks. deep neural networks USED-FOR existential and probabilistic queries. OtherScientificTerm is Boolean logic. Generic is methods. Method is BNNs. Metric is accuracy. ,"This paper proposes a new method for solving SAT solvers. The method is based on the Binarized Neural Network (BNN) architecture, which can be used to solve Boolean logic problems. The authors show that the proposed method outperforms existing methods on both existential and probabilistic queries. They also show that their method can be applied to deep neural networks.","This paper proposes a new method for solving SAT solvers. The method is based on the Binarized Neural Network (BNN) architecture, which can be used to solve Boolean logic problems. The authors show that the proposed method outperforms existing methods on both existential and probabilistic queries. They also show that their method can be applied to deep neural networks."
21780,SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"expressive power FEATURE-OF graph neural networks. message - passing framework ( GNNmp ) FEATURE-OF graph neural networks. node attributes CONJUNCTION layer expressiveness. layer expressiveness CONJUNCTION node attributes. technique USED-FOR impossibility statements. approximation USED-FOR tasks. Method are GNNmp, and distributed computing. OtherScientificTerm is lower bounds. Material is graphs. Generic is problems. ","This paper studies the problem of impossibility statements in graph neural networks. The authors propose a new algorithm for computing the impossibility statements. The algorithm is based on the message-passing framework (GNNmp), which is a graph neural network framework. The main contribution of the paper is to derive lower bounds for the impossibility statement of GNNs. The paper also provides a theoretical analysis of the impossibility of the algorithm.","This paper studies the problem of impossibility statements in graph neural networks. The authors propose a new algorithm for computing the impossibility statements. The algorithm is based on the message-passing framework (GNNmp), which is a graph neural network framework. The main contribution of the paper is to derive lower bounds for the impossibility statement of GNNs. The paper also provides a theoretical analysis of the impossibility of the algorithm."
21789,SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"flow - based density models USED-FOR target distributions. complicated topologies FEATURE-OF target distributions. continuous bijections USED-FOR flow - based density models. stacked continuous mixtures of bijections PART-OF LGFs. flow - based methods USED-FOR LGF model. method COMPARE flow - based methods. flow - based methods COMPARE method. normalising flows COMPARE LGFs. LGFs COMPARE normalising flows. density estimation tasks EVALUATE-FOR LGFs. Method are localised generative flows ( LGFs ), and variational scheme. OtherScientificTerm are bijection, and log likelihoods. ","This paper proposes a variational scheme for localised generative flows (LGFs) that can be applied to the problem of density estimation. The proposed method is based on the idea of stacked continuous mixtures of bijections, which can be used to approximate the log likelihoods of the target distributions. The authors show that the proposed method outperforms the state-of-the-art methods on several density estimation tasks. ","This paper proposes a variational scheme for localised generative flows (LGFs) that can be applied to the problem of density estimation. The proposed method is based on the idea of stacked continuous mixtures of bijections, which can be used to approximate the log likelihoods of the target distributions. The authors show that the proposed method outperforms the state-of-the-art methods on several density estimation tasks. "
21798,SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"environment re - splitting CONJUNCTION feature replacement. feature replacement CONJUNCTION environment re - splitting. environment re - splitting USED-FOR diagnosis experiments. feature replacement USED-FOR diagnosis experiments. language CONJUNCTION navigational graph. navigational graph CONJUNCTION language. ResNet features USED-FOR low - level visual appearance. low - level visual information FEATURE-OF semantic representations. features USED-FOR agent. baseline agent model CONJUNCTION training method. training method CONJUNCTION baseline agent model. R4R CONJUNCTION CVDN. CVDN CONJUNCTION R4R. R2R CONJUNCTION R4R. R4R CONJUNCTION R2R. R2R HYPONYM-OF datasets. OtherScientificTerm are naturallanguage instructions, step - by - step navigational instructions, and semantic features. Task is VLN. Method are neural agent models, and agent model. Generic is state - of - the - art models. ","This paper proposes a new dataset for the task of visual navigation. The dataset is composed of a navigational graph, a language, and a navigation instructions. The navigational instructions are given to the agent using a ResNet-based model, and the agent is trained using a baseline agent model. The agent is also trained using an environment re-splitting method. The authors show that the proposed dataset outperforms the baselines on a number of tasks. ","This paper proposes a new dataset for the task of visual navigation. The dataset is composed of a navigational graph, a language, and a navigation instructions. The navigational instructions are given to the agent using a ResNet-based model, and the agent is trained using a baseline agent model. The agent is also trained using an environment re-splitting method. The authors show that the proposed dataset outperforms the baselines on a number of tasks. "
21807,SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"implicit human feedback USED-FOR DRL algorithm. expert labeling CONJUNCTION demonstrations. demonstrations CONJUNCTION expert labeling. agent ’s learning USED-FOR RL tasks. implicit feedback USED-FOR agent ’s learning. system USED-FOR implicit human feedback. implicit human feedback USED-FOR state - action pairs. Atari - type environment FEATURE-OF state - action pairs. error - related event potentials HYPONYM-OF implicit human feedback. auxiliary reward function USED-FOR DRL algorithm. them USED-FOR DRL algorithm. DRL algorithm USED-FOR learning of the game. them USED-FOR auxiliary reward function. electroencephalogram ( EEG ) cap USED-FOR error - potentials. definition USED-FOR game. frameworks USED-FOR error - potential based feedback system. DRL USED-FOR error - potential based feedback system. implicit human feedback USED-FOR complex environments ( games ). synthetic and real user experiments EVALUATE-FOR approach. OtherScientificTerm are human feedback, non - expert humans, human ’s intrinsic reactions, event - related electric potentials, and Atari - games. Generic is paradigm. Method is RL agent. ",This paper proposes an implicit human feedback method for reinforcement learning (RL) that leverages human feedback to improve the agent’s performance in Atari-type environments. The authors propose to use the electroencephalogram (EEG) cap as an auxiliary reward function for learning the state-action pairs in an Atari-like environment. The proposed method is evaluated on a synthetic and real-world Atari environment and is shown to outperform baselines.,This paper proposes an implicit human feedback method for reinforcement learning (RL) that leverages human feedback to improve the agent’s performance in Atari-type environments. The authors propose to use the electroencephalogram (EEG) cap as an auxiliary reward function for learning the state-action pairs in an Atari-like environment. The proposed method is evaluated on a synthetic and real-world Atari environment and is shown to outperform baselines.
21816,SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"laconic classification USED-FOR diverse image classifiers. classifier USED-FOR approximate minimal - entropy positive image. classifier USED-FOR approximate minimal - entropy positive image. colour reduction CONJUNCTION resolution reduction. resolution reduction CONJUNCTION colour reduction. reductions USED-FOR classification. crop CONJUNCTION colour reduction. colour reduction CONJUNCTION crop. crop HYPONYM-OF reductions. resolution reduction HYPONYM-OF reductions. colour reduction HYPONYM-OF reductions. complementary frameworks USED-FOR minimal - entropy positive images. cropping CONJUNCTION reduced colour. reduced colour CONJUNCTION cropping. texture bias FEATURE-OF ILSVRC - trained models. minimal - entropy positive images USED-FOR human and machine classifiers. complementary frameworks USED-FOR human and machine classifiers. reduced resolution FEATURE-OF humans. machines CONJUNCTION reduced resolution. reduced resolution CONJUNCTION machines. cropping USED-FOR machines. reduced colour USED-FOR machines. Metric are entropy, and precision. Material is ILSVRC test - set. Method are machine classifiers, and machine models. ",This paper studies the problem of laconic classification in the context of the ILSVRC test-set. The authors propose two complementary frameworks to improve the performance of the classifier. The first framework is based on the notion of minimal-entropy positive image (MEP) and the second framework uses the concept of reduced colour and reduced resolution. The paper shows that the proposed framework is able to improve performance of both human and machine classifiers. ,This paper studies the problem of laconic classification in the context of the ILSVRC test-set. The authors propose two complementary frameworks to improve the performance of the classifier. The first framework is based on the notion of minimal-entropy positive image (MEP) and the second framework uses the concept of reduced colour and reduced resolution. The paper shows that the proposed framework is able to improve performance of both human and machine classifiers. 
21825,SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"defenses USED-FOR Convolutional Neural Networks. instability assumption USED-FOR defense techniques. deterministic lossy compression algorithms CONJUNCTION randomized perturbations. randomized perturbations CONJUNCTION deterministic lossy compression algorithms. robustness EVALUATE-FOR randomized perturbations. deterministic lossy compression algorithms PART-OF defenses. randomized perturbations PART-OF defenses. Material is adversarial examples. OtherScientificTerm are small perturbations, and decision space. Method is perturbation defenses. ","This paper studies the robustness of adversarial examples to small perturbations in the decision space of convolutional neural networks (CNNs). The authors consider the instability assumption of the adversarial example, which is a well-studied assumption in the literature. The authors show that the instability of the perturbation is bounded by the number of small perturbed examples. They then propose two methods to mitigate this instability: (1) deterministic lossy compression (DLC) and (2) randomized adversarial attacks (RPA). They show that DLC and RPA are more robust than randomized attacks. They also show that randomized attacks are more stable than DLC.","This paper studies the robustness of adversarial examples to small perturbations in the decision space of convolutional neural networks (CNNs). The authors consider the instability assumption of the adversarial example, which is a well-studied assumption in the literature. The authors show that the instability of the perturbation is bounded by the number of small perturbed examples. They then propose two methods to mitigate this instability: (1) deterministic lossy compression (DLC) and (2) randomized adversarial attacks (RPA). They show that DLC and RPA are more robust than randomized attacks. They also show that randomized attacks are more stable than DLC."
21834,SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"view prediction HYPONYM-OF prediction tasks. view prediction USED-FOR 3D visual recognition. moving camera USED-FOR 2.5D ( color and depth ) video streams. 2.5D ( color and depth ) video streams USED-FOR neural 3D mapping networks. contrastive prediction losses COMPARE color regression loss. color regression loss COMPARE contrastive prediction losses. visual representations USED-FOR semi - supervised learning of 3D object detectors. model USED-FOR visual representations. visual representations USED-FOR unsupervised learning of 3D moving object detectors. model USED-FOR unsupervised learning of 3D moving object detectors. semi - supervised learning of 3D object detectors CONJUNCTION unsupervised learning of 3D moving object detectors. unsupervised learning of 3D moving object detectors CONJUNCTION semi - supervised learning of 3D object detectors. videos of dynamic scenes FEATURE-OF motion of the inferred 3D feature maps. scalable self - supervised task USED-FOR 3D object detection. view prediction USED-FOR 3D object detection. view prediction HYPONYM-OF scalable self - supervised task. Method is Predictive coding theories. Generic are task, and them. OtherScientificTerm are perception, retinas, and 3D feature maps. Material is complex photorealistic data. ","This paper proposes a new self-supervised learning task for 3D object detection. The proposed task is based on the view prediction task, where the goal is to predict the 3D feature maps of a scene from a moving camera. The authors propose to use a contrastive loss to improve the performance of the model. They show that the proposed method outperforms the state-of-the-art methods on a number of datasets. ","This paper proposes a new self-supervised learning task for 3D object detection. The proposed task is based on the view prediction task, where the goal is to predict the 3D feature maps of a scene from a moving camera. The authors propose to use a contrastive loss to improve the performance of the model. They show that the proposed method outperforms the state-of-the-art methods on a number of datasets. "
21843,SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"model USED-FOR applications. Optimal Transport ( OT ) framework USED-FOR UDT. low energy transformations FEATURE-OF mappings. methods USED-FOR mappings. theoretical guarantees EVALUATE-FOR methods. approach USED-FOR UDT. dynamic formulation of OT CONJUNCTION CycleGAN. CycleGAN CONJUNCTION dynamic formulation of OT. mapping USED-FOR domain translation. translation USED-FOR problems. image captioning CONJUNCTION natural language translation. natural language translation CONJUNCTION image captioning. neural network USED-FOR mapping. hidden layers PART-OF neural network. photographs CONJUNCTION paintings. paintings CONJUNCTION photographs. model USED-FOR task. dynamical formulation USED-FOR model. Optimal Transport theory USED-FOR UDT models. model COMPARE CycleGAN - like models. CycleGAN - like models COMPARE model. Task are Unsupervised Domain Translation ( UDT ), UDT problems, and UDT problem. OtherScientificTerm are implicit biases, implicit bias, map, unwanted pairings, objective function, and well - behaved mappings. Generic are approaches, and models. Method are CycleGAN model, and networks of minimal complexity. Metric is complexity. ","This paper proposes a dynamic formulation of Optimal Transport (OT) framework for unsupervised domain translation (UDT). The proposed method is based on the CycleGAN framework. The key idea is to use a neural network to learn a mapping between the input and target domain, which is then used as the objective function. The authors show that the proposed method outperforms CycleGAN and CycleGAN-like models on a variety of UDT tasks. ","This paper proposes a dynamic formulation of Optimal Transport (OT) framework for unsupervised domain translation (UDT). The proposed method is based on the CycleGAN framework. The key idea is to use a neural network to learn a mapping between the input and target domain, which is then used as the objective function. The authors show that the proposed method outperforms CycleGAN and CycleGAN-like models on a variety of UDT tasks. "
21852,SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,regularization method USED-FOR neural networks. RotationOut USED-FOR neural networks. RotationOut HYPONYM-OF regularization method. Dropout USED-FOR neuron / channel. Dropout COMPARE RotationOut. RotationOut COMPARE Dropout. convolutional layers CONJUNCTION recurrent layers. recurrent layers CONJUNCTION convolutional layers. RotationOut USED-FOR recurrent layers. RotationOut USED-FOR convolutional layers. RotationOut USED-FOR co - adaptation reduction. Dropout USED-FOR co - adaptation reduction. RotationOut CONJUNCTION Dropout. Dropout CONJUNCTION RotationOut. noise analysis method USED-FOR co - adaptation reduction. RotationOut / Dropout CONJUNCTION Batch Normalization. Batch Normalization CONJUNCTION RotationOut / Dropout. vision and language tasks EVALUATE-FOR method. RotationOut CONJUNCTION RotationOut. RotationOut CONJUNCTION RotationOut. Method is regularization. ,This paper proposes a new regularization method called Rotation out for convolutional neural networks. The proposed method is based on Dropout and RotationOut. The authors show that the proposed method outperforms Dropout in terms of co-adaptation reduction and noise analysis. They also show that rotation out outperforms Batch Normalization and Dropout. ,This paper proposes a new regularization method called Rotation out for convolutional neural networks. The proposed method is based on Dropout and RotationOut. The authors show that the proposed method outperforms Dropout in terms of co-adaptation reduction and noise analysis. They also show that rotation out outperforms Batch Normalization and Dropout. 
21861,SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"method USED-FOR Universal Adversarial Perturbations ( UAP ). Universal Adversarial Perturbations ( UAP ) USED-FOR CNN. sequential optimization USED-FOR adversarial perturbation. dilate loss USED-FOR sequential optimization. dilate loss USED-FOR adversarial perturbation. Euclidean norm EVALUATE-FOR Dilate loss. method COMPARE data - free work. data - free work COMPARE method. fooling rate EVALUATE-FOR data - free work. fooling rate EVALUATE-FOR method. Method is Data - free approaches. Task are crafting adversaries, adversary generation, and crafting UAPs. OtherScientificTerm are nonlinearity, perturbation, ReLU activation function, and limited data cases. ","This paper proposes a novel method for generating adversarial perturbations for CNNs. The method is based on the dilate loss, which is a regularization of the ReLU activation function. The authors show that the proposed method is able to generate adversarial examples that are more fool-hardy than data-free methods. The paper also shows that the method can be applied to adversarial generation. ","This paper proposes a novel method for generating adversarial perturbations for CNNs. The method is based on the dilate loss, which is a regularization of the ReLU activation function. The authors show that the proposed method is able to generate adversarial examples that are more fool-hardy than data-free methods. The paper also shows that the method can be applied to adversarial generation. "
21870,SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"Neural Architecture Search ( NAS ) COMPARE hand - designed networks. hand - designed networks COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR artificial intelligence areas. architecture USED-FOR task. NAS CONJUNCTION fast adaptation of neural architectures. fast adaptation of neural architectures CONJUNCTION NAS. T - NAS HYPONYM-OF Transferable Neural Architecture Search method. meta - learning USED-FOR Transferable Neural Architecture Search method. architecture USED-FOR task. T - NAS USED-FOR meta - architecture. few - shot learning CONJUNCTION supervised learning. supervised learning CONJUNCTION few - shot learning. supervised learning EVALUATE-FOR T - NAS. few - shot learning EVALUATE-FOR T - NAS. Method are NAS methods, and neural architectures. Metric is searching cost. Generic is method. ","This paper proposes a meta-learning method for transferable Neural Architecture Search (NAS). The proposed method is based on the idea of meta-training, where the meta-learner is trained on a set of neural architectures, and the task is to find the best architecture for a given task. The authors show that the proposed method outperforms the state-of-the-art NAS methods in few-shot learning, supervised learning, and supervised learning. ","This paper proposes a meta-learning method for transferable Neural Architecture Search (NAS). The proposed method is based on the idea of meta-training, where the meta-learner is trained on a set of neural architectures, and the task is to find the best architecture for a given task. The authors show that the proposed method outperforms the state-of-the-art NAS methods in few-shot learning, supervised learning, and supervised learning. "
21879,SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"variational information bottleneck ( VIB ) CONJUNCTION noise regularized learning. noise regularized learning CONJUNCTION variational information bottleneck ( VIB ). dropout CONJUNCTION Bayesian neural networks. Bayesian neural networks CONJUNCTION dropout. Bayesian neural networks CONJUNCTION variational information bottleneck ( VIB ). variational information bottleneck ( VIB ) CONJUNCTION Bayesian neural networks. noise regularized learning HYPONYM-OF paradigms. dropout HYPONYM-OF paradigms. Bayesian neural networks HYPONYM-OF paradigms. variational information bottleneck ( VIB ) HYPONYM-OF paradigms. network compression CONJUNCTION robustness. robustness CONJUNCTION network compression. generalization CONJUNCTION network compression. network compression CONJUNCTION generalization. adversarial attack CONJUNCTION label noise. label noise CONJUNCTION adversarial attack. robustness FEATURE-OF adversarial attack. activation uncertainty CONJUNCTION activation variability. activation variability CONJUNCTION activation uncertainty. pruning CONJUNCTION adversarial defense. adversarial defense CONJUNCTION pruning. SNNs COMPARE SE - SNN. SE - SNN COMPARE SNNs. adversarial defense CONJUNCTION learning with label noise. learning with label noise CONJUNCTION adversarial defense. network compression EVALUATE-FOR SE - SNN. adversarial defense USED-FOR network compression. pruning USED-FOR network compression. Method are Stochastic neural networks ( SNNs ), and neural network variants. Generic is networks. Task is discriminative learning. ",This paper studies the robustness of stochastic neural networks (SNNs) against adversarial attacks and noise regularization. The authors show that SNNs with SE-SNN are more robust to label noise and adversarial perturbations. They also show that SE-sNNs can be used for network compression and robustness to pruning.,This paper studies the robustness of stochastic neural networks (SNNs) against adversarial attacks and noise regularization. The authors show that SNNs with SE-SNN are more robust to label noise and adversarial perturbations. They also show that SE-sNNs can be used for network compression and robustness to pruning.
21888,SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"inner loop USED-FOR reinforcement learning. curiosity mechanisms USED-FOR agent ’s reward signal. meta - learning USED-FOR generating curious behavior. reward signal USED-FOR reinforcement learning. reward signal USED-FOR inner loop. transferring neural network weights USED-FOR meta - RL methods. nearest - neighbor modules CONJUNCTION custom loss functions. custom loss functions CONJUNCTION nearest - neighbor modules. buffers CONJUNCTION nearest - neighbor modules. nearest - neighbor modules CONJUNCTION buffers. neural networks PART-OF rich language of programs. custom loss functions PART-OF rich language of programs. image inputs CONJUNCTION acrobot. acrobot CONJUNCTION image inputs. curiosity algorithms COMPARE human - designed published curiosity algorithms. human - designed published curiosity algorithms COMPARE curiosity algorithms. acrobot CONJUNCTION ant. ant CONJUNCTION acrobot. grid navigation CONJUNCTION acrobot. acrobot CONJUNCTION grid navigation. image inputs USED-FOR grid navigation. OtherScientificTerm are curiosity, and outer loop. Method are evolution, and meta - learn algorithms. Material is ML papers. Generic is approach. ","This paper proposes a meta-learning approach for learning curiosity mechanisms in reinforcement learning. The motivation is that curiosity mechanisms can be used to improve the performance of reinforcement learning algorithms. The authors propose to use a neural network to learn the inner loop of a reward function, and then use the reward function to learn a curiosity mechanism. The inner loop is then used to train an agent to generate the curiosity mechanism, and the outer loop is used to learn an agent’s reward function. Experiments show that the proposed method outperforms human-designed curiosity algorithms on a variety of tasks.","This paper proposes a meta-learning approach for learning curiosity mechanisms in reinforcement learning. The motivation is that curiosity mechanisms can be used to improve the performance of reinforcement learning algorithms. The authors propose to use a neural network to learn the inner loop of a reward function, and then use the reward function to learn a curiosity mechanism. The inner loop is then used to train an agent to generate the curiosity mechanism, and the outer loop is used to learn an agent’s reward function. Experiments show that the proposed method outperforms human-designed curiosity algorithms on a variety of tasks."
21897,SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"approach USED-FOR AnyC2C. approach USED-FOR code snippet. strict syntax of programming languages USED-FOR code snippet. tree – structural language modeling ( SLM ) USED-FOR code snippet. strict syntax of programming languages USED-FOR approach. neural model USED-FOR conditional probabilities. AST paths USED-FOR neural model. structural techniques COMPARE approach. approach COMPARE structural techniques. structural techniques USED-FOR expressions. structured approaches USED-FOR Java and C # code. model COMPARE seq2seq. seq2seq COMPARE model. model COMPARE structured approaches. structured approaches COMPARE model. model USED-FOR Java and C # code. seq2seq CONJUNCTION structured approaches. structured approaches CONJUNCTION seq2seq. Generic are problem, it, and task. OtherScientificTerm are structural information, and programming language. Method is SLM. ","This paper proposes a tree-structured language modeling (SLM) approach for AnyC2C. The proposed approach is based on the idea that the code snippet can be represented as a sequence of conditional probabilities, which can be computed by a neural network model. The model is trained on the AST paths of the code, and the conditional probabilities are then used to train the neural network to predict the next code snippet. The approach is evaluated on both Java and C code.","This paper proposes a tree-structured language modeling (SLM) approach for AnyC2C. The proposed approach is based on the idea that the code snippet can be represented as a sequence of conditional probabilities, which can be computed by a neural network model. The model is trained on the AST paths of the code, and the conditional probabilities are then used to train the neural network to predict the next code snippet. The approach is evaluated on both Java and C code."
21906,SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"gradient descent methods USED-FOR non - convex optimization problems. objective functions USED-FOR NNs. NN model space CONJUNCTION canonical space. canonical space CONJUNCTION NN model space. disparity matrix HYPONYM-OF pointwise linear transformation. pointwise linear transformation USED-FOR gradients. gradient descent methods USED-FOR global minimum of zero loss. full rank FEATURE-OF disparity matrices. learning of NNs COMPARE normal convex optimization. normal convex optimization COMPARE learning of NNs. gradient decent algorithms USED-FOR global minimum of zero loss. Method are large - scale neural networks ( NN ), large NNs, and over - parameterized NNs. OtherScientificTerm are canonical model space, full - rank condition, and singular disparity matrices. ","This paper studies the convergence of gradient descent methods for non-convex optimization problems. In particular, the authors consider the problem of learning large-scale neural networks (NNs) in the canonical model space and the over-parameterized NNs. The authors show that the convergence rate of the gradient descent method is bounded by the singular disparity matrix, which is a pointwise linear transformation of the gradients. They also show that gradient descent algorithms can converge to the global minimum of zero loss.","This paper studies the convergence of gradient descent methods for non-convex optimization problems. In particular, the authors consider the problem of learning large-scale neural networks (NNs) in the canonical model space and the over-parameterized NNs. The authors show that the convergence rate of the gradient descent method is bounded by the singular disparity matrix, which is a pointwise linear transformation of the gradients. They also show that gradient descent algorithms can converge to the global minimum of zero loss."
21915,SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"Large - scale ground truth data sets USED-FOR deep learning based segmentation models. interactive graph - based segmentation algorithms USED-FOR connectivity. instanceaware heuristic USED-FOR discrete Potts model. feature maps USED-FOR DCNN. feature maps USED-FOR algorithms. RGB USED-FOR algorithms. PASCAL VOC 2012 CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION PASCAL VOC 2012. Cityscapes dataset EVALUATE-FOR semantic ( and panoptic ) segmentation. PASCAL VOC 2012 EVALUATE-FOR semantic ( and panoptic ) segmentation. VOC validation set EVALUATE-FOR mIoU. mIoU EVALUATE-FOR interactive approach. VOC validation set EVALUATE-FOR interactive approach. They USED-FOR interactive annotation. They USED-FOR weakly supervised learning framework. OtherScientificTerm are global optimum, and scribbles. ",This paper proposes a novel approach for interactive graph-based segmentation based on a discrete Potts model. The proposed approach is based on the instance-aware heuristic. The authors show that the proposed approach can be applied to a wide range of deep learning based segmentation models. The approach is evaluated on the PASCAL VOC dataset and Cityscapes dataset. ,This paper proposes a novel approach for interactive graph-based segmentation based on a discrete Potts model. The proposed approach is based on the instance-aware heuristic. The authors show that the proposed approach can be applied to a wide range of deep learning based segmentation models. The approach is evaluated on the PASCAL VOC dataset and Cityscapes dataset. 
21924,SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"salient features FEATURE-OF image. saliency tools USED-FOR adversarial examples. salient features USED-FOR defense. it COMPARE baseline. baseline COMPARE it. gradient - based saliency tools USED-FOR adversarial defense. model USED-FOR baseline. saliency map USED-FOR baseline. learnt saliency models USED-FOR saliency. computational cost EVALUATE-FOR learnt saliency models. saliency models USED-FOR real - time defense. learnt saliency model USED-FOR defense. adversarial images CONJUNCTION natural images. natural images CONJUNCTION adversarial images. CNN USED-FOR adversarial images. CNN USED-FOR natural images. CNN HYPONYM-OF defense. salient pixels USED-FOR CNN. CIFAR-10 CONJUNCTION ASSIRA. ASSIRA CONJUNCTION CIFAR-10. defense USED-FOR adversarial attacks. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. MNIST EVALUATE-FOR defense. ASSIRA EVALUATE-FOR defense. CIFAR-10 EVALUATE-FOR defense. saliency map USED-FOR defense. C&W CONJUNCTION DeepFool. DeepFool CONJUNCTION C&W. weak defenses USED-FOR adversarial images. attacks USED-FOR adversarial images. DeepFool USED-FOR adversarial images. DeepFool HYPONYM-OF attacks. C&W HYPONYM-OF attacks. OtherScientificTerm are Adversarial perturbations, misclassification, and adversarial perturbations. ",This paper proposes a new adversarial defense method based on gradient-based saliency models. The proposed method is based on the idea of learning a saliency map that maps the salient features of an image to the adversarial examples. The authors show that the proposed method outperforms the baseline in terms of robustness to adversarial perturbations. They also show that their method can be used to defend against adversarial attacks.,This paper proposes a new adversarial defense method based on gradient-based saliency models. The proposed method is based on the idea of learning a saliency map that maps the salient features of an image to the adversarial examples. The authors show that the proposed method outperforms the baseline in terms of robustness to adversarial perturbations. They also show that their method can be used to defend against adversarial attacks.
21933,SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"global adversarial robustness guarantees FEATURE-OF machine learning models. measurability FEATURE-OF local robustness properties. concentration inequalities USED-FOR global robustness. Fashion - MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION Fashion - MNIST. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. neural networks architectures CONJUNCTION training methods. training methods CONJUNCTION neural networks architectures. robustness / accuracy trade - off EVALUATE-FOR neural networks architectures. training methods USED-FOR MNIST. training methods USED-FOR Fashion - MNIST. robustness EVALUATE-FOR networks. accuracy EVALUATE-FOR networks. robustness CONJUNCTION accuracy. accuracy CONJUNCTION robustness. stochastic gradient descent CONJUNCTION iterative pruning techniques. iterative pruning techniques CONJUNCTION stochastic gradient descent. Bayesian settings FEATURE-OF them. iterative pruning techniques USED-FOR networks. stochastic gradient descent USED-FOR networks. Generic are model, and methods. OtherScientificTerm are adversarial attacks, and estimation error. ",This paper studies the global adversarial robustness guarantees of machine learning models. The authors show that the global robustness of a neural network is guaranteed by concentration inequalities. They also show that this guarantee holds for Bayesian settings. The paper also provides a theoretical analysis of the trade-off between robustness and accuracy. ,This paper studies the global adversarial robustness guarantees of machine learning models. The authors show that the global robustness of a neural network is guaranteed by concentration inequalities. They also show that this guarantee holds for Bayesian settings. The paper also provides a theoretical analysis of the trade-off between robustness and accuracy. 
21942,SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"robustness FEATURE-OF environmental dynamics. learning algorithms USED-FOR robustness. robustness FEATURE-OF system dynamics. transition probability HYPONYM-OF system dynamics. Wasserstein distance USED-FOR disturbance. infinite - dimensional optimization problem CONJUNCTION finite - dimensional risk - aware problem. finite - dimensional risk - aware problem CONJUNCTION infinite - dimensional optimization problem. risk - aware optimal Bellman equation USED-FOR optimal robust policies. sensitivity analysis USED-FOR perturbations. Wasserstein Robust HYPONYM-OF robust learning algorithm. Cart - Pole environment EVALUATE-FOR algorithm. OtherScientificTerm are optimal policy, simulated environmental parameters, reference transition kernel, transition kernel disturbance, and state disturbance. ","This paper studies the problem of learning a robust policy that is robust to perturbations in the environment. The authors consider the case where the transition probability of the system is bounded by the Wasserstein distance between the state and the transition kernel, and the optimal Bellman equation is a risk-aware optimization problem. In this setting, the authors propose an algorithm that learns a robust learning algorithm based on the risk minimization of the Bellman Equation. They show that the proposed algorithm is able to learn robust policies in the Cart-Pole environment. ","This paper studies the problem of learning a robust policy that is robust to perturbations in the environment. The authors consider the case where the transition probability of the system is bounded by the Wasserstein distance between the state and the transition kernel, and the optimal Bellman equation is a risk-aware optimization problem. In this setting, the authors propose an algorithm that learns a robust learning algorithm based on the risk minimization of the Bellman Equation. They show that the proposed algorithm is able to learn robust policies in the Cart-Pole environment. "
21951,SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"Nash equilibrium PART-OF multi - player games. deep learning based approaches USED-FOR pure strategy Nash equilibrium. method USED-FOR mixed strategy Nash equilibria. multi - player continuous games FEATURE-OF mixed strategy Nash equilibria. pure ones PART-OF method. pushforward measure technique USED-FOR mixed strategy. continuous spaces FEATURE-OF mixed strategy. joint strategy profile CONJUNCTION Nash equilibrium. Nash equilibrium CONJUNCTION joint strategy profile. gradient descent algorithm USED-FOR approach. convexity assumption FEATURE-OF payoff functions. approach USED-FOR stationary Nash equilibrium. blotto games CONJUNCTION GAMUT games. GAMUT games CONJUNCTION blotto games. method COMPARE works. works COMPARE method. quadratic games CONJUNCTION blotto games. blotto games CONJUNCTION quadratic games. works USED-FOR Nash equilibrium. method USED-FOR Nash equilibrium. approximating Nash equilibrium USED-FOR quadratic games. approximating Nash equilibrium CONJUNCTION blotto games. blotto games CONJUNCTION approximating Nash equilibrium. GAMUT games EVALUATE-FOR method. OtherScientificTerm are continuous strategy spaces, and pure strategy weakness. Task is generative adversarial networks. Generic is equilibrium. ",This paper studies the problem of finding Nash equilibria in multi-player continuous games. The authors propose a method for finding mixed strategy Nash equilibrium in continuous spaces. The main contribution of the paper is the use of a pushforward measure technique to approximate the Nash equilibrium. The paper also proposes a gradient descent algorithm to find the stationary Nash equilibrium under convexity assumption. Experiments on quadratic games and GAMUT games demonstrate the effectiveness of the proposed method. ,This paper studies the problem of finding Nash equilibria in multi-player continuous games. The authors propose a method for finding mixed strategy Nash equilibrium in continuous spaces. The main contribution of the paper is the use of a pushforward measure technique to approximate the Nash equilibrium. The paper also proposes a gradient descent algorithm to find the stationary Nash equilibrium under convexity assumption. Experiments on quadratic games and GAMUT games demonstrate the effectiveness of the proposed method. 
21960,SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"deep neural networks USED-FOR NLP tasks. labeled data USED-FOR data - hungry models. sufficient domain knowledge USED-FOR labeled data. supervision USED-FOR sufficient domain knowledge. supervision FEATURE-OF Natural language ( NL ) explanations. them USED-FOR augmenting model learning. modularized model USED-FOR semantics. linguistic variants FEATURE-OF NL explanations. Neural Execution Tree ( NExT ) framework1 USED-FOR text classification. NL explanations USED-FOR Neural Execution Tree ( NExT ) framework1. NExT USED-FOR actions. NL explanations USED-FOR executable logical forms. logical forms USED-FOR actions. semantic parsing USED-FOR NL explanations. semantic parsing USED-FOR executable logical forms. relation extraction CONJUNCTION sentiment analysis. sentiment analysis CONJUNCTION relation extraction. NLP tasks EVALUATE-FOR baseline methods. sentiment analysis HYPONYM-OF NLP tasks. relation extraction HYPONYM-OF NLP tasks. Task are data annotation, and multi - hop question answering. Metric is annotation time. Method is model learning. OtherScientificTerm is NL explanation. ","This paper proposes a framework for learning natural language explanations (NL explanations) for NLP tasks. The authors propose a modularized model for NL explanations that can be used to augment the model learning. The proposed framework is based on the Neural Execution Tree (NExT) framework, which is used for text classification and multi-hop question answering tasks. NExT is trained to learn logical forms of NL explanations, which are then used to train a neural network to predict the next logical form of a given NL explanation. Experiments show that the proposed method is able to outperform baselines on several tasks. ","This paper proposes a framework for learning natural language explanations (NL explanations) for NLP tasks. The authors propose a modularized model for NL explanations that can be used to augment the model learning. The proposed framework is based on the Neural Execution Tree (NExT) framework, which is used for text classification and multi-hop question answering tasks. NExT is trained to learn logical forms of NL explanations, which are then used to train a neural network to predict the next logical form of a given NL explanation. Experiments show that the proposed method is able to outperform baselines on several tasks. "
21969,SP:a9b5f7257dedd719cfe341fca275776734af1d98,"robustness FEATURE-OF misclassification. machine learning models USED-FOR Formal verification. robustness HYPONYM-OF properties. it USED-FOR complex specifications. it USED-FOR recurrent neural network architectures. recurrent neural network architectures CONJUNCTION complex specifications. complex specifications CONJUNCTION recurrent neural network architectures. specifications USED-FOR temporal properties. adversarial robustness FEATURE-OF complex specifications. it USED-FOR verified training. specifications HYPONYM-OF complex specifications. verified training method USED-FOR models. verified training method USED-FOR models. training USED-FOR models. OtherScientificTerm are perturbations of the input features, and desired specifications. Method are verification procedure, verifiably robust models, and language model. ","This paper studies the problem of verifying the robustness of machine learning models to perturbations of the input features. The authors propose a verification procedure to verify the verifiability of a language model. The verification procedure is based on the fact that the training of the language model can be performed in an adversarial setting. The method is evaluated on a variety of tasks, and is shown to be robust to adversarial attacks. ","This paper studies the problem of verifying the robustness of machine learning models to perturbations of the input features. The authors propose a verification procedure to verify the verifiability of a language model. The verification procedure is based on the fact that the training of the language model can be performed in an adversarial setting. The method is evaluated on a variety of tasks, and is shown to be robust to adversarial attacks. "
21978,SP:3903680e07b676409e3cf6a1044b67291fe38630,"learned state representations USED-FOR constant. technique COMPARE domain randomization. domain randomization COMPARE technique. generalization scores EVALUATE-FOR domain randomization. generalization scores EVALUATE-FOR technique. Task are reinforcement learning, and visual domain randomization problem. Generic is method. Method are visual domain randomization, and regularization method. OtherScientificTerm are policies, and randomization parameters. ",This paper studies the problem of visual domain randomization in reinforcement learning. The authors propose a new regularization method for the problem. The proposed method is based on the idea that the learned state representations of the policy should be used as a constant for the randomization parameters. The paper shows that the proposed method outperforms the state-of-the-art methods in terms of generalization. ,This paper studies the problem of visual domain randomization in reinforcement learning. The authors propose a new regularization method for the problem. The proposed method is based on the idea that the learned state representations of the policy should be used as a constant for the randomization parameters. The paper shows that the proposed method outperforms the state-of-the-art methods in terms of generalization. 
21987,SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"deep learning USED-FOR Deep metric learning ( DML ). complicated losses CONJUNCTION hard example mining methods. hard example mining methods CONJUNCTION complicated losses. pairwise binary classification problem USED-FOR DML. framework USED-FOR model. distributionally robust optimization USED-FOR robust loss. uncertainty decision set FEATURE-OF dual variable. uncertainty decision set USED-FOR complicated losses. benchmark data sets EVALUATE-FOR method. method COMPARE state. state COMPARE method. Task is computer vision. Generic are problem, and variants. OtherScientificTerm is imbalanced data pairs. ",This paper proposes a new framework for deep metric learning (DML) based on the pairwise binary classification problem. The proposed framework is based on distributionally robust optimization (DQN) and the uncertainty decision set (UDS) framework. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on several benchmark datasets. ,This paper proposes a new framework for deep metric learning (DML) based on the pairwise binary classification problem. The proposed framework is based on distributionally robust optimization (DQN) and the uncertainty decision set (UDS) framework. The authors show that the proposed method is able to achieve better performance than the state-of-the-art on several benchmark datasets. 
21996,SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"local minimum PART-OF non - convex finite - sum minimization. inexact gradient and Hessian estimation USED-FOR trust region method. stochastic trust region ( STR ) algorithm USED-FOR (, √ ) -approximate local minimum. runtime complexity EVALUATE-FOR Hessian - free STR algorithms. Metric is convergence rate. Method are differential estimations, and Hessian estimator. OtherScientificTerm is stochastic Hessian oracle queries. Generic is algorithms. ",This paper studies the problem of finding the local minimum of a non-convex finite-sum minimization problem. The authors propose a stochastic trust region (STR) algorithm to solve the problem. They show that the algorithm converges to a local minimum that is $\epsilon$-approximate to the Hessian estimator. ,This paper studies the problem of finding the local minimum of a non-convex finite-sum minimization problem. The authors propose a stochastic trust region (STR) algorithm to solve the problem. They show that the algorithm converges to a local minimum that is $\epsilon$-approximate to the Hessian estimator. 
22005,SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"batch normalization CONJUNCTION weight initialization. weight initialization CONJUNCTION batch normalization. linear programming USED-FOR Farkas layers. benchmark datasets EVALUATE-FOR network sizes. ReLU activation USED-FOR residual networks. Method are deep neural networks, and geometrically motivated method. Generic is method. Metric is training capacity. OtherScientificTerm is initialization. ",This paper proposes a geometrically motivated method for training deep neural networks. The proposed method is based on linear programming. The authors show that linear programming can be used to train a deep neural network with a large number of Farkas layers. They also show that the proposed method can be applied to the case where the number of layers is larger than the training capacity of the network. ,This paper proposes a geometrically motivated method for training deep neural networks. The proposed method is based on linear programming. The authors show that linear programming can be used to train a deep neural network with a large number of Farkas layers. They also show that the proposed method can be applied to the case where the number of layers is larger than the training capacity of the network. 
22014,SP:1d325b148e3efe407241c1f1cbe8d17400499741,"decision boundary PART-OF classifier. adversarial examples FEATURE-OF robustness certificate. minimum distance FEATURE-OF robustness certificate. robustness certificates USED-FOR deep classifiers. nonconvex optimization USED-FOR it. computationally - efficient robustness certificates USED-FOR deep classifiers. differentiable activation functions USED-FOR computationally - efficient robustness certificates. eigenvalues FEATURE-OF Hessian. Hessian FEATURE-OF network. l2 norm FEATURE-OF robustness certificate. convex optimization USED-FOR robustness certificate. curvature FEATURE-OF deep network. computationallyefficient differentiable upper bound USED-FOR deep network. curvature FEATURE-OF computationallyefficient differentiable upper bound. curvature bound USED-FOR regularization term. regularization term USED-FOR network. curvature bound USED-FOR network. adversarial examples FEATURE-OF certified robustness. Curvature - based Robustness Certificate ( CRC ) CONJUNCTION Curvature - based Robust Training ( CRT ). Curvature - based Robust Training ( CRT ) CONJUNCTION Curvature - based Robustness Certificate ( CRC ). CRT COMPARE adversarial training. adversarial training COMPARE CRT. CRC COMPARE CROWN ’s certificate. CROWN ’s certificate COMPARE CRC. certified accuracy EVALUATE-FOR adversarial training. CRC COMPARE CRT. CRT COMPARE CRC. CRC COMPARE adversarial training. adversarial training COMPARE CRC. regularizer USED-FOR CRC. regularizer USED-FOR CROWN ’s certificate. certified accuracy EVALUATE-FOR CRC. certified accuracy EVALUATE-FOR CRT. OtherScientificTerm are lower bound, and classification output. ",This paper studies the problem of computing robustness certificates for deep neural networks. The authors consider the case where the decision boundary of a classifier is a convex function of the eigenvalues of the Hessian of the classifier. They show that the robustness certificate can be computed in a non-convex optimization problem. They then propose two differentiable activation functions that are computationally efficient to compute. ,This paper studies the problem of computing robustness certificates for deep neural networks. The authors consider the case where the decision boundary of a classifier is a convex function of the eigenvalues of the Hessian of the classifier. They show that the robustness certificate can be computed in a non-convex optimization problem. They then propose two differentiable activation functions that are computationally efficient to compute. 
22023,SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"method USED-FOR compressed sensing recovery. untrained deep generative models USED-FOR method. convolutional weights PART-OF network. Deep Image Prior ( DIP ) USED-FOR method. approach USED-FOR differentiable linear inverse problem. approaches COMPARE method. method COMPARE approaches. generative models USED-FOR approaches. prior information FEATURE-OF network weights. prior information PART-OF learned regularization technique. DIP optimization approach USED-FOR overparameterized single - layer networks. Method are unlearned methods, and early stopping. OtherScientificTerm are pre - training, and noisy measurements. Metric is reconstruction error. Task is fitting problem. ",This paper proposes a new method for compressed sensing recovery based on Deep Image Prior (DIP) optimization. The proposed method is based on a learned regularization technique that learns the prior information of the network weights. The authors show that the proposed method outperforms the baselines in terms of reconstruction error and compression. They also show that their method can be applied to over-parameterized single-layer networks.,This paper proposes a new method for compressed sensing recovery based on Deep Image Prior (DIP) optimization. The proposed method is based on a learned regularization technique that learns the prior information of the network weights. The authors show that the proposed method outperforms the baselines in terms of reconstruction error and compression. They also show that their method can be applied to over-parameterized single-layer networks.
22032,SP:23c0f621e6041003b59bf0532130760694cf6a4a,"reinforcement learning ( RL ) USED-FOR real - world problems. long time horizons FEATURE-OF action - reward correlation. hand - tuned network structure CONJUNCTION pre - defined subgoals. pre - defined subgoals CONJUNCTION hand - tuned network structure. hand - tuned network structure PART-OF hierarchies. pre - defined subgoals PART-OF hierarchies. HRL framework USED-FOR temporal abstraction. TAIC USED-FOR temporal abstraction. approach USED-FOR latent space. information - theoretic constraints USED-FOR approach. latent representations of action sequences USED-FOR temporal abstraction problem. latent variables CONJUNCTION state changes. state changes CONJUNCTION latent variables. algorithm USED-FOR abstraction of the long action sequences. abstraction USED-FOR tasks. convergence rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION convergence rate. sample efficiency EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR technique. sample efficiency EVALUATE-FOR technique. Method are Hierarchical reinforcement learning ( HRL ) methods, and temporal abstractions. OtherScientificTerm are task - specific knowledge, and visualization of the latent space. Metric is mutual information. Material is benchmark learning problems. ","This paper proposes a Hierarchical Reinforcement Learning (HRL) framework for temporal abstraction of the long-horizon action-reward correlation problem. The proposed method is based on the TAIC framework, which uses information-theoretic constraints on the latent space of the action sequences. The authors show that the proposed method can achieve better sample efficiency and sample efficiency compared to existing HRL algorithms. ","This paper proposes a Hierarchical Reinforcement Learning (HRL) framework for temporal abstraction of the long-horizon action-reward correlation problem. The proposed method is based on the TAIC framework, which uses information-theoretic constraints on the latent space of the action sequences. The authors show that the proposed method can achieve better sample efficiency and sample efficiency compared to existing HRL algorithms. "
22041,SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"Graph Convolutional Network ( GCN ) USED-FOR graph representation learning. small graphs USED-FOR shallow models. acceleration methods USED-FOR GCNs. larger graphs CONJUNCTION deeper layers. deeper layers CONJUNCTION larger graphs. GCN - like models USED-FOR deeper layers. GCN - like models USED-FOR larger graphs. local bi - directional influence ( correlation ) FEATURE-OF mini - batch of nodes. recursive propagation CONJUNCTION skip connection. skip connection CONJUNCTION recursive propagation. first - order and higher - order proximities FEATURE-OF single layer propagation process. first - order and higher - order proximities PART-OF model. large benchmark graphs EVALUATE-FOR model. Task is graph - based applications. Method are layer - wise sampling strategy, and self - attention mechanism. Metric is time complexity. OtherScientificTerm is sampled nodes. ",This paper proposes a new sampling strategy for graph convolutional networks (GCNs). The proposed sampling strategy is based on local bi-directional influence (LID) and local bi directional influence (ID). The authors show that the proposed method is able to achieve better performance than existing GCN-based methods in terms of time complexity. The authors also propose a self-attention mechanism to improve the performance of GCNs. ,This paper proposes a new sampling strategy for graph convolutional networks (GCNs). The proposed sampling strategy is based on local bi-directional influence (LID) and local bi directional influence (ID). The authors show that the proposed method is able to achieve better performance than existing GCN-based methods in terms of time complexity. The authors also propose a self-attention mechanism to improve the performance of GCNs. 
22050,SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,velocities CONJUNCTION interactions. interactions CONJUNCTION velocities. STOVE HYPONYM-OF state - space model. state - space model USED-FOR videos. image model CONJUNCTION dynamics model. dynamics model CONJUNCTION image model. dynamics model USED-FOR inference. image model USED-FOR It. dynamics model USED-FOR It. STOVE COMPARE unsupervised models. unsupervised models COMPARE STOVE. STOVE COMPARE supervised baselines. supervised baselines COMPARE STOVE. unsupervised models COMPARE supervised baselines. supervised baselines COMPARE unsupervised models. model USED-FOR model - based control. Method is physical system. Generic is models. Task is regularizing training. OtherScientificTerm is physical behavior. ,This paper proposes a new state-space model called STOVE for video-based control. The model is based on an image-based model and a dynamics model. The dynamics model is used for inference and the image model for training. The authors show that the proposed model outperforms unsupervised and supervised baselines.,This paper proposes a new state-space model called STOVE for video-based control. The model is based on an image-based model and a dynamics model. The dynamics model is used for inference and the image model for training. The authors show that the proposed model outperforms unsupervised and supervised baselines.
22059,SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). variational autoencoders ( VAE ) PART-OF autoencoding model. model USED-FOR λ - Jeffreys divergence. Gaussian CONJUNCTION Laplace. Laplace CONJUNCTION Gaussian. Laplace HYPONYM-OF explicit likelihood. Gaussian HYPONYM-OF explicit likelihood. approach USED-FOR VAE model. implicit likelihood USED-FOR approach. adversarially trained discriminator USED-FOR implicit likelihood. adversarially trained discriminator USED-FOR approach. implicit likelihood USED-FOR VAE model. mode - seeking CONJUNCTION mass - covering behaviour. mass - covering behaviour CONJUNCTION mode - seeking. mode - seeking FEATURE-OF model. CIFAR-10 and TinyImagent datasets EVALUATE-FOR model. mass - covering behaviour FEATURE-OF model. Method are GAN, VAE, and adversarial training. OtherScientificTerm are mode collapsing problem, model distribution, and VAE loss. Generic are it, It, and objective. ","This paper studies the mode collapsing problem in variational autoencoders (VAE) and generative adversarial networks (GANs). The authors propose a novel adversarial training method for VAE models. The proposed method is based on the notion of implicit likelihood, which is a special case of the Laplace divergence between the model distribution and the target distribution. The authors show that the proposed method outperforms existing methods on CIFAR-10 and TinyImagent datasets. ","This paper studies the mode collapsing problem in variational autoencoders (VAE) and generative adversarial networks (GANs). The authors propose a novel adversarial training method for VAE models. The proposed method is based on the notion of implicit likelihood, which is a special case of the Laplace divergence between the model distribution and the target distribution. The authors show that the proposed method outperforms existing methods on CIFAR-10 and TinyImagent datasets. "
22068,SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"Adversarial attacks FEATURE-OF CNN classifiers. unreasonably linear extrapolation USED-FOR CNNs. attacks USED-FOR Bayes - Optimal classifier. Bayes - Optimal classifier USED-FOR class distributions. optimal classifier USED-FOR attacks. smooth decision surface FEATURE-OF classifier. datasets EVALUATE-FOR optimal classifier. datasets USED-FOR Bayes - Optimal classifier. adversarial examples USED-FOR it. large - margin methods USED-FOR classifier. machine learning USED-FOR adversarial vulnerability. Task is classification. OtherScientificTerm are geometry of high dimensions, data distribution, optimal decision boundary, and low dimensions. Material is digits. Method are CNN training, and suboptimal training methods. ","This paper studies adversarial vulnerability of Bayes-optimal classifiers. The authors show that the optimal classifier is vulnerable to adversarial examples in the low-dimensional space. They also show that under certain assumptions on the decision boundary of the classifier, the optimal decision boundary is also vulnerable to attacks. ","This paper studies adversarial vulnerability of Bayes-optimal classifiers. The authors show that the optimal classifier is vulnerable to adversarial examples in the low-dimensional space. They also show that under certain assumptions on the decision boundary of the classifier, the optimal decision boundary is also vulnerable to attacks. "
22077,SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"top-1 accuracy EVALUATE-FOR sparse and non - sparse models. Method are Neural network pruning techniques, network, pruning, abstract representations, and fine - grained classification. Metric are top1 test set accuracy, and image quality. OtherScientificTerm are pruning identified exemplars ( PIEs ), and sparsity. Material are PIE images, and hard - to - generalize - to images. ","This paper studies the problem of pruning identified exemplars (PIEs) in neural network pruning. The authors propose two pruning techniques for sparse and non-sparse models. The first pruning method prunes the top-1 accuracy of the model, while the second pruning technique prunes top-2 accuracy. They show that both pruning methods are effective in reducing the sparsity of PIEs. ","This paper studies the problem of pruning identified exemplars (PIEs) in neural network pruning. The authors propose two pruning techniques for sparse and non-sparse models. The first pruning method prunes the top-1 accuracy of the model, while the second pruning technique prunes top-2 accuracy. They show that both pruning methods are effective in reducing the sparsity of PIEs. "
22086,SP:4b17edaa7ec6201891433320d85f9a415656b763,"Interactive Fiction games HYPONYM-OF text - based simulations. natural language understanding CONJUNCTION partial observability. partial observability CONJUNCTION natural language understanding. reinforcement learning agents USED-FOR natural language understanding. partial observability CONJUNCTION action generation. action generation CONJUNCTION partial observability. combinatorially - large text - based action spaces FEATURE-OF action generation. template - based action space USED-FOR KG - A2C1. knowledge graph USED-FOR game state. knowledge graph USED-FOR natural language generation. KG - A2C COMPARE IF agents. IF agents COMPARE KG - A2C. IF games EVALUATE-FOR KG - A2C. OtherScientificTerm are natural language, dynamic knowledge graph, combinatorially large natural language actions, and action space size. Generic is They. ","This paper proposes a novel reinforcement learning agent for interactive fiction games. The agent learns a dynamic knowledge graph, which is used to learn a template-based action space and a knowledge graph for the game state. The authors show that the learned knowledge graph can be used to improve the performance of reinforcement learning agents in the context of text-based games. They also show that KG-A2C1 outperforms the state-of-the-art RL agents in a number of games.","This paper proposes a novel reinforcement learning agent for interactive fiction games. The agent learns a dynamic knowledge graph, which is used to learn a template-based action space and a knowledge graph for the game state. The authors show that the learned knowledge graph can be used to improve the performance of reinforcement learning agents in the context of text-based games. They also show that KG-A2C1 outperforms the state-of-the-art RL agents in a number of games."
22095,SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"language generation HYPONYM-OF sequence prediction problems. maximum likelihood estimation ( MLE ) USED-FOR sequence prediction problems. data - dependent Gaussian prior CONJUNCTION detailed training prediction. detailed training prediction CONJUNCTION data - dependent Gaussian prior. data - dependent Gaussian prior USED-FOR Kullback – Leibler divergence term. text summarization CONJUNCTION storytelling. storytelling CONJUNCTION text summarization. supervised and unsupervised machine translation CONJUNCTION text summarization. text summarization CONJUNCTION supervised and unsupervised machine translation. storytelling CONJUNCTION image captioning. image captioning CONJUNCTION storytelling. language generation tasks EVALUATE-FOR method. image captioning HYPONYM-OF language generation tasks. supervised and unsupervised machine translation HYPONYM-OF language generation tasks. text summarization HYPONYM-OF language generation tasks. storytelling HYPONYM-OF language generation tasks. Generic is it. Method is MLE. OtherScientificTerm are negative diversity ignorance, and prior topological order of tokens. Metric is MLE loss. ",This paper studies the problem of maximum likelihood estimation (MLE) for sequence prediction problems. The authors propose a new loss function based on the Kullback-Leibler divergence term. The proposed loss function is based on a data-dependent Gaussian prior and a detailed training prediction. The paper shows that the proposed loss is more robust to negative diversity ignorance than the standard MLE loss. ,This paper studies the problem of maximum likelihood estimation (MLE) for sequence prediction problems. The authors propose a new loss function based on the Kullback-Leibler divergence term. The proposed loss function is based on a data-dependent Gaussian prior and a detailed training prediction. The paper shows that the proposed loss is more robust to negative diversity ignorance than the standard MLE loss. 
22104,SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"Temperature scaling USED-FOR DNN. Temperature scaling HYPONYM-OF calibration approach. cross - entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross - entropy loss. focal loss USED-FOR models. temperature scaling CONJUNCTION focal loss. focal loss CONJUNCTION temperature scaling. confidence FEATURE-OF model. focal loss USED-FOR calibrated models. accuracy EVALUATE-FOR focal loss. network architectures USED-FOR approach. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. accuracy EVALUATE-FOR approach. calibration EVALUATE-FOR approach. Task are Miscalibration, downstream tasks, and miscalibration. Method is Deep Neural Networks ( DNNs ). Generic is networks. Material is NLP ( SST, 20 Newsgroup ) datasets. ",This paper proposes a new calibration method for deep neural networks. The proposed method is based on temperature scaling and focal loss. The authors show that the proposed method can improve the calibration performance of DNNs. ,This paper proposes a new calibration method for deep neural networks. The proposed method is based on temperature scaling and focal loss. The authors show that the proposed method can improve the calibration performance of DNNs. 
22113,SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,Lipschitz constant FEATURE-OF neural networks. LiPopt HYPONYM-OF polynomial optimization framework. sparse connectivity USED-FOR network. sparse connectivity USED-FOR complexity of computation. approach COMPARE baselines. baselines COMPARE approach. networks CONJUNCTION networks. networks CONJUNCTION networks. ` ∞-Lipschitz constant EVALUATE-FOR approach. random weights CONJUNCTION networks. networks CONJUNCTION random weights. random weights USED-FOR networks. MNIST USED-FOR networks. Task is optimization problems. Method is convolutional as well as pruned neural networks. ,This paper proposes a new polynomial optimization framework for neural networks. The main idea is to use sparse connectivity to reduce the complexity of computation of the optimization problem. The authors show that this approach can be applied to both convolutional as well as pruned neural networks and random weights. The paper also shows that the proposed method can be used to improve the performance of pruned networks.,This paper proposes a new polynomial optimization framework for neural networks. The main idea is to use sparse connectivity to reduce the complexity of computation of the optimization problem. The authors show that this approach can be applied to both convolutional as well as pruned neural networks and random weights. The paper also shows that the proposed method can be used to improve the performance of pruned networks.
22122,SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,self - supervised learning approach USED-FOR video features. video classification CONJUNCTION captioning and segmentation. captioning and segmentation CONJUNCTION video classification. tasks EVALUATE-FOR methods. self - supervised learning approach USED-FOR tasks. self - supervised learning approach COMPARE methods. methods COMPARE self - supervised learning approach. captioning and segmentation HYPONYM-OF tasks. video classification HYPONYM-OF tasks. BERT model USED-FOR text sequences. softmax loss CONJUNCTION noise contrastive estimation ( NCE ). noise contrastive estimation ( NCE ) CONJUNCTION softmax loss. sequences of real - valued feature vectors USED-FOR method. BERT model USED-FOR method. sequences of visual features CONJUNCTION sequences of words. sequences of words CONJUNCTION sequences of visual features. automatic speech recognition USED-FOR sequences of words. sequences of words USED-FOR representations. sequences of visual features USED-FOR representations. Method is cross - modal training. ,This paper proposes a self-supervised learning approach for video features. The proposed method is based on cross-modal training. The method is evaluated on video classification and segmentation tasks.,This paper proposes a self-supervised learning approach for video features. The proposed method is based on cross-modal training. The method is evaluated on video classification and segmentation tasks.
22131,SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"selection masks CONJUNCTION neural network. neural network CONJUNCTION selection masks. Task are inference phase, and data transfer. OtherScientificTerm is public storage server. Method is machine learning models. Generic are framework, model, and masks. ","This paper proposes a new framework for data transfer in the inference phase of machine learning models. The proposed framework is based on the idea of public storage server, which is used to store the training data of a model on a public server. The model is trained on the public server and then transferred to a private server, where the model is updated with the new data. The authors show that the proposed method is able to transfer the model to the private server without the need for masking. ","This paper proposes a new framework for data transfer in the inference phase of machine learning models. The proposed framework is based on the idea of public storage server, which is used to store the training data of a model on a public server. The model is trained on the public server and then transferred to a private server, where the model is updated with the new data. The authors show that the proposed method is able to transfer the model to the private server without the need for masking. "
22140,SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"Deep neural networks USED-FOR classification tasks. methodology USED-FOR neural network. it USED-FOR outof - distribution ( OOD ) examples. Outlier Exposure ( OE ) technique USED-FOR loss function. loss function USED-FOR out - of - distribution detection. image and text classification tasks EVALUATE-FOR out - of - distribution detection. OE USED-FOR loss function. image and text classification tasks EVALUATE-FOR loss function. OE USED-FOR out - of - distribution detection. method CONJUNCTION Mahalanobis distance - based classifier. Mahalanobis distance - based classifier CONJUNCTION method. OOD detection task EVALUATE-FOR method. Task is artificial intelligence. Method are neural networks, and classification algorithms. OtherScientificTerm is novel class distributions. Metric is classification accuracy. ","This paper proposes a novel method for out-of-distribution (OOD) detection based on Outlier Exposure (OE) technique. OE is a technique for OOD detection, which is an extension of the outlier exposure (OPE) technique for outlier detection. The main idea is to use OE as a loss function to detect OOD examples. The proposed method is evaluated on image classification and text classification tasks.","This paper proposes a novel method for out-of-distribution (OOD) detection based on Outlier Exposure (OE) technique. OE is a technique for OOD detection, which is an extension of the outlier exposure (OPE) technique for outlier detection. The main idea is to use OE as a loss function to detect OOD examples. The proposed method is evaluated on image classification and text classification tasks."
22149,SP:89bc528ef801182365ac279e8963803afccb391d,end - to - end deep learning model USED-FOR RNA secondary structure prediction. E2Efold HYPONYM-OF end - to - end deep learning model. unrolled algorithm USED-FOR constrained programming. E2Efold USED-FOR RNA base - pairing matrix. unrolled algorithm USED-FOR deep architectures. deep architectures USED-FOR constraints. it COMPARE SOTA. SOTA COMPARE it. it COMPARE algorithms. algorithms COMPARE it. benchmark datasets EVALUATE-FOR E2Efold. E2Efold COMPARE it. it COMPARE E2Efold. SOTA USED-FOR pseudoknotted structures. E2Efold COMPARE SOTA. SOTA COMPARE E2Efold. inference time EVALUATE-FOR algorithms. ,This paper proposes an end-to-end deep learning model for RNA secondary structure prediction. The proposed method is based on the unrolled algorithm for constrained programming. The authors show that the proposed method outperforms SOTA on several benchmark datasets.,This paper proposes an end-to-end deep learning model for RNA secondary structure prediction. The proposed method is based on the unrolled algorithm for constrained programming. The authors show that the proposed method outperforms SOTA on several benchmark datasets.
22158,SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"collective policies COMPARE individually trained policies. individually trained policies COMPARE collective policies. OtherScientificTerm are biases, virtual simulation, agents ’ simulations, biased representations, and internal simulations. Method is collective policy. Material is real - world environment. ","This paper studies the problem of how to train a policy in a virtual simulation. The authors propose a method to learn a collective policy that can be used in a real-world environment. The proposed method is based on the observation that, when training a policy on a simulated environment, it is more likely to be biased than if the policy is trained on a real world environment. To address this issue, the authors propose to train the collective policy on the simulated environment and then use it in the real world. The method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines. ","This paper studies the problem of how to train a policy in a virtual simulation. The authors propose a method to learn a collective policy that can be used in a real-world environment. The proposed method is based on the observation that, when training a policy on a simulated environment, it is more likely to be biased than if the policy is trained on a real world environment. To address this issue, the authors propose to train the collective policy on the simulated environment and then use it in the real world. The method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines. "
22167,SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"Generic responses HYPONYM-OF open - domain dialog generation. one - to - one task USED-FOR one - to - many task. dialog generation model USED-FOR semantic latent space. prompt USED-FOR features. model USED-FOR semantically related responses. regression task USED-FOR pair relationship. model COMPARE baselines. baselines COMPARE model. coherence EVALUATE-FOR baselines. model USED-FOR generic response problem. coherence EVALUATE-FOR model. OtherScientificTerm are latent space, and MLE loss. Method is autoencoder. ",This paper proposes a dialog generation model for one-to-many dialog generation. The model is based on an autoencoder that learns the semantic latent space of the dialog prompt and uses the latent space to generate a set of semantically related responses. The proposed model is evaluated on a number of open-domain dialog generation tasks. The authors show that the proposed model outperforms baselines in terms of coherence and generalization.,This paper proposes a dialog generation model for one-to-many dialog generation. The model is based on an autoencoder that learns the semantic latent space of the dialog prompt and uses the latent space to generate a set of semantically related responses. The proposed model is evaluated on a number of open-domain dialog generation tasks. The authors show that the proposed model outperforms baselines in terms of coherence and generalization.
22176,SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"it USED-FOR life - affecting decisions. Gaussian light and shadow ( GLAS ) HYPONYM-OF salient explanation method. feature perturbation USED-FOR deep models. GLAS USED-FOR coarseto - fine control. scalability of Gaussian mask USED-FOR GLAS. scalability of Gaussian mask USED-FOR coarseto - fine control. GLAS USED-FOR fine - grained classification. fine - grained classification dataset EVALUATE-FOR GLAS. high speed EVALUATE-FOR GLAS. ImageNet Large Scale Visual Recognition Challenge EVALUATE-FOR GLAS. OtherScientificTerm are discriminative features, salient explanation, and 224×224 image. Task is fine - grained classification task. Method are Gaussian mask, and recursive GLAS. ",This paper proposes a novel Gaussian light and shadow (GLAS) method for fine-grained classification. The proposed method is based on the idea of salient explanation (SIG) which is an extension of the salient explanation method for feature perturbation. The main contribution of this paper is to introduce a recursive version of the SIG method. The authors show that the proposed method can be used to improve the scalability of the Gaussian mask and the coarseto-fine control. ,This paper proposes a novel Gaussian light and shadow (GLAS) method for fine-grained classification. The proposed method is based on the idea of salient explanation (SIG) which is an extension of the salient explanation method for feature perturbation. The main contribution of this paper is to introduce a recursive version of the SIG method. The authors show that the proposed method can be used to improve the scalability of the Gaussian mask and the coarseto-fine control. 
22185,SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"Convolution PART-OF Convolutional Neural Networks ( CNNs ). kernel USED-FOR Convolutional Neural Networks ( CNNs ). convolutional kernels USED-FOR redundant data. procedure USED-FOR pixel - wise and channel - wise correlations. computational cost EVALUATE-FOR convolution layer. deconvolution filters PART-OF network. center - surround structure FEATURE-OF biological neurons. center - surround structure HYPONYM-OF deconvolution filters. visual regions of the brain FEATURE-OF biological neurons. Filtering USED-FOR sparse representation. kernels USED-FOR sparse representation. kernels USED-FOR Filtering. network deconvolution operation USED-FOR neural network models. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION Fashion - MNIST. CIFAR-100 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION CIFAR-100. Cityscapes CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Cityscapes. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Fashion - MNIST. MNIST EVALUATE-FOR network deconvolution operation. Fashion - MNIST EVALUATE-FOR network deconvolution operation. CIFAR-100 EVALUATE-FOR network deconvolution operation. ImageNet datasets EVALUATE-FOR network deconvolution operation. CIFAR-10 EVALUATE-FOR network deconvolution operation. Material is real - world image data. Task is neural network training. Method are network deconvolution, Network deconvolution, neural networks, and batch normalization. Metric is faster convergence. ","This paper proposes a method to reduce the computational cost of convolutional neural networks (CNNs) by deconvolutional filters. The method is based on the center-surround structure of neural networks. The authors show that the proposed method is able to achieve faster convergence on CIFAR-10, MNIST, Fashion-MNIST and Fashion-MISIST datasets. The paper also shows that the method can be applied to batch normalization.","This paper proposes a method to reduce the computational cost of convolutional neural networks (CNNs) by deconvolutional filters. The method is based on the center-surround structure of neural networks. The authors show that the proposed method is able to achieve faster convergence on CIFAR-10, MNIST, Fashion-MNIST and Fashion-MISIST datasets. The paper also shows that the method can be applied to batch normalization."
22194,SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"smartphones HYPONYM-OF edge devices. neural network quantization methods USED-FOR GANs. CNN quantization methods USED-FOR GAN models. CNN quantization methods USED-FOR extreme low bits. quantization method USED-FOR GANs. QGAN HYPONYM-OF quantization method. EM algorithms USED-FOR quantization method. multi - precision algorithm USED-FOR quantization precision. multi - precision algorithm USED-FOR GANs. quantization precision FEATURE-OF GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. QGAN USED-FOR GANs. QGAN COMPARE models. models COMPARE QGAN. CIFAR-10 EVALUATE-FOR QGAN. CelebA EVALUATE-FOR QGAN. QGAN USED-FOR 1 - bit or 2 - bit representations. Method are generative adversarial neural networks ( GANs ), convolutional neural networks ( CNNs ), quantization algorithms, and generator and discriminator networks. Generic is them. OtherScientificTerm is image qualities requirements. ","This paper proposes a new quantization method for GANs. The proposed method, called QGAN, is a multi-precision quantization algorithm that can be applied to any GAN model. The authors show that the proposed method is able to achieve state-of-the-art performance on CIFAR-10 and CelebA datasets.","This paper proposes a new quantization method for GANs. The proposed method, called QGAN, is a multi-precision quantization algorithm that can be applied to any GAN model. The authors show that the proposed method is able to achieve state-of-the-art performance on CIFAR-10 and CelebA datasets."
22203,SP:58c4905f59f04a50b30d27c99521126a6455d38a,"Generative Adversarial Networks HYPONYM-OF nonconvex applications. bilinear and convex - strongly concave settings FEATURE-OF global last - iterate convergence rates. Simultaneous Gradient Descent / Ascent HYPONYM-OF natural algorithms. linear convergence FEATURE-OF HAMILTONIAN GRADIENT DESCENT ( HGD ) algorithm. “ sufficiently bilinear ” condition FEATURE-OF convex - concave problems. convergence rates FEATURE-OF stochastic HGD. Task are convex - concave min - max optimization, and training Generative Adversarial Networks. OtherScientificTerm are averageiterate convergence results, last - iterate convergence guarantees, last - iterate convergence, and convex - concave min - max settings. Method is Consensus Optimization algorithm. ","This paper studies the last-iterate convergence rate of Simultaneous Gradient Descent/Ascent (SGD) algorithms for convex-concave min-max optimization in bilinear and convex strongly concave settings. In particular, the authors show that SGD converges to the optimal solution of a convex convex problem in the last iterate. They also show that the convergence rate for SGD is bounded in the convex case. ","This paper studies the last-iterate convergence rate of Simultaneous Gradient Descent/Ascent (SGD) algorithms for convex-concave min-max optimization in bilinear and convex strongly concave settings. In particular, the authors show that SGD converges to the optimal solution of a convex convex problem in the last iterate. They also show that the convergence rate for SGD is bounded in the convex case. "
22212,SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"stability FEATURE-OF ResNet. stability FEATURE-OF ResNet. gradient descent USED-FOR global minima. over - parameterization requirement FEATURE-OF ResNet. ResNet COMPARE vanilla feedforward network. vanilla feedforward network COMPARE ResNet. normalization layer USED-FOR deep ResNet. normalization layer USED-FOR ResNet. Method is ResNet structure. OtherScientificTerm are ResNet block hl, ReLU activation, initialization, residual blocks, and global convergence. Metric is τ. Generic is forward process. ",This paper studies the stability of deep Resnets. The authors show that the global minima of a deep ResNet converges to the global minimum of the ReLU activation of the block hl. They show that this convergence is due to the over-parameterization requirement of the forward process. They also show that global convergence is also related to the normalization layer of the deep Resnet. ,This paper studies the stability of deep Resnets. The authors show that the global minima of a deep ResNet converges to the global minimum of the ReLU activation of the block hl. They show that this convergence is due to the over-parameterization requirement of the forward process. They also show that global convergence is also related to the normalization layer of the deep Resnet. 
22221,SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,Sparse neural networks COMPARE dense networks. dense networks COMPARE Sparse neural networks. they USED-FOR wall clock inference times. sparse networks USED-FOR inference. dense networks USED-FOR sparse networks. method USED-FOR sparse neural networks. fixed parameter count CONJUNCTION fixed computational cost. fixed computational cost CONJUNCTION fixed parameter count. accuracy EVALUATE-FOR dense - to - sparse training methods. fixed computational cost USED-FOR method. fixed parameter count USED-FOR method. parameter magnitudes CONJUNCTION infrequent gradient calculations. infrequent gradient calculations CONJUNCTION parameter magnitudes. method USED-FOR topology. topology FEATURE-OF network. parameter magnitudes USED-FOR topology. parameter magnitudes USED-FOR method. infrequent gradient calculations USED-FOR method. approach COMPARE prior techniques. prior techniques COMPARE approach. accuracy EVALUATE-FOR prior techniques. floating - point operations ( FLOPs ) USED-FOR approach. accuracy EVALUATE-FOR approach. MobileNet v1 CONJUNCTION MobileNet v2. MobileNet v2 CONJUNCTION MobileNet v1. ResNet-50 CONJUNCTION MobileNet v1. MobileNet v1 CONJUNCTION ResNet-50. WideResNets CONJUNCTION RNNs. RNNs CONJUNCTION WideResNets. ImageNet-2012 dataset CONJUNCTION WideResNets. WideResNets CONJUNCTION ImageNet-2012 dataset. WideResNets CONJUNCTION CIFAR-10 dataset. CIFAR-10 dataset CONJUNCTION WideResNets. MobileNet v2 CONJUNCTION WideResNets. WideResNets CONJUNCTION MobileNet v2. WikiText-103 dataset EVALUATE-FOR RNNs. ResNet-50 USED-FOR sparse training. ImageNet-2012 dataset EVALUATE-FOR MobileNet v2. Method is trainable sparse model. Task is optimization. OtherScientificTerm is local minima. ,"This paper proposes a method for training sparse neural networks. The method is based on the idea that the topology of a sparse network is a function of the parameter magnitudes and the infrequent gradient calculations. The authors propose to use floating-point operations (FLOPs) to reduce the number of FLOPs required to train a sparse model. The proposed method is evaluated on a variety of datasets, including CIFAR-10, ImageNet-2012, MobileNet-50, WideResnets, and WikiText-103.","This paper proposes a method for training sparse neural networks. The method is based on the idea that the topology of a sparse network is a function of the parameter magnitudes and the infrequent gradient calculations. The authors propose to use floating-point operations (FLOPs) to reduce the number of FLOPs required to train a sparse model. The proposed method is evaluated on a variety of datasets, including CIFAR-10, ImageNet-2012, MobileNet-50, WideResnets, and WikiText-103."
22230,SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"deep generative models USED-FOR photo - realistic images. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep generative models USED-FOR visual or textual content embeddings. photo - realistic images CONJUNCTION visual or textual content embeddings. visual or textual content embeddings CONJUNCTION photo - realistic images. deep generative models USED-FOR natural language processing. deep generative models USED-FOR computer vision. translation CONJUNCTION zoom. zoom CONJUNCTION translation. zoom CONJUNCTION color variations. color variations CONJUNCTION zoom. color variations HYPONYM-OF transformations. translation HYPONYM-OF transformations. zoom HYPONYM-OF transformations. GANs CONJUNCTION variational auto - encoders. variational auto - encoders CONJUNCTION GANs. variational auto - encoders EVALUATE-FOR method. GANs EVALUATE-FOR method. approach CONJUNCTION BigGAN model. BigGAN model CONJUNCTION approach. Method are generative process, generative models, and generative model. OtherScientificTerm are latent space, and human annotations. ","This paper proposes a generative model that can be used for image generation and natural language processing tasks. The model is based on a GAN-based approach, and is trained on the latent space of an image. The authors show that the model can be applied to a variety of tasks, including image generation, text generation, and language processing. The paper also shows that the proposed model is able to achieve better performance than the baselines. ","This paper proposes a generative model that can be used for image generation and natural language processing tasks. The model is based on a GAN-based approach, and is trained on the latent space of an image. The authors show that the model can be applied to a variety of tasks, including image generation, text generation, and language processing. The paper also shows that the proposed model is able to achieve better performance than the baselines. "
22239,SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"model USED-FOR unsupervised physical parameter estimation of systems. differential equations USED-FOR scene dynamics. video USED-FOR unsupervised physical parameter estimation of systems. object state supervision USED-FOR physical scene understanding methods. objects CONJUNCTION state and velocity representations. state and velocity representations CONJUNCTION objects. framework USED-FOR long term extrapolative video prediction. framework USED-FOR vision - based model - predictive control. long term extrapolative video prediction CONJUNCTION vision - based model - predictive control. vision - based model - predictive control CONJUNCTION long term extrapolative video prediction. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. unsupervised methods USED-FOR long - term future frame prediction of systems. approach USED-FOR long - term future frame prediction of systems. dynamics PART-OF model. interacting objects FEATURE-OF long - term future frame prediction of systems. inductive bias USED-FOR dynamics. vision - actuated model - based control USED-FOR pendulum system. goal - driven control CONJUNCTION physical reasoning. physical reasoning CONJUNCTION goal - driven control. controller ’s interpretability USED-FOR goal - driven control. physical reasoning USED-FOR zero - data adaptation. controller ’s interpretability USED-FOR physical reasoning. controller ’s interpretability USED-FOR zero - data adaptation. goal - driven control USED-FOR zero - data adaptation. OtherScientificTerm is labeled states. Method are differentiable physics, physics - as - inverse - graphics approach, and vision - physics integration. ","This paper proposes a physics-as-inverse-graphics approach for long-term extrapolative video prediction of physical system dynamics. The proposed method is based on a vision-based model-predictive control framework, which is able to predict the future state of a pendulum system from a video frame. The authors show that the proposed method outperforms state-of-the-art unsupervised methods for video prediction and goal-driven control. ","This paper proposes a physics-as-inverse-graphics approach for long-term extrapolative video prediction of physical system dynamics. The proposed method is based on a vision-based model-predictive control framework, which is able to predict the future state of a pendulum system from a video frame. The authors show that the proposed method outperforms state-of-the-art unsupervised methods for video prediction and goal-driven control. "
22248,SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"Graph Convolutional Networks ( GCN ) USED-FOR class relevance. Graph Convolutional Networks ( GCN ) USED-FOR structure of clean and noisy data. graph per class USED-FOR structure of clean and noisy data. GCN - inferred “ clean ” probability USED-FOR relevance measure. GCN USED-FOR binary classifier. weighted binary cross - entropy loss function USED-FOR GCN. weighted binary cross - entropy loss function USED-FOR binary classifier. few - shot learning problem EVALUATE-FOR method. classification accuracy EVALUATE-FOR few - shot classification. few - shot classification EVALUATE-FOR GCN - based cleaning process. classification accuracy EVALUATE-FOR GCN - based cleaning process. GCN - based method COMPARE transductive approach. transductive approach COMPARE GCN - based method. Method is classifier. OtherScientificTerm are clean from noisy examples, and relevance. Material is noisy data. ","This paper proposes a new method for few-shot learning based on graph convolutional networks (GCN). The proposed method is based on the idea of class relevance, which is a measure of the similarity between clean and noisy data. The authors show that GCN-based methods can be used to improve the performance of the classifier. The method is evaluated on the few shot learning problem.","This paper proposes a new method for few-shot learning based on graph convolutional networks (GCN). The proposed method is based on the idea of class relevance, which is a measure of the similarity between clean and noisy data. The authors show that GCN-based methods can be used to improve the performance of the classifier. The method is evaluated on the few shot learning problem."
22257,SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"edge features CONJUNCTION message passing channels. message passing channels CONJUNCTION edge features. differentiable objective USED-FOR MI. variational approach USED-FOR differentiable objective. variational approach USED-FOR MI. objective USED-FOR model. MI - maximized models USED-FOR learning tasks. objective USED-FOR edge information. edge information FEATURE-OF model. molecular graphs USED-FOR regression. relation prediction in knowledge graphs HYPONYM-OF learning tasks. regression HYPONYM-OF learning tasks. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are representation vector, and parameterized transform matrix. Metric is Mutual Information ( MI ). Material is knowledge graphs. ","This paper proposes a new objective for mutual information (MI) maximization in graph neural networks (GNNs). The proposed objective is based on a variational approach to learn a differentiable objective that maximizes the mutual information between edge features and message passing channels. The authors show that the proposed objective can be used to improve the performance of GNNs on a variety of tasks, including regression and relation prediction. ","This paper proposes a new objective for mutual information (MI) maximization in graph neural networks (GNNs). The proposed objective is based on a variational approach to learn a differentiable objective that maximizes the mutual information between edge features and message passing channels. The authors show that the proposed objective can be used to improve the performance of GNNs on a variety of tasks, including regression and relation prediction. "
22266,SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"models USED-FOR specifying visual transformations. Generative networks USED-FOR specifying visual transformations. Generative networks HYPONYM-OF models. verification methods USED-FOR generative networks. verifier USED-FOR generative networks. APPROXLINE HYPONYM-OF verifier. deterministic and probabilistic abstract interpretation USED-FOR APPROXLINE. Task is certification of generative models. Method is generative models. OtherScientificTerm are sufficient non - convexity, non - convexity, and network ’s latent space. ","This paper proposes a new verification method for generative models. The proposed method, APPROXLINE, is based on a deterministic and probabilistic abstract interpretation of the latent space of a generative model. The key idea is to use the deterministic interpretation of a latent space to verify the non-convexity of a given latent space. The method is evaluated on MNIST, CIFAR-10, and ImageNet.","This paper proposes a new verification method for generative models. The proposed method, APPROXLINE, is based on a deterministic and probabilistic abstract interpretation of the latent space of a generative model. The key idea is to use the deterministic interpretation of a latent space to verify the non-convexity of a given latent space. The method is evaluated on MNIST, CIFAR-10, and ImageNet."
22275,SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"deep architectures USED-FOR models. spectral graph convolutional operator USED-FOR graph neural networks ( GNNs ). GNNs USED-FOR suspended animation problem. GNNs USED-FOR suspended animation problem. nodes ’ raw features CONJUNCTION intermediate representations. intermediate representations CONJUNCTION nodes ’ raw features. intermediate representations PART-OF graph. intermediate representations USED-FOR model layers. GRESNET ( Graph Residual Network ) framework USED-FOR connected highways. norm preservation perspective USED-FOR graph residual terms. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GAT CONJUNCTION LOOPYNET. LOOPYNET CONJUNCTION GAT. GRESNET framework USED-FOR GNNs. LOOPYNET HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. GAT HYPONYM-OF GNNs. Generic are problem, model, and learning settings. OtherScientificTerm are model depth, suspended animation limit, and node ’s representations. Material are graph data, and real - world benchmark datasets. Method is residual learning methods. ","This paper proposes a new framework for graph residual learning based on the spectral graph convolutional operator (GCN) and graph residual networks (GCNs). The proposed framework is based on a norm preservation perspective, which is used to preserve the graph residual terms. The authors show that the proposed method outperforms existing methods on several benchmark datasets. ","This paper proposes a new framework for graph residual learning based on the spectral graph convolutional operator (GCN) and graph residual networks (GCNs). The proposed framework is based on a norm preservation perspective, which is used to preserve the graph residual terms. The authors show that the proposed method outperforms existing methods on several benchmark datasets. "
22284,SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"face prior knowledge USED-FOR reconstruction process. limited scan data USED-FOR face prior knowledge. linear 3D morphable models ( 3DMM ) HYPONYM-OF face prior knowledge. limited scan data USED-FOR linear 3D morphable models ( 3DMM ). face prior knowledge USED-FOR ambiguity. expressions CONJUNCTION poses. poses CONJUNCTION expressions. poses CONJUNCTION lightings. lightings CONJUNCTION poses. expressions FEATURE-OF facial images. poses FEATURE-OF facial images. linear parametric models USED-FOR methods. convolutional neural networks ( CNN ) USED-FOR nonlinear parametric model. linear 3DMM USED-FOR dataset. dataset USED-FOR models. identity and expression representations PART-OF models. semi - supervised manner FEATURE-OF adversarial loss. unconstrained photo collections USED-FOR unlabeled face images. adversarial loss USED-FOR model. semi - supervised manner USED-FOR model. hybrid batches of unlabeled and labeled face images USED-FOR model. identity shape FEATURE-OF facial images. expression CONJUNCTION pose. pose CONJUNCTION expression. identity CONJUNCTION expression. expression CONJUNCTION identity. pose CONJUNCTION lighting representations. lighting representations CONJUNCTION pose. model USED-FOR facial editing applications. model USED-FOR expression. model USED-FOR identity. model USED-FOR lighting representations. expression transfer HYPONYM-OF facial editing applications. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. model USED-FOR expression. Task are Recovering 3D geometry shape, ill - posed problem, and reconstruction. OtherScientificTerm is center loss. ",This paper proposes a new method for face reconstruction based on face prior knowledge. The proposed method is based on convolutional neural networks (CNN) and a nonlinear parametric model (3DMM). The authors show that the proposed method outperforms state-of-the-art methods in terms of reconstruction accuracy and reconstruction speed. The authors also show that their method can be applied to face reconstruction in a semi-supervised manner.,This paper proposes a new method for face reconstruction based on face prior knowledge. The proposed method is based on convolutional neural networks (CNN) and a nonlinear parametric model (3DMM). The authors show that the proposed method outperforms state-of-the-art methods in terms of reconstruction accuracy and reconstruction speed. The authors also show that their method can be applied to face reconstruction in a semi-supervised manner.
22293,SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"transition kernel USED-FOR Model - based imitation learning methods. partial knowledge FEATURE-OF transition kernel. Reinforcement Learning ( RL ) USED-FOR imitation problems. unknown transition kernel CONJUNCTION synthetic kernel. synthetic kernel CONJUNCTION unknown transition kernel. synthetic kernel USED-FOR transition of state components. kernel USED-FOR state components. transition kernel USED-FOR transition of state components. multiplayer games FEATURE-OF imitation tasks. policy gradient algorithm CONJUNCTION model. model CONJUNCTION policy gradient algorithm. policy gradient algorithm COMPARE simulation - free alternative. simulation - free alternative COMPARE policy gradient algorithm. model COMPARE simulation - free alternative. simulation - free alternative COMPARE model. Task is policy evaluation. Method are eMDP, and transition model. Generic is components. OtherScientificTerm is sr. ",This paper proposes a model-based imitation learning method for reinforcement learning. The key idea is to use the transition kernel as a partial knowledge for the transition of state components. The authors show that the proposed method outperforms the state-of-the-art methods on a number of imitation learning tasks. ,This paper proposes a model-based imitation learning method for reinforcement learning. The key idea is to use the transition kernel as a partial knowledge for the transition of state components. The authors show that the proposed method outperforms the state-of-the-art methods on a number of imitation learning tasks. 
22302,SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"manually - designed reward function USED-FOR Learning useful skills. OpenAI Gym CONJUNCTION navigation task. navigation task CONJUNCTION OpenAI Gym. OpenAI Gym FEATURE-OF simulated robotic manipulation tasks. navigation task HYPONYM-OF simulated robotic manipulation tasks. Gazebo simulator USED-FOR navigation task. navigation task EVALUATE-FOR approach. simulated robotic manipulation tasks EVALUATE-FOR approach. intrinsic mutual information rewards USED-FOR method. mutual information discriminator USED-FOR learning. pre - trained policy USED-FOR learning. pre - trained policy CONJUNCTION mutual information discriminator. mutual information discriminator CONJUNCTION pre - trained policy. learning USED-FOR task rewards. mutual information discriminator USED-FOR task rewards. pre - trained policy USED-FOR task rewards. sparse rewards USED-FOR robotic manipulation tasks. Method are reinforcement learning, and selfsupervised Reinforcement Learning approach. OtherScientificTerm are external reward function, intrinsic objective, context states, robot states, states of interest, and mutual information. ",This paper proposes a self-supervised reinforcement learning approach for learning useful skills. The key idea is to learn a task-specific reward function that maximizes the mutual information between the agent and the environment. The proposed method is evaluated on the Gazebo simulator and the OpenAI Gym. The authors show that the proposed method outperforms the baselines on a number of tasks.,This paper proposes a self-supervised reinforcement learning approach for learning useful skills. The key idea is to learn a task-specific reward function that maximizes the mutual information between the agent and the environment. The proposed method is evaluated on the Gazebo simulator and the OpenAI Gym. The authors show that the proposed method outperforms the baselines on a number of tasks.
22311,SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"Neural network ( NN ) trojaning attack HYPONYM-OF attack. adversarial attacks COMPARE it. it COMPARE adversarial attacks. it USED-FOR malicious functionality. small datasets USED-FOR NN trojaning attacks. generality CONJUNCTION stealthiness. stealthiness CONJUNCTION generality. trojaning attack method USED-FOR large models. capability CONJUNCTION generality. generality CONJUNCTION capability. trojaning attack method COMPARE studies. studies COMPARE trojaning attack method. generality EVALUATE-FOR studies. capability EVALUATE-FOR studies. stealthiness EVALUATE-FOR studies. trojaning attack USED-FOR small domain. large - scale dataset USED-FOR trojaned model. biased behavior FEATURE-OF trojan. Method is NN models. OtherScientificTerm are weight parameters, fixed target classes, and malicious misclassification target. ","This paper proposes a new adversarial attack method for neural network (NN) trojaning attacks. The proposed method is based on the idea that the trojaned model can be seen as a fixed classifier, and thus can be used as a malicious misclassification target. The authors show that the proposed method can be applied to both small and large NN models, and that it is able to achieve better performance than adversarial attacks. ","This paper proposes a new adversarial attack method for neural network (NN) trojaning attacks. The proposed method is based on the idea that the trojaned model can be seen as a fixed classifier, and thus can be used as a malicious misclassification target. The authors show that the proposed method can be applied to both small and large NN models, and that it is able to achieve better performance than adversarial attacks. "
22320,SP:35ea626ee4dd1a7a368a660eb852192924966b7f,prediction tasks PART-OF drug 1 discovery. few - shot regression ( FSR ) problems USED-FOR prediction tasks. modelling of biological assays HYPONYM-OF few - shot regression ( FSR ) problems. reinforcement learning methods USED-FOR applications. few - shot classification USED-FOR applications. few - shot classification CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION few - shot classification. FSR methods USED-FOR tasks. real - world constraints FEATURE-OF tasks. deep kernel learning USED-FOR FSR 6 algorithm. kernel function CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION kernel function. deep network CONJUNCTION kernel function. kernel function CONJUNCTION deep network. deep network CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION deep network. deep network PART-OF algorithm. differentiable kernel 8 algorithm PART-OF algorithm. kernel function PART-OF algorithm. kernel USED-FOR task. algorithm USED-FOR kernel. kernel USED-FOR inference. task PART-OF inference. algorithm USED-FOR task. It COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE It. It USED-FOR complex task distributions. real - world benchmarks EVALUATE-FOR state - of - the - art algorithms. real - world benchmarks EVALUATE-FOR It. FSR algorithms USED-FOR noisy and uncertain environments. biological assays USED-FOR benchmarks. drug discovery HYPONYM-OF noisy and uncertain environments. Task is data generation. ,This paper proposes a new algorithm for few-shot regression (FSR) based on deep kernel learning. The main idea is to use a differentiable kernel function and a deep network to solve the FSR problem. The authors show that the proposed algorithm outperforms state-of-the-art FSR algorithms on synthetic and real-world datasets. ,This paper proposes a new algorithm for few-shot regression (FSR) based on deep kernel learning. The main idea is to use a differentiable kernel function and a deep network to solve the FSR problem. The authors show that the proposed algorithm outperforms state-of-the-art FSR algorithms on synthetic and real-world datasets. 
22329,SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"Neural networks USED-FOR reasoning tasks. Graph Neural Networks ( GNNs ) USED-FOR tasks. Graph Neural Networks ( GNNs ) HYPONYM-OF network structures. network structures USED-FOR tasks. expressive power FEATURE-OF they. framework USED-FOR reasoning tasks. computation structure COMPARE algorithmic structure. algorithmic structure COMPARE computation structure. algorithmic structure FEATURE-OF reasoning process. network USED-FOR reasoning tasks. sample complexity bound EVALUATE-FOR alignment. framework EVALUATE-FOR reasoning models. visual question answering CONJUNCTION shortest paths. shortest paths CONJUNCTION visual question answering. intuitive physics CONJUNCTION visual question answering. visual question answering CONJUNCTION intuitive physics. dynamic programming ( DP ) HYPONYM-OF algorithmic paradigm. dynamic programming ( DP ) USED-FOR reasoning tasks. algorithmic paradigm USED-FOR reasoning tasks. intuitive physics HYPONYM-OF reasoning tasks. shortest paths HYPONYM-OF reasoning tasks. visual question answering HYPONYM-OF reasoning tasks. GNNs USED-FOR tasks. GNNs COMPARE DP. DP COMPARE GNNs. reasoning tasks EVALUATE-FOR theory. Method is structured networks. OtherScientificTerm are network structure, and algorithmic alignment. ","This paper studies the problem of learning reasoning models that can be used for reasoning tasks. The authors propose an algorithmic alignment framework that is based on dynamic programming (DP) to learn a reasoning model that is able to perform well on a variety of reasoning tasks such as intuitive physics, shortest paths, and visual question answering. The proposed framework is evaluated on a number of tasks and shows that the proposed framework outperforms GNNs and DP-based reasoning models. ","This paper studies the problem of learning reasoning models that can be used for reasoning tasks. The authors propose an algorithmic alignment framework that is based on dynamic programming (DP) to learn a reasoning model that is able to perform well on a variety of reasoning tasks such as intuitive physics, shortest paths, and visual question answering. The proposed framework is evaluated on a number of tasks and shows that the proposed framework outperforms GNNs and DP-based reasoning models. "
22338,SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,conditional consistency CONJUNCTION intra - conditioning diversity. intra - conditioning diversity CONJUNCTION conditional consistency. image quality CONJUNCTION conditional consistency. conditional consistency CONJUNCTION image quality. conditional consistency HYPONYM-OF metrics. metrics EVALUATE-FOR models. it USED-FOR properties. Fréchet distance FEATURE-OF joint distributions. metric USED-FOR it. metric USED-FOR properties. controllable synthetic dataset EVALUATE-FOR FJD. FJD COMPARE metrics. metrics COMPARE FJD. bounding boxes CONJUNCTION images. images CONJUNCTION bounding boxes. object masks CONJUNCTION bounding boxes. bounding boxes CONJUNCTION object masks. images CONJUNCTION text captions. text captions CONJUNCTION images. class labels CONJUNCTION object masks. object masks CONJUNCTION class labels. metric EVALUATE-FOR cGAN - based models. cGAN - based models USED-FOR conditioning modalities. text captions HYPONYM-OF conditioning modalities. images HYPONYM-OF conditioning modalities. class labels HYPONYM-OF conditioning modalities. bounding boxes HYPONYM-OF conditioning modalities. object masks HYPONYM-OF conditioning modalities. cGAN benchmarking CONJUNCTION model selection. model selection CONJUNCTION cGAN benchmarking. metric USED-FOR cGAN benchmarking. metric USED-FOR model selection. FJD USED-FOR metric. FJD USED-FOR cGAN benchmarking. Method is Conditional Generative Adversarial Networks ( cGANs ). Task is model benchmarking. Metric is Fréchet Joint Distance ( FJD ). OtherScientificTerm is conditioning. ,This paper proposes a new benchmarking metric for conditional generative adversarial networks (cGANs) based on the Fréchet Joint Distance (FJD). FJD measures the distance between the joint distributions of a pair of conditioned samples and the joint distribution of the two conditioned samples. The authors show that FJD can be used to evaluate the performance of cGAN-based models on a controllable synthetic dataset. ,This paper proposes a new benchmarking metric for conditional generative adversarial networks (cGANs) based on the Fréchet Joint Distance (FJD). FJD measures the distance between the joint distributions of a pair of conditioned samples and the joint distribution of the two conditioned samples. The authors show that FJD can be used to evaluate the performance of cGAN-based models on a controllable synthetic dataset. 
22347,SP:fa822e8472efae17c7dfde8258057898383ecbbb,"decision states USED-FOR exploration. exploration USED-FOR downstream goal - driven tasks. partially observable environments FEATURE-OF downstream goal - driven tasks. Method is VIC framework. OtherScientificTerm are empowerment objective, and extrinsic rewards. Task is identification of decision states. ",This paper proposes a new exploration framework for goal-driven tasks. The proposed framework is based on the VIC framework. The main idea is to learn a set of decision states that can be used to guide exploration. The authors show that the proposed method is able to learn decision states in partially observable environments. They also show that their method can be applied to a variety of downstream tasks.,This paper proposes a new exploration framework for goal-driven tasks. The proposed framework is based on the VIC framework. The main idea is to learn a set of decision states that can be used to guide exploration. The authors show that the proposed method is able to learn decision states in partially observable environments. They also show that their method can be applied to a variety of downstream tasks.
22356,SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"architectures USED-FOR irregularly - sampled and asynchronous time series. real - world datasets FEATURE-OF irregularly - sampled and asynchronous time series. healthcare applications HYPONYM-OF real - world datasets. framework USED-FOR classifying irregularly sampled time series. data efficiency EVALUATE-FOR framework. unaligned measurements USED-FOR irregularly sampled time series. method COMPARE competitors. competitors COMPARE method. runtime EVALUATE-FOR it. healthcare time series datasets EVALUATE-FOR competitors. healthcare time series datasets EVALUATE-FOR method. Method are deep neural networks, and differentiable set function learning. Task is online monitoring scenarios. ","This paper proposes a framework for classifying irregularly sampled and asynchronous time series. The proposed framework is based on the idea of differentiable set function learning. The authors propose to use a differentiable neural network to learn the set function of an unaligned set of time series, which is then used to train a deep neural network. The method is evaluated on a number of real-world time series datasets, and is shown to be competitive with existing methods.","This paper proposes a framework for classifying irregularly sampled and asynchronous time series. The proposed framework is based on the idea of differentiable set function learning. The authors propose to use a differentiable neural network to learn the set function of an unaligned set of time series, which is then used to train a deep neural network. The method is evaluated on a number of real-world time series datasets, and is shown to be competitive with existing methods."
22365,SP:4ae89d64460b08749acc192004545c1fa8b7553b,Convolutional neural networks ( CNNs ) USED-FOR image recognition. inductive biases USED-FOR natural image priors. inductive biases FEATURE-OF CNNs. deep networks USED-FOR audio signals. inductive biases FEATURE-OF audio signals. inductive biases FEATURE-OF deep networks. network architectures USED-FOR audio processing. local neighborhoods USED-FOR convolutional kernels. harmonic series USED-FOR kernels. networks USED-FOR audio priors. networks USED-FOR unsupervised audio restoration. Harmonic Convolution USED-FOR networks. Harmonic Convolution USED-FOR they. they USED-FOR supervised musical source separation. Harmonic Convolution USED-FOR supervised musical source separation. generalization EVALUATE-FOR supervised musical source separation. generalization EVALUATE-FOR they. Generic is priors. OtherScientificTerm is harmonic structure. ,"This paper proposes a method for unsupervised audio restoration based on harmonic convolutional neural networks. The method is based on the idea of harmonic convolutions, which is an extension of the harmonic series theory. The authors show that the proposed method can be used to improve the generalization performance of audio restoration tasks. The proposed method is evaluated on a variety of tasks, including supervised audio restoration and supervised musical source separation. ","This paper proposes a method for unsupervised audio restoration based on harmonic convolutional neural networks. The method is based on the idea of harmonic convolutions, which is an extension of the harmonic series theory. The authors show that the proposed method can be used to improve the generalization performance of audio restoration tasks. The proposed method is evaluated on a variety of tasks, including supervised audio restoration and supervised musical source separation. "
22374,SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"specialized hardware accelerators USED-FOR neural network training. GPUs USED-FOR neural network training. GPUs CONJUNCTION specialized hardware accelerators. specialized hardware accelerators CONJUNCTION GPUs. disk I / O CONJUNCTION data preprocessing. data preprocessing CONJUNCTION disk I / O. workloads EVALUATE-FOR data echoing algorithms. data echoing algorithm COMPARE baseline. baseline COMPARE data echoing algorithm. upstream computation USED-FOR data echoing algorithm. upstream computation USED-FOR baseline. wall - clock time FEATURE-OF ResNet-50. ImageNet FEATURE-OF ResNet-50. Task are training pipeline, and training. OtherScientificTerm are accelerators, echoing, and batch sizes. Method are data echoing, pipeline stages, Data echoing, and network. Metric is training time. ",This paper studies the problem of data echoing in the training of deep neural networks. The authors propose a data-augmentation-based approach to improve the performance of the training process. The main contribution of the paper is to propose a new algorithm for data echoing. The proposed method is based on the idea that the number of layers in the network can be reduced to a fixed number of times. The paper shows that the proposed method outperforms the baselines in terms of wall-clock time and batch size.,This paper studies the problem of data echoing in the training of deep neural networks. The authors propose a data-augmentation-based approach to improve the performance of the training process. The main contribution of the paper is to propose a new algorithm for data echoing. The proposed method is based on the idea that the number of layers in the network can be reduced to a fixed number of times. The paper shows that the proposed method outperforms the baselines in terms of wall-clock time and batch size.
22383,SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"policy COMPARE policies. policies COMPARE policy. controllable subspace FEATURE-OF Markov decision process. Markov decision process FEATURE-OF behaviors. controllable subspace FEATURE-OF behaviors. Successor features USED-FOR generalization problem. grounded feature space FEATURE-OF reward function. generalization CONJUNCTION task inference. task inference CONJUNCTION generalization. algorithm USED-FOR generalization. algorithm USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) HYPONYM-OF algorithm. successor features framework USED-FOR task inference. Atari suite EVALUATE-FOR VISR. human - level performance EVALUATE-FOR VISR. Generic are formulation, tasks, techniques, and method. OtherScientificTerm are controllable features, and limited feedback. ","This paper proposes a successor features framework for task inference and generalization. The key idea is to learn a set of successor features for each task, and then use the learned successor features to improve the generalization performance of the policy. The proposed method is evaluated on the Atari suite. ","This paper proposes a successor features framework for task inference and generalization. The key idea is to learn a set of successor features for each task, and then use the learned successor features to improve the generalization performance of the policy. The proposed method is evaluated on the Atari suite. "
22392,SP:83500230586a9134f910ad067b7233dc563dc1ba,"functional view USED-FOR networks. functional view USED-FOR them. smoothness of the functional approximation CONJUNCTION flat initial approximation. flat initial approximation CONJUNCTION smoothness of the functional approximation. smoothness of the functional approximation USED-FOR generalization. flat initial approximation USED-FOR generalization. Method are deep neural networks, functional view of these networks, and massively overparamaterized networks. OtherScientificTerm are initializations, loss surface, and smoothness. ","This paper studies the smoothness of the initializations of deep neural networks. The authors show that the flat initial approximation of the functional approximation can be viewed as a function of the loss surface of the network. They show that this smoothness can be used as a measure of the generalization ability of the networks. They also show that for large-scale overparameterized networks, the initialization is smooth. ","This paper studies the smoothness of the initializations of deep neural networks. The authors show that the flat initial approximation of the functional approximation can be viewed as a function of the loss surface of the network. They show that this smoothness can be used as a measure of the generalization ability of the networks. They also show that for large-scale overparameterized networks, the initialization is smooth. "
22401,SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"Generative Adversarial Network ( GAN ) USED-FOR image - to - image translation problem. imbalance problem FEATURE-OF GAN - based methods. mode collapse CONJUNCTION diminished gradients. diminished gradients CONJUNCTION mode collapse. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. relative model capacities FEATURE-OF generator. relative model capacities FEATURE-OF discriminator. attention mechanism USED-FOR GuideGAN. it USED-FOR attention map. attention mechanism USED-FOR discriminator. attention map USED-FOR generator. image transfer tasks EVALUATE-FOR GuideGAN framework. Task are supervised and unsupervised manner, and prediction. Generic is approach. ",This paper proposes a novel approach to the image-to-image translation problem. The proposed method is based on the idea that the generator and discriminator should be able to share the same attention map. The authors propose to use the attention map as the discriminator and the generator as the generator. They show that the proposed method outperforms the baselines in both supervised and unsupervised tasks. ,This paper proposes a novel approach to the image-to-image translation problem. The proposed method is based on the idea that the generator and discriminator should be able to share the same attention map. The authors propose to use the attention map as the discriminator and the generator as the generator. They show that the proposed method outperforms the baselines in both supervised and unsupervised tasks. 
22410,SP:41c089ba65393174dae1dc136f79030a0a4fc532,"attention layers CONJUNCTION hypernetworks. hypernetworks CONJUNCTION attention layers. hypernetworks CONJUNCTION dynamic convolutions. dynamic convolutions CONJUNCTION hypernetworks. multiplicative interaction USED-FOR neural network architectural motifs. gating CONJUNCTION attention layers. attention layers CONJUNCTION gating. dynamic convolutions HYPONYM-OF neural network architectural motifs. hypernetworks HYPONYM-OF neural network architectural motifs. attention layers HYPONYM-OF neural network architectural motifs. gating HYPONYM-OF neural network architectural motifs. Multiplicative interaction layers HYPONYM-OF primitive operations. multiplicative interactions USED-FOR inductive bias. multiplicative interactions USED-FOR neural network architectures. them USED-FOR multiplicative interactions. Generic are layers, and they. Method is neural networks. OtherScientificTerm are conditional computation, and concatenation operation. ","This paper studies the multiplicative interactions in neural networks. The authors show that there exists a family of primitive operations that can be used to perform multiplicative interaction in neural network architectures. They show that these primitive operations can be applied to a wide range of neural networks, including dynamic convolutions, hypernetworks, gating, and attention layers. They also show that they can be combined with the concatenation operation, which is used in a number of existing neural networks to improve the performance of the network. ","This paper studies the multiplicative interactions in neural networks. The authors show that there exists a family of primitive operations that can be used to perform multiplicative interaction in neural network architectures. They show that these primitive operations can be applied to a wide range of neural networks, including dynamic convolutions, hypernetworks, gating, and attention layers. They also show that they can be combined with the concatenation operation, which is used in a number of existing neural networks to improve the performance of the network. "
22419,SP:5144391584e6d3825e12684b7c053e4e282cff2b,"algorithm USED-FOR batch active learning. deep neural network models USED-FOR algorithm. deep neural network models USED-FOR batch active learning. predictive uncertainty CONJUNCTION sample diversity. sample diversity CONJUNCTION predictive uncertainty. algorithm USED-FOR Batch Active learning. predictive uncertainty PART-OF strategy. Diverse Gradient Embeddings ( BADGE ) USED-FOR Batch Active learning. uncertainty CONJUNCTION diversity. diversity CONJUNCTION uncertainty. diversity EVALUATE-FOR BADGE. uncertainty EVALUATE-FOR BADGE. it USED-FOR real world active learning problems. approaches COMPARE BADGE. BADGE COMPARE approaches. approaches USED-FOR batch sizes. BADGE USED-FOR real world active learning problems. OtherScientificTerm are hallucinated gradient space, and hand - tuned hyperparameters. ","This paper proposes a novel algorithm for batch active learning. The proposed algorithm, called Diverse Gradient Embeddings (BADGE), is based on the idea of hallucinated gradient embedding (HGE), which is an extension of the recent work on hallucinated gradients. The key idea of HGE is to learn a neural network model that can be used to train a classifier with a large number of samples. The authors show that the proposed algorithm is able to achieve better sample diversity and predictive uncertainty compared to existing methods. They also show that BADGE can be applied to real-world active learning problems.","This paper proposes a novel algorithm for batch active learning. The proposed algorithm, called Diverse Gradient Embeddings (BADGE), is based on the idea of hallucinated gradient embedding (HGE), which is an extension of the recent work on hallucinated gradients. The key idea of HGE is to learn a neural network model that can be used to train a classifier with a large number of samples. The authors show that the proposed algorithm is able to achieve better sample diversity and predictive uncertainty compared to existing methods. They also show that BADGE can be applied to real-world active learning problems."
22428,SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"models USED-FOR decision making parameters. obscure feature extraction CONJUNCTION transformation process. transformation process CONJUNCTION obscure feature extraction. complex architectures CONJUNCTION obscure feature extraction. obscure feature extraction CONJUNCTION complex architectures. non - linearity FEATURE-OF activation functions. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. low level features FEATURE-OF hidden layer. high level features FEATURE-OF hidden layer. feature leveling architecture USED-FOR low level features. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. GLM layer PART-OF architecture. GLM layer USED-FOR feature leveling architecture. per - layer basis USED-FOR high level features. models COMPARE main - stream architectures. main - stream architectures COMPARE models. Method are Self - explaining models, General Linear Models ( GLMs ), deep neural networks ( DNNs ), and deep architectures. Task is model reasoning process. OtherScientificTerm is model weights. ",This paper studies the problem of self-explaining deep neural networks (DNNs). The authors propose a novel feature-leveling method for deep DNNs based on General Linear Models (GLMs). The main idea is to use a per-layer basis for low level features and per-level features for high level features. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on a number of benchmark datasets. ,This paper studies the problem of self-explaining deep neural networks (DNNs). The authors propose a novel feature-leveling method for deep DNNs based on General Linear Models (GLMs). The main idea is to use a per-layer basis for low level features and per-level features for high level features. The authors show that the proposed method outperforms the state-of-the-art in terms of performance on a number of benchmark datasets. 
22437,SP:b70ceead1bf6c7dc684c74501716e7012b891022,"gradient cost FEATURE-OF softmax regression. uniform negative sampling USED-FOR scalable softmax approximation. training method USED-FOR gradient signal. adversarial model USED-FOR data distribution. adversarial model USED-FOR negative samples. negative samples USED-FOR training method. adversarial sampling mechanism USED-FOR negative samples. adversarial sampling USED-FOR gradient variance. large scale data sets EVALUATE-FOR competitive baselines. adversarial sampling mechanism USED-FOR gradient updates. training time EVALUATE-FOR competitive baselines. non - uniform sampling USED-FOR bias. Method are classifier, and extreme classification. Task is technology. OtherScientificTerm is slow convergence. Metric is signal - to - noise ratio. ",This paper proposes a new adversarial training method for softmax regression. The proposed method is based on the idea of uniform negative sampling. The authors show that the proposed method can reduce the gradient variance and improve the training time. ,This paper proposes a new adversarial training method for softmax regression. The proposed method is based on the idea of uniform negative sampling. The authors show that the proposed method can reduce the gradient variance and improve the training time. 
22446,SP:29b52fee83309268d9864f3b1fc3617948577d41,approach USED-FOR efficient exploration. lowdimensional encoding of the environment USED-FOR approach. modelbased and model - free objectives USED-FOR lowdimensional encoding of the environment. weighted distance of nearest neighbors USED-FOR novelty. intrinsic rewards USED-FOR novelty. low dimensional representational space FEATURE-OF weighted distance of nearest neighbors. weighted distance of nearest neighbors USED-FOR intrinsic rewards. intrinsic rewards USED-FOR approach. intrinsic rewards USED-FOR sample - efficient exploration. intrinsic rewards USED-FOR planning routines. representational space FEATURE-OF planning routines. planning routines USED-FOR sample - efficient exploration. exploration approach COMPARE baselines. baselines COMPARE exploration approach. maze tasks CONJUNCTION control problem. control problem CONJUNCTION maze tasks. control problem EVALUATE-FOR approach. maze tasks EVALUATE-FOR approach. Metric is model accuracy. ,This paper proposes a low-dimensional encoding of the environment for efficient exploration. The key idea is to use a weighted distance of nearest neighbors as an intrinsic reward to encourage exploration in the low dimensional representation space. The proposed method is evaluated on a number of tasks and shows that it outperforms baselines in terms of sample efficiency. ,This paper proposes a low-dimensional encoding of the environment for efficient exploration. The key idea is to use a weighted distance of nearest neighbors as an intrinsic reward to encourage exploration in the low dimensional representation space. The proposed method is evaluated on a number of tasks and shows that it outperforms baselines in terms of sample efficiency. 
22455,SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,learning model USED-FOR few - shot classification. it USED-FOR out - of - distribution inputs. few - shot classification CONJUNCTION out - of - distribution detection. out - of - distribution detection CONJUNCTION few - shot classification. tasks USED-FOR outof - distribution detection. few - shot setting FEATURE-OF outof - distribution detection. few - shot classification datasets USED-FOR tasks. few - shot classification datasets USED-FOR benchmark datasets. methods USED-FOR task. benchmark datasets EVALUATE-FOR metrics. benchmark datasets EVALUATE-FOR baseline out - of - distribution detection. metrics USED-FOR baseline out - of - distribution detection. ,This paper proposes a few-shot learning model for out-of-distribution detection. The proposed method is based on the idea of learning a model that is able to detect out of distribution inputs. The model is trained on a set of benchmark datasets and is evaluated on several out of the distribution detection tasks. ,This paper proposes a few-shot learning model for out-of-distribution detection. The proposed method is based on the idea of learning a model that is able to detect out of distribution inputs. The model is trained on a set of benchmark datasets and is evaluated on several out of the distribution detection tasks. 
22464,SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,question - answering CONJUNCTION natural language inference. natural language inference CONJUNCTION question - answering. Undirected neural sequence models USED-FOR discriminative natural language understanding tasks. BERT HYPONYM-OF discriminative natural language understanding tasks. BERT HYPONYM-OF Undirected neural sequence models. natural language inference HYPONYM-OF discriminative natural language understanding tasks. question - answering HYPONYM-OF discriminative natural language understanding tasks. monotonic generation USED-FOR directed sequence models. models USED-FOR generating sequences. decoding PART-OF directed and undirected models. decoding PART-OF generalized model of sequence generation. framework USED-FOR generation. framework USED-FOR neural sequence models. refinement - based non - autoregressive models HYPONYM-OF neural sequence models. autoregressive HYPONYM-OF neural sequence models. decoding algorithms USED-FOR directed sequence models. decoding algorithms USED-FOR undirected models. decoding strategies USED-FOR cross - lingual masked translation model. framework USED-FOR undirected sequence models. approach USED-FOR constant - time translation. approach COMPARE linear - time translation. linear - time translation COMPARE approach. constant - time translation COMPARE linear - time translation. linear - time translation COMPARE constant - time translation. model USED-FOR linear - time translation. Material is WMT’14 English - German translation. Method is autoregressive model. ,This paper presents a framework for decoding and decoding of undirected neural sequence models. The authors propose a cross-lingual masked translation model that can be applied to both directed and unsupervised sequence generation. The model is based on a generalized model of sequence generation and decoding. The proposed model is evaluated on the WMT’14 English-German translation task and is shown to be competitive with the state-of-the-art. ,This paper presents a framework for decoding and decoding of undirected neural sequence models. The authors propose a cross-lingual masked translation model that can be applied to both directed and unsupervised sequence generation. The model is based on a generalized model of sequence generation and decoding. The proposed model is evaluated on the WMT’14 English-German translation task and is shown to be competitive with the state-of-the-art. 
22473,SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"real scenes USED-FOR MEs recognition. two - stage approach USED-FOR LaTeX sequence. printed mathematical expression image USED-FOR two - stage approach. method USED-FOR math symbols. object detection algorithm USED-FOR method. object detection algorithm USED-FOR math symbols. seq2seq model USED-FOR LaTeX sequences. attention mechanism USED-FOR seq2seq model. position information USED-FOR math symbols. two - stage method COMPARE end - to - end method. end - to - end method COMPARE two - stage method. ExpRate(expression recognition rate ) EVALUATE-FOR model. model COMPARE end - to - end model. end - to - end model COMPARE model. ExpRate(expression recognition rate ) EVALUATE-FOR end - to - end model. Task are mathematical expressions ( MEs ) recognition, and detection of mathematical symbols. Method is neutral network. OtherScientificTerm are mathematical symbols, and mathematical formulas. Metric are recognition accuracy, and generalization ability. ",This paper proposes a two-stage approach for mathematical expression recognition. The first stage is to learn a seq2seq model for LaTeX sequences. The second stage is the end-to-end method. Experiments show that the proposed method is able to achieve better performance than the end to end method.,This paper proposes a two-stage approach for mathematical expression recognition. The first stage is to learn a seq2seq model for LaTeX sequences. The second stage is the end-to-end method. Experiments show that the proposed method is able to achieve better performance than the end to end method.
22482,SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"memory footprint FEATURE-OF convolutional network architectures. loss reconstruction error FEATURE-OF in - domain inputs. it USED-FOR in - domain inputs. it USED-FOR loss reconstruction error. bytealigned codebooks USED-FOR compressed weights. method USED-FOR inference. unlabelled data USED-FOR quantization time. CPU USED-FOR inference. bytealigned codebooks USED-FOR method. unlabelled data USED-FOR method. bytealigned codebooks USED-FOR inference. top-1 accuracy EVALUATE-FOR approach. Method are vector quantization method, ResNet-50 model, and Mask R - CNN. OtherScientificTerm is memory size. Metric is compression factor. Task is ImageNet object classification. ","This paper proposes a vector quantization method to reduce the memory footprint of convolutional networks. The proposed method is based on the idea of byte-aligned codebooks, which can be used to compute the quantization time and the reconstruction error of in-domain inputs. The method is evaluated on ImageNet object classification tasks and achieves state-of-the-art performance.","This paper proposes a vector quantization method to reduce the memory footprint of convolutional networks. The proposed method is based on the idea of byte-aligned codebooks, which can be used to compute the quantization time and the reconstruction error of in-domain inputs. The method is evaluated on ImageNet object classification tasks and achieves state-of-the-art performance."
22491,SP:74850ad70241948f93fed95ba1f0ac11360437c1,Tensor - Product Representations USED-FOR explicit representation of relation structure. Tensor - Product Representations PART-OF Transformer. free - form math wordproblems FEATURE-OF Mathematics Dataset. Mathematics Dataset EVALUATE-FOR TensorProduct Transformer ( TP - Transformer ). TP - Attention HYPONYM-OF attention mechanism. attention USED-FOR ambiguities. TP - Transformer ’s attention maps USED-FOR it. Generic is model. Task is representation - building. Method is Pretrained models. ,This paper proposes a Transformer-based method for learning representations of relation structure in mathematics. The proposed method is based on the TensorProduct Transformer (TP-Transformer) framework. The authors show that the proposed method can be used to learn representations of the relation structure of mathematical word-problems. The method is evaluated on the Mathematics Dataset (MDP) and shows promising results. ,This paper proposes a Transformer-based method for learning representations of relation structure in mathematics. The proposed method is based on the TensorProduct Transformer (TP-Transformer) framework. The authors show that the proposed method can be used to learn representations of the relation structure of mathematical word-problems. The method is evaluated on the Mathematics Dataset (MDP) and shows promising results. 
22500,SP:d319df820c6630c409fab32097652a083e8f53ea,"identically distributed training and test sets EVALUATE-FOR Deep artificial neural networks. training and test accuracies EVALUATE-FOR Deep artificial neural networks. training and test sets COMPARE empirical sample set. empirical sample set COMPARE training and test sets. real - world input samples PART-OF empirical sample set. procedure USED-FOR source code. learning algorithm USED-FOR source code. procedure USED-FOR learning algorithm. Kolmogorov complexity USED-FOR universal cognitive similarity metric. information distance HYPONYM-OF universal cognitive similarity metric. optimization problem USED-FOR classification function. condition USED-FOR optimization problem. features USED-FOR classifier. empirical sample set FEATURE-OF training and test sets. model COMPARE model. model COMPARE model. model USED-FOR corruptions. corruptions CONJUNCTION adversarial perturbations. adversarial perturbations CONJUNCTION corruptions. corrupted or perturbed input features PART-OF empirical sample set. model USED-FOR adversarial perturbations. uncoded input features USED-FOR model. uncoded input features USED-FOR model. projected gradient descent USED-FOR adversarial perturbations. encoded input features USED-FOR model. Gaussian and shot noise HYPONYM-OF corruptions. Task are generalization, inference, and image classification. Metric is training and inference accuracies. OtherScientificTerm is channel codes. ",This paper studies the problem of learning a classifier that is robust to corruptions and adversarial perturbations. The authors propose to use the Kolmogorov complexity of the classifier as a measure of the similarity between the source code and the test set. They show that this is a universal cognitive similarity metric that can be used to measure the generalization performance of deep neural networks. They also show that the proposed method is able to generalize well to adversarial and corruptions. ,This paper studies the problem of learning a classifier that is robust to corruptions and adversarial perturbations. The authors propose to use the Kolmogorov complexity of the classifier as a measure of the similarity between the source code and the test set. They show that this is a universal cognitive similarity metric that can be used to measure the generalization performance of deep neural networks. They also show that the proposed method is able to generalize well to adversarial and corruptions. 
22509,SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,Deep Graph Neural Networks ( GNNs ) USED-FOR graph - based regression tasks. Deep Graph Neural Networks ( GNNs ) USED-FOR graph classification. graph classification CONJUNCTION graph - based regression tasks. graph - based regression tasks CONJUNCTION graph classification. graph pooling USED-FOR GNNs. GNNs USED-FOR graphs. HaarPooling HYPONYM-OF graph pooling operation. compressive Haar transforms USED-FOR graph pooling operation. sequential clusterings USED-FOR HaarPooling. compressive Haar basis USED-FOR clustering. compressive Haar basis USED-FOR pooling layer. HaarPooling USED-FOR fine detail information. compressive Haar transforms USED-FOR HaarPooling. synthesis of nodes USED-FOR HaarPooling. compressive Haar transforms USED-FOR fine detail information. transforms USED-FOR structure information. sparsity of the Haar basis USED-FOR HaarPooling. linear complexity FEATURE-OF HaarPooling. HaarPooling CONJUNCTION graph convolution layers. graph convolution layers CONJUNCTION HaarPooling. diverse graph classification problems EVALUATE-FOR GNN. graph convolution layers USED-FOR GNN. HaarPooling USED-FOR GNN. Generic is tasks. OtherScientificTerm is cluster. ,"This paper proposes a new graph pooling method, called HaarPooling, which is based on compressive Haar transforms. The authors show that the pooling operation can be decomposed into two parts: (1) sequential clusterings and (2) graph convolution layers. The proposed method is evaluated on a variety of graph classification and regression tasks. ","This paper proposes a new graph pooling method, called HaarPooling, which is based on compressive Haar transforms. The authors show that the pooling operation can be decomposed into two parts: (1) sequential clusterings and (2) graph convolution layers. The proposed method is evaluated on a variety of graph classification and regression tasks. "
22518,SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"Point clouds USED-FOR 3D objects. encoder networks USED-FOR semantics of their input point clouds. fully - connected networks USED-FOR shape representations. fully - connected networks USED-FOR point - cloud decoders. decoder architectures USED-FOR semantics of variable sized point clouds. sample - based point - cloud decoders USED-FOR shape representation. Metric is precision. OtherScientificTerm are point feature distribution, and sampled features. Method are sample - based decoder architectures, and feedforward architectures. ","This paper studies the problem of learning the semantics of variable-sized point clouds. In particular, the authors propose to use fully-connected point cloud decoders and feedforward architectures to learn the shape representation of the input point cloud. The authors show that sample-based point-cloud decoder architectures are able to achieve better performance than the state-of-the-art in terms of precision and accuracy. The paper also provides a theoretical analysis of the relationship between the precision of the decoder and the quality of the learned shape representation. ","This paper studies the problem of learning the semantics of variable-sized point clouds. In particular, the authors propose to use fully-connected point cloud decoders and feedforward architectures to learn the shape representation of the input point cloud. The authors show that sample-based point-cloud decoder architectures are able to achieve better performance than the state-of-the-art in terms of precision and accuracy. The paper also provides a theoretical analysis of the relationship between the precision of the decoder and the quality of the learned shape representation. "
22527,SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"controlled synthetic noise USED-FOR deep learning. controlled noise levels FEATURE-OF realworld noisy labels. Deep Neural Networks ( DNNs ) USED-FOR real - world noise. noisy data EVALUATE-FOR ImageNet architectures. Robust learning methods USED-FOR synthetic noise. Robust learning methods USED-FOR real - world noise. OtherScientificTerm are noise levels, networks, and Real - world noise. Material are benchmark of realworld noisy labels, and real - world noisy data. Method are DNNs, and robust DNN methods. ","This paper studies the problem of robustness to real-world noisy labels in deep learning. The authors propose a new benchmark of real world noisy labels, and show that robust DNNs can be trained on this benchmark. They also show that the robustness of robust learning methods can be improved by using synthetic noise. ","This paper studies the problem of robustness to real-world noisy labels in deep learning. The authors propose a new benchmark of real world noisy labels, and show that robust DNNs can be trained on this benchmark. They also show that the robustness of robust learning methods can be improved by using synthetic noise. "
22536,SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"pain - staking human supervision USED-FOR labeled data. rule - exemplar method USED-FOR collecting human supervision. it USED-FOR learning. training algorithm USED-FOR rules. coverage and label variables FEATURE-OF soft implication loss. latent coverage variables USED-FOR training algorithm. soft implication loss USED-FOR model. latent coverage variables USED-FOR rules. model USED-FOR inference. denoised rules CONJUNCTION model. model CONJUNCTION denoised rules. denoised rules USED-FOR inference. coupled rule - exemplar supervision USED-FOR denoising rules. algorithm COMPARE methods. methods COMPARE algorithm. tasks EVALUATE-FOR algorithm. tasks EVALUATE-FOR methods. clean and noisy supervision USED-FOR algorithm. clean and noisy supervision USED-FOR methods. OtherScientificTerm are human supervision, and supervision. ",This paper proposes a new method for learning denoising rules in the presence of human supervision. The proposed method is based on the idea of learning a set of denoised rules that can be used to train a model that is able to learn denoized rules. The method is evaluated on a number of tasks and shows that the proposed method outperforms the baselines. ,This paper proposes a new method for learning denoising rules in the presence of human supervision. The proposed method is based on the idea of learning a set of denoised rules that can be used to train a model that is able to learn denoized rules. The method is evaluated on a number of tasks and shows that the proposed method outperforms the baselines. 
22545,SP:6f2c656dbb7629f652a4291d6971625184d8118b,"Graph neural networks ( GNNs ) HYPONYM-OF deep models. memory layer USED-FOR GNNs. GNNs USED-FOR node representations. memory layer USED-FOR node representations. graph memory network ( GMN ) USED-FOR hierarchical graph representations. memory - based GNN ( MemGNN ) CONJUNCTION graph memory network ( GMN ). graph memory network ( GMN ) CONJUNCTION memory - based GNN ( MemGNN ). networks USED-FOR hierarchical graph representations. graph memory network ( GMN ) HYPONYM-OF networks. memory - based GNN ( MemGNN ) HYPONYM-OF networks. layer USED-FOR networks. graph classification and regression benchmarks EVALUATE-FOR models. chemical features PART-OF molecule data. chemical features FEATURE-OF representations. OtherScientificTerm are graphs, and graph. ",This paper proposes a memory-based graph memory network (MemGNN) for graph neural networks (GNNs). The proposed MemGNN is an extension of graph memory networks (GMN) that can be used for graph classification and regression tasks. The authors show that the proposed model outperforms the state-of-the-art GNNs on several benchmark datasets. ,This paper proposes a memory-based graph memory network (MemGNN) for graph neural networks (GNNs). The proposed MemGNN is an extension of graph memory networks (GMN) that can be used for graph classification and regression tasks. The authors show that the proposed model outperforms the state-of-the-art GNNs on several benchmark datasets. 
22554,SP:81bc52d734c86975d741b6482d65ca71a9d81620,"initial parameter values USED-FOR gradient - based optimization. convergence times CONJUNCTION model. model CONJUNCTION convergence times. gradient - based optimization USED-FOR deep neural networks. initialization USED-FOR deep linear networks. convergence COMPARE Gaussian initialization. Gaussian initialization COMPARE convergence. iid weights USED-FOR Gaussian initialization. Gaussian initializations USED-FOR convergence. initialization USED-FOR learning. dynamical isometry USED-FOR deep non - linear networks. Method are deep learning systems, initialization schemes, deep networks, and orthogonal initializations. OtherScientificTerm are orthogonal group, and global minimum. ","This paper studies the convergence of initialization schemes for deep neural networks. The authors show that orthogonal initialization is a generalization of Gaussian initialization for deep linear networks. They show that the global minimum of the initialization of a deep neural network is a function of the number of initializations. They also show that for deep non-linear networks, orthogonality of the initializations is a global minimum. ","This paper studies the convergence of initialization schemes for deep neural networks. The authors show that orthogonal initialization is a generalization of Gaussian initialization for deep linear networks. They show that the global minimum of the initialization of a deep neural network is a function of the number of initializations. They also show that for deep non-linear networks, orthogonality of the initializations is a global minimum. "
22563,SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"quantization methods USED-FOR deep neural networks. coarse quantization USED-FOR layers. 2 - bit quantization HYPONYM-OF high rate compression. additivity property FEATURE-OF deep neural networks. method USED-FOR optimization problem. Lagrangian Formulation USED-FOR method. joint framework USED-FOR optimal bit allocation problem. Lagrangian Formulation USED-FOR optimization problem. deep neural networks EVALUATE-FOR method. It USED-FOR deep CNN ResNet-50. accuracy loss EVALUATE-FOR It. Metric is accuracy. Generic is methods. OtherScientificTerm are equal bit rate, quantization, and additivity of output error. Task is deep CNNs compression. Method is deep CNNs. ","This paper studies the problem of quantization of deep neural networks. The authors propose a joint framework to optimize the bit allocation problem and the Lagrangian formulation of the optimization problem. The main contribution of this paper is to propose a new quantization method for deep neural network quantization. The proposed method is based on the idea that the quantization problem can be formulated as an optimization problem, and the authors show that the proposed method can achieve the optimal bit allocation. ","This paper studies the problem of quantization of deep neural networks. The authors propose a joint framework to optimize the bit allocation problem and the Lagrangian formulation of the optimization problem. The main contribution of this paper is to propose a new quantization method for deep neural network quantization. The proposed method is based on the idea that the quantization problem can be formulated as an optimization problem, and the authors show that the proposed method can achieve the optimal bit allocation. "
22572,SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"metric USED-FOR convergence. Wasserstein distance USED-FOR Wasserstein GAN ( WGAN ). auto - encoders CONJUNCTION WGANs. WGANs CONJUNCTION auto - encoders. framework USED-FOR auto - encoders. framework USED-FOR WGANs. encoder network CONJUNCTION generative network. generative network CONJUNCTION encoder network. encoder network PART-OF iWGAN. generative network PART-OF iWGAN. iterative primal dual optimization process USED-FOR generative network. iterative primal dual optimization process USED-FOR iWGAN. generalization error bound FEATURE-OF iWGANs. maximum likelihood estimation USED-FOR model. iWGAN COMPARE autoencoder GANs. autoencoder GANs COMPARE iWGAN. stopping criteria FEATURE-OF iWGAN. model USED-FOR convergence. model USED-FOR mode collapse. measurement of quality check EVALUATE-FOR model. benchmark datasets EVALUATE-FOR state - of - the - art. benchmark datasets EVALUATE-FOR iWGANs. Method are Generative Adversarial Networks ( GANs ), minmax two - player training of GANs, and inference WGAN ( iWGAN ) model. OtherScientificTerm is unstable training. ","This paper proposes a new method to improve the generalization performance of WGANs. The method is based on the Wasserstein distance, which is used to measure the convergence of a WGAN. The authors show that the proposed method is able to achieve better generalization error bounds than the state-of-the-art autoencoder GANs. They also show that their method is more robust to mode collapse and unstable training.","This paper proposes a new method to improve the generalization performance of WGANs. The method is based on the Wasserstein distance, which is used to measure the convergence of a WGAN. The authors show that the proposed method is able to achieve better generalization error bounds than the state-of-the-art autoencoder GANs. They also show that their method is more robust to mode collapse and unstable training."
22581,SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,coreference CONJUNCTION NLP. NLP CONJUNCTION coreference. Crowdsourcing COMPARE expert annotation. expert annotation COMPARE Crowdsourcing. classification tasks USED-FOR annotation. adjudication USED-FOR crowdsourcing. MPA USED-FOR sparsity. sparsity FEATURE-OF crowdsourcing environments. stick breaking process USED-FOR nonparametric partially pooled structure. large - scale crowdsourced anaphora dataset EVALUATE-FOR model. crowdsourcing setups FEATURE-OF model. annotation tasks USED-FOR classification. model USED-FOR annotation tasks. Task is anaphoric annotation. OtherScientificTerm is coreference chains. Generic is it. ,This paper proposes a method for crowdsourcing anaphoric annotation tasks. The method is based on the MPA framework. The authors propose to use a non-parametric partially pooled structure and a stick breaking process to reduce the sparsity of crowdsourcing datasets. The proposed method is evaluated on a large-scale crowdsourced anaphora dataset. ,This paper proposes a method for crowdsourcing anaphoric annotation tasks. The method is based on the MPA framework. The authors propose to use a non-parametric partially pooled structure and a stick breaking process to reduce the sparsity of crowdsourcing datasets. The proposed method is evaluated on a large-scale crowdsourced anaphora dataset. 
22597,SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"Exploration FEATURE-OF sparse reward reinforcement learning. intrinsic motivation USED-FOR sparse extrinsic reward signal. drives USED-FOR stabilize learning. exploration CONJUNCTION stabilize learning. stabilize learning CONJUNCTION exploration. successor feature control ( SFC ) HYPONYM-OF intrinsic reward. It COMPARE methods. methods COMPARE It. local information USED-FOR intrinsic motivation. statistics over complete trajectories USED-FOR It. local information USED-FOR methods. DeepMind Lab CONJUNCTION DeepMind Control Suite. DeepMind Control Suite CONJUNCTION DeepMind Lab. VizDoom CONJUNCTION DeepMind Lab. DeepMind Lab CONJUNCTION VizDoom. environments EVALUATE-FOR scheduled intrinsic drive ( SID ) agent. pure visual inputs USED-FOR environments. pure visual inputs USED-FOR scheduled intrinsic drive ( SID ) agent. DeepMind Lab HYPONYM-OF pure visual inputs. VizDoom HYPONYM-OF environments. DeepMind Control Suite HYPONYM-OF environments. DeepMind Lab HYPONYM-OF environments. exploration efficiency EVALUATE-FOR SFC. Generic is signals. OtherScientificTerm are bonus rewards, and intrinsic drives. Method are mixture policy, and intrinsic and extrinsic task policies. ","This paper proposes successor feature control (SFC), a method for sparse reward reinforcement learning that combines intrinsic and extrinsic reward. The intrinsic reward is learned by a mixture policy, where the intrinsic reward can be used to guide exploration, and the extrinsics are used to encourage the agent to explore. The authors show that the proposed method outperforms the baselines in terms of exploration efficiency, stability, and intrinsic reward. They also show that SFC is able to achieve better exploration efficiency than prior work. ","This paper proposes successor feature control (SFC), a method for sparse reward reinforcement learning that combines intrinsic and extrinsic reward. The intrinsic reward is learned by a mixture policy, where the intrinsic reward can be used to guide exploration, and the extrinsics are used to encourage the agent to explore. The authors show that the proposed method outperforms the baselines in terms of exploration efficiency, stability, and intrinsic reward. They also show that SFC is able to achieve better exploration efficiency than prior work. "
22613,SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"video - sentence pairs USED-FOR model. visual and language representations USED-FOR latent correspondence. multi - level co - attention mechanism USED-FOR multimodal representations. Frame - By - Word interaction module CONJUNCTION Word - Conditioned Visual Graph ( WCVG ). Word - Conditioned Visual Graph ( WCVG ) CONJUNCTION Frame - By - Word interaction module. Word - Conditioned Visual Graph ( WCVG ) PART-OF mechanism. Frame - By - Word interaction module PART-OF mechanism. positional encodings USED-FOR visual - semantic representations. positional encodings USED-FOR Transformers. iterative message - passing USED-FOR positional encodings. positional encodings PART-OF approach. iterative message - passing USED-FOR visual - semantic representations. wMAN model COMPARE weakly - supervised method. weakly - supervised method COMPARE wMAN model. DiDeMo and Charades - STA datasets EVALUATE-FOR representations. Recall@1 accuracy metric EVALUATE-FOR wMAN model. Task is weakly - supervised video moment retrieval. OtherScientificTerm are temporal annotations, and temporal sequence. Material is DiDeMo dataset. ",This paper proposes a multi-level co-attention mechanism for video moment retrieval. The proposed method is based on the Frame-By-Word interaction module and Word-Conditioned Visual Graph (WCVG) modules. The authors show that the proposed method achieves state-of-the-art performance on DiDeMo and Charades-STA datasets. ,This paper proposes a multi-level co-attention mechanism for video moment retrieval. The proposed method is based on the Frame-By-Word interaction module and Word-Conditioned Visual Graph (WCVG) modules. The authors show that the proposed method achieves state-of-the-art performance on DiDeMo and Charades-STA datasets. 
22629,SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"image - based rendering CONJUNCTION GAN - based image synthesis. GAN - based image synthesis CONJUNCTION image - based rendering. image - based rendering USED-FOR learned image - guided rendering technique. GAN - based image synthesis USED-FOR learned image - guided rendering technique. virtual showrooms CONJUNCTION virtual tours. virtual tours CONJUNCTION virtual showrooms. virtual tours CONJUNCTION digital inspection of historical artifacts. digital inspection of historical artifacts CONJUNCTION virtual tours. digital inspection of historical artifacts HYPONYM-OF virtual and augmented reality applications. virtual tours HYPONYM-OF virtual and augmented reality applications. virtual showrooms HYPONYM-OF virtual and augmented reality applications. handling of view - dependent effects PART-OF work. object - specific deep neural network USED-FOR view - dependent appearance. video USED-FOR proxy geometry. multi - view stereo USED-FOR proxy geometry. diffuse surfaces USED-FOR warping. specular highlights HYPONYM-OF view - dependent effects. deep neural network USED-FOR view - dependent effects. EffectsNet HYPONYM-OF deep neural network. pipeline USED-FOR view - dependent effects. composition network USED-FOR photo - realistic results. image - guided approach USED-FOR network. it USED-FOR appearance of captured images. real data EVALUATE-FOR approach. Generic are method, and estimations. OtherScientificTerm are 3D proxy, appearance, and diffuse images. ",This paper proposes a method to learn a 3D proxy for view-dependent effects. The method is based on a GAN-based image synthesis method and a learned image-guided rendering technique. The proposed method is evaluated on synthetic and real-world datasets. The paper shows that the proposed method achieves state-of-the-art results. ,This paper proposes a method to learn a 3D proxy for view-dependent effects. The method is based on a GAN-based image synthesis method and a learned image-guided rendering technique. The proposed method is evaluated on synthetic and real-world datasets. The paper shows that the proposed method achieves state-of-the-art results. 
22645,SP:257d124367b1da9a595dc11a9df750d6bade298e,"sparse representation of model uncertainty USED-FOR deep neural networks ( DNNs ). diagonal correction of the Kronecker - factored eigenbasis PART-OF scalable Laplace Approximation scheme. scalable Laplace Approximation scheme USED-FOR model uncertainty. operation USED-FOR full Bayesian analysis. low - rank approximation USED-FOR spectral sparsity. spectral sparsity FEATURE-OF DNNs. low - rank approximation USED-FOR eigenbasis. Methods USED-FOR sparsification. Methods USED-FOR memory - wise tractable sampling computations. approach COMPARE methods. methods COMPARE approach. OtherScientificTerm are information form, Kronecker - factored eigenbasis, and information matrix. Task is inversion of the information matrix. ",This paper proposes a scalable Laplace approximation scheme for the diagonal correction of the Kronecker-factored eigenbasis of deep neural networks (DNNs). The proposed method is based on the Laplace Approximation (LAP) algorithm. The authors show that the proposed method can be used to improve the memory-wise tractability of sampling computations in DNNs. They also show that their method is computationally tractable for memory-efficient sampling. ,This paper proposes a scalable Laplace approximation scheme for the diagonal correction of the Kronecker-factored eigenbasis of deep neural networks (DNNs). The proposed method is based on the Laplace Approximation (LAP) algorithm. The authors show that the proposed method can be used to improve the memory-wise tractability of sampling computations in DNNs. They also show that their method is computationally tractable for memory-efficient sampling. 
22661,SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"Minwise Hashing ( MinHash ) USED-FOR set similarities. compact high - dimensional data USED-FOR learning and searching. set similarities CONJUNCTION compact high - dimensional data. compact high - dimensional data CONJUNCTION set similarities. Minwise Hashing ( MinHash ) USED-FOR compact high - dimensional data. MinHash USED-FOR MinHash values. permutation ( hash function ) USED-FOR MinHash values. permutation ( hash function ) USED-FOR Permutation Hashing ( OPH ). strategies USED-FOR densification. densification HYPONYM-OF remedial strategy. Amortization Hashing ( AHash ) USED-FOR empty bins. Amortization Hashing ( AHash ) HYPONYM-OF load - balanced hashing. AHash COMPARE densification strategies. densification strategies COMPARE AHash. AHash COMPARE OPH. OPH COMPARE AHash. OPH CONJUNCTION densification strategies. densification strategies CONJUNCTION OPH. runtime efficiency EVALUATE-FOR densification strategies. runtime efficiency EVALUATE-FOR AHash. Material are high - dimensional data, and real datasets. OtherScientificTerm are bins, and unbalanced load. Task is false similarity computation. ","This paper proposes a new hashing method called Amortization Hashing (AHash) to address the problem of load-balanced hashing. AHash is a variant of Minwise Hashing, which is a popular hashing method for compact high-dimensional data. The main difference between AHash and MinHash is that AHash uses a permutation (hash function) instead of a min-hash function, which can be used to reduce the size of the bin. The authors also propose a new method called Permutation Hashing to improve the performance of AHash. Experiments are conducted on synthetic and real-world datasets.","This paper proposes a new hashing method called Amortization Hashing (AHash) to address the problem of load-balanced hashing. AHash is a variant of Minwise Hashing, which is a popular hashing method for compact high-dimensional data. The main difference between AHash and MinHash is that AHash uses a permutation (hash function) instead of a min-hash function, which can be used to reduce the size of the bin. The authors also propose a new method called Permutation Hashing to improve the performance of AHash. Experiments are conducted on synthetic and real-world datasets."
22677,SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,"feature extraction USED-FOR periodic signals. power generation CONJUNCTION industrial machine. industrial machine CONJUNCTION power generation. industrial machine CONJUNCTION robotic system. robotic system CONJUNCTION industrial machine. mechanized transportation vehicle CONJUNCTION power generation. power generation CONJUNCTION mechanized transportation vehicle. rotating shafts PART-OF robotic system. rotating shafts PART-OF mechanized transportation vehicle. multi - layer perceptron HYPONYM-OF methods. robust method USED-FOR features. machine learning architecture USED-FOR graph data. robust method USED-FOR phase shift data. cyclic permutation USED-FOR machine learning architecture. phase shift data USED-FOR features. cyclic permutation FEATURE-OF graph data. graph structure USED-FOR robust method. OtherScientificTerm are periodicity, shaft ’s rotation, Imprecise timing, phase shifts, and phase shift. ",This paper proposes a method to extract periodic signals from graph data using a multi-layer perceptron. The proposed method is based on the cyclic permutation of the graph structure. The authors show that the proposed method can be used to extract features from phase shift data. They also show that their method is robust to phase shift. ,This paper proposes a method to extract periodic signals from graph data using a multi-layer perceptron. The proposed method is based on the cyclic permutation of the graph structure. The authors show that the proposed method can be used to extract features from phase shift data. They also show that their method is robust to phase shift. 
22693,SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,them USED-FOR real world systems. controllability FEATURE-OF systems. precision USED-FOR real world systems. confidence score FEATURE-OF confidence oriented decoder. calibration technique USED-FOR faithful generation. calibration technique USED-FOR inference time. approach COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE approach. automatic metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic metrics. structured data - to - text dataset EVALUATE-FOR approach. structured data - to - text dataset EVALUATE-FOR state - of - the - art approaches. WikiBio HYPONYM-OF structured data - to - text dataset. human evaluation EVALUATE-FOR approach. human evaluation EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR approach. Method is Neural conditional text generation systems. OtherScientificTerm is variational Bayes objective. ,This paper proposes a confidence-oriented decoder for conditional text generation. The proposed method is based on the variational Bayes objective. The authors propose a calibration technique to reduce the inference time and improve the accuracy of the decoder. Experiments show that the proposed method outperforms state-of-the-art methods on WikiBio dataset.,This paper proposes a confidence-oriented decoder for conditional text generation. The proposed method is based on the variational Bayes objective. The authors propose a calibration technique to reduce the inference time and improve the accuracy of the decoder. Experiments show that the proposed method outperforms state-of-the-art methods on WikiBio dataset.
22709,SP:03307deac29173b2968fbd08f95fc77eb1f82410,Magnitude - based pruning USED-FOR pruning neural networks. magnitude - based pruning USED-FOR pruning modern architectures. Frobenius distortion FEATURE-OF linear operator. magnitude - based pruning USED-FOR Frobenius distortion. single layer optimization USED-FOR multi - layer optimization. single layer FEATURE-OF linear operator. lookahead pruning HYPONYM-OF pruning method. single layer optimization USED-FOR pruning method. method COMPARE magnitude - based pruning. magnitude - based pruning COMPARE method. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. networks EVALUATE-FOR magnitude - based pruning. networks EVALUATE-FOR method. VGG HYPONYM-OF networks. ResNet HYPONYM-OF networks. Method is neural networks. OtherScientificTerm is high - sparsity regime. ,This paper proposes a method for pruning neural networks based on magnitude-based pruning. The main idea is to use the Frobenius distortion of the linear operator as a single-layer optimization for multi-layer pruning in order to reduce the Frobensius distortion in the pruning process. The authors show that the proposed method is able to achieve better performance than the existing lookahead pruning method in the high-sparsity regime. ,This paper proposes a method for pruning neural networks based on magnitude-based pruning. The main idea is to use the Frobenius distortion of the linear operator as a single-layer optimization for multi-layer pruning in order to reduce the Frobensius distortion in the pruning process. The authors show that the proposed method is able to achieve better performance than the existing lookahead pruning method in the high-sparsity regime. 
22725,SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"parallel workers PART-OF graph. technique USED-FOR decentralized SGD. quantized communication USED-FOR technique. quantized communication USED-FOR decentralized SGD. asymptotic rate EVALUATE-FOR algorithm. Moniqua COMPARE algorithm. algorithm COMPARE Moniqua. full - precision communication USED-FOR algorithm. Moniqua COMPARE quantized decentralized algorithms. quantized decentralized algorithms COMPARE Moniqua. wall clock time EVALUATE-FOR Moniqua. bit - budgets FEATURE-OF Moniqua. 4 - bits - per - parameter communication USED-FOR Moniqua. CIFAR10 EVALUATE-FOR VGG16. Method is Decentralized stochastic gradient descent ( SGD ). OtherScientificTerm are memory, biased or linear quantizers, and convergence. Task is non - convex objectives. ","This paper studies the problem of decentralized stochastic gradient descent (SGD). The authors propose a new algorithm, called Moniqua, which uses quantized communication to improve the convergence rate of decentralized SGD. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on CIFAR-10, VGG16, and wall clock time. The main contribution of the paper is the introduction of a new quantization scheme, which is based on the idea that the communication between workers can be reduced to a fixed number of bits per parameter. The proposed method is evaluated on a number of benchmarks and shows that it outperforms existing decentralized algorithms.","This paper studies the problem of decentralized stochastic gradient descent (SGD). The authors propose a new algorithm, called Moniqua, which uses quantized communication to improve the convergence rate of decentralized SGD. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on CIFAR-10, VGG16, and wall clock time. The main contribution of the paper is the introduction of a new quantization scheme, which is based on the idea that the communication between workers can be reduced to a fixed number of bits per parameter. The proposed method is evaluated on a number of benchmarks and shows that it outperforms existing decentralized algorithms."
22741,SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"Method are reinforcement learning, and partial models. Generic are it, and they. Task is jointly modeling future observations. Material is images. ","This paper proposes a method for jointly modeling future observations in the context of reinforcement learning. The method is based on the idea of partial reinforcement learning, which is an extension of previous work on the problem of learning from partial models. The authors propose a method to learn a partial model that can be used to jointly model future observations. They show that the proposed method is able to achieve state-of-the-art performance on a number of tasks. ","This paper proposes a method for jointly modeling future observations in the context of reinforcement learning. The method is based on the idea of partial reinforcement learning, which is an extension of previous work on the problem of learning from partial models. The authors propose a method to learn a partial model that can be used to jointly model future observations. They show that the proposed method is able to achieve state-of-the-art performance on a number of tasks. "
22757,SP:c70479b2096a52584b242de58272ca8d8565feea,"succinct common representation of two correlated data variables USED-FOR conditional and joint generation tasks. variational autoencoder ( VAE ) model USED-FOR succinct common representation of two correlated data variables. distributed simulation CONJUNCTION channel synthesis. channel synthesis CONJUNCTION distributed simulation. information theoretic problems USED-FOR Wyner VAE model. channel synthesis HYPONYM-OF information theoretic problems. distributed simulation HYPONYM-OF information theoretic problems. common representation CONJUNCTION local representations. local representations CONJUNCTION common representation. Wyner VAE USED-FOR correlated data variables. mutual information FEATURE-OF regularization term. common representation PART-OF correlated data variables. local representations PART-OF Wyner VAE. shared concept HYPONYM-OF common representation. synthetic data CONJUNCTION real images. real images CONJUNCTION synthetic data. approach USED-FOR joint and conditional generation. real images USED-FOR style control. synthetic data USED-FOR style control. style control USED-FOR joint and conditional generation. VAE variants CONJUNCTION variational information bottleneck method. variational information bottleneck method CONJUNCTION VAE variants. model COMPARE VAE variants. VAE variants COMPARE model. model COMPARE variational information bottleneck method. variational information bottleneck method COMPARE model. succinct common representation USED-FOR generative. OtherScientificTerm are Wyner ’s common information, and data variables. ","This paper proposes a new variational autoencoder (VAE) model for joint and conditional generation tasks. The proposed model is based on the idea of mutual information between the shared concept and the local representation of two correlated data variables. The authors show that the proposed model can achieve better performance than existing VAE variants on both conditional and joint generation tasks, and can also be used for style control tasks. ","This paper proposes a new variational autoencoder (VAE) model for joint and conditional generation tasks. The proposed model is based on the idea of mutual information between the shared concept and the local representation of two correlated data variables. The authors show that the proposed model can achieve better performance than existing VAE variants on both conditional and joint generation tasks, and can also be used for style control tasks. "
