,paper_id,gen_sum
0,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,This paper proposes a new algorithm for federated learning (FL) based on selective quantized stochastic gradient descent (sqSGD). The proposed algorithm is motivated by the fact that the privacy-preserving quantization of FL algorithms is not always possible in practice. The authors propose to use a randomized rotation to reduce the quantization error of the gradients of the FL algorithm. They also propose a gradient subsampling strategy to improve the communication efficiency of the algorithm. The experimental results show that the proposed algorithm outperforms existing FL algorithms in terms of training performance and communication costs.
1,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,This paper proposes a new algorithm for federated learning (FL) based on selective quantized stochastic gradient descent (sqSGD). The proposed algorithm is motivated by the fact that the privacy-preserving quantization of FL algorithms is not always possible in practice. The authors propose to use a randomized rotation to reduce the quantization error of the gradients of the FL algorithm. They also propose a gradient subsampling strategy to improve the communication efficiency of the algorithm. The experimental results show that the proposed algorithm outperforms existing FL algorithms in terms of training performance and communication costs.
2,SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96,This paper proposes a new algorithm for federated learning (FL) based on selective quantized stochastic gradient descent (sqSGD). The proposed algorithm is motivated by the fact that the privacy-preserving quantization of FL algorithms is not always possible in practice. The authors propose to use a randomized rotation to reduce the quantization error of the gradients of the FL algorithm. They also propose a gradient subsampling strategy to improve the communication efficiency of the algorithm. The experimental results show that the proposed algorithm outperforms existing FL algorithms in terms of training performance and communication costs.
3,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a method for self-attention networks (SANs) that leverages prior knowledge to improve the performance of the model. The method is based on a combination of prior word frequency knowledge, prior translation lexicon knowledge, and prior bilingual data. The authors show that the proposed method is able to achieve better performance than the state-of-the-art Transformer-based baseline."
4,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a method for self-attention networks (SANs) that leverages prior knowledge to improve the performance of the model. The method is based on a combination of prior word frequency knowledge, prior translation lexicon knowledge, and prior bilingual data. The authors show that the proposed method is able to achieve better performance than the state-of-the-art Transformer-based baseline."
5,SP:632666b52c7c551d67fbbe70c06ed589c3a5e187,"This paper proposes a method for self-attention networks (SANs) that leverages prior knowledge to improve the performance of the model. The method is based on a combination of prior word frequency knowledge, prior translation lexicon knowledge, and prior bilingual data. The authors show that the proposed method is able to achieve better performance than the state-of-the-art Transformer-based baseline."
6,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,This paper proposes a Bayesian Stackelberg Markov Game (BSMG) model for moving target defense (MTD). BSMGs are a variant of Bayesian Markov Games (BMGs) that can be viewed as an extension of BSMG. The authors propose to use a reinforcement learning approach to learn the optimal movement policy for BSMGGs. The proposed method is evaluated on a variety of MTD domains and compared to other state-of-the-art methods.
7,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,This paper proposes a Bayesian Stackelberg Markov Game (BSMG) model for moving target defense (MTD). BSMGs are a variant of Bayesian Markov Games (BMGs) that can be viewed as an extension of BSMG. The authors propose to use a reinforcement learning approach to learn the optimal movement policy for BSMGGs. The proposed method is evaluated on a variety of MTD domains and compared to other state-of-the-art methods.
8,SP:e3e728837f26acb9da283a42c219b6c3b3e131cb,This paper proposes a Bayesian Stackelberg Markov Game (BSMG) model for moving target defense (MTD). BSMGs are a variant of Bayesian Markov Games (BMGs) that can be viewed as an extension of BSMG. The authors propose to use a reinforcement learning approach to learn the optimal movement policy for BSMGGs. The proposed method is evaluated on a variety of MTD domains and compared to other state-of-the-art methods.
9,SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper studies the problem of unsupervised disentangled representation learning in VAE-based models. The authors propose a method to disentangle the latent representations of VAE models using a signed permutation transformation. The method is motivated by the observation that VAE im-plementation choices can lead to PCA-like behavior of data sam4 ples. To address this issue, the authors propose to use multi-plea VAEs, which can be viewed as a VAE ensemble. The main contribution of the paper is to show that the proposed method can be applied to VAE ensembles, and that it is able to achieve better disentanglement performance than existing methods."
10,SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper studies the problem of unsupervised disentangled representation learning in VAE-based models. The authors propose a method to disentangle the latent representations of VAE models using a signed permutation transformation. The method is motivated by the observation that VAE im-plementation choices can lead to PCA-like behavior of data sam4 ples. To address this issue, the authors propose to use multi-plea VAEs, which can be viewed as a VAE ensemble. The main contribution of the paper is to show that the proposed method can be applied to VAE ensembles, and that it is able to achieve better disentanglement performance than existing methods."
11,SP:97911e02bf06b34d022e7548beb5169a1d825903,"This paper studies the problem of unsupervised disentangled representation learning in VAE-based models. The authors propose a method to disentangle the latent representations of VAE models using a signed permutation transformation. The method is motivated by the observation that VAE im-plementation choices can lead to PCA-like behavior of data sam4 ples. To address this issue, the authors propose to use multi-plea VAEs, which can be viewed as a VAE ensemble. The main contribution of the paper is to show that the proposed method can be applied to VAE ensembles, and that it is able to achieve better disentanglement performance than existing methods."
12,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach for automated machine learning (AutoML). The proposed method is based on a graph neural network and a meta-feature extractor. The proposed model is trained using a transformer-based language embedding and a free-text description model. The authors show that the proposed model outperforms the state-of-the-art models on a variety of tasks.
13,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach for automated machine learning (AutoML). The proposed method is based on a graph neural network and a meta-feature extractor. The proposed model is trained using a transformer-based language embedding and a free-text description model. The authors show that the proposed model outperforms the state-of-the-art models on a variety of tasks.
14,SP:59f9de3ebe4a04d2fc8778d8e3415bf85efb7822,This paper proposes a zero-shot approach for automated machine learning (AutoML). The proposed method is based on a graph neural network and a meta-feature extractor. The proposed model is trained using a transformer-based language embedding and a free-text description model. The authors show that the proposed model outperforms the state-of-the-art models on a variety of tasks.
15,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the problem of compositional generalization in neural network optimization. The authors argue that compositional learning is important for generalization, and propose a new method for learning compositional models. The method is based on the idea that the compositionality of a neural network can be achieved by learning a compositionality loss that is independent of the architecture of the network. "
16,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the problem of compositional generalization in neural network optimization. The authors argue that compositional learning is important for generalization, and propose a new method for learning compositional models. The method is based on the idea that the compositionality of a neural network can be achieved by learning a compositionality loss that is independent of the architecture of the network. "
17,SP:0f74dff929a4908405ebfa8e60fe1860eec6364f,"This paper studies the problem of compositional generalization in neural network optimization. The authors argue that compositional learning is important for generalization, and propose a new method for learning compositional models. The method is based on the idea that the compositionality of a neural network can be achieved by learning a compositionality loss that is independent of the architecture of the network. "
18,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,This paper proposes a method for entity alignment in Knowledge Graph (KG) representation learning. The proposed method is based on a scoring function that measures the distance between the embedding space of an entity and a pre-aligned entity in the KG. The authors propose to use a margin to measure the discrepancy between the representations of the two entities. The margin is defined as the difference between the representation of the pre-aligned entity and the embeddings of the aligned entity.  The authors show that the proposed method outperforms the state-of-the-art embedding-based entity alignment methods. 
19,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,This paper proposes a method for entity alignment in Knowledge Graph (KG) representation learning. The proposed method is based on a scoring function that measures the distance between the embedding space of an entity and a pre-aligned entity in the KG. The authors propose to use a margin to measure the discrepancy between the representations of the two entities. The margin is defined as the difference between the representation of the pre-aligned entity and the embeddings of the aligned entity.  The authors show that the proposed method outperforms the state-of-the-art embedding-based entity alignment methods. 
20,SP:f99a1b2dbcb7a7b30dbfcfc60668e94b4ad53410,This paper proposes a method for entity alignment in Knowledge Graph (KG) representation learning. The proposed method is based on a scoring function that measures the distance between the embedding space of an entity and a pre-aligned entity in the KG. The authors propose to use a margin to measure the discrepancy between the representations of the two entities. The margin is defined as the difference between the representation of the pre-aligned entity and the embeddings of the aligned entity.  The authors show that the proposed method outperforms the state-of-the-art embedding-based entity alignment methods. 
21,SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor-based method to reduce the storage and energy cost of a CNN-based imaging system, called SACoD. The proposed method is based on differential neural architecture search (DNS) and uses a mask-based architecture search method to find the best mask for a given sensor. The method is evaluated on the PhlatCam imaging system and compared to the state-of-the-art SOTA and SOTA-based methods."
22,SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor-based method to reduce the storage and energy cost of a CNN-based imaging system, called SACoD. The proposed method is based on differential neural architecture search (DNS) and uses a mask-based architecture search method to find the best mask for a given sensor. The method is evaluated on the PhlatCam imaging system and compared to the state-of-the-art SOTA and SOTA-based methods."
23,SP:0e42de72d10040289283516ec1bd324788f7d371,"This paper proposes a sensor-based method to reduce the storage and energy cost of a CNN-based imaging system, called SACoD. The proposed method is based on differential neural architecture search (DNS) and uses a mask-based architecture search method to find the best mask for a given sensor. The method is evaluated on the PhlatCam imaging system and compared to the state-of-the-art SOTA and SOTA-based methods."
24,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a method for estimating the average developmental path of a population of honey bees. The authors use a dataset of lifetime trajectories of individual bees and use a temporal matrix factorization model to estimate the average trajectory of each individual bee. The method is based on the fact that the number of individuals in the population is correlated with the size of the social network, and the authors show that the model is able to capture behavioral heterogeneity in the behavior of the bees. "
25,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a method for estimating the average developmental path of a population of honey bees. The authors use a dataset of lifetime trajectories of individual bees and use a temporal matrix factorization model to estimate the average trajectory of each individual bee. The method is based on the fact that the number of individuals in the population is correlated with the size of the social network, and the authors show that the model is able to capture behavioral heterogeneity in the behavior of the bees. "
26,SP:493afcfa3fd64967785928ba3acecf3ffa6ce579,"This paper proposes a method for estimating the average developmental path of a population of honey bees. The authors use a dataset of lifetime trajectories of individual bees and use a temporal matrix factorization model to estimate the average trajectory of each individual bee. The method is based on the fact that the number of individuals in the population is correlated with the size of the social network, and the authors show that the model is able to capture behavioral heterogeneity in the behavior of the bees. "
27,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation method for accelerated magnetic resonance imaging (MRI) reconstruction. The proposed method is based on the idea of data augmentations (DA), which is a popular method for image restoration and reconstruction in medical imaging. The main idea is to augment the data with invariances that are not present in the original data. The authors show that the proposed method can be applied to both the low-data and high-data regime of MRI reconstruction, and show that it is able to improve the performance on the fastMRI dataset."
28,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation method for accelerated magnetic resonance imaging (MRI) reconstruction. The proposed method is based on the idea of data augmentations (DA), which is a popular method for image restoration and reconstruction in medical imaging. The main idea is to augment the data with invariances that are not present in the original data. The authors show that the proposed method can be applied to both the low-data and high-data regime of MRI reconstruction, and show that it is able to improve the performance on the fastMRI dataset."
29,SP:08ae056f269c731b92b5a3d59e18f9ccfc0b703c,"This paper proposes a data augmentation method for accelerated magnetic resonance imaging (MRI) reconstruction. The proposed method is based on the idea of data augmentations (DA), which is a popular method for image restoration and reconstruction in medical imaging. The main idea is to augment the data with invariances that are not present in the original data. The authors show that the proposed method can be applied to both the low-data and high-data regime of MRI reconstruction, and show that it is able to improve the performance on the fastMRI dataset."
30,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a deep repulsive clustering (DRC) algorithm for order learning. The proposed algorithm is based on the order-identity decomposition (ORID) network, which decomposes order-related features and identity features into a repulsive term and a rank term. The authors demonstrate the effectiveness of the proposed algorithm on a variety of tasks, including age estimation, aesthetic score regression, and color image classification."
31,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a deep repulsive clustering (DRC) algorithm for order learning. The proposed algorithm is based on the order-identity decomposition (ORID) network, which decomposes order-related features and identity features into a repulsive term and a rank term. The authors demonstrate the effectiveness of the proposed algorithm on a variety of tasks, including age estimation, aesthetic score regression, and color image classification."
32,SP:3fdaae674a2b9d437a43d32778437dc7df9c1686,"This paper proposes a deep repulsive clustering (DRC) algorithm for order learning. The proposed algorithm is based on the order-identity decomposition (ORID) network, which decomposes order-related features and identity features into a repulsive term and a rank term. The authors demonstrate the effectiveness of the proposed algorithm on a variety of tasks, including age estimation, aesthetic score regression, and color image classification."
33,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a method for sparse MuJoCo tasks. The method is based on the idea of episode-level exploration, where the goal is to maximize the exploration score over the course of an episode. The authors show that the proposed method can outperform existing methods on MiniGrid environments."
34,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a method for sparse MuJoCo tasks. The method is based on the idea of episode-level exploration, where the goal is to maximize the exploration score over the course of an episode. The authors show that the proposed method can outperform existing methods on MiniGrid environments."
35,SP:a5775441639529d61b7fee4b4298fd82a0c93bb5,"This paper proposes a method for sparse MuJoCo tasks. The method is based on the idea of episode-level exploration, where the goal is to maximize the exploration score over the course of an episode. The authors show that the proposed method can outperform existing methods on MiniGrid environments."
36,SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes a method for zero-shot learning (ZSL) based on the Isometric Propagation Network (IPN). The main idea of the method is to learn a class-dependent representation of the image, which is then used to train an auto-generator that maps the image to a class representation. The method is evaluated on a variety of benchmark datasets and compared to a number of baselines."
37,SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes a method for zero-shot learning (ZSL) based on the Isometric Propagation Network (IPN). The main idea of the method is to learn a class-dependent representation of the image, which is then used to train an auto-generator that maps the image to a class representation. The method is evaluated on a variety of benchmark datasets and compared to a number of baselines."
38,SP:30024ac5aef153ae24c893a53bad93ead2526476,"This paper proposes a method for zero-shot learning (ZSL) based on the Isometric Propagation Network (IPN). The main idea of the method is to learn a class-dependent representation of the image, which is then used to train an auto-generator that maps the image to a class representation. The method is evaluated on a variety of benchmark datasets and compared to a number of baselines."
39,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a new architecture for multi-task learning based on hyper-grid transformers. The proposed architecture is based on a decomposable hyper-network, which can be used for both global and local task-agnostic tasks. The authors show that the proposed method is able to achieve state-of-the-art performance on GLUE and SuperGLUE tasks. "
40,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a new architecture for multi-task learning based on hyper-grid transformers. The proposed architecture is based on a decomposable hyper-network, which can be used for both global and local task-agnostic tasks. The authors show that the proposed method is able to achieve state-of-the-art performance on GLUE and SuperGLUE tasks. "
41,SP:1d7c174f4f7a0eb26edceecc117f9af1528802e5,"This paper proposes a new architecture for multi-task learning based on hyper-grid transformers. The proposed architecture is based on a decomposable hyper-network, which can be used for both global and local task-agnostic tasks. The authors show that the proposed method is able to achieve state-of-the-art performance on GLUE and SuperGLUE tasks. "
42,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper proposes a method for self-driving car training. The proposed method is based on a sensitivity analysis of the input image and the output of a neural network trained on the input. The method is evaluated on a variety of image classification tasks, and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of accuracy and robustness to adversarial attacks."
43,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper proposes a method for self-driving car training. The proposed method is based on a sensitivity analysis of the input image and the output of a neural network trained on the input. The method is evaluated on a variety of image classification tasks, and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of accuracy and robustness to adversarial attacks."
44,SP:d957241c02163c1c5bc03a688aa4a2eb486fb9f1,"This paper proposes a method for self-driving car training. The proposed method is based on a sensitivity analysis of the input image and the output of a neural network trained on the input. The method is evaluated on a variety of image classification tasks, and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of accuracy and robustness to adversarial attacks."
45,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), a method for solving problems with hard constraints. The main idea of DC3 is to use gradient-based corrections to address inequality constraints and equality constraints. In particular, DC3 uses a differentiable procedure to estimate the feasibility of the proposed method. Experiments on synthetic optimization tasks and AC optimal power flow show that DC3 can achieve near-optimal objective values."
46,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), a method for solving problems with hard constraints. The main idea of DC3 is to use gradient-based corrections to address inequality constraints and equality constraints. In particular, DC3 uses a differentiable procedure to estimate the feasibility of the proposed method. Experiments on synthetic optimization tasks and AC optimal power flow show that DC3 can achieve near-optimal objective values."
47,SP:2df9ba21f72e041f80c7bc9ecfe89353f172b058,"This paper proposes Deep Constraint Completion and Correction (DC3), a method for solving problems with hard constraints. The main idea of DC3 is to use gradient-based corrections to address inequality constraints and equality constraints. In particular, DC3 uses a differentiable procedure to estimate the feasibility of the proposed method. Experiments on synthetic optimization tasks and AC optimal power flow show that DC3 can achieve near-optimal objective values."
48,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper studies the problem of pruning deep neural networks in the presence of sparsity. The authors propose a new regularization method, L2 regularization, which is based on the Hessian approximation problem. The main idea of the proposed method is to use a growing penalty scheme to penalize the weights of the network during the pruning process. The proposed method can be applied to both structured and unstructured pruning, and is shown to outperform existing methods on CIFAR-10 and ImageNet."
49,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper studies the problem of pruning deep neural networks in the presence of sparsity. The authors propose a new regularization method, L2 regularization, which is based on the Hessian approximation problem. The main idea of the proposed method is to use a growing penalty scheme to penalize the weights of the network during the pruning process. The proposed method can be applied to both structured and unstructured pruning, and is shown to outperform existing methods on CIFAR-10 and ImageNet."
50,SP:61a0163b21dc8f92dd699c1e154f53d30c80b2fe,"This paper studies the problem of pruning deep neural networks in the presence of sparsity. The authors propose a new regularization method, L2 regularization, which is based on the Hessian approximation problem. The main idea of the proposed method is to use a growing penalty scheme to penalize the weights of the network during the pruning process. The proposed method can be applied to both structured and unstructured pruning, and is shown to outperform existing methods on CIFAR-10 and ImageNet."
51,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper proposes a model-based reinforcement learning (MBRL) algorithm, MuZero, that combines model-free reinforcement learning with planning. The main idea of MuZero is to use Monte-Carlo rollouts to learn a shallow tree, which is then used for policy updates. The experiments show that MuZero outperforms the state-of-the-art models on a variety of tasks, including zero-shot learning and Atari."
52,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper proposes a model-based reinforcement learning (MBRL) algorithm, MuZero, that combines model-free reinforcement learning with planning. The main idea of MuZero is to use Monte-Carlo rollouts to learn a shallow tree, which is then used for policy updates. The experiments show that MuZero outperforms the state-of-the-art models on a variety of tasks, including zero-shot learning and Atari."
53,SP:7b2bf0e36c926d1ed5ab9593a11e4ebce49df6ba,"This paper proposes a model-based reinforcement learning (MBRL) algorithm, MuZero, that combines model-free reinforcement learning with planning. The main idea of MuZero is to use Monte-Carlo rollouts to learn a shallow tree, which is then used for policy updates. The experiments show that MuZero outperforms the state-of-the-art models on a variety of tasks, including zero-shot learning and Atari."
54,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes Latent Value Iteration Networks (XLVINs), a deep reinforcement learning model for planning in Markov Decision Processes (MDPs). XLVIN is a combination of VINs and contrastive self-supervised learning (DSL). The main idea is to learn a latent representation of the state space of the MDP, which is then used to train a value iteration network (VIN) that iteratively iteratively updates the value of the current state in the latent space. The authors show that the proposed model outperforms the state-of-the-art VIN-like models on a variety of MDPs. "
55,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes Latent Value Iteration Networks (XLVINs), a deep reinforcement learning model for planning in Markov Decision Processes (MDPs). XLVIN is a combination of VINs and contrastive self-supervised learning (DSL). The main idea is to learn a latent representation of the state space of the MDP, which is then used to train a value iteration network (VIN) that iteratively iteratively updates the value of the current state in the latent space. The authors show that the proposed model outperforms the state-of-the-art VIN-like models on a variety of MDPs. "
56,SP:96afc34acb196af0b37f66ca9c89ae22ee7b6521,"This paper proposes Latent Value Iteration Networks (XLVINs), a deep reinforcement learning model for planning in Markov Decision Processes (MDPs). XLVIN is a combination of VINs and contrastive self-supervised learning (DSL). The main idea is to learn a latent representation of the state space of the MDP, which is then used to train a value iteration network (VIN) that iteratively iteratively updates the value of the current state in the latent space. The authors show that the proposed model outperforms the state-of-the-art VIN-like models on a variety of MDPs. "
57,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,This paper studies the problem of learning read-then-write (DNF) functions in the distribution free setting. The authors propose a novel method for learning DNFs that is based on a convex neural network and a gradient descent algorithm. The main contribution of the paper is a proof of the inductive bias of the learning process in the case of DNF functions. The proof relies on a computer assisted proof of a logical formula for the DNF. The paper also provides a theoretical analysis of the risk of learning a DNF under margin constraints.
58,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,This paper studies the problem of learning read-then-write (DNF) functions in the distribution free setting. The authors propose a novel method for learning DNFs that is based on a convex neural network and a gradient descent algorithm. The main contribution of the paper is a proof of the inductive bias of the learning process in the case of DNF functions. The proof relies on a computer assisted proof of a logical formula for the DNF. The paper also provides a theoretical analysis of the risk of learning a DNF under margin constraints.
59,SP:b0fa24ad48e7e60d6899bd799adcd03473cadd6e,This paper studies the problem of learning read-then-write (DNF) functions in the distribution free setting. The authors propose a novel method for learning DNFs that is based on a convex neural network and a gradient descent algorithm. The main contribution of the paper is a proof of the inductive bias of the learning process in the case of DNF functions. The proof relies on a computer assisted proof of a logical formula for the DNF. The paper also provides a theoretical analysis of the risk of learning a DNF under margin constraints.
60,SP:6e600bedbf995375fd41cc0b517ddefb918318af,This paper proposes a graph structured reinforcement learning (GSRL) algorithm for sparse reward functions. The authors propose to use a replay buffer to store historical trajectories and then use a graph attention strategy to map the state transitions to a dynamic graph. The proposed algorithm is evaluated on a variety of sparse reward tasks and compared to several baselines.
61,SP:6e600bedbf995375fd41cc0b517ddefb918318af,This paper proposes a graph structured reinforcement learning (GSRL) algorithm for sparse reward functions. The authors propose to use a replay buffer to store historical trajectories and then use a graph attention strategy to map the state transitions to a dynamic graph. The proposed algorithm is evaluated on a variety of sparse reward tasks and compared to several baselines.
62,SP:6e600bedbf995375fd41cc0b517ddefb918318af,This paper proposes a graph structured reinforcement learning (GSRL) algorithm for sparse reward functions. The authors propose to use a replay buffer to store historical trajectories and then use a graph attention strategy to map the state transitions to a dynamic graph. The proposed algorithm is evaluated on a variety of sparse reward tasks and compared to several baselines.
63,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method for systematic generalization of reinforcement learning agents in simulated environments. The method is based on the idea of prioritized level replay, which is a method that re-evaluates the state of an agent at different levels of training. The authors show that the proposed method is able to improve sample efficiency, generalization, and robustness to temporal-difference (TD) errors on Procgen and MiniGrid environments. "
64,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method for systematic generalization of reinforcement learning agents in simulated environments. The method is based on the idea of prioritized level replay, which is a method that re-evaluates the state of an agent at different levels of training. The authors show that the proposed method is able to improve sample efficiency, generalization, and robustness to temporal-difference (TD) errors on Procgen and MiniGrid environments. "
65,SP:80c62de18a6a7433c9728fe0d731f733bb89e898,"This paper proposes a method for systematic generalization of reinforcement learning agents in simulated environments. The method is based on the idea of prioritized level replay, which is a method that re-evaluates the state of an agent at different levels of training. The authors show that the proposed method is able to improve sample efficiency, generalization, and robustness to temporal-difference (TD) errors on Procgen and MiniGrid environments. "
66,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a model-agnostic framework for multi-task learning. The proposed method is based on a combination of automatic differentiation procedures and randomized singular value decomposition. The main contribution of this paper is to propose a novel algorithm that can be applied to both pre-training and multitask learning. In particular, the authors propose an auxiliary task gradients that are independent of the primary task loss and can be used as auxiliary updates to the main task loss. Experiments are conducted on both out-of-distribution and in-domain datasets and show the effectiveness of the proposed method."
67,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a model-agnostic framework for multi-task learning. The proposed method is based on a combination of automatic differentiation procedures and randomized singular value decomposition. The main contribution of this paper is to propose a novel algorithm that can be applied to both pre-training and multitask learning. In particular, the authors propose an auxiliary task gradients that are independent of the primary task loss and can be used as auxiliary updates to the main task loss. Experiments are conducted on both out-of-distribution and in-domain datasets and show the effectiveness of the proposed method."
68,SP:fd92d766a7721a411ff8c422bec18391d028fa78,"This paper proposes a model-agnostic framework for multi-task learning. The proposed method is based on a combination of automatic differentiation procedures and randomized singular value decomposition. The main contribution of this paper is to propose a novel algorithm that can be applied to both pre-training and multitask learning. In particular, the authors propose an auxiliary task gradients that are independent of the primary task loss and can be used as auxiliary updates to the main task loss. Experiments are conducted on both out-of-distribution and in-domain datasets and show the effectiveness of the proposed method."
69,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,This paper proposes a method for one-shot word-object binding in a 3D environment. The method is based on meta-learning and episodic memory. The authors propose to use a dual-coding external memory to store both short-term and long-term information. They also propose a memory writing mechanism to encourage intrinsic motivation. Experiments on ShapeNet show that the proposed method outperforms the baselines.
70,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,This paper proposes a method for one-shot word-object binding in a 3D environment. The method is based on meta-learning and episodic memory. The authors propose to use a dual-coding external memory to store both short-term and long-term information. They also propose a memory writing mechanism to encourage intrinsic motivation. Experiments on ShapeNet show that the proposed method outperforms the baselines.
71,SP:8eb8c34e56de137bfc32ea0fd8cd94e4bff5907d,This paper proposes a method for one-shot word-object binding in a 3D environment. The method is based on meta-learning and episodic memory. The authors propose to use a dual-coding external memory to store both short-term and long-term information. They also propose a memory writing mechanism to encourage intrinsic motivation. Experiments on ShapeNet show that the proposed method outperforms the baselines.
72,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of few-shot class-imbalance in supervised learning. In particular, the authors study the relationship between the dataset and support set imbalance. The authors show that the dataset level is more sensitive to the imbalance than the support set level, and propose a few strategies to address this issue. The experiments are conducted on CIFAR-10 and ImageNet."
73,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of few-shot class-imbalance in supervised learning. In particular, the authors study the relationship between the dataset and support set imbalance. The authors show that the dataset level is more sensitive to the imbalance than the support set level, and propose a few strategies to address this issue. The experiments are conducted on CIFAR-10 and ImageNet."
74,SP:9bd3d99bce743d356eb18692ef93365c78e5fcec,"This paper studies the problem of few-shot class-imbalance in supervised learning. In particular, the authors study the relationship between the dataset and support set imbalance. The authors show that the dataset level is more sensitive to the imbalance than the support set level, and propose a few strategies to address this issue. The experiments are conducted on CIFAR-10 and ImageNet."
75,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a novel graph convolutional neural network (GCN) architecture based on a neighborhood aggregating scheme. The proposed method is based on the Polynomial Graph Convolution (PGC) layer, which is an extension of the original GCN. In particular, the authors propose a new receptive field for the convolution operator, which allows for a wider receptive field. The authors show that the proposed method outperforms the existing GCN on several graph classification benchmarks."
76,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a novel graph convolutional neural network (GCN) architecture based on a neighborhood aggregating scheme. The proposed method is based on the Polynomial Graph Convolution (PGC) layer, which is an extension of the original GCN. In particular, the authors propose a new receptive field for the convolution operator, which allows for a wider receptive field. The authors show that the proposed method outperforms the existing GCN on several graph classification benchmarks."
77,SP:2a9cbbe3661d2f02f71472d0111f22a739412226,"This paper proposes a novel graph convolutional neural network (GCN) architecture based on a neighborhood aggregating scheme. The proposed method is based on the Polynomial Graph Convolution (PGC) layer, which is an extension of the original GCN. In particular, the authors propose a new receptive field for the convolution operator, which allows for a wider receptive field. The authors show that the proposed method outperforms the existing GCN on several graph classification benchmarks."
78,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes Sim2SG, a method for sim-to-real transfer of scene graph (SG) generation. The method is based on the observation that there is a large gap between real-world data and synthetic data. To bridge this gap, the authors propose to use supervised data from the real world to generate the scene graph, and then use the generated scene graph to train the model on the synthetic data to bridge the gap between the two domains. The authors evaluate the proposed method on toy simulators and realistic simulators, and show that the method outperforms the baselines."
79,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes Sim2SG, a method for sim-to-real transfer of scene graph (SG) generation. The method is based on the observation that there is a large gap between real-world data and synthetic data. To bridge this gap, the authors propose to use supervised data from the real world to generate the scene graph, and then use the generated scene graph to train the model on the synthetic data to bridge the gap between the two domains. The authors evaluate the proposed method on toy simulators and realistic simulators, and show that the method outperforms the baselines."
80,SP:b0a6873eb4bbf5cdc4a5dfa08782225ae91fc589,"This paper proposes Sim2SG, a method for sim-to-real transfer of scene graph (SG) generation. The method is based on the observation that there is a large gap between real-world data and synthetic data. To bridge this gap, the authors propose to use supervised data from the real world to generate the scene graph, and then use the generated scene graph to train the model on the synthetic data to bridge the gap between the two domains. The authors evaluate the proposed method on toy simulators and realistic simulators, and show that the method outperforms the baselines."
81,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,This paper proposes a model-free deep reinforcement learning algorithm called REDQ. The main idea of REDQ is to use a random subset of Q functions in the ensemble to minimize the in-target minimization. The authors show that REDQ outperforms model-based methods on MuJoCo benchmark.
82,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,This paper proposes a model-free deep reinforcement learning algorithm called REDQ. The main idea of REDQ is to use a random subset of Q functions in the ensemble to minimize the in-target minimization. The authors show that REDQ outperforms model-based methods on MuJoCo benchmark.
83,SP:ccc72f26d0637476d01671c147b5cb5d30fa8c2d,This paper proposes a model-free deep reinforcement learning algorithm called REDQ. The main idea of REDQ is to use a random subset of Q functions in the ensemble to minimize the in-target minimization. The authors show that REDQ outperforms model-based methods on MuJoCo benchmark.
84,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a meta-feature learning method for classification and regression tasks. The proposed method is based on the idea of meta-features (meta-features are features that are learned from a set of labeled data points) that are permuted by a fixed permutation of the features. The authors show that the proposed method can be applied to a wide range of tasks, including SVM, logistic regression, linear SGD, k-NN, and DATASET2VEC. The experiments are conducted on OpenML benchmarking suite, where the authors compare the performance of their proposed method with the state-of-the-art methods."
85,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a meta-feature learning method for classification and regression tasks. The proposed method is based on the idea of meta-features (meta-features are features that are learned from a set of labeled data points) that are permuted by a fixed permutation of the features. The authors show that the proposed method can be applied to a wide range of tasks, including SVM, logistic regression, linear SGD, k-NN, and DATASET2VEC. The experiments are conducted on OpenML benchmarking suite, where the authors compare the performance of their proposed method with the state-of-the-art methods."
86,SP:c424d050996a7f383d2f12418dfdcea90d94ea65,"This paper proposes a meta-feature learning method for classification and regression tasks. The proposed method is based on the idea of meta-features (meta-features are features that are learned from a set of labeled data points) that are permuted by a fixed permutation of the features. The authors show that the proposed method can be applied to a wide range of tasks, including SVM, logistic regression, linear SGD, k-NN, and DATASET2VEC. The experiments are conducted on OpenML benchmarking suite, where the authors compare the performance of their proposed method with the state-of-the-art methods."
87,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes a new graph learning method based on spectral graph densification (GRASPEL) for ultra-sparse undirected graphs. The proposed method is based on the graphLaplacian-like matrix of the graphical Lasso, which is a graph-based approximation of the precision matrix. The authors show that the proposed method can be used to reduce the dimensionality of the graph to a low-dimensional space. They also show that their method can reduce the number of spectrally critical edges in the graph. The experimental results on synthetic and real-world datasets show the effectiveness of the method."
88,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes a new graph learning method based on spectral graph densification (GRASPEL) for ultra-sparse undirected graphs. The proposed method is based on the graphLaplacian-like matrix of the graphical Lasso, which is a graph-based approximation of the precision matrix. The authors show that the proposed method can be used to reduce the dimensionality of the graph to a low-dimensional space. They also show that their method can reduce the number of spectrally critical edges in the graph. The experimental results on synthetic and real-world datasets show the effectiveness of the method."
89,SP:3e5d5b61dceca85c444b3d0d06577229c3146664,"This paper proposes a new graph learning method based on spectral graph densification (GRASPEL) for ultra-sparse undirected graphs. The proposed method is based on the graphLaplacian-like matrix of the graphical Lasso, which is a graph-based approximation of the precision matrix. The authors show that the proposed method can be used to reduce the dimensionality of the graph to a low-dimensional space. They also show that their method can reduce the number of spectrally critical edges in the graph. The experimental results on synthetic and real-world datasets show the effectiveness of the method."
90,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a method for unsupervised learning of a goal-conditioned policy. The method is based on intrinsic motivation (GPIM) and uses a discriminator to estimate the intrinsic reward function of the abstract-level policy and the goal-conditional policy. In particular, the discriminator is trained to predict the trajectory of the goal conditioned policy, which is then used as a latent variable for the abstract policy. Experiments on a variety of robotic tasks show that the proposed method outperforms the state-of-the-art."
91,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a method for unsupervised learning of a goal-conditioned policy. The method is based on intrinsic motivation (GPIM) and uses a discriminator to estimate the intrinsic reward function of the abstract-level policy and the goal-conditional policy. In particular, the discriminator is trained to predict the trajectory of the goal conditioned policy, which is then used as a latent variable for the abstract policy. Experiments on a variety of robotic tasks show that the proposed method outperforms the state-of-the-art."
92,SP:7e6c73a642a8b3d64156c1d0ecf11f84e7222a22,"This paper proposes a method for unsupervised learning of a goal-conditioned policy. The method is based on intrinsic motivation (GPIM) and uses a discriminator to estimate the intrinsic reward function of the abstract-level policy and the goal-conditional policy. In particular, the discriminator is trained to predict the trajectory of the goal conditioned policy, which is then used as a latent variable for the abstract policy. Experiments on a variety of robotic tasks show that the proposed method outperforms the state-of-the-art."
93,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,This paper studies the problem of policy switching in deep reinforcement learning (DRL). The authors propose a novel feature-switching criterion based on the feature distance between the policy and the representation learned by the Q-network. The authors show that the proposed feature switching criterion can be used to improve the sample efficiency of DRL algorithms.
94,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,This paper studies the problem of policy switching in deep reinforcement learning (DRL). The authors propose a novel feature-switching criterion based on the feature distance between the policy and the representation learned by the Q-network. The authors show that the proposed feature switching criterion can be used to improve the sample efficiency of DRL algorithms.
95,SP:bdf293bf2118a927cbec6b96be03bfcad0243640,This paper studies the problem of policy switching in deep reinforcement learning (DRL). The authors propose a novel feature-switching criterion based on the feature distance between the policy and the representation learned by the Q-network. The authors show that the proposed feature switching criterion can be used to improve the sample efficiency of DRL algorithms.
96,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper studies the problem of first-order stochastic gradient descent (H-SGD) for non-convex optimization problems. The main contribution of this paper is the introduction of a new homotopy parameter to the SGD algorithm. The authors show that this new parameter can be used to improve the global convergence rate of the algorithm. In particular, they show that the global linear rate of convergence of the proposed algorithm is faster than SGD."
97,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper studies the problem of first-order stochastic gradient descent (H-SGD) for non-convex optimization problems. The main contribution of this paper is the introduction of a new homotopy parameter to the SGD algorithm. The authors show that this new parameter can be used to improve the global convergence rate of the algorithm. In particular, they show that the global linear rate of convergence of the proposed algorithm is faster than SGD."
98,SP:d06bef9ee5e9bdda1571478b6a8a7a2d3ab42f1b,"This paper studies the problem of first-order stochastic gradient descent (H-SGD) for non-convex optimization problems. The main contribution of this paper is the introduction of a new homotopy parameter to the SGD algorithm. The authors show that this new parameter can be used to improve the global convergence rate of the algorithm. In particular, they show that the global linear rate of convergence of the proposed algorithm is faster than SGD."
99,SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning method for shape completion. The proposed method is based on the idea that the latent representation of the encoder and the posterior distribution of the decoder should be similar to each other. The authors propose to use a sparse point cloud to learn the latent representations of the two encoders, and then use the learned latent representations to train the meta-learner. The method is evaluated on the ICL-NUIM benchmark and shows better performance than the baselines."
100,SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning method for shape completion. The proposed method is based on the idea that the latent representation of the encoder and the posterior distribution of the decoder should be similar to each other. The authors propose to use a sparse point cloud to learn the latent representations of the two encoders, and then use the learned latent representations to train the meta-learner. The method is evaluated on the ICL-NUIM benchmark and shows better performance than the baselines."
101,SP:195d090d9df0bda33103edcbbaf300e43f4562be,"This paper proposes a meta-learning method for shape completion. The proposed method is based on the idea that the latent representation of the encoder and the posterior distribution of the decoder should be similar to each other. The authors propose to use a sparse point cloud to learn the latent representations of the two encoders, and then use the learned latent representations to train the meta-learner. The method is evaluated on the ICL-NUIM benchmark and shows better performance than the baselines."
102,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,This paper studies the activation magnitudes of adversarial examples in the intermediate layer of DNNs. The authors propose a defense adversarial training (CAS) method to reduce the activation magnitude of the adversarial example. The main contribution of this paper is to study the channel-wise activation of adversarially perturbed examples. The paper also provides a theoretical analysis of the effect of different activation magnitude on the robustness of the model.
103,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,This paper studies the activation magnitudes of adversarial examples in the intermediate layer of DNNs. The authors propose a defense adversarial training (CAS) method to reduce the activation magnitude of the adversarial example. The main contribution of this paper is to study the channel-wise activation of adversarially perturbed examples. The paper also provides a theoretical analysis of the effect of different activation magnitude on the robustness of the model.
104,SP:ca637a2692cf2424d1ec5c7d2051c7881a5816f4,This paper studies the activation magnitudes of adversarial examples in the intermediate layer of DNNs. The authors propose a defense adversarial training (CAS) method to reduce the activation magnitude of the adversarial example. The main contribution of this paper is to study the channel-wise activation of adversarially perturbed examples. The paper also provides a theoretical analysis of the effect of different activation magnitude on the robustness of the model.
105,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper studies the effect of random initialization on the convergence of gradient flow/descent in neural networks with infinite width. The authors show that the convergence rate of the gradient flow converges to a min-norm solution of the operator norm distance between the network parameters and the low-dimensional manifold. They also provide an upper-bound on the number of hidden layers in the network. Finally, the authors provide a theoretical analysis of the overparametrized single-hidden layer linear networks."
106,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper studies the effect of random initialization on the convergence of gradient flow/descent in neural networks with infinite width. The authors show that the convergence rate of the gradient flow converges to a min-norm solution of the operator norm distance between the network parameters and the low-dimensional manifold. They also provide an upper-bound on the number of hidden layers in the network. Finally, the authors provide a theoretical analysis of the overparametrized single-hidden layer linear networks."
107,SP:a50e9aeb17340b141f7b88d522911a5c9229f7d3,"This paper studies the effect of random initialization on the convergence of gradient flow/descent in neural networks with infinite width. The authors show that the convergence rate of the gradient flow converges to a min-norm solution of the operator norm distance between the network parameters and the low-dimensional manifold. They also provide an upper-bound on the number of hidden layers in the network. Finally, the authors provide a theoretical analysis of the overparametrized single-hidden layer linear networks."
108,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of kernels in deep networks. The authors propose a kernel framework for deep networks that can be viewed as a two-layer neural network with ReLU activations. The main contribution of this paper is to show that the eigenvalue decay of the integral operator of the kernel can be decomposed into two parts: (1) the kernel function is differentiable, and (2) it can be expressed as a function of the depth of the network and (3) the number of layers in the network. "
109,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of kernels in deep networks. The authors propose a kernel framework for deep networks that can be viewed as a two-layer neural network with ReLU activations. The main contribution of this paper is to show that the eigenvalue decay of the integral operator of the kernel can be decomposed into two parts: (1) the kernel function is differentiable, and (2) it can be expressed as a function of the depth of the network and (3) the number of layers in the network. "
110,SP:7341f8e456c0b80a59595f1cc145b776add3db3f,"This paper studies the approximation properties of kernels in deep networks. The authors propose a kernel framework for deep networks that can be viewed as a two-layer neural network with ReLU activations. The main contribution of this paper is to show that the eigenvalue decay of the integral operator of the kernel can be decomposed into two parts: (1) the kernel function is differentiable, and (2) it can be expressed as a function of the depth of the network and (3) the number of layers in the network. "
111,SP:3dd495394b880cf2fa055ee3fe218477625d2605,This paper studies the overestimation problem in deep reinforcement learning (DRL). The authors propose a method to improve the performance of DRL by combining the combined value of weighted critics and independent critics. The proposed method is evaluated on a variety of continuous control tasks.
112,SP:3dd495394b880cf2fa055ee3fe218477625d2605,This paper studies the overestimation problem in deep reinforcement learning (DRL). The authors propose a method to improve the performance of DRL by combining the combined value of weighted critics and independent critics. The proposed method is evaluated on a variety of continuous control tasks.
113,SP:3dd495394b880cf2fa055ee3fe218477625d2605,This paper studies the overestimation problem in deep reinforcement learning (DRL). The authors propose a method to improve the performance of DRL by combining the combined value of weighted critics and independent critics. The proposed method is evaluated on a variety of continuous control tasks.
114,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper studies inverse reinforcement learning (IRL), where the goal is to learn a policy that maximizes the probability distribution over reward functions. The authors propose a new formulation of the inverse problem, which they call the ill-posed inverse problem (ILP). The authors show that the proposed formulation can be used to solve the SIRL problem, and that it can be applied to the IRL problem as well."
115,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper studies inverse reinforcement learning (IRL), where the goal is to learn a policy that maximizes the probability distribution over reward functions. The authors propose a new formulation of the inverse problem, which they call the ill-posed inverse problem (ILP). The authors show that the proposed formulation can be used to solve the SIRL problem, and that it can be applied to the IRL problem as well."
116,SP:a7f72a5f99f2e3e1a643e9bb83bf0416a859ec06,"This paper studies inverse reinforcement learning (IRL), where the goal is to learn a policy that maximizes the probability distribution over reward functions. The authors propose a new formulation of the inverse problem, which they call the ill-posed inverse problem (ILP). The authors show that the proposed formulation can be used to solve the SIRL problem, and that it can be applied to the IRL problem as well."
117,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper studies the effect of self-training on the generalization of deep neural networks. The authors show that under certain assumptions on the margin and Lipschitzness of the network, self-trained neural networks can achieve better generalization bounds than linear models. They also show that the sample complexity of deep networks can be improved by using input consistency regularization, and show that this regularization can be used to improve the accuracy of the learned network. Finally, the authors provide a theoretical analysis of the impact of the regularization term on the performance of the trained network."
118,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper studies the effect of self-training on the generalization of deep neural networks. The authors show that under certain assumptions on the margin and Lipschitzness of the network, self-trained neural networks can achieve better generalization bounds than linear models. They also show that the sample complexity of deep networks can be improved by using input consistency regularization, and show that this regularization can be used to improve the accuracy of the learned network. Finally, the authors provide a theoretical analysis of the impact of the regularization term on the performance of the trained network."
119,SP:ee628e3ddc01de3f915b04834245c2250015e4d0,"This paper studies the effect of self-training on the generalization of deep neural networks. The authors show that under certain assumptions on the margin and Lipschitzness of the network, self-trained neural networks can achieve better generalization bounds than linear models. They also show that the sample complexity of deep networks can be improved by using input consistency regularization, and show that this regularization can be used to improve the accuracy of the learned network. Finally, the authors provide a theoretical analysis of the impact of the regularization term on the performance of the trained network."
120,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method for video prediction based on a stochastic recurrent estimator for long-term video prediction. The proposed method is based on the idea that video is a discrete semantic structure space, which can be represented as a set of discrete semantic structures. The method is evaluated on video prediction tasks such as car driving, dancing, and video translation."
121,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method for video prediction based on a stochastic recurrent estimator for long-term video prediction. The proposed method is based on the idea that video is a discrete semantic structure space, which can be represented as a set of discrete semantic structures. The method is evaluated on video prediction tasks such as car driving, dancing, and video translation."
122,SP:daa229d78712808420aad4c50604fc28fd2a4aba,"This paper proposes a method for video prediction based on a stochastic recurrent estimator for long-term video prediction. The proposed method is based on the idea that video is a discrete semantic structure space, which can be represented as a set of discrete semantic structures. The method is evaluated on video prediction tasks such as car driving, dancing, and video translation."
123,SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a method for learning a high-level representation of graph nodes in the form of a fractal set of IFSs. The proposed method is based on the Iterated Function System (IFS) which is a dynamical system that can be viewed as an extension of the iterated function system. The main idea is to use the affine transformation of the IFS as an adjoint probability vector to the graph neural network (GNN) layer. The authors show that the proposed method can be applied to semi-supervised node classification on Cora, PubMed, and citeser datasets."
124,SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a method for learning a high-level representation of graph nodes in the form of a fractal set of IFSs. The proposed method is based on the Iterated Function System (IFS) which is a dynamical system that can be viewed as an extension of the iterated function system. The main idea is to use the affine transformation of the IFS as an adjoint probability vector to the graph neural network (GNN) layer. The authors show that the proposed method can be applied to semi-supervised node classification on Cora, PubMed, and citeser datasets."
125,SP:e50b1931800daa7de577efd3edca523771227b3f,"This paper proposes a method for learning a high-level representation of graph nodes in the form of a fractal set of IFSs. The proposed method is based on the Iterated Function System (IFS) which is a dynamical system that can be viewed as an extension of the iterated function system. The main idea is to use the affine transformation of the IFS as an adjoint probability vector to the graph neural network (GNN) layer. The authors show that the proposed method can be applied to semi-supervised node classification on Cora, PubMed, and citeser datasets."
126,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a generalization of Wasserstein GAN (Wasserstein Generative Adversarial Network) for graph generation. The main contribution of this paper is the introduction of a geometric interpretation of GG-GAN, which allows the model to be applied to graphs with geometric interpretation. The authors show that the proposed method can be used to generate graphs that are isomorphic to isomorphic graphs. The proposed method is evaluated on a variety of graph generation tasks."
127,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a generalization of Wasserstein GAN (Wasserstein Generative Adversarial Network) for graph generation. The main contribution of this paper is the introduction of a geometric interpretation of GG-GAN, which allows the model to be applied to graphs with geometric interpretation. The authors show that the proposed method can be used to generate graphs that are isomorphic to isomorphic graphs. The proposed method is evaluated on a variety of graph generation tasks."
128,SP:89d65999a0600ec4f81daf6232fb5897676b3ce3,"This paper proposes a generalization of Wasserstein GAN (Wasserstein Generative Adversarial Network) for graph generation. The main contribution of this paper is the introduction of a geometric interpretation of GG-GAN, which allows the model to be applied to graphs with geometric interpretation. The authors show that the proposed method can be used to generate graphs that are isomorphic to isomorphic graphs. The proposed method is evaluated on a variety of graph generation tasks."
129,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper studies the problem of learning noise-robust rules for neural networks. The authors propose a new algorithm, EXPLAINN, which is based on the Minimum Description Length principle. The main idea of the algorithm is to use the minimum description length principle to estimate the activation values of a neural network, and then use a super-charge prototyping method to learn a set of rules that are robust to noise. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets to show the effectiveness of the proposed algorithm."
130,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper studies the problem of learning noise-robust rules for neural networks. The authors propose a new algorithm, EXPLAINN, which is based on the Minimum Description Length principle. The main idea of the algorithm is to use the minimum description length principle to estimate the activation values of a neural network, and then use a super-charge prototyping method to learn a set of rules that are robust to noise. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets to show the effectiveness of the proposed algorithm."
131,SP:4f9388c18e44995fb1c6830256c520ff47a2e6ee,"This paper studies the problem of learning noise-robust rules for neural networks. The authors propose a new algorithm, EXPLAINN, which is based on the Minimum Description Length principle. The main idea of the algorithm is to use the minimum description length principle to estimate the activation values of a neural network, and then use a super-charge prototyping method to learn a set of rules that are robust to noise. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets to show the effectiveness of the proposed algorithm."
132,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,This paper studies the problem of pessimistic policy optimization in fixed-dataset settings. The authors propose a pessimism principle for the case where the value of the policy is overestimated by a factor of at least $O(1/\epsilon)$. The authors show that pessimistic algorithms can be used to improve the worst-case performance of policy optimization algorithms in this regime. They also provide a theoretical analysis of the pessimistic algorithms.
133,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,This paper studies the problem of pessimistic policy optimization in fixed-dataset settings. The authors propose a pessimism principle for the case where the value of the policy is overestimated by a factor of at least $O(1/\epsilon)$. The authors show that pessimistic algorithms can be used to improve the worst-case performance of policy optimization algorithms in this regime. They also provide a theoretical analysis of the pessimistic algorithms.
134,SP:fc75d8d62ac5cc4cdde1b923ae54659a0dfba28b,This paper studies the problem of pessimistic policy optimization in fixed-dataset settings. The authors propose a pessimism principle for the case where the value of the policy is overestimated by a factor of at least $O(1/\epsilon)$. The authors show that pessimistic algorithms can be used to improve the worst-case performance of policy optimization algorithms in this regime. They also provide a theoretical analysis of the pessimistic algorithms.
135,SP:363661edd15a06a800b51abc1541a3191311ee0e,This paper proposes an asynchronous leapfrog (ALF) solver for Neural ODEs. The main contribution of this paper is the use of adaptive checkpoint adjoint method (ACA) to reduce the memory cost of the ALF Integrator (MALI). The authors show that the proposed method can achieve better accuracy than the naive ALF integrator (ALI) while maintaining the same memory cost. 
136,SP:363661edd15a06a800b51abc1541a3191311ee0e,This paper proposes an asynchronous leapfrog (ALF) solver for Neural ODEs. The main contribution of this paper is the use of adaptive checkpoint adjoint method (ACA) to reduce the memory cost of the ALF Integrator (MALI). The authors show that the proposed method can achieve better accuracy than the naive ALF integrator (ALI) while maintaining the same memory cost. 
137,SP:363661edd15a06a800b51abc1541a3191311ee0e,This paper proposes an asynchronous leapfrog (ALF) solver for Neural ODEs. The main contribution of this paper is the use of adaptive checkpoint adjoint method (ACA) to reduce the memory cost of the ALF Integrator (MALI). The authors show that the proposed method can achieve better accuracy than the naive ALF integrator (ALI) while maintaining the same memory cost. 
138,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a new method for scene conditional generation based on compositionality. In particular, the authors propose a new loss function, which is based on the idea of scene-graph perceptual similarity, to improve the quality of the generated images. The authors also propose to use instance-wise spatial conditioning normalizations to further improve the robustness of unseen conditionings. Experiments show that the proposed method is able to generate images that are more robust to image quality degradation."
139,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a new method for scene conditional generation based on compositionality. In particular, the authors propose a new loss function, which is based on the idea of scene-graph perceptual similarity, to improve the quality of the generated images. The authors also propose to use instance-wise spatial conditioning normalizations to further improve the robustness of unseen conditionings. Experiments show that the proposed method is able to generate images that are more robust to image quality degradation."
140,SP:45b6d522ed9a2ecda2db0a3d45688ed3b0f32875,"This paper proposes a new method for scene conditional generation based on compositionality. In particular, the authors propose a new loss function, which is based on the idea of scene-graph perceptual similarity, to improve the quality of the generated images. The authors also propose to use instance-wise spatial conditioning normalizations to further improve the robustness of unseen conditionings. Experiments show that the proposed method is able to generate images that are more robust to image quality degradation."
141,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,This paper proposes a generalization of Graph Neural Networks (GNNs) to non-isomorphic graphs. The main contribution of this paper is the introduction of a new operator family of GNNs that can be used to learn node-wise functions. The proposed GA-MLP can be viewed as an extension of the Graph Neural Network (GAN) family of MLPs. The authors show that the proposed operator family is more expressive than the original GNN family and can be applied to a wide range of graph isomorphism testing tasks. 
142,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,This paper proposes a generalization of Graph Neural Networks (GNNs) to non-isomorphic graphs. The main contribution of this paper is the introduction of a new operator family of GNNs that can be used to learn node-wise functions. The proposed GA-MLP can be viewed as an extension of the Graph Neural Network (GAN) family of MLPs. The authors show that the proposed operator family is more expressive than the original GNN family and can be applied to a wide range of graph isomorphism testing tasks. 
143,SP:77bce8c5d383f6be82ebc694bf66fb1a408ad751,This paper proposes a generalization of Graph Neural Networks (GNNs) to non-isomorphic graphs. The main contribution of this paper is the introduction of a new operator family of GNNs that can be used to learn node-wise functions. The proposed GA-MLP can be viewed as an extension of the Graph Neural Network (GAN) family of MLPs. The authors show that the proposed operator family is more expressive than the original GNN family and can be applied to a wide range of graph isomorphism testing tasks. 
144,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes an actor-learner distillation method for distributed reinforcement learning. The main idea of the method is to distill the actor model into a transformer model, and then use the transformer model to train the actor. The authors show that the proposed method is able to reduce the computational complexity of the LSTM and the transformer models."
145,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes an actor-learner distillation method for distributed reinforcement learning. The main idea of the method is to distill the actor model into a transformer model, and then use the transformer model to train the actor. The authors show that the proposed method is able to reduce the computational complexity of the LSTM and the transformer models."
146,SP:5c0783e92017fc808ebd44a7d1aa7f6b92baacd8,"This paper proposes an actor-learner distillation method for distributed reinforcement learning. The main idea of the method is to distill the actor model into a transformer model, and then use the transformer model to train the actor. The authors show that the proposed method is able to reduce the computational complexity of the LSTM and the transformer models."
147,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a universal representation transformer (URT) for few-shot image classification. The proposed URT is based on the idea that universal features can be used to represent diverse data sources. The authors also propose a heatmap-based method to evaluate the performance of URT on the Meta-Dataset. The experimental results show that the proposed method outperforms the baselines.
148,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a universal representation transformer (URT) for few-shot image classification. The proposed URT is based on the idea that universal features can be used to represent diverse data sources. The authors also propose a heatmap-based method to evaluate the performance of URT on the Meta-Dataset. The experimental results show that the proposed method outperforms the baselines.
149,SP:ccd59c3acb3d0886030451bbaea68fb83ef4dfa5,This paper proposes a universal representation transformer (URT) for few-shot image classification. The proposed URT is based on the idea that universal features can be used to represent diverse data sources. The authors also propose a heatmap-based method to evaluate the performance of URT on the Meta-Dataset. The experimental results show that the proposed method outperforms the baselines.
150,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes a self-taught associative memory (STAM) module for unsupervised Progressive Learning (UPL) problems. The proposed STAM module consists of three modules: (1) an online clustering module, (2) a novelty detection module, and (3) a hierarchical memory module. The novelty detection and novelty detection modules are based on the self-attention module, while the hierarchy module is based on STAM. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets."
151,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes a self-taught associative memory (STAM) module for unsupervised Progressive Learning (UPL) problems. The proposed STAM module consists of three modules: (1) an online clustering module, (2) a novelty detection module, and (3) a hierarchical memory module. The novelty detection and novelty detection modules are based on the self-attention module, while the hierarchy module is based on STAM. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets."
152,SP:beaa3dfef4bdf3d8fea64d4cf86911f45edd2873,"This paper proposes a self-taught associative memory (STAM) module for unsupervised Progressive Learning (UPL) problems. The proposed STAM module consists of three modules: (1) an online clustering module, (2) a novelty detection module, and (3) a hierarchical memory module. The novelty detection and novelty detection modules are based on the self-attention module, while the hierarchy module is based on STAM. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets."
153,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between decentralized and centralized deep learning models in the presence of large compute clusters. In particular, the authors study the effect of network size, communication topology, data partitioning, and learning rate on generalization. The authors show that decentralized networks tend to be more general than centralized networks in terms of generalization error, while centralized networks are more sensitive to the size of the compute cluster. "
154,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between decentralized and centralized deep learning models in the presence of large compute clusters. In particular, the authors study the effect of network size, communication topology, data partitioning, and learning rate on generalization. The authors show that decentralized networks tend to be more general than centralized networks in terms of generalization error, while centralized networks are more sensitive to the size of the compute cluster. "
155,SP:f7a8e5a580524d54f4a0cd08bd3cb0a0a074528b,"This paper studies the generalization gap between decentralized and centralized deep learning models in the presence of large compute clusters. In particular, the authors study the effect of network size, communication topology, data partitioning, and learning rate on generalization. The authors show that decentralized networks tend to be more general than centralized networks in terms of generalization error, while centralized networks are more sensitive to the size of the compute cluster. "
156,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,"This paper proposes a siamese recurrent neural network (RNN) model for learning the distance between similar sequences. The model is based on the Gated Recurrent Unit (GURU) model, which is an extension of the classical Gated RNN. The key idea of the RNN is to use a dynamical system to model the dynamics of the sequence, and then use a RNN to predict the dynamics. The proposed RNN model is evaluated on the activity recognition dataset, where it is shown to outperform the baselines."
157,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,"This paper proposes a siamese recurrent neural network (RNN) model for learning the distance between similar sequences. The model is based on the Gated Recurrent Unit (GURU) model, which is an extension of the classical Gated RNN. The key idea of the RNN is to use a dynamical system to model the dynamics of the sequence, and then use a RNN to predict the dynamics. The proposed RNN model is evaluated on the activity recognition dataset, where it is shown to outperform the baselines."
158,SP:08ab81a53ae0b51b214442f2f9d6edca0df9118c,"This paper proposes a siamese recurrent neural network (RNN) model for learning the distance between similar sequences. The model is based on the Gated Recurrent Unit (GURU) model, which is an extension of the classical Gated RNN. The key idea of the RNN is to use a dynamical system to model the dynamics of the sequence, and then use a RNN to predict the dynamics. The proposed RNN model is evaluated on the activity recognition dataset, where it is shown to outperform the baselines."
159,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation (codistillation) on the performance of distributed training in a distributed setting. In particular, the authors study the impact of different batch sizes, learning rate schedules, and batch sizes on the accuracy of the model. The authors show that the performance degradation of the models with large batch sizes and large learning rate schedule can be attributed to the use of the auxiliary loss, and propose a solution to address this issue. The proposed solution is a combination of two techniques: (1) adding a regularization term to the loss function, and (2) adding an auxiliary loss to the original loss function. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets."
160,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation (codistillation) on the performance of distributed training in a distributed setting. In particular, the authors study the impact of different batch sizes, learning rate schedules, and batch sizes on the accuracy of the model. The authors show that the performance degradation of the models with large batch sizes and large learning rate schedule can be attributed to the use of the auxiliary loss, and propose a solution to address this issue. The proposed solution is a combination of two techniques: (1) adding a regularization term to the loss function, and (2) adding an auxiliary loss to the original loss function. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets."
161,SP:e32bb6044bcb26cad3b0161db19170d726c40fae,"This paper studies the effect of codistillation (codistillation) on the performance of distributed training in a distributed setting. In particular, the authors study the impact of different batch sizes, learning rate schedules, and batch sizes on the accuracy of the model. The authors show that the performance degradation of the models with large batch sizes and large learning rate schedule can be attributed to the use of the auxiliary loss, and propose a solution to address this issue. The proposed solution is a combination of two techniques: (1) adding a regularization term to the loss function, and (2) adding an auxiliary loss to the original loss function. Experiments are conducted on MNIST, CIFAR-10, and CelebA datasets."
162,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of stochastic gradient descent (SGD) for linear regression on Gaussian data. The authors show that SGD can converge to a heavy-tailed stationary distribution when the batch size b is larger than a certain threshold. They show that this is due to the fact that the eigenvalues of the Hessian of the network weights depend on the tail-index of the weights, which is a function of the number of iterations of the SGD algorithm. They also show that if the weights are sufficiently large, then SGD converges to a local minimum."
163,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of stochastic gradient descent (SGD) for linear regression on Gaussian data. The authors show that SGD can converge to a heavy-tailed stationary distribution when the batch size b is larger than a certain threshold. They show that this is due to the fact that the eigenvalues of the Hessian of the network weights depend on the tail-index of the weights, which is a function of the number of iterations of the SGD algorithm. They also show that if the weights are sufficiently large, then SGD converges to a local minimum."
164,SP:cd03bc0b12cf44e9d538d274de7dfe44acdb1e35,"This paper studies the generalization properties of stochastic gradient descent (SGD) for linear regression on Gaussian data. The authors show that SGD can converge to a heavy-tailed stationary distribution when the batch size b is larger than a certain threshold. They show that this is due to the fact that the eigenvalues of the Hessian of the network weights depend on the tail-index of the weights, which is a function of the number of iterations of the SGD algorithm. They also show that if the weights are sufficiently large, then SGD converges to a local minimum."
165,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the spectral properties of bandpass filtering in the context of supervised community detection (GCN) models. The authors propose to use a graph structure of Euclidean graphs to model the spectral components of a GCN, and propose a cascade of filtering steps to reduce the frequency of spectral components in the low-frequency domain. The proposed method is evaluated on a number of datasets and compared to other methods."
166,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the spectral properties of bandpass filtering in the context of supervised community detection (GCN) models. The authors propose to use a graph structure of Euclidean graphs to model the spectral components of a GCN, and propose a cascade of filtering steps to reduce the frequency of spectral components in the low-frequency domain. The proposed method is evaluated on a number of datasets and compared to other methods."
167,SP:89f995142f8a2fcdc8c7b9f2e2cd1a4f75df3226,"This paper studies the spectral properties of bandpass filtering in the context of supervised community detection (GCN) models. The authors propose to use a graph structure of Euclidean graphs to model the spectral components of a GCN, and propose a cascade of filtering steps to reduce the frequency of spectral components in the low-frequency domain. The proposed method is evaluated on a number of datasets and compared to other methods."
168,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a self-supervised graph neural network (SLAPS) method for learning graph structure. The proposed method is motivated by the observation that existing graph neural networks (GNNs) do not generalize well to new tasks. To address this issue, the authors propose to learn a task-specific latent representation of the graph, which is then used to train a GNN to predict the graph structure of the new task. The experimental results show that the proposed SLAPS method outperforms existing GNNs."
169,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a self-supervised graph neural network (SLAPS) method for learning graph structure. The proposed method is motivated by the observation that existing graph neural networks (GNNs) do not generalize well to new tasks. To address this issue, the authors propose to learn a task-specific latent representation of the graph, which is then used to train a GNN to predict the graph structure of the new task. The experimental results show that the proposed SLAPS method outperforms existing GNNs."
170,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"This paper proposes a self-supervised graph neural network (SLAPS) method for learning graph structure. The proposed method is motivated by the observation that existing graph neural networks (GNNs) do not generalize well to new tasks. To address this issue, the authors propose to learn a task-specific latent representation of the graph, which is then used to train a GNN to predict the graph structure of the new task. The experimental results show that the proposed SLAPS method outperforms existing GNNs."
171,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel novelty detection method for continual learning. The novelty detection is based on the idea of class-imbalance detection, which aims to detect class-overlapping classes in a label-agnostic incremental setting. The proposed method is evaluated on CIFAR-10/100, SVHN, and CRIB."
172,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel novelty detection method for continual learning. The novelty detection is based on the idea of class-imbalance detection, which aims to detect class-overlapping classes in a label-agnostic incremental setting. The proposed method is evaluated on CIFAR-10/100, SVHN, and CRIB."
173,SP:8f8e1fa4cd025fc056a60c0b6ba9915e8617447f,"This paper proposes a novel novelty detection method for continual learning. The novelty detection is based on the idea of class-imbalance detection, which aims to detect class-overlapping classes in a label-agnostic incremental setting. The proposed method is evaluated on CIFAR-10/100, SVHN, and CRIB."
174,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a method for safe reinforcement learning (RL) in the presence of natural language constraints. The main idea is to use a policy network to learn a representation of forbidden states and a constraint interpreter to enforce the constraints on the policy. The authors propose a modular architecture that consists of two modules: (1) a policy encoder and (2) a constraint encoder. The encoder encodes the constraints into a latent space, which is then fed to the policy network, and (3) the policy decoder is used to extract the representations of the forbidden states from the latent space. Experiments on the HAZARDworld benchmark show that the proposed method outperforms the baselines in terms of both reward and constraint violations."
175,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a method for safe reinforcement learning (RL) in the presence of natural language constraints. The main idea is to use a policy network to learn a representation of forbidden states and a constraint interpreter to enforce the constraints on the policy. The authors propose a modular architecture that consists of two modules: (1) a policy encoder and (2) a constraint encoder. The encoder encodes the constraints into a latent space, which is then fed to the policy network, and (3) the policy decoder is used to extract the representations of the forbidden states from the latent space. Experiments on the HAZARDworld benchmark show that the proposed method outperforms the baselines in terms of both reward and constraint violations."
176,SP:1d242517748c52f2be8f0613316cda3a54d1d2f7,"This paper proposes a method for safe reinforcement learning (RL) in the presence of natural language constraints. The main idea is to use a policy network to learn a representation of forbidden states and a constraint interpreter to enforce the constraints on the policy. The authors propose a modular architecture that consists of two modules: (1) a policy encoder and (2) a constraint encoder. The encoder encodes the constraints into a latent space, which is then fed to the policy network, and (3) the policy decoder is used to extract the representations of the forbidden states from the latent space. Experiments on the HAZARDworld benchmark show that the proposed method outperforms the baselines in terms of both reward and constraint violations."
177,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a few-shot semantic edge detection method, called CAFENet, which aims to recover the boundaries of novel categories. The proposed method is based on a meta-learning strategy, where the semantic segmentation module is trained using a multi-split matching method, and the decoder module is used to learn a low-dimensional sub-vector representation of the attention map. The method is evaluated on FSE-1000 and SBD-5 datasets."
178,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a few-shot semantic edge detection method, called CAFENet, which aims to recover the boundaries of novel categories. The proposed method is based on a meta-learning strategy, where the semantic segmentation module is trained using a multi-split matching method, and the decoder module is used to learn a low-dimensional sub-vector representation of the attention map. The method is evaluated on FSE-1000 and SBD-5 datasets."
179,SP:bc9f37b4622868a92f9812d2ea901def79229d41,"This paper proposes a few-shot semantic edge detection method, called CAFENet, which aims to recover the boundaries of novel categories. The proposed method is based on a meta-learning strategy, where the semantic segmentation module is trained using a multi-split matching method, and the decoder module is used to learn a low-dimensional sub-vector representation of the attention map. The method is evaluated on FSE-1000 and SBD-5 datasets."
180,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a new method for evaluating the interpretability of graph neural networks (GNNs). The proposed method, Causal Screening, is based on the idea that a GNN can be used as a model-agnostic tool for generating explanations for graph features. The method is evaluated on a variety of graph classification datasets and compared to a number of existing methods. The results show that the proposed method outperforms existing methods in terms of both predictive accuracy and contrastivity."
181,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a new method for evaluating the interpretability of graph neural networks (GNNs). The proposed method, Causal Screening, is based on the idea that a GNN can be used as a model-agnostic tool for generating explanations for graph features. The method is evaluated on a variety of graph classification datasets and compared to a number of existing methods. The results show that the proposed method outperforms existing methods in terms of both predictive accuracy and contrastivity."
182,SP:5e99fee48137d3d3d88017a02f7285ce35dce970,"This paper proposes a new method for evaluating the interpretability of graph neural networks (GNNs). The proposed method, Causal Screening, is based on the idea that a GNN can be used as a model-agnostic tool for generating explanations for graph features. The method is evaluated on a variety of graph classification datasets and compared to a number of existing methods. The results show that the proposed method outperforms existing methods in terms of both predictive accuracy and contrastivity."
183,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper proposes a new measure to evaluate the simplicity of a neural network. The proposed measure is based on Occam’s razor, which measures the difference between the number of parameters of the network and its parameters. The authors show that the proposed measure can be used to compare the flatness of minima and the margin of a network. They also show that their measure is more efficient than other flatness-based measures such as the optimization speed and models’ margin."
184,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper proposes a new measure to evaluate the simplicity of a neural network. The proposed measure is based on Occam’s razor, which measures the difference between the number of parameters of the network and its parameters. The authors show that the proposed measure can be used to compare the flatness of minima and the margin of a network. They also show that their measure is more efficient than other flatness-based measures such as the optimization speed and models’ margin."
185,SP:5b3d76b9e67bc39a813979b5d232a59f597d257d,"This paper proposes a new measure to evaluate the simplicity of a neural network. The proposed measure is based on Occam’s razor, which measures the difference between the number of parameters of the network and its parameters. The authors show that the proposed measure can be used to compare the flatness of minima and the margin of a network. They also show that their measure is more efficient than other flatness-based measures such as the optimization speed and models’ margin."
186,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,This paper proposes a novel method for learning discrete representations for long-horizon tasks. The proposed method is based on the mutual information maximization (MIM) objective. The main idea is to learn a low-level model-predictive controller that predicts the next state of the discrete representation. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks.
187,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,This paper proposes a novel method for learning discrete representations for long-horizon tasks. The proposed method is based on the mutual information maximization (MIM) objective. The main idea is to learn a low-level model-predictive controller that predicts the next state of the discrete representation. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks.
188,SP:835d01ee91523fb29595cae8339dfe49de7d3a7c,This paper proposes a novel method for learning discrete representations for long-horizon tasks. The proposed method is based on the mutual information maximization (MIM) objective. The main idea is to learn a low-level model-predictive controller that predicts the next state of the discrete representation. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks.
189,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) for mixed-precision quantization. The authors propose a differentiable bit-sparsity regularizer to reduce the precision of all-zero bits in the quantization scheme. The proposed method is based on a gradient-based optimization process, where a hyperparameter is used to optimize the gradient of the weight elements of the model. Experiments on CIFAR-10 and ImageNet datasets show that the proposed method can achieve better accuracy and bit reduction compared to existing methods."
190,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) for mixed-precision quantization. The authors propose a differentiable bit-sparsity regularizer to reduce the precision of all-zero bits in the quantization scheme. The proposed method is based on a gradient-based optimization process, where a hyperparameter is used to optimize the gradient of the weight elements of the model. Experiments on CIFAR-10 and ImageNet datasets show that the proposed method can achieve better accuracy and bit reduction compared to existing methods."
191,SP:2e31a542a7a60b1d425d95dd26e62374ba799cb8,"This paper proposes bit-level sparsity quantization (BSQ) for mixed-precision quantization. The authors propose a differentiable bit-sparsity regularizer to reduce the precision of all-zero bits in the quantization scheme. The proposed method is based on a gradient-based optimization process, where a hyperparameter is used to optimize the gradient of the weight elements of the model. Experiments on CIFAR-10 and ImageNet datasets show that the proposed method can achieve better accuracy and bit reduction compared to existing methods."
192,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the problem of gradient-based adversarial attacks on quantized neural networks. The authors propose a temperature scaling approach to attack the decision boundary of a quantized network, which is based on the forward-backward signal propagation of the network. The proposed method is evaluated on CIFAR-10/100 datasets and multiple network architectures. The results show that the proposed method outperforms the state-of-the-art."
193,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the problem of gradient-based adversarial attacks on quantized neural networks. The authors propose a temperature scaling approach to attack the decision boundary of a quantized network, which is based on the forward-backward signal propagation of the network. The proposed method is evaluated on CIFAR-10/100 datasets and multiple network architectures. The results show that the proposed method outperforms the state-of-the-art."
194,SP:9b8ae88357e03447c73c792ff5c173ddc3d365e7,"This paper studies the problem of gradient-based adversarial attacks on quantized neural networks. The authors propose a temperature scaling approach to attack the decision boundary of a quantized network, which is based on the forward-backward signal propagation of the network. The proposed method is evaluated on CIFAR-10/100 datasets and multiple network architectures. The results show that the proposed method outperforms the state-of-the-art."
195,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes ProtoryNet, an interpretable recurrent neural network (RNN) model that uses prototype trajectories as input to the RNN backbone. The proposed method is based on prototype theory and proposes a prototype-based RNN model that learns to predict the temporal pattern of prototypes in the input sequence. The authors also propose a prototype based RNN architecture that is able to learn the temporal patterns of prototypes. The experimental results show that the proposed method outperforms the state-of-the-art prototypes-based methods."
196,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes ProtoryNet, an interpretable recurrent neural network (RNN) model that uses prototype trajectories as input to the RNN backbone. The proposed method is based on prototype theory and proposes a prototype-based RNN model that learns to predict the temporal pattern of prototypes in the input sequence. The authors also propose a prototype based RNN architecture that is able to learn the temporal patterns of prototypes. The experimental results show that the proposed method outperforms the state-of-the-art prototypes-based methods."
197,SP:16dddbe1432e4ffbf4b2a9180bf3c67495bd9e81,"This paper proposes ProtoryNet, an interpretable recurrent neural network (RNN) model that uses prototype trajectories as input to the RNN backbone. The proposed method is based on prototype theory and proposes a prototype-based RNN model that learns to predict the temporal pattern of prototypes in the input sequence. The authors also propose a prototype based RNN architecture that is able to learn the temporal patterns of prototypes. The experimental results show that the proposed method outperforms the state-of-the-art prototypes-based methods."
198,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,This paper proposes a new hidden Markov model (HMRNN) for Alzheimer’s disease. The proposed HMRNN is a variant of the discrete-observation HMM. The main difference between the proposed model and discrete-Observation HMM is that the proposed method is able to estimate the parameters of the HMRN using the Baum-Welch algorithm. The authors also propose a new method for parameter estimation of the parameter estimates of the hidden-Markov-RNN. The method is evaluated on the Alzheimer's disease dataset and compared to the state-of-the-art.
199,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,This paper proposes a new hidden Markov model (HMRNN) for Alzheimer’s disease. The proposed HMRNN is a variant of the discrete-observation HMM. The main difference between the proposed model and discrete-Observation HMM is that the proposed method is able to estimate the parameters of the HMRN using the Baum-Welch algorithm. The authors also propose a new method for parameter estimation of the parameter estimates of the hidden-Markov-RNN. The method is evaluated on the Alzheimer's disease dataset and compared to the state-of-the-art.
200,SP:ac8a9afa6e87f9c36d080c2e7085c4e096af64ff,This paper proposes a new hidden Markov model (HMRNN) for Alzheimer’s disease. The proposed HMRNN is a variant of the discrete-observation HMM. The main difference between the proposed model and discrete-Observation HMM is that the proposed method is able to estimate the parameters of the HMRN using the Baum-Welch algorithm. The authors also propose a new method for parameter estimation of the parameter estimates of the hidden-Markov-RNN. The method is evaluated on the Alzheimer's disease dataset and compared to the state-of-the-art.
201,SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a novel method for learning the phenotypic factor of a single-cell RNA-Seq dataset. The proposed method is based on factorized linear discriminant analysis (FLDA), which is a supervised learning approach for gene expression data. The authors propose a sparsity-based regularization algorithm to ensure that the feature combination between the gene expression and phenotypical feature is close to that of the data. They also propose a novel feature extractor based on the sparsity regularization method. The experimental results show that the proposed method outperforms the baselines in terms of both dendritic and axonal phenotypes of Drosophila T4/T5 neurons."
202,SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a novel method for learning the phenotypic factor of a single-cell RNA-Seq dataset. The proposed method is based on factorized linear discriminant analysis (FLDA), which is a supervised learning approach for gene expression data. The authors propose a sparsity-based regularization algorithm to ensure that the feature combination between the gene expression and phenotypical feature is close to that of the data. They also propose a novel feature extractor based on the sparsity regularization method. The experimental results show that the proposed method outperforms the baselines in terms of both dendritic and axonal phenotypes of Drosophila T4/T5 neurons."
203,SP:6355337707f1dd373813290e26e9c0a264b993f9,"This paper proposes a novel method for learning the phenotypic factor of a single-cell RNA-Seq dataset. The proposed method is based on factorized linear discriminant analysis (FLDA), which is a supervised learning approach for gene expression data. The authors propose a sparsity-based regularization algorithm to ensure that the feature combination between the gene expression and phenotypical feature is close to that of the data. They also propose a novel feature extractor based on the sparsity regularization method. The experimental results show that the proposed method outperforms the baselines in terms of both dendritic and axonal phenotypes of Drosophila T4/T5 neurons."
204,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper studies the interpretation of saliency maps of image classifiers. The authors propose a variational approximation method for the posterior distribution of the saliency map of the image classifier, which is based on the positive correlation between the image and the prior distribution. They show that the proposed method is more interpretable than existing methods, and that it can be used to estimate the predictive probability of the classifier’s predictive probability. "
205,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper studies the interpretation of saliency maps of image classifiers. The authors propose a variational approximation method for the posterior distribution of the saliency map of the image classifier, which is based on the positive correlation between the image and the prior distribution. They show that the proposed method is more interpretable than existing methods, and that it can be used to estimate the predictive probability of the classifier’s predictive probability. "
206,SP:28e61a4f51f9f7283397d6336ea114375ae6a004,"This paper studies the interpretation of saliency maps of image classifiers. The authors propose a variational approximation method for the posterior distribution of the saliency map of the image classifier, which is based on the positive correlation between the image and the prior distribution. They show that the proposed method is more interpretable than existing methods, and that it can be used to estimate the predictive probability of the classifier’s predictive probability. "
207,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a neural debiasing method for sentence-level fairness in pretrained text encoders. Specifically, the authors propose a fair filter (FairFil) network that uses a contrastive learning framework to learn a debiased representation of the debased representations of the pretrained sentence encoder. The authors show that the proposed method is able to reduce the bias degree of pretrained pretrained BERT by a factor of 1.5. They also show that FairFil can be applied to downstream tasks."
208,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a neural debiasing method for sentence-level fairness in pretrained text encoders. Specifically, the authors propose a fair filter (FairFil) network that uses a contrastive learning framework to learn a debiased representation of the debased representations of the pretrained sentence encoder. The authors show that the proposed method is able to reduce the bias degree of pretrained pretrained BERT by a factor of 1.5. They also show that FairFil can be applied to downstream tasks."
209,SP:01acd8b88768d86bcf21b8c20a930d706c5645a7,"This paper proposes a neural debiasing method for sentence-level fairness in pretrained text encoders. Specifically, the authors propose a fair filter (FairFil) network that uses a contrastive learning framework to learn a debiased representation of the debased representations of the pretrained sentence encoder. The authors show that the proposed method is able to reduce the bias degree of pretrained pretrained BERT by a factor of 1.5. They also show that FairFil can be applied to downstream tasks."
210,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a method for certifying the robustness of a model to adversarial perturbations. The main idea is to use sample-wise randomized smoothing to reduce the noise level of the perturbation, and then use a pretrain-to-finetune framework to fine-tune the model. The proposed method is evaluated on CIFAR-10 and MNIST datasets."
211,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a method for certifying the robustness of a model to adversarial perturbations. The main idea is to use sample-wise randomized smoothing to reduce the noise level of the perturbation, and then use a pretrain-to-finetune framework to fine-tune the model. The proposed method is evaluated on CIFAR-10 and MNIST datasets."
212,SP:f2f505d3f07ca3bb2f16f6f6f5d00fee98da6531,"This paper proposes a method for certifying the robustness of a model to adversarial perturbations. The main idea is to use sample-wise randomized smoothing to reduce the noise level of the perturbation, and then use a pretrain-to-finetune framework to fine-tune the model. The proposed method is evaluated on CIFAR-10 and MNIST datasets."
213,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,This paper proposes a variational method for unsupervised recovery of corrupted data. The proposed method is based on a reduced entropy condition approximate inference method. The authors show that the proposed method can recover the posteriors of clean values and de-noising from corrupted data without imputation. They also show that their method can be applied to real data sets with missing values and noise.
214,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,This paper proposes a variational method for unsupervised recovery of corrupted data. The proposed method is based on a reduced entropy condition approximate inference method. The authors show that the proposed method can recover the posteriors of clean values and de-noising from corrupted data without imputation. They also show that their method can be applied to real data sets with missing values and noise.
215,SP:692c7b9f6d982bbc5a22e566296a97e8a530b87c,This paper proposes a variational method for unsupervised recovery of corrupted data. The proposed method is based on a reduced entropy condition approximate inference method. The authors show that the proposed method can recover the posteriors of clean values and de-noising from corrupted data without imputation. They also show that their method can be applied to real data sets with missing values and noise.
216,SP:4b7d050f57507166992034e5e264cccab3cb874f,This paper proposes a new self-attention mechanism for graph representation learning. The proposed method is based on diffusion prior. The diffusion prior is used to compute the attention scores for each node in the graph. The authors show that the proposed method can achieve better performance than the state-of-the-art.
217,SP:4b7d050f57507166992034e5e264cccab3cb874f,This paper proposes a new self-attention mechanism for graph representation learning. The proposed method is based on diffusion prior. The diffusion prior is used to compute the attention scores for each node in the graph. The authors show that the proposed method can achieve better performance than the state-of-the-art.
218,SP:4b7d050f57507166992034e5e264cccab3cb874f,This paper proposes a new self-attention mechanism for graph representation learning. The proposed method is based on diffusion prior. The diffusion prior is used to compute the attention scores for each node in the graph. The authors show that the proposed method can achieve better performance than the state-of-the-art.
219,SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper presents a multi-task multitask accuracy test for text models. The authors compare the performance of GPT-3 model and random chance model on a variety of tasks. The results show that the GPT3 model outperforms random chance on most of the tasks. However, the results are not consistent across all tasks. "
220,SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper presents a multi-task multitask accuracy test for text models. The authors compare the performance of GPT-3 model and random chance model on a variety of tasks. The results show that the GPT3 model outperforms random chance on most of the tasks. However, the results are not consistent across all tasks. "
221,SP:36310d761deb19e71c8a57de19b48f857707d48b,"This paper presents a multi-task multitask accuracy test for text models. The authors compare the performance of GPT-3 model and random chance model on a variety of tasks. The results show that the GPT3 model outperforms random chance on most of the tasks. However, the results are not consistent across all tasks. "
222,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,This paper proposes a pre-training method for table semantic parsing. The proposed method is based on synchronous context-free grammar (SCFG) and uses synthetic question-SQL pairs to train the model. The authors evaluate the proposed method on both table- and language datasets and show that it outperforms baselines.
223,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,This paper proposes a pre-training method for table semantic parsing. The proposed method is based on synchronous context-free grammar (SCFG) and uses synthetic question-SQL pairs to train the model. The authors evaluate the proposed method on both table- and language datasets and show that it outperforms baselines.
224,SP:7d7d34ba6e9fb36f2658cf4be44b137cdd73d34c,This paper proposes a pre-training method for table semantic parsing. The proposed method is based on synchronous context-free grammar (SCFG) and uses synthetic question-SQL pairs to train the model. The authors evaluate the proposed method on both table- and language datasets and show that it outperforms baselines.
225,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,This paper studies the problem of multi-task and transfer learning. The authors propose a new method to train a single-task LS-SVM with random matrix analysis for Gaussian mixture data model. The main contribution of this paper is to provide a theoretical analysis of the deterministic limit of the MTL-LSVM algorithm. The theoretical analysis is based on the fact that the number of tasks required to train the single task LS-VMs is deterministic in the small-dimensional case. The paper then proposes a cross-validation procedure to evaluate the performance of the proposed method. The experimental results show that the proposed approach outperforms the state of the art.
226,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,This paper studies the problem of multi-task and transfer learning. The authors propose a new method to train a single-task LS-SVM with random matrix analysis for Gaussian mixture data model. The main contribution of this paper is to provide a theoretical analysis of the deterministic limit of the MTL-LSVM algorithm. The theoretical analysis is based on the fact that the number of tasks required to train the single task LS-VMs is deterministic in the small-dimensional case. The paper then proposes a cross-validation procedure to evaluate the performance of the proposed method. The experimental results show that the proposed approach outperforms the state of the art.
227,SP:ebbb25902804b4f9f4985311c5debe2ef0ad7c7c,This paper studies the problem of multi-task and transfer learning. The authors propose a new method to train a single-task LS-SVM with random matrix analysis for Gaussian mixture data model. The main contribution of this paper is to provide a theoretical analysis of the deterministic limit of the MTL-LSVM algorithm. The theoretical analysis is based on the fact that the number of tasks required to train the single task LS-VMs is deterministic in the small-dimensional case. The paper then proposes a cross-validation procedure to evaluate the performance of the proposed method. The experimental results show that the proposed approach outperforms the state of the art.
228,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,This paper proposes a conditional neural process (CNP) based on group equivariant conditional neural processes (CNPs). The main contribution of this paper is to decompose the permutation-invariant and group-equivariant maps in the latent space of the encoder and decoder into permutation invariance and group invariance. The decomposition theorem is proved for both permutation and group symmetries. The authors then propose a new encoder-decoder architecture based on Lie group convolutional layers and show that the proposed architecture can achieve better performance than the state-of-the-art CNPs on a 1D regression task and a zero-shot generalization task.
229,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,This paper proposes a conditional neural process (CNP) based on group equivariant conditional neural processes (CNPs). The main contribution of this paper is to decompose the permutation-invariant and group-equivariant maps in the latent space of the encoder and decoder into permutation invariance and group invariance. The decomposition theorem is proved for both permutation and group symmetries. The authors then propose a new encoder-decoder architecture based on Lie group convolutional layers and show that the proposed architecture can achieve better performance than the state-of-the-art CNPs on a 1D regression task and a zero-shot generalization task.
230,SP:2be727b1333122fef3abfd2f7c576d2fc467893f,This paper proposes a conditional neural process (CNP) based on group equivariant conditional neural processes (CNPs). The main contribution of this paper is to decompose the permutation-invariant and group-equivariant maps in the latent space of the encoder and decoder into permutation invariance and group invariance. The decomposition theorem is proved for both permutation and group symmetries. The authors then propose a new encoder-decoder architecture based on Lie group convolutional layers and show that the proposed architecture can achieve better performance than the state-of-the-art CNPs on a 1D regression task and a zero-shot generalization task.
231,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper studies the problem of offline reinforcement learning (RL) for text generation. The authors propose a novel approach to deal with the problem. The main idea of the paper is to use an importance weighting algorithm to weight the importance of each example in the history distribution of the model. The paper also proposes a policy gradient based approach to reduce the model-generated history bias. The proposed method is evaluated on three tasks: summarization, question generation, and machine translation."
232,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper studies the problem of offline reinforcement learning (RL) for text generation. The authors propose a novel approach to deal with the problem. The main idea of the paper is to use an importance weighting algorithm to weight the importance of each example in the history distribution of the model. The paper also proposes a policy gradient based approach to reduce the model-generated history bias. The proposed method is evaluated on three tasks: summarization, question generation, and machine translation."
233,SP:a54b0358a0a2900f76a2da7a0a99348805c8d66a,"This paper studies the problem of offline reinforcement learning (RL) for text generation. The authors propose a novel approach to deal with the problem. The main idea of the paper is to use an importance weighting algorithm to weight the importance of each example in the history distribution of the model. The paper also proposes a policy gradient based approach to reduce the model-generated history bias. The proposed method is evaluated on three tasks: summarization, question generation, and machine translation."
234,SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes InfoPro, a new loss function for local supervised learning. InfoPro is based on the information propagation (E2E) loss, which is an extension of the InfoNet loss. The main difference between InfoNet and E2E is that InfoNet uses a normal cross-entropy/contrastive term, while InfoPro uses a reconstruction term. The authors show that InfoPro can be used as a surrogate optimization objective to reduce the memory footprint of local modules. The proposed method is evaluated on ImageNet, Cityscapes, STL-10 and CIFAR-10."
235,SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes InfoPro, a new loss function for local supervised learning. InfoPro is based on the information propagation (E2E) loss, which is an extension of the InfoNet loss. The main difference between InfoNet and E2E is that InfoNet uses a normal cross-entropy/contrastive term, while InfoPro uses a reconstruction term. The authors show that InfoPro can be used as a surrogate optimization objective to reduce the memory footprint of local modules. The proposed method is evaluated on ImageNet, Cityscapes, STL-10 and CIFAR-10."
236,SP:e77eca51db362909681965092186af2e502aaedc,"This paper proposes InfoPro, a new loss function for local supervised learning. InfoPro is based on the information propagation (E2E) loss, which is an extension of the InfoNet loss. The main difference between InfoNet and E2E is that InfoNet uses a normal cross-entropy/contrastive term, while InfoPro uses a reconstruction term. The authors show that InfoPro can be used as a surrogate optimization objective to reduce the memory footprint of local modules. The proposed method is evaluated on ImageNet, Cityscapes, STL-10 and CIFAR-10."
237,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,This paper proposes a novel neighborhood aggregation method for graph neural networks (GNNs). The proposed method is based on the idea of using multiple aggregation functions to aggregate different neighborhoods of nodes and graphs. The authors show that the proposed method can be applied to both GCN and GAT. The experimental results on several benchmark datasets show the effectiveness of the proposed approach.
238,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,This paper proposes a novel neighborhood aggregation method for graph neural networks (GNNs). The proposed method is based on the idea of using multiple aggregation functions to aggregate different neighborhoods of nodes and graphs. The authors show that the proposed method can be applied to both GCN and GAT. The experimental results on several benchmark datasets show the effectiveness of the proposed approach.
239,SP:21e44dddd20db1768de0dab869f8b0d3d5a598b7,This paper proposes a novel neighborhood aggregation method for graph neural networks (GNNs). The proposed method is based on the idea of using multiple aggregation functions to aggregate different neighborhoods of nodes and graphs. The authors show that the proposed method can be applied to both GCN and GAT. The experimental results on several benchmark datasets show the effectiveness of the proposed approach.
240,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper proposes a new method for improving the robustness of video machine learning models against corruptions. The method is based on adversarial training, where the adversarial examples are generated by corruptions at different levels of corruption, and the model is trained to be robust to these attacks. The proposed method is evaluated on a variety of video classification tasks, including action recognition, multi-object tracking, and video classification. The results show that the proposed method outperforms the state-of-the-art methods."
241,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper proposes a new method for improving the robustness of video machine learning models against corruptions. The method is based on adversarial training, where the adversarial examples are generated by corruptions at different levels of corruption, and the model is trained to be robust to these attacks. The proposed method is evaluated on a variety of video classification tasks, including action recognition, multi-object tracking, and video classification. The results show that the proposed method outperforms the state-of-the-art methods."
242,SP:e9a8956f067a55b794508ac69f93b4b0290a664c,"This paper proposes a new method for improving the robustness of video machine learning models against corruptions. The method is based on adversarial training, where the adversarial examples are generated by corruptions at different levels of corruption, and the model is trained to be robust to these attacks. The proposed method is evaluated on a variety of video classification tasks, including action recognition, multi-object tracking, and video classification. The results show that the proposed method outperforms the state-of-the-art methods."
243,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a method for learning representations for time-varying graphs. The authors propose a skip-gram embedding approach to learn tensor representations of time-evolving graphs, which can be used for downstream tasks such as network reconstruction, disease spreading, and contagion risk estimation. The proposed method is based on a skip gram embedding method, which is a combination of skip embedding and implicit tensor factorization. Experiments show that the proposed method outperforms state-of-the-art methods on a variety of downstream tasks."
244,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a method for learning representations for time-varying graphs. The authors propose a skip-gram embedding approach to learn tensor representations of time-evolving graphs, which can be used for downstream tasks such as network reconstruction, disease spreading, and contagion risk estimation. The proposed method is based on a skip gram embedding method, which is a combination of skip embedding and implicit tensor factorization. Experiments show that the proposed method outperforms state-of-the-art methods on a variety of downstream tasks."
245,SP:35f77a7dcce3f6e09db0db9d22207a6da1fdbe5c,"This paper proposes a method for learning representations for time-varying graphs. The authors propose a skip-gram embedding approach to learn tensor representations of time-evolving graphs, which can be used for downstream tasks such as network reconstruction, disease spreading, and contagion risk estimation. The proposed method is based on a skip gram embedding method, which is a combination of skip embedding and implicit tensor factorization. Experiments show that the proposed method outperforms state-of-the-art methods on a variety of downstream tasks."
246,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a new skip-sequence task to evaluate the performance of self-supervised language models for logical reasoning. The proposed skip-tree task is based on the idea of skip-sequences, which is an extension of the skip sequence task. The authors show that the proposed method outperforms the state-of-the-art models on a variety of downstream tasks."
247,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a new skip-sequence task to evaluate the performance of self-supervised language models for logical reasoning. The proposed skip-tree task is based on the idea of skip-sequences, which is an extension of the skip sequence task. The authors show that the proposed method outperforms the state-of-the-art models on a variety of downstream tasks."
248,SP:bac0a2d3478dd277cb1ceafedd7fff64e107a222,"This paper proposes a new skip-sequence task to evaluate the performance of self-supervised language models for logical reasoning. The proposed skip-tree task is based on the idea of skip-sequences, which is an extension of the skip sequence task. The authors show that the proposed method outperforms the state-of-the-art models on a variety of downstream tasks."
249,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper studies the problem of adversarial robustness against imbalanced gradients. The authors propose a novel margin decomposition (MD) attack to attack the imbalanced gradient. The MD attack is based on a two-stage process, where the first stage is to minimize the margin loss, and the second stage is a label smoothing step to improve the robustness of the model. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed MD attack."
250,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper studies the problem of adversarial robustness against imbalanced gradients. The authors propose a novel margin decomposition (MD) attack to attack the imbalanced gradient. The MD attack is based on a two-stage process, where the first stage is to minimize the margin loss, and the second stage is a label smoothing step to improve the robustness of the model. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed MD attack."
251,SP:808f6d3af382876f5518e8e3a14ea73cc59c0a2b,"This paper studies the problem of adversarial robustness against imbalanced gradients. The authors propose a novel margin decomposition (MD) attack to attack the imbalanced gradient. The MD attack is based on a two-stage process, where the first stage is to minimize the margin loss, and the second stage is a label smoothing step to improve the robustness of the model. Experiments are conducted on CIFAR-10 and ImageNet to demonstrate the effectiveness of the proposed MD attack."
252,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a method for proving semantic equivalence in linear algebra. The method is based on incremental graph-to-sequence (GTS) networks, which are trained on a set of linear algebra expressions. The system is evaluated on a synthetic dataset and on a real-world dataset. The results show that the proposed method outperforms existing methods."
253,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a method for proving semantic equivalence in linear algebra. The method is based on incremental graph-to-sequence (GTS) networks, which are trained on a set of linear algebra expressions. The system is evaluated on a synthetic dataset and on a real-world dataset. The results show that the proposed method outperforms existing methods."
254,SP:2180e15ad0bbecc98e043b41f6525d2a8061d304,"This paper proposes a method for proving semantic equivalence in linear algebra. The method is based on incremental graph-to-sequence (GTS) networks, which are trained on a set of linear algebra expressions. The system is evaluated on a synthetic dataset and on a real-world dataset. The results show that the proposed method outperforms existing methods."
255,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a low-rank approximation method for multi-modal Transformer models. The proposed method is based on the idea of negative sampling, which aims to reduce the number of modality-specific parameters in the model. The authors propose to use a low rank approximation of the CNN embedding space, which is then used to compute the instance similarity between different modalities. The method is evaluated on audio-visual video classification tasks and shows promising results."
256,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a low-rank approximation method for multi-modal Transformer models. The proposed method is based on the idea of negative sampling, which aims to reduce the number of modality-specific parameters in the model. The authors propose to use a low rank approximation of the CNN embedding space, which is then used to compute the instance similarity between different modalities. The method is evaluated on audio-visual video classification tasks and shows promising results."
257,SP:19e32803278a7ad2be5343187468cd2e26335bc8,"This paper proposes a low-rank approximation method for multi-modal Transformer models. The proposed method is based on the idea of negative sampling, which aims to reduce the number of modality-specific parameters in the model. The authors propose to use a low rank approximation of the CNN embedding space, which is then used to compute the instance similarity between different modalities. The method is evaluated on audio-visual video classification tasks and shows promising results."
258,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,This paper proposes a method for online few-shot learning in the continual learning setting. The main idea is to learn a contextual prototypical memory model for spatiotemporal contextual information. The proposed method is based on the idea of using a spatio-temporal encoder-decoder model to learn the object classes. The model is trained on a large scale indoor imagery dataset and is evaluated on several tasks.
259,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,This paper proposes a method for online few-shot learning in the continual learning setting. The main idea is to learn a contextual prototypical memory model for spatiotemporal contextual information. The proposed method is based on the idea of using a spatio-temporal encoder-decoder model to learn the object classes. The model is trained on a large scale indoor imagery dataset and is evaluated on several tasks.
260,SP:a5c22c090413ef4448db8e7f5b39332b3db6c73f,This paper proposes a method for online few-shot learning in the continual learning setting. The main idea is to learn a contextual prototypical memory model for spatiotemporal contextual information. The proposed method is based on the idea of using a spatio-temporal encoder-decoder model to learn the object classes. The model is trained on a large scale indoor imagery dataset and is evaluated on several tasks.
261,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of training graph neural networks (GNNs) on temporal graphs. The authors propose a new method to train GNNs on a temporal graph, where each node in the graph is associated with a time window, and the goal is to train a GNN to predict the next time step in the temporal window. The main contribution of this paper is to provide a theoretical analysis of the performance of GNN on the temporal graph. The paper is well-written and easy to follow. The experimental results show that the proposed method outperforms the state-of-the-art in terms of accuracy."
262,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of training graph neural networks (GNNs) on temporal graphs. The authors propose a new method to train GNNs on a temporal graph, where each node in the graph is associated with a time window, and the goal is to train a GNN to predict the next time step in the temporal window. The main contribution of this paper is to provide a theoretical analysis of the performance of GNN on the temporal graph. The paper is well-written and easy to follow. The experimental results show that the proposed method outperforms the state-of-the-art in terms of accuracy."
263,SP:9c87f7778b8ee5d3e65fb1204b8067f12aac94e1,"This paper studies the problem of training graph neural networks (GNNs) on temporal graphs. The authors propose a new method to train GNNs on a temporal graph, where each node in the graph is associated with a time window, and the goal is to train a GNN to predict the next time step in the temporal window. The main contribution of this paper is to provide a theoretical analysis of the performance of GNN on the temporal graph. The paper is well-written and easy to follow. The experimental results show that the proposed method outperforms the state-of-the-art in terms of accuracy."
264,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"This paper proposes a new self-constraint method for representation learning in reinforcement learning. The proposed method is based on the idea of cross-state self-consistency (CSSC), which aims to improve the representation similarity between the learned representations of the agent and the environment. In particular, the authors propose to use implicit feedback to encourage the agent to learn representations that are more similar to the environment than the representations learned by the agent. The authors show that the proposed method can improve the performance on ProcGen games."
265,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"This paper proposes a new self-constraint method for representation learning in reinforcement learning. The proposed method is based on the idea of cross-state self-consistency (CSSC), which aims to improve the representation similarity between the learned representations of the agent and the environment. In particular, the authors propose to use implicit feedback to encourage the agent to learn representations that are more similar to the environment than the representations learned by the agent. The authors show that the proposed method can improve the performance on ProcGen games."
266,SP:dce0bbc266a9ac746f0db5099836fa57a3055f4a,"This paper proposes a new self-constraint method for representation learning in reinforcement learning. The proposed method is based on the idea of cross-state self-consistency (CSSC), which aims to improve the representation similarity between the learned representations of the agent and the environment. In particular, the authors propose to use implicit feedback to encourage the agent to learn representations that are more similar to the environment than the representations learned by the agent. The authors show that the proposed method can improve the performance on ProcGen games."
267,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on Graph Neural Networks (GNNs) in the restricted near-black-box setting. The authors propose a new attack strategy based on the influence maximization problem, which aims to maximize the influence of each node in the graph. The proposed attack strategy is evaluated on three different GNN models, and compared to several baselines."
268,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on Graph Neural Networks (GNNs) in the restricted near-black-box setting. The authors propose a new attack strategy based on the influence maximization problem, which aims to maximize the influence of each node in the graph. The proposed attack strategy is evaluated on three different GNN models, and compared to several baselines."
269,SP:3dda3d53fdc4bd8045db22cac740322e31e67bcf,"This paper studies adversarial attacks on Graph Neural Networks (GNNs) in the restricted near-black-box setting. The authors propose a new attack strategy based on the influence maximization problem, which aims to maximize the influence of each node in the graph. The proposed attack strategy is evaluated on three different GNN models, and compared to several baselines."
270,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of causal structure learning in directed acyclic graphs (DAGs). The authors propose a low-rank assumption on the (weighted) adjacency matrix of the DAG causal model. The authors show that the low rank assumption is necessary for DAGs with scale-free networks, and show that it is also necessary for scale-based networks. "
271,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of causal structure learning in directed acyclic graphs (DAGs). The authors propose a low-rank assumption on the (weighted) adjacency matrix of the DAG causal model. The authors show that the low rank assumption is necessary for DAGs with scale-free networks, and show that it is also necessary for scale-based networks. "
272,SP:fca0583b19bd08f59fdb0e46f86a4b27495dd0df,"This paper studies the problem of causal structure learning in directed acyclic graphs (DAGs). The authors propose a low-rank assumption on the (weighted) adjacency matrix of the DAG causal model. The authors show that the low rank assumption is necessary for DAGs with scale-free networks, and show that it is also necessary for scale-based networks. "
273,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization method for neural architecture search (NAS) and hyper-parameter optimization (HPO) in the context of AutoML. The main idea is to jointly optimize the search and retraining stages of the two components of the AutoML pipeline. The proposed method is based on the idea of DiffAutoML, which is an end-to-end AutoML algorithm. The authors show that the proposed method can achieve better performance than the state-of-the-art on ImageNet and CIFAR-10."
274,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization method for neural architecture search (NAS) and hyper-parameter optimization (HPO) in the context of AutoML. The main idea is to jointly optimize the search and retraining stages of the two components of the AutoML pipeline. The proposed method is based on the idea of DiffAutoML, which is an end-to-end AutoML algorithm. The authors show that the proposed method can achieve better performance than the state-of-the-art on ImageNet and CIFAR-10."
275,SP:f2c8172adcb82ed1c0e047ffed65412f3f1c1ac7,"This paper proposes a differentiable joint optimization method for neural architecture search (NAS) and hyper-parameter optimization (HPO) in the context of AutoML. The main idea is to jointly optimize the search and retraining stages of the two components of the AutoML pipeline. The proposed method is based on the idea of DiffAutoML, which is an end-to-end AutoML algorithm. The authors show that the proposed method can achieve better performance than the state-of-the-art on ImageNet and CIFAR-10."
276,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper proposes a method for ensemble distribution distillation (EnD) to improve the performance of prior networks. The main idea is to distill the ensemble of models into a single model, which is then used to estimate the uncertainty of the ensemble. The authors show that EnD can improve the accuracy and calibration of a prior network on a variety of tasks. They also show that the proposed method can be applied to regression tasks."
277,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper proposes a method for ensemble distribution distillation (EnD) to improve the performance of prior networks. The main idea is to distill the ensemble of models into a single model, which is then used to estimate the uncertainty of the ensemble. The authors show that EnD can improve the accuracy and calibration of a prior network on a variety of tasks. They also show that the proposed method can be applied to regression tasks."
278,SP:1ab30867e0bd8b6b65fad602cd80bada70b3f1ec,"This paper proposes a method for ensemble distribution distillation (EnD) to improve the performance of prior networks. The main idea is to distill the ensemble of models into a single model, which is then used to estimate the uncertainty of the ensemble. The authors show that EnD can improve the accuracy and calibration of a prior network on a variety of tasks. They also show that the proposed method can be applied to regression tasks."
279,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper proposes a Bayesian online meta-learning framework for few-shot classification. The proposed method is based on the MAML framework. The main idea is to learn a meta-learned model that can be used to solve the catastrophic forgetting problem. The paper also proposes a variational inference method for the meta-learner. The experimental results show that the proposed method outperforms the state-of-the-art baselines.
280,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper proposes a Bayesian online meta-learning framework for few-shot classification. The proposed method is based on the MAML framework. The main idea is to learn a meta-learned model that can be used to solve the catastrophic forgetting problem. The paper also proposes a variational inference method for the meta-learner. The experimental results show that the proposed method outperforms the state-of-the-art baselines.
281,SP:fe2aa4706defcac74e529d0cc3e1622d77451eca,This paper proposes a Bayesian online meta-learning framework for few-shot classification. The proposed method is based on the MAML framework. The main idea is to learn a meta-learned model that can be used to solve the catastrophic forgetting problem. The paper also proposes a variational inference method for the meta-learner. The experimental results show that the proposed method outperforms the state-of-the-art baselines.
282,SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a recursive pooling technique for graph neural networks (GNNs) to reduce the computational cost of learning with graphs. The proposed method is based on Recurrent Neural Network (RNP) and Local Relational Pooling (LRP) networks. The authors compare the computational complexity of RNP-GNN and LRP networks with a higher-order k-GAN and a lower-order LRP network, and show that the RNP network has a lower computational complexity compared to LRP and LNP networks. They also provide a theoretical analysis of the proposed method."
283,SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a recursive pooling technique for graph neural networks (GNNs) to reduce the computational cost of learning with graphs. The proposed method is based on Recurrent Neural Network (RNP) and Local Relational Pooling (LRP) networks. The authors compare the computational complexity of RNP-GNN and LRP networks with a higher-order k-GAN and a lower-order LRP network, and show that the RNP network has a lower computational complexity compared to LRP and LNP networks. They also provide a theoretical analysis of the proposed method."
284,SP:89d2765946e70455105a608d998c3b900969cb8d,"This paper proposes a recursive pooling technique for graph neural networks (GNNs) to reduce the computational cost of learning with graphs. The proposed method is based on Recurrent Neural Network (RNP) and Local Relational Pooling (LRP) networks. The authors compare the computational complexity of RNP-GNN and LRP networks with a higher-order k-GAN and a lower-order LRP network, and show that the RNP network has a lower computational complexity compared to LRP and LNP networks. They also provide a theoretical analysis of the proposed method."
285,SP:c43f5deb340555d78599a3496318514a826b1aae,This paper studies the dynamics of two-person zero-sum games in the presence of Lyapunov chaos. The authors propose a novel method to characterize the behavior of the game in terms of the volume of the cumulative payoff space. The main contribution of the paper is to provide a local equivalence between the volume change and the volume-expansion argument in the canonical game decomposition. The paper also provides a theoretical analysis of the dynamics in the case of bimatrix games.
286,SP:c43f5deb340555d78599a3496318514a826b1aae,This paper studies the dynamics of two-person zero-sum games in the presence of Lyapunov chaos. The authors propose a novel method to characterize the behavior of the game in terms of the volume of the cumulative payoff space. The main contribution of the paper is to provide a local equivalence between the volume change and the volume-expansion argument in the canonical game decomposition. The paper also provides a theoretical analysis of the dynamics in the case of bimatrix games.
287,SP:c43f5deb340555d78599a3496318514a826b1aae,This paper studies the dynamics of two-person zero-sum games in the presence of Lyapunov chaos. The authors propose a novel method to characterize the behavior of the game in terms of the volume of the cumulative payoff space. The main contribution of the paper is to provide a local equivalence between the volume change and the volume-expansion argument in the canonical game decomposition. The paper also provides a theoretical analysis of the dynamics in the case of bimatrix games.
288,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,This paper studies the convergence of adaptive algorithms for deep learning. The authors show that the convergence rate of AMSGrad and Radam converge to a proximal function of the marginal regret bound minimization of the adaptive algorithms. They also provide a theoretical analysis of the convergence rates of these algorithms. 
289,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,This paper studies the convergence of adaptive algorithms for deep learning. The authors show that the convergence rate of AMSGrad and Radam converge to a proximal function of the marginal regret bound minimization of the adaptive algorithms. They also provide a theoretical analysis of the convergence rates of these algorithms. 
290,SP:0cf756ba6b172f9b29e84945c093dfd89ae62803,This paper studies the convergence of adaptive algorithms for deep learning. The authors show that the convergence rate of AMSGrad and Radam converge to a proximal function of the marginal regret bound minimization of the adaptive algorithms. They also provide a theoretical analysis of the convergence rates of these algorithms. 
291,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper studies the problem of risk minimization in the context of expected quadratic utility maximization (EQUM), which is an extension of the mean-variance control (MVC) algorithm. The authors propose a new algorithm that combines reward-constrained variance minimization (UCM) and reward-regularized regularization (PRM) to improve the performance of the proposed algorithm. In particular, the authors show that the proposed method can achieve better performance than the state-of-the-art in terms of expected utility maximisation (UUM) on both simulated and real-world datasets. "
292,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper studies the problem of risk minimization in the context of expected quadratic utility maximization (EQUM), which is an extension of the mean-variance control (MVC) algorithm. The authors propose a new algorithm that combines reward-constrained variance minimization (UCM) and reward-regularized regularization (PRM) to improve the performance of the proposed algorithm. In particular, the authors show that the proposed method can achieve better performance than the state-of-the-art in terms of expected utility maximisation (UUM) on both simulated and real-world datasets. "
293,SP:b6b594fc555bd12b33f156970f0665e2bf793484,"This paper studies the problem of risk minimization in the context of expected quadratic utility maximization (EQUM), which is an extension of the mean-variance control (MVC) algorithm. The authors propose a new algorithm that combines reward-constrained variance minimization (UCM) and reward-regularized regularization (PRM) to improve the performance of the proposed algorithm. In particular, the authors show that the proposed method can achieve better performance than the state-of-the-art in terms of expected utility maximisation (UUM) on both simulated and real-world datasets. "
294,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes AuxiLearn, a multi-task learning method that uses auxiliary tasks to learn a coherent loss function. The auxiliary tasks are designed to be nonlinear interactions between the objective function and the auxiliary tasks. The proposed method is evaluated on image segmentation and classification tasks."
295,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes AuxiLearn, a multi-task learning method that uses auxiliary tasks to learn a coherent loss function. The auxiliary tasks are designed to be nonlinear interactions between the objective function and the auxiliary tasks. The proposed method is evaluated on image segmentation and classification tasks."
296,SP:bf9d66f713b6502d274143c6273b2d071a0c045e,"This paper proposes AuxiLearn, a multi-task learning method that uses auxiliary tasks to learn a coherent loss function. The auxiliary tasks are designed to be nonlinear interactions between the objective function and the auxiliary tasks. The proposed method is evaluated on image segmentation and classification tasks."
297,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"This paper proposes a measure of dropout uncertainty for out-of-distribution sentences in neural machine translation (NMT) models. The proposed measure is based on the dropout error of the Transformer model, which is a Bayesian deep learning model. The authors show that the proposed measure can be used to evaluate the performance of NMT models on a variety of tasks, including German-English translation, German-French translation, and English-German translation. They also show that their measure is useful for evaluating the quality of the model."
298,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"This paper proposes a measure of dropout uncertainty for out-of-distribution sentences in neural machine translation (NMT) models. The proposed measure is based on the dropout error of the Transformer model, which is a Bayesian deep learning model. The authors show that the proposed measure can be used to evaluate the performance of NMT models on a variety of tasks, including German-English translation, German-French translation, and English-German translation. They also show that their measure is useful for evaluating the quality of the model."
299,SP:3070fd64f8eb4d7ece6521cb975fd1fe64d6329f,"This paper proposes a measure of dropout uncertainty for out-of-distribution sentences in neural machine translation (NMT) models. The proposed measure is based on the dropout error of the Transformer model, which is a Bayesian deep learning model. The authors show that the proposed measure can be used to evaluate the performance of NMT models on a variety of tasks, including German-English translation, German-French translation, and English-German translation. They also show that their measure is useful for evaluating the quality of the model."
300,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the problem of pruning neural networks. The authors propose three pruning heuristics: SNIP, GraSP, and magnitude pruning. They show that SNIP and GraSP perform better than random pruning, and that the magnitude pruner performs better than the random pruner. In addition, the authors show that magnitude pruners are more effective than SNIP."
301,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the problem of pruning neural networks. The authors propose three pruning heuristics: SNIP, GraSP, and magnitude pruning. They show that SNIP and GraSP perform better than random pruning, and that the magnitude pruner performs better than the random pruner. In addition, the authors show that magnitude pruners are more effective than SNIP."
302,SP:d5a1d9596b8329312533b3a0047c815f8e71a201,"This paper studies the problem of pruning neural networks. The authors propose three pruning heuristics: SNIP, GraSP, and magnitude pruning. They show that SNIP and GraSP perform better than random pruning, and that the magnitude pruner performs better than the random pruner. In addition, the authors show that magnitude pruners are more effective than SNIP."
303,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,This paper proposes a new federated learning (FL) protocol called FED-LEARNING. The main idea of the paper is to use a robust mean estimator to estimate the dimension-free estimation error of the FL protocol. The authors show that the proposed method is able to achieve optimal or close-to-optimal performance on a variety of FL benchmarks. The paper also proposes a secure aggregation method called FilterL2 to improve the robustness of the proposed algorithm.
304,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,This paper proposes a new federated learning (FL) protocol called FED-LEARNING. The main idea of the paper is to use a robust mean estimator to estimate the dimension-free estimation error of the FL protocol. The authors show that the proposed method is able to achieve optimal or close-to-optimal performance on a variety of FL benchmarks. The paper also proposes a secure aggregation method called FilterL2 to improve the robustness of the proposed algorithm.
305,SP:1f6b266021da24bbf02b5a47f2b5eb23b4912166,This paper proposes a new federated learning (FL) protocol called FED-LEARNING. The main idea of the paper is to use a robust mean estimator to estimate the dimension-free estimation error of the FL protocol. The authors show that the proposed method is able to achieve optimal or close-to-optimal performance on a variety of FL benchmarks. The paper also proposes a secure aggregation method called FilterL2 to improve the robustness of the proposed algorithm.
306,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper proposes a method for offline multi-objective reinforcement learning (MBO) based on deep neural network function approximators. The main idea is to train a neural network to approximate the objective function of the offline MBO task, and then use the learned model to evaluate the performance of the model on a set of offline tasks. The paper presents a new benchmark, Design-Bench, which is a suite of benchmark tasks for offline reinforcement learning. The proposed method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines."
307,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper proposes a method for offline multi-objective reinforcement learning (MBO) based on deep neural network function approximators. The main idea is to train a neural network to approximate the objective function of the offline MBO task, and then use the learned model to evaluate the performance of the model on a set of offline tasks. The paper presents a new benchmark, Design-Bench, which is a suite of benchmark tasks for offline reinforcement learning. The proposed method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines."
308,SP:9f89ff90b203d86a569e3d5148546942f5bf2093,"This paper proposes a method for offline multi-objective reinforcement learning (MBO) based on deep neural network function approximators. The main idea is to train a neural network to approximate the objective function of the offline MBO task, and then use the learned model to evaluate the performance of the model on a set of offline tasks. The paper presents a new benchmark, Design-Bench, which is a suite of benchmark tasks for offline reinforcement learning. The proposed method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines."
309,SP:073958946c266bf760d1ad66bd39bc28a24c8521,This paper proposes a generalized ELBO formulation for multi-modal data. The main idea of the paper is to consider the joint distribution of the joint data distribution and the joint posterior approximation function. The proposed method is evaluated on a variety of self-supervised and generative learning tasks.
310,SP:073958946c266bf760d1ad66bd39bc28a24c8521,This paper proposes a generalized ELBO formulation for multi-modal data. The main idea of the paper is to consider the joint distribution of the joint data distribution and the joint posterior approximation function. The proposed method is evaluated on a variety of self-supervised and generative learning tasks.
311,SP:073958946c266bf760d1ad66bd39bc28a24c8521,This paper proposes a generalized ELBO formulation for multi-modal data. The main idea of the paper is to consider the joint distribution of the joint data distribution and the joint posterior approximation function. The proposed method is evaluated on a variety of self-supervised and generative learning tasks.
312,SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a prior-guided Bayesian Optimization (PrBO) method for optimizing expensive black-box functions. The main idea is to use a probabilistic model as a pseudo-prior for the optimization process, and then use a Bayesian optimizer (BO) to optimize the pseudo-posterior. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of benchmarks."
313,SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a prior-guided Bayesian Optimization (PrBO) method for optimizing expensive black-box functions. The main idea is to use a probabilistic model as a pseudo-prior for the optimization process, and then use a Bayesian optimizer (BO) to optimize the pseudo-posterior. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of benchmarks."
314,SP:98004554447b82b3d2eb9724ec551250eec7a595,"This paper proposes a prior-guided Bayesian Optimization (PrBO) method for optimizing expensive black-box functions. The main idea is to use a probabilistic model as a pseudo-prior for the optimization process, and then use a Bayesian optimizer (BO) to optimize the pseudo-posterior. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of benchmarks."
315,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper studies the computational cost of binary weight normalization in generative models. The authors argue that binary weights are more computationally efficient than regular models, and propose to use binarized models to reduce the computational complexity of binary models. In particular, the authors show that binarization can reduce the number of parameters in binary models by a factor of at least 1. They also show that binary models are more efficient than the regular models in terms of computational cost. "
316,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper studies the computational cost of binary weight normalization in generative models. The authors argue that binary weights are more computationally efficient than regular models, and propose to use binarized models to reduce the computational complexity of binary models. In particular, the authors show that binarization can reduce the number of parameters in binary models by a factor of at least 1. They also show that binary models are more efficient than the regular models in terms of computational cost. "
317,SP:e0e9cd5f39a60b5db1c4363ffdc2c593300ef43a,"This paper studies the computational cost of binary weight normalization in generative models. The authors argue that binary weights are more computationally efficient than regular models, and propose to use binarized models to reduce the computational complexity of binary models. In particular, the authors show that binarization can reduce the number of parameters in binary models by a factor of at least 1. They also show that binary models are more efficient than the regular models in terms of computational cost. "
318,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper studies the problem of model-based robustness in deep learning (DL). The authors propose a new paradigm for DL, which is based on perturbation-based adversarial robustness (ERM). The main idea of ERM is to train a classifier that is robust to perturbations that are bounded by the norm of the data distribution. The authors show that ERM can be used to improve the robustness of models of natural variation, and show that it can be combined with adversarial training and domain adaptation techniques to improve robustness. The proposed method is evaluated on ImageNet-c and CIFAR-10."
319,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper studies the problem of model-based robustness in deep learning (DL). The authors propose a new paradigm for DL, which is based on perturbation-based adversarial robustness (ERM). The main idea of ERM is to train a classifier that is robust to perturbations that are bounded by the norm of the data distribution. The authors show that ERM can be used to improve the robustness of models of natural variation, and show that it can be combined with adversarial training and domain adaptation techniques to improve robustness. The proposed method is evaluated on ImageNet-c and CIFAR-10."
320,SP:7e9a83552c0ff001d3090a5a7162013b5dc6f47f,"This paper studies the problem of model-based robustness in deep learning (DL). The authors propose a new paradigm for DL, which is based on perturbation-based adversarial robustness (ERM). The main idea of ERM is to train a classifier that is robust to perturbations that are bounded by the norm of the data distribution. The authors show that ERM can be used to improve the robustness of models of natural variation, and show that it can be combined with adversarial training and domain adaptation techniques to improve robustness. The proposed method is evaluated on ImageNet-c and CIFAR-10."
321,SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the problem of learning ReLU activations for two-layer and three-layer convolutional neural networks (CNNs) with ReLU layers. The authors propose a regularization of the regularizer of the ReLU layer of a CNN to improve the sparsity of the network. The main contribution of the paper is to show that the proposed regularizer can be viewed as a regularized version of the `1 norm regularized convex program. This regularizer is shown to have a semi-infinite duality, and the authors show that it can be used to reduce the number of layers in a CNN. They also show that this regularization can be applied to multi-layer circular CNNs."
322,SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the problem of learning ReLU activations for two-layer and three-layer convolutional neural networks (CNNs) with ReLU layers. The authors propose a regularization of the regularizer of the ReLU layer of a CNN to improve the sparsity of the network. The main contribution of the paper is to show that the proposed regularizer can be viewed as a regularized version of the `1 norm regularized convex program. This regularizer is shown to have a semi-infinite duality, and the authors show that it can be used to reduce the number of layers in a CNN. They also show that this regularization can be applied to multi-layer circular CNNs."
323,SP:011dab90d225550e77235cbec1615e583ae3297e,"This paper studies the problem of learning ReLU activations for two-layer and three-layer convolutional neural networks (CNNs) with ReLU layers. The authors propose a regularization of the regularizer of the ReLU layer of a CNN to improve the sparsity of the network. The main contribution of the paper is to show that the proposed regularizer can be viewed as a regularized version of the `1 norm regularized convex program. This regularizer is shown to have a semi-infinite duality, and the authors show that it can be used to reduce the number of layers in a CNN. They also show that this regularization can be applied to multi-layer circular CNNs."
324,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a probabilistic generative model for the problem of training a robot to solve a set of table-top robot manipulation tasks. The problem is formulated as a generative optimization problem, where the goal is to find the optimal configuration of a soft sponge that maximizes the performance of the robot on the given task. The authors propose to use a neural network to model the soft sponge, which is trained to predict the distribution of the soft sponges. The model is trained using a combination of a deep neural network (DNN) and an encoder-decoder network (CNN). The authors evaluate the proposed model on a series of tasks on the PR2 robot and show that the proposed method outperforms the baselines."
325,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a probabilistic generative model for the problem of training a robot to solve a set of table-top robot manipulation tasks. The problem is formulated as a generative optimization problem, where the goal is to find the optimal configuration of a soft sponge that maximizes the performance of the robot on the given task. The authors propose to use a neural network to model the soft sponge, which is trained to predict the distribution of the soft sponges. The model is trained using a combination of a deep neural network (DNN) and an encoder-decoder network (CNN). The authors evaluate the proposed model on a series of tasks on the PR2 robot and show that the proposed method outperforms the baselines."
326,SP:98760a3b1a5058a485a5a1ed1b778c1d4fb2ff22,"This paper proposes a probabilistic generative model for the problem of training a robot to solve a set of table-top robot manipulation tasks. The problem is formulated as a generative optimization problem, where the goal is to find the optimal configuration of a soft sponge that maximizes the performance of the robot on the given task. The authors propose to use a neural network to model the soft sponge, which is trained to predict the distribution of the soft sponges. The model is trained using a combination of a deep neural network (DNN) and an encoder-decoder network (CNN). The authors evaluate the proposed model on a series of tasks on the PR2 robot and show that the proposed method outperforms the baselines."
327,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes a novel method for fine-tuning pretrained language models for low-resource tasks. The main idea is to use Variational Information Bottleneck (VIB) to remove irrelevant features from sentence representations. The proposed method is evaluated on a variety of tasks, including transfer learning, transfer learning on out-of-domain datasets, and generalization to new tasks. Experiments show that the proposed method outperforms existing methods."
328,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes a novel method for fine-tuning pretrained language models for low-resource tasks. The main idea is to use Variational Information Bottleneck (VIB) to remove irrelevant features from sentence representations. The proposed method is evaluated on a variety of tasks, including transfer learning, transfer learning on out-of-domain datasets, and generalization to new tasks. Experiments show that the proposed method outperforms existing methods."
329,SP:e171d8c4eadf73852734c0fb8a74a69d80969e4b,"This paper proposes a novel method for fine-tuning pretrained language models for low-resource tasks. The main idea is to use Variational Information Bottleneck (VIB) to remove irrelevant features from sentence representations. The proposed method is evaluated on a variety of tasks, including transfer learning, transfer learning on out-of-domain datasets, and generalization to new tasks. Experiments show that the proposed method outperforms existing methods."
330,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method for 3D shape reconstruction using a pre-trained generative adversarial network (GAN). The main idea is to use a 2D GAN to generate a 3D keypoint and an annotated 3D image, which is then used to train the GAN. The method is evaluated on a variety of tasks, including face rotation, object relighting, and shape reconstruction."
331,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method for 3D shape reconstruction using a pre-trained generative adversarial network (GAN). The main idea is to use a 2D GAN to generate a 3D keypoint and an annotated 3D image, which is then used to train the GAN. The method is evaluated on a variety of tasks, including face rotation, object relighting, and shape reconstruction."
332,SP:a1ab99bee74a0a1310537beced0d89dc1e5ad7be,"This paper proposes a method for 3D shape reconstruction using a pre-trained generative adversarial network (GAN). The main idea is to use a 2D GAN to generate a 3D keypoint and an annotated 3D image, which is then used to train the GAN. The method is evaluated on a variety of tasks, including face rotation, object relighting, and shape reconstruction."
333,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a novel method for long-tailed classification. The proposed method is based on a distribution-aware diversity loss, which aims to mitigate the model bias and model variance. The authors also propose a dynamic expert routing module to reduce the computational cost of the training data. The experimental results on CIFAR-100-LT and ImageNet-LT show that the proposed method outperforms the state-of-the-art."
334,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a novel method for long-tailed classification. The proposed method is based on a distribution-aware diversity loss, which aims to mitigate the model bias and model variance. The authors also propose a dynamic expert routing module to reduce the computational cost of the training data. The experimental results on CIFAR-100-LT and ImageNet-LT show that the proposed method outperforms the state-of-the-art."
335,SP:eac0679dfee4dae78c1e515f8b325c9523b795dc,"This paper proposes a novel method for long-tailed classification. The proposed method is based on a distribution-aware diversity loss, which aims to mitigate the model bias and model variance. The authors also propose a dynamic expert routing module to reduce the computational cost of the training data. The experimental results on CIFAR-100-LT and ImageNet-LT show that the proposed method outperforms the state-of-the-art."
336,SP:f4d0e821de6830722a3458fd40d8d6793a107827,This paper studies the problem of channel pruning in convolutional neural networks (CNNs). The authors propose two pruning criteria: layer-wise pruning and global pruning. The first pruning criterion is based on the Convolutional Weight Distribution Assumption (CWDA) and the second one uses the Importance Score (IS) to determine the importance of each filter. The authors show that the weight distribution of the pruned filters is Gaussian-similar to the original distribution of filters. They also show that layer pruning can be combined with layer-wide pruning to find the optimal pruning strategy.
337,SP:f4d0e821de6830722a3458fd40d8d6793a107827,This paper studies the problem of channel pruning in convolutional neural networks (CNNs). The authors propose two pruning criteria: layer-wise pruning and global pruning. The first pruning criterion is based on the Convolutional Weight Distribution Assumption (CWDA) and the second one uses the Importance Score (IS) to determine the importance of each filter. The authors show that the weight distribution of the pruned filters is Gaussian-similar to the original distribution of filters. They also show that layer pruning can be combined with layer-wide pruning to find the optimal pruning strategy.
338,SP:f4d0e821de6830722a3458fd40d8d6793a107827,This paper studies the problem of channel pruning in convolutional neural networks (CNNs). The authors propose two pruning criteria: layer-wise pruning and global pruning. The first pruning criterion is based on the Convolutional Weight Distribution Assumption (CWDA) and the second one uses the Importance Score (IS) to determine the importance of each filter. The authors show that the weight distribution of the pruned filters is Gaussian-similar to the original distribution of filters. They also show that layer pruning can be combined with layer-wide pruning to find the optimal pruning strategy.
339,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for code understanding. The model is based on the Transformer architecture. The main idea is to use a graph-guided masked attention function to learn the structure of code. The proposed model is evaluated on three tasks: code refinement, code search, and clone detection."
340,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for code understanding. The model is based on the Transformer architecture. The main idea is to use a graph-guided masked attention function to learn the structure of code. The proposed model is evaluated on three tasks: code refinement, code search, and clone detection."
341,SP:eadb827653b2e1b608bb923d5549089cb2482d90,"This paper proposes a pre-trained model for code understanding. The model is based on the Transformer architecture. The main idea is to use a graph-guided masked attention function to learn the structure of code. The proposed model is evaluated on three tasks: code refinement, code search, and clone detection."
342,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method for regression on unlabeled data. The proposed method is based on adversarial network, where the model is trained to estimate the true distribution of the labeled data, and then the authors propose a forcing algorithm for regression. The authors show that the proposed method can improve the accuracy of regression models on pLogP and Diamond datasets. They also show that their method can be applied to real-world datasets."
343,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method for regression on unlabeled data. The proposed method is based on adversarial network, where the model is trained to estimate the true distribution of the labeled data, and then the authors propose a forcing algorithm for regression. The authors show that the proposed method can improve the accuracy of regression models on pLogP and Diamond datasets. They also show that their method can be applied to real-world datasets."
344,SP:2c7a128e19cd2d39b0ca1b946b01604c3f7cead5,"This paper proposes a method for regression on unlabeled data. The proposed method is based on adversarial network, where the model is trained to estimate the true distribution of the labeled data, and then the authors propose a forcing algorithm for regression. The authors show that the proposed method can improve the accuracy of regression models on pLogP and Diamond datasets. They also show that their method can be applied to real-world datasets."
345,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper proposes a method for learning compositional representations for out-of-distribution generalization in neural networks. The authors propose to use a regularized auxiliary reconstruction network to reconstruct the original representation of the input data, and then use the reconstructed representation as a regularizer for the reconstruction of the original data. The proposed method is evaluated on MNIST and CIFAR-10 datasets, and compared to several baselines."
346,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper proposes a method for learning compositional representations for out-of-distribution generalization in neural networks. The authors propose to use a regularized auxiliary reconstruction network to reconstruct the original representation of the input data, and then use the reconstructed representation as a regularizer for the reconstruction of the original data. The proposed method is evaluated on MNIST and CIFAR-10 datasets, and compared to several baselines."
347,SP:fee1e40275fa743aa6ad011ae742b3ea3fd137df,"This paper proposes a method for learning compositional representations for out-of-distribution generalization in neural networks. The authors propose to use a regularized auxiliary reconstruction network to reconstruct the original representation of the input data, and then use the reconstructed representation as a regularizer for the reconstruction of the original data. The proposed method is evaluated on MNIST and CIFAR-10 datasets, and compared to several baselines."
348,SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper studies the problem of poisoning in reinforcement learning (RL). The authors propose a new poisoning algorithm, VA2C-P, which is based on the stability radius of the MDP. The stability radius is defined as the distance between the agent's state and the target state. The authors show that the proposed algorithm can be applied to both policy-based and on-policy deep RL agents."
349,SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper studies the problem of poisoning in reinforcement learning (RL). The authors propose a new poisoning algorithm, VA2C-P, which is based on the stability radius of the MDP. The stability radius is defined as the distance between the agent's state and the target state. The authors show that the proposed algorithm can be applied to both policy-based and on-policy deep RL agents."
350,SP:ffab573a977c819e86601de74690c29a39c264cd,"This paper studies the problem of poisoning in reinforcement learning (RL). The authors propose a new poisoning algorithm, VA2C-P, which is based on the stability radius of the MDP. The stability radius is defined as the distance between the agent's state and the target state. The authors show that the proposed algorithm can be applied to both policy-based and on-policy deep RL agents."
351,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper studies the problem of dynamic checkpointing in deep learning models. The authors propose a greedy online algorithm called Dynamic Tensor Rematerialization (DTR) for checkpointing. DTR is based on the idea of lightweight metadata in tensors. Theoretically, the authors show that DTR can be used to reduce the memory budget of an N-layer linear feedforward network with O(N) tensor operations. Experiments on PyTorch demonstrate the effectiveness of DTR."
352,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper studies the problem of dynamic checkpointing in deep learning models. The authors propose a greedy online algorithm called Dynamic Tensor Rematerialization (DTR) for checkpointing. DTR is based on the idea of lightweight metadata in tensors. Theoretically, the authors show that DTR can be used to reduce the memory budget of an N-layer linear feedforward network with O(N) tensor operations. Experiments on PyTorch demonstrate the effectiveness of DTR."
353,SP:06ebd437ff2d1b5068f7a651716d3c1a60c2a001,"This paper studies the problem of dynamic checkpointing in deep learning models. The authors propose a greedy online algorithm called Dynamic Tensor Rematerialization (DTR) for checkpointing. DTR is based on the idea of lightweight metadata in tensors. Theoretically, the authors show that DTR can be used to reduce the memory budget of an N-layer linear feedforward network with O(N) tensor operations. Experiments on PyTorch demonstrate the effectiveness of DTR."
354,SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a novel method for hallucination detection based on neural sequence models. The proposed method is based on the idea of token-level hallucination labels, which can be used as a fine-grained loss for low-resource machine translation and abstract text summarization tasks. The method is evaluated on two benchmark datasets and compared to several baseline methods. The results show that the proposed method outperforms the baseline methods on both synthetic and real-world datasets."
355,SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a novel method for hallucination detection based on neural sequence models. The proposed method is based on the idea of token-level hallucination labels, which can be used as a fine-grained loss for low-resource machine translation and abstract text summarization tasks. The method is evaluated on two benchmark datasets and compared to several baseline methods. The results show that the proposed method outperforms the baseline methods on both synthetic and real-world datasets."
356,SP:20efc610911443724b56f57f857060d0e0302243,"This paper proposes a novel method for hallucination detection based on neural sequence models. The proposed method is based on the idea of token-level hallucination labels, which can be used as a fine-grained loss for low-resource machine translation and abstract text summarization tasks. The method is evaluated on two benchmark datasets and compared to several baseline methods. The results show that the proposed method outperforms the baseline methods on both synthetic and real-world datasets."
357,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,This paper proposes a method for conditional generative adversarial networks (cGANs) where the class-specific information is not available to the class generator. The main idea is to use a weight-sharing pipeline and a mixed-architecture optimization method to optimize the search space of the cGAN. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets and compared to the state-of-the-art cGAN models.
358,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,This paper proposes a method for conditional generative adversarial networks (cGANs) where the class-specific information is not available to the class generator. The main idea is to use a weight-sharing pipeline and a mixed-architecture optimization method to optimize the search space of the cGAN. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets and compared to the state-of-the-art cGAN models.
359,SP:3d0d026888cf87073df5bd74edd986f15351ff5a,This paper proposes a method for conditional generative adversarial networks (cGANs) where the class-specific information is not available to the class generator. The main idea is to use a weight-sharing pipeline and a mixed-architecture optimization method to optimize the search space of the cGAN. The proposed method is evaluated on CIFAR-10 and Cifar-100 datasets and compared to the state-of-the-art cGAN models.
360,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a novel method for unconfounded treatment assignment in the context of causal inference. The method is based on the orthogonality constraint, which is a regularization term that encourages orthogonality between treatments. The authors show that the proposed method can be used to estimate the average causal effect of a treatment in the setting where the treatment assignment is not known. The main contribution of this paper is to propose a novel regularization method for the unconfounded treatment assignment. "
361,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a novel method for unconfounded treatment assignment in the context of causal inference. The method is based on the orthogonality constraint, which is a regularization term that encourages orthogonality between treatments. The authors show that the proposed method can be used to estimate the average causal effect of a treatment in the setting where the treatment assignment is not known. The main contribution of this paper is to propose a novel regularization method for the unconfounded treatment assignment. "
362,SP:8cdf6e8af07daaec6680c2bed6c1787a53580584,"This paper proposes a novel method for unconfounded treatment assignment in the context of causal inference. The method is based on the orthogonality constraint, which is a regularization term that encourages orthogonality between treatments. The authors show that the proposed method can be used to estimate the average causal effect of a treatment in the setting where the treatment assignment is not known. The main contribution of this paper is to propose a novel regularization method for the unconfounded treatment assignment. "
363,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the effect of random initializations on the performance of deep neural networks. The authors propose a new method called BatchNorm, which is based on affine transformations of features. The main idea is to use the affine parameters of a neural network as input to a batch-norm network, and then to update the parameters of the network based on the output of the batch norm network. The proposed method is evaluated on a variety of tasks, including style transfer, multitask learning, and image classification. The results show that the proposed method outperforms the baselines in terms of accuracy and style transfer."
364,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the effect of random initializations on the performance of deep neural networks. The authors propose a new method called BatchNorm, which is based on affine transformations of features. The main idea is to use the affine parameters of a neural network as input to a batch-norm network, and then to update the parameters of the network based on the output of the batch norm network. The proposed method is evaluated on a variety of tasks, including style transfer, multitask learning, and image classification. The results show that the proposed method outperforms the baselines in terms of accuracy and style transfer."
365,SP:77ec2512837df5c0a94000602dc2ef5c03fe41dd,"This paper studies the effect of random initializations on the performance of deep neural networks. The authors propose a new method called BatchNorm, which is based on affine transformations of features. The main idea is to use the affine parameters of a neural network as input to a batch-norm network, and then to update the parameters of the network based on the output of the batch norm network. The proposed method is evaluated on a variety of tasks, including style transfer, multitask learning, and image classification. The results show that the proposed method outperforms the baselines in terms of accuracy and style transfer."
366,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a new method for source-free domain adaptation based on channel-wise affine transformations. The proposed method, Tent, is based on the idea of normalization statistics, which can be used for test-time adaptation. The method is evaluated on the VisDA-C benchmark and the CIFAR-10/100 benchmark. Tent is shown to outperform the state-of-the-art methods on both SVHN and MNIST/MNIST-M/USPS datasets."
367,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a new method for source-free domain adaptation based on channel-wise affine transformations. The proposed method, Tent, is based on the idea of normalization statistics, which can be used for test-time adaptation. The method is evaluated on the VisDA-C benchmark and the CIFAR-10/100 benchmark. Tent is shown to outperform the state-of-the-art methods on both SVHN and MNIST/MNIST-M/USPS datasets."
368,SP:6683ceea773ff6d7fb613e503c583bb2979c7e89,"This paper proposes a new method for source-free domain adaptation based on channel-wise affine transformations. The proposed method, Tent, is based on the idea of normalization statistics, which can be used for test-time adaptation. The method is evaluated on the VisDA-C benchmark and the CIFAR-10/100 benchmark. Tent is shown to outperform the state-of-the-art methods on both SVHN and MNIST/MNIST-M/USPS datasets."
369,SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method for uncertainty quantification in RNNs. The method is based on the observation that the probability distribution of the state transition distribution of a RNN can be approximated by a probabilistic automata. The authors propose a method to estimate the probability of the transition distribution for a given RNN. The proposed method is evaluated on a variety of tasks, including out-of-distribution detection and reinforcement learning."
370,SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method for uncertainty quantification in RNNs. The method is based on the observation that the probability distribution of the state transition distribution of a RNN can be approximated by a probabilistic automata. The authors propose a method to estimate the probability of the transition distribution for a given RNN. The proposed method is evaluated on a variety of tasks, including out-of-distribution detection and reinforcement learning."
371,SP:ed544ee661580592063aa17aee8924cc99919130,"This paper proposes a method for uncertainty quantification in RNNs. The method is based on the observation that the probability distribution of the state transition distribution of a RNN can be approximated by a probabilistic automata. The authors propose a method to estimate the probability of the transition distribution for a given RNN. The proposed method is evaluated on a variety of tasks, including out-of-distribution detection and reinforcement learning."
372,SP:a38c523196f68a90b5db45671f9dbd87981a024c,This paper studies the problem of privacy-preserving deep learning in the context of residual perturbation. The authors propose a new method for privacy preserving deep learning based on the stochastic differential equation (SDE). The authors show that the proposed method can be applied to the residual mapping of ResNets. The proposed method is motivated by the observation that the generalization gap between DP and ResNet can be reduced to zero when the data privacy is preserved. 
373,SP:a38c523196f68a90b5db45671f9dbd87981a024c,This paper studies the problem of privacy-preserving deep learning in the context of residual perturbation. The authors propose a new method for privacy preserving deep learning based on the stochastic differential equation (SDE). The authors show that the proposed method can be applied to the residual mapping of ResNets. The proposed method is motivated by the observation that the generalization gap between DP and ResNet can be reduced to zero when the data privacy is preserved. 
374,SP:a38c523196f68a90b5db45671f9dbd87981a024c,This paper studies the problem of privacy-preserving deep learning in the context of residual perturbation. The authors propose a new method for privacy preserving deep learning based on the stochastic differential equation (SDE). The authors show that the proposed method can be applied to the residual mapping of ResNets. The proposed method is motivated by the observation that the generalization gap between DP and ResNet can be reduced to zero when the data privacy is preserved. 
375,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes a novel extension of the Drop-and-Restore (DR) algorithm for large-scale transformers. The authors propose to use a structural variant of dropout, called LengthDrop, which is a multi-objective evolutionary search to find the optimal length configuration for each word-vector. The proposed method is evaluated on span-based question-answer and sequence-level classification tasks, and compared with PoWER-BERT, SQuAD, and MNLI-m. "
376,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes a novel extension of the Drop-and-Restore (DR) algorithm for large-scale transformers. The authors propose to use a structural variant of dropout, called LengthDrop, which is a multi-objective evolutionary search to find the optimal length configuration for each word-vector. The proposed method is evaluated on span-based question-answer and sequence-level classification tasks, and compared with PoWER-BERT, SQuAD, and MNLI-m. "
377,SP:9cbe32c1317889d6a3ec1b0798112d9b82cc7f67,"This paper proposes a novel extension of the Drop-and-Restore (DR) algorithm for large-scale transformers. The authors propose to use a structural variant of dropout, called LengthDrop, which is a multi-objective evolutionary search to find the optimal length configuration for each word-vector. The proposed method is evaluated on span-based question-answer and sequence-level classification tasks, and compared with PoWER-BERT, SQuAD, and MNLI-m. "
378,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressiveness of aggregation-based graph neural networks (GNNs) under the 1-WL test. In particular, the authors propose a new aggregation coefficient matrix for aggregators and injective aggregation for GNNs. The authors show that the aggregation coefficient can be used to evaluate the rank of hidden features in GNN layers. The paper also shows that the expressive power of GNN can be improved by adding nonlinear units."
379,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressiveness of aggregation-based graph neural networks (GNNs) under the 1-WL test. In particular, the authors propose a new aggregation coefficient matrix for aggregators and injective aggregation for GNNs. The authors show that the aggregation coefficient can be used to evaluate the rank of hidden features in GNN layers. The paper also shows that the expressive power of GNN can be improved by adding nonlinear units."
380,SP:e5b4098ea22a5da2b9659219dc24f885c493a011,"This paper studies the expressiveness of aggregation-based graph neural networks (GNNs) under the 1-WL test. In particular, the authors propose a new aggregation coefficient matrix for aggregators and injective aggregation for GNNs. The authors show that the aggregation coefficient can be used to evaluate the rank of hidden features in GNN layers. The paper also shows that the expressive power of GNN can be improved by adding nonlinear units."
381,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,This paper proposes a method for learning disentangled representations. The key idea is to measure disentanglement between two conditional submanifolds of a representation by measuring the topological similarity of the conditional sub-manifold. The method is based on a generative model that is trained on a set of samples from a dataset. The authors show that the proposed method can be applied to both unsupervised and supervised settings. The experimental results show that their method is able to outperform the state-of-the-art.
382,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,This paper proposes a method for learning disentangled representations. The key idea is to measure disentanglement between two conditional submanifolds of a representation by measuring the topological similarity of the conditional sub-manifold. The method is based on a generative model that is trained on a set of samples from a dataset. The authors show that the proposed method can be applied to both unsupervised and supervised settings. The experimental results show that their method is able to outperform the state-of-the-art.
383,SP:4dd6fb8e5a356af270d3b296ce3d50ae5753513c,This paper proposes a method for learning disentangled representations. The key idea is to measure disentanglement between two conditional submanifolds of a representation by measuring the topological similarity of the conditional sub-manifold. The method is based on a generative model that is trained on a set of samples from a dataset. The authors show that the proposed method can be applied to both unsupervised and supervised settings. The experimental results show that their method is able to outperform the state-of-the-art.
384,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper studies the problem of unauthorized data exploitation in the context of error-minimizing noise in deep learning models. The authors propose a new definition of error minimization noise, which they call Error Minimizing Noise (EMN), which is defined as the difference between the sample-wise and class-wise noise. They show that EMN can be expressed as a function of the sample size and the class of the model, and show that it can be viewed as a measure of the data utility of a model. They also provide a theoretical analysis of EMN."
385,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper studies the problem of unauthorized data exploitation in the context of error-minimizing noise in deep learning models. The authors propose a new definition of error minimization noise, which they call Error Minimizing Noise (EMN), which is defined as the difference between the sample-wise and class-wise noise. They show that EMN can be expressed as a function of the sample size and the class of the model, and show that it can be viewed as a measure of the data utility of a model. They also provide a theoretical analysis of EMN."
386,SP:ef1ee7b77e1c2fb3d76db27049a3bce42760d14e,"This paper studies the problem of unauthorized data exploitation in the context of error-minimizing noise in deep learning models. The authors propose a new definition of error minimization noise, which they call Error Minimizing Noise (EMN), which is defined as the difference between the sample-wise and class-wise noise. They show that EMN can be expressed as a function of the sample size and the class of the model, and show that it can be viewed as a measure of the data utility of a model. They also provide a theoretical analysis of EMN."
387,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,This paper proposes a novel algorithm for solving MuZero games. The algorithm is based on a model of environmental dynamics that is trained to predict the future state of the environment. The model is trained using a combination of MuZero and a tree search algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art AlphaZero algorithm on Atari games.
388,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,This paper proposes a novel algorithm for solving MuZero games. The algorithm is based on a model of environmental dynamics that is trained to predict the future state of the environment. The model is trained using a combination of MuZero and a tree search algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art AlphaZero algorithm on Atari games.
389,SP:4e8a835174f20df36d3d8d27fbcbbf2c68490032,This paper proposes a novel algorithm for solving MuZero games. The algorithm is based on a model of environmental dynamics that is trained to predict the future state of the environment. The model is trained using a combination of MuZero and a tree search algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art AlphaZero algorithm on Atari games.
390,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes Hindsight Off-Policy Options (HO2), an option learning algorithm that combines temporal and action abstraction. The authors propose a dynamic programming inference procedure for off-policy training and a backpropagation procedure for option learning. The experimental results show that HO2 outperforms the state-of-the-art option learning algorithms."
391,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes Hindsight Off-Policy Options (HO2), an option learning algorithm that combines temporal and action abstraction. The authors propose a dynamic programming inference procedure for off-policy training and a backpropagation procedure for option learning. The experimental results show that HO2 outperforms the state-of-the-art option learning algorithms."
392,SP:73ae9c167dac3d92788a08891b0831f3e4997140,"This paper proposes Hindsight Off-Policy Options (HO2), an option learning algorithm that combines temporal and action abstraction. The authors propose a dynamic programming inference procedure for off-policy training and a backpropagation procedure for option learning. The experimental results show that HO2 outperforms the state-of-the-art option learning algorithms."
393,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel method for optimizing the expected cumulative return in reinforcement learning (RL) algorithms. The main idea is to use a functional form of the Bellman equation to approximate the expected maximum reward, which can be expressed as a weighted sum of Bellman operators. The authors show that the proposed method can be applied to the problem of synthesizable molecule generation and show that it can achieve better performance than the state-of-the-art methods."
394,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel method for optimizing the expected cumulative return in reinforcement learning (RL) algorithms. The main idea is to use a functional form of the Bellman equation to approximate the expected maximum reward, which can be expressed as a weighted sum of Bellman operators. The authors show that the proposed method can be applied to the problem of synthesizable molecule generation and show that it can achieve better performance than the state-of-the-art methods."
395,SP:f79d9722256fb6b258bc1310bf1f6fb842303a0a,"This paper proposes a novel method for optimizing the expected cumulative return in reinforcement learning (RL) algorithms. The main idea is to use a functional form of the Bellman equation to approximate the expected maximum reward, which can be expressed as a weighted sum of Bellman operators. The authors show that the proposed method can be applied to the problem of synthesizable molecule generation and show that it can achieve better performance than the state-of-the-art methods."
396,SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model for few-shot learning. The proposed model is based on a partial variational autoencoder (P-VAE) and a contextual hypernetwork. The authors show that the proposed CHN is able to recover the missing features in sparsely-observed data. The experimental results show that CHN outperforms the baselines in few shot learning."
397,SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model for few-shot learning. The proposed model is based on a partial variational autoencoder (P-VAE) and a contextual hypernetwork. The authors show that the proposed CHN is able to recover the missing features in sparsely-observed data. The experimental results show that CHN outperforms the baselines in few shot learning."
398,SP:bd4b1781448def4327214c78f07538d285119ef9,"This paper proposes Contextual HyperNetwork (CHN), an auxiliary model for few-shot learning. The proposed model is based on a partial variational autoencoder (P-VAE) and a contextual hypernetwork. The authors show that the proposed CHN is able to recover the missing features in sparsely-observed data. The experimental results show that CHN outperforms the baselines in few shot learning."
399,SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a Bayesian deep learning method for Bayesian inference. The proposed method is based on a subnetwork selection procedure, where the subnetwork is trained to minimize the posterior uncertainty of the full model. The authors show that the proposed method can be used to approximate the full covariance Gaussian posterior approximation of a deep neural network. The main contribution of this paper is to propose a method for estimating the posterior of a neural network that is expressive of the model parameters. The method is evaluated on MNIST and CIFAR-10 datasets."
400,SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a Bayesian deep learning method for Bayesian inference. The proposed method is based on a subnetwork selection procedure, where the subnetwork is trained to minimize the posterior uncertainty of the full model. The authors show that the proposed method can be used to approximate the full covariance Gaussian posterior approximation of a deep neural network. The main contribution of this paper is to propose a method for estimating the posterior of a neural network that is expressive of the model parameters. The method is evaluated on MNIST and CIFAR-10 datasets."
401,SP:8e4677cc6071a33397347679308165c10dca2aae,"This paper proposes a Bayesian deep learning method for Bayesian inference. The proposed method is based on a subnetwork selection procedure, where the subnetwork is trained to minimize the posterior uncertainty of the full model. The authors show that the proposed method can be used to approximate the full covariance Gaussian posterior approximation of a deep neural network. The main contribution of this paper is to propose a method for estimating the posterior of a neural network that is expressive of the model parameters. The method is evaluated on MNIST and CIFAR-10 datasets."
402,SP:be361952fe9de545f68b8a060f790d54c6755998,This paper proposes a model-free reinforcement learning method that jointly learns embeddings for both the policy and the state representations. The proposed method is based on the idea that the embedding of the policy should be similar to that of the state representation. The authors show that the proposed method outperforms the baselines in both discrete and continuous domains.
403,SP:be361952fe9de545f68b8a060f790d54c6755998,This paper proposes a model-free reinforcement learning method that jointly learns embeddings for both the policy and the state representations. The proposed method is based on the idea that the embedding of the policy should be similar to that of the state representation. The authors show that the proposed method outperforms the baselines in both discrete and continuous domains.
404,SP:be361952fe9de545f68b8a060f790d54c6755998,This paper proposes a model-free reinforcement learning method that jointly learns embeddings for both the policy and the state representations. The proposed method is based on the idea that the embedding of the policy should be similar to that of the state representation. The authors show that the proposed method outperforms the baselines in both discrete and continuous domains.
405,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper studies the problem of unsupervised representation learning in the presence of adversarial perturbations. The authors propose a viewmaker network, which is a generative model that can be trained to generate views that are robust to adversarial attacks. The proposed method is evaluated on CIFAR-10, SimCLR, and wearable sensor data. The results show that the proposed method outperforms state-of-the-art methods."
406,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper studies the problem of unsupervised representation learning in the presence of adversarial perturbations. The authors propose a viewmaker network, which is a generative model that can be trained to generate views that are robust to adversarial attacks. The proposed method is evaluated on CIFAR-10, SimCLR, and wearable sensor data. The results show that the proposed method outperforms state-of-the-art methods."
407,SP:ebb6bffcc4c2129e09ef5561c19df43c42ad18c0,"This paper studies the problem of unsupervised representation learning in the presence of adversarial perturbations. The authors propose a viewmaker network, which is a generative model that can be trained to generate views that are robust to adversarial attacks. The proposed method is evaluated on CIFAR-10, SimCLR, and wearable sensor data. The results show that the proposed method outperforms state-of-the-art methods."
408,SP:ef7735be9423ad53059505c170e75201ca134573,"This paper proposes a method to detect outliers in MNIST, CIFAR-10, and SVHN datasets. The method is based on the observation that outliers are more likely to appear in out-of-distribution (OOD) data than in the in-domain data. The authors propose a method for detecting outliers based on statistical, geometric, or topological signatures of outliers. The proposed method is evaluated on a variety of datasets and architectures. The results show that the proposed method outperforms existing methods."
409,SP:ef7735be9423ad53059505c170e75201ca134573,"This paper proposes a method to detect outliers in MNIST, CIFAR-10, and SVHN datasets. The method is based on the observation that outliers are more likely to appear in out-of-distribution (OOD) data than in the in-domain data. The authors propose a method for detecting outliers based on statistical, geometric, or topological signatures of outliers. The proposed method is evaluated on a variety of datasets and architectures. The results show that the proposed method outperforms existing methods."
410,SP:ef7735be9423ad53059505c170e75201ca134573,"This paper proposes a method to detect outliers in MNIST, CIFAR-10, and SVHN datasets. The method is based on the observation that outliers are more likely to appear in out-of-distribution (OOD) data than in the in-domain data. The authors propose a method for detecting outliers based on statistical, geometric, or topological signatures of outliers. The proposed method is evaluated on a variety of datasets and architectures. The results show that the proposed method outperforms existing methods."
411,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model for high-resolution images. The authors propose a hierarchical VAE with stochastic depth and a multiscale generative procedure. The proposed method is evaluated on ImageNet, FFHQ and FFHQ-256 and compared to the state-of-the-art PixelCNN. The results show that the proposed method outperforms the PixelCNN on all three datasets."
412,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model for high-resolution images. The authors propose a hierarchical VAE with stochastic depth and a multiscale generative procedure. The proposed method is evaluated on ImageNet, FFHQ and FFHQ-256 and compared to the state-of-the-art PixelCNN. The results show that the proposed method outperforms the PixelCNN on all three datasets."
413,SP:33920ec7ffefe3c1525cd5d4d53a851210d519da,"This paper proposes a VAE-based generative model for high-resolution images. The authors propose a hierarchical VAE with stochastic depth and a multiscale generative procedure. The proposed method is evaluated on ImageNet, FFHQ and FFHQ-256 and compared to the state-of-the-art PixelCNN. The results show that the proposed method outperforms the PixelCNN on all three datasets."
414,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a novel contrastive learning method for unsupervised visual representation learning. The authors propose to use semi-hard negatives as negative examples for contrastive representation learning, and then use a contrastive loss to estimate the mutual information between the negative examples and the positive examples. The proposed method is evaluated on a variety of image datasets and compared to other contrastive methods. The results show that the proposed method outperforms the existing methods."
415,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a novel contrastive learning method for unsupervised visual representation learning. The authors propose to use semi-hard negatives as negative examples for contrastive representation learning, and then use a contrastive loss to estimate the mutual information between the negative examples and the positive examples. The proposed method is evaluated on a variety of image datasets and compared to other contrastive methods. The results show that the proposed method outperforms the existing methods."
416,SP:0a4e6c8017a1294fe2424799a0048d58eaf04cb3,"This paper proposes a novel contrastive learning method for unsupervised visual representation learning. The authors propose to use semi-hard negatives as negative examples for contrastive representation learning, and then use a contrastive loss to estimate the mutual information between the negative examples and the positive examples. The proposed method is evaluated on a variety of image datasets and compared to other contrastive methods. The results show that the proposed method outperforms the existing methods."
417,SP:613a0e2d8cbe703f37c182553801be7537333f64,"This paper studies the problem of catastrophic data leakage in federated learning (FL). The authors propose a new method called CAFE, which aggregates gradients from different clients. The authors show that CAFE is able to recover large batches of training data from a single client. They also provide theoretical analysis of CAFE."
418,SP:613a0e2d8cbe703f37c182553801be7537333f64,"This paper studies the problem of catastrophic data leakage in federated learning (FL). The authors propose a new method called CAFE, which aggregates gradients from different clients. The authors show that CAFE is able to recover large batches of training data from a single client. They also provide theoretical analysis of CAFE."
419,SP:613a0e2d8cbe703f37c182553801be7537333f64,"This paper studies the problem of catastrophic data leakage in federated learning (FL). The authors propose a new method called CAFE, which aggregates gradients from different clients. The authors show that CAFE is able to recover large batches of training data from a single client. They also provide theoretical analysis of CAFE."
420,SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for dynamic relational inference. The model is trained on a simulated physics system with periodic and additive dynamics, and is evaluated on real-world multi-agent basketball trajectories. The results show that the proposed model outperforms the baselines."
421,SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for dynamic relational inference. The model is trained on a simulated physics system with periodic and additive dynamics, and is evaluated on real-world multi-agent basketball trajectories. The results show that the proposed model outperforms the baselines."
422,SP:ce229295081ff04b26f33829f2c3396b90897b5d,"This paper proposes a generative model for dynamic relational inference. The model is trained on a simulated physics system with periodic and additive dynamics, and is evaluated on real-world multi-agent basketball trajectories. The results show that the proposed model outperforms the baselines."
423,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes a collaborative filtering framework for predicting potential user-item ratings. The proposed model is based on the Transductive Matrix Factorization (TMF) framework, where the latent factors of the rating matrix are modeled as a hidden relational graph, and the relation inference model is used to infer the user-specific latent factors. The model is evaluated on several matrix completion benchmarks and compared to other feature-driven inductive learning methods. "
424,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes a collaborative filtering framework for predicting potential user-item ratings. The proposed model is based on the Transductive Matrix Factorization (TMF) framework, where the latent factors of the rating matrix are modeled as a hidden relational graph, and the relation inference model is used to infer the user-specific latent factors. The model is evaluated on several matrix completion benchmarks and compared to other feature-driven inductive learning methods. "
425,SP:9f4b77d39f1deca28324fb637a0a77e89976baa8,"This paper proposes a collaborative filtering framework for predicting potential user-item ratings. The proposed model is based on the Transductive Matrix Factorization (TMF) framework, where the latent factors of the rating matrix are modeled as a hidden relational graph, and the relation inference model is used to infer the user-specific latent factors. The model is evaluated on several matrix completion benchmarks and compared to other feature-driven inductive learning methods. "
426,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a method for learning disentangled representations for image data. The proposed method is based on a variational autoencoder-based representation learning method. The main idea is to use a deep generative model to model the missing correlated latent variables in the image data, and then use a multi-stage model to disentangle the latent factors. The authors show that the proposed method outperforms the state-of-the-art methods in terms of disentanglement and reconstruction quality."
427,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a method for learning disentangled representations for image data. The proposed method is based on a variational autoencoder-based representation learning method. The main idea is to use a deep generative model to model the missing correlated latent variables in the image data, and then use a multi-stage model to disentangle the latent factors. The authors show that the proposed method outperforms the state-of-the-art methods in terms of disentanglement and reconstruction quality."
428,SP:9f9e9b0e37e59267d8516ab914bd619c53fbc9ec,"This paper proposes a method for learning disentangled representations for image data. The proposed method is based on a variational autoencoder-based representation learning method. The main idea is to use a deep generative model to model the missing correlated latent variables in the image data, and then use a multi-stage model to disentangle the latent factors. The authors show that the proposed method outperforms the state-of-the-art methods in terms of disentanglement and reconstruction quality."
429,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,This paper studies the problem of learning representations of data in reinforcement learning (RL). The authors propose to use mutual information maximization (MI) to learn representations of high-dimensional observations. The authors show that the representation learned by the proposed method can be better than the representations learned by existing methods. 
430,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,This paper studies the problem of learning representations of data in reinforcement learning (RL). The authors propose to use mutual information maximization (MI) to learn representations of high-dimensional observations. The authors show that the representation learned by the proposed method can be better than the representations learned by existing methods. 
431,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,This paper studies the problem of learning representations of data in reinforcement learning (RL). The authors propose to use mutual information maximization (MI) to learn representations of high-dimensional observations. The authors show that the representation learned by the proposed method can be better than the representations learned by existing methods. 
432,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the problem of learning a neural network from a finite-dimensional convex copositive program. The authors show that the global optimum of the neural network training problem is a convex semi-infinite dual of the copoitive relaxation problem, which is a special case of the non-convex convex neural network problem. They also show that a soft-thresholded SVD algorithm can be used to find the global minimum of the vector output neural network with a fixed filter size. Finally, the authors provide a theoretical analysis of the computational complexity of the proposed algorithm."
433,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the problem of learning a neural network from a finite-dimensional convex copositive program. The authors show that the global optimum of the neural network training problem is a convex semi-infinite dual of the copoitive relaxation problem, which is a special case of the non-convex convex neural network problem. They also show that a soft-thresholded SVD algorithm can be used to find the global minimum of the vector output neural network with a fixed filter size. Finally, the authors provide a theoretical analysis of the computational complexity of the proposed algorithm."
434,SP:881185782a9ec32fcbab14b42b78bf94edeba4b0,"This paper studies the problem of learning a neural network from a finite-dimensional convex copositive program. The authors show that the global optimum of the neural network training problem is a convex semi-infinite dual of the copoitive relaxation problem, which is a special case of the non-convex convex neural network problem. They also show that a soft-thresholded SVD algorithm can be used to find the global minimum of the vector output neural network with a fixed filter size. Finally, the authors provide a theoretical analysis of the computational complexity of the proposed algorithm."
435,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a language-based approach for learning disentangled, object-centric scene representations. The key idea is to learn a set of concepts for each object in the scene, and then use these concepts to train a language encoder that outputs a representation of the scene. The proposed approach is evaluated on a variety of tasks, including object segmentation, referring expression comprehension, and language modeling."
436,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a language-based approach for learning disentangled, object-centric scene representations. The key idea is to learn a set of concepts for each object in the scene, and then use these concepts to train a language encoder that outputs a representation of the scene. The proposed approach is evaluated on a variety of tasks, including object segmentation, referring expression comprehension, and language modeling."
437,SP:8613b2fcfd076d3e28a9940bad0c490a6557c10c,"This paper proposes a language-based approach for learning disentangled, object-centric scene representations. The key idea is to learn a set of concepts for each object in the scene, and then use these concepts to train a language encoder that outputs a representation of the scene. The proposed approach is evaluated on a variety of tasks, including object segmentation, referring expression comprehension, and language modeling."
438,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a new embedding method for knowledge graph completion. The proposed method, EM-RBR, is based on the concept of relational background knowledge. The authors propose to use a set of logic rules to guide the embedding of the fact triplets in the knowledge graph. The method is evaluated on FB15k-R and WN18. The results show that the proposed method outperforms the state-of-the-art models."
439,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a new embedding method for knowledge graph completion. The proposed method, EM-RBR, is based on the concept of relational background knowledge. The authors propose to use a set of logic rules to guide the embedding of the fact triplets in the knowledge graph. The method is evaluated on FB15k-R and WN18. The results show that the proposed method outperforms the state-of-the-art models."
440,SP:5e73b99c9942dd85bf70a65ad3e3c6a45d69b66b,"This paper proposes a new embedding method for knowledge graph completion. The proposed method, EM-RBR, is based on the concept of relational background knowledge. The authors propose to use a set of logic rules to guide the embedding of the fact triplets in the knowledge graph. The method is evaluated on FB15k-R and WN18. The results show that the proposed method outperforms the state-of-the-art models."
441,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a method for learning declarative and procedural knowledge in a video game. The method is based on a combination of LSTM, GRU, and a drop-in replacement method. The main contribution of the paper is the introduction of a new architecture that combines LSTMs, GRUs, and Drop-In Replacements. The proposed method is evaluated on a simple physics benchmark and compared to several baselines."
442,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a method for learning declarative and procedural knowledge in a video game. The method is based on a combination of LSTM, GRU, and a drop-in replacement method. The main contribution of the paper is the introduction of a new architecture that combines LSTMs, GRUs, and Drop-In Replacements. The proposed method is evaluated on a simple physics benchmark and compared to several baselines."
443,SP:19b74093512c4e5f8c504e96c590ac1eb7e2ce9b,"This paper proposes a method for learning declarative and procedural knowledge in a video game. The method is based on a combination of LSTM, GRU, and a drop-in replacement method. The main contribution of the paper is the introduction of a new architecture that combines LSTMs, GRUs, and Drop-In Replacements. The proposed method is evaluated on a simple physics benchmark and compared to several baselines."
444,SP:42a3c0453ab136537b5944a577d63412f3c22560,"This paper proposes a video-grounded neural module network (VilNMN) for visual question answering (VQA) and video-based dialogues. VilNMN consists of two modules: one that takes the image as input, and the other that takes a video as input. The first module takes the input as input and outputs the answer, while the second module takes an action and outputs a response. The authors show that the first module is better than the second one on VQA and video dialogues, and that the latter module is worse than the first one on video QA."
445,SP:42a3c0453ab136537b5944a577d63412f3c22560,"This paper proposes a video-grounded neural module network (VilNMN) for visual question answering (VQA) and video-based dialogues. VilNMN consists of two modules: one that takes the image as input, and the other that takes a video as input. The first module takes the input as input and outputs the answer, while the second module takes an action and outputs a response. The authors show that the first module is better than the second one on VQA and video dialogues, and that the latter module is worse than the first one on video QA."
446,SP:42a3c0453ab136537b5944a577d63412f3c22560,"This paper proposes a video-grounded neural module network (VilNMN) for visual question answering (VQA) and video-based dialogues. VilNMN consists of two modules: one that takes the image as input, and the other that takes a video as input. The first module takes the input as input and outputs the answer, while the second module takes an action and outputs a response. The authors show that the first module is better than the second one on VQA and video dialogues, and that the latter module is worse than the first one on video QA."
447,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper studies the problem of policy-space response oracles (PSRO) for multi-agent reinforcement learning. In particular, the authors propose to use a mixture of opponent policies to learn the best response for each agent. The authors also propose two algorithms to train PSRO."
448,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper studies the problem of policy-space response oracles (PSRO) for multi-agent reinforcement learning. In particular, the authors propose to use a mixture of opponent policies to learn the best response for each agent. The authors also propose two algorithms to train PSRO."
449,SP:126ce41b7f44975e5962f8bcb43f61bf2ed315c4,"This paper studies the problem of policy-space response oracles (PSRO) for multi-agent reinforcement learning. In particular, the authors propose to use a mixture of opponent policies to learn the best response for each agent. The authors also propose two algorithms to train PSRO."
450,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,This paper proposes a novel method for evaluating the robustness of Tacotron 2 text-to-speech models. The proposed method is based on a variational auto-encoder and a Gaussian upsampling method. The authors show that the proposed method outperforms the existing methods in terms of both unsupervised and semi-supervised evaluation. They also show that their method is more natural than the existing approaches.
451,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,This paper proposes a novel method for evaluating the robustness of Tacotron 2 text-to-speech models. The proposed method is based on a variational auto-encoder and a Gaussian upsampling method. The authors show that the proposed method outperforms the existing methods in terms of both unsupervised and semi-supervised evaluation. They also show that their method is more natural than the existing approaches.
452,SP:33e0b6099b32a6a2c0f2c7a8caa57ba2935d8b00,This paper proposes a novel method for evaluating the robustness of Tacotron 2 text-to-speech models. The proposed method is based on a variational auto-encoder and a Gaussian upsampling method. The authors show that the proposed method outperforms the existing methods in terms of both unsupervised and semi-supervised evaluation. They also show that their method is more natural than the existing approaches.
453,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method for bird’s eye view layout estimation. The method is based on inverse perspective mapping (IPM) and is trained with a supervised end-to-end framework. The main contribution of the paper is the proposed method, which is able to estimate the disparity between the bird's eye view and the ground truth view. The proposed method is evaluated on synthetic and real-world datasets."
454,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method for bird’s eye view layout estimation. The method is based on inverse perspective mapping (IPM) and is trained with a supervised end-to-end framework. The main contribution of the paper is the proposed method, which is able to estimate the disparity between the bird's eye view and the ground truth view. The proposed method is evaluated on synthetic and real-world datasets."
455,SP:ab9532306d294f85db84b9419ce826f046a7d95e,"This paper proposes a method for bird’s eye view layout estimation. The method is based on inverse perspective mapping (IPM) and is trained with a supervised end-to-end framework. The main contribution of the paper is the proposed method, which is able to estimate the disparity between the bird's eye view and the ground truth view. The proposed method is evaluated on synthetic and real-world datasets."
456,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,"This paper proposes a relation-aware GNN architecture for long-range modeling between nodes in a multi-relational graph. The authors propose a vector-based approach to model the relation between two nodes in the graph, which is based on the Graph Attention Network (GAN). The authors also propose a gated skip connection to improve the performance of the GNN. The experimental results show that the proposed method outperforms existing GNNs on both synthetic and real data."
457,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,"This paper proposes a relation-aware GNN architecture for long-range modeling between nodes in a multi-relational graph. The authors propose a vector-based approach to model the relation between two nodes in the graph, which is based on the Graph Attention Network (GAN). The authors also propose a gated skip connection to improve the performance of the GNN. The experimental results show that the proposed method outperforms existing GNNs on both synthetic and real data."
458,SP:3a151e18a5e623e9bf6e39a6065bfba1d5156fc1,"This paper proposes a relation-aware GNN architecture for long-range modeling between nodes in a multi-relational graph. The authors propose a vector-based approach to model the relation between two nodes in the graph, which is based on the Graph Attention Network (GAN). The authors also propose a gated skip connection to improve the performance of the GNN. The experimental results show that the proposed method outperforms existing GNNs on both synthetic and real data."
459,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a novel method for solving the minimal input mask discovery problem. The main idea is to use the Satisfiability Modulo Theory (SMT) solver to solve the SMT constraint of the input mask. The authors propose to use Integrated Gradient Information (IGI) to extract the gradient information from the saliency map of the mask and use it as a regularizer to improve the performance of the proposed method. The proposed method is evaluated on MNIST, ImageNet, and Beer Reviews. "
460,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a novel method for solving the minimal input mask discovery problem. The main idea is to use the Satisfiability Modulo Theory (SMT) solver to solve the SMT constraint of the input mask. The authors propose to use Integrated Gradient Information (IGI) to extract the gradient information from the saliency map of the mask and use it as a regularizer to improve the performance of the proposed method. The proposed method is evaluated on MNIST, ImageNet, and Beer Reviews. "
461,SP:f9906d99f6ae5e32dda548bdccce9ae92d25b205,"This paper proposes a novel method for solving the minimal input mask discovery problem. The main idea is to use the Satisfiability Modulo Theory (SMT) solver to solve the SMT constraint of the input mask. The authors propose to use Integrated Gradient Information (IGI) to extract the gradient information from the saliency map of the mask and use it as a regularizer to improve the performance of the proposed method. The proposed method is evaluated on MNIST, ImageNet, and Beer Reviews. "
462,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a new method for 3D pose estimation based on a generative model of neural feature activations. The proposed method, NeMo, consists of a feature extractor and a contrastive learning module to extract the feature representations of the target image and the target feature from the feature representation of the source image. The reconstruction error of NeMo is based on the reconstruction error for the target images and the reconstruction loss of the feature extraction module. The paper also proposes a differentiable rendering method to estimate the 3D object pose. The method is evaluated on three datasets: PASCAL3D, occluded-PASCAL, and occludged-PSC3D."
463,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a new method for 3D pose estimation based on a generative model of neural feature activations. The proposed method, NeMo, consists of a feature extractor and a contrastive learning module to extract the feature representations of the target image and the target feature from the feature representation of the source image. The reconstruction error of NeMo is based on the reconstruction error for the target images and the reconstruction loss of the feature extraction module. The paper also proposes a differentiable rendering method to estimate the 3D object pose. The method is evaluated on three datasets: PASCAL3D, occluded-PASCAL, and occludged-PSC3D."
464,SP:fc96fe4d0eeb0723bb7e4c9120c77981fc14731c,"This paper proposes a new method for 3D pose estimation based on a generative model of neural feature activations. The proposed method, NeMo, consists of a feature extractor and a contrastive learning module to extract the feature representations of the target image and the target feature from the feature representation of the source image. The reconstruction error of NeMo is based on the reconstruction error for the target images and the reconstruction loss of the feature extraction module. The paper also proposes a differentiable rendering method to estimate the 3D object pose. The method is evaluated on three datasets: PASCAL3D, occluded-PASCAL, and occludged-PSC3D."
465,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes a novel method for feature compatible learning (FCL) based on the random walk algorithm. The proposed method is based on an embedding model and a pseudo-classifier. The model is trained using a random walk method, and the pseudo classifier is trained with the original model. The authors show that the proposed method outperforms the state-of-the-art FCL methods on ImageNet ILSVRC 2012 and Places365."
466,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes a novel method for feature compatible learning (FCL) based on the random walk algorithm. The proposed method is based on an embedding model and a pseudo-classifier. The model is trained using a random walk method, and the pseudo classifier is trained with the original model. The authors show that the proposed method outperforms the state-of-the-art FCL methods on ImageNet ILSVRC 2012 and Places365."
467,SP:bde5b5b05d4a10634bd21a90cf0d8d22e2cda22d,"This paper proposes a novel method for feature compatible learning (FCL) based on the random walk algorithm. The proposed method is based on an embedding model and a pseudo-classifier. The model is trained using a random walk method, and the pseudo classifier is trained with the original model. The authors show that the proposed method outperforms the state-of-the-art FCL methods on ImageNet ILSVRC 2012 and Places365."
468,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the relationship between the generalization error and the gradient norm in hyperparameter optimization. The authors first show that the difference between the two measures is a function of the number of parameters in the network. Then, the authors propose a new objective function for hyper-parameter search, which is based on the bandit-based or population-based algorithm BOHB. Finally, they provide empirical evidence that the proposed objective function is more effective than the original objective function."
469,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the relationship between the generalization error and the gradient norm in hyperparameter optimization. The authors first show that the difference between the two measures is a function of the number of parameters in the network. Then, the authors propose a new objective function for hyper-parameter search, which is based on the bandit-based or population-based algorithm BOHB. Finally, they provide empirical evidence that the proposed objective function is more effective than the original objective function."
470,SP:a9aa11e7ee77d9f6957266e4ad822c7dc0f82354,"This paper studies the relationship between the generalization error and the gradient norm in hyperparameter optimization. The authors first show that the difference between the two measures is a function of the number of parameters in the network. Then, the authors propose a new objective function for hyper-parameter search, which is based on the bandit-based or population-based algorithm BOHB. Finally, they provide empirical evidence that the proposed objective function is more effective than the original objective function."
471,SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes a method for entity disambiguation in Wikipedia. The method is based on autoregressive encoder-decoder architecture, where the encoder encodes the entity embeddings and the decoder decodes them into a vector dot product. The authors propose to use a softmax loss to disentangle the entities from the context and the context context from the entity. The proposed method is evaluated on three datasets and compared to several baselines."
472,SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes a method for entity disambiguation in Wikipedia. The method is based on autoregressive encoder-decoder architecture, where the encoder encodes the entity embeddings and the decoder decodes them into a vector dot product. The authors propose to use a softmax loss to disentangle the entities from the context and the context context from the entity. The proposed method is evaluated on three datasets and compared to several baselines."
473,SP:13359456defb953dd2d19e1f879100ce392d6be6,"This paper proposes a method for entity disambiguation in Wikipedia. The method is based on autoregressive encoder-decoder architecture, where the encoder encodes the entity embeddings and the decoder decodes them into a vector dot product. The authors propose to use a softmax loss to disentangle the entities from the context and the context context from the entity. The proposed method is evaluated on three datasets and compared to several baselines."
474,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper studies the problem of routing users on road networks with unknown congestion functions. The authors consider the case where the congestion function is unknown and the users have infinite time horizons. The main contribution of this paper is to propose a routing algorithm for this problem. The proposed algorithm is based on an existing algorithm for routing users in the presence of known congestion functions, and the authors show that the proposed algorithm can achieve a regret of $O(1/\sqrt{T})$, which is a lower bound on the total cost and the minimum cost of the algorithm. "
475,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper studies the problem of routing users on road networks with unknown congestion functions. The authors consider the case where the congestion function is unknown and the users have infinite time horizons. The main contribution of this paper is to propose a routing algorithm for this problem. The proposed algorithm is based on an existing algorithm for routing users in the presence of known congestion functions, and the authors show that the proposed algorithm can achieve a regret of $O(1/\sqrt{T})$, which is a lower bound on the total cost and the minimum cost of the algorithm. "
476,SP:9dfb808ce4c045c45436b35ceb03bc6fe6ed9745,"This paper studies the problem of routing users on road networks with unknown congestion functions. The authors consider the case where the congestion function is unknown and the users have infinite time horizons. The main contribution of this paper is to propose a routing algorithm for this problem. The proposed algorithm is based on an existing algorithm for routing users in the presence of known congestion functions, and the authors show that the proposed algorithm can achieve a regret of $O(1/\sqrt{T})$, which is a lower bound on the total cost and the minimum cost of the algorithm. "
477,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,This paper proposes a new masking strategy for BERT based on Pointwise Mutual Information (PMI) for masking. PMI-Masking is based on the observation that BERT masks the tokens of a token n-grams in the input to a BERT model. The authors argue that masking the tokens with PMI can help reduce the training time of BERT. The proposed method is evaluated on a variety of datasets and compared to other masking methods.
478,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,This paper proposes a new masking strategy for BERT based on Pointwise Mutual Information (PMI) for masking. PMI-Masking is based on the observation that BERT masks the tokens of a token n-grams in the input to a BERT model. The authors argue that masking the tokens with PMI can help reduce the training time of BERT. The proposed method is evaluated on a variety of datasets and compared to other masking methods.
479,SP:580ac3b74951bef5d5772e4471b01a805ff3dd68,This paper proposes a new masking strategy for BERT based on Pointwise Mutual Information (PMI) for masking. PMI-Masking is based on the observation that BERT masks the tokens of a token n-grams in the input to a BERT model. The authors argue that masking the tokens with PMI can help reduce the training time of BERT. The proposed method is evaluated on a variety of datasets and compared to other masking methods.
480,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,This paper studies the problem of amortised inference in sequential latent variable models (LVMs). The authors propose a new objective for amortized inference based on the evidence lower bound (ELBO) objective. The ELBO objective is based on a mixture of smoothing posteriors and a Bayesian filter. The authors show that the proposed objective can be used to approximate the approximate posteriors of a variational variational model. The paper also shows that the ELBO can be applied to partially-conditioned posteriors.
481,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,This paper studies the problem of amortised inference in sequential latent variable models (LVMs). The authors propose a new objective for amortized inference based on the evidence lower bound (ELBO) objective. The ELBO objective is based on a mixture of smoothing posteriors and a Bayesian filter. The authors show that the proposed objective can be used to approximate the approximate posteriors of a variational variational model. The paper also shows that the ELBO can be applied to partially-conditioned posteriors.
482,SP:038cdd2df643edccb16dfd72e6eb123f6a6c0839,This paper studies the problem of amortised inference in sequential latent variable models (LVMs). The authors propose a new objective for amortized inference based on the evidence lower bound (ELBO) objective. The ELBO objective is based on a mixture of smoothing posteriors and a Bayesian filter. The authors show that the proposed objective can be used to approximate the approximate posteriors of a variational variational model. The paper also shows that the ELBO can be applied to partially-conditioned posteriors.
483,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies distributed kernel ridge regression (KRR) with random features (DKRR-RF) and divide-and-conquer (D&C) techniques. In particular, the authors propose a new generalization bound for KRR with D&C and random features, which is based on a theoretical analysis of the optimal generalization bounds. The authors also propose a communication strategy to improve the performance of KRR."
484,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies distributed kernel ridge regression (KRR) with random features (DKRR-RF) and divide-and-conquer (D&C) techniques. In particular, the authors propose a new generalization bound for KRR with D&C and random features, which is based on a theoretical analysis of the optimal generalization bounds. The authors also propose a communication strategy to improve the performance of KRR."
485,SP:f2574c0d6cdec78389fa1301d6a10976d1756279,"This paper studies distributed kernel ridge regression (KRR) with random features (DKRR-RF) and divide-and-conquer (D&C) techniques. In particular, the authors propose a new generalization bound for KRR with D&C and random features, which is based on a theoretical analysis of the optimal generalization bounds. The authors also propose a communication strategy to improve the performance of KRR."
486,SP:129872706a12d89f0886c2ad0fd4083d0632343c,This paper proposes a proxy search space (PS) for the RandomNAS-based approach. The proposed method is based on the idea that the top-performing architectures in the global search space can be found in the PS. The method is evaluated on the NASBench-201 benchmark and compared to the state-of-the-art.
487,SP:129872706a12d89f0886c2ad0fd4083d0632343c,This paper proposes a proxy search space (PS) for the RandomNAS-based approach. The proposed method is based on the idea that the top-performing architectures in the global search space can be found in the PS. The method is evaluated on the NASBench-201 benchmark and compared to the state-of-the-art.
488,SP:129872706a12d89f0886c2ad0fd4083d0632343c,This paper proposes a proxy search space (PS) for the RandomNAS-based approach. The proposed method is based on the idea that the top-performing architectures in the global search space can be found in the PS. The method is evaluated on the NASBench-201 benchmark and compared to the state-of-the-art.
489,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,This paper proposes a method for meta-reinforcement learning (meta-RL) based on Probabilistic Embeddings (PERIL). PERIL is a meta-RL method that combines imitation learning and meta reinforcement learning. The main idea is to use a probabilistic embedding of the reward function to guide the exploration policy. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks.
490,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,This paper proposes a method for meta-reinforcement learning (meta-RL) based on Probabilistic Embeddings (PERIL). PERIL is a meta-RL method that combines imitation learning and meta reinforcement learning. The main idea is to use a probabilistic embedding of the reward function to guide the exploration policy. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks.
491,SP:27701f374d0b7e8b269d9133d6c3a10bca03b548,This paper proposes a method for meta-reinforcement learning (meta-RL) based on Probabilistic Embeddings (PERIL). PERIL is a meta-RL method that combines imitation learning and meta reinforcement learning. The main idea is to use a probabilistic embedding of the reward function to guide the exploration policy. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of tasks.
492,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the problem of learning over-parameterized convolutional neural networks (CNNs) for image classification. The authors propose a learning algorithm based on pattern statistics inductive bias (PSI), which is based on the dot product between the pattern statistics and the dot-product of the filter dimension. In particular, the authors show that under certain conditions, the PSI can be reduced to a lower bound on the VC dimension of the network. The paper also provides a sample complexity analysis of the proposed algorithm."
493,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the problem of learning over-parameterized convolutional neural networks (CNNs) for image classification. The authors propose a learning algorithm based on pattern statistics inductive bias (PSI), which is based on the dot product between the pattern statistics and the dot-product of the filter dimension. In particular, the authors show that under certain conditions, the PSI can be reduced to a lower bound on the VC dimension of the network. The paper also provides a sample complexity analysis of the proposed algorithm."
494,SP:118758f563fa6e9e46d52a6f250005c06cf2f19f,"This paper studies the problem of learning over-parameterized convolutional neural networks (CNNs) for image classification. The authors propose a learning algorithm based on pattern statistics inductive bias (PSI), which is based on the dot product between the pattern statistics and the dot-product of the filter dimension. In particular, the authors show that under certain conditions, the PSI can be reduced to a lower bound on the VC dimension of the network. The paper also provides a sample complexity analysis of the proposed algorithm."
495,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper proposes a contrastive learning method for document classification. The main idea is to learn a representation of documents that is similar to the representation learned by a linear classifier. The authors show that the proposed method can be applied to a semi-supervised setting. The proposed method is evaluated on a variety of document classification tasks.
496,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper proposes a contrastive learning method for document classification. The main idea is to learn a representation of documents that is similar to the representation learned by a linear classifier. The authors show that the proposed method can be applied to a semi-supervised setting. The proposed method is evaluated on a variety of document classification tasks.
497,SP:a051b615da3a99562d2cd2dfbec5cd78af98d9b4,This paper proposes a contrastive learning method for document classification. The main idea is to learn a representation of documents that is similar to the representation learned by a linear classifier. The authors show that the proposed method can be applied to a semi-supervised setting. The proposed method is evaluated on a variety of document classification tasks.
498,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive framework for multi-modal generative model learning. The proposed method is based on contrastive learning, which is an extension of the contrastive VAE framework. The main idea is to use contrastive loss to encourage the model to learn representations that are similar to each other. The authors show that the proposed method outperforms the state-of-the-art VAE models on a variety of datasets."
499,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive framework for multi-modal generative model learning. The proposed method is based on contrastive learning, which is an extension of the contrastive VAE framework. The main idea is to use contrastive loss to encourage the model to learn representations that are similar to each other. The authors show that the proposed method outperforms the state-of-the-art VAE models on a variety of datasets."
500,SP:8d011d4a77ced1f8cd849181d5293420f161ffd3,"This paper proposes a contrastive framework for multi-modal generative model learning. The proposed method is based on contrastive learning, which is an extension of the contrastive VAE framework. The main idea is to use contrastive loss to encourage the model to learn representations that are similar to each other. The authors show that the proposed method outperforms the state-of-the-art VAE models on a variety of datasets."
501,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper studies the prior of variational autoencoders (VAEs) in the context of prior hole problem. The authors propose a method to address this problem by using a reweighting factor based on the energy-based prior and a noise contrastive estimator. The proposed method is evaluated on CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. The results show that the proposed method outperforms the baselines."
502,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper studies the prior of variational autoencoders (VAEs) in the context of prior hole problem. The authors propose a method to address this problem by using a reweighting factor based on the energy-based prior and a noise contrastive estimator. The proposed method is evaluated on CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. The results show that the proposed method outperforms the baselines."
503,SP:3a0d3f1d63cd57b0613c40176e694435ed3eee50,"This paper studies the prior of variational autoencoders (VAEs) in the context of prior hole problem. The authors propose a method to address this problem by using a reweighting factor based on the energy-based prior and a noise contrastive estimator. The proposed method is evaluated on CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. The results show that the proposed method outperforms the baselines."
504,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper studies inverse reinforcement learning (IRL) with convex regularizers. The authors propose a new regularizer for inverse RL, which is based on the Shannon-entropy regularizer. They show that the proposed regularizer can be applied to both discrete and continuous control tasks. They also provide theoretical backing for the proposed method."
505,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper studies inverse reinforcement learning (IRL) with convex regularizers. The authors propose a new regularizer for inverse RL, which is based on the Shannon-entropy regularizer. They show that the proposed regularizer can be applied to both discrete and continuous control tasks. They also provide theoretical backing for the proposed method."
506,SP:86b813ac0f5211a7c45884451f59f3ebaeeb4b83,"This paper studies inverse reinforcement learning (IRL) with convex regularizers. The authors propose a new regularizer for inverse RL, which is based on the Shannon-entropy regularizer. They show that the proposed regularizer can be applied to both discrete and continuous control tasks. They also provide theoretical backing for the proposed method."
507,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR is based on hard binary gates, which can be used for LS or shared paths. CLSR can be applied to both top and/or bottom encoder/decoder layers of a multilingual Transformer. Experiments on OPUS-100 and WMT datasets show that CLSR outperforms one-to-one translation and unbalanced training data."
508,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR is based on hard binary gates, which can be used for LS or shared paths. CLSR can be applied to both top and/or bottom encoder/decoder layers of a multilingual Transformer. Experiments on OPUS-100 and WMT datasets show that CLSR outperforms one-to-one translation and unbalanced training data."
509,SP:6f4a520cdc9901c2c87a7e887ce2535ad0b36f69,"This paper proposes conditional language-specific routing (CLSR) for multilingual neural machine translation (MNMT). CLSR is based on hard binary gates, which can be used for LS or shared paths. CLSR can be applied to both top and/or bottom encoder/decoder layers of a multilingual Transformer. Experiments on OPUS-100 and WMT datasets show that CLSR outperforms one-to-one translation and unbalanced training data."
510,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,This paper proposes a new method to deal with noisy labels. The proposed method is based on the Wasserstein distributional normalization (WDN) algorithm. The main contribution of this paper is to propose a new loss function for the WDN algorithm. This loss function is motivated by the geometric relationship between the label distribution and the geometric constraints. The authors show that the proposed loss function can be applied to a wide range of noisy labels and achieve better performance than existing methods.
511,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,This paper proposes a new method to deal with noisy labels. The proposed method is based on the Wasserstein distributional normalization (WDN) algorithm. The main contribution of this paper is to propose a new loss function for the WDN algorithm. This loss function is motivated by the geometric relationship between the label distribution and the geometric constraints. The authors show that the proposed loss function can be applied to a wide range of noisy labels and achieve better performance than existing methods.
512,SP:1e932b21e9557b1bbc1950c4e1701f5a3ecf50df,This paper proposes a new method to deal with noisy labels. The proposed method is based on the Wasserstein distributional normalization (WDN) algorithm. The main contribution of this paper is to propose a new loss function for the WDN algorithm. This loss function is motivated by the geometric relationship between the label distribution and the geometric constraints. The authors show that the proposed loss function can be applied to a wide range of noisy labels and achieve better performance than existing methods.
513,SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a new method for estimating the predictive set of a classifier using Platt scaling. The authors propose a conformal prediction algorithm to estimate the probability of the classifier for a given set of samples. The proposed method is based on the notion of finite-sample coverage, and the authors provide a formal finite sample coverage guarantee for the proposed method. The paper also provides a theoretical analysis of the convergence rate of the proposed algorithm. Experiments on Imagenet-V2 and ResNet-152 show that the proposed approach outperforms existing methods."
514,SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a new method for estimating the predictive set of a classifier using Platt scaling. The authors propose a conformal prediction algorithm to estimate the probability of the classifier for a given set of samples. The proposed method is based on the notion of finite-sample coverage, and the authors provide a formal finite sample coverage guarantee for the proposed method. The paper also provides a theoretical analysis of the convergence rate of the proposed algorithm. Experiments on Imagenet-V2 and ResNet-152 show that the proposed approach outperforms existing methods."
515,SP:e0029422e28c250dfb8c62c29a15b375030069e8,"This paper proposes a new method for estimating the predictive set of a classifier using Platt scaling. The authors propose a conformal prediction algorithm to estimate the probability of the classifier for a given set of samples. The proposed method is based on the notion of finite-sample coverage, and the authors provide a formal finite sample coverage guarantee for the proposed method. The paper also provides a theoretical analysis of the convergence rate of the proposed algorithm. Experiments on Imagenet-V2 and ResNet-152 show that the proposed approach outperforms existing methods."
516,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,This paper studies the problem of optimal transport for Wasserstein-2 barycenters. The authors propose a new algorithm for optimal transport based on a cycle-consistency regularization and an entropic or quadratic regularization. The main contribution of this paper is to provide a lower bound on the error of the optimal transport algorithm. The paper is well-written and easy to follow.
517,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,This paper studies the problem of optimal transport for Wasserstein-2 barycenters. The authors propose a new algorithm for optimal transport based on a cycle-consistency regularization and an entropic or quadratic regularization. The main contribution of this paper is to provide a lower bound on the error of the optimal transport algorithm. The paper is well-written and easy to follow.
518,SP:bf93641cbeaaa147ad0307de694e20adc23c290a,This paper studies the problem of optimal transport for Wasserstein-2 barycenters. The authors propose a new algorithm for optimal transport based on a cycle-consistency regularization and an entropic or quadratic regularization. The main contribution of this paper is to provide a lower bound on the error of the optimal transport algorithm. The paper is well-written and easy to follow.
519,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the generalization of deep ReLU networks on multiple manifolds. The main contribution of the paper is a non-asymptotic analysis of training overparameterized neural networks, and the analysis is based on the martingale concentration of the network depth L. The paper also provides a theoretical analysis of the NTK regime."
520,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the generalization of deep ReLU networks on multiple manifolds. The main contribution of the paper is a non-asymptotic analysis of training overparameterized neural networks, and the analysis is based on the martingale concentration of the network depth L. The paper also provides a theoretical analysis of the NTK regime."
521,SP:39aae6a094f7141bee6d4fa78be03fd20cf12b13,"This paper studies the generalization of deep ReLU networks on multiple manifolds. The main contribution of the paper is a non-asymptotic analysis of training overparameterized neural networks, and the analysis is based on the martingale concentration of the network depth L. The paper also provides a theoretical analysis of the NTK regime."
522,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,This paper proposes a method for off-policy reinforcement learning based on advantage-weighted regression (AWR). The main idea of AWR is to learn a policy that maximizes the difference between the value function of the current policy and the value of the next policy. The authors propose to use experience replay as a regularizer to encourage the policy to maximize the advantage of the previous policy over the new policy. They show that this regularizer can be used to improve the performance of their algorithm on a variety of continuous control tasks.
523,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,This paper proposes a method for off-policy reinforcement learning based on advantage-weighted regression (AWR). The main idea of AWR is to learn a policy that maximizes the difference between the value function of the current policy and the value of the next policy. The authors propose to use experience replay as a regularizer to encourage the policy to maximize the advantage of the previous policy over the new policy. They show that this regularizer can be used to improve the performance of their algorithm on a variety of continuous control tasks.
524,SP:c5afd0a7485aa8dc732f6fa90d81a85a8bb51b3c,This paper proposes a method for off-policy reinforcement learning based on advantage-weighted regression (AWR). The main idea of AWR is to learn a policy that maximizes the difference between the value function of the current policy and the value of the next policy. The authors propose to use experience replay as a regularizer to encourage the policy to maximize the advantage of the previous policy over the new policy. They show that this regularizer can be used to improve the performance of their algorithm on a variety of continuous control tasks.
525,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes WaveQ, a method for training quantized networks with heterogeneous bitwidths. WaveQ is based on a parametrized sinusoidal regularizer, which is a gradient-based mechanism for quantized weights. The authors show that WaveQ can be applied to both quantized and non-quantized weights, and that it is able to achieve better accuracy and compute efficiency compared to other quantized training algorithms. In addition, the authors also show that the proposed method can be used to train heterogeneous networks."
526,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes WaveQ, a method for training quantized networks with heterogeneous bitwidths. WaveQ is based on a parametrized sinusoidal regularizer, which is a gradient-based mechanism for quantized weights. The authors show that WaveQ can be applied to both quantized and non-quantized weights, and that it is able to achieve better accuracy and compute efficiency compared to other quantized training algorithms. In addition, the authors also show that the proposed method can be used to train heterogeneous networks."
527,SP:54da307c1f9aac020ae7e3c439653765dbd8b3fe,"This paper proposes WaveQ, a method for training quantized networks with heterogeneous bitwidths. WaveQ is based on a parametrized sinusoidal regularizer, which is a gradient-based mechanism for quantized weights. The authors show that WaveQ can be applied to both quantized and non-quantized weights, and that it is able to achieve better accuracy and compute efficiency compared to other quantized training algorithms. In addition, the authors also show that the proposed method can be used to train heterogeneous networks."
528,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,This paper proposes a data augmentation method for neural machine translation (NMT) models. The proposed method is based on back-translation. The main idea is to augment the data with bilingual embeddings of the source and target word pairs. The method is evaluated on both small and large scale datasets. The experimental results show the effectiveness of the proposed method.
529,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,This paper proposes a data augmentation method for neural machine translation (NMT) models. The proposed method is based on back-translation. The main idea is to augment the data with bilingual embeddings of the source and target word pairs. The method is evaluated on both small and large scale datasets. The experimental results show the effectiveness of the proposed method.
530,SP:84ced6627d1dc3e78c9ffc726174e76db5f77795,This paper proposes a data augmentation method for neural machine translation (NMT) models. The proposed method is based on back-translation. The main idea is to augment the data with bilingual embeddings of the source and target word pairs. The method is evaluated on both small and large scale datasets. The experimental results show the effectiveness of the proposed method.
531,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"This paper proposes a new contribution measurement method for federated learning. The contribution measurement mechanism is based on Shapley Value, which is a well-known metric in game theory. The authors show that the proposed method can be used to measure the contribution rate in real-time. They also provide a theoretical analysis of the proposed mechanism."
532,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"This paper proposes a new contribution measurement method for federated learning. The contribution measurement mechanism is based on Shapley Value, which is a well-known metric in game theory. The authors show that the proposed method can be used to measure the contribution rate in real-time. They also provide a theoretical analysis of the proposed mechanism."
533,SP:c1890bcafac6ac8fd5a3d2ff2dd1c37b71865a5a,"This paper proposes a new contribution measurement method for federated learning. The contribution measurement mechanism is based on Shapley Value, which is a well-known metric in game theory. The authors show that the proposed method can be used to measure the contribution rate in real-time. They also provide a theoretical analysis of the proposed mechanism."
534,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,This paper studies the problem of learning Bayesian networks with robust mean estimation. The authors propose a new algorithm for this problem that is nearly linear in the number of parameters and has a dimension-independent error guarantee. The main contribution of this paper is to provide a nearly linear time algorithm for the Bayesian network learning problem. The paper also provides a theoretical analysis of the error guarantees of the proposed algorithm.
535,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,This paper studies the problem of learning Bayesian networks with robust mean estimation. The authors propose a new algorithm for this problem that is nearly linear in the number of parameters and has a dimension-independent error guarantee. The main contribution of this paper is to provide a nearly linear time algorithm for the Bayesian network learning problem. The paper also provides a theoretical analysis of the error guarantees of the proposed algorithm.
536,SP:b766979b4d3b15a039db4e5eebd8353521aea4bb,This paper studies the problem of learning Bayesian networks with robust mean estimation. The authors propose a new algorithm for this problem that is nearly linear in the number of parameters and has a dimension-independent error guarantee. The main contribution of this paper is to provide a nearly linear time algorithm for the Bayesian network learning problem. The paper also provides a theoretical analysis of the error guarantees of the proposed algorithm.
537,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-horizon visual planning in the context of image-based reinforcement learning. The method is based on a probabilistic latent variable model, where the latent variable is modeled as a mixture of latent variables and the goal is to maximize the mutual information between the latent variables. The proposed method is evaluated on a variety of visual control tasks and compared to a number of baselines."
538,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-horizon visual planning in the context of image-based reinforcement learning. The method is based on a probabilistic latent variable model, where the latent variable is modeled as a mixture of latent variables and the goal is to maximize the mutual information between the latent variables. The proposed method is evaluated on a variety of visual control tasks and compared to a number of baselines."
539,SP:b3d507bd8fe8876f3a4f7696bc0483d0052484c8,"This paper proposes a method for long-horizon visual planning in the context of image-based reinforcement learning. The method is based on a probabilistic latent variable model, where the latent variable is modeled as a mixture of latent variables and the goal is to maximize the mutual information between the latent variables. The proposed method is evaluated on a variety of visual control tasks and compared to a number of baselines."
540,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model to model the uncertainty in the posterior of Bayesian neural networks (BNNs). The authors argue that the uncertainty is caused by the fact that BNNs are trained with a “tempered” or “cold” posterior, where the likelihood of the posterior is lower than that of the warm posterior. The authors propose to use the generative models to model this uncertainty and show that this uncertainty can be controlled by the likelihood in the latent space of the BNN. "
541,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model to model the uncertainty in the posterior of Bayesian neural networks (BNNs). The authors argue that the uncertainty is caused by the fact that BNNs are trained with a “tempered” or “cold” posterior, where the likelihood of the posterior is lower than that of the warm posterior. The authors propose to use the generative models to model this uncertainty and show that this uncertainty can be controlled by the likelihood in the latent space of the BNN. "
542,SP:cfe57a61dc20207b64b7fff45f7cb33126dce558,"This paper proposes a generative model to model the uncertainty in the posterior of Bayesian neural networks (BNNs). The authors argue that the uncertainty is caused by the fact that BNNs are trained with a “tempered” or “cold” posterior, where the likelihood of the posterior is lower than that of the warm posterior. The authors propose to use the generative models to model this uncertainty and show that this uncertainty can be controlled by the likelihood in the latent space of the BNN. "
543,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the trade-off between autoregressive and non-autoregressive neural machine translation (NMT) models. The authors propose a single-layer auto-regressive decoder for NMT, and show that it can achieve comparable performance to the state-of-the-art models on a variety of tasks. They also show that the performance of autoregressive models can be improved by reducing the number of encoders in NMT models."
544,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the trade-off between autoregressive and non-autoregressive neural machine translation (NMT) models. The authors propose a single-layer auto-regressive decoder for NMT, and show that it can achieve comparable performance to the state-of-the-art models on a variety of tasks. They also show that the performance of autoregressive models can be improved by reducing the number of encoders in NMT models."
545,SP:4ebd3874ecea94ed9d0ca7b2fb13bf246b556938,"This paper studies the trade-off between autoregressive and non-autoregressive neural machine translation (NMT) models. The authors propose a single-layer auto-regressive decoder for NMT, and show that it can achieve comparable performance to the state-of-the-art models on a variety of tasks. They also show that the performance of autoregressive models can be improved by reducing the number of encoders in NMT models."
546,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper studies the bias-variance analysis of epoch-wise double descent in deep neural networks (DNNs). The authors propose a new metric called the optimization variance (OV) to measure the diversity of model updates in DNNs. The authors show that the OV is a bell-shaped curve and that it can be used to estimate the test error of a DNN with unknown test error. They also show that when the model complexity is small, the Ov can be computed using zero-one loss."
547,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper studies the bias-variance analysis of epoch-wise double descent in deep neural networks (DNNs). The authors propose a new metric called the optimization variance (OV) to measure the diversity of model updates in DNNs. The authors show that the OV is a bell-shaped curve and that it can be used to estimate the test error of a DNN with unknown test error. They also show that when the model complexity is small, the Ov can be computed using zero-one loss."
548,SP:ce8cf444681a8e38408c6485029fe42b89a1f172,"This paper studies the bias-variance analysis of epoch-wise double descent in deep neural networks (DNNs). The authors propose a new metric called the optimization variance (OV) to measure the diversity of model updates in DNNs. The authors show that the OV is a bell-shaped curve and that it can be used to estimate the test error of a DNN with unknown test error. They also show that when the model complexity is small, the Ov can be computed using zero-one loss."
549,SP:8d8b738c676938952e62a6b2aea42e79518ece06,This paper studies the problem of few-shot learning with adversarial robustness. The authors propose a meta-learning approach to improve the robustness of the meta-model. The main contribution of the paper is to propose a regularization term that encourages robustness in the training protocol. The paper also proposes an auxiliary contrastive learning task to further improve the performance of the model. Experiments are conducted on a few datasets to show the effectiveness of the proposed method.
550,SP:8d8b738c676938952e62a6b2aea42e79518ece06,This paper studies the problem of few-shot learning with adversarial robustness. The authors propose a meta-learning approach to improve the robustness of the meta-model. The main contribution of the paper is to propose a regularization term that encourages robustness in the training protocol. The paper also proposes an auxiliary contrastive learning task to further improve the performance of the model. Experiments are conducted on a few datasets to show the effectiveness of the proposed method.
551,SP:8d8b738c676938952e62a6b2aea42e79518ece06,This paper studies the problem of few-shot learning with adversarial robustness. The authors propose a meta-learning approach to improve the robustness of the meta-model. The main contribution of the paper is to propose a regularization term that encourages robustness in the training protocol. The paper also proposes an auxiliary contrastive learning task to further improve the performance of the model. Experiments are conducted on a few datasets to show the effectiveness of the proposed method.
552,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,This paper proposes a meta-gradient method for learning to learn optimizers for optimization problems with metagradient explosion/vanishing problems. The main idea is to use meta-gradients to train a neural network to estimate the meta-optimizer and then use backpropagation to reduce the step size of the quadratic loss. The authors show that the proposed method is able to achieve better performance than the state-of-the-art meta-learning algorithms.
553,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,This paper proposes a meta-gradient method for learning to learn optimizers for optimization problems with metagradient explosion/vanishing problems. The main idea is to use meta-gradients to train a neural network to estimate the meta-optimizer and then use backpropagation to reduce the step size of the quadratic loss. The authors show that the proposed method is able to achieve better performance than the state-of-the-art meta-learning algorithms.
554,SP:1fdce0afe8fd8c082f62f1a4b9823830d81860e8,This paper proposes a meta-gradient method for learning to learn optimizers for optimization problems with metagradient explosion/vanishing problems. The main idea is to use meta-gradients to train a neural network to estimate the meta-optimizer and then use backpropagation to reduce the step size of the quadratic loss. The authors show that the proposed method is able to achieve better performance than the state-of-the-art meta-learning algorithms.
555,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a new method for semisupervised learning of node classification. The proposed method is based on the dual view approach, which is a combination of two loss functions: view-consistent loss and pseudo-label loss. In particular, the view-concavity loss is a regularization term that aims to ensure that the representations of the two views are consistent. The pseudo label loss is an additional loss function that aims at ensuring that the pseudo labels are close to the ground truth labels, while the view consistent loss aims to encourage the views to be close to each other. Experiments show that the proposed method outperforms the baselines on several node classification tasks."
556,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a new method for semisupervised learning of node classification. The proposed method is based on the dual view approach, which is a combination of two loss functions: view-consistent loss and pseudo-label loss. In particular, the view-concavity loss is a regularization term that aims to ensure that the representations of the two views are consistent. The pseudo label loss is an additional loss function that aims at ensuring that the pseudo labels are close to the ground truth labels, while the view consistent loss aims to encourage the views to be close to each other. Experiments show that the proposed method outperforms the baselines on several node classification tasks."
557,SP:c8a9ab50888585b58369c4fb425be1170c96c14d,"This paper proposes a new method for semisupervised learning of node classification. The proposed method is based on the dual view approach, which is a combination of two loss functions: view-consistent loss and pseudo-label loss. In particular, the view-concavity loss is a regularization term that aims to ensure that the representations of the two views are consistent. The pseudo label loss is an additional loss function that aims at ensuring that the pseudo labels are close to the ground truth labels, while the view consistent loss aims to encourage the views to be close to each other. Experiments show that the proposed method outperforms the baselines on several node classification tasks."
558,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper proposes a method to learn the dynamics of a system using a neural network. The method is based on the Hamiltonian dynamics of the system, which can be expressed as a linear combination of a Hamiltonian and a loss function. The authors show that the network can be used to learn conserved quantities such as (angular) momentum and (cyclic) momentum, which are conserved in the classical physics literature. The proposed method is evaluated on both synthetic and real-world data."
559,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper proposes a method to learn the dynamics of a system using a neural network. The method is based on the Hamiltonian dynamics of the system, which can be expressed as a linear combination of a Hamiltonian and a loss function. The authors show that the network can be used to learn conserved quantities such as (angular) momentum and (cyclic) momentum, which are conserved in the classical physics literature. The proposed method is evaluated on both synthetic and real-world data."
560,SP:9c8619d2c0df81c1222ba28cecbacc42408d0019,"This paper proposes a method to learn the dynamics of a system using a neural network. The method is based on the Hamiltonian dynamics of the system, which can be expressed as a linear combination of a Hamiltonian and a loss function. The authors show that the network can be used to learn conserved quantities such as (angular) momentum and (cyclic) momentum, which are conserved in the classical physics literature. The proposed method is evaluated on both synthetic and real-world data."
561,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper studies the problem of graph representation learning in the heterogeneous setting. The authors propose a novel architecture for graph neural networks (GNNs) that combines gradient boosted decision trees (GBDT) and GNNs. The main contribution of this paper is to propose a new architecture for GNN based on GBDT. The proposed architecture is based on a combination of GNN and GBDT, and the authors show that the proposed architecture outperforms existing GNN-based models on heterogeneous graphs. "
562,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper studies the problem of graph representation learning in the heterogeneous setting. The authors propose a novel architecture for graph neural networks (GNNs) that combines gradient boosted decision trees (GBDT) and GNNs. The main contribution of this paper is to propose a new architecture for GNN based on GBDT. The proposed architecture is based on a combination of GNN and GBDT, and the authors show that the proposed architecture outperforms existing GNN-based models on heterogeneous graphs. "
563,SP:d1e78b1759eef8fc16e5b7ad7f0e290e9dc5dea0,"This paper studies the problem of graph representation learning in the heterogeneous setting. The authors propose a novel architecture for graph neural networks (GNNs) that combines gradient boosted decision trees (GBDT) and GNNs. The main contribution of this paper is to propose a new architecture for GNN based on GBDT. The proposed architecture is based on a combination of GNN and GBDT, and the authors show that the proposed architecture outperforms existing GNN-based models on heterogeneous graphs. "
564,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,This paper studies the problem of linear centroid meta-learning with data splitting. The authors propose two data splitting methods: (1) train-validation split and (2) non-splitting method. They show that the splitting method is equivalent to the non-split method in terms of the asymptotic excess risk. They also provide an analysis of the regularization parameter and the split ratio of the two methods.
565,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,This paper studies the problem of linear centroid meta-learning with data splitting. The authors propose two data splitting methods: (1) train-validation split and (2) non-splitting method. They show that the splitting method is equivalent to the non-split method in terms of the asymptotic excess risk. They also provide an analysis of the regularization parameter and the split ratio of the two methods.
566,SP:9f9dbff2fe7defd41b9ed1a6c9dcad07e932dea7,This paper studies the problem of linear centroid meta-learning with data splitting. The authors propose two data splitting methods: (1) train-validation split and (2) non-splitting method. They show that the splitting method is equivalent to the non-split method in terms of the asymptotic excess risk. They also provide an analysis of the regularization parameter and the split ratio of the two methods.
567,SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes Me-Momentum, a method for extracting hard confident examples from noisy training data. The key idea is to use momentum as a regularizer to encourage the classifier to produce more confident examples. The method is evaluated on a variety of datasets and compared to other methods."
568,SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes Me-Momentum, a method for extracting hard confident examples from noisy training data. The key idea is to use momentum as a regularizer to encourage the classifier to produce more confident examples. The method is evaluated on a variety of datasets and compared to other methods."
569,SP:bb566eda95867f83a80664b2f685ad373147c87b,"This paper proposes Me-Momentum, a method for extracting hard confident examples from noisy training data. The key idea is to use momentum as a regularizer to encourage the classifier to produce more confident examples. The method is evaluated on a variety of datasets and compared to other methods."
570,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the problem of certified robustness against data poisoning attacks. The authors propose two methods for certifying the robustness of a machine learning model. The first method is based on the intrinsic majority vote mechanism, which is a simple extension of the k-nearest neighbor (kNN) algorithm. The second method is a modified version of the radius nearest neighbor (rNN) method, where the intrinsic vote mechanism is the same as in kNN, but the authors propose to use a different type of certified defense. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
571,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the problem of certified robustness against data poisoning attacks. The authors propose two methods for certifying the robustness of a machine learning model. The first method is based on the intrinsic majority vote mechanism, which is a simple extension of the k-nearest neighbor (kNN) algorithm. The second method is a modified version of the radius nearest neighbor (rNN) method, where the intrinsic vote mechanism is the same as in kNN, but the authors propose to use a different type of certified defense. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
572,SP:ca57b693e5eff372c872f42d66b18b8aa1d07c87,"This paper studies the problem of certified robustness against data poisoning attacks. The authors propose two methods for certifying the robustness of a machine learning model. The first method is based on the intrinsic majority vote mechanism, which is a simple extension of the k-nearest neighbor (kNN) algorithm. The second method is a modified version of the radius nearest neighbor (rNN) method, where the intrinsic vote mechanism is the same as in kNN, but the authors propose to use a different type of certified defense. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
573,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for graph neural network (GNN). The authors consider the variance of gradients, compute time, and mini-batch size. The authors propose an estimator for the gradients and compute time based on the randomness of randomness. The proposed estimator can be used to select the optimal batch size for GNNs."
574,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for graph neural network (GNN). The authors consider the variance of gradients, compute time, and mini-batch size. The authors propose an estimator for the gradients and compute time based on the randomness of randomness. The proposed estimator can be used to select the optimal batch size for GNNs."
575,SP:6cfe70be8ac34d6f61009e7e583e537e9adeb648,"This paper studies the batch size selection problem for graph neural network (GNN). The authors consider the variance of gradients, compute time, and mini-batch size. The authors propose an estimator for the gradients and compute time based on the randomness of randomness. The proposed estimator can be used to select the optimal batch size for GNNs."
576,SP:30d97322709cd292a49f936c767099f11b0e2913,This paper proposes a method for estimating confidence scores for neural network classifiers. The method is based on Gaussian Processes (GPs) and uses a Gaussian process to estimate the confidence scores. The proposed method is evaluated on UCI and CIFAR-10 datasets. The results show that the proposed method outperforms existing methods.
577,SP:30d97322709cd292a49f936c767099f11b0e2913,This paper proposes a method for estimating confidence scores for neural network classifiers. The method is based on Gaussian Processes (GPs) and uses a Gaussian process to estimate the confidence scores. The proposed method is evaluated on UCI and CIFAR-10 datasets. The results show that the proposed method outperforms existing methods.
578,SP:30d97322709cd292a49f936c767099f11b0e2913,This paper proposes a method for estimating confidence scores for neural network classifiers. The method is based on Gaussian Processes (GPs) and uses a Gaussian process to estimate the confidence scores. The proposed method is evaluated on UCI and CIFAR-10 datasets. The results show that the proposed method outperforms existing methods.
579,SP:131b3da98f56d3af273171f496b217b90754a0a7,This paper proposes a method for knowledge distillation in open domain question answering. The main idea is to train a retriever model to generate synthetic labels for the retriever and a reader model to read the retrieved documents. The retriever is trained on the synthetic labels and the reader model is trained to predict the attention scores of the reader. The authors show that the proposed method is able to outperform the state-of-the-art methods on the question answering task.
580,SP:131b3da98f56d3af273171f496b217b90754a0a7,This paper proposes a method for knowledge distillation in open domain question answering. The main idea is to train a retriever model to generate synthetic labels for the retriever and a reader model to read the retrieved documents. The retriever is trained on the synthetic labels and the reader model is trained to predict the attention scores of the reader. The authors show that the proposed method is able to outperform the state-of-the-art methods on the question answering task.
581,SP:131b3da98f56d3af273171f496b217b90754a0a7,This paper proposes a method for knowledge distillation in open domain question answering. The main idea is to train a retriever model to generate synthetic labels for the retriever and a reader model to read the retrieved documents. The retriever is trained on the synthetic labels and the reader model is trained to predict the attention scores of the reader. The authors show that the proposed method is able to outperform the state-of-the-art methods on the question answering task.
582,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper studies the problem of safety in reinforcement learning. The authors propose a method for learning a controller that is safe in the presence of constraints. The main idea is to use finite automata to model the dynamics of a constrained Markov decision process, which is then used to learn a cost function for the MDP state. The cost function is learned in a formal language, and the authors show that the cost function can be expressed as a function of the number of constraints in the state space. The paper also shows that the proposed method can be applied to Atari games."
583,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper studies the problem of safety in reinforcement learning. The authors propose a method for learning a controller that is safe in the presence of constraints. The main idea is to use finite automata to model the dynamics of a constrained Markov decision process, which is then used to learn a cost function for the MDP state. The cost function is learned in a formal language, and the authors show that the cost function can be expressed as a function of the number of constraints in the state space. The paper also shows that the proposed method can be applied to Atari games."
584,SP:a516fff3cabc13cea1b8ed07dbf9eb1acb7dbb0e,"This paper studies the problem of safety in reinforcement learning. The authors propose a method for learning a controller that is safe in the presence of constraints. The main idea is to use finite automata to model the dynamics of a constrained Markov decision process, which is then used to learn a cost function for the MDP state. The cost function is learned in a formal language, and the authors show that the cost function can be expressed as a function of the number of constraints in the state space. The paper also shows that the proposed method can be applied to Atari games."
585,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model for binary classification. The main idea is to replace the decision tree with a cascade of decision subtrees, where each decision subtree is associated with a different explanation path. The authors show that this cascading model is able to achieve better performance than the monolithic decision tree. "
586,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model for binary classification. The main idea is to replace the decision tree with a cascade of decision subtrees, where each decision subtree is associated with a different explanation path. The authors show that this cascading model is able to achieve better performance than the monolithic decision tree. "
587,SP:e18cfc1502c4087422d3baf655c244d4f3924a76,"This paper proposes a cascading decision tree model for binary classification. The main idea is to replace the decision tree with a cascade of decision subtrees, where each decision subtree is associated with a different explanation path. The authors show that this cascading model is able to achieve better performance than the monolithic decision tree. "
588,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the sparsity pattern of weight tensors in neural networks. The authors show that the random, static sparsity patterns of weights can be explained by Gaussian Process kernels, which can be viewed as a special case of the infinite-width model kernel. They show that this is the case for finite-width models with Gaussian process kernels. They also show that for infinite width models, the random and static patterns can be understood as Gaussian processes. They further show that when the network width is finite, the weight tensor can be represented as a Gaussian kernel."
589,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the sparsity pattern of weight tensors in neural networks. The authors show that the random, static sparsity patterns of weights can be explained by Gaussian Process kernels, which can be viewed as a special case of the infinite-width model kernel. They show that this is the case for finite-width models with Gaussian process kernels. They also show that for infinite width models, the random and static patterns can be understood as Gaussian processes. They further show that when the network width is finite, the weight tensor can be represented as a Gaussian kernel."
590,SP:0508336b2ec032b9b98a1039e94ea223f3987cec,"This paper studies the sparsity pattern of weight tensors in neural networks. The authors show that the random, static sparsity patterns of weights can be explained by Gaussian Process kernels, which can be viewed as a special case of the infinite-width model kernel. They show that this is the case for finite-width models with Gaussian process kernels. They also show that for infinite width models, the random and static patterns can be understood as Gaussian processes. They further show that when the network width is finite, the weight tensor can be represented as a Gaussian kernel."
591,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,This paper proposes a joint pre-training framework for knowledge graph and language models. The main idea is to jointly train a knowledge module and a language module. The language module is used to generate context-aware initial embeddings for the knowledge graph. The knowledge module is then used to train the language model. The experimental results show that the proposed JAKET can achieve state-of-the-art results on several knowledge-aware NLP tasks.
592,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,This paper proposes a joint pre-training framework for knowledge graph and language models. The main idea is to jointly train a knowledge module and a language module. The language module is used to generate context-aware initial embeddings for the knowledge graph. The knowledge module is then used to train the language model. The experimental results show that the proposed JAKET can achieve state-of-the-art results on several knowledge-aware NLP tasks.
593,SP:92e5a610ed13ada6d25d433b03ac06fa5eebd963,This paper proposes a joint pre-training framework for knowledge graph and language models. The main idea is to jointly train a knowledge module and a language module. The language module is used to generate context-aware initial embeddings for the knowledge graph. The knowledge module is then used to train the language model. The experimental results show that the proposed JAKET can achieve state-of-the-art results on several knowledge-aware NLP tasks.
594,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes a method for learning autoregressive orders for language modeling. The authors propose to use a neural network to model the latent variable of the input sequence, and then use an unsupervised learner to estimate the order of the latent variables. The proposed method is evaluated on a sequence modeling task and compared to the state-of-the-art methods."
595,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes a method for learning autoregressive orders for language modeling. The authors propose to use a neural network to model the latent variable of the input sequence, and then use an unsupervised learner to estimate the order of the latent variables. The proposed method is evaluated on a sequence modeling task and compared to the state-of-the-art methods."
596,SP:1db95a377f3d5ed129aa0511f840f647375e3528,"This paper proposes a method for learning autoregressive orders for language modeling. The authors propose to use a neural network to model the latent variable of the input sequence, and then use an unsupervised learner to estimate the order of the latent variables. The proposed method is evaluated on a sequence modeling task and compared to the state-of-the-art methods."
597,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a new sampling method for graph convolutional networks (GCNs) based on doubly variance reduction. The proposed method is based on the idea that the variance of sampling methods depends on the forward propagation and backward propagation. The authors show that the proposed method converges to a stationary point with O(1/T) convergence rate.
598,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a new sampling method for graph convolutional networks (GCNs) based on doubly variance reduction. The proposed method is based on the idea that the variance of sampling methods depends on the forward propagation and backward propagation. The authors show that the proposed method converges to a stationary point with O(1/T) convergence rate.
599,SP:1c310f02acda4aa14e4d043c8d6de8c94a8ecf44,This paper proposes a new sampling method for graph convolutional networks (GCNs) based on doubly variance reduction. The proposed method is based on the idea that the variance of sampling methods depends on the forward propagation and backward propagation. The authors show that the proposed method converges to a stationary point with O(1/T) convergence rate.
600,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a conditional adversarial generator for image manipulation tasks. The generator is trained with a single image training step, and then the generator is used to generate a new image for a new task. The proposed method is evaluated on a variety of tasks, including image manipulation, image segmentation, and image reconstruction."
601,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a conditional adversarial generator for image manipulation tasks. The generator is trained with a single image training step, and then the generator is used to generate a new image for a new task. The proposed method is evaluated on a variety of tasks, including image manipulation, image segmentation, and image reconstruction."
602,SP:02e100a9ad4eedab8cba043d3726f022bc09a3af,"This paper proposes a conditional adversarial generator for image manipulation tasks. The generator is trained with a single image training step, and then the generator is used to generate a new image for a new task. The proposed method is evaluated on a variety of tasks, including image manipulation, image segmentation, and image reconstruction."
603,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,This paper proposes a method for searching for the Maximum Common Subgraph (MCS) in graph neural networks (GNNs). The main idea of the method is to use a reinforcement learning algorithm to guide the search process. The method is evaluated on synthetic and real-world large graph pairs and compared to several state-of-the-art GNNs.
604,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,This paper proposes a method for searching for the Maximum Common Subgraph (MCS) in graph neural networks (GNNs). The main idea of the method is to use a reinforcement learning algorithm to guide the search process. The method is evaluated on synthetic and real-world large graph pairs and compared to several state-of-the-art GNNs.
605,SP:4d7c1e30fa8eb3e7c67a4ec3bccc5d3ef713a773,This paper proposes a method for searching for the Maximum Common Subgraph (MCS) in graph neural networks (GNNs). The main idea of the method is to use a reinforcement learning algorithm to guide the search process. The method is evaluated on synthetic and real-world large graph pairs and compared to several state-of-the-art GNNs.
606,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes a method for recovering the original wireframe from a 3D point cloud. The proposed method is based on PC2WF, which is a neural network architecture that learns to predict the vertices, edges, and features of a point cloud from a set of candidate vertices and edges. The network is trained on a synthetic dataset and on a real-world dataset, where it is able to recover the ground truth wireframe."
607,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes a method for recovering the original wireframe from a 3D point cloud. The proposed method is based on PC2WF, which is a neural network architecture that learns to predict the vertices, edges, and features of a point cloud from a set of candidate vertices and edges. The network is trained on a synthetic dataset and on a real-world dataset, where it is able to recover the ground truth wireframe."
608,SP:581c6d218e75b0df808bc2c83c8731a94e94a5b3,"This paper proposes a method for recovering the original wireframe from a 3D point cloud. The proposed method is based on PC2WF, which is a neural network architecture that learns to predict the vertices, edges, and features of a point cloud from a set of candidate vertices and edges. The network is trained on a synthetic dataset and on a real-world dataset, where it is able to recover the ground truth wireframe."
609,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies the convergence of stochastic gradient descent algorithms for neural networks under the assumption that the noise level of the gradient is non-stationary. The authors provide a uniform bound on the moments of the gradients of the noise, and provide a theoretical analysis of the convergence rate of SGD and RMSProp under this assumption. In particular, the authors show that SGD converges to a stationary point at a rate of $O(1/\epsilon^2)$, which is the rate of convergence for SGD with a fixed step-size. They also provide a lower bound for the rate at which the gradient converges with respect to the step size."
610,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies the convergence of stochastic gradient descent algorithms for neural networks under the assumption that the noise level of the gradient is non-stationary. The authors provide a uniform bound on the moments of the gradients of the noise, and provide a theoretical analysis of the convergence rate of SGD and RMSProp under this assumption. In particular, the authors show that SGD converges to a stationary point at a rate of $O(1/\epsilon^2)$, which is the rate of convergence for SGD with a fixed step-size. They also provide a lower bound for the rate at which the gradient converges with respect to the step size."
611,SP:3e0fd62d9815d7de5e5139a1d6d2e80eea917154,"This paper studies the convergence of stochastic gradient descent algorithms for neural networks under the assumption that the noise level of the gradient is non-stationary. The authors provide a uniform bound on the moments of the gradients of the noise, and provide a theoretical analysis of the convergence rate of SGD and RMSProp under this assumption. In particular, the authors show that SGD converges to a stationary point at a rate of $O(1/\epsilon^2)$, which is the rate of convergence for SGD with a fixed step-size. They also provide a lower bound for the rate at which the gradient converges with respect to the step size."
612,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,This paper proposes a method to improve the decoding speed of neural machine translation (NMT) models by incorporating prior word alignment information into the model. The proposed method is based on the idea of using a model that learns to predict the target information from the target input. The model is trained on the target data and the prior information is used as a regularizer to improve decoding speed. The method is evaluated on English-Korean and English-French datasets. The results show that the proposed method outperforms the baseline models.
613,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,This paper proposes a method to improve the decoding speed of neural machine translation (NMT) models by incorporating prior word alignment information into the model. The proposed method is based on the idea of using a model that learns to predict the target information from the target input. The model is trained on the target data and the prior information is used as a regularizer to improve decoding speed. The method is evaluated on English-Korean and English-French datasets. The results show that the proposed method outperforms the baseline models.
614,SP:71c4e6ab911962d730461eda0f2d72d810fc017c,This paper proposes a method to improve the decoding speed of neural machine translation (NMT) models by incorporating prior word alignment information into the model. The proposed method is based on the idea of using a model that learns to predict the target information from the target input. The model is trained on the target data and the prior information is used as a regularizer to improve decoding speed. The method is evaluated on English-Korean and English-French datasets. The results show that the proposed method outperforms the baseline models.
615,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper studies the hardness of MDP Playground, a popular benchmark for evaluating the performance of reinforcement learning algorithms. The main contribution of this paper is to propose a new hardness metric for MDP playground. The new metric is based on the difference between the state of the environment and the state-action space. The paper also provides a theoretical analysis of the proposed metric."
616,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper studies the hardness of MDP Playground, a popular benchmark for evaluating the performance of reinforcement learning algorithms. The main contribution of this paper is to propose a new hardness metric for MDP playground. The new metric is based on the difference between the state of the environment and the state-action space. The paper also provides a theoretical analysis of the proposed metric."
617,SP:c26255a8ad441f11cfbe18fd6dad14773aca4a2b,"This paper studies the hardness of MDP Playground, a popular benchmark for evaluating the performance of reinforcement learning algorithms. The main contribution of this paper is to propose a new hardness metric for MDP playground. The new metric is based on the difference between the state of the environment and the state-action space. The paper also provides a theoretical analysis of the proposed metric."
618,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes a quantile calibration method for regression models. The proposed method is based on Isotonic Regression (IR), which is a probabilistic regression model. The main idea is to train a model on a set of samples, and then use the quantile regularizer to calibrate the model with respect to the true quantile. The authors show that the proposed method can be applied to a variety of models, including Dropout VI, Deep Ensembles, and Deep Ensemble."
619,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes a quantile calibration method for regression models. The proposed method is based on Isotonic Regression (IR), which is a probabilistic regression model. The main idea is to train a model on a set of samples, and then use the quantile regularizer to calibrate the model with respect to the true quantile. The authors show that the proposed method can be applied to a variety of models, including Dropout VI, Deep Ensembles, and Deep Ensemble."
620,SP:e8cbe62252aa671a6deaf12b97063063dfc6d1b0,"This paper proposes a quantile calibration method for regression models. The proposed method is based on Isotonic Regression (IR), which is a probabilistic regression model. The main idea is to train a model on a set of samples, and then use the quantile regularizer to calibrate the model with respect to the true quantile. The authors show that the proposed method can be applied to a variety of models, including Dropout VI, Deep Ensembles, and Deep Ensemble."
621,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,This paper proposes a method for 3D dense reconstruction in spatial environments. The proposed method is based on a deep state-space model and a differentiable raycaster. The model is trained with a variational inference method and a neural network. The method is evaluated on simulated aerial vehicle flight data. The results show that the proposed method outperforms the baselines.
622,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,This paper proposes a method for 3D dense reconstruction in spatial environments. The proposed method is based on a deep state-space model and a differentiable raycaster. The model is trained with a variational inference method and a neural network. The method is evaluated on simulated aerial vehicle flight data. The results show that the proposed method outperforms the baselines.
623,SP:9c71ab8dcc433b59d9da3f0db377b74a369112bc,This paper proposes a method for 3D dense reconstruction in spatial environments. The proposed method is based on a deep state-space model and a differentiable raycaster. The model is trained with a variational inference method and a neural network. The method is evaluated on simulated aerial vehicle flight data. The results show that the proposed method outperforms the baselines.
624,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for training a multi-modal entity-conditioned attention module (EMMA) that is able to generalize to unseen environments. EMMA is trained on a large corpus of text manuals, where it is shown that it can generalize well to unseen states. The authors also show that EMMA can be trained with noisy descriptions, which is a good sign that the grounding is working well."
625,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for training a multi-modal entity-conditioned attention module (EMMA) that is able to generalize to unseen environments. EMMA is trained on a large corpus of text manuals, where it is shown that it can generalize well to unseen states. The authors also show that EMMA can be trained with noisy descriptions, which is a good sign that the grounding is working well."
626,SP:bacb279ab6d1997bf44b7b2af583f29679219c36,"This paper proposes a method for training a multi-modal entity-conditioned attention module (EMMA) that is able to generalize to unseen environments. EMMA is trained on a large corpus of text manuals, where it is shown that it can generalize well to unseen states. The authors also show that EMMA can be trained with noisy descriptions, which is a good sign that the grounding is working well."
627,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"This paper proposes a policy gradient method for actor-critic reinforcement learning. The main idea of the method is to train a policy-gradient estimator to estimate the value function of the actor, which is then used to train the critic. The method is evaluated on a variety of continuous control tasks, where it is shown to outperform the baselines."
628,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"This paper proposes a policy gradient method for actor-critic reinforcement learning. The main idea of the method is to train a policy-gradient estimator to estimate the value function of the actor, which is then used to train the critic. The method is evaluated on a variety of continuous control tasks, where it is shown to outperform the baselines."
629,SP:d90da59c651ae3e97af1cf85f3ab1f12cd56d149,"This paper proposes a policy gradient method for actor-critic reinforcement learning. The main idea of the method is to train a policy-gradient estimator to estimate the value function of the actor, which is then used to train the critic. The method is evaluated on a variety of continuous control tasks, where it is shown to outperform the baselines."
630,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper studies the generalization performance of neural tangent kernels (NTKs) in the over-parameterized regime. The authors show that NTKs have depth dependence on the number of local labels, and that the depth dependence can be controlled by the local labels and global labels. They also show that the NTK can be used as a regularizer for feature learning."
631,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper studies the generalization performance of neural tangent kernels (NTKs) in the over-parameterized regime. The authors show that NTKs have depth dependence on the number of local labels, and that the depth dependence can be controlled by the local labels and global labels. They also show that the NTK can be used as a regularizer for feature learning."
632,SP:62d79bf04817bba3fdffb2c0c9209923a8428533,"This paper studies the generalization performance of neural tangent kernels (NTKs) in the over-parameterized regime. The authors show that NTKs have depth dependence on the number of local labels, and that the depth dependence can be controlled by the local labels and global labels. They also show that the NTK can be used as a regularizer for feature learning."
633,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies the sample complexity of representation learning for few-shot learning. The authors consider the case where the number of samples required to learn a linear representation is bounded by a factor of 1/\sqrt{T}. The authors show that for linear regression and neural networks, sample complexity can be reduced to $\Omega(1/\epsilon^2)$, where $T$ is the dimension of the input space. The main contribution of this paper is to provide a lower bound on sample complexity for linear representation learning. "
634,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies the sample complexity of representation learning for few-shot learning. The authors consider the case where the number of samples required to learn a linear representation is bounded by a factor of 1/\sqrt{T}. The authors show that for linear regression and neural networks, sample complexity can be reduced to $\Omega(1/\epsilon^2)$, where $T$ is the dimension of the input space. The main contribution of this paper is to provide a lower bound on sample complexity for linear representation learning. "
635,SP:9f8a9299ee67b9c707b241ce84cf41f4917ef735,"This paper studies the sample complexity of representation learning for few-shot learning. The authors consider the case where the number of samples required to learn a linear representation is bounded by a factor of 1/\sqrt{T}. The authors show that for linear regression and neural networks, sample complexity can be reduced to $\Omega(1/\epsilon^2)$, where $T$ is the dimension of the input space. The main contribution of this paper is to provide a lower bound on sample complexity for linear representation learning. "
636,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,"This paper proposes a method for improving the robustness of neural networks against adversarial attacks. The authors propose a black-box approach to learn a set of features for each layer of the network that are robust to adversarial perturbations. The main idea is to train a network to predict the features of each layer, and then use these features to train an ensemble of networks to generate adversarial examples. The proposed method is evaluated on PCA and L2-distortion datasets, and compared to the state-of-the-art."
637,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,"This paper proposes a method for improving the robustness of neural networks against adversarial attacks. The authors propose a black-box approach to learn a set of features for each layer of the network that are robust to adversarial perturbations. The main idea is to train a network to predict the features of each layer, and then use these features to train an ensemble of networks to generate adversarial examples. The proposed method is evaluated on PCA and L2-distortion datasets, and compared to the state-of-the-art."
638,SP:e29ce50c1c28f9264613736b6c2d20afc4f312c1,"This paper proposes a method for improving the robustness of neural networks against adversarial attacks. The authors propose a black-box approach to learn a set of features for each layer of the network that are robust to adversarial perturbations. The main idea is to train a network to predict the features of each layer, and then use these features to train an ensemble of networks to generate adversarial examples. The proposed method is evaluated on PCA and L2-distortion datasets, and compared to the state-of-the-art."
639,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task reinforcement learning. The main idea is to learn a policy that maximizes the expected return of a given set of tasks, and then use the learned policy to train a new policy for the new task. The method is based on the idea of expectation maximization, which is an extension of the expectation-maximization algorithm proposed in [1]. The main contribution of this paper is that the authors propose to use the expectation of the new policy as a regularizer for the original policy. The authors show that the proposed method outperforms the state-of-the-art methods in both discrete and continuous control tasks."
640,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task reinforcement learning. The main idea is to learn a policy that maximizes the expected return of a given set of tasks, and then use the learned policy to train a new policy for the new task. The method is based on the idea of expectation maximization, which is an extension of the expectation-maximization algorithm proposed in [1]. The main contribution of this paper is that the authors propose to use the expectation of the new policy as a regularizer for the original policy. The authors show that the proposed method outperforms the state-of-the-art methods in both discrete and continuous control tasks."
641,SP:e3fdb96a8c321a86b136e765abe796019d6f9c7a,"This paper proposes a method for multi-task reinforcement learning. The main idea is to learn a policy that maximizes the expected return of a given set of tasks, and then use the learned policy to train a new policy for the new task. The method is based on the idea of expectation maximization, which is an extension of the expectation-maximization algorithm proposed in [1]. The main contribution of this paper is that the authors propose to use the expectation of the new policy as a regularizer for the original policy. The authors show that the proposed method outperforms the state-of-the-art methods in both discrete and continuous control tasks."
642,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning method for non-stationary time series. The proposed method is based on temporal neighborhood coding (TNC), which aims to learn representations for time series with stationary properties. The authors propose a debiased contrastive objective for the representation learning objective, which is motivated by the fact that the representation learned by TNC is more general than the representations learned by unsupervised methods. Experiments on clustering and classification tasks demonstrate the effectiveness of the proposed method."
643,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning method for non-stationary time series. The proposed method is based on temporal neighborhood coding (TNC), which aims to learn representations for time series with stationary properties. The authors propose a debiased contrastive objective for the representation learning objective, which is motivated by the fact that the representation learned by TNC is more general than the representations learned by unsupervised methods. Experiments on clustering and classification tasks demonstrate the effectiveness of the proposed method."
644,SP:b3805eb7114391ed15d5806b1c3eb383bff44250,"This paper proposes a self-supervised representation learning method for non-stationary time series. The proposed method is based on temporal neighborhood coding (TNC), which aims to learn representations for time series with stationary properties. The authors propose a debiased contrastive objective for the representation learning objective, which is motivated by the fact that the representation learned by TNC is more general than the representations learned by unsupervised methods. Experiments on clustering and classification tasks demonstrate the effectiveness of the proposed method."
645,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,This paper proposes a method for learning modular networks with soft weight sharing for multi-task learning and transfer learning. The authors propose a fully differentiable approach to learn the order of pre-trained modules in a modular network. The proposed method is based on the idea of self-organization of modules. The method is evaluated on image classification tasks and domain adaptation tasks. The experiments show that the proposed method outperforms the baselines.
646,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,This paper proposes a method for learning modular networks with soft weight sharing for multi-task learning and transfer learning. The authors propose a fully differentiable approach to learn the order of pre-trained modules in a modular network. The proposed method is based on the idea of self-organization of modules. The method is evaluated on image classification tasks and domain adaptation tasks. The experiments show that the proposed method outperforms the baselines.
647,SP:60b2ea4624997d6ccf862742fb9eb21b819d7eb1,This paper proposes a method for learning modular networks with soft weight sharing for multi-task learning and transfer learning. The authors propose a fully differentiable approach to learn the order of pre-trained modules in a modular network. The proposed method is based on the idea of self-organization of modules. The method is evaluated on image classification tasks and domain adaptation tasks. The experiments show that the proposed method outperforms the baselines.
648,SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the variance parameter of Variational Autoencoders (VAEs) in the presence of posterior collapse. In particular, the authors show that the variance of the objective function is a function of the posterior smoothness of the latent space. The authors propose to use this parameter as a regularizer to improve the performance of VAEs. The variance parameter is defined as the difference between the FID of the generated image and the original image. The paper also shows that this regularizer can be used as an additional regularizer for AR-ELBO."
649,SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the variance parameter of Variational Autoencoders (VAEs) in the presence of posterior collapse. In particular, the authors show that the variance of the objective function is a function of the posterior smoothness of the latent space. The authors propose to use this parameter as a regularizer to improve the performance of VAEs. The variance parameter is defined as the difference between the FID of the generated image and the original image. The paper also shows that this regularizer can be used as an additional regularizer for AR-ELBO."
650,SP:cae669c631e11fe703bf6cb511404866b19f474a,"This paper studies the variance parameter of Variational Autoencoders (VAEs) in the presence of posterior collapse. In particular, the authors show that the variance of the objective function is a function of the posterior smoothness of the latent space. The authors propose to use this parameter as a regularizer to improve the performance of VAEs. The variance parameter is defined as the difference between the FID of the generated image and the original image. The paper also shows that this regularizer can be used as an additional regularizer for AR-ELBO."
651,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes a new method for learning disentangled representations of latent variables in VAEs. The proposed method is based on a mixture model and a global Gaussian latent variable, where the mixture model is a mixture of a local and data-dependent space, and the Gaussian variable is a global latent variable. The authors show that the proposed method can learn disentanglement representations that are interpretable and interpretable in a domain-agnostic manner. They also provide a user-defined regularization term to encourage domain alignment."
652,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes a new method for learning disentangled representations of latent variables in VAEs. The proposed method is based on a mixture model and a global Gaussian latent variable, where the mixture model is a mixture of a local and data-dependent space, and the Gaussian variable is a global latent variable. The authors show that the proposed method can learn disentanglement representations that are interpretable and interpretable in a domain-agnostic manner. They also provide a user-defined regularization term to encourage domain alignment."
653,SP:cb3c10afbdd8a49cdc23e3ea71ea46ab27253b85,"This paper proposes a new method for learning disentangled representations of latent variables in VAEs. The proposed method is based on a mixture model and a global Gaussian latent variable, where the mixture model is a mixture of a local and data-dependent space, and the Gaussian variable is a global latent variable. The authors show that the proposed method can learn disentanglement representations that are interpretable and interpretable in a domain-agnostic manner. They also provide a user-defined regularization term to encourage domain alignment."
654,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"This paper proposes a new representation learning method based on human interaction and attention. The proposed method, called “muscly-supervised” representation learning (MoCo), is based on the idea that human interactions and attention can be used to improve the representation learning. MoCo is trained on a large dataset of human interactions, including gaze, body part movements, and gaze. The authors evaluate MoCo on a variety of tasks, including scene classification (semantic), action recognition (temporal), depth estimation (geometric), and affordance."
655,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"This paper proposes a new representation learning method based on human interaction and attention. The proposed method, called “muscly-supervised” representation learning (MoCo), is based on the idea that human interactions and attention can be used to improve the representation learning. MoCo is trained on a large dataset of human interactions, including gaze, body part movements, and gaze. The authors evaluate MoCo on a variety of tasks, including scene classification (semantic), action recognition (temporal), depth estimation (geometric), and affordance."
656,SP:33792375012ff9dcffab598cc8fe5ebc71c98af4,"This paper proposes a new representation learning method based on human interaction and attention. The proposed method, called “muscly-supervised” representation learning (MoCo), is based on the idea that human interactions and attention can be used to improve the representation learning. MoCo is trained on a large dataset of human interactions, including gaze, body part movements, and gaze. The authors evaluate MoCo on a variety of tasks, including scene classification (semantic), action recognition (temporal), depth estimation (geometric), and affordance."
657,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the effect of negative pretraining on the generalization ability of neural networks. The authors propose a new method to mitigate the negative effects of negative pre-training. The method is based on the observation that when the learning rate of a neural network is low, the performance of the model can be affected by the number of iterations of the network. To mitigate this effect, the authors propose to change the learning rates of the neural network during the training process. The experiments on MNIST and CIFAR-10 show that the proposed method is effective in reducing the negative effect of pretraining."
658,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the effect of negative pretraining on the generalization ability of neural networks. The authors propose a new method to mitigate the negative effects of negative pre-training. The method is based on the observation that when the learning rate of a neural network is low, the performance of the model can be affected by the number of iterations of the network. To mitigate this effect, the authors propose to change the learning rates of the neural network during the training process. The experiments on MNIST and CIFAR-10 show that the proposed method is effective in reducing the negative effect of pretraining."
659,SP:6873a5e80e6142983c9bbd22931bfded7eed2f59,"This paper studies the effect of negative pretraining on the generalization ability of neural networks. The authors propose a new method to mitigate the negative effects of negative pre-training. The method is based on the observation that when the learning rate of a neural network is low, the performance of the model can be affected by the number of iterations of the network. To mitigate this effect, the authors propose to change the learning rates of the neural network during the training process. The experiments on MNIST and CIFAR-10 show that the proposed method is effective in reducing the negative effect of pretraining."
660,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper studies the problem of adversarial defense. The authors propose a bi-level optimization algorithm to generate safe spots for adversarially trained classifiers. They also propose a safe spot inducing model training scheme and an out-of-distribution detection algorithm to detect near-distinguishable outliers. The experimental results show that the proposed method can improve the certified robustness of smoothed classifiers on ImageNet datasets.
661,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper studies the problem of adversarial defense. The authors propose a bi-level optimization algorithm to generate safe spots for adversarially trained classifiers. They also propose a safe spot inducing model training scheme and an out-of-distribution detection algorithm to detect near-distinguishable outliers. The experimental results show that the proposed method can improve the certified robustness of smoothed classifiers on ImageNet datasets.
662,SP:5d27e5a301ed4f224fb2baecad77006a9fbb2189,This paper studies the problem of adversarial defense. The authors propose a bi-level optimization algorithm to generate safe spots for adversarially trained classifiers. They also propose a safe spot inducing model training scheme and an out-of-distribution detection algorithm to detect near-distinguishable outliers. The experimental results show that the proposed method can improve the certified robustness of smoothed classifiers on ImageNet datasets.
663,SP:1350ab543b6a5cf579827835fb27011751cc047f,This paper proposes a point spatio-temporal (PST) convolutional network for 3D point cloud sequences. The proposed method is based on the idea of spatial convolution and temporal convolution. The main difference between the proposed method and previous work is the use of a deep network to represent the 3D spatial features. Experiments are conducted on 3D action recognition and 4D semantic segmentation datasets.
664,SP:1350ab543b6a5cf579827835fb27011751cc047f,This paper proposes a point spatio-temporal (PST) convolutional network for 3D point cloud sequences. The proposed method is based on the idea of spatial convolution and temporal convolution. The main difference between the proposed method and previous work is the use of a deep network to represent the 3D spatial features. Experiments are conducted on 3D action recognition and 4D semantic segmentation datasets.
665,SP:1350ab543b6a5cf579827835fb27011751cc047f,This paper proposes a point spatio-temporal (PST) convolutional network for 3D point cloud sequences. The proposed method is based on the idea of spatial convolution and temporal convolution. The main difference between the proposed method and previous work is the use of a deep network to represent the 3D spatial features. Experiments are conducted on 3D action recognition and 4D semantic segmentation datasets.
666,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for text-to-speech (TTS) adaptation. The main idea is to use acoustic information from the source TTS model as input to an acoustic encoder and a speaker embedding. The acoustic information is used to estimate the utterance-level and phoneme-level vectors of the speaker, which are then used to train a decoder to extract the mel-spectrogram decoder for the speaker. The proposed method is evaluated on both VCTK and LJSpeech datasets and compared to several baselines."
667,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for text-to-speech (TTS) adaptation. The main idea is to use acoustic information from the source TTS model as input to an acoustic encoder and a speaker embedding. The acoustic information is used to estimate the utterance-level and phoneme-level vectors of the speaker, which are then used to train a decoder to extract the mel-spectrogram decoder for the speaker. The proposed method is evaluated on both VCTK and LJSpeech datasets and compared to several baselines."
668,SP:a808583e924f85ec847c6b2597bae5c3eeec0ca7,"This paper proposes AdaSpeech, an adaptive TTS system for text-to-speech (TTS) adaptation. The main idea is to use acoustic information from the source TTS model as input to an acoustic encoder and a speaker embedding. The acoustic information is used to estimate the utterance-level and phoneme-level vectors of the speaker, which are then used to train a decoder to extract the mel-spectrogram decoder for the speaker. The proposed method is evaluated on both VCTK and LJSpeech datasets and compared to several baselines."
669,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,This paper studies the problem of training sparse neural networks with regularizers and optimizers. The authors show that the gradient flow of a sparse network can be optimized in a way that is similar to that of a dense network. They show that this can be achieved by optimizing the gradients of the weights of the network in a manner similar to the optimization of dense networks. They also provide a theoretical analysis of the convergence rate of the proposed method.
670,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,This paper studies the problem of training sparse neural networks with regularizers and optimizers. The authors show that the gradient flow of a sparse network can be optimized in a way that is similar to that of a dense network. They show that this can be achieved by optimizing the gradients of the weights of the network in a manner similar to the optimization of dense networks. They also provide a theoretical analysis of the convergence rate of the proposed method.
671,SP:66f56cc202aed1382a342e13ecfe0c5af87f6fee,This paper studies the problem of training sparse neural networks with regularizers and optimizers. The authors show that the gradient flow of a sparse network can be optimized in a way that is similar to that of a dense network. They show that this can be achieved by optimizing the gradients of the weights of the network in a manner similar to the optimization of dense networks. They also provide a theoretical analysis of the convergence rate of the proposed method.
672,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,This paper proposes a unified model for node classification. The proposed model is a combination of Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). The main idea is to combine the features of LPA and GCN in an end-to-end model. The authors show that the proposed model outperforms the state-of-the-art in terms of node classification accuracy.
673,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,This paper proposes a unified model for node classification. The proposed model is a combination of Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). The main idea is to combine the features of LPA and GCN in an end-to-end model. The authors show that the proposed model outperforms the state-of-the-art in terms of node classification accuracy.
674,SP:d9f17344cd266b16a70c37d891b2c64a6d454908,This paper proposes a unified model for node classification. The proposed model is a combination of Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). The main idea is to combine the features of LPA and GCN in an end-to-end model. The authors show that the proposed model outperforms the state-of-the-art in terms of node classification accuracy.
675,SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of a classifier trained on the ground truth label generating function (GLF) and a generator function space. The authors show that under certain conditions, the VC-dimension and Rademacher complexity of the classifier can be reduced to a lower bound on the R-complexity of the generator function spaces. They also show that the co-computation of the two functions can be used as a measure of generalization performance. The main contribution of this paper is to provide a theoretical analysis of the relationship between the complexity of classifier and generator functions, and to show that it can be expressed as a sum of two terms: (1) dissociation co-consistency term and (2) invariance co-compatibility term. "
676,SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of a classifier trained on the ground truth label generating function (GLF) and a generator function space. The authors show that under certain conditions, the VC-dimension and Rademacher complexity of the classifier can be reduced to a lower bound on the R-complexity of the generator function spaces. They also show that the co-computation of the two functions can be used as a measure of generalization performance. The main contribution of this paper is to provide a theoretical analysis of the relationship between the complexity of classifier and generator functions, and to show that it can be expressed as a sum of two terms: (1) dissociation co-consistency term and (2) invariance co-compatibility term. "
677,SP:c5883e3a59e6575eff044251b38175a6ed024034,"This paper studies the generalization error of a classifier trained on the ground truth label generating function (GLF) and a generator function space. The authors show that under certain conditions, the VC-dimension and Rademacher complexity of the classifier can be reduced to a lower bound on the R-complexity of the generator function spaces. They also show that the co-computation of the two functions can be used as a measure of generalization performance. The main contribution of this paper is to provide a theoretical analysis of the relationship between the complexity of classifier and generator functions, and to show that it can be expressed as a sum of two terms: (1) dissociation co-consistency term and (2) invariance co-compatibility term. "
678,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a new post-training quantization method, BRECQ, which aims to improve the performance of neural network quantization. The main contribution of the paper is to propose a mixed precision technique for the quantization of neural networks. The authors show that the proposed method can achieve better performance than the existing quantization-aware training (QAT) method, which is based on INT2. The paper also shows that the mixed precision method can be used to reduce the cross-layer and intra-layer sensitivity of the network. Experiments are conducted on 4-bit ResNet, MobileNetV2 and ImageNet."
679,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a new post-training quantization method, BRECQ, which aims to improve the performance of neural network quantization. The main contribution of the paper is to propose a mixed precision technique for the quantization of neural networks. The authors show that the proposed method can achieve better performance than the existing quantization-aware training (QAT) method, which is based on INT2. The paper also shows that the mixed precision method can be used to reduce the cross-layer and intra-layer sensitivity of the network. Experiments are conducted on 4-bit ResNet, MobileNetV2 and ImageNet."
680,SP:9bb36be61f1d4db88d806092219eba39bf1b99db,"This paper proposes a new post-training quantization method, BRECQ, which aims to improve the performance of neural network quantization. The main contribution of the paper is to propose a mixed precision technique for the quantization of neural networks. The authors show that the proposed method can achieve better performance than the existing quantization-aware training (QAT) method, which is based on INT2. The paper also shows that the mixed precision method can be used to reduce the cross-layer and intra-layer sensitivity of the network. Experiments are conducted on 4-bit ResNet, MobileNetV2 and ImageNet."
681,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the problem of calibration of neural networks on large labeled datasets. The authors propose a new dataset curation method to mitigate the class imbalance problem. The proposed method is based on the idea that the calibration error of a neural network is proportional to the number of labeled examples, and the authors show that the proposed method can be applied to a variety of datasets."
682,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the problem of calibration of neural networks on large labeled datasets. The authors propose a new dataset curation method to mitigate the class imbalance problem. The proposed method is based on the idea that the calibration error of a neural network is proportional to the number of labeled examples, and the authors show that the proposed method can be applied to a variety of datasets."
683,SP:3035318ac36cad693a5e4ee7bed43db8df6fb492,"This paper studies the problem of calibration of neural networks on large labeled datasets. The authors propose a new dataset curation method to mitigate the class imbalance problem. The proposed method is based on the idea that the calibration error of a neural network is proportional to the number of labeled examples, and the authors show that the proposed method can be applied to a variety of datasets."
684,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper proposes a method to train agents to communicate with each other in a 3D environment. The main idea is to learn a set of symbolic channels that can be used to communicate information about the state of the environment, and then use these channels to train a policy that can communicate with other agents. The authors show that the proposed method is able to achieve better performance than existing methods on a variety of tasks. "
685,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper proposes a method to train agents to communicate with each other in a 3D environment. The main idea is to learn a set of symbolic channels that can be used to communicate information about the state of the environment, and then use these channels to train a policy that can communicate with other agents. The authors show that the proposed method is able to achieve better performance than existing methods on a variety of tasks. "
686,SP:17d90f9d3f5891ac56f5ed6375a21d0c1517fd62,"This paper proposes a method to train agents to communicate with each other in a 3D environment. The main idea is to learn a set of symbolic channels that can be used to communicate information about the state of the environment, and then use these channels to train a policy that can communicate with other agents. The authors show that the proposed method is able to achieve better performance than existing methods on a variety of tasks. "
687,SP:5ba686e2eef369fa49b10ba3f41f102740836859,"This paper proposes a method for estimating uncertainty estimates for sequential regression with deep recurrent networks. The main idea is to use a neural network to estimate the uncertainty of the input signal, which is then used as a regularizer for the model. The authors show that the proposed method can be used to estimate both symmetric and asymmetric uncertainty estimates. The proposed method is evaluated on both drift and non-drift scenarios."
688,SP:5ba686e2eef369fa49b10ba3f41f102740836859,"This paper proposes a method for estimating uncertainty estimates for sequential regression with deep recurrent networks. The main idea is to use a neural network to estimate the uncertainty of the input signal, which is then used as a regularizer for the model. The authors show that the proposed method can be used to estimate both symmetric and asymmetric uncertainty estimates. The proposed method is evaluated on both drift and non-drift scenarios."
689,SP:5ba686e2eef369fa49b10ba3f41f102740836859,"This paper proposes a method for estimating uncertainty estimates for sequential regression with deep recurrent networks. The main idea is to use a neural network to estimate the uncertainty of the input signal, which is then used as a regularizer for the model. The authors show that the proposed method can be used to estimate both symmetric and asymmetric uncertainty estimates. The proposed method is evaluated on both drift and non-drift scenarios."
690,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"This paper studies the problem of unbalanced Gromov-Wasserstein (GW) distance between two measures. The main contribution of this paper is the formulation of the GW distance as a quadratic assignment problem, which can be formulated as a parallelizable and GPU-friendly iterative scheme for non-convex optimization problems. The paper also provides a theoretical analysis of the convergence of the proposed method."
691,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"This paper studies the problem of unbalanced Gromov-Wasserstein (GW) distance between two measures. The main contribution of this paper is the formulation of the GW distance as a quadratic assignment problem, which can be formulated as a parallelizable and GPU-friendly iterative scheme for non-convex optimization problems. The paper also provides a theoretical analysis of the convergence of the proposed method."
692,SP:0a58694abd6898a925b1d917ad2a68eefd0567e9,"This paper studies the problem of unbalanced Gromov-Wasserstein (GW) distance between two measures. The main contribution of this paper is the formulation of the GW distance as a quadratic assignment problem, which can be formulated as a parallelizable and GPU-friendly iterative scheme for non-convex optimization problems. The paper also provides a theoretical analysis of the convergence of the proposed method."
693,SP:47dcefd5515e772f29e03219c01713e2403643ce,This paper proposes a new pruning method called all-alive pruning (AAP) to reduce the computational cost of pruned networks. The main idea of AAP is to train a network with sparse parameters and then prune the weights of the pruned network. The authors show that the proposed method outperforms existing saliency-based pruning methods and one-shot pruning. They also show that dynamic pruning and iterative pruning can be combined with AAP.
694,SP:47dcefd5515e772f29e03219c01713e2403643ce,This paper proposes a new pruning method called all-alive pruning (AAP) to reduce the computational cost of pruned networks. The main idea of AAP is to train a network with sparse parameters and then prune the weights of the pruned network. The authors show that the proposed method outperforms existing saliency-based pruning methods and one-shot pruning. They also show that dynamic pruning and iterative pruning can be combined with AAP.
695,SP:47dcefd5515e772f29e03219c01713e2403643ce,This paper proposes a new pruning method called all-alive pruning (AAP) to reduce the computational cost of pruned networks. The main idea of AAP is to train a network with sparse parameters and then prune the weights of the pruned network. The authors show that the proposed method outperforms existing saliency-based pruning methods and one-shot pruning. They also show that dynamic pruning and iterative pruning can be combined with AAP.
696,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for controllable semantic image editing. The proposed method is based on a GAN-based model, which is trained with a content loss and an adversarial loss. The model is evaluated on both synthetic and real-world datasets, and compared to a number of state-of-the-art methods."
697,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for controllable semantic image editing. The proposed method is based on a GAN-based model, which is trained with a content loss and an adversarial loss. The model is evaluated on both synthetic and real-world datasets, and compared to a number of state-of-the-art methods."
698,SP:9eb7b946e00085b89844c485bcd94a392146d2b7,"This paper proposes a method for controllable semantic image editing. The proposed method is based on a GAN-based model, which is trained with a content loss and an adversarial loss. The model is evaluated on both synthetic and real-world datasets, and compared to a number of state-of-the-art methods."
699,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,This paper proposes Feature Statistics Alignment (FSA) for generating discrete sequences of discrete elements in GANs. The proposed method is based on the Gumbel-Softmax framework. The main contribution of the paper is to propose a feature alignment regularization to improve the performance of the GAN. The authors show that the proposed method outperforms the state-of-the-art methods on synthetic and real-world datasets.
700,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,This paper proposes Feature Statistics Alignment (FSA) for generating discrete sequences of discrete elements in GANs. The proposed method is based on the Gumbel-Softmax framework. The main contribution of the paper is to propose a feature alignment regularization to improve the performance of the GAN. The authors show that the proposed method outperforms the state-of-the-art methods on synthetic and real-world datasets.
701,SP:d9d9d5ade0253be2733d8b035f755ebf82e7e18b,This paper proposes Feature Statistics Alignment (FSA) for generating discrete sequences of discrete elements in GANs. The proposed method is based on the Gumbel-Softmax framework. The main contribution of the paper is to propose a feature alignment regularization to improve the performance of the GAN. The authors show that the proposed method outperforms the state-of-the-art methods on synthetic and real-world datasets.
702,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes a method for improving the performance of value-based deep reinforcement learning agents in Atari games. The proposed method is based on the Spectral DQN method, which is an extension of Spectral-DQN. The main idea of the paper is to use Spectral Q-values as a reward function to encourage the agent to explore more. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of Atari games and benchmarks."
703,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes a method for improving the performance of value-based deep reinforcement learning agents in Atari games. The proposed method is based on the Spectral DQN method, which is an extension of Spectral-DQN. The main idea of the paper is to use Spectral Q-values as a reward function to encourage the agent to explore more. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of Atari games and benchmarks."
704,SP:3ffa34b54779998f473f4e9a52287bcd0485cec8,"This paper proposes a method for improving the performance of value-based deep reinforcement learning agents in Atari games. The proposed method is based on the Spectral DQN method, which is an extension of Spectral-DQN. The main idea of the paper is to use Spectral Q-values as a reward function to encourage the agent to explore more. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of Atari games and benchmarks."
705,SP:bff215c695b302ce31311f2dd105dace06307cfc,This paper studies the learning dynamics of deep neural networks for image classification tasks. The authors propose a method to regularize the learning rate and the learning-size of a deep network. The method is based on the idea that the representation learned by the network should be sufficient for the task. The main contribution of this paper is to propose a regularization term that encourages the network to learn representations that are sufficient for a given task. 
706,SP:bff215c695b302ce31311f2dd105dace06307cfc,This paper studies the learning dynamics of deep neural networks for image classification tasks. The authors propose a method to regularize the learning rate and the learning-size of a deep network. The method is based on the idea that the representation learned by the network should be sufficient for the task. The main contribution of this paper is to propose a regularization term that encourages the network to learn representations that are sufficient for a given task. 
707,SP:bff215c695b302ce31311f2dd105dace06307cfc,This paper studies the learning dynamics of deep neural networks for image classification tasks. The authors propose a method to regularize the learning rate and the learning-size of a deep network. The method is based on the idea that the representation learned by the network should be sufficient for the task. The main contribution of this paper is to propose a regularization term that encourages the network to learn representations that are sufficient for a given task. 
708,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper studies the variance reduction algorithm SREDA for min-max optimization problems. The authors propose a zeroth-order variant of the algorithm, called ZO-SREDA-Boost, which can be viewed as a variant of SREDa. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of the proposed algorithm. The convergence rate is shown to be a function of the initialization accuracy and the number of iterations. The paper also provides an empirical study of the performance of the new algorithm."
709,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper studies the variance reduction algorithm SREDA for min-max optimization problems. The authors propose a zeroth-order variant of the algorithm, called ZO-SREDA-Boost, which can be viewed as a variant of SREDa. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of the proposed algorithm. The convergence rate is shown to be a function of the initialization accuracy and the number of iterations. The paper also provides an empirical study of the performance of the new algorithm."
710,SP:c175ea892c831c2d0c38aded9b5e86d25b86545c,"This paper studies the variance reduction algorithm SREDA for min-max optimization problems. The authors propose a zeroth-order variant of the algorithm, called ZO-SREDA-Boost, which can be viewed as a variant of SREDa. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of the proposed algorithm. The convergence rate is shown to be a function of the initialization accuracy and the number of iterations. The paper also provides an empirical study of the performance of the new algorithm."
711,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,This paper studies the problem of few-shot object detection. The authors propose a metric learning approach to measure the generalization gap between different object categories. The paper also proposes a novel method to detect novel objects. The proposed method is evaluated on COCO dataset.
712,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,This paper studies the problem of few-shot object detection. The authors propose a metric learning approach to measure the generalization gap between different object categories. The paper also proposes a novel method to detect novel objects. The proposed method is evaluated on COCO dataset.
713,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,This paper studies the problem of few-shot object detection. The authors propose a metric learning approach to measure the generalization gap between different object categories. The paper also proposes a novel method to detect novel objects. The proposed method is evaluated on COCO dataset.
714,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,This paper studies the problem of few-shot object detection. The authors propose a metric learning approach to measure the generalization gap between different object categories. The paper also proposes a novel method to detect novel objects. The proposed method is evaluated on COCO dataset.
715,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a method for 3D implicit surface reconstruction from a single-view image. The proposed method is based on the DGS module, which is an extension of the original Pix3D model. The main contribution of this paper is to add a spatial gradient to the implicit field of the feature map of the image. This spatial gradient is used to train the model to reconstruct the 3D surface of a single view object. The method is evaluated on two datasets: ShapeNet and ScannetV2. The results show that the proposed method outperforms the state-of-the-art."
716,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a method for 3D implicit surface reconstruction from a single-view image. The proposed method is based on the DGS module, which is an extension of the original Pix3D model. The main contribution of this paper is to add a spatial gradient to the implicit field of the feature map of the image. This spatial gradient is used to train the model to reconstruct the 3D surface of a single view object. The method is evaluated on two datasets: ShapeNet and ScannetV2. The results show that the proposed method outperforms the state-of-the-art."
717,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a method for 3D implicit surface reconstruction from a single-view image. The proposed method is based on the DGS module, which is an extension of the original Pix3D model. The main contribution of this paper is to add a spatial gradient to the implicit field of the feature map of the image. This spatial gradient is used to train the model to reconstruct the 3D surface of a single view object. The method is evaluated on two datasets: ShapeNet and ScannetV2. The results show that the proposed method outperforms the state-of-the-art."
718,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"This paper proposes a method for 3D implicit surface reconstruction from a single-view image. The proposed method is based on the DGS module, which is an extension of the original Pix3D model. The main contribution of this paper is to add a spatial gradient to the implicit field of the feature map of the image. This spatial gradient is used to train the model to reconstruct the 3D surface of a single view object. The method is evaluated on two datasets: ShapeNet and ScannetV2. The results show that the proposed method outperforms the state-of-the-art."
719,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper studies the problem of reducing the memory footprint of large models. The authors propose a new training strategy called Pseudo-to-Real, which is a combination of pseudo-training and real training. The main idea is to train a pseudo-model on top of a real model, and then use the pseudo model to train the real model on the real one. The experiments show that the proposed method can significantly reduce the number of GPU resources."
720,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper studies the problem of reducing the memory footprint of large models. The authors propose a new training strategy called Pseudo-to-Real, which is a combination of pseudo-training and real training. The main idea is to train a pseudo-model on top of a real model, and then use the pseudo model to train the real model on the real one. The experiments show that the proposed method can significantly reduce the number of GPU resources."
721,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper studies the problem of reducing the memory footprint of large models. The authors propose a new training strategy called Pseudo-to-Real, which is a combination of pseudo-training and real training. The main idea is to train a pseudo-model on top of a real model, and then use the pseudo model to train the real model on the real one. The experiments show that the proposed method can significantly reduce the number of GPU resources."
722,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"This paper studies the problem of reducing the memory footprint of large models. The authors propose a new training strategy called Pseudo-to-Real, which is a combination of pseudo-training and real training. The main idea is to train a pseudo-model on top of a real model, and then use the pseudo model to train the real model on the real one. The experiments show that the proposed method can significantly reduce the number of GPU resources."
723,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,This paper proposes a dual formulation of the Fenchel duality for maximum likelihood estimation in energy-based models (EBMs). The main idea of the dual formulation is to use the energy of a neural network as a variational approximation of the Gibbs distribution. The authors show that the proposed dual formulation can be applied to both the active regime and the passive regime of EBMs. The main contribution of this paper is to propose a dual algorithm for training EBMs with shallow overparametrized neural network energies. 
724,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,This paper proposes a dual formulation of the Fenchel duality for maximum likelihood estimation in energy-based models (EBMs). The main idea of the dual formulation is to use the energy of a neural network as a variational approximation of the Gibbs distribution. The authors show that the proposed dual formulation can be applied to both the active regime and the passive regime of EBMs. The main contribution of this paper is to propose a dual algorithm for training EBMs with shallow overparametrized neural network energies. 
725,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,This paper proposes a dual formulation of the Fenchel duality for maximum likelihood estimation in energy-based models (EBMs). The main idea of the dual formulation is to use the energy of a neural network as a variational approximation of the Gibbs distribution. The authors show that the proposed dual formulation can be applied to both the active regime and the passive regime of EBMs. The main contribution of this paper is to propose a dual algorithm for training EBMs with shallow overparametrized neural network energies. 
726,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,This paper proposes a dual formulation of the Fenchel duality for maximum likelihood estimation in energy-based models (EBMs). The main idea of the dual formulation is to use the energy of a neural network as a variational approximation of the Gibbs distribution. The authors show that the proposed dual formulation can be applied to both the active regime and the passive regime of EBMs. The main contribution of this paper is to propose a dual algorithm for training EBMs with shallow overparametrized neural network energies. 
727,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies differentially private ERM (DP-ERM) for convex functions. In particular, the authors consider the case where the auxiliary dimension of the convex function is $\mathcal{O}(\sqrt{T})$ and the loss function is $O(\frac{2}{\epsilon}^2)$. The authors prove lower bounds on the logarithmic terms of the loss for the case when the dimension is $\Omega(T)$ and $O(T^{-1/2})$. They also provide lower bounds for the unconstrained case. The authors also provide a theoretical analysis of the one-way marginals of the proposed loss function."
728,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies differentially private ERM (DP-ERM) for convex functions. In particular, the authors consider the case where the auxiliary dimension of the convex function is $\mathcal{O}(\sqrt{T})$ and the loss function is $O(\frac{2}{\epsilon}^2)$. The authors prove lower bounds on the logarithmic terms of the loss for the case when the dimension is $\Omega(T)$ and $O(T^{-1/2})$. They also provide lower bounds for the unconstrained case. The authors also provide a theoretical analysis of the one-way marginals of the proposed loss function."
729,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies differentially private ERM (DP-ERM) for convex functions. In particular, the authors consider the case where the auxiliary dimension of the convex function is $\mathcal{O}(\sqrt{T})$ and the loss function is $O(\frac{2}{\epsilon}^2)$. The authors prove lower bounds on the logarithmic terms of the loss for the case when the dimension is $\Omega(T)$ and $O(T^{-1/2})$. They also provide lower bounds for the unconstrained case. The authors also provide a theoretical analysis of the one-way marginals of the proposed loss function."
730,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"This paper studies differentially private ERM (DP-ERM) for convex functions. In particular, the authors consider the case where the auxiliary dimension of the convex function is $\mathcal{O}(\sqrt{T})$ and the loss function is $O(\frac{2}{\epsilon}^2)$. The authors prove lower bounds on the logarithmic terms of the loss for the case when the dimension is $\Omega(T)$ and $O(T^{-1/2})$. They also provide lower bounds for the unconstrained case. The authors also provide a theoretical analysis of the one-way marginals of the proposed loss function."
731,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The main idea is to use a variational formulation of the objective function to approximate the JKO proximal map, which is a primal-dual optimization problem. The paper also provides a theoretical analysis of the proposed method."
732,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The main idea is to use a variational formulation of the objective function to approximate the JKO proximal map, which is a primal-dual optimization problem. The paper also provides a theoretical analysis of the proposed method."
733,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The main idea is to use a variational formulation of the objective function to approximate the JKO proximal map, which is a primal-dual optimization problem. The paper also provides a theoretical analysis of the proposed method."
734,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a scalable proximal gradient type algorithm for Wasserstein gradient flow. The main idea is to use a variational formulation of the objective function to approximate the JKO proximal map, which is a primal-dual optimization problem. The paper also provides a theoretical analysis of the proposed method."
735,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,This paper proposes a meta-feature-based approach for meta-learning in the context of AutoML. The proposed method is based on the Optimal Transport procedure (OTP). The authors propose to use meta-features to learn the topology of the auto-learned features. The authors show that the proposed method outperforms the existing methods on the OpenML CC-18 benchmark. 
736,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,This paper proposes a meta-feature-based approach for meta-learning in the context of AutoML. The proposed method is based on the Optimal Transport procedure (OTP). The authors propose to use meta-features to learn the topology of the auto-learned features. The authors show that the proposed method outperforms the existing methods on the OpenML CC-18 benchmark. 
737,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,This paper proposes a meta-feature-based approach for meta-learning in the context of AutoML. The proposed method is based on the Optimal Transport procedure (OTP). The authors propose to use meta-features to learn the topology of the auto-learned features. The authors show that the proposed method outperforms the existing methods on the OpenML CC-18 benchmark. 
738,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,This paper proposes a meta-feature-based approach for meta-learning in the context of AutoML. The proposed method is based on the Optimal Transport procedure (OTP). The authors propose to use meta-features to learn the topology of the auto-learned features. The authors show that the proposed method outperforms the existing methods on the OpenML CC-18 benchmark. 
739,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants in federated learning. The main idea is to split the network into base sub-networks, which are trained together, and then use the base network for customization. The experiments show that the proposed method outperforms the baselines in terms of in-situ customization."
740,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants in federated learning. The main idea is to split the network into base sub-networks, which are trained together, and then use the base network for customization. The experiments show that the proposed method outperforms the baselines in terms of in-situ customization."
741,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants in federated learning. The main idea is to split the network into base sub-networks, which are trained together, and then use the base network for customization. The experiments show that the proposed method outperforms the baselines in terms of in-situ customization."
742,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"This paper proposes a split-mix FL strategy for heterogeneous participants in federated learning. The main idea is to split the network into base sub-networks, which are trained together, and then use the base network for customization. The experiments show that the proposed method outperforms the baselines in terms of in-situ customization."
743,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper studies the problem of solving nonconvex-nonconcave minimax problems with weak variational inequality (MVI). The main contribution of this paper is to propose a new algorithm for solving this problem. The main idea is to use an extension of an existing algorithm for the weak MVI, which is based on the extension of a previous algorithm. The authors show that the proposed algorithm is equivalent to the original algorithm in a monotone setting, and that it can be used to train a generative adversarial network with stochastic oracles."
744,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper studies the problem of solving nonconvex-nonconcave minimax problems with weak variational inequality (MVI). The main contribution of this paper is to propose a new algorithm for solving this problem. The main idea is to use an extension of an existing algorithm for the weak MVI, which is based on the extension of a previous algorithm. The authors show that the proposed algorithm is equivalent to the original algorithm in a monotone setting, and that it can be used to train a generative adversarial network with stochastic oracles."
745,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper studies the problem of solving nonconvex-nonconcave minimax problems with weak variational inequality (MVI). The main contribution of this paper is to propose a new algorithm for solving this problem. The main idea is to use an extension of an existing algorithm for the weak MVI, which is based on the extension of a previous algorithm. The authors show that the proposed algorithm is equivalent to the original algorithm in a monotone setting, and that it can be used to train a generative adversarial network with stochastic oracles."
746,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper studies the problem of solving nonconvex-nonconcave minimax problems with weak variational inequality (MVI). The main contribution of this paper is to propose a new algorithm for solving this problem. The main idea is to use an extension of an existing algorithm for the weak MVI, which is based on the extension of a previous algorithm. The authors show that the proposed algorithm is equivalent to the original algorithm in a monotone setting, and that it can be used to train a generative adversarial network with stochastic oracles."
747,SP:af22742091277b726f67e7155b412dd35f29e804,This paper studies the problem of contextual bandits in the context of deep neural networks. The authors propose a learning algorithm for the last linear layer of a deep neural network. The main idea is to use the upper confidence bound (UCB) approach to explore the last layer of the network and then use a shallow exploration layer to explore deeper layers. The paper shows that the proposed algorithm can achieve a regret of $O(1/\sqrt{T})$ with respect to the reward generating function. 
748,SP:af22742091277b726f67e7155b412dd35f29e804,This paper studies the problem of contextual bandits in the context of deep neural networks. The authors propose a learning algorithm for the last linear layer of a deep neural network. The main idea is to use the upper confidence bound (UCB) approach to explore the last layer of the network and then use a shallow exploration layer to explore deeper layers. The paper shows that the proposed algorithm can achieve a regret of $O(1/\sqrt{T})$ with respect to the reward generating function. 
749,SP:af22742091277b726f67e7155b412dd35f29e804,This paper studies the problem of contextual bandits in the context of deep neural networks. The authors propose a learning algorithm for the last linear layer of a deep neural network. The main idea is to use the upper confidence bound (UCB) approach to explore the last layer of the network and then use a shallow exploration layer to explore deeper layers. The paper shows that the proposed algorithm can achieve a regret of $O(1/\sqrt{T})$ with respect to the reward generating function. 
750,SP:af22742091277b726f67e7155b412dd35f29e804,This paper studies the problem of contextual bandits in the context of deep neural networks. The authors propose a learning algorithm for the last linear layer of a deep neural network. The main idea is to use the upper confidence bound (UCB) approach to explore the last layer of the network and then use a shallow exploration layer to explore deeper layers. The paper shows that the proposed algorithm can achieve a regret of $O(1/\sqrt{T})$ with respect to the reward generating function. 
751,SP:a9a2c21110e00f19882d27bef0063c422a15e576,This paper proposes a Shapley-inspired method for action space selection in reinforcement learning. The main idea is to use Monte Carlo simulation to reduce the computational cost of Shapley computations. The method is evaluated on a cloud infrastructure resource tuning case study and shows that it outperforms existing methods.
752,SP:a9a2c21110e00f19882d27bef0063c422a15e576,This paper proposes a Shapley-inspired method for action space selection in reinforcement learning. The main idea is to use Monte Carlo simulation to reduce the computational cost of Shapley computations. The method is evaluated on a cloud infrastructure resource tuning case study and shows that it outperforms existing methods.
753,SP:a9a2c21110e00f19882d27bef0063c422a15e576,This paper proposes a Shapley-inspired method for action space selection in reinforcement learning. The main idea is to use Monte Carlo simulation to reduce the computational cost of Shapley computations. The method is evaluated on a cloud infrastructure resource tuning case study and shows that it outperforms existing methods.
754,SP:a9a2c21110e00f19882d27bef0063c422a15e576,This paper proposes a Shapley-inspired method for action space selection in reinforcement learning. The main idea is to use Monte Carlo simulation to reduce the computational cost of Shapley computations. The method is evaluated on a cloud infrastructure resource tuning case study and shows that it outperforms existing methods.
755,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method to estimate the importance weights for PAC prediction sets. The main idea is to use the confidence intervals of importance weights as a proxy for the confidence of the prediction set. The authors show that the confidence interval can be computed using the average normalized size of the data distribution. The paper also shows that the proposed method can be applied to the PAC constraint problem.
756,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method to estimate the importance weights for PAC prediction sets. The main idea is to use the confidence intervals of importance weights as a proxy for the confidence of the prediction set. The authors show that the confidence interval can be computed using the average normalized size of the data distribution. The paper also shows that the proposed method can be applied to the PAC constraint problem.
757,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method to estimate the importance weights for PAC prediction sets. The main idea is to use the confidence intervals of importance weights as a proxy for the confidence of the prediction set. The authors show that the confidence interval can be computed using the average normalized size of the data distribution. The paper also shows that the proposed method can be applied to the PAC constraint problem.
758,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,This paper proposes a method to estimate the importance weights for PAC prediction sets. The main idea is to use the confidence intervals of importance weights as a proxy for the confidence of the prediction set. The authors show that the confidence interval can be computed using the average normalized size of the data distribution. The paper also shows that the proposed method can be applied to the PAC constraint problem.
759,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,This paper studies the generalization error of iterative SSL algorithms for binary Gaussian mixture models. The authors consider the case where the model parameters and class conditional variances are unknown and the class labels are pseudo-labelled. The main contribution of this paper is to provide a theoretical analysis of the generalizability of SSL algorithms. The paper is well-written and easy to follow. 
760,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,This paper studies the generalization error of iterative SSL algorithms for binary Gaussian mixture models. The authors consider the case where the model parameters and class conditional variances are unknown and the class labels are pseudo-labelled. The main contribution of this paper is to provide a theoretical analysis of the generalizability of SSL algorithms. The paper is well-written and easy to follow. 
761,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,This paper studies the generalization error of iterative SSL algorithms for binary Gaussian mixture models. The authors consider the case where the model parameters and class conditional variances are unknown and the class labels are pseudo-labelled. The main contribution of this paper is to provide a theoretical analysis of the generalizability of SSL algorithms. The paper is well-written and easy to follow. 
762,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,This paper studies the generalization error of iterative SSL algorithms for binary Gaussian mixture models. The authors consider the case where the model parameters and class conditional variances are unknown and the class labels are pseudo-labelled. The main contribution of this paper is to provide a theoretical analysis of the generalizability of SSL algorithms. The paper is well-written and easy to follow. 
763,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method (GPM) for model-free reinforcement learning. The main idea of GPM is to generate a sequence of multi-step trajectories for each step of an exploration step, and then use the generated trajectories to generate an initial plan for the next step of the exploration. The authors show that the proposed method outperforms several baselines on a variety of benchmark environments."
764,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method (GPM) for model-free reinforcement learning. The main idea of GPM is to generate a sequence of multi-step trajectories for each step of an exploration step, and then use the generated trajectories to generate an initial plan for the next step of the exploration. The authors show that the proposed method outperforms several baselines on a variety of benchmark environments."
765,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method (GPM) for model-free reinforcement learning. The main idea of GPM is to generate a sequence of multi-step trajectories for each step of an exploration step, and then use the generated trajectories to generate an initial plan for the next step of the exploration. The authors show that the proposed method outperforms several baselines on a variety of benchmark environments."
766,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a generative planning method (GPM) for model-free reinforcement learning. The main idea of GPM is to generate a sequence of multi-step trajectories for each step of an exploration step, and then use the generated trajectories to generate an initial plan for the next step of the exploration. The authors show that the proposed method outperforms several baselines on a variety of benchmark environments."
767,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper studies the problem of matrix and tensor factorization in deep linear and nonlinear matrix factorization. The authors propose a multi-mode deep matrix-and-tensor factorization method that combines the best of both matrices and tensors. The main contribution of the paper is to provide a generalization error bound for the multi-modal deep matrix and the tensor-based factorization methods. The paper also provides a theoretical analysis of the performance of the proposed method.
768,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper studies the problem of matrix and tensor factorization in deep linear and nonlinear matrix factorization. The authors propose a multi-mode deep matrix-and-tensor factorization method that combines the best of both matrices and tensors. The main contribution of the paper is to provide a generalization error bound for the multi-modal deep matrix and the tensor-based factorization methods. The paper also provides a theoretical analysis of the performance of the proposed method.
769,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper studies the problem of matrix and tensor factorization in deep linear and nonlinear matrix factorization. The authors propose a multi-mode deep matrix-and-tensor factorization method that combines the best of both matrices and tensors. The main contribution of the paper is to provide a generalization error bound for the multi-modal deep matrix and the tensor-based factorization methods. The paper also provides a theoretical analysis of the performance of the proposed method.
770,SP:ce6a93847209a0926ed0be5190378a3f61db1935,This paper studies the problem of matrix and tensor factorization in deep linear and nonlinear matrix factorization. The authors propose a multi-mode deep matrix-and-tensor factorization method that combines the best of both matrices and tensors. The main contribution of the paper is to provide a generalization error bound for the multi-modal deep matrix and the tensor-based factorization methods. The paper also provides a theoretical analysis of the performance of the proposed method.
771,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,This paper proposes a method for learning interpretable representations of structured models. The authors propose to use energy-based training to train an interpreter function that can be used to interpret the output of a structured model. The main contribution of this paper is to propose a method to learn interpretable representation of the input space. The proposed method is evaluated on simulated and real data sets.
772,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,This paper proposes a method for learning interpretable representations of structured models. The authors propose to use energy-based training to train an interpreter function that can be used to interpret the output of a structured model. The main contribution of this paper is to propose a method to learn interpretable representation of the input space. The proposed method is evaluated on simulated and real data sets.
773,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,This paper proposes a method for learning interpretable representations of structured models. The authors propose to use energy-based training to train an interpreter function that can be used to interpret the output of a structured model. The main contribution of this paper is to propose a method to learn interpretable representation of the input space. The proposed method is evaluated on simulated and real data sets.
774,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,This paper proposes a method for learning interpretable representations of structured models. The authors propose to use energy-based training to train an interpreter function that can be used to interpret the output of a structured model. The main contribution of this paper is to propose a method to learn interpretable representation of the input space. The proposed method is evaluated on simulated and real data sets.
775,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a new method for policy optimization in deep reinforcement learning (DRL) based on the cumulative distribution function (CDF). The main idea is to estimate the policy gradient as a function of the cumulative CDF, and then use this estimate to sample from the CDF. The authors show that the proposed method is able to achieve better performance than PPO and Proximal Policy Optimization (PPO) in the OpenAI Safety Gym environments. "
776,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a new method for policy optimization in deep reinforcement learning (DRL) based on the cumulative distribution function (CDF). The main idea is to estimate the policy gradient as a function of the cumulative CDF, and then use this estimate to sample from the CDF. The authors show that the proposed method is able to achieve better performance than PPO and Proximal Policy Optimization (PPO) in the OpenAI Safety Gym environments. "
777,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a new method for policy optimization in deep reinforcement learning (DRL) based on the cumulative distribution function (CDF). The main idea is to estimate the policy gradient as a function of the cumulative CDF, and then use this estimate to sample from the CDF. The authors show that the proposed method is able to achieve better performance than PPO and Proximal Policy Optimization (PPO) in the OpenAI Safety Gym environments. "
778,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"This paper proposes a new method for policy optimization in deep reinforcement learning (DRL) based on the cumulative distribution function (CDF). The main idea is to estimate the policy gradient as a function of the cumulative CDF, and then use this estimate to sample from the CDF. The authors show that the proposed method is able to achieve better performance than PPO and Proximal Policy Optimization (PPO) in the OpenAI Safety Gym environments. "
779,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease simulations. The proposed method is based on a neural process model and a deep sequence model, where the neural process is modeled as a spatiotemporal neural process, and the sequence model is used as a surrogate model for the simulator dynamics. The authors show that the proposed method can achieve better sample complexity than random sampling in terms of the number of simulated scenarios and the sample complexity of the acquisition function. "
780,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease simulations. The proposed method is based on a neural process model and a deep sequence model, where the neural process is modeled as a spatiotemporal neural process, and the sequence model is used as a surrogate model for the simulator dynamics. The authors show that the proposed method can achieve better sample complexity than random sampling in terms of the number of simulated scenarios and the sample complexity of the acquisition function. "
781,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease simulations. The proposed method is based on a neural process model and a deep sequence model, where the neural process is modeled as a spatiotemporal neural process, and the sequence model is used as a surrogate model for the simulator dynamics. The authors show that the proposed method can achieve better sample complexity than random sampling in terms of the number of simulated scenarios and the sample complexity of the acquisition function. "
782,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"This paper proposes a Bayesian active learning method for simulating infectious disease simulations. The proposed method is based on a neural process model and a deep sequence model, where the neural process is modeled as a spatiotemporal neural process, and the sequence model is used as a surrogate model for the simulator dynamics. The authors show that the proposed method can achieve better sample complexity than random sampling in terms of the number of simulated scenarios and the sample complexity of the acquisition function. "
783,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper proposes a method for training differentially private (DP) models of text. The main idea is to train the model on a small number of training examples and then fine-tune the parameters of the model using gradient descent. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of NLP tasks. 
784,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper proposes a method for training differentially private (DP) models of text. The main idea is to train the model on a small number of training examples and then fine-tune the parameters of the model using gradient descent. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of NLP tasks. 
785,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper proposes a method for training differentially private (DP) models of text. The main idea is to train the model on a small number of training examples and then fine-tune the parameters of the model using gradient descent. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of NLP tasks. 
786,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,This paper proposes a method for training differentially private (DP) models of text. The main idea is to train the model on a small number of training examples and then fine-tune the parameters of the model using gradient descent. The authors show that the proposed method is able to achieve state-of-the-art performance on a variety of NLP tasks. 
787,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to use a conditional policy gradient method to learn a joint policy for both the design and the control of the agent. The policy gradient is based on a graph-based policy, which is then used to learn the joint policy. The authors show that the proposed method is able to converge faster than the state-of-the-art methods."
788,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to use a conditional policy gradient method to learn a joint policy for both the design and the control of the agent. The policy gradient is based on a graph-based policy, which is then used to learn the joint policy. The authors show that the proposed method is able to converge faster than the state-of-the-art methods."
789,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to use a conditional policy gradient method to learn a joint policy for both the design and the control of the agent. The policy gradient is based on a graph-based policy, which is then used to learn the joint policy. The authors show that the proposed method is able to converge faster than the state-of-the-art methods."
790,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a method for joint optimization of agent design and control. The main idea is to use a conditional policy gradient method to learn a joint policy for both the design and the control of the agent. The policy gradient is based on a graph-based policy, which is then used to learn the joint policy. The authors show that the proposed method is able to converge faster than the state-of-the-art methods."
791,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a split MLP architecture for implicit neural representation learning. The main idea is to use coordinate-based MLPs for the initial layers of the network, and then use CoordX for the intermediate layers. The proposed method is evaluated on a variety of implicit representation learning tasks, including 3D object rendering, view synthesis, and image classification. The results show that the proposed method outperforms the state-of-the-art."
792,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a split MLP architecture for implicit neural representation learning. The main idea is to use coordinate-based MLPs for the initial layers of the network, and then use CoordX for the intermediate layers. The proposed method is evaluated on a variety of implicit representation learning tasks, including 3D object rendering, view synthesis, and image classification. The results show that the proposed method outperforms the state-of-the-art."
793,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a split MLP architecture for implicit neural representation learning. The main idea is to use coordinate-based MLPs for the initial layers of the network, and then use CoordX for the intermediate layers. The proposed method is evaluated on a variety of implicit representation learning tasks, including 3D object rendering, view synthesis, and image classification. The results show that the proposed method outperforms the state-of-the-art."
794,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"This paper proposes a split MLP architecture for implicit neural representation learning. The main idea is to use coordinate-based MLPs for the initial layers of the network, and then use CoordX for the intermediate layers. The proposed method is evaluated on a variety of implicit representation learning tasks, including 3D object rendering, view synthesis, and image classification. The results show that the proposed method outperforms the state-of-the-art."
795,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method for learning 3D representations of visual scenes. The proposed method is based on a differentiable rendering process, where the rendered scene is represented as a neural radiance field, and a reconstruction loss is used to reconstruct the 2D views of the scene. The method is evaluated on the CATER dataset, where it is shown to outperform the state-of-the-art methods."
796,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method for learning 3D representations of visual scenes. The proposed method is based on a differentiable rendering process, where the rendered scene is represented as a neural radiance field, and a reconstruction loss is used to reconstruct the 2D views of the scene. The method is evaluated on the CATER dataset, where it is shown to outperform the state-of-the-art methods."
797,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method for learning 3D representations of visual scenes. The proposed method is based on a differentiable rendering process, where the rendered scene is represented as a neural radiance field, and a reconstruction loss is used to reconstruct the 2D views of the scene. The method is evaluated on the CATER dataset, where it is shown to outperform the state-of-the-art methods."
798,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a method for learning 3D representations of visual scenes. The proposed method is based on a differentiable rendering process, where the rendered scene is represented as a neural radiance field, and a reconstruction loss is used to reconstruct the 2D views of the scene. The method is evaluated on the CATER dataset, where it is shown to outperform the state-of-the-art methods."
799,SP:05c61145f3fc9486728aca19c4543065fe04e99c,This paper proposes a feature attribution framework for graph neural networks (GNNs). The main idea is to use a surrogate variable to estimate the importance of each node in a subgraph. This surrogate variable is then used to evaluate the influence of a node in the subgraph on the prediction of the GNN. The proposed method is evaluated on three datasets and compared to several baselines.
800,SP:05c61145f3fc9486728aca19c4543065fe04e99c,This paper proposes a feature attribution framework for graph neural networks (GNNs). The main idea is to use a surrogate variable to estimate the importance of each node in a subgraph. This surrogate variable is then used to evaluate the influence of a node in the subgraph on the prediction of the GNN. The proposed method is evaluated on three datasets and compared to several baselines.
801,SP:05c61145f3fc9486728aca19c4543065fe04e99c,This paper proposes a feature attribution framework for graph neural networks (GNNs). The main idea is to use a surrogate variable to estimate the importance of each node in a subgraph. This surrogate variable is then used to evaluate the influence of a node in the subgraph on the prediction of the GNN. The proposed method is evaluated on three datasets and compared to several baselines.
802,SP:05c61145f3fc9486728aca19c4543065fe04e99c,This paper proposes a feature attribution framework for graph neural networks (GNNs). The main idea is to use a surrogate variable to estimate the importance of each node in a subgraph. This surrogate variable is then used to evaluate the influence of a node in the subgraph on the prediction of the GNN. The proposed method is evaluated on three datasets and compared to several baselines.
803,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper studies the problem of active learning in few-shot settings. The authors propose a method for disambiguating examples in the presence of spurious correlations and latent minority groups. The method is based on uncertainty sampling, and the authors show that the proposed method outperforms random sampling and uncertainty sampling with pretrained models. They also show that active learning can be used to improve the performance of the pretrained model."
804,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper studies the problem of active learning in few-shot settings. The authors propose a method for disambiguating examples in the presence of spurious correlations and latent minority groups. The method is based on uncertainty sampling, and the authors show that the proposed method outperforms random sampling and uncertainty sampling with pretrained models. They also show that active learning can be used to improve the performance of the pretrained model."
805,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper studies the problem of active learning in few-shot settings. The authors propose a method for disambiguating examples in the presence of spurious correlations and latent minority groups. The method is based on uncertainty sampling, and the authors show that the proposed method outperforms random sampling and uncertainty sampling with pretrained models. They also show that active learning can be used to improve the performance of the pretrained model."
806,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,"This paper studies the problem of active learning in few-shot settings. The authors propose a method for disambiguating examples in the presence of spurious correlations and latent minority groups. The method is based on uncertainty sampling, and the authors show that the proposed method outperforms random sampling and uncertainty sampling with pretrained models. They also show that active learning can be used to improve the performance of the pretrained model."
807,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder. The pre-training strategy is a deleted sub-tree reconstruction, which is used to train the model. The model is evaluated on the Wild Java benchmark and compared with BART and CodeBERT. The results show that the proposed model outperforms the baselines."
808,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder. The pre-training strategy is a deleted sub-tree reconstruction, which is used to train the model. The model is evaluated on the Wild Java benchmark and compared with BART and CodeBERT. The results show that the proposed model outperforms the baselines."
809,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder. The pre-training strategy is a deleted sub-tree reconstruction, which is used to train the model. The model is evaluated on the Wild Java benchmark and compared with BART and CodeBERT. The results show that the proposed model outperforms the baselines."
810,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"This paper proposes a pre-trained graph edit model for automatically detecting and fixing bugs in Java programs. The proposed model is based on a multi-head graph encoder and an autoregressive tree decoder. The pre-training strategy is a deleted sub-tree reconstruction, which is used to train the model. The model is evaluated on the Wild Java benchmark and compared with BART and CodeBERT. The results show that the proposed model outperforms the baselines."
811,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper studies the problem of federated adversarial training (FAT) in the context of data privacy and governance. The authors propose a new algorithm called α-WFAT, which is based on the inner-maximization of Adversarial Training (AT). The authors show that under certain conditions, the convergence of the proposed algorithm can be improved to a lower bound of $O(1/\sqrt{T})$, where $T$ is the number of clients in the federated learning setting. The main contribution of this paper is to show that the convergence rate of the α-weighted WFAT algorithm is better than that of the original FAT algorithm."
812,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper studies the problem of federated adversarial training (FAT) in the context of data privacy and governance. The authors propose a new algorithm called α-WFAT, which is based on the inner-maximization of Adversarial Training (AT). The authors show that under certain conditions, the convergence of the proposed algorithm can be improved to a lower bound of $O(1/\sqrt{T})$, where $T$ is the number of clients in the federated learning setting. The main contribution of this paper is to show that the convergence rate of the α-weighted WFAT algorithm is better than that of the original FAT algorithm."
813,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper studies the problem of federated adversarial training (FAT) in the context of data privacy and governance. The authors propose a new algorithm called α-WFAT, which is based on the inner-maximization of Adversarial Training (AT). The authors show that under certain conditions, the convergence of the proposed algorithm can be improved to a lower bound of $O(1/\sqrt{T})$, where $T$ is the number of clients in the federated learning setting. The main contribution of this paper is to show that the convergence rate of the α-weighted WFAT algorithm is better than that of the original FAT algorithm."
814,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper studies the problem of federated adversarial training (FAT) in the context of data privacy and governance. The authors propose a new algorithm called α-WFAT, which is based on the inner-maximization of Adversarial Training (AT). The authors show that under certain conditions, the convergence of the proposed algorithm can be improved to a lower bound of $O(1/\sqrt{T})$, where $T$ is the number of clients in the federated learning setting. The main contribution of this paper is to show that the convergence rate of the α-weighted WFAT algorithm is better than that of the original FAT algorithm."
815,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a new method for continual semi-supervised learning (LSL). The proposed method, called Mako, is based on data programming. The main idea of the proposed method is to train a model on unlabeled data, and then use the learned model to train on the labeled data. The method is evaluated on CIFAR-10, Cifar-100, and MNIST."
816,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a new method for continual semi-supervised learning (LSL). The proposed method, called Mako, is based on data programming. The main idea of the proposed method is to train a model on unlabeled data, and then use the learned model to train on the labeled data. The method is evaluated on CIFAR-10, Cifar-100, and MNIST."
817,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a new method for continual semi-supervised learning (LSL). The proposed method, called Mako, is based on data programming. The main idea of the proposed method is to train a model on unlabeled data, and then use the learned model to train on the labeled data. The method is evaluated on CIFAR-10, Cifar-100, and MNIST."
818,SP:ff3c787512035e2af20778d53586752852196be9,"This paper proposes a new method for continual semi-supervised learning (LSL). The proposed method, called Mako, is based on data programming. The main idea of the proposed method is to train a model on unlabeled data, and then use the learned model to train on the labeled data. The method is evaluated on CIFAR-10, Cifar-100, and MNIST."
819,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes a new method to detect adversarial examples in gradient-based adversarial attacks. The proposed method is based on the idea that the gradients of the adversarial example should be orthogonal to the original gradients. The authors show that the proposed method can be used in combination with other attack techniques, such as Orthogonal Projected Gradient Descent (OGD) and Selective Projected Descent, to improve the detection rate of the attack. The method is evaluated on CIFAR-10 and ImageNet."
820,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes a new method to detect adversarial examples in gradient-based adversarial attacks. The proposed method is based on the idea that the gradients of the adversarial example should be orthogonal to the original gradients. The authors show that the proposed method can be used in combination with other attack techniques, such as Orthogonal Projected Gradient Descent (OGD) and Selective Projected Descent, to improve the detection rate of the attack. The method is evaluated on CIFAR-10 and ImageNet."
821,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes a new method to detect adversarial examples in gradient-based adversarial attacks. The proposed method is based on the idea that the gradients of the adversarial example should be orthogonal to the original gradients. The authors show that the proposed method can be used in combination with other attack techniques, such as Orthogonal Projected Gradient Descent (OGD) and Selective Projected Descent, to improve the detection rate of the attack. The method is evaluated on CIFAR-10 and ImageNet."
822,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes a new method to detect adversarial examples in gradient-based adversarial attacks. The proposed method is based on the idea that the gradients of the adversarial example should be orthogonal to the original gradients. The authors show that the proposed method can be used in combination with other attack techniques, such as Orthogonal Projected Gradient Descent (OGD) and Selective Projected Descent, to improve the detection rate of the attack. The method is evaluated on CIFAR-10 and ImageNet."
823,SP:5eef907024017849303477eed92f317438c87a69,"This paper studies the problem of estimating the Shapley value of an energy-based model in cooperative games. The authors propose a mean-field variational inference approach to estimate Shapley and Banzhaf values based on the maximum entropy principle. The proposed approach is based on a one-step fixed point iteration of the ELBO objective, where the value is estimated using a mean field variational estimator. The paper also provides a theoretical analysis of the Variational Index, which is a variant of Shapley-Banzhaf value. The experiments show that the proposed approach outperforms the baselines on both synthetic and real-world problems."
824,SP:5eef907024017849303477eed92f317438c87a69,"This paper studies the problem of estimating the Shapley value of an energy-based model in cooperative games. The authors propose a mean-field variational inference approach to estimate Shapley and Banzhaf values based on the maximum entropy principle. The proposed approach is based on a one-step fixed point iteration of the ELBO objective, where the value is estimated using a mean field variational estimator. The paper also provides a theoretical analysis of the Variational Index, which is a variant of Shapley-Banzhaf value. The experiments show that the proposed approach outperforms the baselines on both synthetic and real-world problems."
825,SP:5eef907024017849303477eed92f317438c87a69,"This paper studies the problem of estimating the Shapley value of an energy-based model in cooperative games. The authors propose a mean-field variational inference approach to estimate Shapley and Banzhaf values based on the maximum entropy principle. The proposed approach is based on a one-step fixed point iteration of the ELBO objective, where the value is estimated using a mean field variational estimator. The paper also provides a theoretical analysis of the Variational Index, which is a variant of Shapley-Banzhaf value. The experiments show that the proposed approach outperforms the baselines on both synthetic and real-world problems."
826,SP:5eef907024017849303477eed92f317438c87a69,"This paper studies the problem of estimating the Shapley value of an energy-based model in cooperative games. The authors propose a mean-field variational inference approach to estimate Shapley and Banzhaf values based on the maximum entropy principle. The proposed approach is based on a one-step fixed point iteration of the ELBO objective, where the value is estimated using a mean field variational estimator. The paper also provides a theoretical analysis of the Variational Index, which is a variant of Shapley-Banzhaf value. The experiments show that the proposed approach outperforms the baselines on both synthetic and real-world problems."
827,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty in the presence of intrinsic uncertainty. The method is based on the idea that epistemic uncertainties are intrinsic to the model variance, and that the intrinsic uncertainty is a measure of intrinsic unpredictability. The authors propose a method called Direct Epistemic Uncertainty Prediction (DEUP) to estimate intrinsic uncertainty, and show that DEUP can be used for estimating uncertainty in a variety of settings, including active learning, reinforcement learning, and sequential model optimization. DEUP is evaluated on image classification and reinforcement learning tasks, where it is shown to outperform existing methods."
828,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty in the presence of intrinsic uncertainty. The method is based on the idea that epistemic uncertainties are intrinsic to the model variance, and that the intrinsic uncertainty is a measure of intrinsic unpredictability. The authors propose a method called Direct Epistemic Uncertainty Prediction (DEUP) to estimate intrinsic uncertainty, and show that DEUP can be used for estimating uncertainty in a variety of settings, including active learning, reinforcement learning, and sequential model optimization. DEUP is evaluated on image classification and reinforcement learning tasks, where it is shown to outperform existing methods."
829,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty in the presence of intrinsic uncertainty. The method is based on the idea that epistemic uncertainties are intrinsic to the model variance, and that the intrinsic uncertainty is a measure of intrinsic unpredictability. The authors propose a method called Direct Epistemic Uncertainty Prediction (DEUP) to estimate intrinsic uncertainty, and show that DEUP can be used for estimating uncertainty in a variety of settings, including active learning, reinforcement learning, and sequential model optimization. DEUP is evaluated on image classification and reinforcement learning tasks, where it is shown to outperform existing methods."
830,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method for estimating epistemic uncertainty in the presence of intrinsic uncertainty. The method is based on the idea that epistemic uncertainties are intrinsic to the model variance, and that the intrinsic uncertainty is a measure of intrinsic unpredictability. The authors propose a method called Direct Epistemic Uncertainty Prediction (DEUP) to estimate intrinsic uncertainty, and show that DEUP can be used for estimating uncertainty in a variety of settings, including active learning, reinforcement learning, and sequential model optimization. DEUP is evaluated on image classification and reinforcement learning tasks, where it is shown to outperform existing methods."
831,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,This paper proposes a novel block Givens coordinate descent algorithm for approximate nearest neighbor (ANN) search. The main contribution of this paper is the use of a special orthogonal group SO(n) for the inner product computation. The authors show that the proposed algorithm can be used to compute the rotation matrix of the product quantization matrix. The paper also provides a theoretical analysis of the convergence rate of the proposed method.
832,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,This paper proposes a novel block Givens coordinate descent algorithm for approximate nearest neighbor (ANN) search. The main contribution of this paper is the use of a special orthogonal group SO(n) for the inner product computation. The authors show that the proposed algorithm can be used to compute the rotation matrix of the product quantization matrix. The paper also provides a theoretical analysis of the convergence rate of the proposed method.
833,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,This paper proposes a novel block Givens coordinate descent algorithm for approximate nearest neighbor (ANN) search. The main contribution of this paper is the use of a special orthogonal group SO(n) for the inner product computation. The authors show that the proposed algorithm can be used to compute the rotation matrix of the product quantization matrix. The paper also provides a theoretical analysis of the convergence rate of the proposed method.
834,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,This paper proposes a novel block Givens coordinate descent algorithm for approximate nearest neighbor (ANN) search. The main contribution of this paper is the use of a special orthogonal group SO(n) for the inner product computation. The authors show that the proposed algorithm can be used to compute the rotation matrix of the product quantization matrix. The paper also provides a theoretical analysis of the convergence rate of the proposed method.
835,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework for learning visual analogies using Raven's Progressive Matrices (RPM). The main contribution of the paper is to propose a neural structure mapping (NSM) approach for visual analogical reasoning. The main idea is to use a multi-task visual relationship encoder and a neural module net-based analogy inference engine. The proposed method is evaluated on the Raven’s Progressive Matrix, which is a visual reasoning test of fluid intelligence. The experiments show that the proposed method outperforms the baselines."
836,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework for learning visual analogies using Raven's Progressive Matrices (RPM). The main contribution of the paper is to propose a neural structure mapping (NSM) approach for visual analogical reasoning. The main idea is to use a multi-task visual relationship encoder and a neural module net-based analogy inference engine. The proposed method is evaluated on the Raven’s Progressive Matrix, which is a visual reasoning test of fluid intelligence. The experiments show that the proposed method outperforms the baselines."
837,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework for learning visual analogies using Raven's Progressive Matrices (RPM). The main contribution of the paper is to propose a neural structure mapping (NSM) approach for visual analogical reasoning. The main idea is to use a multi-task visual relationship encoder and a neural module net-based analogy inference engine. The proposed method is evaluated on the Raven’s Progressive Matrix, which is a visual reasoning test of fluid intelligence. The experiments show that the proposed method outperforms the baselines."
838,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a two-stage neural framework for learning visual analogies using Raven's Progressive Matrices (RPM). The main contribution of the paper is to propose a neural structure mapping (NSM) approach for visual analogical reasoning. The main idea is to use a multi-task visual relationship encoder and a neural module net-based analogy inference engine. The proposed method is evaluated on the Raven’s Progressive Matrix, which is a visual reasoning test of fluid intelligence. The experiments show that the proposed method outperforms the baselines."
839,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method is based on a contrastive loss and a context network. The encoder network is trained to extract features from the reverse-complement of genomic sequences, and the context network is used to extract semantic representations from the latent space. Experiments are conducted on two datasets and show that the proposed method outperforms the state-of-the-art."
840,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method is based on a contrastive loss and a context network. The encoder network is trained to extract features from the reverse-complement of genomic sequences, and the context network is used to extract semantic representations from the latent space. Experiments are conducted on two datasets and show that the proposed method outperforms the state-of-the-art."
841,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method is based on a contrastive loss and a context network. The encoder network is trained to extract features from the reverse-complement of genomic sequences, and the context network is used to extract semantic representations from the latent space. Experiments are conducted on two datasets and show that the proposed method outperforms the state-of-the-art."
842,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper proposes a self-supervised method for nucleotide genome representation learning. The proposed method is based on a contrastive loss and a context network. The encoder network is trained to extract features from the reverse-complement of genomic sequences, and the context network is used to extract semantic representations from the latent space. Experiments are conducted on two datasets and show that the proposed method outperforms the state-of-the-art."
843,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,This paper proposes a steerable convolutional neural network for point clouds. The proposed method is based on a conformal embedding of Euclidean space with a 3D steerability constraint. The authors propose to use spherical filter banks to learn invariant class predictions for known point sets and unknown orientations for unknown point sets. Experiments on synthetic point set and real-world 3D skeleton data show the effectiveness of the proposed method.
844,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,This paper proposes a steerable convolutional neural network for point clouds. The proposed method is based on a conformal embedding of Euclidean space with a 3D steerability constraint. The authors propose to use spherical filter banks to learn invariant class predictions for known point sets and unknown orientations for unknown point sets. Experiments on synthetic point set and real-world 3D skeleton data show the effectiveness of the proposed method.
845,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,This paper proposes a steerable convolutional neural network for point clouds. The proposed method is based on a conformal embedding of Euclidean space with a 3D steerability constraint. The authors propose to use spherical filter banks to learn invariant class predictions for known point sets and unknown orientations for unknown point sets. Experiments on synthetic point set and real-world 3D skeleton data show the effectiveness of the proposed method.
846,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,This paper proposes a steerable convolutional neural network for point clouds. The proposed method is based on a conformal embedding of Euclidean space with a 3D steerability constraint. The authors propose to use spherical filter banks to learn invariant class predictions for known point sets and unknown orientations for unknown point sets. Experiments on synthetic point set and real-world 3D skeleton data show the effectiveness of the proposed method.
847,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the problem of continual learning of pre-trained language models (PLMs) in the context of NLP tasks. The authors propose a new continual learning method, called Continual Learning (CL), which aims to improve the performance of PLMs on a variety of tasks in a continual learning setting. The main contribution of the paper is to propose a representation probing analysis of the PLMs’ performance on different tasks. They show that PLMs are more susceptible to forgetting than other continual learning methods. They also show that CL methods are more sensitive to layer-wise and task-wise changes in PLMs."
848,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the problem of continual learning of pre-trained language models (PLMs) in the context of NLP tasks. The authors propose a new continual learning method, called Continual Learning (CL), which aims to improve the performance of PLMs on a variety of tasks in a continual learning setting. The main contribution of the paper is to propose a representation probing analysis of the PLMs’ performance on different tasks. They show that PLMs are more susceptible to forgetting than other continual learning methods. They also show that CL methods are more sensitive to layer-wise and task-wise changes in PLMs."
849,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the problem of continual learning of pre-trained language models (PLMs) in the context of NLP tasks. The authors propose a new continual learning method, called Continual Learning (CL), which aims to improve the performance of PLMs on a variety of tasks in a continual learning setting. The main contribution of the paper is to propose a representation probing analysis of the PLMs’ performance on different tasks. They show that PLMs are more susceptible to forgetting than other continual learning methods. They also show that CL methods are more sensitive to layer-wise and task-wise changes in PLMs."
850,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"This paper studies the problem of continual learning of pre-trained language models (PLMs) in the context of NLP tasks. The authors propose a new continual learning method, called Continual Learning (CL), which aims to improve the performance of PLMs on a variety of tasks in a continual learning setting. The main contribution of the paper is to propose a representation probing analysis of the PLMs’ performance on different tasks. They show that PLMs are more susceptible to forgetting than other continual learning methods. They also show that CL methods are more sensitive to layer-wise and task-wise changes in PLMs."
851,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper studies the problem of model poisoning attacks in multi-party federated learning. The authors propose TESSERACT, a defense against directed deviation attacks. The proposed defense is based on the idea of reputation scores, which can be used to evaluate the quality of a model. The defense is evaluated on three Byzantine-reinforced federated algorithms: FABA, Bulyan, and FoolsGold. The results show that the proposed defense outperforms the baselines."
852,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper studies the problem of model poisoning attacks in multi-party federated learning. The authors propose TESSERACT, a defense against directed deviation attacks. The proposed defense is based on the idea of reputation scores, which can be used to evaluate the quality of a model. The defense is evaluated on three Byzantine-reinforced federated algorithms: FABA, Bulyan, and FoolsGold. The results show that the proposed defense outperforms the baselines."
853,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper studies the problem of model poisoning attacks in multi-party federated learning. The authors propose TESSERACT, a defense against directed deviation attacks. The proposed defense is based on the idea of reputation scores, which can be used to evaluate the quality of a model. The defense is evaluated on three Byzantine-reinforced federated algorithms: FABA, Bulyan, and FoolsGold. The results show that the proposed defense outperforms the baselines."
854,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"This paper studies the problem of model poisoning attacks in multi-party federated learning. The authors propose TESSERACT, a defense against directed deviation attacks. The proposed defense is based on the idea of reputation scores, which can be used to evaluate the quality of a model. The defense is evaluated on three Byzantine-reinforced federated algorithms: FABA, Bulyan, and FoolsGold. The results show that the proposed defense outperforms the baselines."
855,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper studies the problem of estimating the average treatment effect functional of a continuous treatment. The authors propose a neural net-based estimator of the Riesz function, which is a combination of Neural Nets and Random Forests. The main contribution of this paper is to derive a correction term for the neural net estimator, which can be used as a regularization term in a plug-in estimator. The paper also proposes a multi-tasking Neural Net debiasing method, which combines the Neural Nets with the Random Forest method. Experiments are conducted on synthetic data of gasoline price changes and a semi-synthetic dataset of gasoline demand."
856,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper studies the problem of estimating the average treatment effect functional of a continuous treatment. The authors propose a neural net-based estimator of the Riesz function, which is a combination of Neural Nets and Random Forests. The main contribution of this paper is to derive a correction term for the neural net estimator, which can be used as a regularization term in a plug-in estimator. The paper also proposes a multi-tasking Neural Net debiasing method, which combines the Neural Nets with the Random Forest method. Experiments are conducted on synthetic data of gasoline price changes and a semi-synthetic dataset of gasoline demand."
857,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper studies the problem of estimating the average treatment effect functional of a continuous treatment. The authors propose a neural net-based estimator of the Riesz function, which is a combination of Neural Nets and Random Forests. The main contribution of this paper is to derive a correction term for the neural net estimator, which can be used as a regularization term in a plug-in estimator. The paper also proposes a multi-tasking Neural Net debiasing method, which combines the Neural Nets with the Random Forest method. Experiments are conducted on synthetic data of gasoline price changes and a semi-synthetic dataset of gasoline demand."
858,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"This paper studies the problem of estimating the average treatment effect functional of a continuous treatment. The authors propose a neural net-based estimator of the Riesz function, which is a combination of Neural Nets and Random Forests. The main contribution of this paper is to derive a correction term for the neural net estimator, which can be used as a regularization term in a plug-in estimator. The paper also proposes a multi-tasking Neural Net debiasing method, which combines the Neural Nets with the Random Forest method. Experiments are conducted on synthetic data of gasoline price changes and a semi-synthetic dataset of gasoline demand."
859,SP:96e1da163020441f9724985ae15674233e0cfe0d,This paper studies the problem of reinforcement learning in the multi-agent reinforcement learning setting. The authors consider the setting where there are multiple agents and each agent receives a different reward. The goal is to maximize the average reward of the agents. The main contribution of this paper is to provide a sample complexity bound for the single-agent actor-critic algorithm. The sample complexity of the proposed algorithm is shown to be polynomial in the number of agents.
860,SP:96e1da163020441f9724985ae15674233e0cfe0d,This paper studies the problem of reinforcement learning in the multi-agent reinforcement learning setting. The authors consider the setting where there are multiple agents and each agent receives a different reward. The goal is to maximize the average reward of the agents. The main contribution of this paper is to provide a sample complexity bound for the single-agent actor-critic algorithm. The sample complexity of the proposed algorithm is shown to be polynomial in the number of agents.
861,SP:96e1da163020441f9724985ae15674233e0cfe0d,This paper studies the problem of reinforcement learning in the multi-agent reinforcement learning setting. The authors consider the setting where there are multiple agents and each agent receives a different reward. The goal is to maximize the average reward of the agents. The main contribution of this paper is to provide a sample complexity bound for the single-agent actor-critic algorithm. The sample complexity of the proposed algorithm is shown to be polynomial in the number of agents.
862,SP:96e1da163020441f9724985ae15674233e0cfe0d,This paper studies the problem of reinforcement learning in the multi-agent reinforcement learning setting. The authors consider the setting where there are multiple agents and each agent receives a different reward. The goal is to maximize the average reward of the agents. The main contribution of this paper is to provide a sample complexity bound for the single-agent actor-critic algorithm. The sample complexity of the proposed algorithm is shown to be polynomial in the number of agents.
863,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper studies the problem of learning class-separated representations for self-supervised learning. The main contribution of this paper is to provide a conditional independence guarantee for contrastive learning. In particular, the authors show that the conditional independence assumption is sufficient to guarantee that the representation of a class is independent of the class of the target task. The authors also provide a theoretical analysis of the proposed method. The experimental results on synthetic and real-world datasets demonstrate the effectiveness of the method."
864,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper studies the problem of learning class-separated representations for self-supervised learning. The main contribution of this paper is to provide a conditional independence guarantee for contrastive learning. In particular, the authors show that the conditional independence assumption is sufficient to guarantee that the representation of a class is independent of the class of the target task. The authors also provide a theoretical analysis of the proposed method. The experimental results on synthetic and real-world datasets demonstrate the effectiveness of the method."
865,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper studies the problem of learning class-separated representations for self-supervised learning. The main contribution of this paper is to provide a conditional independence guarantee for contrastive learning. In particular, the authors show that the conditional independence assumption is sufficient to guarantee that the representation of a class is independent of the class of the target task. The authors also provide a theoretical analysis of the proposed method. The experimental results on synthetic and real-world datasets demonstrate the effectiveness of the method."
866,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"This paper studies the problem of learning class-separated representations for self-supervised learning. The main contribution of this paper is to provide a conditional independence guarantee for contrastive learning. In particular, the authors show that the conditional independence assumption is sufficient to guarantee that the representation of a class is independent of the class of the target task. The authors also provide a theoretical analysis of the proposed method. The experimental results on synthetic and real-world datasets demonstrate the effectiveness of the method."
867,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a generative framework for single-task speech restoration (SSR). The proposed method, VoiceFixer1, consists of two stages: a synthesis stage and an analysis stage. The synthesis stage is based on a neural vocoder, while the analysis stage uses a ResUNet. The experiments show that the proposed method can achieve state-of-the-art results on low-resolution, clipping, room reverberation, and old movies."
868,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a generative framework for single-task speech restoration (SSR). The proposed method, VoiceFixer1, consists of two stages: a synthesis stage and an analysis stage. The synthesis stage is based on a neural vocoder, while the analysis stage uses a ResUNet. The experiments show that the proposed method can achieve state-of-the-art results on low-resolution, clipping, room reverberation, and old movies."
869,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a generative framework for single-task speech restoration (SSR). The proposed method, VoiceFixer1, consists of two stages: a synthesis stage and an analysis stage. The synthesis stage is based on a neural vocoder, while the analysis stage uses a ResUNet. The experiments show that the proposed method can achieve state-of-the-art results on low-resolution, clipping, room reverberation, and old movies."
870,SP:b491314336c503b276e34e410cf461cb81294890,"This paper proposes a generative framework for single-task speech restoration (SSR). The proposed method, VoiceFixer1, consists of two stages: a synthesis stage and an analysis stage. The synthesis stage is based on a neural vocoder, while the analysis stage uses a ResUNet. The experiments show that the proposed method can achieve state-of-the-art results on low-resolution, clipping, room reverberation, and old movies."
871,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a new method for multivariate time series forecasting. The proposed method is based on residual tensor networks (MVSRNets). The main idea is to use a low-rank approximation of a tensor network to represent the latent space of the time series as a series variable encoder, and then use a skip-connection layer to share information between the latent variable and the series variable. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of time series datasets."
872,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a new method for multivariate time series forecasting. The proposed method is based on residual tensor networks (MVSRNets). The main idea is to use a low-rank approximation of a tensor network to represent the latent space of the time series as a series variable encoder, and then use a skip-connection layer to share information between the latent variable and the series variable. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of time series datasets."
873,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a new method for multivariate time series forecasting. The proposed method is based on residual tensor networks (MVSRNets). The main idea is to use a low-rank approximation of a tensor network to represent the latent space of the time series as a series variable encoder, and then use a skip-connection layer to share information between the latent variable and the series variable. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of time series datasets."
874,SP:c80a7392ec6147395a664734601fb389a1eb4470,"This paper proposes a new method for multivariate time series forecasting. The proposed method is based on residual tensor networks (MVSRNets). The main idea is to use a low-rank approximation of a tensor network to represent the latent space of the time series as a series variable encoder, and then use a skip-connection layer to share information between the latent variable and the series variable. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of time series datasets."
875,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper studies the problem of training GNNs with sparse moving averages. The authors propose a new SCO algorithm that uses a fixed size buffer and a fixed number of moving averages to train the GNN. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of the proposed algorithm.
876,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper studies the problem of training GNNs with sparse moving averages. The authors propose a new SCO algorithm that uses a fixed size buffer and a fixed number of moving averages to train the GNN. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of the proposed algorithm.
877,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper studies the problem of training GNNs with sparse moving averages. The authors propose a new SCO algorithm that uses a fixed size buffer and a fixed number of moving averages to train the GNN. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of the proposed algorithm.
878,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,This paper studies the problem of training GNNs with sparse moving averages. The authors propose a new SCO algorithm that uses a fixed size buffer and a fixed number of moving averages to train the GNN. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of the proposed algorithm.
879,SP:72e0cac289dce803582053614ec9ee93e783c838,"This paper proposes Circulant MinHash (C-MinHash), which is a variant of MinHash that uses independent random permutations of random hashes to estimate the Jaccard (similarity) similarity between two random hashes. The authors propose a circulant version of the original MinHash, where each random permutation is independent of each other, and the authors show that the proposed method can achieve better estimation accuracy than the classical MinHash. "
880,SP:72e0cac289dce803582053614ec9ee93e783c838,"This paper proposes Circulant MinHash (C-MinHash), which is a variant of MinHash that uses independent random permutations of random hashes to estimate the Jaccard (similarity) similarity between two random hashes. The authors propose a circulant version of the original MinHash, where each random permutation is independent of each other, and the authors show that the proposed method can achieve better estimation accuracy than the classical MinHash. "
881,SP:72e0cac289dce803582053614ec9ee93e783c838,"This paper proposes Circulant MinHash (C-MinHash), which is a variant of MinHash that uses independent random permutations of random hashes to estimate the Jaccard (similarity) similarity between two random hashes. The authors propose a circulant version of the original MinHash, where each random permutation is independent of each other, and the authors show that the proposed method can achieve better estimation accuracy than the classical MinHash. "
882,SP:72e0cac289dce803582053614ec9ee93e783c838,"This paper proposes Circulant MinHash (C-MinHash), which is a variant of MinHash that uses independent random permutations of random hashes to estimate the Jaccard (similarity) similarity between two random hashes. The authors propose a circulant version of the original MinHash, where each random permutation is independent of each other, and the authors show that the proposed method can achieve better estimation accuracy than the classical MinHash. "
883,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training method for lp-robust models. The proposed method is based on a union of two adversarial models, where the first model is a single lp threat model, and the second one is a multi-parameter model. The main idea is to train the multi-norm adversarial robustness of the two models separately, and then jointly train the single model with the multiple-norm model. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the state-of-the-art."
884,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training method for lp-robust models. The proposed method is based on a union of two adversarial models, where the first model is a single lp threat model, and the second one is a multi-parameter model. The main idea is to train the multi-norm adversarial robustness of the two models separately, and then jointly train the single model with the multiple-norm model. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the state-of-the-art."
885,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training method for lp-robust models. The proposed method is based on a union of two adversarial models, where the first model is a single lp threat model, and the second one is a multi-parameter model. The main idea is to train the multi-norm adversarial robustness of the two models separately, and then jointly train the single model with the multiple-norm model. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the state-of-the-art."
886,SP:d254b38331b6b6f30de398bae09380cd5c951698,"This paper proposes a new adversarial training method for lp-robust models. The proposed method is based on a union of two adversarial models, where the first model is a single lp threat model, and the second one is a multi-parameter model. The main idea is to train the multi-norm adversarial robustness of the two models separately, and then jointly train the single model with the multiple-norm model. Experiments on CIFAR-10 and ImageNet show that the proposed method outperforms the state-of-the-art."
887,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper studies the problem of safe multi-task learning (MTL). The authors propose a new method called Safe Multi-Task Learning (SMTL), which is based on the idea of negative sharing. The key idea of SMTL is to train a gate that is independent of the encoder and decoder, and then use the gate to train the decoder and the gate. The authors show that SMTL can be used to train gates and decoders simultaneously, and show that the gate can be trained with SMTL."
888,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper studies the problem of safe multi-task learning (MTL). The authors propose a new method called Safe Multi-Task Learning (SMTL), which is based on the idea of negative sharing. The key idea of SMTL is to train a gate that is independent of the encoder and decoder, and then use the gate to train the decoder and the gate. The authors show that SMTL can be used to train gates and decoders simultaneously, and show that the gate can be trained with SMTL."
889,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper studies the problem of safe multi-task learning (MTL). The authors propose a new method called Safe Multi-Task Learning (SMTL), which is based on the idea of negative sharing. The key idea of SMTL is to train a gate that is independent of the encoder and decoder, and then use the gate to train the decoder and the gate. The authors show that SMTL can be used to train gates and decoders simultaneously, and show that the gate can be trained with SMTL."
890,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper studies the problem of safe multi-task learning (MTL). The authors propose a new method called Safe Multi-Task Learning (SMTL), which is based on the idea of negative sharing. The key idea of SMTL is to train a gate that is independent of the encoder and decoder, and then use the gate to train the decoder and the gate. The authors show that SMTL can be used to train gates and decoders simultaneously, and show that the gate can be trained with SMTL."
891,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the expressive properties of energy-based sequence models. The authors show that the partition function of the model can be expressed as a rational number, which is an important property of such models. They also provide asymptotic bounds on the number of parameters required for the model to be expressive."
892,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the expressive properties of energy-based sequence models. The authors show that the partition function of the model can be expressed as a rational number, which is an important property of such models. They also provide asymptotic bounds on the number of parameters required for the model to be expressive."
893,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the expressive properties of energy-based sequence models. The authors show that the partition function of the model can be expressed as a rational number, which is an important property of such models. They also provide asymptotic bounds on the number of parameters required for the model to be expressive."
894,SP:c4cee0d44198559c417750ec4729d26b41061929,"This paper studies the expressive properties of energy-based sequence models. The authors show that the partition function of the model can be expressed as a rational number, which is an important property of such models. They also provide asymptotic bounds on the number of parameters required for the model to be expressive."
895,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), which is a new metric for computing the distance between two points on a hypersurface. The proposed method is based on the idea of augmenting the sliced-Wasserstein distance (SWD) with a random projection. The authors show that the proposed method can be used to compute the sliced SWD as well as the original SWD. The paper also shows that ASWD can be applied to the case where the data distribution is non-linear."
896,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), which is a new metric for computing the distance between two points on a hypersurface. The proposed method is based on the idea of augmenting the sliced-Wasserstein distance (SWD) with a random projection. The authors show that the proposed method can be used to compute the sliced SWD as well as the original SWD. The paper also shows that ASWD can be applied to the case where the data distribution is non-linear."
897,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), which is a new metric for computing the distance between two points on a hypersurface. The proposed method is based on the idea of augmenting the sliced-Wasserstein distance (SWD) with a random projection. The authors show that the proposed method can be used to compute the sliced SWD as well as the original SWD. The paper also shows that ASWD can be applied to the case where the data distribution is non-linear."
898,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"This paper proposes augmented sliced Wasserstein distances (ASWDs), which is a new metric for computing the distance between two points on a hypersurface. The proposed method is based on the idea of augmenting the sliced-Wasserstein distance (SWD) with a random projection. The authors show that the proposed method can be used to compute the sliced SWD as well as the original SWD. The paper also shows that ASWD can be applied to the case where the data distribution is non-linear."
899,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a framework for multi-agent reinforcement learning (MARL) that combines reinforcement learning and switching controls. The main idea of the proposed method is to use intrinsic rewards to encourage the agents to explore together. The authors show that the proposed LIGS framework can be applied to a variety of RL problems, including Foraging, StarCraft, and Switching Control. The experiments are conducted on a number of Atari games."
900,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a framework for multi-agent reinforcement learning (MARL) that combines reinforcement learning and switching controls. The main idea of the proposed method is to use intrinsic rewards to encourage the agents to explore together. The authors show that the proposed LIGS framework can be applied to a variety of RL problems, including Foraging, StarCraft, and Switching Control. The experiments are conducted on a number of Atari games."
901,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a framework for multi-agent reinforcement learning (MARL) that combines reinforcement learning and switching controls. The main idea of the proposed method is to use intrinsic rewards to encourage the agents to explore together. The authors show that the proposed LIGS framework can be applied to a variety of RL problems, including Foraging, StarCraft, and Switching Control. The experiments are conducted on a number of Atari games."
902,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper proposes a framework for multi-agent reinforcement learning (MARL) that combines reinforcement learning and switching controls. The main idea of the proposed method is to use intrinsic rewards to encourage the agents to explore together. The authors show that the proposed LIGS framework can be applied to a variety of RL problems, including Foraging, StarCraft, and Switching Control. The experiments are conducted on a number of Atari games."
903,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a new model for multi-hop QA. The main idea is to learn a compact neural representation of q, which is then used to train a neural network to predict the output of the QA system. The authors show that the proposed model is able to outperform the state-of-the-art methods on a variety of datasets."
904,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a new model for multi-hop QA. The main idea is to learn a compact neural representation of q, which is then used to train a neural network to predict the output of the QA system. The authors show that the proposed model is able to outperform the state-of-the-art methods on a variety of datasets."
905,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a new model for multi-hop QA. The main idea is to learn a compact neural representation of q, which is then used to train a neural network to predict the output of the QA system. The authors show that the proposed model is able to outperform the state-of-the-art methods on a variety of datasets."
906,SP:9eadc19f7f712c488cf50d091f372092f6352930,"This paper proposes a new model for multi-hop QA. The main idea is to learn a compact neural representation of q, which is then used to train a neural network to predict the output of the QA system. The authors show that the proposed model is able to outperform the state-of-the-art methods on a variety of datasets."
907,SP:4e79b326bbda5d1509e88869dde9886764366d41,This paper proposes a semi-supervised learning method for voice casting. The proposed method is based on the idea of label refining. The authors propose a method to refine the labels of a given modality based on its characteristic. The method is evaluated on the MassEffect 3 video game. The results show that the proposed method outperforms existing methods.
908,SP:4e79b326bbda5d1509e88869dde9886764366d41,This paper proposes a semi-supervised learning method for voice casting. The proposed method is based on the idea of label refining. The authors propose a method to refine the labels of a given modality based on its characteristic. The method is evaluated on the MassEffect 3 video game. The results show that the proposed method outperforms existing methods.
909,SP:4e79b326bbda5d1509e88869dde9886764366d41,This paper proposes a semi-supervised learning method for voice casting. The proposed method is based on the idea of label refining. The authors propose a method to refine the labels of a given modality based on its characteristic. The method is evaluated on the MassEffect 3 video game. The results show that the proposed method outperforms existing methods.
910,SP:4e79b326bbda5d1509e88869dde9886764366d41,This paper proposes a semi-supervised learning method for voice casting. The proposed method is based on the idea of label refining. The authors propose a method to refine the labels of a given modality based on its characteristic. The method is evaluated on the MassEffect 3 video game. The results show that the proposed method outperforms existing methods.
911,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,This paper proposes a method for multi-task learning in vision transformers. The proposed method is based on the idea of alternating training of the heads and tails of the ViT. The main idea is to train the head and the tail separately and then use the same network for all the other tasks. The experiments show that the proposed method outperforms the state of the art.
912,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,This paper proposes a method for multi-task learning in vision transformers. The proposed method is based on the idea of alternating training of the heads and tails of the ViT. The main idea is to train the head and the tail separately and then use the same network for all the other tasks. The experiments show that the proposed method outperforms the state of the art.
913,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,This paper proposes a method for multi-task learning in vision transformers. The proposed method is based on the idea of alternating training of the heads and tails of the ViT. The main idea is to train the head and the tail separately and then use the same network for all the other tasks. The experiments show that the proposed method outperforms the state of the art.
914,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,This paper proposes a method for multi-task learning in vision transformers. The proposed method is based on the idea of alternating training of the heads and tails of the ViT. The main idea is to train the head and the tail separately and then use the same network for all the other tasks. The experiments show that the proposed method outperforms the state of the art.
915,SP:249a72ef4e9cf02221243428174bb749068af6b2,This paper studies the problem of reward hacking in reinforcement learning (RL). The authors propose a new anomaly detection task to detect anomalous policies. The authors also propose a proxy reward to encourage the agent to explore more. The paper is well-written and easy to follow.
916,SP:249a72ef4e9cf02221243428174bb749068af6b2,This paper studies the problem of reward hacking in reinforcement learning (RL). The authors propose a new anomaly detection task to detect anomalous policies. The authors also propose a proxy reward to encourage the agent to explore more. The paper is well-written and easy to follow.
917,SP:249a72ef4e9cf02221243428174bb749068af6b2,This paper studies the problem of reward hacking in reinforcement learning (RL). The authors propose a new anomaly detection task to detect anomalous policies. The authors also propose a proxy reward to encourage the agent to explore more. The paper is well-written and easy to follow.
918,SP:249a72ef4e9cf02221243428174bb749068af6b2,This paper studies the problem of reward hacking in reinforcement learning (RL). The authors propose a new anomaly detection task to detect anomalous policies. The authors also propose a proxy reward to encourage the agent to explore more. The paper is well-written and easy to follow.
919,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes f-TVO, which is an extension of the thermodynamic variational objective (TVO) to the case of a deformed geodesic perspective. The main idea is to use the dual function of model evidence f∗(p(x)$ to approximate the true posterior distribution of the TVO. The authors show that fTVO can be approximated by a family of exponential family exponential functions, which can be expressed as a function of the deformed χ-geodesic of the model. They also provide a reparameterization trick to make the model evidence more interpretable."
920,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes f-TVO, which is an extension of the thermodynamic variational objective (TVO) to the case of a deformed geodesic perspective. The main idea is to use the dual function of model evidence f∗(p(x)$ to approximate the true posterior distribution of the TVO. The authors show that fTVO can be approximated by a family of exponential family exponential functions, which can be expressed as a function of the deformed χ-geodesic of the model. They also provide a reparameterization trick to make the model evidence more interpretable."
921,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes f-TVO, which is an extension of the thermodynamic variational objective (TVO) to the case of a deformed geodesic perspective. The main idea is to use the dual function of model evidence f∗(p(x)$ to approximate the true posterior distribution of the TVO. The authors show that fTVO can be approximated by a family of exponential family exponential functions, which can be expressed as a function of the deformed χ-geodesic of the model. They also provide a reparameterization trick to make the model evidence more interpretable."
922,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"This paper proposes f-TVO, which is an extension of the thermodynamic variational objective (TVO) to the case of a deformed geodesic perspective. The main idea is to use the dual function of model evidence f∗(p(x)$ to approximate the true posterior distribution of the TVO. The authors show that fTVO can be approximated by a family of exponential family exponential functions, which can be expressed as a function of the deformed χ-geodesic of the model. They also provide a reparameterization trick to make the model evidence more interpretable."
923,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper proposes a method for ensemble-based actor-critic exploration in continuous control tasks. The proposed method is based on a combination of UCB, weighted Bellman backup, and weighted Q-learning. The authors show that the proposed method outperforms UCB in terms of exploration and evaluation performance. They also show that their method can be combined with other existing methods such as clipped double Q-Learning and ED2."
924,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper proposes a method for ensemble-based actor-critic exploration in continuous control tasks. The proposed method is based on a combination of UCB, weighted Bellman backup, and weighted Q-learning. The authors show that the proposed method outperforms UCB in terms of exploration and evaluation performance. They also show that their method can be combined with other existing methods such as clipped double Q-Learning and ED2."
925,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper proposes a method for ensemble-based actor-critic exploration in continuous control tasks. The proposed method is based on a combination of UCB, weighted Bellman backup, and weighted Q-learning. The authors show that the proposed method outperforms UCB in terms of exploration and evaluation performance. They also show that their method can be combined with other existing methods such as clipped double Q-Learning and ED2."
926,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper proposes a method for ensemble-based actor-critic exploration in continuous control tasks. The proposed method is based on a combination of UCB, weighted Bellman backup, and weighted Q-learning. The authors show that the proposed method outperforms UCB in terms of exploration and evaluation performance. They also show that their method can be combined with other existing methods such as clipped double Q-Learning and ED2."
927,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of fairness in supervised learning models. The authors propose an algorithm for solving the Equalized Loss (EL) fair predictor problem, which is a non-convex optimization problem. The main contribution of this paper is the formulation of the EL fair predictor as a constrained optimization problem, where the loss function is a convex function of the prediction error/loss function. The proposed algorithm is based on the ELminimizer algorithm, which can be viewed as an optimization algorithm for convex optimization problems. The paper also provides an empirical evaluation of the proposed algorithm on real-world data."
928,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of fairness in supervised learning models. The authors propose an algorithm for solving the Equalized Loss (EL) fair predictor problem, which is a non-convex optimization problem. The main contribution of this paper is the formulation of the EL fair predictor as a constrained optimization problem, where the loss function is a convex function of the prediction error/loss function. The proposed algorithm is based on the ELminimizer algorithm, which can be viewed as an optimization algorithm for convex optimization problems. The paper also provides an empirical evaluation of the proposed algorithm on real-world data."
929,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of fairness in supervised learning models. The authors propose an algorithm for solving the Equalized Loss (EL) fair predictor problem, which is a non-convex optimization problem. The main contribution of this paper is the formulation of the EL fair predictor as a constrained optimization problem, where the loss function is a convex function of the prediction error/loss function. The proposed algorithm is based on the ELminimizer algorithm, which can be viewed as an optimization algorithm for convex optimization problems. The paper also provides an empirical evaluation of the proposed algorithm on real-world data."
930,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies the problem of fairness in supervised learning models. The authors propose an algorithm for solving the Equalized Loss (EL) fair predictor problem, which is a non-convex optimization problem. The main contribution of this paper is the formulation of the EL fair predictor as a constrained optimization problem, where the loss function is a convex function of the prediction error/loss function. The proposed algorithm is based on the ELminimizer algorithm, which can be viewed as an optimization algorithm for convex optimization problems. The paper also provides an empirical evaluation of the proposed algorithm on real-world data."
931,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks in the context of semantic parsing. The authors propose a meaningful learning principle for semantic parsing, which is motivated by the observation that inductive learning and deductive learning have been shown to fail to generalize well to unseen data. The paper proposes to use semantic linking as a regularizer to improve the generalization performance of neural network models. The proposed method is evaluated on SCAN dataset and real-world datasets. The results show that semantic linking improves the performance of CNNs, RNNs, and Transformers."
932,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks in the context of semantic parsing. The authors propose a meaningful learning principle for semantic parsing, which is motivated by the observation that inductive learning and deductive learning have been shown to fail to generalize well to unseen data. The paper proposes to use semantic linking as a regularizer to improve the generalization performance of neural network models. The proposed method is evaluated on SCAN dataset and real-world datasets. The results show that semantic linking improves the performance of CNNs, RNNs, and Transformers."
933,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks in the context of semantic parsing. The authors propose a meaningful learning principle for semantic parsing, which is motivated by the observation that inductive learning and deductive learning have been shown to fail to generalize well to unseen data. The paper proposes to use semantic linking as a regularizer to improve the generalization performance of neural network models. The proposed method is evaluated on SCAN dataset and real-world datasets. The results show that semantic linking improves the performance of CNNs, RNNs, and Transformers."
934,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper studies systematic generalization of neural networks in the context of semantic parsing. The authors propose a meaningful learning principle for semantic parsing, which is motivated by the observation that inductive learning and deductive learning have been shown to fail to generalize well to unseen data. The paper proposes to use semantic linking as a regularizer to improve the generalization performance of neural network models. The proposed method is evaluated on SCAN dataset and real-world datasets. The results show that semantic linking improves the performance of CNNs, RNNs, and Transformers."
935,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,This paper proposes a method for 3D shape representation learning based on multi-scale wavelet decomposition. The proposed method is based on a transformer-based approach to learn the wavelet coefficients of 3D shapes. The main contribution of this paper is to propose a lifting scheme to lift the wavelets of the high or low sub-band components of the 3D wavelet. The authors show that the proposed method outperforms the state-of-the-art methods on 3D ShapeNet and CIFAR-10.
936,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,This paper proposes a method for 3D shape representation learning based on multi-scale wavelet decomposition. The proposed method is based on a transformer-based approach to learn the wavelet coefficients of 3D shapes. The main contribution of this paper is to propose a lifting scheme to lift the wavelets of the high or low sub-band components of the 3D wavelet. The authors show that the proposed method outperforms the state-of-the-art methods on 3D ShapeNet and CIFAR-10.
937,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,This paper proposes a method for 3D shape representation learning based on multi-scale wavelet decomposition. The proposed method is based on a transformer-based approach to learn the wavelet coefficients of 3D shapes. The main contribution of this paper is to propose a lifting scheme to lift the wavelets of the high or low sub-band components of the 3D wavelet. The authors show that the proposed method outperforms the state-of-the-art methods on 3D ShapeNet and CIFAR-10.
938,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,This paper proposes a method for 3D shape representation learning based on multi-scale wavelet decomposition. The proposed method is based on a transformer-based approach to learn the wavelet coefficients of 3D shapes. The main contribution of this paper is to propose a lifting scheme to lift the wavelets of the high or low sub-band components of the 3D wavelet. The authors show that the proposed method outperforms the state-of-the-art methods on 3D ShapeNet and CIFAR-10.
939,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,This paper studies the effect of finetuning in-distribution (ID) and out-of-domain (OOD) on the performance of pretrained language models. The authors show that full and lightweight finetuned models are more likely to exhibit OOD behavior than full and full models. They also show that distillation can be used to improve the performance.
940,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,This paper studies the effect of finetuning in-distribution (ID) and out-of-domain (OOD) on the performance of pretrained language models. The authors show that full and lightweight finetuned models are more likely to exhibit OOD behavior than full and full models. They also show that distillation can be used to improve the performance.
941,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,This paper studies the effect of finetuning in-distribution (ID) and out-of-domain (OOD) on the performance of pretrained language models. The authors show that full and lightweight finetuned models are more likely to exhibit OOD behavior than full and full models. They also show that distillation can be used to improve the performance.
942,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,This paper studies the effect of finetuning in-distribution (ID) and out-of-domain (OOD) on the performance of pretrained language models. The authors show that full and lightweight finetuned models are more likely to exhibit OOD behavior than full and full models. They also show that distillation can be used to improve the performance.
943,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes a method for active learning of weakly supervised models. The proposed method is based on data programming, where the model is trained on a set of pointillistically labeled data, and the goal is to improve the performance of the model. The method is evaluated on a variety of medical classification datasets, and it is shown that the proposed method outperforms baselines."
944,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes a method for active learning of weakly supervised models. The proposed method is based on data programming, where the model is trained on a set of pointillistically labeled data, and the goal is to improve the performance of the model. The method is evaluated on a variety of medical classification datasets, and it is shown that the proposed method outperforms baselines."
945,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes a method for active learning of weakly supervised models. The proposed method is based on data programming, where the model is trained on a set of pointillistically labeled data, and the goal is to improve the performance of the model. The method is evaluated on a variety of medical classification datasets, and it is shown that the proposed method outperforms baselines."
946,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes a method for active learning of weakly supervised models. The proposed method is based on data programming, where the model is trained on a set of pointillistically labeled data, and the goal is to improve the performance of the model. The method is evaluated on a variety of medical classification datasets, and it is shown that the proposed method outperforms baselines."
947,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper studies the problem of group annotated training data for classification models. The authors propose a new algorithm called Group-DRO, which is based on the empirical risk minimization (ERM) objective. The main idea is to use a regularized loss function to encourage the model to learn features that are shared across groups. The proposed algorithm is evaluated on a variety of benchmark datasets and compared to ERM and Group DRO."
948,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper studies the problem of group annotated training data for classification models. The authors propose a new algorithm called Group-DRO, which is based on the empirical risk minimization (ERM) objective. The main idea is to use a regularized loss function to encourage the model to learn features that are shared across groups. The proposed algorithm is evaluated on a variety of benchmark datasets and compared to ERM and Group DRO."
949,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper studies the problem of group annotated training data for classification models. The authors propose a new algorithm called Group-DRO, which is based on the empirical risk minimization (ERM) objective. The main idea is to use a regularized loss function to encourage the model to learn features that are shared across groups. The proposed algorithm is evaluated on a variety of benchmark datasets and compared to ERM and Group DRO."
950,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,"This paper studies the problem of group annotated training data for classification models. The authors propose a new algorithm called Group-DRO, which is based on the empirical risk minimization (ERM) objective. The main idea is to use a regularized loss function to encourage the model to learn features that are shared across groups. The proposed algorithm is evaluated on a variety of benchmark datasets and compared to ERM and Group DRO."
951,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a new method for explaining the influence of features in black-box models. The proposed method is based on Shapley value explanations. The main idea is to use a directed graph to explain the importance of each feature in the feature space. The authors show that the proposed method can be applied to a variety of models and datasets. The method is evaluated on a range of datasets, including CIFAR-10, IMDB, Census, and gene data."
952,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a new method for explaining the influence of features in black-box models. The proposed method is based on Shapley value explanations. The main idea is to use a directed graph to explain the importance of each feature in the feature space. The authors show that the proposed method can be applied to a variety of models and datasets. The method is evaluated on a range of datasets, including CIFAR-10, IMDB, Census, and gene data."
953,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a new method for explaining the influence of features in black-box models. The proposed method is based on Shapley value explanations. The main idea is to use a directed graph to explain the importance of each feature in the feature space. The authors show that the proposed method can be applied to a variety of models and datasets. The method is evaluated on a range of datasets, including CIFAR-10, IMDB, Census, and gene data."
954,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper proposes a new method for explaining the influence of features in black-box models. The proposed method is based on Shapley value explanations. The main idea is to use a directed graph to explain the importance of each feature in the feature space. The authors show that the proposed method can be applied to a variety of models and datasets. The method is evaluated on a range of datasets, including CIFAR-10, IMDB, Census, and gene data."
955,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The proposed method is based on decision trees (POETREE), which is an extension of Decision Tree (DET). The main idea is to learn a representation of the history of the patient using a probabilistic tree, which is then used to train a decision tree policy. The authors show that the proposed method outperforms state-of-the-art methods on both synthetic and real-world medical datasets."
956,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The proposed method is based on decision trees (POETREE), which is an extension of Decision Tree (DET). The main idea is to learn a representation of the history of the patient using a probabilistic tree, which is then used to train a decision tree policy. The authors show that the proposed method outperforms state-of-the-art methods on both synthetic and real-world medical datasets."
957,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The proposed method is based on decision trees (POETREE), which is an extension of Decision Tree (DET). The main idea is to learn a representation of the history of the patient using a probabilistic tree, which is then used to train a decision tree policy. The authors show that the proposed method outperforms state-of-the-art methods on both synthetic and real-world medical datasets."
958,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a method for learning interpretable policy models for medical decision-making. The proposed method is based on decision trees (POETREE), which is an extension of Decision Tree (DET). The main idea is to learn a representation of the history of the patient using a probabilistic tree, which is then used to train a decision tree policy. The authors show that the proposed method outperforms state-of-the-art methods on both synthetic and real-world medical datasets."
959,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"This paper proposes a method for data augmentation (DA) based on patch auto-augmentation. The proposed method is based on the idea of joint optimal augmentation policies, where each agent is given a set of patches and the goal is to find a patch that maximizes the mutual information between all the patches in the grid. The method is evaluated on CIFAR-10, ImageNet, and CUB-200-2011. The results show that the proposed method outperforms the state-of-the-art methods."
960,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"This paper proposes a method for data augmentation (DA) based on patch auto-augmentation. The proposed method is based on the idea of joint optimal augmentation policies, where each agent is given a set of patches and the goal is to find a patch that maximizes the mutual information between all the patches in the grid. The method is evaluated on CIFAR-10, ImageNet, and CUB-200-2011. The results show that the proposed method outperforms the state-of-the-art methods."
961,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"This paper proposes a method for data augmentation (DA) based on patch auto-augmentation. The proposed method is based on the idea of joint optimal augmentation policies, where each agent is given a set of patches and the goal is to find a patch that maximizes the mutual information between all the patches in the grid. The method is evaluated on CIFAR-10, ImageNet, and CUB-200-2011. The results show that the proposed method outperforms the state-of-the-art methods."
962,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"This paper proposes a method for data augmentation (DA) based on patch auto-augmentation. The proposed method is based on the idea of joint optimal augmentation policies, where each agent is given a set of patches and the goal is to find a patch that maximizes the mutual information between all the patches in the grid. The method is evaluated on CIFAR-10, ImageNet, and CUB-200-2011. The results show that the proposed method outperforms the state-of-the-art methods."
963,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs) against adversarial attacks. The authors propose a new method for generating adversarial examples that is based on causal reasoning. The main idea is to use a graph-based approach to identify the source of distribution change in the adversarial distribution, which is then used to align the distribution of natural and adversarial distributions. The proposed method is evaluated on a variety of datasets."
964,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs) against adversarial attacks. The authors propose a new method for generating adversarial examples that is based on causal reasoning. The main idea is to use a graph-based approach to identify the source of distribution change in the adversarial distribution, which is then used to align the distribution of natural and adversarial distributions. The proposed method is evaluated on a variety of datasets."
965,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs) against adversarial attacks. The authors propose a new method for generating adversarial examples that is based on causal reasoning. The main idea is to use a graph-based approach to identify the source of distribution change in the adversarial distribution, which is then used to align the distribution of natural and adversarial distributions. The proposed method is evaluated on a variety of datasets."
966,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"This paper studies the problem of adversarial vulnerability of deep neural networks (DNNs) against adversarial attacks. The authors propose a new method for generating adversarial examples that is based on causal reasoning. The main idea is to use a graph-based approach to identify the source of distribution change in the adversarial distribution, which is then used to align the distribution of natural and adversarial distributions. The proposed method is evaluated on a variety of datasets."
967,SP:9f09449a47464efb5458d0732df7664865558e6f,This paper proposes a method for continual learning of convolutional filters. The proposed method is based on atom swapping. The authors propose to swap the filter atoms in the filter subspace of each layer of the network. The method is evaluated on a variety of benchmark datasets and compared to state-of-the-art methods.
968,SP:9f09449a47464efb5458d0732df7664865558e6f,This paper proposes a method for continual learning of convolutional filters. The proposed method is based on atom swapping. The authors propose to swap the filter atoms in the filter subspace of each layer of the network. The method is evaluated on a variety of benchmark datasets and compared to state-of-the-art methods.
969,SP:9f09449a47464efb5458d0732df7664865558e6f,This paper proposes a method for continual learning of convolutional filters. The proposed method is based on atom swapping. The authors propose to swap the filter atoms in the filter subspace of each layer of the network. The method is evaluated on a variety of benchmark datasets and compared to state-of-the-art methods.
970,SP:9f09449a47464efb5458d0732df7664865558e6f,This paper proposes a method for continual learning of convolutional filters. The proposed method is based on atom swapping. The authors propose to swap the filter atoms in the filter subspace of each layer of the network. The method is evaluated on a variety of benchmark datasets and compared to state-of-the-art methods.
971,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD). The authors show that SVGD has a proportional asymptotic limit of variance collapse, and show that the equilibrium variance of SVGD is near-orthogonality. The authors also show that for high-dimensional isotropic Gaussians, MMD-descent can be used to approximate SVGD."
972,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD). The authors show that SVGD has a proportional asymptotic limit of variance collapse, and show that the equilibrium variance of SVGD is near-orthogonality. The authors also show that for high-dimensional isotropic Gaussians, MMD-descent can be used to approximate SVGD."
973,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD). The authors show that SVGD has a proportional asymptotic limit of variance collapse, and show that the equilibrium variance of SVGD is near-orthogonality. The authors also show that for high-dimensional isotropic Gaussians, MMD-descent can be used to approximate SVGD."
974,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper studies the variance collapse phenomenon of Stein variational gradient descent (SVGD). The authors show that SVGD has a proportional asymptotic limit of variance collapse, and show that the equilibrium variance of SVGD is near-orthogonality. The authors also show that for high-dimensional isotropic Gaussians, MMD-descent can be used to approximate SVGD."
975,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the problem of robustness to adversarial examples. The authors propose a new measure called the ""noisy-class boundary"" (AT) that measures the distance between the class boundary and the noisy-label boundary of the classifier. They show that the AT is a geometric property of the adversarial example, and that it can be used as a general-purpose robust learning criterion. They also show that AT can be applied to the case where the noisy label boundary is close to the noisy class boundary, and provide a theoretical analysis of the smoothing effect of AT."
976,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the problem of robustness to adversarial examples. The authors propose a new measure called the ""noisy-class boundary"" (AT) that measures the distance between the class boundary and the noisy-label boundary of the classifier. They show that the AT is a geometric property of the adversarial example, and that it can be used as a general-purpose robust learning criterion. They also show that AT can be applied to the case where the noisy label boundary is close to the noisy class boundary, and provide a theoretical analysis of the smoothing effect of AT."
977,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the problem of robustness to adversarial examples. The authors propose a new measure called the ""noisy-class boundary"" (AT) that measures the distance between the class boundary and the noisy-label boundary of the classifier. They show that the AT is a geometric property of the adversarial example, and that it can be used as a general-purpose robust learning criterion. They also show that AT can be applied to the case where the noisy label boundary is close to the noisy class boundary, and provide a theoretical analysis of the smoothing effect of AT."
978,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This paper studies the problem of robustness to adversarial examples. The authors propose a new measure called the ""noisy-class boundary"" (AT) that measures the distance between the class boundary and the noisy-label boundary of the classifier. They show that the AT is a geometric property of the adversarial example, and that it can be used as a general-purpose robust learning criterion. They also show that AT can be applied to the case where the noisy label boundary is close to the noisy class boundary, and provide a theoretical analysis of the smoothing effect of AT."
979,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper studies the problem of certifying the robustness of neural network models against adversarial attacks. The authors propose a new method for certifying a model’s robustness based on its expected robustness. The proposed method is based on a statistical method to estimate the expected expected risk of a given model under a given input perturbation. The paper also proposes a categorial robustness metric to measure the risk and robustness levels of a model. The main contribution of the paper is the use of categorical robustness as a measure of model robustness, which can be used for risk mitigation. "
980,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper studies the problem of certifying the robustness of neural network models against adversarial attacks. The authors propose a new method for certifying a model’s robustness based on its expected robustness. The proposed method is based on a statistical method to estimate the expected expected risk of a given model under a given input perturbation. The paper also proposes a categorial robustness metric to measure the risk and robustness levels of a model. The main contribution of the paper is the use of categorical robustness as a measure of model robustness, which can be used for risk mitigation. "
981,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper studies the problem of certifying the robustness of neural network models against adversarial attacks. The authors propose a new method for certifying a model’s robustness based on its expected robustness. The proposed method is based on a statistical method to estimate the expected expected risk of a given model under a given input perturbation. The paper also proposes a categorial robustness metric to measure the risk and robustness levels of a model. The main contribution of the paper is the use of categorical robustness as a measure of model robustness, which can be used for risk mitigation. "
982,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper studies the problem of certifying the robustness of neural network models against adversarial attacks. The authors propose a new method for certifying a model’s robustness based on its expected robustness. The proposed method is based on a statistical method to estimate the expected expected risk of a given model under a given input perturbation. The paper also proposes a categorial robustness metric to measure the risk and robustness levels of a model. The main contribution of the paper is the use of categorical robustness as a measure of model robustness, which can be used for risk mitigation. "
983,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM is an extension of the hierarchical graph neural network (GNN) model, which is a generalization of GNN. The main difference between GNN and HCM lies in the fact that in GNN, the number of atoms in the graph is much smaller than in HCM. The authors show that HCM can learn a hierarchy of chunk representations that are independent of each other. They also show that the learned representation can be used to learn representations for non-i.i.d. data."
984,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM is an extension of the hierarchical graph neural network (GNN) model, which is a generalization of GNN. The main difference between GNN and HCM lies in the fact that in GNN, the number of atoms in the graph is much smaller than in HCM. The authors show that HCM can learn a hierarchy of chunk representations that are independent of each other. They also show that the learned representation can be used to learn representations for non-i.i.d. data."
985,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM is an extension of the hierarchical graph neural network (GNN) model, which is a generalization of GNN. The main difference between GNN and HCM lies in the fact that in GNN, the number of atoms in the graph is much smaller than in HCM. The authors show that HCM can learn a hierarchy of chunk representations that are independent of each other. They also show that the learned representation can be used to learn representations for non-i.i.d. data."
986,SP:6ba17dd4b31a39478abd995df894447675f2f974,"This paper proposes a hierarchical chunking model (HCM) for representation learning. HCM is an extension of the hierarchical graph neural network (GNN) model, which is a generalization of GNN. The main difference between GNN and HCM lies in the fact that in GNN, the number of atoms in the graph is much smaller than in HCM. The authors show that HCM can learn a hierarchy of chunk representations that are independent of each other. They also show that the learned representation can be used to learn representations for non-i.i.d. data."
987,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of learning a graph convolutional network (GCN) with gradient flow. The main contribution of this paper is to show that the gradient flow of a GCN can be viewed as a neural reparametrization of the loss function of a graph neural network (GNN). In particular, it is shown that the gradients of the losses of the GNN can be expressed as a function of the Hessian of the original loss function and the gradient of the new loss function. The authors then show that this can be used as a way to approximate the Hessians of the GCN loss function, which is then used to compute the gradient gradient of a GNN. The paper also shows that the proposed method can be applied to network synchronization and persistent homology optimization problems."
988,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of learning a graph convolutional network (GCN) with gradient flow. The main contribution of this paper is to show that the gradient flow of a GCN can be viewed as a neural reparametrization of the loss function of a graph neural network (GNN). In particular, it is shown that the gradients of the losses of the GNN can be expressed as a function of the Hessian of the original loss function and the gradient of the new loss function. The authors then show that this can be used as a way to approximate the Hessians of the GCN loss function, which is then used to compute the gradient gradient of a GNN. The paper also shows that the proposed method can be applied to network synchronization and persistent homology optimization problems."
989,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of learning a graph convolutional network (GCN) with gradient flow. The main contribution of this paper is to show that the gradient flow of a GCN can be viewed as a neural reparametrization of the loss function of a graph neural network (GNN). In particular, it is shown that the gradients of the losses of the GNN can be expressed as a function of the Hessian of the original loss function and the gradient of the new loss function. The authors then show that this can be used as a way to approximate the Hessians of the GCN loss function, which is then used to compute the gradient gradient of a GNN. The paper also shows that the proposed method can be applied to network synchronization and persistent homology optimization problems."
990,SP:625e3908502fd5be949bb915116ed7569ba84298,"This paper studies the problem of learning a graph convolutional network (GCN) with gradient flow. The main contribution of this paper is to show that the gradient flow of a GCN can be viewed as a neural reparametrization of the loss function of a graph neural network (GNN). In particular, it is shown that the gradients of the losses of the GNN can be expressed as a function of the Hessian of the original loss function and the gradient of the new loss function. The authors then show that this can be used as a way to approximate the Hessians of the GCN loss function, which is then used to compute the gradient gradient of a GNN. The paper also shows that the proposed method can be applied to network synchronization and persistent homology optimization problems."
991,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a new method for non-parametric image classification. The proposed method is based on the SVMnet architecture, which is an extension of the SVM architecture. The main difference between SVM and DCNN is that SVM is a parametric network, whereas DCNNs are parametric networks. The authors show that the proposed method can achieve better performance than SVM on a variety of image classification tasks."
992,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a new method for non-parametric image classification. The proposed method is based on the SVMnet architecture, which is an extension of the SVM architecture. The main difference between SVM and DCNN is that SVM is a parametric network, whereas DCNNs are parametric networks. The authors show that the proposed method can achieve better performance than SVM on a variety of image classification tasks."
993,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a new method for non-parametric image classification. The proposed method is based on the SVMnet architecture, which is an extension of the SVM architecture. The main difference between SVM and DCNN is that SVM is a parametric network, whereas DCNNs are parametric networks. The authors show that the proposed method can achieve better performance than SVM on a variety of image classification tasks."
994,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"This paper proposes a new method for non-parametric image classification. The proposed method is based on the SVMnet architecture, which is an extension of the SVM architecture. The main difference between SVM and DCNN is that SVM is a parametric network, whereas DCNNs are parametric networks. The authors show that the proposed method can achieve better performance than SVM on a variety of image classification tasks."
995,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a new distributed GNN training method called Learn Locally, Correct Globally (LLCG). The main idea is to train a local GNN on the server and then send the local model to the server to correct it. The proposed method is based on the idea of periodic model averaging. The authors show that the proposed method converges faster than previous methods."
996,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a new distributed GNN training method called Learn Locally, Correct Globally (LLCG). The main idea is to train a local GNN on the server and then send the local model to the server to correct it. The proposed method is based on the idea of periodic model averaging. The authors show that the proposed method converges faster than previous methods."
997,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a new distributed GNN training method called Learn Locally, Correct Globally (LLCG). The main idea is to train a local GNN on the server and then send the local model to the server to correct it. The proposed method is based on the idea of periodic model averaging. The authors show that the proposed method converges faster than previous methods."
998,SP:a18f4697f350a864866dac871f581b8fc67e8088,"This paper proposes a new distributed GNN training method called Learn Locally, Correct Globally (LLCG). The main idea is to train a local GNN on the server and then send the local model to the server to correct it. The proposed method is based on the idea of periodic model averaging. The authors show that the proposed method converges faster than previous methods."
999,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for any-time image classification. The main idea is to use a unified and end-to-end model for anytime pixel-level recognition. The architecture consists of two parts: (1) a new exit architecture that is designed to adaptively adapt to the depth and spatial resolution of features, and (2) a feature-based stochastic sampling approach. The proposed architecture is evaluated on Cityscapes semantic segmentation and MPII human pose estimation tasks. The experimental results show that the proposed architecture outperforms the state-of-the-art models."
1000,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for any-time image classification. The main idea is to use a unified and end-to-end model for anytime pixel-level recognition. The architecture consists of two parts: (1) a new exit architecture that is designed to adaptively adapt to the depth and spatial resolution of features, and (2) a feature-based stochastic sampling approach. The proposed architecture is evaluated on Cityscapes semantic segmentation and MPII human pose estimation tasks. The experimental results show that the proposed architecture outperforms the state-of-the-art models."
1001,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for any-time image classification. The main idea is to use a unified and end-to-end model for anytime pixel-level recognition. The architecture consists of two parts: (1) a new exit architecture that is designed to adaptively adapt to the depth and spatial resolution of features, and (2) a feature-based stochastic sampling approach. The proposed architecture is evaluated on Cityscapes semantic segmentation and MPII human pose estimation tasks. The experimental results show that the proposed architecture outperforms the state-of-the-art models."
1002,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"This paper proposes a novel architecture for any-time image classification. The main idea is to use a unified and end-to-end model for anytime pixel-level recognition. The architecture consists of two parts: (1) a new exit architecture that is designed to adaptively adapt to the depth and spatial resolution of features, and (2) a feature-based stochastic sampling approach. The proposed architecture is evaluated on Cityscapes semantic segmentation and MPII human pose estimation tasks. The experimental results show that the proposed architecture outperforms the state-of-the-art models."
1003,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a new method for bootstrapping neural processes. The proposed method is based on the bootstrap method of B(A)NP. The main idea is to use a Gaussian distribution of random functions as a bootstrap distribution to model the functional uncertainty of the latent variable. The method is evaluated on a variety of tasks, including Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks."
1004,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a new method for bootstrapping neural processes. The proposed method is based on the bootstrap method of B(A)NP. The main idea is to use a Gaussian distribution of random functions as a bootstrap distribution to model the functional uncertainty of the latent variable. The method is evaluated on a variety of tasks, including Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks."
1005,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a new method for bootstrapping neural processes. The proposed method is based on the bootstrap method of B(A)NP. The main idea is to use a Gaussian distribution of random functions as a bootstrap distribution to model the functional uncertainty of the latent variable. The method is evaluated on a variety of tasks, including Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks."
1006,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"This paper proposes a new method for bootstrapping neural processes. The proposed method is based on the bootstrap method of B(A)NP. The main idea is to use a Gaussian distribution of random functions as a bootstrap distribution to model the functional uncertainty of the latent variable. The method is evaluated on a variety of tasks, including Bayesian optimization, contextual multi-armed bandit, and sequential decision-making tasks."
1007,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes GeneBERT, a method for pre-training a model on large-scale regulatory genomics data. The proposed method is based on multi-modal and self-supervised learning. The authors evaluate the proposed method on the ATAC-seq dataset and show that it can achieve state-of-the-art performance on several downstream tasks."
1008,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes GeneBERT, a method for pre-training a model on large-scale regulatory genomics data. The proposed method is based on multi-modal and self-supervised learning. The authors evaluate the proposed method on the ATAC-seq dataset and show that it can achieve state-of-the-art performance on several downstream tasks."
1009,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes GeneBERT, a method for pre-training a model on large-scale regulatory genomics data. The proposed method is based on multi-modal and self-supervised learning. The authors evaluate the proposed method on the ATAC-seq dataset and show that it can achieve state-of-the-art performance on several downstream tasks."
1010,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"This paper proposes GeneBERT, a method for pre-training a model on large-scale regulatory genomics data. The proposed method is based on multi-modal and self-supervised learning. The authors evaluate the proposed method on the ATAC-seq dataset and show that it can achieve state-of-the-art performance on several downstream tasks."
1011,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a method for out-of-distribution (OOD) detection. The proposed method is based on the Fisher-Rao distance between the discriminator and the features of the pre-trained neural network. The method is evaluated on MNIST, CIFAR-10, and CelebA datasets."
1012,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a method for out-of-distribution (OOD) detection. The proposed method is based on the Fisher-Rao distance between the discriminator and the features of the pre-trained neural network. The method is evaluated on MNIST, CIFAR-10, and CelebA datasets."
1013,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a method for out-of-distribution (OOD) detection. The proposed method is based on the Fisher-Rao distance between the discriminator and the features of the pre-trained neural network. The method is evaluated on MNIST, CIFAR-10, and CelebA datasets."
1014,SP:841b12443d0274e34b78940f220b17d36798899b,"This paper proposes a method for out-of-distribution (OOD) detection. The proposed method is based on the Fisher-Rao distance between the discriminator and the features of the pre-trained neural network. The method is evaluated on MNIST, CIFAR-10, and CelebA datasets."
1015,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper studies the equivariance of binary dichotomies in the context of group-invariant representation learning. The authors derive the Cover’s Function Counting Theorem for linearly separable and group invariant binary dichotomy, and show that under certain conditions, they are equivariant to group action and group action. The paper also provides a theoretical analysis of the relation between global pooling, convolutions, and element-wise nonlinearities."
1016,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper studies the equivariance of binary dichotomies in the context of group-invariant representation learning. The authors derive the Cover’s Function Counting Theorem for linearly separable and group invariant binary dichotomy, and show that under certain conditions, they are equivariant to group action and group action. The paper also provides a theoretical analysis of the relation between global pooling, convolutions, and element-wise nonlinearities."
1017,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper studies the equivariance of binary dichotomies in the context of group-invariant representation learning. The authors derive the Cover’s Function Counting Theorem for linearly separable and group invariant binary dichotomy, and show that under certain conditions, they are equivariant to group action and group action. The paper also provides a theoretical analysis of the relation between global pooling, convolutions, and element-wise nonlinearities."
1018,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This paper studies the equivariance of binary dichotomies in the context of group-invariant representation learning. The authors derive the Cover’s Function Counting Theorem for linearly separable and group invariant binary dichotomy, and show that under certain conditions, they are equivariant to group action and group action. The paper also provides a theoretical analysis of the relation between global pooling, convolutions, and element-wise nonlinearities."
1019,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper studies the problem of concept counterfactual explanations (CCEs) in the context of concept drift and mitigating biases. The authors propose a systematic approach to train a classifier that is able to identify spurious correlations between concepts in the training set and the test set. The proposed approach is motivated by the observation that the spurious correlations can arise when the classifier is trained on a small subset of the training data. To address this issue, the authors propose to use concept activation vectors, which are based on the concept activation vector of the original classifier, to train the model on a larger subset of training data, and then use the concept activations of the model to train an additional classifier. Experiments show that the proposed CCE method outperforms existing methods on a variety of datasets."
1020,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper studies the problem of concept counterfactual explanations (CCEs) in the context of concept drift and mitigating biases. The authors propose a systematic approach to train a classifier that is able to identify spurious correlations between concepts in the training set and the test set. The proposed approach is motivated by the observation that the spurious correlations can arise when the classifier is trained on a small subset of the training data. To address this issue, the authors propose to use concept activation vectors, which are based on the concept activation vector of the original classifier, to train the model on a larger subset of training data, and then use the concept activations of the model to train an additional classifier. Experiments show that the proposed CCE method outperforms existing methods on a variety of datasets."
1021,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper studies the problem of concept counterfactual explanations (CCEs) in the context of concept drift and mitigating biases. The authors propose a systematic approach to train a classifier that is able to identify spurious correlations between concepts in the training set and the test set. The proposed approach is motivated by the observation that the spurious correlations can arise when the classifier is trained on a small subset of the training data. To address this issue, the authors propose to use concept activation vectors, which are based on the concept activation vector of the original classifier, to train the model on a larger subset of training data, and then use the concept activations of the model to train an additional classifier. Experiments show that the proposed CCE method outperforms existing methods on a variety of datasets."
1022,SP:47889067620e5ac2e304681769af9d1d930f6d2b,"This paper studies the problem of concept counterfactual explanations (CCEs) in the context of concept drift and mitigating biases. The authors propose a systematic approach to train a classifier that is able to identify spurious correlations between concepts in the training set and the test set. The proposed approach is motivated by the observation that the spurious correlations can arise when the classifier is trained on a small subset of the training data. To address this issue, the authors propose to use concept activation vectors, which are based on the concept activation vector of the original classifier, to train the model on a larger subset of training data, and then use the concept activations of the model to train an additional classifier. Experiments show that the proposed CCE method outperforms existing methods on a variety of datasets."
1023,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,This paper proposes a new kernel point convolution (KPConv) module for 3D point cloud applications. The proposed module is based on the neighbor-kernel attention (NKA) module. The main difference between the proposed module and the original KPConv is that the proposed NKA module uses a depth-wise kernel instead of a depthwise kernel. The experimental results on 3D Point cloud classification and segmentation benchmarks show the effectiveness of the proposed proposed module.
1024,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,This paper proposes a new kernel point convolution (KPConv) module for 3D point cloud applications. The proposed module is based on the neighbor-kernel attention (NKA) module. The main difference between the proposed module and the original KPConv is that the proposed NKA module uses a depth-wise kernel instead of a depthwise kernel. The experimental results on 3D Point cloud classification and segmentation benchmarks show the effectiveness of the proposed proposed module.
1025,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,This paper proposes a new kernel point convolution (KPConv) module for 3D point cloud applications. The proposed module is based on the neighbor-kernel attention (NKA) module. The main difference between the proposed module and the original KPConv is that the proposed NKA module uses a depth-wise kernel instead of a depthwise kernel. The experimental results on 3D Point cloud classification and segmentation benchmarks show the effectiveness of the proposed proposed module.
1026,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,This paper proposes a new kernel point convolution (KPConv) module for 3D point cloud applications. The proposed module is based on the neighbor-kernel attention (NKA) module. The main difference between the proposed module and the original KPConv is that the proposed NKA module uses a depth-wise kernel instead of a depthwise kernel. The experimental results on 3D Point cloud classification and segmentation benchmarks show the effectiveness of the proposed proposed module.
1027,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a new adversarial training strategy that aims to improve the quality of the adversarial examples. The proposed strategy is based on the idea that adversarial perturbations can lead to overestimation and under-estimation of the robustness of the model.
1028,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a new adversarial training strategy that aims to improve the quality of the adversarial examples. The proposed strategy is based on the idea that adversarial perturbations can lead to overestimation and under-estimation of the robustness of the model.
1029,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a new adversarial training strategy that aims to improve the quality of the adversarial examples. The proposed strategy is based on the idea that adversarial perturbations can lead to overestimation and under-estimation of the robustness of the model.
1030,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,This paper studies the problem of adversarial robustness in the presence of low-quality data. The authors propose a new adversarial training strategy that aims to improve the quality of the adversarial examples. The proposed strategy is based on the idea that adversarial perturbations can lead to overestimation and under-estimation of the robustness of the model.
1031,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper considers the problem of approximating Korobov functions of bounded second mixed derivatives with a neural network. The main contribution of this paper is to provide an upper bound on the number of parameters required to approximate such functions. This is achieved by considering a continuous function approximator, which is a special case of the ReLU activation function. The authors show that this function can be approximated by a deep neural network, and show that it is a near-optimal function approximating the function. They also provide a lower bound for ReLU functions."
1032,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper considers the problem of approximating Korobov functions of bounded second mixed derivatives with a neural network. The main contribution of this paper is to provide an upper bound on the number of parameters required to approximate such functions. This is achieved by considering a continuous function approximator, which is a special case of the ReLU activation function. The authors show that this function can be approximated by a deep neural network, and show that it is a near-optimal function approximating the function. They also provide a lower bound for ReLU functions."
1033,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper considers the problem of approximating Korobov functions of bounded second mixed derivatives with a neural network. The main contribution of this paper is to provide an upper bound on the number of parameters required to approximate such functions. This is achieved by considering a continuous function approximator, which is a special case of the ReLU activation function. The authors show that this function can be approximated by a deep neural network, and show that it is a near-optimal function approximating the function. They also provide a lower bound for ReLU functions."
1034,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper considers the problem of approximating Korobov functions of bounded second mixed derivatives with a neural network. The main contribution of this paper is to provide an upper bound on the number of parameters required to approximate such functions. This is achieved by considering a continuous function approximator, which is a special case of the ReLU activation function. The authors show that this function can be approximated by a deep neural network, and show that it is a near-optimal function approximating the function. They also provide a lower bound for ReLU functions."
1035,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper studies the effect of training speed and network capacity on the emergent language properties of the speaker-listener Lewis Game. The authors show that training speed, network capacity, and training speed heterogeneities are important factors in the evolution of language. They also show that the relative difference of factors can be used to explain the diversity of language properties."
1036,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper studies the effect of training speed and network capacity on the emergent language properties of the speaker-listener Lewis Game. The authors show that training speed, network capacity, and training speed heterogeneities are important factors in the evolution of language. They also show that the relative difference of factors can be used to explain the diversity of language properties."
1037,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper studies the effect of training speed and network capacity on the emergent language properties of the speaker-listener Lewis Game. The authors show that training speed, network capacity, and training speed heterogeneities are important factors in the evolution of language. They also show that the relative difference of factors can be used to explain the diversity of language properties."
1038,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"This paper studies the effect of training speed and network capacity on the emergent language properties of the speaker-listener Lewis Game. The authors show that training speed, network capacity, and training speed heterogeneities are important factors in the evolution of language. They also show that the relative difference of factors can be used to explain the diversity of language properties."
1039,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper studies the problem of learning polynomial graph filters for graph neural networks (GNNs) on heterophilic graphs. In particular, the authors study the spectrum of adaptive polynomials that can be used in GNNs. The authors show that the spectrum can be extended to the eigendecomposition of the graph, e.g., eigencomponents of the eigenfunctions, and eigenvalues of the filters. They then propose a model that learns a latent filter for each eigenvalue of the spectrum, which is then used to train a GNN. The proposed model is evaluated on the node classification task on the MNIST dataset."
1040,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper studies the problem of learning polynomial graph filters for graph neural networks (GNNs) on heterophilic graphs. In particular, the authors study the spectrum of adaptive polynomials that can be used in GNNs. The authors show that the spectrum can be extended to the eigendecomposition of the graph, e.g., eigencomponents of the eigenfunctions, and eigenvalues of the filters. They then propose a model that learns a latent filter for each eigenvalue of the spectrum, which is then used to train a GNN. The proposed model is evaluated on the node classification task on the MNIST dataset."
1041,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper studies the problem of learning polynomial graph filters for graph neural networks (GNNs) on heterophilic graphs. In particular, the authors study the spectrum of adaptive polynomials that can be used in GNNs. The authors show that the spectrum can be extended to the eigendecomposition of the graph, e.g., eigencomponents of the eigenfunctions, and eigenvalues of the filters. They then propose a model that learns a latent filter for each eigenvalue of the spectrum, which is then used to train a GNN. The proposed model is evaluated on the node classification task on the MNIST dataset."
1042,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca,"This paper studies the problem of learning polynomial graph filters for graph neural networks (GNNs) on heterophilic graphs. In particular, the authors study the spectrum of adaptive polynomials that can be used in GNNs. The authors show that the spectrum can be extended to the eigendecomposition of the graph, e.g., eigencomponents of the eigenfunctions, and eigenvalues of the filters. They then propose a model that learns a latent filter for each eigenvalue of the spectrum, which is then used to train a GNN. The proposed model is evaluated on the node classification task on the MNIST dataset."
1043,SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper proposes a new algorithm for embedding-based distance prediction on graphs. The proposed algorithm is based on a combination of truncated random walk and Pointwise Mutual Information (PMI)-based optimization. In particular, the authors propose to use the intrinsic metric between distance between two nodes as a predictor for the distance between the two nodes’ mutual shortest distance. The authors also propose a new steering optimization objective to optimize the global distance matrix of the pairwise mutual information between the pairs of nodes. Experiments on several graph datasets demonstrate the effectiveness of the proposed algorithm."
1044,SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper proposes a new algorithm for embedding-based distance prediction on graphs. The proposed algorithm is based on a combination of truncated random walk and Pointwise Mutual Information (PMI)-based optimization. In particular, the authors propose to use the intrinsic metric between distance between two nodes as a predictor for the distance between the two nodes’ mutual shortest distance. The authors also propose a new steering optimization objective to optimize the global distance matrix of the pairwise mutual information between the pairs of nodes. Experiments on several graph datasets demonstrate the effectiveness of the proposed algorithm."
1045,SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper proposes a new algorithm for embedding-based distance prediction on graphs. The proposed algorithm is based on a combination of truncated random walk and Pointwise Mutual Information (PMI)-based optimization. In particular, the authors propose to use the intrinsic metric between distance between two nodes as a predictor for the distance between the two nodes’ mutual shortest distance. The authors also propose a new steering optimization objective to optimize the global distance matrix of the pairwise mutual information between the pairs of nodes. Experiments on several graph datasets demonstrate the effectiveness of the proposed algorithm."
1046,SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper proposes a new algorithm for embedding-based distance prediction on graphs. The proposed algorithm is based on a combination of truncated random walk and Pointwise Mutual Information (PMI)-based optimization. In particular, the authors propose to use the intrinsic metric between distance between two nodes as a predictor for the distance between the two nodes’ mutual shortest distance. The authors also propose a new steering optimization objective to optimize the global distance matrix of the pairwise mutual information between the pairs of nodes. Experiments on several graph datasets demonstrate the effectiveness of the proposed algorithm."
1047,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper studies the problem of continual knowledge learning (CKL) in the context of large language models (LMs), where the goal is to maintain invariant knowledge over time. The authors propose a new metric to measure the retention of time-invariant world knowledge and the update of outdated knowledge. They also propose a benchmark to evaluate the performance of CKL on a variety of knowledge-dependent downstream tasks, including question answering, fact-checking, and open dialogue. They show that CKL can achieve better performance than baseline methods on these tasks."
1048,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper studies the problem of continual knowledge learning (CKL) in the context of large language models (LMs), where the goal is to maintain invariant knowledge over time. The authors propose a new metric to measure the retention of time-invariant world knowledge and the update of outdated knowledge. They also propose a benchmark to evaluate the performance of CKL on a variety of knowledge-dependent downstream tasks, including question answering, fact-checking, and open dialogue. They show that CKL can achieve better performance than baseline methods on these tasks."
1049,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper studies the problem of continual knowledge learning (CKL) in the context of large language models (LMs), where the goal is to maintain invariant knowledge over time. The authors propose a new metric to measure the retention of time-invariant world knowledge and the update of outdated knowledge. They also propose a benchmark to evaluate the performance of CKL on a variety of knowledge-dependent downstream tasks, including question answering, fact-checking, and open dialogue. They show that CKL can achieve better performance than baseline methods on these tasks."
1050,SP:13db440061fed785f05bb41d0767225403ecf7a1,"This paper studies the problem of continual knowledge learning (CKL) in the context of large language models (LMs), where the goal is to maintain invariant knowledge over time. The authors propose a new metric to measure the retention of time-invariant world knowledge and the update of outdated knowledge. They also propose a benchmark to evaluate the performance of CKL on a variety of knowledge-dependent downstream tasks, including question answering, fact-checking, and open dialogue. They show that CKL can achieve better performance than baseline methods on these tasks."
1051,SP:639fd88482330389019fb5be7446a909b99a8609,"This paper proposes a new algorithm for decision tree learning. The main idea is to use an exhaustive search algorithm to find the criterion minimization problem. The algorithm is based on the Haar tree, which is an oblique tree. The authors show that their algorithm can achieve better performance than the baseline non-stochastic approach. They also show that the algorithm can be used to train a tree on the MNIST dataset."
1052,SP:639fd88482330389019fb5be7446a909b99a8609,"This paper proposes a new algorithm for decision tree learning. The main idea is to use an exhaustive search algorithm to find the criterion minimization problem. The algorithm is based on the Haar tree, which is an oblique tree. The authors show that their algorithm can achieve better performance than the baseline non-stochastic approach. They also show that the algorithm can be used to train a tree on the MNIST dataset."
1053,SP:639fd88482330389019fb5be7446a909b99a8609,"This paper proposes a new algorithm for decision tree learning. The main idea is to use an exhaustive search algorithm to find the criterion minimization problem. The algorithm is based on the Haar tree, which is an oblique tree. The authors show that their algorithm can achieve better performance than the baseline non-stochastic approach. They also show that the algorithm can be used to train a tree on the MNIST dataset."
1054,SP:639fd88482330389019fb5be7446a909b99a8609,"This paper proposes a new algorithm for decision tree learning. The main idea is to use an exhaustive search algorithm to find the criterion minimization problem. The algorithm is based on the Haar tree, which is an oblique tree. The authors show that their algorithm can achieve better performance than the baseline non-stochastic approach. They also show that the algorithm can be used to train a tree on the MNIST dataset."
1055,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the learning rate schedule of SGD for quadratic objectives. In particular, the authors consider the case where the eigenvalue distribution of the Hessian matrix is different from that of the original Hessian. The authors propose a new learning rate scheduler, called Eigencurve, which is a variant of step decay. The main idea is to use the Eigenvalue Distribution of Hessian as a regularizer for the eigendecomposition of the SGD objective. The paper shows that the proposed EigenCurve can converge to the minimax optimal convergence rate for SGD. The empirical results on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
1056,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the learning rate schedule of SGD for quadratic objectives. In particular, the authors consider the case where the eigenvalue distribution of the Hessian matrix is different from that of the original Hessian. The authors propose a new learning rate scheduler, called Eigencurve, which is a variant of step decay. The main idea is to use the Eigenvalue Distribution of Hessian as a regularizer for the eigendecomposition of the SGD objective. The paper shows that the proposed EigenCurve can converge to the minimax optimal convergence rate for SGD. The empirical results on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
1057,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the learning rate schedule of SGD for quadratic objectives. In particular, the authors consider the case where the eigenvalue distribution of the Hessian matrix is different from that of the original Hessian. The authors propose a new learning rate scheduler, called Eigencurve, which is a variant of step decay. The main idea is to use the Eigenvalue Distribution of Hessian as a regularizer for the eigendecomposition of the SGD objective. The paper shows that the proposed EigenCurve can converge to the minimax optimal convergence rate for SGD. The empirical results on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
1058,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"This paper studies the learning rate schedule of SGD for quadratic objectives. In particular, the authors consider the case where the eigenvalue distribution of the Hessian matrix is different from that of the original Hessian. The authors propose a new learning rate scheduler, called Eigencurve, which is a variant of step decay. The main idea is to use the Eigenvalue Distribution of Hessian as a regularizer for the eigendecomposition of the SGD objective. The paper shows that the proposed EigenCurve can converge to the minimax optimal convergence rate for SGD. The empirical results on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
1059,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning. The authors propose a Bayesian Optimization-based approach to address the issue of model uncertainty in offline RL. The main contribution of this paper is to study the total variation distance between the model and the MDP, which is the difference between the expected return and the pessimistic return of the model under the pessimistic MDP. The paper also proposes a new uncertainty heuristic for offline RL based on Bayesian optimization."
1060,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning. The authors propose a Bayesian Optimization-based approach to address the issue of model uncertainty in offline RL. The main contribution of this paper is to study the total variation distance between the model and the MDP, which is the difference between the expected return and the pessimistic return of the model under the pessimistic MDP. The paper also proposes a new uncertainty heuristic for offline RL based on Bayesian optimization."
1061,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning. The authors propose a Bayesian Optimization-based approach to address the issue of model uncertainty in offline RL. The main contribution of this paper is to study the total variation distance between the model and the MDP, which is the difference between the expected return and the pessimistic return of the model under the pessimistic MDP. The paper also proposes a new uncertainty heuristic for offline RL based on Bayesian optimization."
1062,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,"This paper studies the problem of offline model-based reinforcement learning. The authors propose a Bayesian Optimization-based approach to address the issue of model uncertainty in offline RL. The main contribution of this paper is to study the total variation distance between the model and the MDP, which is the difference between the expected return and the pessimistic return of the model under the pessimistic MDP. The paper also proposes a new uncertainty heuristic for offline RL based on Bayesian optimization."
1063,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a method for experience replay in model-based reinforcement learning (MbRL). The main idea is to use a critic network to predict the Q-value of an experience. The critic network is trained to predict Q-values using the TD-error of the policy. The proposed method, MaPER, is evaluated on a variety of Mujoco tasks."
1064,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a method for experience replay in model-based reinforcement learning (MbRL). The main idea is to use a critic network to predict the Q-value of an experience. The critic network is trained to predict Q-values using the TD-error of the policy. The proposed method, MaPER, is evaluated on a variety of Mujoco tasks."
1065,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a method for experience replay in model-based reinforcement learning (MbRL). The main idea is to use a critic network to predict the Q-value of an experience. The critic network is trained to predict Q-values using the TD-error of the policy. The proposed method, MaPER, is evaluated on a variety of Mujoco tasks."
1066,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"This paper proposes a method for experience replay in model-based reinforcement learning (MbRL). The main idea is to use a critic network to predict the Q-value of an experience. The critic network is trained to predict Q-values using the TD-error of the policy. The proposed method, MaPER, is evaluated on a variety of Mujoco tasks."
1067,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a method for improving the sample efficiency of reinforcement learning agents in the presence of human intervention. The main idea is to use a proxy state-action value function that is learned from a partial demonstration of the agent’s actions. This proxy value function is then used to train the agent to maximize the value of the proxy state action. The proposed method is evaluated on a safe driving benchmark and compared to several imitation learning baselines.
1068,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a method for improving the sample efficiency of reinforcement learning agents in the presence of human intervention. The main idea is to use a proxy state-action value function that is learned from a partial demonstration of the agent’s actions. This proxy value function is then used to train the agent to maximize the value of the proxy state action. The proposed method is evaluated on a safe driving benchmark and compared to several imitation learning baselines.
1069,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a method for improving the sample efficiency of reinforcement learning agents in the presence of human intervention. The main idea is to use a proxy state-action value function that is learned from a partial demonstration of the agent’s actions. This proxy value function is then used to train the agent to maximize the value of the proxy state action. The proposed method is evaluated on a safe driving benchmark and compared to several imitation learning baselines.
1070,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposes a method for improving the sample efficiency of reinforcement learning agents in the presence of human intervention. The main idea is to use a proxy state-action value function that is learned from a partial demonstration of the agent’s actions. This proxy value function is then used to train the agent to maximize the value of the proxy state action. The proposed method is evaluated on a safe driving benchmark and compared to several imitation learning baselines.
1071,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes a hierarchical meta imitation learning method, Dual Meta Imitation Learning (DMIL), which uses a high-level network to learn modular sub-skills for long-horizon imitation learning. The high level network is trained to adapt to new tasks, while the low level network adapts to the new task. The proposed method is evaluated on a few-shot imitation learning task in a kitchen environment. The experiments show that the proposed method outperforms the state-of-the-art methods."
1072,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes a hierarchical meta imitation learning method, Dual Meta Imitation Learning (DMIL), which uses a high-level network to learn modular sub-skills for long-horizon imitation learning. The high level network is trained to adapt to new tasks, while the low level network adapts to the new task. The proposed method is evaluated on a few-shot imitation learning task in a kitchen environment. The experiments show that the proposed method outperforms the state-of-the-art methods."
1073,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes a hierarchical meta imitation learning method, Dual Meta Imitation Learning (DMIL), which uses a high-level network to learn modular sub-skills for long-horizon imitation learning. The high level network is trained to adapt to new tasks, while the low level network adapts to the new task. The proposed method is evaluated on a few-shot imitation learning task in a kitchen environment. The experiments show that the proposed method outperforms the state-of-the-art methods."
1074,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,"This paper proposes a hierarchical meta imitation learning method, Dual Meta Imitation Learning (DMIL), which uses a high-level network to learn modular sub-skills for long-horizon imitation learning. The high level network is trained to adapt to new tasks, while the low level network adapts to the new task. The proposed method is evaluated on a few-shot imitation learning task in a kitchen environment. The experiments show that the proposed method outperforms the state-of-the-art methods."
1075,SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a node embedding compression method for graph neural networks (GNNs). The proposed method is based on the idea that node embeddings can be decomposed into a bit vector and a float-point vector, which can then be used to compress the input node representation. The authors show that the proposed method can be applied to both GNNs as well as graph embedding methods, and that it is able to achieve better compression performance than existing methods. "
1076,SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a node embedding compression method for graph neural networks (GNNs). The proposed method is based on the idea that node embeddings can be decomposed into a bit vector and a float-point vector, which can then be used to compress the input node representation. The authors show that the proposed method can be applied to both GNNs as well as graph embedding methods, and that it is able to achieve better compression performance than existing methods. "
1077,SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a node embedding compression method for graph neural networks (GNNs). The proposed method is based on the idea that node embeddings can be decomposed into a bit vector and a float-point vector, which can then be used to compress the input node representation. The authors show that the proposed method can be applied to both GNNs as well as graph embedding methods, and that it is able to achieve better compression performance than existing methods. "
1078,SP:fb0efa670729796471a7a562b231172103bb8749,"This paper proposes a node embedding compression method for graph neural networks (GNNs). The proposed method is based on the idea that node embeddings can be decomposed into a bit vector and a float-point vector, which can then be used to compress the input node representation. The authors show that the proposed method can be applied to both GNNs as well as graph embedding methods, and that it is able to achieve better compression performance than existing methods. "
1079,SP:15c243829ed3b2505ed1e122bd499089f8a862da,This paper studies the problem of learning invariant representations for domain adaptation. The authors propose to use gradient descent and high-order ODE solvers to solve the problem. They provide asymptotic convergence guarantees for the optimizer and show that it converges to local Nash equilibria. They also provide a drop-in replacement method to improve the learning rates of the optimizers.
1080,SP:15c243829ed3b2505ed1e122bd499089f8a862da,This paper studies the problem of learning invariant representations for domain adaptation. The authors propose to use gradient descent and high-order ODE solvers to solve the problem. They provide asymptotic convergence guarantees for the optimizer and show that it converges to local Nash equilibria. They also provide a drop-in replacement method to improve the learning rates of the optimizers.
1081,SP:15c243829ed3b2505ed1e122bd499089f8a862da,This paper studies the problem of learning invariant representations for domain adaptation. The authors propose to use gradient descent and high-order ODE solvers to solve the problem. They provide asymptotic convergence guarantees for the optimizer and show that it converges to local Nash equilibria. They also provide a drop-in replacement method to improve the learning rates of the optimizers.
1082,SP:15c243829ed3b2505ed1e122bd499089f8a862da,This paper studies the problem of learning invariant representations for domain adaptation. The authors propose to use gradient descent and high-order ODE solvers to solve the problem. They provide asymptotic convergence guarantees for the optimizer and show that it converges to local Nash equilibria. They also provide a drop-in replacement method to improve the learning rates of the optimizers.
1083,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularization method called Individual Flooding (iFlood), which is based on the idea of individual Flooding. The idea is to use instance-level constraints on the training loss function to force the model to be more robust to under-fitted instances. The method is evaluated on image classification and language understanding tasks and compared to other regularization methods. "
1084,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularization method called Individual Flooding (iFlood), which is based on the idea of individual Flooding. The idea is to use instance-level constraints on the training loss function to force the model to be more robust to under-fitted instances. The method is evaluated on image classification and language understanding tasks and compared to other regularization methods. "
1085,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularization method called Individual Flooding (iFlood), which is based on the idea of individual Flooding. The idea is to use instance-level constraints on the training loss function to force the model to be more robust to under-fitted instances. The method is evaluated on image classification and language understanding tasks and compared to other regularization methods. "
1086,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper proposes a new regularization method called Individual Flooding (iFlood), which is based on the idea of individual Flooding. The idea is to use instance-level constraints on the training loss function to force the model to be more robust to under-fitted instances. The method is evaluated on image classification and language understanding tasks and compared to other regularization methods. "
1087,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,This paper proposes a Hierarchical Reinforcement Learning (HERL) method to learn low-level skills for long-horizon tasks. The key idea is to learn a lower-level skill that represents the value function of the higher level skill. This is achieved by using a value function space to represent the value functions of the lower level skills. The paper shows that the proposed method can achieve better zero-shot generalization compared to model-free and model-based methods.
1088,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,This paper proposes a Hierarchical Reinforcement Learning (HERL) method to learn low-level skills for long-horizon tasks. The key idea is to learn a lower-level skill that represents the value function of the higher level skill. This is achieved by using a value function space to represent the value functions of the lower level skills. The paper shows that the proposed method can achieve better zero-shot generalization compared to model-free and model-based methods.
1089,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,This paper proposes a Hierarchical Reinforcement Learning (HERL) method to learn low-level skills for long-horizon tasks. The key idea is to learn a lower-level skill that represents the value function of the higher level skill. This is achieved by using a value function space to represent the value functions of the lower level skills. The paper shows that the proposed method can achieve better zero-shot generalization compared to model-free and model-based methods.
1090,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,This paper proposes a Hierarchical Reinforcement Learning (HERL) method to learn low-level skills for long-horizon tasks. The key idea is to learn a lower-level skill that represents the value function of the higher level skill. This is achieved by using a value function space to represent the value functions of the lower level skills. The paper shows that the proposed method can achieve better zero-shot generalization compared to model-free and model-based methods.
1091,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for variational autoencoders (VAE) and generative adversarial networks (GANs). The proposed method is based on Transformer layers, graph neural networks, and the prior vector. The authors show that the proposed method outperforms the state-of-the-art one-shot generation methods on SetMNIST and QM9 datasets."
1092,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for variational autoencoders (VAE) and generative adversarial networks (GANs). The proposed method is based on Transformer layers, graph neural networks, and the prior vector. The authors show that the proposed method outperforms the state-of-the-art one-shot generation methods on SetMNIST and QM9 datasets."
1093,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for variational autoencoders (VAE) and generative adversarial networks (GANs). The proposed method is based on Transformer layers, graph neural networks, and the prior vector. The authors show that the proposed method outperforms the state-of-the-art one-shot generation methods on SetMNIST and QM9 datasets."
1094,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper proposes Top-n, a deterministic, non-exchangeable set creation mechanism for variational autoencoders (VAE) and generative adversarial networks (GANs). The proposed method is based on Transformer layers, graph neural networks, and the prior vector. The authors show that the proposed method outperforms the state-of-the-art one-shot generation methods on SetMNIST and QM9 datasets."
1095,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the problem of solving elliptic partial differential equations (PDEs) using deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). In particular, the authors consider the Schrödinger equation, which is a prototype elliptic PDE with a zero Dirichlet boundary condition. The authors show that the Ritz method and PINN can be used to derive lower bounds on the rate generalization bound for the problem, and the authors also provide a lower bound on the dimension dependent power law for deep PDE solvers. In addition, they provide an upper bound of the rate of convergence of the deep model to a Sobolev space."
1096,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the problem of solving elliptic partial differential equations (PDEs) using deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). In particular, the authors consider the Schrödinger equation, which is a prototype elliptic PDE with a zero Dirichlet boundary condition. The authors show that the Ritz method and PINN can be used to derive lower bounds on the rate generalization bound for the problem, and the authors also provide a lower bound on the dimension dependent power law for deep PDE solvers. In addition, they provide an upper bound of the rate of convergence of the deep model to a Sobolev space."
1097,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the problem of solving elliptic partial differential equations (PDEs) using deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). In particular, the authors consider the Schrödinger equation, which is a prototype elliptic PDE with a zero Dirichlet boundary condition. The authors show that the Ritz method and PINN can be used to derive lower bounds on the rate generalization bound for the problem, and the authors also provide a lower bound on the dimension dependent power law for deep PDE solvers. In addition, they provide an upper bound of the rate of convergence of the deep model to a Sobolev space."
1098,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the problem of solving elliptic partial differential equations (PDEs) using deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINN). In particular, the authors consider the Schrödinger equation, which is a prototype elliptic PDE with a zero Dirichlet boundary condition. The authors show that the Ritz method and PINN can be used to derive lower bounds on the rate generalization bound for the problem, and the authors also provide a lower bound on the dimension dependent power law for deep PDE solvers. In addition, they provide an upper bound of the rate of convergence of the deep model to a Sobolev space."
1099,SP:80614db60d27a48c3c1b1882844e298666b798d4,This paper studies the relationship between robustness and generalization in the context of adversarial training and data augmentation. The authors show that the Jacobian norm of the last layer and the data augmentations (DA) are sufficient conditions for robustness to generalize. They also show that DA can be used to regularize the function class regularization process.
1100,SP:80614db60d27a48c3c1b1882844e298666b798d4,This paper studies the relationship between robustness and generalization in the context of adversarial training and data augmentation. The authors show that the Jacobian norm of the last layer and the data augmentations (DA) are sufficient conditions for robustness to generalize. They also show that DA can be used to regularize the function class regularization process.
1101,SP:80614db60d27a48c3c1b1882844e298666b798d4,This paper studies the relationship between robustness and generalization in the context of adversarial training and data augmentation. The authors show that the Jacobian norm of the last layer and the data augmentations (DA) are sufficient conditions for robustness to generalize. They also show that DA can be used to regularize the function class regularization process.
1102,SP:80614db60d27a48c3c1b1882844e298666b798d4,This paper studies the relationship between robustness and generalization in the context of adversarial training and data augmentation. The authors show that the Jacobian norm of the last layer and the data augmentations (DA) are sufficient conditions for robustness to generalize. They also show that DA can be used to regularize the function class regularization process.
1103,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of meta-learning for few-shot learning. The authors propose a new method called deconfounder, which aims to improve the memorization performance of the meta-learner. The main contribution of the paper is the introduction of a causal perspective to explain the effect of the label space on memorization. The causal perspective is based on the fact that the label spaces of different tasks have different labels, and the authors propose to use the causal perspective as a regularizer to improve memorization of the labels. The paper also proposes to use dropout as an augmentation to the regularizer. The experimental results show that the proposed method outperforms the baselines on several benchmark datasets."
1104,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of meta-learning for few-shot learning. The authors propose a new method called deconfounder, which aims to improve the memorization performance of the meta-learner. The main contribution of the paper is the introduction of a causal perspective to explain the effect of the label space on memorization. The causal perspective is based on the fact that the label spaces of different tasks have different labels, and the authors propose to use the causal perspective as a regularizer to improve memorization of the labels. The paper also proposes to use dropout as an augmentation to the regularizer. The experimental results show that the proposed method outperforms the baselines on several benchmark datasets."
1105,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of meta-learning for few-shot learning. The authors propose a new method called deconfounder, which aims to improve the memorization performance of the meta-learner. The main contribution of the paper is the introduction of a causal perspective to explain the effect of the label space on memorization. The causal perspective is based on the fact that the label spaces of different tasks have different labels, and the authors propose to use the causal perspective as a regularizer to improve memorization of the labels. The paper also proposes to use dropout as an augmentation to the regularizer. The experimental results show that the proposed method outperforms the baselines on several benchmark datasets."
1106,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper studies the problem of meta-learning for few-shot learning. The authors propose a new method called deconfounder, which aims to improve the memorization performance of the meta-learner. The main contribution of the paper is the introduction of a causal perspective to explain the effect of the label space on memorization. The causal perspective is based on the fact that the label spaces of different tasks have different labels, and the authors propose to use the causal perspective as a regularizer to improve memorization of the labels. The paper also proposes to use dropout as an augmentation to the regularizer. The experimental results show that the proposed method outperforms the baselines on several benchmark datasets."
1107,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method is based on an information-based regularizer that learns a proxy representation of the learned variables. This proxy representation is then used to train an autonomous agent. The experimental results show that the proposed method outperforms several baselines.
1108,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method is based on an information-based regularizer that learns a proxy representation of the learned variables. This proxy representation is then used to train an autonomous agent. The experimental results show that the proposed method outperforms several baselines.
1109,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method is based on an information-based regularizer that learns a proxy representation of the learned variables. This proxy representation is then used to train an autonomous agent. The experimental results show that the proposed method outperforms several baselines.
1110,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,This paper proposes a reinforcement learning framework for ad hoc teamwork. The proposed method is based on an information-based regularizer that learns a proxy representation of the learned variables. This proxy representation is then used to train an autonomous agent. The experimental results show that the proposed method outperforms several baselines.
1111,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The main idea is to use a normalizing flow (NF) model to model the data space, and then use Expectation-Maximization (EM) algorithm to estimate the model parameters. The authors show that EMFlow outperforms existing methods on both multivariate and image datasets."
1112,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The main idea is to use a normalizing flow (NF) model to model the data space, and then use Expectation-Maximization (EM) algorithm to estimate the model parameters. The authors show that EMFlow outperforms existing methods on both multivariate and image datasets."
1113,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The main idea is to use a normalizing flow (NF) model to model the data space, and then use Expectation-Maximization (EM) algorithm to estimate the model parameters. The authors show that EMFlow outperforms existing methods on both multivariate and image datasets."
1114,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper proposes a new method for imputation of missing values in high-dimensional data. The main idea is to use a normalizing flow (NF) model to model the data space, and then use Expectation-Maximization (EM) algorithm to estimate the model parameters. The authors show that EMFlow outperforms existing methods on both multivariate and image datasets."
1115,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a deep linear network (DLGN) based on ReLUs. The authors show that DLGN can be viewed as a neural path kernel (NPK) with gates and convolutional layers, and that it can be used as a deep neural network (DNN). The authors also show that the weights of DLGN are more interpretable and disentangled than those of DNNs. In addition, the authors provide a theoretical analysis of the dual and primal linearity of the path space of the weights network. The experiments on CIFAR-10, Cifar-100, and ImageNet demonstrate the effectiveness of the proposed method."
1116,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a deep linear network (DLGN) based on ReLUs. The authors show that DLGN can be viewed as a neural path kernel (NPK) with gates and convolutional layers, and that it can be used as a deep neural network (DNN). The authors also show that the weights of DLGN are more interpretable and disentangled than those of DNNs. In addition, the authors provide a theoretical analysis of the dual and primal linearity of the path space of the weights network. The experiments on CIFAR-10, Cifar-100, and ImageNet demonstrate the effectiveness of the proposed method."
1117,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a deep linear network (DLGN) based on ReLUs. The authors show that DLGN can be viewed as a neural path kernel (NPK) with gates and convolutional layers, and that it can be used as a deep neural network (DNN). The authors also show that the weights of DLGN are more interpretable and disentangled than those of DNNs. In addition, the authors provide a theoretical analysis of the dual and primal linearity of the path space of the weights network. The experiments on CIFAR-10, Cifar-100, and ImageNet demonstrate the effectiveness of the proposed method."
1118,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"This paper proposes a deep linear network (DLGN) based on ReLUs. The authors show that DLGN can be viewed as a neural path kernel (NPK) with gates and convolutional layers, and that it can be used as a deep neural network (DNN). The authors also show that the weights of DLGN are more interpretable and disentangled than those of DNNs. In addition, the authors provide a theoretical analysis of the dual and primal linearity of the path space of the weights network. The experiments on CIFAR-10, Cifar-100, and ImageNet demonstrate the effectiveness of the proposed method."
1119,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The proposed method is based on the idea of dynamic token normalization (DNT), which is an extension of Layer Normalization. The main idea of DTN is to use a combination of DNT and LN to reduce the computational cost of the transformer. Experiments on the Long-Range Arena show that the proposed method outperforms the state-of-the-art."
1120,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The proposed method is based on the idea of dynamic token normalization (DNT), which is an extension of Layer Normalization. The main idea of DTN is to use a combination of DNT and LN to reduce the computational cost of the transformer. Experiments on the Long-Range Arena show that the proposed method outperforms the state-of-the-art."
1121,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The proposed method is based on the idea of dynamic token normalization (DNT), which is an extension of Layer Normalization. The main idea of DTN is to use a combination of DNT and LN to reduce the computational cost of the transformer. Experiments on the Long-Range Arena show that the proposed method outperforms the state-of-the-art."
1122,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper proposes Dynamic Token Normalization (DTN), a new normalization method for vision transformers. The proposed method is based on the idea of dynamic token normalization (DNT), which is an extension of Layer Normalization. The main idea of DTN is to use a combination of DNT and LN to reduce the computational cost of the transformer. Experiments on the Long-Range Arena show that the proposed method outperforms the state-of-the-art."
1123,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,This paper studies the spectral bias of deep learning models. The authors propose a regularization method to improve the spectral performance of deep neural networks. The proposed regularization is based on the idea that low frequencies of the input should be used to train the network. The experiments show that the proposed regularizer improves the performance of the proposed deep neural network.
1124,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,This paper studies the spectral bias of deep learning models. The authors propose a regularization method to improve the spectral performance of deep neural networks. The proposed regularization is based on the idea that low frequencies of the input should be used to train the network. The experiments show that the proposed regularizer improves the performance of the proposed deep neural network.
1125,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,This paper studies the spectral bias of deep learning models. The authors propose a regularization method to improve the spectral performance of deep neural networks. The proposed regularization is based on the idea that low frequencies of the input should be used to train the network. The experiments show that the proposed regularizer improves the performance of the proposed deep neural network.
1126,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,This paper studies the spectral bias of deep learning models. The authors propose a regularization method to improve the spectral performance of deep neural networks. The proposed regularization is based on the idea that low frequencies of the input should be used to train the network. The experiments show that the proposed regularizer improves the performance of the proposed deep neural network.
1127,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper studies the problem of two-mode exploration in reinforcement learning. In particular, the authors propose a novel algorithm for switching between two modes of exploration. The proposed algorithm is based on the idea that switching between modes can be done in an episodic manner. The authors show that the proposed algorithm can be applied to a variety of Atari games, and show that it is able to achieve better performance than the state-of-the-art on Atari games."
1128,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper studies the problem of two-mode exploration in reinforcement learning. In particular, the authors propose a novel algorithm for switching between two modes of exploration. The proposed algorithm is based on the idea that switching between modes can be done in an episodic manner. The authors show that the proposed algorithm can be applied to a variety of Atari games, and show that it is able to achieve better performance than the state-of-the-art on Atari games."
1129,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper studies the problem of two-mode exploration in reinforcement learning. In particular, the authors propose a novel algorithm for switching between two modes of exploration. The proposed algorithm is based on the idea that switching between modes can be done in an episodic manner. The authors show that the proposed algorithm can be applied to a variety of Atari games, and show that it is able to achieve better performance than the state-of-the-art on Atari games."
1130,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper studies the problem of two-mode exploration in reinforcement learning. In particular, the authors propose a novel algorithm for switching between two modes of exploration. The proposed algorithm is based on the idea that switching between modes can be done in an episodic manner. The authors show that the proposed algorithm can be applied to a variety of Atari games, and show that it is able to achieve better performance than the state-of-the-art on Atari games."
1131,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper studies the problem of initialization for k-median clustering in the metric space. The authors propose a new initialization scheme based on a metric embedding tree structure, which is motivated by differential privacy (DP) and HST initialization. The main contribution of this paper is to provide an initialization scheme that is DP-agnostic and can be used in the non-DP setting. In particular, the authors show that the proposed initialization scheme can be combined with DP local search to obtain the initial centers of the clustering algorithm. "
1132,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper studies the problem of initialization for k-median clustering in the metric space. The authors propose a new initialization scheme based on a metric embedding tree structure, which is motivated by differential privacy (DP) and HST initialization. The main contribution of this paper is to provide an initialization scheme that is DP-agnostic and can be used in the non-DP setting. In particular, the authors show that the proposed initialization scheme can be combined with DP local search to obtain the initial centers of the clustering algorithm. "
1133,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper studies the problem of initialization for k-median clustering in the metric space. The authors propose a new initialization scheme based on a metric embedding tree structure, which is motivated by differential privacy (DP) and HST initialization. The main contribution of this paper is to provide an initialization scheme that is DP-agnostic and can be used in the non-DP setting. In particular, the authors show that the proposed initialization scheme can be combined with DP local search to obtain the initial centers of the clustering algorithm. "
1134,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper studies the problem of initialization for k-median clustering in the metric space. The authors propose a new initialization scheme based on a metric embedding tree structure, which is motivated by differential privacy (DP) and HST initialization. The main contribution of this paper is to provide an initialization scheme that is DP-agnostic and can be used in the non-DP setting. In particular, the authors show that the proposed initialization scheme can be combined with DP local search to obtain the initial centers of the clustering algorithm. "
1135,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a method for video prediction. The authors propose a new architecture, FitVid, which is based on the idea of using an image augmentation technique to improve the performance of the model. The proposed method is evaluated on a variety of video prediction benchmarks and compared to several state-of-the-art models. The results show that the proposed method outperforms the existing models."
1136,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a method for video prediction. The authors propose a new architecture, FitVid, which is based on the idea of using an image augmentation technique to improve the performance of the model. The proposed method is evaluated on a variety of video prediction benchmarks and compared to several state-of-the-art models. The results show that the proposed method outperforms the existing models."
1137,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a method for video prediction. The authors propose a new architecture, FitVid, which is based on the idea of using an image augmentation technique to improve the performance of the model. The proposed method is evaluated on a variety of video prediction benchmarks and compared to several state-of-the-art models. The results show that the proposed method outperforms the existing models."
1138,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper proposes a method for video prediction. The authors propose a new architecture, FitVid, which is based on the idea of using an image augmentation technique to improve the performance of the model. The proposed method is evaluated on a variety of video prediction benchmarks and compared to several state-of-the-art models. The results show that the proposed method outperforms the existing models."
1139,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the problem of estimating the test loss of SGD under the data structure of the data distribution. In particular, the authors consider the case where the data is Gaussian and the test distribution is non-linear. The authors propose to use a Gaussian model as the model for SGD, and then use a non-Gaussian model to estimate the test error. The main contribution of this paper is to provide a theoretical analysis of the training and test error of SGDL. The theoretical analysis is based on the fact that the test covariance matrix can be expressed as a function of the feature correlation matrix of the input data, and the authors show that the optimal batch size for SGDL can depend on the correlation matrix and the number of samples in the training set. "
1140,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the problem of estimating the test loss of SGD under the data structure of the data distribution. In particular, the authors consider the case where the data is Gaussian and the test distribution is non-linear. The authors propose to use a Gaussian model as the model for SGD, and then use a non-Gaussian model to estimate the test error. The main contribution of this paper is to provide a theoretical analysis of the training and test error of SGDL. The theoretical analysis is based on the fact that the test covariance matrix can be expressed as a function of the feature correlation matrix of the input data, and the authors show that the optimal batch size for SGDL can depend on the correlation matrix and the number of samples in the training set. "
1141,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the problem of estimating the test loss of SGD under the data structure of the data distribution. In particular, the authors consider the case where the data is Gaussian and the test distribution is non-linear. The authors propose to use a Gaussian model as the model for SGD, and then use a non-Gaussian model to estimate the test error. The main contribution of this paper is to provide a theoretical analysis of the training and test error of SGDL. The theoretical analysis is based on the fact that the test covariance matrix can be expressed as a function of the feature correlation matrix of the input data, and the authors show that the optimal batch size for SGDL can depend on the correlation matrix and the number of samples in the training set. "
1142,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"This paper studies the problem of estimating the test loss of SGD under the data structure of the data distribution. In particular, the authors consider the case where the data is Gaussian and the test distribution is non-linear. The authors propose to use a Gaussian model as the model for SGD, and then use a non-Gaussian model to estimate the test error. The main contribution of this paper is to provide a theoretical analysis of the training and test error of SGDL. The theoretical analysis is based on the fact that the test covariance matrix can be expressed as a function of the feature correlation matrix of the input data, and the authors show that the optimal batch size for SGDL can depend on the correlation matrix and the number of samples in the training set. "
1143,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the convergence of SGD to sharp minima in a nonlinear, nonconvex problem. The authors show that SGD converges to local minima when the learning rate and the model are small, and that the sharpness of the minima is proportional to the number of parameters of the network. They also show that AMSGrad is a minimal neural network-like construction that can be used to approximate the sharpest minima. The paper is well-written and easy to follow."
1144,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the convergence of SGD to sharp minima in a nonlinear, nonconvex problem. The authors show that SGD converges to local minima when the learning rate and the model are small, and that the sharpness of the minima is proportional to the number of parameters of the network. They also show that AMSGrad is a minimal neural network-like construction that can be used to approximate the sharpest minima. The paper is well-written and easy to follow."
1145,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the convergence of SGD to sharp minima in a nonlinear, nonconvex problem. The authors show that SGD converges to local minima when the learning rate and the model are small, and that the sharpness of the minima is proportional to the number of parameters of the network. They also show that AMSGrad is a minimal neural network-like construction that can be used to approximate the sharpest minima. The paper is well-written and easy to follow."
1146,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"This paper studies the convergence of SGD to sharp minima in a nonlinear, nonconvex problem. The authors show that SGD converges to local minima when the learning rate and the model are small, and that the sharpness of the minima is proportional to the number of parameters of the network. They also show that AMSGrad is a minimal neural network-like construction that can be used to approximate the sharpest minima. The paper is well-written and easy to follow."
1147,SP:22d01913b78ef447b064c65a646fa301b861d3f7,This paper proposes a method for online hyperparameter optimization (HO) based on gradient-based meta-learning. The main idea is to use the Jacobian-vector product (JVP) as a second-order term in the inner optimization step of the HO step. The proposed method is evaluated on several benchmark datasets and compared to other methods.
1148,SP:22d01913b78ef447b064c65a646fa301b861d3f7,This paper proposes a method for online hyperparameter optimization (HO) based on gradient-based meta-learning. The main idea is to use the Jacobian-vector product (JVP) as a second-order term in the inner optimization step of the HO step. The proposed method is evaluated on several benchmark datasets and compared to other methods.
1149,SP:22d01913b78ef447b064c65a646fa301b861d3f7,This paper proposes a method for online hyperparameter optimization (HO) based on gradient-based meta-learning. The main idea is to use the Jacobian-vector product (JVP) as a second-order term in the inner optimization step of the HO step. The proposed method is evaluated on several benchmark datasets and compared to other methods.
1150,SP:22d01913b78ef447b064c65a646fa301b861d3f7,This paper proposes a method for online hyperparameter optimization (HO) based on gradient-based meta-learning. The main idea is to use the Jacobian-vector product (JVP) as a second-order term in the inner optimization step of the HO step. The proposed method is evaluated on several benchmark datasets and compared to other methods.
1151,SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a method for meta-learning of new tasks. The main idea is to use a global meta-learner to learn a task-specific representation for each new task, and then use a meta-path learner to map the learned representation to the task representation of the base learner. The proposed method, called CTML, is evaluated on a few-shot image classification task and a cold-start recommendation task. The results show that the proposed method outperforms the baselines."
1152,SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a method for meta-learning of new tasks. The main idea is to use a global meta-learner to learn a task-specific representation for each new task, and then use a meta-path learner to map the learned representation to the task representation of the base learner. The proposed method, called CTML, is evaluated on a few-shot image classification task and a cold-start recommendation task. The results show that the proposed method outperforms the baselines."
1153,SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a method for meta-learning of new tasks. The main idea is to use a global meta-learner to learn a task-specific representation for each new task, and then use a meta-path learner to map the learned representation to the task representation of the base learner. The proposed method, called CTML, is evaluated on a few-shot image classification task and a cold-start recommendation task. The results show that the proposed method outperforms the baselines."
1154,SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a method for meta-learning of new tasks. The main idea is to use a global meta-learner to learn a task-specific representation for each new task, and then use a meta-path learner to map the learned representation to the task representation of the base learner. The proposed method, called CTML, is evaluated on a few-shot image classification task and a cold-start recommendation task. The results show that the proposed method outperforms the baselines."
1155,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND). The proposed method is based on the idea of regularizing the weights of an ensemble of neural networks. The authors show that this regularization can be applied to both labeled and unlabeled data, and that it can improve the performance of the proposed method."
1156,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND). The proposed method is based on the idea of regularizing the weights of an ensemble of neural networks. The authors show that this regularization can be applied to both labeled and unlabeled data, and that it can improve the performance of the proposed method."
1157,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND). The proposed method is based on the idea of regularizing the weights of an ensemble of neural networks. The authors show that this regularization can be applied to both labeled and unlabeled data, and that it can improve the performance of the proposed method."
1158,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"This paper proposes an ensemble-based method for semi-supervised novelty detection (SSND). The proposed method is based on the idea of regularizing the weights of an ensemble of neural networks. The authors show that this regularization can be applied to both labeled and unlabeled data, and that it can improve the performance of the proposed method."
1159,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a method for multi-agent trajectory prediction in Omniglot data. The proposed method is based on the latent variable sequential set transformer (LVSTT) architecture, which is an extension of the Latent Variable Sequential Set Transformers (LVSSTT). The proposed model is able to model both temporal and social dimensions of the trajectories. The authors also show that the model is equivariant to the number of agents. The experimental results on the nuScenes vehicle motion prediction leaderboard demonstrate the effectiveness of the proposed method."
1160,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a method for multi-agent trajectory prediction in Omniglot data. The proposed method is based on the latent variable sequential set transformer (LVSTT) architecture, which is an extension of the Latent Variable Sequential Set Transformers (LVSSTT). The proposed model is able to model both temporal and social dimensions of the trajectories. The authors also show that the model is equivariant to the number of agents. The experimental results on the nuScenes vehicle motion prediction leaderboard demonstrate the effectiveness of the proposed method."
1161,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a method for multi-agent trajectory prediction in Omniglot data. The proposed method is based on the latent variable sequential set transformer (LVSTT) architecture, which is an extension of the Latent Variable Sequential Set Transformers (LVSSTT). The proposed model is able to model both temporal and social dimensions of the trajectories. The authors also show that the model is equivariant to the number of agents. The experimental results on the nuScenes vehicle motion prediction leaderboard demonstrate the effectiveness of the proposed method."
1162,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,"This paper proposes a method for multi-agent trajectory prediction in Omniglot data. The proposed method is based on the latent variable sequential set transformer (LVSTT) architecture, which is an extension of the Latent Variable Sequential Set Transformers (LVSSTT). The proposed model is able to model both temporal and social dimensions of the trajectories. The authors also show that the model is equivariant to the number of agents. The experimental results on the nuScenes vehicle motion prediction leaderboard demonstrate the effectiveness of the proposed method."
1163,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper proposes a method for generating counterfactual explanations for image classification models. The idea is to use an invertible neural network to generate a synthetic dataset, which is then used to train a classifier that is trained on the generated dataset. The proposed method is evaluated on a variety of image classification tasks, and compared to baseline explanations and concept-based explanations. The results show that the proposed method outperforms the baseline explanation method."
1164,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper proposes a method for generating counterfactual explanations for image classification models. The idea is to use an invertible neural network to generate a synthetic dataset, which is then used to train a classifier that is trained on the generated dataset. The proposed method is evaluated on a variety of image classification tasks, and compared to baseline explanations and concept-based explanations. The results show that the proposed method outperforms the baseline explanation method."
1165,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper proposes a method for generating counterfactual explanations for image classification models. The idea is to use an invertible neural network to generate a synthetic dataset, which is then used to train a classifier that is trained on the generated dataset. The proposed method is evaluated on a variety of image classification tasks, and compared to baseline explanations and concept-based explanations. The results show that the proposed method outperforms the baseline explanation method."
1166,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"This paper proposes a method for generating counterfactual explanations for image classification models. The idea is to use an invertible neural network to generate a synthetic dataset, which is then used to train a classifier that is trained on the generated dataset. The proposed method is evaluated on a variety of image classification tasks, and compared to baseline explanations and concept-based explanations. The results show that the proposed method outperforms the baseline explanation method."
1167,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes a method for backdoor data poisoning attacks against deep neural networks. The main idea is to train an ensemble of weak learners on poisoned data and then use a boosting framework to extract clean data from the poisoned data. The proposed method is based on a bootstrapped measure of generalization, which is used to evaluate the generalization of the network. Experiments are conducted on a few datasets and show that the proposed method outperforms existing defenses."
1168,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes a method for backdoor data poisoning attacks against deep neural networks. The main idea is to train an ensemble of weak learners on poisoned data and then use a boosting framework to extract clean data from the poisoned data. The proposed method is based on a bootstrapped measure of generalization, which is used to evaluate the generalization of the network. Experiments are conducted on a few datasets and show that the proposed method outperforms existing defenses."
1169,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes a method for backdoor data poisoning attacks against deep neural networks. The main idea is to train an ensemble of weak learners on poisoned data and then use a boosting framework to extract clean data from the poisoned data. The proposed method is based on a bootstrapped measure of generalization, which is used to evaluate the generalization of the network. Experiments are conducted on a few datasets and show that the proposed method outperforms existing defenses."
1170,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper proposes a method for backdoor data poisoning attacks against deep neural networks. The main idea is to train an ensemble of weak learners on poisoned data and then use a boosting framework to extract clean data from the poisoned data. The proposed method is based on a bootstrapped measure of generalization, which is used to evaluate the generalization of the network. Experiments are conducted on a few datasets and show that the proposed method outperforms existing defenses."
1171,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a new model for multi-label text classification. The model is based on the idea of latent-label encodings, which can be used to model label correlations between text tokens. The authors show that the proposed model is able to outperform the state-of-the-art BERT model on several benchmark datasets. "
1172,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a new model for multi-label text classification. The model is based on the idea of latent-label encodings, which can be used to model label correlations between text tokens. The authors show that the proposed model is able to outperform the state-of-the-art BERT model on several benchmark datasets. "
1173,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a new model for multi-label text classification. The model is based on the idea of latent-label encodings, which can be used to model label correlations between text tokens. The authors show that the proposed model is able to outperform the state-of-the-art BERT model on several benchmark datasets. "
1174,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"This paper proposes a new model for multi-label text classification. The model is based on the idea of latent-label encodings, which can be used to model label correlations between text tokens. The authors show that the proposed model is able to outperform the state-of-the-art BERT model on several benchmark datasets. "
1175,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the generalization properties of convolutional kernels in the context of RKHS. In particular, the authors consider the case where the convolution and pooling layers of the kernel are additive models of interaction terms. The authors show that the RkHS can be viewed as a regularization of the norm of the pooling and convolution layers. They also provide sample complexity bounds for the patches of the network."
1176,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the generalization properties of convolutional kernels in the context of RKHS. In particular, the authors consider the case where the convolution and pooling layers of the kernel are additive models of interaction terms. The authors show that the RkHS can be viewed as a regularization of the norm of the pooling and convolution layers. They also provide sample complexity bounds for the patches of the network."
1177,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the generalization properties of convolutional kernels in the context of RKHS. In particular, the authors consider the case where the convolution and pooling layers of the kernel are additive models of interaction terms. The authors show that the RkHS can be viewed as a regularization of the norm of the pooling and convolution layers. They also provide sample complexity bounds for the patches of the network."
1178,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"This paper studies the generalization properties of convolutional kernels in the context of RKHS. In particular, the authors consider the case where the convolution and pooling layers of the kernel are additive models of interaction terms. The authors show that the RkHS can be viewed as a regularization of the norm of the pooling and convolution layers. They also provide sample complexity bounds for the patches of the network."
1179,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of neural tangent kernel (NTK) computation in the finite width limit of neural networks. The main contribution of this paper is to provide a theoretical analysis of the computation complexity of NTK in the infinite width limit. In particular, the authors show that NTK computation in finite width networks is computationally tractable with the help of convolutions, attention, recurrence, and general-purpose JAX function transformations. The authors then propose a number of algorithms for compute and memory efficiency in NTK. The experiments show that the proposed algorithms outperform the state-of-the-art in terms of computational complexity."
1180,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of neural tangent kernel (NTK) computation in the finite width limit of neural networks. The main contribution of this paper is to provide a theoretical analysis of the computation complexity of NTK in the infinite width limit. In particular, the authors show that NTK computation in finite width networks is computationally tractable with the help of convolutions, attention, recurrence, and general-purpose JAX function transformations. The authors then propose a number of algorithms for compute and memory efficiency in NTK. The experiments show that the proposed algorithms outperform the state-of-the-art in terms of computational complexity."
1181,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of neural tangent kernel (NTK) computation in the finite width limit of neural networks. The main contribution of this paper is to provide a theoretical analysis of the computation complexity of NTK in the infinite width limit. In particular, the authors show that NTK computation in finite width networks is computationally tractable with the help of convolutions, attention, recurrence, and general-purpose JAX function transformations. The authors then propose a number of algorithms for compute and memory efficiency in NTK. The experiments show that the proposed algorithms outperform the state-of-the-art in terms of computational complexity."
1182,SP:7bee8d65c68765cbfe38767743fec27981879d34,"This paper studies the computational complexity of neural tangent kernel (NTK) computation in the finite width limit of neural networks. The main contribution of this paper is to provide a theoretical analysis of the computation complexity of NTK in the infinite width limit. In particular, the authors show that NTK computation in finite width networks is computationally tractable with the help of convolutions, attention, recurrence, and general-purpose JAX function transformations. The authors then propose a number of algorithms for compute and memory efficiency in NTK. The experiments show that the proposed algorithms outperform the state-of-the-art in terms of computational complexity."
1183,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes an offline constrained reinforcement learning (RL) algorithm called COptiDICE. The main idea is to learn a cost-conservative policy that maximizes the expected return of the policy in the offline constrained setting. The authors show that the cost upper bound of the proposed algorithm is $O(1/\sqrt{T})$, where $T$ is the number of episodes, and $\tilde{O}(T^{-1/T})$ is a function of $T$. The authors also provide a theoretical analysis of the error of their algorithm."
1184,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes an offline constrained reinforcement learning (RL) algorithm called COptiDICE. The main idea is to learn a cost-conservative policy that maximizes the expected return of the policy in the offline constrained setting. The authors show that the cost upper bound of the proposed algorithm is $O(1/\sqrt{T})$, where $T$ is the number of episodes, and $\tilde{O}(T^{-1/T})$ is a function of $T$. The authors also provide a theoretical analysis of the error of their algorithm."
1185,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes an offline constrained reinforcement learning (RL) algorithm called COptiDICE. The main idea is to learn a cost-conservative policy that maximizes the expected return of the policy in the offline constrained setting. The authors show that the cost upper bound of the proposed algorithm is $O(1/\sqrt{T})$, where $T$ is the number of episodes, and $\tilde{O}(T^{-1/T})$ is a function of $T$. The authors also provide a theoretical analysis of the error of their algorithm."
1186,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"This paper proposes an offline constrained reinforcement learning (RL) algorithm called COptiDICE. The main idea is to learn a cost-conservative policy that maximizes the expected return of the policy in the offline constrained setting. The authors show that the cost upper bound of the proposed algorithm is $O(1/\sqrt{T})$, where $T$ is the number of episodes, and $\tilde{O}(T^{-1/T})$ is a function of $T$. The authors also provide a theoretical analysis of the error of their algorithm."
1187,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a parallel training scheme for Gated Recurrent Unit (GRU) networks. The main idea is to use multigrid reduction in time (MGRIT) solver to solve the parallel training problem. The authors also propose a hierarchical correction of the hidden state to reduce the end-to-end communication. Experiments on the HMDB51 dataset demonstrate the effectiveness of the proposed parallel training method.
1188,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a parallel training scheme for Gated Recurrent Unit (GRU) networks. The main idea is to use multigrid reduction in time (MGRIT) solver to solve the parallel training problem. The authors also propose a hierarchical correction of the hidden state to reduce the end-to-end communication. Experiments on the HMDB51 dataset demonstrate the effectiveness of the proposed parallel training method.
1189,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a parallel training scheme for Gated Recurrent Unit (GRU) networks. The main idea is to use multigrid reduction in time (MGRIT) solver to solve the parallel training problem. The authors also propose a hierarchical correction of the hidden state to reduce the end-to-end communication. Experiments on the HMDB51 dataset demonstrate the effectiveness of the proposed parallel training method.
1190,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,This paper proposes a parallel training scheme for Gated Recurrent Unit (GRU) networks. The main idea is to use multigrid reduction in time (MGRIT) solver to solve the parallel training problem. The authors also propose a hierarchical correction of the hidden state to reduce the end-to-end communication. Experiments on the HMDB51 dataset demonstrate the effectiveness of the proposed parallel training method.
1191,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method for cross-subject translation of fMRI signals. The authors propose to use a neural network to learn a common embedding of the input signal, which is then used to train a classifier to classify the stimulus features. The proposed method is based on PCA and SRM, and the authors show that the proposed method outperforms the existing methods in terms of classification accuracy. "
1192,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method for cross-subject translation of fMRI signals. The authors propose to use a neural network to learn a common embedding of the input signal, which is then used to train a classifier to classify the stimulus features. The proposed method is based on PCA and SRM, and the authors show that the proposed method outperforms the existing methods in terms of classification accuracy. "
1193,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method for cross-subject translation of fMRI signals. The authors propose to use a neural network to learn a common embedding of the input signal, which is then used to train a classifier to classify the stimulus features. The proposed method is based on PCA and SRM, and the authors show that the proposed method outperforms the existing methods in terms of classification accuracy. "
1194,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"This paper proposes a method for cross-subject translation of fMRI signals. The authors propose to use a neural network to learn a common embedding of the input signal, which is then used to train a classifier to classify the stimulus features. The proposed method is based on PCA and SRM, and the authors show that the proposed method outperforms the existing methods in terms of classification accuracy. "
1195,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution anomaly detection. The proposed benchmark is based on ImageNet-21K, which is a large-scale multi-label anomaly detector. The main contribution of this paper is the introduction of a new segmentation benchmark for anomaly detection on road anomalies. "
1196,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution anomaly detection. The proposed benchmark is based on ImageNet-21K, which is a large-scale multi-label anomaly detector. The main contribution of this paper is the introduction of a new segmentation benchmark for anomaly detection on road anomalies. "
1197,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution anomaly detection. The proposed benchmark is based on ImageNet-21K, which is a large-scale multi-label anomaly detector. The main contribution of this paper is the introduction of a new segmentation benchmark for anomaly detection on road anomalies. "
1198,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"This paper proposes a new benchmark for out-of-distribution anomaly detection. The proposed benchmark is based on ImageNet-21K, which is a large-scale multi-label anomaly detector. The main contribution of this paper is the introduction of a new segmentation benchmark for anomaly detection on road anomalies. "
1199,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the representation of intransitive tournaments in a parametric model. The authors consider a class of tournaments that are restricted to a specific class of flips, which they call the forbidden flip class. They show that the smallest representation dimension of a given tournament can be expressed as a sign-rank of the sign of a matrix. They also provide a lower bound on the minimum dimension of the feedback arc set of a specific tournament class. Finally, they provide an upper bound for the smallest number of tournaments in the forbidden class."
1200,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the representation of intransitive tournaments in a parametric model. The authors consider a class of tournaments that are restricted to a specific class of flips, which they call the forbidden flip class. They show that the smallest representation dimension of a given tournament can be expressed as a sign-rank of the sign of a matrix. They also provide a lower bound on the minimum dimension of the feedback arc set of a specific tournament class. Finally, they provide an upper bound for the smallest number of tournaments in the forbidden class."
1201,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the representation of intransitive tournaments in a parametric model. The authors consider a class of tournaments that are restricted to a specific class of flips, which they call the forbidden flip class. They show that the smallest representation dimension of a given tournament can be expressed as a sign-rank of the sign of a matrix. They also provide a lower bound on the minimum dimension of the feedback arc set of a specific tournament class. Finally, they provide an upper bound for the smallest number of tournaments in the forbidden class."
1202,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"This paper studies the representation of intransitive tournaments in a parametric model. The authors consider a class of tournaments that are restricted to a specific class of flips, which they call the forbidden flip class. They show that the smallest representation dimension of a given tournament can be expressed as a sign-rank of the sign of a matrix. They also provide a lower bound on the minimum dimension of the feedback arc set of a specific tournament class. Finally, they provide an upper bound for the smallest number of tournaments in the forbidden class."
1203,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a method for learning context embeddings for Neural Processes (NPs). The proposed method is based on information theory and uses a stochastic attention mechanism to extract the context information from the context dataset. The authors show that the proposed method can be applied to both noisy data sets and restricted task distributions. They also show that their method is permutation invariant to the aggregation functions of aggregation functions. They evaluate their method on 1D regression, predator-prey model, and image completion."
1204,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a method for learning context embeddings for Neural Processes (NPs). The proposed method is based on information theory and uses a stochastic attention mechanism to extract the context information from the context dataset. The authors show that the proposed method can be applied to both noisy data sets and restricted task distributions. They also show that their method is permutation invariant to the aggregation functions of aggregation functions. They evaluate their method on 1D regression, predator-prey model, and image completion."
1205,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a method for learning context embeddings for Neural Processes (NPs). The proposed method is based on information theory and uses a stochastic attention mechanism to extract the context information from the context dataset. The authors show that the proposed method can be applied to both noisy data sets and restricted task distributions. They also show that their method is permutation invariant to the aggregation functions of aggregation functions. They evaluate their method on 1D regression, predator-prey model, and image completion."
1206,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"This paper proposes a method for learning context embeddings for Neural Processes (NPs). The proposed method is based on information theory and uses a stochastic attention mechanism to extract the context information from the context dataset. The authors show that the proposed method can be applied to both noisy data sets and restricted task distributions. They also show that their method is permutation invariant to the aggregation functions of aggregation functions. They evaluate their method on 1D regression, predator-prey model, and image completion."
1207,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a new architecture for Transformer-based NLP models. The main idea is to use a prototype network to train the model and then use the prototype network as the input for the final model. The prototype network is trained on a small set of examples from the training set. The paper shows that the proposed architecture is more interpretable than the original Transformer model. 
1208,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a new architecture for Transformer-based NLP models. The main idea is to use a prototype network to train the model and then use the prototype network as the input for the final model. The prototype network is trained on a small set of examples from the training set. The paper shows that the proposed architecture is more interpretable than the original Transformer model. 
1209,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a new architecture for Transformer-based NLP models. The main idea is to use a prototype network to train the model and then use the prototype network as the input for the final model. The prototype network is trained on a small set of examples from the training set. The paper shows that the proposed architecture is more interpretable than the original Transformer model. 
1210,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes a new architecture for Transformer-based NLP models. The main idea is to use a prototype network to train the model and then use the prototype network as the input for the final model. The prototype network is trained on a small set of examples from the training set. The paper shows that the proposed architecture is more interpretable than the original Transformer model. 
1211,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,This paper proposes a new method for continual learning. The key idea is to use a layer-wise scaling matrix and a scaled weight projection to transfer the knowledge from one task to the next. The proposed method is evaluated on a variety of tasks and compared to several baselines.
1212,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,This paper proposes a new method for continual learning. The key idea is to use a layer-wise scaling matrix and a scaled weight projection to transfer the knowledge from one task to the next. The proposed method is evaluated on a variety of tasks and compared to several baselines.
1213,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,This paper proposes a new method for continual learning. The key idea is to use a layer-wise scaling matrix and a scaled weight projection to transfer the knowledge from one task to the next. The proposed method is evaluated on a variety of tasks and compared to several baselines.
1214,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,This paper proposes a new method for continual learning. The key idea is to use a layer-wise scaling matrix and a scaled weight projection to transfer the knowledge from one task to the next. The proposed method is evaluated on a variety of tasks and compared to several baselines.
1215,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,This paper studies the generalization error of optimization and generalization in machine learning. The authors propose a new generalization bound based on the length of the optimization trajectory. The main contribution of this paper is to derive a generalization bounds for the length-based generalization of the gradient flow algorithm. The paper also provides a theoretical analysis of the effect of initialization and gradient flow on generalization. 
1216,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,This paper studies the generalization error of optimization and generalization in machine learning. The authors propose a new generalization bound based on the length of the optimization trajectory. The main contribution of this paper is to derive a generalization bounds for the length-based generalization of the gradient flow algorithm. The paper also provides a theoretical analysis of the effect of initialization and gradient flow on generalization. 
1217,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,This paper studies the generalization error of optimization and generalization in machine learning. The authors propose a new generalization bound based on the length of the optimization trajectory. The main contribution of this paper is to derive a generalization bounds for the length-based generalization of the gradient flow algorithm. The paper also provides a theoretical analysis of the effect of initialization and gradient flow on generalization. 
1218,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,This paper studies the generalization error of optimization and generalization in machine learning. The authors propose a new generalization bound based on the length of the optimization trajectory. The main contribution of this paper is to derive a generalization bounds for the length-based generalization of the gradient flow algorithm. The paper also provides a theoretical analysis of the effect of initialization and gradient flow on generalization. 
1219,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper proposes a frequency-based understanding of adversarial examples. The authors show that the frequency of the adversarial example can be used as an explanation for the robustness of the model to high frequency noise. In addition, the authors propose to use frequency constraints to train robust models. The experiments are conducted on CIFAR-10 and ImageNet-derived datasets."
1220,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper proposes a frequency-based understanding of adversarial examples. The authors show that the frequency of the adversarial example can be used as an explanation for the robustness of the model to high frequency noise. In addition, the authors propose to use frequency constraints to train robust models. The experiments are conducted on CIFAR-10 and ImageNet-derived datasets."
1221,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper proposes a frequency-based understanding of adversarial examples. The authors show that the frequency of the adversarial example can be used as an explanation for the robustness of the model to high frequency noise. In addition, the authors propose to use frequency constraints to train robust models. The experiments are conducted on CIFAR-10 and ImageNet-derived datasets."
1222,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This paper proposes a frequency-based understanding of adversarial examples. The authors show that the frequency of the adversarial example can be used as an explanation for the robustness of the model to high frequency noise. In addition, the authors propose to use frequency constraints to train robust models. The experiments are conducted on CIFAR-10 and ImageNet-derived datasets."
1223,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper studies the problem of heterophily in graph neural networks (GNNs). The authors propose a new aggregation operation called Adaptive Channel Mixing (ACM) for GNNs, which is based on the idea of adaptive channel mixing. The main idea of ACM is to mix the identity channels of the GNN layer with the identity channel of the filterbanks. The authors show that the proposed ACM can reduce the harmful heterophilicity of GNN layers. The experiments on synthetic and real-world node classification tasks show the effectiveness of the proposed method."
1224,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper studies the problem of heterophily in graph neural networks (GNNs). The authors propose a new aggregation operation called Adaptive Channel Mixing (ACM) for GNNs, which is based on the idea of adaptive channel mixing. The main idea of ACM is to mix the identity channels of the GNN layer with the identity channel of the filterbanks. The authors show that the proposed ACM can reduce the harmful heterophilicity of GNN layers. The experiments on synthetic and real-world node classification tasks show the effectiveness of the proposed method."
1225,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper studies the problem of heterophily in graph neural networks (GNNs). The authors propose a new aggregation operation called Adaptive Channel Mixing (ACM) for GNNs, which is based on the idea of adaptive channel mixing. The main idea of ACM is to mix the identity channels of the GNN layer with the identity channel of the filterbanks. The authors show that the proposed ACM can reduce the harmful heterophilicity of GNN layers. The experiments on synthetic and real-world node classification tasks show the effectiveness of the proposed method."
1226,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"This paper studies the problem of heterophily in graph neural networks (GNNs). The authors propose a new aggregation operation called Adaptive Channel Mixing (ACM) for GNNs, which is based on the idea of adaptive channel mixing. The main idea of ACM is to mix the identity channels of the GNN layer with the identity channel of the filterbanks. The authors show that the proposed ACM can reduce the harmful heterophilicity of GNN layers. The experiments on synthetic and real-world node classification tasks show the effectiveness of the proposed method."
1227,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a new method for training deep reinforcement learning models that can generalize to larger-sized instances. The main idea is to use a deep learning architecture to learn a representation of the value landscape, which is then used to train a deep RL model. The proposed method is evaluated on a variety of tasks, and compared to a number of state-of-the-art deep RL methods. The results show that the proposed method outperforms the existing methods."
1228,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a new method for training deep reinforcement learning models that can generalize to larger-sized instances. The main idea is to use a deep learning architecture to learn a representation of the value landscape, which is then used to train a deep RL model. The proposed method is evaluated on a variety of tasks, and compared to a number of state-of-the-art deep RL methods. The results show that the proposed method outperforms the existing methods."
1229,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a new method for training deep reinforcement learning models that can generalize to larger-sized instances. The main idea is to use a deep learning architecture to learn a representation of the value landscape, which is then used to train a deep RL model. The proposed method is evaluated on a variety of tasks, and compared to a number of state-of-the-art deep RL methods. The results show that the proposed method outperforms the existing methods."
1230,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"This paper proposes a new method for training deep reinforcement learning models that can generalize to larger-sized instances. The main idea is to use a deep learning architecture to learn a representation of the value landscape, which is then used to train a deep RL model. The proposed method is evaluated on a variety of tasks, and compared to a number of state-of-the-art deep RL methods. The results show that the proposed method outperforms the existing methods."
1231,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a federated learning (FL) framework to address the non-IID issue in FL. Specifically, the authors propose a local generative adversarial network (GAN) to generate synthetic data from a global shared synthetic dataset, and a parameter server (PS) to store the local model gradients of the local GAN. The authors also propose a pseudo-labeling method to improve the confidence of the PS. Experiments are conducted on both supervised and semi-supervised settings."
1232,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a federated learning (FL) framework to address the non-IID issue in FL. Specifically, the authors propose a local generative adversarial network (GAN) to generate synthetic data from a global shared synthetic dataset, and a parameter server (PS) to store the local model gradients of the local GAN. The authors also propose a pseudo-labeling method to improve the confidence of the PS. Experiments are conducted on both supervised and semi-supervised settings."
1233,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a federated learning (FL) framework to address the non-IID issue in FL. Specifically, the authors propose a local generative adversarial network (GAN) to generate synthetic data from a global shared synthetic dataset, and a parameter server (PS) to store the local model gradients of the local GAN. The authors also propose a pseudo-labeling method to improve the confidence of the PS. Experiments are conducted on both supervised and semi-supervised settings."
1234,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper proposes a federated learning (FL) framework to address the non-IID issue in FL. Specifically, the authors propose a local generative adversarial network (GAN) to generate synthetic data from a global shared synthetic dataset, and a parameter server (PS) to store the local model gradients of the local GAN. The authors also propose a pseudo-labeling method to improve the confidence of the PS. Experiments are conducted on both supervised and semi-supervised settings."
1235,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a randomized smoothing method to improve the robustness of smoothed classifiers against Gaussian noise. The proposed method is based on the idea of sample-wise control of adversarial robustness. The authors propose to use the worst-case (worst-case) adversarial perturbation as the training objective, and then use the smoothed smoothing objective to train the classifier. The experimental results show that the proposed method improves the certified robustness and prediction confidence of the classifiers."
1236,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a randomized smoothing method to improve the robustness of smoothed classifiers against Gaussian noise. The proposed method is based on the idea of sample-wise control of adversarial robustness. The authors propose to use the worst-case (worst-case) adversarial perturbation as the training objective, and then use the smoothed smoothing objective to train the classifier. The experimental results show that the proposed method improves the certified robustness and prediction confidence of the classifiers."
1237,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a randomized smoothing method to improve the robustness of smoothed classifiers against Gaussian noise. The proposed method is based on the idea of sample-wise control of adversarial robustness. The authors propose to use the worst-case (worst-case) adversarial perturbation as the training objective, and then use the smoothed smoothing objective to train the classifier. The experimental results show that the proposed method improves the certified robustness and prediction confidence of the classifiers."
1238,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes a randomized smoothing method to improve the robustness of smoothed classifiers against Gaussian noise. The proposed method is based on the idea of sample-wise control of adversarial robustness. The authors propose to use the worst-case (worst-case) adversarial perturbation as the training objective, and then use the smoothed smoothing objective to train the classifier. The experimental results show that the proposed method improves the certified robustness and prediction confidence of the classifiers."
1239,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper studies the problem of pretraining BERT on Wikipedia. The authors propose a new method to train BERT with padding tokens. The proposed method is based on a histogram of sequence lengths, which is then used to train a model to predict the order of the padding tokens in the dataset. The paper shows that the proposed method converges to a near optimal packing order for the Wikipedia dataset, and shows that it can achieve a 2x speed-up compared to the existing algorithms."
1240,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper studies the problem of pretraining BERT on Wikipedia. The authors propose a new method to train BERT with padding tokens. The proposed method is based on a histogram of sequence lengths, which is then used to train a model to predict the order of the padding tokens in the dataset. The paper shows that the proposed method converges to a near optimal packing order for the Wikipedia dataset, and shows that it can achieve a 2x speed-up compared to the existing algorithms."
1241,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper studies the problem of pretraining BERT on Wikipedia. The authors propose a new method to train BERT with padding tokens. The proposed method is based on a histogram of sequence lengths, which is then used to train a model to predict the order of the padding tokens in the dataset. The paper shows that the proposed method converges to a near optimal packing order for the Wikipedia dataset, and shows that it can achieve a 2x speed-up compared to the existing algorithms."
1242,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper studies the problem of pretraining BERT on Wikipedia. The authors propose a new method to train BERT with padding tokens. The proposed method is based on a histogram of sequence lengths, which is then used to train a model to predict the order of the padding tokens in the dataset. The paper shows that the proposed method converges to a near optimal packing order for the Wikipedia dataset, and shows that it can achieve a 2x speed-up compared to the existing algorithms."
1243,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper studies the adaptive tree search algorithm for translation models. The main idea is to train a translation model with an autoregressive encoder and a decoder with a noisy channel model, and then use a beam search algorithm to search for high-scoring outputs of the translation model. The authors show that the proposed algorithm outperforms beam search and reranking based methods on the BLEU dataset. They also show that beam search is biased towards models with noisy channel models."
1244,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper studies the adaptive tree search algorithm for translation models. The main idea is to train a translation model with an autoregressive encoder and a decoder with a noisy channel model, and then use a beam search algorithm to search for high-scoring outputs of the translation model. The authors show that the proposed algorithm outperforms beam search and reranking based methods on the BLEU dataset. They also show that beam search is biased towards models with noisy channel models."
1245,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper studies the adaptive tree search algorithm for translation models. The main idea is to train a translation model with an autoregressive encoder and a decoder with a noisy channel model, and then use a beam search algorithm to search for high-scoring outputs of the translation model. The authors show that the proposed algorithm outperforms beam search and reranking based methods on the BLEU dataset. They also show that beam search is biased towards models with noisy channel models."
1246,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"This paper studies the adaptive tree search algorithm for translation models. The main idea is to train a translation model with an autoregressive encoder and a decoder with a noisy channel model, and then use a beam search algorithm to search for high-scoring outputs of the translation model. The authors show that the proposed algorithm outperforms beam search and reranking based methods on the BLEU dataset. They also show that beam search is biased towards models with noisy channel models."
1247,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,This paper proposes an energy based model (EBM) for anomaly detection. The authors propose a meta-learning scheme that learns a plug-and-play feature for the anomaly detector and a sparse coding layer for the EBM. The proposed method is based on the Langevin Dynamics (LD) framework. The experiments show that the proposed method outperforms the state-of-the-art methods.
1248,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,This paper proposes an energy based model (EBM) for anomaly detection. The authors propose a meta-learning scheme that learns a plug-and-play feature for the anomaly detector and a sparse coding layer for the EBM. The proposed method is based on the Langevin Dynamics (LD) framework. The experiments show that the proposed method outperforms the state-of-the-art methods.
1249,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,This paper proposes an energy based model (EBM) for anomaly detection. The authors propose a meta-learning scheme that learns a plug-and-play feature for the anomaly detector and a sparse coding layer for the EBM. The proposed method is based on the Langevin Dynamics (LD) framework. The experiments show that the proposed method outperforms the state-of-the-art methods.
1250,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,This paper proposes an energy based model (EBM) for anomaly detection. The authors propose a meta-learning scheme that learns a plug-and-play feature for the anomaly detector and a sparse coding layer for the EBM. The proposed method is based on the Langevin Dynamics (LD) framework. The experiments show that the proposed method outperforms the state-of-the-art methods.
1251,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in QA systems. The authors propose a new dataset called CONTRAQA, which is a large-scale dataset of contradicting contexts. The paper proposes a new counter-measure to detect misinformation in the context of ContraQA. The counter-measures are based on the question answering and the misinformation detection. The experiments show that the proposed method outperforms existing methods."
1252,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in QA systems. The authors propose a new dataset called CONTRAQA, which is a large-scale dataset of contradicting contexts. The paper proposes a new counter-measure to detect misinformation in the context of ContraQA. The counter-measures are based on the question answering and the misinformation detection. The experiments show that the proposed method outperforms existing methods."
1253,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in QA systems. The authors propose a new dataset called CONTRAQA, which is a large-scale dataset of contradicting contexts. The paper proposes a new counter-measure to detect misinformation in the context of ContraQA. The counter-measures are based on the question answering and the misinformation detection. The experiments show that the proposed method outperforms existing methods."
1254,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper studies the problem of misinformation in QA systems. The authors propose a new dataset called CONTRAQA, which is a large-scale dataset of contradicting contexts. The paper proposes a new counter-measure to detect misinformation in the context of ContraQA. The counter-measures are based on the question answering and the misinformation detection. The experiments show that the proposed method outperforms existing methods."
1255,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,This paper proposes a method for cross-domain imitation learning. The method is based on the Gromov-Wasserstein distance between the expert domain and the state-action space of the imitation agent. The main contribution of the paper is the theoretical analysis of the proposed method. The experimental results show the effectiveness of the method.
1256,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,This paper proposes a method for cross-domain imitation learning. The method is based on the Gromov-Wasserstein distance between the expert domain and the state-action space of the imitation agent. The main contribution of the paper is the theoretical analysis of the proposed method. The experimental results show the effectiveness of the method.
1257,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,This paper proposes a method for cross-domain imitation learning. The method is based on the Gromov-Wasserstein distance between the expert domain and the state-action space of the imitation agent. The main contribution of the paper is the theoretical analysis of the proposed method. The experimental results show the effectiveness of the method.
1258,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,This paper proposes a method for cross-domain imitation learning. The method is based on the Gromov-Wasserstein distance between the expert domain and the state-action space of the imitation agent. The main contribution of the paper is the theoretical analysis of the proposed method. The experimental results show the effectiveness of the method.
1259,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes hierarchical cross contrastive learning (HCCL) for self-supervised learning (SSL). HCCL is based on the hierarchical projection head, where the raw representations of the backbone and the hidden layer of the projection head are combined to form a representation invariant to the contrastive loss. The proposed method is evaluated on few-shot classification, detection, and segmentation tasks."
1260,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes hierarchical cross contrastive learning (HCCL) for self-supervised learning (SSL). HCCL is based on the hierarchical projection head, where the raw representations of the backbone and the hidden layer of the projection head are combined to form a representation invariant to the contrastive loss. The proposed method is evaluated on few-shot classification, detection, and segmentation tasks."
1261,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes hierarchical cross contrastive learning (HCCL) for self-supervised learning (SSL). HCCL is based on the hierarchical projection head, where the raw representations of the backbone and the hidden layer of the projection head are combined to form a representation invariant to the contrastive loss. The proposed method is evaluated on few-shot classification, detection, and segmentation tasks."
1262,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,"This paper proposes hierarchical cross contrastive learning (HCCL) for self-supervised learning (SSL). HCCL is based on the hierarchical projection head, where the raw representations of the backbone and the hidden layer of the projection head are combined to form a representation invariant to the contrastive loss. The proposed method is evaluated on few-shot classification, detection, and segmentation tasks."
1263,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a meta-game-Nash approach for learning dynamic general equilibrium models (DGEs) for real-business-cycle models (RBCs) with heterogeneous, interacting strategic agents in a sequential imperfect-information game. The main contribution of this paper is to propose a method for learning meta-games in RBCs that can be used for training and analyzing DGEs. The proposed method is based on the idea of meta-learning, which is an extension of Nash equilibrium learning. The authors show that the proposed method can be applied to a variety of RBC models, and show that it can learn a meta game-game Nash equilibrium. The paper also provides a theoretical analysis of the proposed approach."
1264,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a meta-game-Nash approach for learning dynamic general equilibrium models (DGEs) for real-business-cycle models (RBCs) with heterogeneous, interacting strategic agents in a sequential imperfect-information game. The main contribution of this paper is to propose a method for learning meta-games in RBCs that can be used for training and analyzing DGEs. The proposed method is based on the idea of meta-learning, which is an extension of Nash equilibrium learning. The authors show that the proposed method can be applied to a variety of RBC models, and show that it can learn a meta game-game Nash equilibrium. The paper also provides a theoretical analysis of the proposed approach."
1265,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a meta-game-Nash approach for learning dynamic general equilibrium models (DGEs) for real-business-cycle models (RBCs) with heterogeneous, interacting strategic agents in a sequential imperfect-information game. The main contribution of this paper is to propose a method for learning meta-games in RBCs that can be used for training and analyzing DGEs. The proposed method is based on the idea of meta-learning, which is an extension of Nash equilibrium learning. The authors show that the proposed method can be applied to a variety of RBC models, and show that it can learn a meta game-game Nash equilibrium. The paper also provides a theoretical analysis of the proposed approach."
1266,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This paper proposes a meta-game-Nash approach for learning dynamic general equilibrium models (DGEs) for real-business-cycle models (RBCs) with heterogeneous, interacting strategic agents in a sequential imperfect-information game. The main contribution of this paper is to propose a method for learning meta-games in RBCs that can be used for training and analyzing DGEs. The proposed method is based on the idea of meta-learning, which is an extension of Nash equilibrium learning. The authors show that the proposed method can be applied to a variety of RBC models, and show that it can learn a meta game-game Nash equilibrium. The paper also provides a theoretical analysis of the proposed approach."
1267,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method for model extraction attacks. The main idea is to use differential privacy (DP) as a defense against model stealing attacks. To this end, the authors propose a method to calibrate the parameters of a model using a differentially private API. The authors also propose a proof-of-work method to prove the correctness of the calibration."
1268,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method for model extraction attacks. The main idea is to use differential privacy (DP) as a defense against model stealing attacks. To this end, the authors propose a method to calibrate the parameters of a model using a differentially private API. The authors also propose a proof-of-work method to prove the correctness of the calibration."
1269,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method for model extraction attacks. The main idea is to use differential privacy (DP) as a defense against model stealing attacks. To this end, the authors propose a method to calibrate the parameters of a model using a differentially private API. The authors also propose a proof-of-work method to prove the correctness of the calibration."
1270,SP:f885c992df9c685f806a653398736432ba38bd80,"This paper proposes a method for model extraction attacks. The main idea is to use differential privacy (DP) as a defense against model stealing attacks. To this end, the authors propose a method to calibrate the parameters of a model using a differentially private API. The authors also propose a proof-of-work method to prove the correctness of the calibration."
1271,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a continuous normalizing flow (CNF) method for image generation and density estimation. The proposed method is based on the Neural Ordinary Differential Equations (ODE) framework. The main idea is to train a neural ODE model that is invertible to the conditional distribution of the fine image, and then use it to estimate the log likelihood of the coarse image. The authors show that the proposed method can be applied to a wide range of image datasets, and show that it is able to achieve state-of-the-art results."
1272,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a continuous normalizing flow (CNF) method for image generation and density estimation. The proposed method is based on the Neural Ordinary Differential Equations (ODE) framework. The main idea is to train a neural ODE model that is invertible to the conditional distribution of the fine image, and then use it to estimate the log likelihood of the coarse image. The authors show that the proposed method can be applied to a wide range of image datasets, and show that it is able to achieve state-of-the-art results."
1273,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a continuous normalizing flow (CNF) method for image generation and density estimation. The proposed method is based on the Neural Ordinary Differential Equations (ODE) framework. The main idea is to train a neural ODE model that is invertible to the conditional distribution of the fine image, and then use it to estimate the log likelihood of the coarse image. The authors show that the proposed method can be applied to a wide range of image datasets, and show that it is able to achieve state-of-the-art results."
1274,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a continuous normalizing flow (CNF) method for image generation and density estimation. The proposed method is based on the Neural Ordinary Differential Equations (ODE) framework. The main idea is to train a neural ODE model that is invertible to the conditional distribution of the fine image, and then use it to estimate the log likelihood of the coarse image. The authors show that the proposed method can be applied to a wide range of image datasets, and show that it is able to achieve state-of-the-art results."
1275,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,This paper proposes a new method to detect noisy labels in deep neural networks. The proposed method is based on a ranking-based approach and a local voting method. The main contribution of this paper is that it proposes to use neighborhood information to train a model with noisy labels. The method is evaluated on both synthetic and real-world label noise. The experimental results show that the proposed method outperforms existing methods.
1276,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,This paper proposes a new method to detect noisy labels in deep neural networks. The proposed method is based on a ranking-based approach and a local voting method. The main contribution of this paper is that it proposes to use neighborhood information to train a model with noisy labels. The method is evaluated on both synthetic and real-world label noise. The experimental results show that the proposed method outperforms existing methods.
1277,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,This paper proposes a new method to detect noisy labels in deep neural networks. The proposed method is based on a ranking-based approach and a local voting method. The main contribution of this paper is that it proposes to use neighborhood information to train a model with noisy labels. The method is evaluated on both synthetic and real-world label noise. The experimental results show that the proposed method outperforms existing methods.
1278,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,This paper proposes a new method to detect noisy labels in deep neural networks. The proposed method is based on a ranking-based approach and a local voting method. The main contribution of this paper is that it proposes to use neighborhood information to train a model with noisy labels. The method is evaluated on both synthetic and real-world label noise. The experimental results show that the proposed method outperforms existing methods.
1279,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,This paper proposes a new adversarial training algorithm for reinforcement learning. The main idea is to train an RL-based adversary that is able to generate the strongest/optimal adversarial perturbations for the agent. The authors propose a new algorithm called PA-Adversarial Training (PA-AD) that is based on a heuristic-based attack method. The proposed algorithm is evaluated on Atari and MuJoCo environments. The experiments show that PA-AD outperforms the state-of-the-art in terms of adversarial robustness.
1280,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,This paper proposes a new adversarial training algorithm for reinforcement learning. The main idea is to train an RL-based adversary that is able to generate the strongest/optimal adversarial perturbations for the agent. The authors propose a new algorithm called PA-Adversarial Training (PA-AD) that is based on a heuristic-based attack method. The proposed algorithm is evaluated on Atari and MuJoCo environments. The experiments show that PA-AD outperforms the state-of-the-art in terms of adversarial robustness.
1281,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,This paper proposes a new adversarial training algorithm for reinforcement learning. The main idea is to train an RL-based adversary that is able to generate the strongest/optimal adversarial perturbations for the agent. The authors propose a new algorithm called PA-Adversarial Training (PA-AD) that is based on a heuristic-based attack method. The proposed algorithm is evaluated on Atari and MuJoCo environments. The experiments show that PA-AD outperforms the state-of-the-art in terms of adversarial robustness.
1282,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,This paper proposes a new adversarial training algorithm for reinforcement learning. The main idea is to train an RL-based adversary that is able to generate the strongest/optimal adversarial perturbations for the agent. The authors propose a new algorithm called PA-Adversarial Training (PA-AD) that is based on a heuristic-based attack method. The proposed algorithm is evaluated on Atari and MuJoCo environments. The experiments show that PA-AD outperforms the state-of-the-art in terms of adversarial robustness.
1283,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper studies the problem of novelty-seeking in reinforcement learning. Specifically, the authors propose a novel novelty metric to measure the novelty of generated policies. The novelty metric is based on the difference between the cumulative reward of a policy and its novelty. The authors also propose an interior point method to solve the novelty problem."
1284,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper studies the problem of novelty-seeking in reinforcement learning. Specifically, the authors propose a novel novelty metric to measure the novelty of generated policies. The novelty metric is based on the difference between the cumulative reward of a policy and its novelty. The authors also propose an interior point method to solve the novelty problem."
1285,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper studies the problem of novelty-seeking in reinforcement learning. Specifically, the authors propose a novel novelty metric to measure the novelty of generated policies. The novelty metric is based on the difference between the cumulative reward of a policy and its novelty. The authors also propose an interior point method to solve the novelty problem."
1286,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"This paper studies the problem of novelty-seeking in reinforcement learning. Specifically, the authors propose a novel novelty metric to measure the novelty of generated policies. The novelty metric is based on the difference between the cumulative reward of a policy and its novelty. The authors also propose an interior point method to solve the novelty problem."
1287,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,This paper proposes a method for generating realistic acoustic renderings of speech using 3D scans of rooms. The method is based on the observation that the reverberation effect of a speaker in a room can be interpreted as a reflection of the speaker's location in the room. The authors propose to use a combination of audio enhancement and speech recognition to generate the acoustic rendering. The proposed method is evaluated on a large-scale dataset of 3D audio recordings of rooms in a 3D home.
1288,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,This paper proposes a method for generating realistic acoustic renderings of speech using 3D scans of rooms. The method is based on the observation that the reverberation effect of a speaker in a room can be interpreted as a reflection of the speaker's location in the room. The authors propose to use a combination of audio enhancement and speech recognition to generate the acoustic rendering. The proposed method is evaluated on a large-scale dataset of 3D audio recordings of rooms in a 3D home.
1289,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,This paper proposes a method for generating realistic acoustic renderings of speech using 3D scans of rooms. The method is based on the observation that the reverberation effect of a speaker in a room can be interpreted as a reflection of the speaker's location in the room. The authors propose to use a combination of audio enhancement and speech recognition to generate the acoustic rendering. The proposed method is evaluated on a large-scale dataset of 3D audio recordings of rooms in a 3D home.
1290,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,This paper proposes a method for generating realistic acoustic renderings of speech using 3D scans of rooms. The method is based on the observation that the reverberation effect of a speaker in a room can be interpreted as a reflection of the speaker's location in the room. The authors propose to use a combination of audio enhancement and speech recognition to generate the acoustic rendering. The proposed method is evaluated on a large-scale dataset of 3D audio recordings of rooms in a 3D home.
1291,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a position representation method for word embeddings. The proposed method, Attention with Linear Biases (ALiBi), is based on the transformer model. The key idea of ALiBi is to learn a position embedding for each word, which is then used to compute the query-key attention scores. The authors show that the proposed method outperforms other position representation methods on the WikiText-103 benchmark."
1292,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a position representation method for word embeddings. The proposed method, Attention with Linear Biases (ALiBi), is based on the transformer model. The key idea of ALiBi is to learn a position embedding for each word, which is then used to compute the query-key attention scores. The authors show that the proposed method outperforms other position representation methods on the WikiText-103 benchmark."
1293,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a position representation method for word embeddings. The proposed method, Attention with Linear Biases (ALiBi), is based on the transformer model. The key idea of ALiBi is to learn a position embedding for each word, which is then used to compute the query-key attention scores. The authors show that the proposed method outperforms other position representation methods on the WikiText-103 benchmark."
1294,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"This paper proposes a position representation method for word embeddings. The proposed method, Attention with Linear Biases (ALiBi), is based on the transformer model. The key idea of ALiBi is to learn a position embedding for each word, which is then used to compute the query-key attention scores. The authors show that the proposed method outperforms other position representation methods on the WikiText-103 benchmark."
1295,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the multi-objective online convex optimization problem. The authors propose two variants of the Online Mirror Multiple Descent algorithm. The first one is based on the L1-regularized min-norm solver, and the second one is a variant of the online mirror multiple descent algorithm. They show that the first one has a lower bound of $O(1/\sqrt{T})$, while the second algorithm has a regret of $\Omega(T)$. The authors also provide a theoretical analysis of the lower bound."
1296,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the multi-objective online convex optimization problem. The authors propose two variants of the Online Mirror Multiple Descent algorithm. The first one is based on the L1-regularized min-norm solver, and the second one is a variant of the online mirror multiple descent algorithm. They show that the first one has a lower bound of $O(1/\sqrt{T})$, while the second algorithm has a regret of $\Omega(T)$. The authors also provide a theoretical analysis of the lower bound."
1297,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the multi-objective online convex optimization problem. The authors propose two variants of the Online Mirror Multiple Descent algorithm. The first one is based on the L1-regularized min-norm solver, and the second one is a variant of the online mirror multiple descent algorithm. They show that the first one has a lower bound of $O(1/\sqrt{T})$, while the second algorithm has a regret of $\Omega(T)$. The authors also provide a theoretical analysis of the lower bound."
1298,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"This paper studies the multi-objective online convex optimization problem. The authors propose two variants of the Online Mirror Multiple Descent algorithm. The first one is based on the L1-regularized min-norm solver, and the second one is a variant of the online mirror multiple descent algorithm. They show that the first one has a lower bound of $O(1/\sqrt{T})$, while the second algorithm has a regret of $\Omega(T)$. The authors also provide a theoretical analysis of the lower bound."
1299,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a generative replay approach for continual learning. The idea is to use generative models to model the replay patterns of previous tasks, and then use them to train a new model for the next task. The authors show that the proposed method is able to improve the classification accuracy on ImageNet-1000."
1300,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a generative replay approach for continual learning. The idea is to use generative models to model the replay patterns of previous tasks, and then use them to train a new model for the next task. The authors show that the proposed method is able to improve the classification accuracy on ImageNet-1000."
1301,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a generative replay approach for continual learning. The idea is to use generative models to model the replay patterns of previous tasks, and then use them to train a new model for the next task. The authors show that the proposed method is able to improve the classification accuracy on ImageNet-1000."
1302,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a generative replay approach for continual learning. The idea is to use generative models to model the replay patterns of previous tasks, and then use them to train a new model for the next task. The authors show that the proposed method is able to improve the classification accuracy on ImageNet-1000."
1303,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework for graph optimization. The main idea is to use a dual graph neural network to model the history of the graph. The proposed method is evaluated on a variety of graph optimization problems, including modularity maximization, modularity minimization, and NCut minimization. The results show that the proposed method outperforms the state-of-the-art methods."
1304,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework for graph optimization. The main idea is to use a dual graph neural network to model the history of the graph. The proposed method is evaluated on a variety of graph optimization problems, including modularity maximization, modularity minimization, and NCut minimization. The results show that the proposed method outperforms the state-of-the-art methods."
1305,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework for graph optimization. The main idea is to use a dual graph neural network to model the history of the graph. The proposed method is evaluated on a variety of graph optimization problems, including modularity maximization, modularity minimization, and NCut minimization. The results show that the proposed method outperforms the state-of-the-art methods."
1306,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"This paper proposes an inductive graph partitioning (IGP) framework for graph optimization. The main idea is to use a dual graph neural network to model the history of the graph. The proposed method is evaluated on a variety of graph optimization problems, including modularity maximization, modularity minimization, and NCut minimization. The results show that the proposed method outperforms the state-of-the-art methods."
1307,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graphs. The proposed model is based on multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE is a hierarchical model that learns a hierarchy of coarsened graphs. In particular, the authors propose to use higher order message passing to improve the resolution level of the graph. Experiments on graph-based image generation, link prediction, and unsupervised molecular representation learning are conducted."
1308,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graphs. The proposed model is based on multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE is a hierarchical model that learns a hierarchy of coarsened graphs. In particular, the authors propose to use higher order message passing to improve the resolution level of the graph. Experiments on graph-based image generation, link prediction, and unsupervised molecular representation learning are conducted."
1309,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graphs. The proposed model is based on multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE is a hierarchical model that learns a hierarchy of coarsened graphs. In particular, the authors propose to use higher order message passing to improve the resolution level of the graph. Experiments on graph-based image generation, link prediction, and unsupervised molecular representation learning are conducted."
1310,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"This paper proposes a hierarchical generative model for graphs. The proposed model is based on multiresolution and equivariant graph variational autoencoders (MGVAE). MGVAE is a hierarchical model that learns a hierarchy of coarsened graphs. In particular, the authors propose to use higher order message passing to improve the resolution level of the graph. Experiments on graph-based image generation, link prediction, and unsupervised molecular representation learning are conducted."
1311,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,This paper studies the problem of nonlinear image classification (ICA) from the perspective of volume preserving flow-based models. The main contribution of this paper is a theoretical analysis of the mixing function of the source and the target in the case of non-linear ICA. Theoretical analysis is provided for the case where the sources are independent components (independent components) and temporal structure is not known. Experiments on synthetic data and real-world images demonstrate the effectiveness of the proposed method.
1312,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,This paper studies the problem of nonlinear image classification (ICA) from the perspective of volume preserving flow-based models. The main contribution of this paper is a theoretical analysis of the mixing function of the source and the target in the case of non-linear ICA. Theoretical analysis is provided for the case where the sources are independent components (independent components) and temporal structure is not known. Experiments on synthetic data and real-world images demonstrate the effectiveness of the proposed method.
1313,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,This paper studies the problem of nonlinear image classification (ICA) from the perspective of volume preserving flow-based models. The main contribution of this paper is a theoretical analysis of the mixing function of the source and the target in the case of non-linear ICA. Theoretical analysis is provided for the case where the sources are independent components (independent components) and temporal structure is not known. Experiments on synthetic data and real-world images demonstrate the effectiveness of the proposed method.
1314,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,This paper studies the problem of nonlinear image classification (ICA) from the perspective of volume preserving flow-based models. The main contribution of this paper is a theoretical analysis of the mixing function of the source and the target in the case of non-linear ICA. Theoretical analysis is provided for the case where the sources are independent components (independent components) and temporal structure is not known. Experiments on synthetic data and real-world images demonstrate the effectiveness of the proposed method.
1315,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a new graph convolutional network (GCN) based on the idea of sharing filters between different channels of a graph. The authors propose to use a shared filter bank for each channel in the graph, and then use adaptive filters for the other channels. The proposed method is evaluated on a variety of graph datasets."
1316,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a new graph convolutional network (GCN) based on the idea of sharing filters between different channels of a graph. The authors propose to use a shared filter bank for each channel in the graph, and then use adaptive filters for the other channels. The proposed method is evaluated on a variety of graph datasets."
1317,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a new graph convolutional network (GCN) based on the idea of sharing filters between different channels of a graph. The authors propose to use a shared filter bank for each channel in the graph, and then use adaptive filters for the other channels. The proposed method is evaluated on a variety of graph datasets."
1318,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a new graph convolutional network (GCN) based on the idea of sharing filters between different channels of a graph. The authors propose to use a shared filter bank for each channel in the graph, and then use adaptive filters for the other channels. The proposed method is evaluated on a variety of graph datasets."
1319,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a method for self-supervised pre-training for image classification. The method is based on the pretraining-finetune paradigm, where a pre-trained model is used to train a model for a downstream task, and then the model is fine-tuned for the downstream task. The authors show that the proposed method, called Look, outperforms the state-of-the-art methods on ImageNet. The main contribution of the paper is that the authors propose a method that can be used for both supervised and self-training. "
1320,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a method for self-supervised pre-training for image classification. The method is based on the pretraining-finetune paradigm, where a pre-trained model is used to train a model for a downstream task, and then the model is fine-tuned for the downstream task. The authors show that the proposed method, called Look, outperforms the state-of-the-art methods on ImageNet. The main contribution of the paper is that the authors propose a method that can be used for both supervised and self-training. "
1321,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a method for self-supervised pre-training for image classification. The method is based on the pretraining-finetune paradigm, where a pre-trained model is used to train a model for a downstream task, and then the model is fine-tuned for the downstream task. The authors show that the proposed method, called Look, outperforms the state-of-the-art methods on ImageNet. The main contribution of the paper is that the authors propose a method that can be used for both supervised and self-training. "
1322,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"This paper proposes a method for self-supervised pre-training for image classification. The method is based on the pretraining-finetune paradigm, where a pre-trained model is used to train a model for a downstream task, and then the model is fine-tuned for the downstream task. The authors show that the proposed method, called Look, outperforms the state-of-the-art methods on ImageNet. The main contribution of the paper is that the authors propose a method that can be used for both supervised and self-training. "
1323,SP:2b3916ba24094c286117126e11032820f8c7c50a,This paper proposes a method for 3D face rendering based on Proxy Shading. The proposed method is based on the idea of hallucinating facial details from a single image. The method is evaluated on the FaceDet3D dataset and compared to the state-of-the-art FaceDet2D dataset. The results show that the proposed method achieves better results than the state of the art.
1324,SP:2b3916ba24094c286117126e11032820f8c7c50a,This paper proposes a method for 3D face rendering based on Proxy Shading. The proposed method is based on the idea of hallucinating facial details from a single image. The method is evaluated on the FaceDet3D dataset and compared to the state-of-the-art FaceDet2D dataset. The results show that the proposed method achieves better results than the state of the art.
1325,SP:2b3916ba24094c286117126e11032820f8c7c50a,This paper proposes a method for 3D face rendering based on Proxy Shading. The proposed method is based on the idea of hallucinating facial details from a single image. The method is evaluated on the FaceDet3D dataset and compared to the state-of-the-art FaceDet2D dataset. The results show that the proposed method achieves better results than the state of the art.
1326,SP:2b3916ba24094c286117126e11032820f8c7c50a,This paper proposes a method for 3D face rendering based on Proxy Shading. The proposed method is based on the idea of hallucinating facial details from a single image. The method is evaluated on the FaceDet3D dataset and compared to the state-of-the-art FaceDet2D dataset. The results show that the proposed method achieves better results than the state of the art.
1327,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The proposed model is based on Transformer-based machine translation models. The main contribution of the paper is to propose an evaluation protocol to evaluate the disentanglement of syntactic roles in the generated content. The evaluation protocol consists of two steps: 1) a latent variable perturbation, 2) an attention maxima. Experiments on the SNLI dataset show that the proposed method outperforms the state-of-the-art Transformer VAEs."
1328,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The proposed model is based on Transformer-based machine translation models. The main contribution of the paper is to propose an evaluation protocol to evaluate the disentanglement of syntactic roles in the generated content. The evaluation protocol consists of two steps: 1) a latent variable perturbation, 2) an attention maxima. Experiments on the SNLI dataset show that the proposed method outperforms the state-of-the-art Transformer VAEs."
1329,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The proposed model is based on Transformer-based machine translation models. The main contribution of the paper is to propose an evaluation protocol to evaluate the disentanglement of syntactic roles in the generated content. The evaluation protocol consists of two steps: 1) a latent variable perturbation, 2) an attention maxima. Experiments on the SNLI dataset show that the proposed method outperforms the state-of-the-art Transformer VAEs."
1330,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes an attention-driven variational autoencoder (ADVAE) model for unsupervised controllable content generation. The proposed model is based on Transformer-based machine translation models. The main contribution of the paper is to propose an evaluation protocol to evaluate the disentanglement of syntactic roles in the generated content. The evaluation protocol consists of two steps: 1) a latent variable perturbation, 2) an attention maxima. Experiments on the SNLI dataset show that the proposed method outperforms the state-of-the-art Transformer VAEs."
1331,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,This paper proposes a method for exploration in multi-agent reinforcement learning (MARL). The proposed method is based on the idea of counterfactual rollouts (counterfactual actions that can be performed in a different way than the original action). The method is evaluated on the StarCraft Multi-Agent Challenge (MAMC) and compared to several baselines.
1332,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,This paper proposes a method for exploration in multi-agent reinforcement learning (MARL). The proposed method is based on the idea of counterfactual rollouts (counterfactual actions that can be performed in a different way than the original action). The method is evaluated on the StarCraft Multi-Agent Challenge (MAMC) and compared to several baselines.
1333,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,This paper proposes a method for exploration in multi-agent reinforcement learning (MARL). The proposed method is based on the idea of counterfactual rollouts (counterfactual actions that can be performed in a different way than the original action). The method is evaluated on the StarCraft Multi-Agent Challenge (MAMC) and compared to several baselines.
1334,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,This paper proposes a method for exploration in multi-agent reinforcement learning (MARL). The proposed method is based on the idea of counterfactual rollouts (counterfactual actions that can be performed in a different way than the original action). The method is evaluated on the StarCraft Multi-Agent Challenge (MAMC) and compared to several baselines.
1335,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc model editing. The proposed method is based on gradient decomposition (MEND), which decomposes the gradient of a pre-trained model into a low-rank decomposition of the gradient and a high-rank one. The authors show that the proposed method can be applied to large pre-training models and small auxiliary editing networks. They also show that MEND can be used to fine-tune the model on GPUs."
1336,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc model editing. The proposed method is based on gradient decomposition (MEND), which decomposes the gradient of a pre-trained model into a low-rank decomposition of the gradient and a high-rank one. The authors show that the proposed method can be applied to large pre-training models and small auxiliary editing networks. They also show that MEND can be used to fine-tune the model on GPUs."
1337,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc model editing. The proposed method is based on gradient decomposition (MEND), which decomposes the gradient of a pre-trained model into a low-rank decomposition of the gradient and a high-rank one. The authors show that the proposed method can be applied to large pre-training models and small auxiliary editing networks. They also show that MEND can be used to fine-tune the model on GPUs."
1338,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"This paper proposes a method for post-hoc model editing. The proposed method is based on gradient decomposition (MEND), which decomposes the gradient of a pre-trained model into a low-rank decomposition of the gradient and a high-rank one. The authors show that the proposed method can be applied to large pre-training models and small auxiliary editing networks. They also show that MEND can be used to fine-tune the model on GPUs."
1339,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a neural network-based method for modeling the constituents of partial differential equations (PDEs) in a one-dimensional and two-dimensional setting. The proposed method is based on a combination of neural networks and physics-aware models. The method is evaluated on diffusion-sorption and diffusion-advection-diffusion processes. The authors show that the proposed method outperforms the state-of-the-art methods on both one-and-two-dimensional PDEs.
1340,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a neural network-based method for modeling the constituents of partial differential equations (PDEs) in a one-dimensional and two-dimensional setting. The proposed method is based on a combination of neural networks and physics-aware models. The method is evaluated on diffusion-sorption and diffusion-advection-diffusion processes. The authors show that the proposed method outperforms the state-of-the-art methods on both one-and-two-dimensional PDEs.
1341,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a neural network-based method for modeling the constituents of partial differential equations (PDEs) in a one-dimensional and two-dimensional setting. The proposed method is based on a combination of neural networks and physics-aware models. The method is evaluated on diffusion-sorption and diffusion-advection-diffusion processes. The authors show that the proposed method outperforms the state-of-the-art methods on both one-and-two-dimensional PDEs.
1342,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,This paper proposes a neural network-based method for modeling the constituents of partial differential equations (PDEs) in a one-dimensional and two-dimensional setting. The proposed method is based on a combination of neural networks and physics-aware models. The method is evaluated on diffusion-sorption and diffusion-advection-diffusion processes. The authors show that the proposed method outperforms the state-of-the-art methods on both one-and-two-dimensional PDEs.
1343,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper studies the problem of learning representations of code in a multi-demand system. The authors propose a method to learn a representation of code that can capture both static and dynamic properties of code. The main contribution of the paper is the use of a neural network to learn the representation of the code, which is then used to train an ML model. The model is trained on a set of Python code examples, and the authors show that the learned representation is better than the original code. "
1344,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper studies the problem of learning representations of code in a multi-demand system. The authors propose a method to learn a representation of code that can capture both static and dynamic properties of code. The main contribution of the paper is the use of a neural network to learn the representation of the code, which is then used to train an ML model. The model is trained on a set of Python code examples, and the authors show that the learned representation is better than the original code. "
1345,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper studies the problem of learning representations of code in a multi-demand system. The authors propose a method to learn a representation of code that can capture both static and dynamic properties of code. The main contribution of the paper is the use of a neural network to learn the representation of the code, which is then used to train an ML model. The model is trained on a set of Python code examples, and the authors show that the learned representation is better than the original code. "
1346,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper studies the problem of learning representations of code in a multi-demand system. The authors propose a method to learn a representation of code that can capture both static and dynamic properties of code. The main contribution of the paper is the use of a neural network to learn the representation of the code, which is then used to train an ML model. The model is trained on a set of Python code examples, and the authors show that the learned representation is better than the original code. "
1347,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper proposes a new Transformer-based model for speech synthesis. The model consists of a speech encoder, a phoneme decoder, and a mel-spectrogram synthesizer. The authors claim that the proposed model is more robust to over-generation and over-speech than the original Transformer model. The proposed Transformer is evaluated on a variety of speech synthesis tasks, including babbling, babbling with long pauses, and long pause with long pause. "
1348,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper proposes a new Transformer-based model for speech synthesis. The model consists of a speech encoder, a phoneme decoder, and a mel-spectrogram synthesizer. The authors claim that the proposed model is more robust to over-generation and over-speech than the original Transformer model. The proposed Transformer is evaluated on a variety of speech synthesis tasks, including babbling, babbling with long pauses, and long pause with long pause. "
1349,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper proposes a new Transformer-based model for speech synthesis. The model consists of a speech encoder, a phoneme decoder, and a mel-spectrogram synthesizer. The authors claim that the proposed model is more robust to over-generation and over-speech than the original Transformer model. The proposed Transformer is evaluated on a variety of speech synthesis tasks, including babbling, babbling with long pauses, and long pause with long pause. "
1350,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper proposes a new Transformer-based model for speech synthesis. The model consists of a speech encoder, a phoneme decoder, and a mel-spectrogram synthesizer. The authors claim that the proposed model is more robust to over-generation and over-speech than the original Transformer model. The proposed Transformer is evaluated on a variety of speech synthesis tasks, including babbling, babbling with long pauses, and long pause with long pause. "
1351,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper proposes zeroshot learning, a method for few-shot learning of attribute-based representations for concept learning. The key idea is to train a model that learns to predict the test attributes of the attribute space based on the predictability of test attributes, and then use the predicted attributes to train the model. The proposed method is evaluated on a small set of semantic classes, and compared with a few supervised learning methods. The results show that the proposed method outperforms the other methods."
1352,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper proposes zeroshot learning, a method for few-shot learning of attribute-based representations for concept learning. The key idea is to train a model that learns to predict the test attributes of the attribute space based on the predictability of test attributes, and then use the predicted attributes to train the model. The proposed method is evaluated on a small set of semantic classes, and compared with a few supervised learning methods. The results show that the proposed method outperforms the other methods."
1353,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper proposes zeroshot learning, a method for few-shot learning of attribute-based representations for concept learning. The key idea is to train a model that learns to predict the test attributes of the attribute space based on the predictability of test attributes, and then use the predicted attributes to train the model. The proposed method is evaluated on a small set of semantic classes, and compared with a few supervised learning methods. The results show that the proposed method outperforms the other methods."
1354,SP:296102e60b842923c94f579f524fa1147328ee4b,"This paper proposes zeroshot learning, a method for few-shot learning of attribute-based representations for concept learning. The key idea is to train a model that learns to predict the test attributes of the attribute space based on the predictability of test attributes, and then use the predicted attributes to train the model. The proposed method is evaluated on a small set of semantic classes, and compared with a few supervised learning methods. The results show that the proposed method outperforms the other methods."
1355,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the relative entropy gradient sampler (REGS), which is an extension of the Wasserstein gradient flow of relative entropy (Wasserstein et al., 2018). The main difference is that instead of sampling from the reference distribution, the authors propose to sample from the path of probability distributions, which is a non-linear transformation of the target distribution. The authors also propose a nonparametric approach to compute the density ratio between the target density and the density of evolving particles in the ODE system. Experiments show that the proposed method outperforms the state-of-the-art methods on both synthetic and real datasets."
1356,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the relative entropy gradient sampler (REGS), which is an extension of the Wasserstein gradient flow of relative entropy (Wasserstein et al., 2018). The main difference is that instead of sampling from the reference distribution, the authors propose to sample from the path of probability distributions, which is a non-linear transformation of the target distribution. The authors also propose a nonparametric approach to compute the density ratio between the target density and the density of evolving particles in the ODE system. Experiments show that the proposed method outperforms the state-of-the-art methods on both synthetic and real datasets."
1357,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the relative entropy gradient sampler (REGS), which is an extension of the Wasserstein gradient flow of relative entropy (Wasserstein et al., 2018). The main difference is that instead of sampling from the reference distribution, the authors propose to sample from the path of probability distributions, which is a non-linear transformation of the target distribution. The authors also propose a nonparametric approach to compute the density ratio between the target density and the density of evolving particles in the ODE system. Experiments show that the proposed method outperforms the state-of-the-art methods on both synthetic and real datasets."
1358,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,"This paper proposes a particle method for sampling from unnormalized distributions. The method is based on the relative entropy gradient sampler (REGS), which is an extension of the Wasserstein gradient flow of relative entropy (Wasserstein et al., 2018). The main difference is that instead of sampling from the reference distribution, the authors propose to sample from the path of probability distributions, which is a non-linear transformation of the target distribution. The authors also propose a nonparametric approach to compute the density ratio between the target density and the density of evolving particles in the ODE system. Experiments show that the proposed method outperforms the state-of-the-art methods on both synthetic and real datasets."
1359,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum neural network for image classification. The proposed method is based on the idea of encoding quantum states into a quantum state space, which is then used to train a neural network to predict the quantum state of the image. The authors show that the proposed method outperforms the state-of-the-art methods on MNIST, CIFAR-10, and ImageNet."
1360,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum neural network for image classification. The proposed method is based on the idea of encoding quantum states into a quantum state space, which is then used to train a neural network to predict the quantum state of the image. The authors show that the proposed method outperforms the state-of-the-art methods on MNIST, CIFAR-10, and ImageNet."
1361,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum neural network for image classification. The proposed method is based on the idea of encoding quantum states into a quantum state space, which is then used to train a neural network to predict the quantum state of the image. The authors show that the proposed method outperforms the state-of-the-art methods on MNIST, CIFAR-10, and ImageNet."
1362,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"This paper proposes a quantum neural network for image classification. The proposed method is based on the idea of encoding quantum states into a quantum state space, which is then used to train a neural network to predict the quantum state of the image. The authors show that the proposed method outperforms the state-of-the-art methods on MNIST, CIFAR-10, and ImageNet."
1363,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,This paper proposes a federated learning approach for face recognition. The authors propose a differentially private local clustering (DPLC) mechanism to sanitized clusters and a consensus-aware recognition loss for global consensuses. The DPLC is based on the idea of Differentially Private Local Clustering (DPC) and the proposed DPLC method is evaluated on a large-scale dataset. 
1364,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,This paper proposes a federated learning approach for face recognition. The authors propose a differentially private local clustering (DPLC) mechanism to sanitized clusters and a consensus-aware recognition loss for global consensuses. The DPLC is based on the idea of Differentially Private Local Clustering (DPC) and the proposed DPLC method is evaluated on a large-scale dataset. 
1365,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,This paper proposes a federated learning approach for face recognition. The authors propose a differentially private local clustering (DPLC) mechanism to sanitized clusters and a consensus-aware recognition loss for global consensuses. The DPLC is based on the idea of Differentially Private Local Clustering (DPC) and the proposed DPLC method is evaluated on a large-scale dataset. 
1366,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,This paper proposes a federated learning approach for face recognition. The authors propose a differentially private local clustering (DPLC) mechanism to sanitized clusters and a consensus-aware recognition loss for global consensuses. The DPLC is based on the idea of Differentially Private Local Clustering (DPC) and the proposed DPLC method is evaluated on a large-scale dataset. 
1367,SP:408d9e1299ee05b89855df9742b608626692b40d,This paper proposes a method for out-of-distribution transfer. The main idea is to train a classification head for the source domain and then fine-tune the intermediate layers for the target domain. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows better performance compared to other methods.
1368,SP:408d9e1299ee05b89855df9742b608626692b40d,This paper proposes a method for out-of-distribution transfer. The main idea is to train a classification head for the source domain and then fine-tune the intermediate layers for the target domain. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows better performance compared to other methods.
1369,SP:408d9e1299ee05b89855df9742b608626692b40d,This paper proposes a method for out-of-distribution transfer. The main idea is to train a classification head for the source domain and then fine-tune the intermediate layers for the target domain. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows better performance compared to other methods.
1370,SP:408d9e1299ee05b89855df9742b608626692b40d,This paper proposes a method for out-of-distribution transfer. The main idea is to train a classification head for the source domain and then fine-tune the intermediate layers for the target domain. The proposed method is evaluated on the Visual Task Adaptation Benchmark (VTAB) and shows better performance compared to other methods.
1371,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes a novel method for planning in text-based games. The main idea is to use an object-to-outlier (OOTD) model to model the dynamics of the game. The OOTD model is trained with a variational objective, which is based on the idea that the dynamics should be stochastic. The authors show that the proposed method outperforms model-free baselines in both object-supervised and self-supervision settings."
1372,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes a novel method for planning in text-based games. The main idea is to use an object-to-outlier (OOTD) model to model the dynamics of the game. The OOTD model is trained with a variational objective, which is based on the idea that the dynamics should be stochastic. The authors show that the proposed method outperforms model-free baselines in both object-supervised and self-supervision settings."
1373,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes a novel method for planning in text-based games. The main idea is to use an object-to-outlier (OOTD) model to model the dynamics of the game. The OOTD model is trained with a variational objective, which is based on the idea that the dynamics should be stochastic. The authors show that the proposed method outperforms model-free baselines in both object-supervised and self-supervision settings."
1374,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper proposes a novel method for planning in text-based games. The main idea is to use an object-to-outlier (OOTD) model to model the dynamics of the game. The OOTD model is trained with a variational objective, which is based on the idea that the dynamics should be stochastic. The authors show that the proposed method outperforms model-free baselines in both object-supervised and self-supervision settings."
1375,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,This paper proposes a new loss function for hierarchical classification based on bijective mapping. The proposed loss function is a combination of a hierarchical cost-sensitive loss and a layer-wise abstaining loss. The authors show that the proposed method is tractable and can be applied to a variety of hierarchical learning problems. The main contribution of the paper is the use of bijectivity in the loss function. The paper also proposes a perclass loss-adjustment heuristic to improve the performance of the proposed cost function.
1376,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,This paper proposes a new loss function for hierarchical classification based on bijective mapping. The proposed loss function is a combination of a hierarchical cost-sensitive loss and a layer-wise abstaining loss. The authors show that the proposed method is tractable and can be applied to a variety of hierarchical learning problems. The main contribution of the paper is the use of bijectivity in the loss function. The paper also proposes a perclass loss-adjustment heuristic to improve the performance of the proposed cost function.
1377,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,This paper proposes a new loss function for hierarchical classification based on bijective mapping. The proposed loss function is a combination of a hierarchical cost-sensitive loss and a layer-wise abstaining loss. The authors show that the proposed method is tractable and can be applied to a variety of hierarchical learning problems. The main contribution of the paper is the use of bijectivity in the loss function. The paper also proposes a perclass loss-adjustment heuristic to improve the performance of the proposed cost function.
1378,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,This paper proposes a new loss function for hierarchical classification based on bijective mapping. The proposed loss function is a combination of a hierarchical cost-sensitive loss and a layer-wise abstaining loss. The authors show that the proposed method is tractable and can be applied to a variety of hierarchical learning problems. The main contribution of the paper is the use of bijectivity in the loss function. The paper also proposes a perclass loss-adjustment heuristic to improve the performance of the proposed cost function.
1379,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes a method for learning a dynamic multi-scale time-frequency representation of multi-channel sound waveforms. The proposed method is based on a Transformer-like backbone, which consists of parallel soft-stitched branches, and a synperiodic filter bank group front-end. The authors show that the proposed method can achieve better time and frequency resolution trade-off compared to existing methods."
1380,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes a method for learning a dynamic multi-scale time-frequency representation of multi-channel sound waveforms. The proposed method is based on a Transformer-like backbone, which consists of parallel soft-stitched branches, and a synperiodic filter bank group front-end. The authors show that the proposed method can achieve better time and frequency resolution trade-off compared to existing methods."
1381,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes a method for learning a dynamic multi-scale time-frequency representation of multi-channel sound waveforms. The proposed method is based on a Transformer-like backbone, which consists of parallel soft-stitched branches, and a synperiodic filter bank group front-end. The authors show that the proposed method can achieve better time and frequency resolution trade-off compared to existing methods."
1382,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper proposes a method for learning a dynamic multi-scale time-frequency representation of multi-channel sound waveforms. The proposed method is based on a Transformer-like backbone, which consists of parallel soft-stitched branches, and a synperiodic filter bank group front-end. The authors show that the proposed method can achieve better time and frequency resolution trade-off compared to existing methods."
1383,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper proposes a method for learning domain invariant representations by iterative self-training. The method is based on the idea of GANs. The authors propose a method called GAN-GIFT, which is an extension of the GAN algorithm. The main difference between GAN and GAN is that GAN can be used to train a model that is invariant to domain shifts. The proposed method is evaluated on a variety of benchmark datasets and compared to other domain adaptation methods."
1384,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper proposes a method for learning domain invariant representations by iterative self-training. The method is based on the idea of GANs. The authors propose a method called GAN-GIFT, which is an extension of the GAN algorithm. The main difference between GAN and GAN is that GAN can be used to train a model that is invariant to domain shifts. The proposed method is evaluated on a variety of benchmark datasets and compared to other domain adaptation methods."
1385,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper proposes a method for learning domain invariant representations by iterative self-training. The method is based on the idea of GANs. The authors propose a method called GAN-GIFT, which is an extension of the GAN algorithm. The main difference between GAN and GAN is that GAN can be used to train a model that is invariant to domain shifts. The proposed method is evaluated on a variety of benchmark datasets and compared to other domain adaptation methods."
1386,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This paper proposes a method for learning domain invariant representations by iterative self-training. The method is based on the idea of GANs. The authors propose a method called GAN-GIFT, which is an extension of the GAN algorithm. The main difference between GAN and GAN is that GAN can be used to train a model that is invariant to domain shifts. The proposed method is evaluated on a variety of benchmark datasets and compared to other domain adaptation methods."
1387,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on an attention-based mechanism, where the model is trained on a set of videos and comments from the user comments. The model is evaluated on a variety of video retrieval benchmarks. The results show that the proposed method outperforms the state-of-the-art."
1388,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on an attention-based mechanism, where the model is trained on a set of videos and comments from the user comments. The model is evaluated on a variety of video retrieval benchmarks. The results show that the proposed method outperforms the state-of-the-art."
1389,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on an attention-based mechanism, where the model is trained on a set of videos and comments from the user comments. The model is evaluated on a variety of video retrieval benchmarks. The results show that the proposed method outperforms the state-of-the-art."
1390,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"This paper proposes a method for multi-modal video-text retrieval. The proposed method is based on an attention-based mechanism, where the model is trained on a set of videos and comments from the user comments. The model is evaluated on a variety of video retrieval benchmarks. The results show that the proposed method outperforms the state-of-the-art."
1391,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes a method for unsupervised skill learning based on an information gain auxiliary objective. The proposed method, DISDAIN, is motivated by the observation that extrinsic rewards in pseudocount-based methods can lead to pessimism. The authors propose to use a discriminator to distinguish between latent-conditioned trajectories and latent-conditional trajectories conditioned on the discriminator. The discriminator is trained with a loss function that penalizes trajectories that are not distinguishable from each other. The method is evaluated on a tabular grid world and shows that it outperforms existing methods."
1392,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes a method for unsupervised skill learning based on an information gain auxiliary objective. The proposed method, DISDAIN, is motivated by the observation that extrinsic rewards in pseudocount-based methods can lead to pessimism. The authors propose to use a discriminator to distinguish between latent-conditioned trajectories and latent-conditional trajectories conditioned on the discriminator. The discriminator is trained with a loss function that penalizes trajectories that are not distinguishable from each other. The method is evaluated on a tabular grid world and shows that it outperforms existing methods."
1393,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes a method for unsupervised skill learning based on an information gain auxiliary objective. The proposed method, DISDAIN, is motivated by the observation that extrinsic rewards in pseudocount-based methods can lead to pessimism. The authors propose to use a discriminator to distinguish between latent-conditioned trajectories and latent-conditional trajectories conditioned on the discriminator. The discriminator is trained with a loss function that penalizes trajectories that are not distinguishable from each other. The method is evaluated on a tabular grid world and shows that it outperforms existing methods."
1394,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper proposes a method for unsupervised skill learning based on an information gain auxiliary objective. The proposed method, DISDAIN, is motivated by the observation that extrinsic rewards in pseudocount-based methods can lead to pessimism. The authors propose to use a discriminator to distinguish between latent-conditioned trajectories and latent-conditional trajectories conditioned on the discriminator. The discriminator is trained with a loss function that penalizes trajectories that are not distinguishable from each other. The method is evaluated on a tabular grid world and shows that it outperforms existing methods."
1395,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a transformer-based method for molecular graph generation. The proposed method is based on the Transformer architecture. The main idea is to construct a spanning tree with residual edges, which is then used to generate molecules. The experimental results on QM9 and ZINC250k demonstrate the effectiveness of the proposed method."
1396,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a transformer-based method for molecular graph generation. The proposed method is based on the Transformer architecture. The main idea is to construct a spanning tree with residual edges, which is then used to generate molecules. The experimental results on QM9 and ZINC250k demonstrate the effectiveness of the proposed method."
1397,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a transformer-based method for molecular graph generation. The proposed method is based on the Transformer architecture. The main idea is to construct a spanning tree with residual edges, which is then used to generate molecules. The experimental results on QM9 and ZINC250k demonstrate the effectiveness of the proposed method."
1398,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper proposes a transformer-based method for molecular graph generation. The proposed method is based on the Transformer architecture. The main idea is to construct a spanning tree with residual edges, which is then used to generate molecules. The experimental results on QM9 and ZINC250k demonstrate the effectiveness of the proposed method."
1399,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of PNNs in terms of the neural tangent kernel (NTK). The authors show that the NTK is biased towards low-frequency functions, and propose a parametrization of the PNN based on polynomials. The authors also propose a new family of neural networks called Π-Net, which is based on the spectral analysis of the Neural Tangent Kernel. "
1400,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of PNNs in terms of the neural tangent kernel (NTK). The authors show that the NTK is biased towards low-frequency functions, and propose a parametrization of the PNN based on polynomials. The authors also propose a new family of neural networks called Π-Net, which is based on the spectral analysis of the Neural Tangent Kernel. "
1401,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of PNNs in terms of the neural tangent kernel (NTK). The authors show that the NTK is biased towards low-frequency functions, and propose a parametrization of the PNN based on polynomials. The authors also propose a new family of neural networks called Π-Net, which is based on the spectral analysis of the Neural Tangent Kernel. "
1402,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper studies the spectral bias of PNNs in terms of the neural tangent kernel (NTK). The authors show that the NTK is biased towards low-frequency functions, and propose a parametrization of the PNN based on polynomials. The authors also propose a new family of neural networks called Π-Net, which is based on the spectral analysis of the Neural Tangent Kernel. "
1403,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method for learning sparse subnetwork masks for sparse neural networks (NNs). The method is based on the edge-popup method, which is a two-stage algorithm for learning hidden subnetworks in randomly initialized NNs. The first step of the method is to learn a sparse mask for each hidden layer of the network, and the second step is to unmask the hidden subnetwork mask. The authors show that the proposed method outperforms edge-updates on CIFAR-10/100 and WideResNet-28 datasets."
1404,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method for learning sparse subnetwork masks for sparse neural networks (NNs). The method is based on the edge-popup method, which is a two-stage algorithm for learning hidden subnetworks in randomly initialized NNs. The first step of the method is to learn a sparse mask for each hidden layer of the network, and the second step is to unmask the hidden subnetwork mask. The authors show that the proposed method outperforms edge-updates on CIFAR-10/100 and WideResNet-28 datasets."
1405,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method for learning sparse subnetwork masks for sparse neural networks (NNs). The method is based on the edge-popup method, which is a two-stage algorithm for learning hidden subnetworks in randomly initialized NNs. The first step of the method is to learn a sparse mask for each hidden layer of the network, and the second step is to unmask the hidden subnetwork mask. The authors show that the proposed method outperforms edge-updates on CIFAR-10/100 and WideResNet-28 datasets."
1406,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper proposes a method for learning sparse subnetwork masks for sparse neural networks (NNs). The method is based on the edge-popup method, which is a two-stage algorithm for learning hidden subnetworks in randomly initialized NNs. The first step of the method is to learn a sparse mask for each hidden layer of the network, and the second step is to unmask the hidden subnetwork mask. The authors show that the proposed method outperforms edge-updates on CIFAR-10/100 and WideResNet-28 datasets."
1407,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a novel method to train a graph neural network (GNN) that is robust to adversarial attacks. The proposed method is based on the GUGNN framework. The main idea is to train the GNN on a set of perturbed graphs, and then use a convolutional network to extract features from the perturbed graph. The authors show that the proposed method can be used to train GNNs that are robust to perturbations in the input graph. "
1408,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a novel method to train a graph neural network (GNN) that is robust to adversarial attacks. The proposed method is based on the GUGNN framework. The main idea is to train the GNN on a set of perturbed graphs, and then use a convolutional network to extract features from the perturbed graph. The authors show that the proposed method can be used to train GNNs that are robust to perturbations in the input graph. "
1409,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a novel method to train a graph neural network (GNN) that is robust to adversarial attacks. The proposed method is based on the GUGNN framework. The main idea is to train the GNN on a set of perturbed graphs, and then use a convolutional network to extract features from the perturbed graph. The authors show that the proposed method can be used to train GNNs that are robust to perturbations in the input graph. "
1410,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper proposes a novel method to train a graph neural network (GNN) that is robust to adversarial attacks. The proposed method is based on the GUGNN framework. The main idea is to train the GNN on a set of perturbed graphs, and then use a convolutional network to extract features from the perturbed graph. The authors show that the proposed method can be used to train GNNs that are robust to perturbations in the input graph. "
1411,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,"This paper proposes a method for texture mapping of 3D surfaces. The method is based on a continuous bijective mapping of 2D texture-space coordinates and 3D surface positions. The authors propose to use a differentiable rendering pipeline to learn a surface parameterization network. The proposed method is evaluated on 3D scene reconstruction, view synthesis, and document texture editing. "
1412,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,"This paper proposes a method for texture mapping of 3D surfaces. The method is based on a continuous bijective mapping of 2D texture-space coordinates and 3D surface positions. The authors propose to use a differentiable rendering pipeline to learn a surface parameterization network. The proposed method is evaluated on 3D scene reconstruction, view synthesis, and document texture editing. "
1413,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,"This paper proposes a method for texture mapping of 3D surfaces. The method is based on a continuous bijective mapping of 2D texture-space coordinates and 3D surface positions. The authors propose to use a differentiable rendering pipeline to learn a surface parameterization network. The proposed method is evaluated on 3D scene reconstruction, view synthesis, and document texture editing. "
1414,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,"This paper proposes a method for texture mapping of 3D surfaces. The method is based on a continuous bijective mapping of 2D texture-space coordinates and 3D surface positions. The authors propose to use a differentiable rendering pipeline to learn a surface parameterization network. The proposed method is evaluated on 3D scene reconstruction, view synthesis, and document texture editing. "
1415,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes a novel method for fine-grained image recognition. The authors propose a method to reduce the resolution of sub-sampled features in a convolutional neural network. The main idea is to use a downsampling algorithm to improve the quality of the feature resolution of the network, and then use a pooling operation to increase the granularity and scale of the receptive field. The proposed method is evaluated on image classification and retrieval tasks."
1416,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes a novel method for fine-grained image recognition. The authors propose a method to reduce the resolution of sub-sampled features in a convolutional neural network. The main idea is to use a downsampling algorithm to improve the quality of the feature resolution of the network, and then use a pooling operation to increase the granularity and scale of the receptive field. The proposed method is evaluated on image classification and retrieval tasks."
1417,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes a novel method for fine-grained image recognition. The authors propose a method to reduce the resolution of sub-sampled features in a convolutional neural network. The main idea is to use a downsampling algorithm to improve the quality of the feature resolution of the network, and then use a pooling operation to increase the granularity and scale of the receptive field. The proposed method is evaluated on image classification and retrieval tasks."
1418,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"This paper proposes a novel method for fine-grained image recognition. The authors propose a method to reduce the resolution of sub-sampled features in a convolutional neural network. The main idea is to use a downsampling algorithm to improve the quality of the feature resolution of the network, and then use a pooling operation to increase the granularity and scale of the receptive field. The proposed method is evaluated on image classification and retrieval tasks."
1419,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,This paper proposes a domain-invariant learning method for graph-structured data. The main idea is to use a graph editer to learn a graph representation that is invariant to domain-distribution shifts. The proposed method is evaluated on several real-world datasets and compared to several baselines.
1420,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,This paper proposes a domain-invariant learning method for graph-structured data. The main idea is to use a graph editer to learn a graph representation that is invariant to domain-distribution shifts. The proposed method is evaluated on several real-world datasets and compared to several baselines.
1421,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,This paper proposes a domain-invariant learning method for graph-structured data. The main idea is to use a graph editer to learn a graph representation that is invariant to domain-distribution shifts. The proposed method is evaluated on several real-world datasets and compared to several baselines.
1422,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,This paper proposes a domain-invariant learning method for graph-structured data. The main idea is to use a graph editer to learn a graph representation that is invariant to domain-distribution shifts. The proposed method is evaluated on several real-world datasets and compared to several baselines.
1423,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for contrastive learning of time series data. The proposed method is based on the idea that augmentations can be used to improve the robustness of the encoder and the discriminative performance of the decoder. The authors propose to use a pre-trained meta-learner to select the augmentations and then use the meta-learned encoder to learn the discriminator. The method is evaluated on the classification task and the forecasting task."
1424,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for contrastive learning of time series data. The proposed method is based on the idea that augmentations can be used to improve the robustness of the encoder and the discriminative performance of the decoder. The authors propose to use a pre-trained meta-learner to select the augmentations and then use the meta-learned encoder to learn the discriminator. The method is evaluated on the classification task and the forecasting task."
1425,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for contrastive learning of time series data. The proposed method is based on the idea that augmentations can be used to improve the robustness of the encoder and the discriminative performance of the decoder. The authors propose to use a pre-trained meta-learner to select the augmentations and then use the meta-learned encoder to learn the discriminator. The method is evaluated on the classification task and the forecasting task."
1426,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a meta-learning method for contrastive learning of time series data. The proposed method is based on the idea that augmentations can be used to improve the robustness of the encoder and the discriminative performance of the decoder. The authors propose to use a pre-trained meta-learner to select the augmentations and then use the meta-learned encoder to learn the discriminator. The method is evaluated on the classification task and the forecasting task."
1427,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the Lottery Ticket Hypothesis (LTH) and proposes a novel method for finding winning tickets in the lottery ticket hypothesis. The main contribution of the paper is to propose a novel renormalization scheme for the lottery tickets. The method is based on the iterative magnitude pruning (ERP) method, which prunes the number of winning tickets by a factor of a factor that depends on the magnitude of the winning ticket. Theoretical analysis of the proposed method is provided. Experiments are conducted on a large scale lottery ticket experiment to show the effectiveness of the method."
1428,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the Lottery Ticket Hypothesis (LTH) and proposes a novel method for finding winning tickets in the lottery ticket hypothesis. The main contribution of the paper is to propose a novel renormalization scheme for the lottery tickets. The method is based on the iterative magnitude pruning (ERP) method, which prunes the number of winning tickets by a factor of a factor that depends on the magnitude of the winning ticket. Theoretical analysis of the proposed method is provided. Experiments are conducted on a large scale lottery ticket experiment to show the effectiveness of the method."
1429,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the Lottery Ticket Hypothesis (LTH) and proposes a novel method for finding winning tickets in the lottery ticket hypothesis. The main contribution of the paper is to propose a novel renormalization scheme for the lottery tickets. The method is based on the iterative magnitude pruning (ERP) method, which prunes the number of winning tickets by a factor of a factor that depends on the magnitude of the winning ticket. Theoretical analysis of the proposed method is provided. Experiments are conducted on a large scale lottery ticket experiment to show the effectiveness of the method."
1430,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"This paper studies the Lottery Ticket Hypothesis (LTH) and proposes a novel method for finding winning tickets in the lottery ticket hypothesis. The main contribution of the paper is to propose a novel renormalization scheme for the lottery tickets. The method is based on the iterative magnitude pruning (ERP) method, which prunes the number of winning tickets by a factor of a factor that depends on the magnitude of the winning ticket. Theoretical analysis of the proposed method is provided. Experiments are conducted on a large scale lottery ticket experiment to show the effectiveness of the method."
1431,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,This paper proposes a new dual-encoder (DE) model for cross-attention (CA) models. The main idea is to use a joint embedding for both the input and the output of the dual encoder. The authors also propose a distillation strategy to improve the performance of the proposed model. The experimental results show that the proposed method outperforms the state-of-the-art DE models.
1432,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,This paper proposes a new dual-encoder (DE) model for cross-attention (CA) models. The main idea is to use a joint embedding for both the input and the output of the dual encoder. The authors also propose a distillation strategy to improve the performance of the proposed model. The experimental results show that the proposed method outperforms the state-of-the-art DE models.
1433,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,This paper proposes a new dual-encoder (DE) model for cross-attention (CA) models. The main idea is to use a joint embedding for both the input and the output of the dual encoder. The authors also propose a distillation strategy to improve the performance of the proposed model. The experimental results show that the proposed method outperforms the state-of-the-art DE models.
1434,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,This paper proposes a new dual-encoder (DE) model for cross-attention (CA) models. The main idea is to use a joint embedding for both the input and the output of the dual encoder. The authors also propose a distillation strategy to improve the performance of the proposed model. The experimental results show that the proposed method outperforms the state-of-the-art DE models.
1435,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper proposes Importance Sampling (IS) for Policy Optimization (PO) algorithms, which is an off-policy method that aims to improve the performance of policy optimization algorithms. The main idea is to use a Monte Carlo simulation to estimate the importance of each policy in order to reduce the variance of the policy distribution. The authors show that the proposed method is more robust to small batch sizes and can be used for policy evaluation. The paper also proposes Optimal Policy Evaluation (POPE) which is a new algorithm for policy optimization."
1436,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper proposes Importance Sampling (IS) for Policy Optimization (PO) algorithms, which is an off-policy method that aims to improve the performance of policy optimization algorithms. The main idea is to use a Monte Carlo simulation to estimate the importance of each policy in order to reduce the variance of the policy distribution. The authors show that the proposed method is more robust to small batch sizes and can be used for policy evaluation. The paper also proposes Optimal Policy Evaluation (POPE) which is a new algorithm for policy optimization."
1437,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper proposes Importance Sampling (IS) for Policy Optimization (PO) algorithms, which is an off-policy method that aims to improve the performance of policy optimization algorithms. The main idea is to use a Monte Carlo simulation to estimate the importance of each policy in order to reduce the variance of the policy distribution. The authors show that the proposed method is more robust to small batch sizes and can be used for policy evaluation. The paper also proposes Optimal Policy Evaluation (POPE) which is a new algorithm for policy optimization."
1438,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"This paper proposes Importance Sampling (IS) for Policy Optimization (PO) algorithms, which is an off-policy method that aims to improve the performance of policy optimization algorithms. The main idea is to use a Monte Carlo simulation to estimate the importance of each policy in order to reduce the variance of the policy distribution. The authors show that the proposed method is more robust to small batch sizes and can be used for policy evaluation. The paper also proposes Optimal Policy Evaluation (POPE) which is a new algorithm for policy optimization."
1439,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,This paper proposes a graph parallelism method for graph neural networks (GNNs) that can be applied to graph-parallelized models. The proposed method is based on the idea that GNNs can be used to model higher-order interactions in graphs. The authors show that their method is able to achieve better performance on the S2EF and IS2RS tasks than the state-of-the-art DimeNet++ and GemNet models on the AFbT metric. 
1440,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,This paper proposes a graph parallelism method for graph neural networks (GNNs) that can be applied to graph-parallelized models. The proposed method is based on the idea that GNNs can be used to model higher-order interactions in graphs. The authors show that their method is able to achieve better performance on the S2EF and IS2RS tasks than the state-of-the-art DimeNet++ and GemNet models on the AFbT metric. 
1441,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,This paper proposes a graph parallelism method for graph neural networks (GNNs) that can be applied to graph-parallelized models. The proposed method is based on the idea that GNNs can be used to model higher-order interactions in graphs. The authors show that their method is able to achieve better performance on the S2EF and IS2RS tasks than the state-of-the-art DimeNet++ and GemNet models on the AFbT metric. 
1442,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,This paper proposes a graph parallelism method for graph neural networks (GNNs) that can be applied to graph-parallelized models. The proposed method is based on the idea that GNNs can be used to model higher-order interactions in graphs. The authors show that their method is able to achieve better performance on the S2EF and IS2RS tasks than the state-of-the-art DimeNet++ and GemNet models on the AFbT metric. 
1443,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a novel method for text generation based on time control. The method is based on a latent stochastic process of interest, where the latent representation of the language model is learned to predict the time step of the document plan. The authors show that the proposed method is able to improve the performance of GPT2 on long text generation tasks. They also show that their method can improve the quality of the generated text."
1444,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a novel method for text generation based on time control. The method is based on a latent stochastic process of interest, where the latent representation of the language model is learned to predict the time step of the document plan. The authors show that the proposed method is able to improve the performance of GPT2 on long text generation tasks. They also show that their method can improve the quality of the generated text."
1445,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a novel method for text generation based on time control. The method is based on a latent stochastic process of interest, where the latent representation of the language model is learned to predict the time step of the document plan. The authors show that the proposed method is able to improve the performance of GPT2 on long text generation tasks. They also show that their method can improve the quality of the generated text."
1446,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a novel method for text generation based on time control. The method is based on a latent stochastic process of interest, where the latent representation of the language model is learned to predict the time step of the document plan. The authors show that the proposed method is able to improve the performance of GPT2 on long text generation tasks. They also show that their method can improve the quality of the generated text."
1447,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for object-centric representation learning. The main idea is to learn a latent code representation of the 3D environment, which is then used to predict the future sensory input of an object. The proposed method is evaluated on a synthetic dataset, and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of object segmentation accuracy."
1448,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for object-centric representation learning. The main idea is to learn a latent code representation of the 3D environment, which is then used to predict the future sensory input of an object. The proposed method is evaluated on a synthetic dataset, and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of object segmentation accuracy."
1449,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for object-centric representation learning. The main idea is to learn a latent code representation of the 3D environment, which is then used to predict the future sensory input of an object. The proposed method is evaluated on a synthetic dataset, and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of object segmentation accuracy."
1450,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"This paper proposes a method for object-centric representation learning. The main idea is to learn a latent code representation of the 3D environment, which is then used to predict the future sensory input of an object. The proposed method is evaluated on a synthetic dataset, and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of object segmentation accuracy."
1451,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based method for disentangled representation learning for spatio-temporal mobility forecasting. The method is based on the VAE architecture and uses a deep generative model to learn a latent representation of the dynamic and static components of the data. The model is trained to reconstruct the spatial and temporal features from the original data using the reconstructed features. Experiments show that the proposed method outperforms the state-of-the-art models.
1452,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based method for disentangled representation learning for spatio-temporal mobility forecasting. The method is based on the VAE architecture and uses a deep generative model to learn a latent representation of the dynamic and static components of the data. The model is trained to reconstruct the spatial and temporal features from the original data using the reconstructed features. Experiments show that the proposed method outperforms the state-of-the-art models.
1453,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based method for disentangled representation learning for spatio-temporal mobility forecasting. The method is based on the VAE architecture and uses a deep generative model to learn a latent representation of the dynamic and static components of the data. The model is trained to reconstruct the spatial and temporal features from the original data using the reconstructed features. Experiments show that the proposed method outperforms the state-of-the-art models.
1454,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a VAE-based method for disentangled representation learning for spatio-temporal mobility forecasting. The method is based on the VAE architecture and uses a deep generative model to learn a latent representation of the dynamic and static components of the data. The model is trained to reconstruct the spatial and temporal features from the original data using the reconstructed features. Experiments show that the proposed method outperforms the state-of-the-art models.
1455,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a new architecture for probabilistic interpolation of irregularly sampled time series. The authors propose a heteroscedastic temporal variational autoencoder (HeTVAE) that combines a temporal VAE with a hetero-symmetric output layer. The proposed architecture is based on a combination of the temporal VAEs and the heteroscaledastic output layers. The experimental results show that the proposed architecture outperforms the state-of-the-art models on a variety of datasets.
1456,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a new architecture for probabilistic interpolation of irregularly sampled time series. The authors propose a heteroscedastic temporal variational autoencoder (HeTVAE) that combines a temporal VAE with a hetero-symmetric output layer. The proposed architecture is based on a combination of the temporal VAEs and the heteroscaledastic output layers. The experimental results show that the proposed architecture outperforms the state-of-the-art models on a variety of datasets.
1457,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a new architecture for probabilistic interpolation of irregularly sampled time series. The authors propose a heteroscedastic temporal variational autoencoder (HeTVAE) that combines a temporal VAE with a hetero-symmetric output layer. The proposed architecture is based on a combination of the temporal VAEs and the heteroscaledastic output layers. The experimental results show that the proposed architecture outperforms the state-of-the-art models on a variety of datasets.
1458,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,This paper proposes a new architecture for probabilistic interpolation of irregularly sampled time series. The authors propose a heteroscedastic temporal variational autoencoder (HeTVAE) that combines a temporal VAE with a hetero-symmetric output layer. The proposed architecture is based on a combination of the temporal VAEs and the heteroscaledastic output layers. The experimental results show that the proposed architecture outperforms the state-of-the-art models on a variety of datasets.
1459,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes a graph-based partitioning method for deep neural networks. The proposed method is based on the idea of partitioning a neural network into a set of ""one"" and ""one-sided"" edges. The authors propose to use a spectrally clustering approach to partition the network into one-sided edges and then use a weighted combination of the weights and the correlations of activations to find the ones with the highest importance and coherence. The paper shows that the proposed method can be applied to a variety of deep neural network architectures."
1460,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes a graph-based partitioning method for deep neural networks. The proposed method is based on the idea of partitioning a neural network into a set of ""one"" and ""one-sided"" edges. The authors propose to use a spectrally clustering approach to partition the network into one-sided edges and then use a weighted combination of the weights and the correlations of activations to find the ones with the highest importance and coherence. The paper shows that the proposed method can be applied to a variety of deep neural network architectures."
1461,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes a graph-based partitioning method for deep neural networks. The proposed method is based on the idea of partitioning a neural network into a set of ""one"" and ""one-sided"" edges. The authors propose to use a spectrally clustering approach to partition the network into one-sided edges and then use a weighted combination of the weights and the correlations of activations to find the ones with the highest importance and coherence. The paper shows that the proposed method can be applied to a variety of deep neural network architectures."
1462,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,"This paper proposes a graph-based partitioning method for deep neural networks. The proposed method is based on the idea of partitioning a neural network into a set of ""one"" and ""one-sided"" edges. The authors propose to use a spectrally clustering approach to partition the network into one-sided edges and then use a weighted combination of the weights and the correlations of activations to find the ones with the highest importance and coherence. The paper shows that the proposed method can be applied to a variety of deep neural network architectures."
1463,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,This paper studies the lottery ticket hypothesis for lightweight speech recognition models. The authors show that sparse subnetworks are more robust to background noise than full models. They also show that the lottery tickets can be seen as a structured sparsity of the weights of the full model. The paper also shows that full models are more sensitive to background noises than the sparse subnets.
1464,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,This paper studies the lottery ticket hypothesis for lightweight speech recognition models. The authors show that sparse subnetworks are more robust to background noise than full models. They also show that the lottery tickets can be seen as a structured sparsity of the weights of the full model. The paper also shows that full models are more sensitive to background noises than the sparse subnets.
1465,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,This paper studies the lottery ticket hypothesis for lightweight speech recognition models. The authors show that sparse subnetworks are more robust to background noise than full models. They also show that the lottery tickets can be seen as a structured sparsity of the weights of the full model. The paper also shows that full models are more sensitive to background noises than the sparse subnets.
1466,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,This paper studies the lottery ticket hypothesis for lightweight speech recognition models. The authors show that sparse subnetworks are more robust to background noise than full models. They also show that the lottery tickets can be seen as a structured sparsity of the weights of the full model. The paper also shows that full models are more sensitive to background noises than the sparse subnets.
1467,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes ZerO, a random weight initialization method for deep neural networks. The method is based on the idea that random weights can be used to improve the reproducibility of the network. The authors show that ZerO can achieve better performance than random weights on ImageNet and CIFAR-10. They also show that the proposed method can be applied to residual networks as well."
1468,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes ZerO, a random weight initialization method for deep neural networks. The method is based on the idea that random weights can be used to improve the reproducibility of the network. The authors show that ZerO can achieve better performance than random weights on ImageNet and CIFAR-10. They also show that the proposed method can be applied to residual networks as well."
1469,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes ZerO, a random weight initialization method for deep neural networks. The method is based on the idea that random weights can be used to improve the reproducibility of the network. The authors show that ZerO can achieve better performance than random weights on ImageNet and CIFAR-10. They also show that the proposed method can be applied to residual networks as well."
1470,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes ZerO, a random weight initialization method for deep neural networks. The method is based on the idea that random weights can be used to improve the reproducibility of the network. The authors show that ZerO can achieve better performance than random weights on ImageNet and CIFAR-10. They also show that the proposed method can be applied to residual networks as well."
1471,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper studies the problem of backdoor attack defense. The authors propose a new algorithm for backdoor defense based on the minimax formulation. The main contribution of this paper is the use of implicit hypergradient to improve the robustness of the proposed algorithm. The paper is well-written and easy to follow. The experiments show that the proposed method outperforms the state-of-the-art backdoor defense methods.
1472,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper studies the problem of backdoor attack defense. The authors propose a new algorithm for backdoor defense based on the minimax formulation. The main contribution of this paper is the use of implicit hypergradient to improve the robustness of the proposed algorithm. The paper is well-written and easy to follow. The experiments show that the proposed method outperforms the state-of-the-art backdoor defense methods.
1473,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper studies the problem of backdoor attack defense. The authors propose a new algorithm for backdoor defense based on the minimax formulation. The main contribution of this paper is the use of implicit hypergradient to improve the robustness of the proposed algorithm. The paper is well-written and easy to follow. The experiments show that the proposed method outperforms the state-of-the-art backdoor defense methods.
1474,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,This paper studies the problem of backdoor attack defense. The authors propose a new algorithm for backdoor defense based on the minimax formulation. The main contribution of this paper is the use of implicit hypergradient to improve the robustness of the proposed algorithm. The paper is well-written and easy to follow. The experiments show that the proposed method outperforms the state-of-the-art backdoor defense methods.
1475,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD with smooth second derivatives on strongly convex functions. The authors show that the convergence rate of SGD is bounded by a factor of 2.5, which is a factor that depends on the number of permutations. They also provide a convergence analysis for quadratic and strongly-convex functions, which shows that the rate of convergence converges faster than that of random SGD. "
1476,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD with smooth second derivatives on strongly convex functions. The authors show that the convergence rate of SGD is bounded by a factor of 2.5, which is a factor that depends on the number of permutations. They also provide a convergence analysis for quadratic and strongly-convex functions, which shows that the rate of convergence converges faster than that of random SGD. "
1477,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD with smooth second derivatives on strongly convex functions. The authors show that the convergence rate of SGD is bounded by a factor of 2.5, which is a factor that depends on the number of permutations. They also provide a convergence analysis for quadratic and strongly-convex functions, which shows that the rate of convergence converges faster than that of random SGD. "
1478,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This paper studies the convergence of permutation-based SGD with smooth second derivatives on strongly convex functions. The authors show that the convergence rate of SGD is bounded by a factor of 2.5, which is a factor that depends on the number of permutations. They also provide a convergence analysis for quadratic and strongly-convex functions, which shows that the rate of convergence converges faster than that of random SGD. "
1479,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,This paper studies the problem of architecture optimization for flow models. The authors propose a method to optimize the architecture of a flow model in a discrete space. The main contribution of this paper is to propose a block-wise alternating optimization algorithm for architecture optimization. The proposed method is based on the idea that the architecture can be optimized in a mixed distribution. The paper provides an approximate upper bound on the global minimum of the optimal architecture in the discrete space and a global minimum for the mixture distribution in the continuous space. 
1480,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,This paper studies the problem of architecture optimization for flow models. The authors propose a method to optimize the architecture of a flow model in a discrete space. The main contribution of this paper is to propose a block-wise alternating optimization algorithm for architecture optimization. The proposed method is based on the idea that the architecture can be optimized in a mixed distribution. The paper provides an approximate upper bound on the global minimum of the optimal architecture in the discrete space and a global minimum for the mixture distribution in the continuous space. 
1481,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,This paper studies the problem of architecture optimization for flow models. The authors propose a method to optimize the architecture of a flow model in a discrete space. The main contribution of this paper is to propose a block-wise alternating optimization algorithm for architecture optimization. The proposed method is based on the idea that the architecture can be optimized in a mixed distribution. The paper provides an approximate upper bound on the global minimum of the optimal architecture in the discrete space and a global minimum for the mixture distribution in the continuous space. 
1482,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,This paper studies the problem of architecture optimization for flow models. The authors propose a method to optimize the architecture of a flow model in a discrete space. The main contribution of this paper is to propose a block-wise alternating optimization algorithm for architecture optimization. The proposed method is based on the idea that the architecture can be optimized in a mixed distribution. The paper provides an approximate upper bound on the global minimum of the optimal architecture in the discrete space and a global minimum for the mixture distribution in the continuous space. 
1483,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,This paper proposes a method for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) and Cluster Learnability (CL). The proposed method is based on a KNN classifier and cluster labels. The authors show that the proposed method outperforms existing SSL methods on ImageNet and CIFAR-10. 
1484,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,This paper proposes a method for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) and Cluster Learnability (CL). The proposed method is based on a KNN classifier and cluster labels. The authors show that the proposed method outperforms existing SSL methods on ImageNet and CIFAR-10. 
1485,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,This paper proposes a method for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) and Cluster Learnability (CL). The proposed method is based on a KNN classifier and cluster labels. The authors show that the proposed method outperforms existing SSL methods on ImageNet and CIFAR-10. 
1486,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,This paper proposes a method for self-supervised learning (SSL) based on the Intrinsic Dimension (ID) and Cluster Learnability (CL). The proposed method is based on a KNN classifier and cluster labels. The authors show that the proposed method outperforms existing SSL methods on ImageNet and CIFAR-10. 
1487,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes a new adversarial attack method for black box attacks. The proposed method, called saliency attack, is a gradient-free black box attack that is based on the observation that the salient region of the perturbation is the most important part of the attack. The authors also propose a detection-based defense method to improve the imperceptibility performance of black box adversarial attacks."
1488,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes a new adversarial attack method for black box attacks. The proposed method, called saliency attack, is a gradient-free black box attack that is based on the observation that the salient region of the perturbation is the most important part of the attack. The authors also propose a detection-based defense method to improve the imperceptibility performance of black box adversarial attacks."
1489,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes a new adversarial attack method for black box attacks. The proposed method, called saliency attack, is a gradient-free black box attack that is based on the observation that the salient region of the perturbation is the most important part of the attack. The authors also propose a detection-based defense method to improve the imperceptibility performance of black box adversarial attacks."
1490,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposes a new adversarial attack method for black box attacks. The proposed method, called saliency attack, is a gradient-free black box attack that is based on the observation that the salient region of the perturbation is the most important part of the attack. The authors also propose a detection-based defense method to improve the imperceptibility performance of black box adversarial attacks."
1491,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a Transformer-based model for molecular graph modeling. The proposed model is based on the Transformer architecture. The main difference between the proposed model and the existing models is that Graph2SMILES is an end-to-end architecture, which is able to handle both long-range and inter-molecular interactions. The authors also propose a global attention encoder to capture long-term and intermolecule interactions. Experiments are conducted on the USPTO-50k dataset and the one-step retrosynthesis dataset."
1492,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a Transformer-based model for molecular graph modeling. The proposed model is based on the Transformer architecture. The main difference between the proposed model and the existing models is that Graph2SMILES is an end-to-end architecture, which is able to handle both long-range and inter-molecular interactions. The authors also propose a global attention encoder to capture long-term and intermolecule interactions. Experiments are conducted on the USPTO-50k dataset and the one-step retrosynthesis dataset."
1493,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a Transformer-based model for molecular graph modeling. The proposed model is based on the Transformer architecture. The main difference between the proposed model and the existing models is that Graph2SMILES is an end-to-end architecture, which is able to handle both long-range and inter-molecular interactions. The authors also propose a global attention encoder to capture long-term and intermolecule interactions. Experiments are conducted on the USPTO-50k dataset and the one-step retrosynthesis dataset."
1494,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"This paper proposes a Transformer-based model for molecular graph modeling. The proposed model is based on the Transformer architecture. The main difference between the proposed model and the existing models is that Graph2SMILES is an end-to-end architecture, which is able to handle both long-range and inter-molecular interactions. The authors also propose a global attention encoder to capture long-term and intermolecule interactions. Experiments are conducted on the USPTO-50k dataset and the one-step retrosynthesis dataset."
1495,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical policy learning method for automatic disease diagnosis in a task-oriented dialogues setting. The proposed method consists of a high-level policy, a low-level model, and a symptom checker. The high level policy is trained to maximize the mutual information between the high level and the low level policy. The low level model is trained on the data generated by the symptom checkers and the disease classifier. The authors show that the proposed method outperforms existing methods in terms of accuracy and symptom recall."
1496,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical policy learning method for automatic disease diagnosis in a task-oriented dialogues setting. The proposed method consists of a high-level policy, a low-level model, and a symptom checker. The high level policy is trained to maximize the mutual information between the high level and the low level policy. The low level model is trained on the data generated by the symptom checkers and the disease classifier. The authors show that the proposed method outperforms existing methods in terms of accuracy and symptom recall."
1497,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical policy learning method for automatic disease diagnosis in a task-oriented dialogues setting. The proposed method consists of a high-level policy, a low-level model, and a symptom checker. The high level policy is trained to maximize the mutual information between the high level and the low level policy. The low level model is trained on the data generated by the symptom checkers and the disease classifier. The authors show that the proposed method outperforms existing methods in terms of accuracy and symptom recall."
1498,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"This paper proposes a hierarchical policy learning method for automatic disease diagnosis in a task-oriented dialogues setting. The proposed method consists of a high-level policy, a low-level model, and a symptom checker. The high level policy is trained to maximize the mutual information between the high level and the low level policy. The low level model is trained on the data generated by the symptom checkers and the disease classifier. The authors show that the proposed method outperforms existing methods in terms of accuracy and symptom recall."
1499,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a personalized federated learning (SSFL) framework for self-supervised learning. The authors propose two algorithms: perFedAvg and Ditto to address label deficiency in the centralized self supervised learning setting. In particular, the authors propose Per-SSFL to address the label deficiency problem in the federated setting. Experiments are conducted on CIFAR-10 and synthetic non-I.I.D. dataset to demonstrate the effectiveness of the proposed algorithms."
1500,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a personalized federated learning (SSFL) framework for self-supervised learning. The authors propose two algorithms: perFedAvg and Ditto to address label deficiency in the centralized self supervised learning setting. In particular, the authors propose Per-SSFL to address the label deficiency problem in the federated setting. Experiments are conducted on CIFAR-10 and synthetic non-I.I.D. dataset to demonstrate the effectiveness of the proposed algorithms."
1501,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a personalized federated learning (SSFL) framework for self-supervised learning. The authors propose two algorithms: perFedAvg and Ditto to address label deficiency in the centralized self supervised learning setting. In particular, the authors propose Per-SSFL to address the label deficiency problem in the federated setting. Experiments are conducted on CIFAR-10 and synthetic non-I.I.D. dataset to demonstrate the effectiveness of the proposed algorithms."
1502,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,"This paper proposes a personalized federated learning (SSFL) framework for self-supervised learning. The authors propose two algorithms: perFedAvg and Ditto to address label deficiency in the centralized self supervised learning setting. In particular, the authors propose Per-SSFL to address the label deficiency problem in the federated setting. Experiments are conducted on CIFAR-10 and synthetic non-I.I.D. dataset to demonstrate the effectiveness of the proposed algorithms."
1503,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,This paper studies the problem of training deep neural networks (DNNs) with partial differential equations (PDEs). The authors propose a method to train DNNs with PDEs. The method is based on the adjustment operator of the ResNet-like DNN. The authors show that the proposed method can improve the robustness of DNN models to adversarial perturbations. They also show that their method can reduce the generalization gap between DNN and baseline models.
1504,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,This paper studies the problem of training deep neural networks (DNNs) with partial differential equations (PDEs). The authors propose a method to train DNNs with PDEs. The method is based on the adjustment operator of the ResNet-like DNN. The authors show that the proposed method can improve the robustness of DNN models to adversarial perturbations. They also show that their method can reduce the generalization gap between DNN and baseline models.
1505,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,This paper studies the problem of training deep neural networks (DNNs) with partial differential equations (PDEs). The authors propose a method to train DNNs with PDEs. The method is based on the adjustment operator of the ResNet-like DNN. The authors show that the proposed method can improve the robustness of DNN models to adversarial perturbations. They also show that their method can reduce the generalization gap between DNN and baseline models.
1506,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,This paper studies the problem of training deep neural networks (DNNs) with partial differential equations (PDEs). The authors propose a method to train DNNs with PDEs. The method is based on the adjustment operator of the ResNet-like DNN. The authors show that the proposed method can improve the robustness of DNN models to adversarial perturbations. They also show that their method can reduce the generalization gap between DNN and baseline models.
1507,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,This paper studies the emergence of language in language games. The authors propose a contrastive loss to improve the expressivity of emergent languages. They show that the proposed loss is more effective than the referential loss. They also provide a theoretical analysis to support their claims.
1508,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,This paper studies the emergence of language in language games. The authors propose a contrastive loss to improve the expressivity of emergent languages. They show that the proposed loss is more effective than the referential loss. They also provide a theoretical analysis to support their claims.
1509,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,This paper studies the emergence of language in language games. The authors propose a contrastive loss to improve the expressivity of emergent languages. They show that the proposed loss is more effective than the referential loss. They also provide a theoretical analysis to support their claims.
1510,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,This paper studies the emergence of language in language games. The authors propose a contrastive loss to improve the expressivity of emergent languages. They show that the proposed loss is more effective than the referential loss. They also provide a theoretical analysis to support their claims.
1511,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a method for exploration in bandit problems. The main idea is to use Sample Average Uncertainty (SAU) as a regularizer to encourage exploration in the bandit setting. The authors also propose a new exploration strategy called δ-exploration, which is based on value predictions. The experimental results show that the proposed method outperforms the baselines in the Deep Q-learning setting."
1512,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a method for exploration in bandit problems. The main idea is to use Sample Average Uncertainty (SAU) as a regularizer to encourage exploration in the bandit setting. The authors also propose a new exploration strategy called δ-exploration, which is based on value predictions. The experimental results show that the proposed method outperforms the baselines in the Deep Q-learning setting."
1513,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a method for exploration in bandit problems. The main idea is to use Sample Average Uncertainty (SAU) as a regularizer to encourage exploration in the bandit setting. The authors also propose a new exploration strategy called δ-exploration, which is based on value predictions. The experimental results show that the proposed method outperforms the baselines in the Deep Q-learning setting."
1514,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"This paper proposes a method for exploration in bandit problems. The main idea is to use Sample Average Uncertainty (SAU) as a regularizer to encourage exploration in the bandit setting. The authors also propose a new exploration strategy called δ-exploration, which is based on value predictions. The experimental results show that the proposed method outperforms the baselines in the Deep Q-learning setting."
1515,SP:2f6e266b03939c96434834579999707d3268c5d6,This paper proposes a generative adversarial network (GAN) for video generation. The main idea is to use implicit neural representations (INRs) to represent the continuous dynamics of videos. The proposed method is evaluated on video synthesis and video extrapolation tasks.
1516,SP:2f6e266b03939c96434834579999707d3268c5d6,This paper proposes a generative adversarial network (GAN) for video generation. The main idea is to use implicit neural representations (INRs) to represent the continuous dynamics of videos. The proposed method is evaluated on video synthesis and video extrapolation tasks.
1517,SP:2f6e266b03939c96434834579999707d3268c5d6,This paper proposes a generative adversarial network (GAN) for video generation. The main idea is to use implicit neural representations (INRs) to represent the continuous dynamics of videos. The proposed method is evaluated on video synthesis and video extrapolation tasks.
1518,SP:2f6e266b03939c96434834579999707d3268c5d6,This paper proposes a generative adversarial network (GAN) for video generation. The main idea is to use implicit neural representations (INRs) to represent the continuous dynamics of videos. The proposed method is evaluated on video synthesis and video extrapolation tasks.
1519,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of privacy-preserving multi-label multi-winner voting in healthcare. The authors propose a new method called DPSGD, which is a combination of Binary and Powerset voting. DPSGD is based on the idea that the power set is composed of a binary vector and a power set of powersets. The paper also proposes a canonical single-label technique called PATE, which can be combined with DPSGD to improve the performance of DPSGD. The experimental results show that DPSGD outperforms Binary and PATE."
1520,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of privacy-preserving multi-label multi-winner voting in healthcare. The authors propose a new method called DPSGD, which is a combination of Binary and Powerset voting. DPSGD is based on the idea that the power set is composed of a binary vector and a power set of powersets. The paper also proposes a canonical single-label technique called PATE, which can be combined with DPSGD to improve the performance of DPSGD. The experimental results show that DPSGD outperforms Binary and PATE."
1521,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of privacy-preserving multi-label multi-winner voting in healthcare. The authors propose a new method called DPSGD, which is a combination of Binary and Powerset voting. DPSGD is based on the idea that the power set is composed of a binary vector and a power set of powersets. The paper also proposes a canonical single-label technique called PATE, which can be combined with DPSGD to improve the performance of DPSGD. The experimental results show that DPSGD outperforms Binary and PATE."
1522,SP:878325384328c885ced7af0ebf31bbf79287c169,"This paper studies the problem of privacy-preserving multi-label multi-winner voting in healthcare. The authors propose a new method called DPSGD, which is a combination of Binary and Powerset voting. DPSGD is based on the idea that the power set is composed of a binary vector and a power set of powersets. The paper also proposes a canonical single-label technique called PATE, which can be combined with DPSGD to improve the performance of DPSGD. The experimental results show that DPSGD outperforms Binary and PATE."
1523,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper studies the effect of optimizer grafting on the performance of SGD on large neural networks. Specifically, the authors study the impact of the step size of the optimizer and the learning rate schedule of the SGD algorithm on the empirical performance of the model. The authors show that SGD with non-adaptive learning rate correction is more effective than SGD without it. They also provide empirical evidence that the bias in SGD is due to the bias of the optimization algorithm."
1524,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper studies the effect of optimizer grafting on the performance of SGD on large neural networks. Specifically, the authors study the impact of the step size of the optimizer and the learning rate schedule of the SGD algorithm on the empirical performance of the model. The authors show that SGD with non-adaptive learning rate correction is more effective than SGD without it. They also provide empirical evidence that the bias in SGD is due to the bias of the optimization algorithm."
1525,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper studies the effect of optimizer grafting on the performance of SGD on large neural networks. Specifically, the authors study the impact of the step size of the optimizer and the learning rate schedule of the SGD algorithm on the empirical performance of the model. The authors show that SGD with non-adaptive learning rate correction is more effective than SGD without it. They also provide empirical evidence that the bias in SGD is due to the bias of the optimization algorithm."
1526,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"This paper studies the effect of optimizer grafting on the performance of SGD on large neural networks. Specifically, the authors study the impact of the step size of the optimizer and the learning rate schedule of the SGD algorithm on the empirical performance of the model. The authors show that SGD with non-adaptive learning rate correction is more effective than SGD without it. They also provide empirical evidence that the bias in SGD is due to the bias of the optimization algorithm."
1527,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning in sparse reward settings. The main idea is to use a policy improvement step and a policy guidance step to guide the policy to learn a sub-optimal behavior policy in the offline demonstration data. The proposed method, called LOGO, uses a censored version of the true state observation from the demonstration data to guide policy improvement and policy guidance steps. The experiments show that the proposed method outperforms the state-of-the-art methods on a variety of tasks."
1528,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning in sparse reward settings. The main idea is to use a policy improvement step and a policy guidance step to guide the policy to learn a sub-optimal behavior policy in the offline demonstration data. The proposed method, called LOGO, uses a censored version of the true state observation from the demonstration data to guide policy improvement and policy guidance steps. The experiments show that the proposed method outperforms the state-of-the-art methods on a variety of tasks."
1529,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning in sparse reward settings. The main idea is to use a policy improvement step and a policy guidance step to guide the policy to learn a sub-optimal behavior policy in the offline demonstration data. The proposed method, called LOGO, uses a censored version of the true state observation from the demonstration data to guide policy improvement and policy guidance steps. The experiments show that the proposed method outperforms the state-of-the-art methods on a variety of tasks."
1530,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"This paper proposes a method for online reinforcement learning in sparse reward settings. The main idea is to use a policy improvement step and a policy guidance step to guide the policy to learn a sub-optimal behavior policy in the offline demonstration data. The proposed method, called LOGO, uses a censored version of the true state observation from the demonstration data to guide policy improvement and policy guidance steps. The experiments show that the proposed method outperforms the state-of-the-art methods on a variety of tasks."
1531,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This paper proposes a hybrid neural Pareto front (HNPF) method for solving the problem of solving the linear scalarization problem. The main contribution of this paper is the introduction of a two-stage neural network to solve the problem. In the first stage, a Stage-1 neural network is used to compute the weak Paretto front and the Stage-2 network is trained to generate the strong front. The second stage is the FJC guided diffusive manifold, where the weak front is computed by a low-cost FJC filter and the stage-2 is trained by a high-cost filter. The authors show that the proposed method outperforms existing methods on several benchmark datasets."
1532,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This paper proposes a hybrid neural Pareto front (HNPF) method for solving the problem of solving the linear scalarization problem. The main contribution of this paper is the introduction of a two-stage neural network to solve the problem. In the first stage, a Stage-1 neural network is used to compute the weak Paretto front and the Stage-2 network is trained to generate the strong front. The second stage is the FJC guided diffusive manifold, where the weak front is computed by a low-cost FJC filter and the stage-2 is trained by a high-cost filter. The authors show that the proposed method outperforms existing methods on several benchmark datasets."
1533,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This paper proposes a hybrid neural Pareto front (HNPF) method for solving the problem of solving the linear scalarization problem. The main contribution of this paper is the introduction of a two-stage neural network to solve the problem. In the first stage, a Stage-1 neural network is used to compute the weak Paretto front and the Stage-2 network is trained to generate the strong front. The second stage is the FJC guided diffusive manifold, where the weak front is computed by a low-cost FJC filter and the stage-2 is trained by a high-cost filter. The authors show that the proposed method outperforms existing methods on several benchmark datasets."
1534,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This paper proposes a hybrid neural Pareto front (HNPF) method for solving the problem of solving the linear scalarization problem. The main contribution of this paper is the introduction of a two-stage neural network to solve the problem. In the first stage, a Stage-1 neural network is used to compute the weak Paretto front and the Stage-2 network is trained to generate the strong front. The second stage is the FJC guided diffusive manifold, where the weak front is computed by a low-cost FJC filter and the stage-2 is trained by a high-cost filter. The authors show that the proposed method outperforms existing methods on several benchmark datasets."
1535,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper proposes a multi-head, multi-task knowledge distillation method. The main idea is to train a generalist model for a given task and then distill the knowledge of the teacher to the student model. The student model is trained on the unlabeled proxy dataset and the generalist teacher on the task-specific teacher(s). Experiments show that the proposed method outperforms the state-of-the-art."
1536,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper proposes a multi-head, multi-task knowledge distillation method. The main idea is to train a generalist model for a given task and then distill the knowledge of the teacher to the student model. The student model is trained on the unlabeled proxy dataset and the generalist teacher on the task-specific teacher(s). Experiments show that the proposed method outperforms the state-of-the-art."
1537,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper proposes a multi-head, multi-task knowledge distillation method. The main idea is to train a generalist model for a given task and then distill the knowledge of the teacher to the student model. The student model is trained on the unlabeled proxy dataset and the generalist teacher on the task-specific teacher(s). Experiments show that the proposed method outperforms the state-of-the-art."
1538,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"This paper proposes a multi-head, multi-task knowledge distillation method. The main idea is to train a generalist model for a given task and then distill the knowledge of the teacher to the student model. The student model is trained on the unlabeled proxy dataset and the generalist teacher on the task-specific teacher(s). Experiments show that the proposed method outperforms the state-of-the-art."
1539,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a meta-algorithm for conformal prediction, which is a variant of conformal learning. The main idea is to add a regularization term to the conformal loss function to ensure that the coverage of the prediction set is not affected by the function class. The authors show that this regularisation term can be used to improve the efficiency of the proposed algorithm. The proposed algorithm is based on a gradient-based algorithm with differentiable surrogate losses and Lagrangians. The experiments show that the proposed method can achieve near-optimal efficiency and approximate valid coverage."
1540,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a meta-algorithm for conformal prediction, which is a variant of conformal learning. The main idea is to add a regularization term to the conformal loss function to ensure that the coverage of the prediction set is not affected by the function class. The authors show that this regularisation term can be used to improve the efficiency of the proposed algorithm. The proposed algorithm is based on a gradient-based algorithm with differentiable surrogate losses and Lagrangians. The experiments show that the proposed method can achieve near-optimal efficiency and approximate valid coverage."
1541,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a meta-algorithm for conformal prediction, which is a variant of conformal learning. The main idea is to add a regularization term to the conformal loss function to ensure that the coverage of the prediction set is not affected by the function class. The authors show that this regularisation term can be used to improve the efficiency of the proposed algorithm. The proposed algorithm is based on a gradient-based algorithm with differentiable surrogate losses and Lagrangians. The experiments show that the proposed method can achieve near-optimal efficiency and approximate valid coverage."
1542,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes a meta-algorithm for conformal prediction, which is a variant of conformal learning. The main idea is to add a regularization term to the conformal loss function to ensure that the coverage of the prediction set is not affected by the function class. The authors show that this regularisation term can be used to improve the efficiency of the proposed algorithm. The proposed algorithm is based on a gradient-based algorithm with differentiable surrogate losses and Lagrangians. The experiments show that the proposed method can achieve near-optimal efficiency and approximate valid coverage."
1543,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a new method for query object localization based on reinforcement learning. The proposed method is based on the notion of transferable reward, which is defined as the distance between the query object and an exemplary set of objects in the query set. The authors show that the proposed method can be used for fine-tuning and test-time policy adaptation. The method is evaluated on corrupted MNIST, corrupted CU-Birds, and COCO datasets."
1544,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a new method for query object localization based on reinforcement learning. The proposed method is based on the notion of transferable reward, which is defined as the distance between the query object and an exemplary set of objects in the query set. The authors show that the proposed method can be used for fine-tuning and test-time policy adaptation. The method is evaluated on corrupted MNIST, corrupted CU-Birds, and COCO datasets."
1545,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a new method for query object localization based on reinforcement learning. The proposed method is based on the notion of transferable reward, which is defined as the distance between the query object and an exemplary set of objects in the query set. The authors show that the proposed method can be used for fine-tuning and test-time policy adaptation. The method is evaluated on corrupted MNIST, corrupted CU-Birds, and COCO datasets."
1546,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper proposes a new method for query object localization based on reinforcement learning. The proposed method is based on the notion of transferable reward, which is defined as the distance between the query object and an exemplary set of objects in the query set. The authors show that the proposed method can be used for fine-tuning and test-time policy adaptation. The method is evaluated on corrupted MNIST, corrupted CU-Birds, and COCO datasets."
1547,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a quadratic computational complexity analysis for quadtree transformers. The main idea is to use token pyramids to reduce the computational complexity of quadtree attention. The paper shows that the proposed quadtree transformer can reduce the computation complexity by a factor of $O(\sqrt{T})$ and $O(1/\epsilon^2)$, which is a significant reduction compared to the original transformer. "
1548,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a quadratic computational complexity analysis for quadtree transformers. The main idea is to use token pyramids to reduce the computational complexity of quadtree attention. The paper shows that the proposed quadtree transformer can reduce the computation complexity by a factor of $O(\sqrt{T})$ and $O(1/\epsilon^2)$, which is a significant reduction compared to the original transformer. "
1549,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a quadratic computational complexity analysis for quadtree transformers. The main idea is to use token pyramids to reduce the computational complexity of quadtree attention. The paper shows that the proposed quadtree transformer can reduce the computation complexity by a factor of $O(\sqrt{T})$ and $O(1/\epsilon^2)$, which is a significant reduction compared to the original transformer. "
1550,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a quadratic computational complexity analysis for quadtree transformers. The main idea is to use token pyramids to reduce the computational complexity of quadtree attention. The paper shows that the proposed quadtree transformer can reduce the computation complexity by a factor of $O(\sqrt{T})$ and $O(1/\epsilon^2)$, which is a significant reduction compared to the original transformer. "
1551,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes InfoMax Termination Critic (IMTC), a method for learning reusable options for transfer learning. The method is based on mutual information (MI) based skill learning, where the goal is to maximize the mutual information between the learned options and the state transitions. The authors propose a scalable approximation of MI maximization using gradient ascent, and show that IMTC is able to adapt to complex domains. "
1552,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes InfoMax Termination Critic (IMTC), a method for learning reusable options for transfer learning. The method is based on mutual information (MI) based skill learning, where the goal is to maximize the mutual information between the learned options and the state transitions. The authors propose a scalable approximation of MI maximization using gradient ascent, and show that IMTC is able to adapt to complex domains. "
1553,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes InfoMax Termination Critic (IMTC), a method for learning reusable options for transfer learning. The method is based on mutual information (MI) based skill learning, where the goal is to maximize the mutual information between the learned options and the state transitions. The authors propose a scalable approximation of MI maximization using gradient ascent, and show that IMTC is able to adapt to complex domains. "
1554,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"This paper proposes InfoMax Termination Critic (IMTC), a method for learning reusable options for transfer learning. The method is based on mutual information (MI) based skill learning, where the goal is to maximize the mutual information between the learned options and the state transitions. The authors propose a scalable approximation of MI maximization using gradient ascent, and show that IMTC is able to adapt to complex domains. "
1555,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper studies the problem of open-world object detection with semantic topology. In particular, the authors propose to use a discriminative feature representation for each node in the semantic graph, which is then used to constrain the open-set error of the detector. The authors show that the proposed semantic graph can be used to improve the performance of the proposed detector."
1556,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper studies the problem of open-world object detection with semantic topology. In particular, the authors propose to use a discriminative feature representation for each node in the semantic graph, which is then used to constrain the open-set error of the detector. The authors show that the proposed semantic graph can be used to improve the performance of the proposed detector."
1557,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper studies the problem of open-world object detection with semantic topology. In particular, the authors propose to use a discriminative feature representation for each node in the semantic graph, which is then used to constrain the open-set error of the detector. The authors show that the proposed semantic graph can be used to improve the performance of the proposed detector."
1558,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"This paper studies the problem of open-world object detection with semantic topology. In particular, the authors propose to use a discriminative feature representation for each node in the semantic graph, which is then used to constrain the open-set error of the detector. The authors show that the proposed semantic graph can be used to improve the performance of the proposed detector."
1559,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a greedy algorithm for learning submodular objective functions for deep learning models. The main idea is to use matroids as the objective function, and to use the matroid as a regularizer to ensure that the objective is independent of the class labels and the decision boundaries. The authors show that the proposed algorithm can be applied to a wide range of classification tasks, including classification on CIFAR-10, Cifar-100, and long-tailed classification on ImageNet."
1560,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a greedy algorithm for learning submodular objective functions for deep learning models. The main idea is to use matroids as the objective function, and to use the matroid as a regularizer to ensure that the objective is independent of the class labels and the decision boundaries. The authors show that the proposed algorithm can be applied to a wide range of classification tasks, including classification on CIFAR-10, Cifar-100, and long-tailed classification on ImageNet."
1561,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a greedy algorithm for learning submodular objective functions for deep learning models. The main idea is to use matroids as the objective function, and to use the matroid as a regularizer to ensure that the objective is independent of the class labels and the decision boundaries. The authors show that the proposed algorithm can be applied to a wide range of classification tasks, including classification on CIFAR-10, Cifar-100, and long-tailed classification on ImageNet."
1562,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposes a greedy algorithm for learning submodular objective functions for deep learning models. The main idea is to use matroids as the objective function, and to use the matroid as a regularizer to ensure that the objective is independent of the class labels and the decision boundaries. The authors show that the proposed algorithm can be applied to a wide range of classification tasks, including classification on CIFAR-10, Cifar-100, and long-tailed classification on ImageNet."
1563,SP:e0432ff922708c6c6e59124d27c1386605930346,This paper proposes an adaptive inference strategy for semantic segmentation. The main idea is to use instance-adaptive Batch Normalization (IaBN) to normalize the feature statistics of the normalization layers. The authors also propose a self-supervised loss to train the model parameters. The experimental results show that the proposed method outperforms the baseline methods.
1564,SP:e0432ff922708c6c6e59124d27c1386605930346,This paper proposes an adaptive inference strategy for semantic segmentation. The main idea is to use instance-adaptive Batch Normalization (IaBN) to normalize the feature statistics of the normalization layers. The authors also propose a self-supervised loss to train the model parameters. The experimental results show that the proposed method outperforms the baseline methods.
1565,SP:e0432ff922708c6c6e59124d27c1386605930346,This paper proposes an adaptive inference strategy for semantic segmentation. The main idea is to use instance-adaptive Batch Normalization (IaBN) to normalize the feature statistics of the normalization layers. The authors also propose a self-supervised loss to train the model parameters. The experimental results show that the proposed method outperforms the baseline methods.
1566,SP:e0432ff922708c6c6e59124d27c1386605930346,This paper proposes an adaptive inference strategy for semantic segmentation. The main idea is to use instance-adaptive Batch Normalization (IaBN) to normalize the feature statistics of the normalization layers. The authors also propose a self-supervised loss to train the model parameters. The experimental results show that the proposed method outperforms the baseline methods.
1567,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a new contrastive loss for tabular data. The main idea is to use contrastive losses to learn a set of hyperparameters for each data point, which are then used to train a classifier to predict the label of the data point. The authors show that the proposed method can be applied to a variety of tabular datasets, and show that it outperforms existing methods. "
1568,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a new contrastive loss for tabular data. The main idea is to use contrastive losses to learn a set of hyperparameters for each data point, which are then used to train a classifier to predict the label of the data point. The authors show that the proposed method can be applied to a variety of tabular datasets, and show that it outperforms existing methods. "
1569,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a new contrastive loss for tabular data. The main idea is to use contrastive losses to learn a set of hyperparameters for each data point, which are then used to train a classifier to predict the label of the data point. The authors show that the proposed method can be applied to a variety of tabular datasets, and show that it outperforms existing methods. "
1570,SP:427100edad574722a6525ca917e84f817ff60d7e,"This paper proposes a new contrastive loss for tabular data. The main idea is to use contrastive losses to learn a set of hyperparameters for each data point, which are then used to train a classifier to predict the label of the data point. The authors show that the proposed method can be applied to a variety of tabular datasets, and show that it outperforms existing methods. "
1571,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a novel approach for neuropsychiatric classification based on a conditional variational auto-encoder (CVAE) model. The proposed approach is based on the idea of mining corresponded nosological relations between the input and the label of a given psychiatric diagnosis. The authors propose to use a conditional VAE model to learn a low-dimensional embedding space for the diagnostic information, which is then used to train an encoder and a decoder. The encoder is trained using a variational autoencoder, and the decoder is used to generate the label. The model is evaluated on a variety of datasets and compared to a number of baselines."
1572,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a novel approach for neuropsychiatric classification based on a conditional variational auto-encoder (CVAE) model. The proposed approach is based on the idea of mining corresponded nosological relations between the input and the label of a given psychiatric diagnosis. The authors propose to use a conditional VAE model to learn a low-dimensional embedding space for the diagnostic information, which is then used to train an encoder and a decoder. The encoder is trained using a variational autoencoder, and the decoder is used to generate the label. The model is evaluated on a variety of datasets and compared to a number of baselines."
1573,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a novel approach for neuropsychiatric classification based on a conditional variational auto-encoder (CVAE) model. The proposed approach is based on the idea of mining corresponded nosological relations between the input and the label of a given psychiatric diagnosis. The authors propose to use a conditional VAE model to learn a low-dimensional embedding space for the diagnostic information, which is then used to train an encoder and a decoder. The encoder is trained using a variational autoencoder, and the decoder is used to generate the label. The model is evaluated on a variety of datasets and compared to a number of baselines."
1574,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper proposes a novel approach for neuropsychiatric classification based on a conditional variational auto-encoder (CVAE) model. The proposed approach is based on the idea of mining corresponded nosological relations between the input and the label of a given psychiatric diagnosis. The authors propose to use a conditional VAE model to learn a low-dimensional embedding space for the diagnostic information, which is then used to train an encoder and a decoder. The encoder is trained using a variational autoencoder, and the decoder is used to generate the label. The model is evaluated on a variety of datasets and compared to a number of baselines."
1575,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a method for learning quantum embeddings using a variational quantum circuit (VQC) and a parametric tensor network (QTN). The main idea is to train a VQC on top of a QTN, and then use a tensor product encoder to encode the quantum embedding of the QTN. Theoretical analysis is provided to support the proposed method. Experiments on MNIST and CIFAR-10 demonstrate the effectiveness of the method."
1576,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a method for learning quantum embeddings using a variational quantum circuit (VQC) and a parametric tensor network (QTN). The main idea is to train a VQC on top of a QTN, and then use a tensor product encoder to encode the quantum embedding of the QTN. Theoretical analysis is provided to support the proposed method. Experiments on MNIST and CIFAR-10 demonstrate the effectiveness of the method."
1577,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a method for learning quantum embeddings using a variational quantum circuit (VQC) and a parametric tensor network (QTN). The main idea is to train a VQC on top of a QTN, and then use a tensor product encoder to encode the quantum embedding of the QTN. Theoretical analysis is provided to support the proposed method. Experiments on MNIST and CIFAR-10 demonstrate the effectiveness of the method."
1578,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"This paper proposes a method for learning quantum embeddings using a variational quantum circuit (VQC) and a parametric tensor network (QTN). The main idea is to train a VQC on top of a QTN, and then use a tensor product encoder to encode the quantum embedding of the QTN. Theoretical analysis is provided to support the proposed method. Experiments on MNIST and CIFAR-10 demonstrate the effectiveness of the method."
1579,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper studies the problem of learning a model embedding space for deep neural networks on low-dimensional manifolds. The authors propose a meta-learning algorithm called DYNAMO, which is based on the idea of meta-training a neural network on top of a pre-trained meta-model. The main idea of the algorithm is to train the meta-network on a set of low dimensional manifolds, and then train the model on these manifolds with the help of a model-embedding vector. The proposed algorithm can be applied to both RNNs and CNNs, and is shown to outperform the baselines on a variety of tasks, including semi-supervised learning and clustering."
1580,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper studies the problem of learning a model embedding space for deep neural networks on low-dimensional manifolds. The authors propose a meta-learning algorithm called DYNAMO, which is based on the idea of meta-training a neural network on top of a pre-trained meta-model. The main idea of the algorithm is to train the meta-network on a set of low dimensional manifolds, and then train the model on these manifolds with the help of a model-embedding vector. The proposed algorithm can be applied to both RNNs and CNNs, and is shown to outperform the baselines on a variety of tasks, including semi-supervised learning and clustering."
1581,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper studies the problem of learning a model embedding space for deep neural networks on low-dimensional manifolds. The authors propose a meta-learning algorithm called DYNAMO, which is based on the idea of meta-training a neural network on top of a pre-trained meta-model. The main idea of the algorithm is to train the meta-network on a set of low dimensional manifolds, and then train the model on these manifolds with the help of a model-embedding vector. The proposed algorithm can be applied to both RNNs and CNNs, and is shown to outperform the baselines on a variety of tasks, including semi-supervised learning and clustering."
1582,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper studies the problem of learning a model embedding space for deep neural networks on low-dimensional manifolds. The authors propose a meta-learning algorithm called DYNAMO, which is based on the idea of meta-training a neural network on top of a pre-trained meta-model. The main idea of the algorithm is to train the meta-network on a set of low dimensional manifolds, and then train the model on these manifolds with the help of a model-embedding vector. The proposed algorithm can be applied to both RNNs and CNNs, and is shown to outperform the baselines on a variety of tasks, including semi-supervised learning and clustering."
1583,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,This paper proposes a method for constraint-based learned simulation. The method is based on a graph neural network and a gradient descent algorithm. The authors show that the proposed method can achieve better simulation accuracy than the state-of-the-art methods.
1584,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,This paper proposes a method for constraint-based learned simulation. The method is based on a graph neural network and a gradient descent algorithm. The authors show that the proposed method can achieve better simulation accuracy than the state-of-the-art methods.
1585,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,This paper proposes a method for constraint-based learned simulation. The method is based on a graph neural network and a gradient descent algorithm. The authors show that the proposed method can achieve better simulation accuracy than the state-of-the-art methods.
1586,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,This paper proposes a method for constraint-based learned simulation. The method is based on a graph neural network and a gradient descent algorithm. The authors show that the proposed method can achieve better simulation accuracy than the state-of-the-art methods.
1587,SP:db07c2c0afdf27692dc504c9c54387c20211d469,This paper proposes a method for improving the diversity of policies in reinforcement learning (RL). The authors propose a method called Clustering-based Selection (EDO-CS) which is based on the idea of clustering based selection. The idea is to cluster the policies into clusters based on their similarity to each other. The authors show that the proposed method outperforms existing methods on a variety of continuous control tasks.
1588,SP:db07c2c0afdf27692dc504c9c54387c20211d469,This paper proposes a method for improving the diversity of policies in reinforcement learning (RL). The authors propose a method called Clustering-based Selection (EDO-CS) which is based on the idea of clustering based selection. The idea is to cluster the policies into clusters based on their similarity to each other. The authors show that the proposed method outperforms existing methods on a variety of continuous control tasks.
1589,SP:db07c2c0afdf27692dc504c9c54387c20211d469,This paper proposes a method for improving the diversity of policies in reinforcement learning (RL). The authors propose a method called Clustering-based Selection (EDO-CS) which is based on the idea of clustering based selection. The idea is to cluster the policies into clusters based on their similarity to each other. The authors show that the proposed method outperforms existing methods on a variety of continuous control tasks.
1590,SP:db07c2c0afdf27692dc504c9c54387c20211d469,This paper proposes a method for improving the diversity of policies in reinforcement learning (RL). The authors propose a method called Clustering-based Selection (EDO-CS) which is based on the idea of clustering based selection. The idea is to cluster the policies into clusters based on their similarity to each other. The authors show that the proposed method outperforms existing methods on a variety of continuous control tasks.
1591,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed stochastic approximation algorithm with Markovian noise and general consensus-type interaction. In particular, the authors consider a time-varying directed graph, where the interconnection matrix is a doubly-stochastic matrices and the interaction matrices are uniformly strongly connected graph sequences. The authors provide a finite-time bound on the mean-square error of the algorithm, and provide a convex combination of convex combinations of the two matrices. They also provide a theoretical analysis of the convergence rate of the proposed algorithm."
1592,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed stochastic approximation algorithm with Markovian noise and general consensus-type interaction. In particular, the authors consider a time-varying directed graph, where the interconnection matrix is a doubly-stochastic matrices and the interaction matrices are uniformly strongly connected graph sequences. The authors provide a finite-time bound on the mean-square error of the algorithm, and provide a convex combination of convex combinations of the two matrices. They also provide a theoretical analysis of the convergence rate of the proposed algorithm."
1593,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed stochastic approximation algorithm with Markovian noise and general consensus-type interaction. In particular, the authors consider a time-varying directed graph, where the interconnection matrix is a doubly-stochastic matrices and the interaction matrices are uniformly strongly connected graph sequences. The authors provide a finite-time bound on the mean-square error of the algorithm, and provide a convex combination of convex combinations of the two matrices. They also provide a theoretical analysis of the convergence rate of the proposed algorithm."
1594,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the convergence of a distributed stochastic approximation algorithm with Markovian noise and general consensus-type interaction. In particular, the authors consider a time-varying directed graph, where the interconnection matrix is a doubly-stochastic matrices and the interaction matrices are uniformly strongly connected graph sequences. The authors provide a finite-time bound on the mean-square error of the algorithm, and provide a convex combination of convex combinations of the two matrices. They also provide a theoretical analysis of the convergence rate of the proposed algorithm."
1595,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a Graph Mechanics Network (GNN) that is equivariant to geometrical constraints. The proposed method is based on the idea of orthogonality-equivariant functions. The authors show that the proposed method can be applied to a variety of problems, including molecular dynamics prediction and human motion capture. Experiments on simulated and real-world datasets show that GMN outperforms existing methods."
1596,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a Graph Mechanics Network (GNN) that is equivariant to geometrical constraints. The proposed method is based on the idea of orthogonality-equivariant functions. The authors show that the proposed method can be applied to a variety of problems, including molecular dynamics prediction and human motion capture. Experiments on simulated and real-world datasets show that GMN outperforms existing methods."
1597,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a Graph Mechanics Network (GNN) that is equivariant to geometrical constraints. The proposed method is based on the idea of orthogonality-equivariant functions. The authors show that the proposed method can be applied to a variety of problems, including molecular dynamics prediction and human motion capture. Experiments on simulated and real-world datasets show that GMN outperforms existing methods."
1598,SP:f7f96d545a907887396393aba310974f4d3f75ff,"This paper proposes a Graph Mechanics Network (GNN) that is equivariant to geometrical constraints. The proposed method is based on the idea of orthogonality-equivariant functions. The authors show that the proposed method can be applied to a variety of problems, including molecular dynamics prediction and human motion capture. Experiments on simulated and real-world datasets show that GMN outperforms existing methods."
1599,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper studies the problem of partial model personalization in federated learning (FL). The authors propose a new algorithm for FL, which is based on alternating updates of the shared and personal parameters. The authors show that the proposed algorithm outperforms the existing algorithms in terms of performance and memory footprint."
1600,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper studies the problem of partial model personalization in federated learning (FL). The authors propose a new algorithm for FL, which is based on alternating updates of the shared and personal parameters. The authors show that the proposed algorithm outperforms the existing algorithms in terms of performance and memory footprint."
1601,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper studies the problem of partial model personalization in federated learning (FL). The authors propose a new algorithm for FL, which is based on alternating updates of the shared and personal parameters. The authors show that the proposed algorithm outperforms the existing algorithms in terms of performance and memory footprint."
1602,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper studies the problem of partial model personalization in federated learning (FL). The authors propose a new algorithm for FL, which is based on alternating updates of the shared and personal parameters. The authors show that the proposed algorithm outperforms the existing algorithms in terms of performance and memory footprint."
1603,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations from videos. The method is based on contrastive learning, which aims to learn representations that are similar to each other. The proposed method is evaluated on a dataset of videos from the ThreeDWorld environment. The results show that the proposed method outperforms other contrastive methods."
1604,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations from videos. The method is based on contrastive learning, which aims to learn representations that are similar to each other. The proposed method is evaluated on a dataset of videos from the ThreeDWorld environment. The results show that the proposed method outperforms other contrastive methods."
1605,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations from videos. The method is based on contrastive learning, which aims to learn representations that are similar to each other. The proposed method is evaluated on a dataset of videos from the ThreeDWorld environment. The results show that the proposed method outperforms other contrastive methods."
1606,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"This paper proposes a method for unsupervised learning of object representations from videos. The method is based on contrastive learning, which aims to learn representations that are similar to each other. The proposed method is evaluated on a dataset of videos from the ThreeDWorld environment. The results show that the proposed method outperforms other contrastive methods."
1607,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes a new algorithm for goal-directed planning based on Monte Carlo Tree Search (MCTS). The main idea is to use a function approximator to approximate the function of the MCTS tree, and then use the function approximation to compute the reward function. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on a variety of tasks. The paper also shows that the algorithm can be applied to quantum computing tasks."
1608,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes a new algorithm for goal-directed planning based on Monte Carlo Tree Search (MCTS). The main idea is to use a function approximator to approximate the function of the MCTS tree, and then use the function approximation to compute the reward function. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on a variety of tasks. The paper also shows that the algorithm can be applied to quantum computing tasks."
1609,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes a new algorithm for goal-directed planning based on Monte Carlo Tree Search (MCTS). The main idea is to use a function approximator to approximate the function of the MCTS tree, and then use the function approximation to compute the reward function. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on a variety of tasks. The paper also shows that the algorithm can be applied to quantum computing tasks."
1610,SP:2fb4af247b5022710b681037faca2420207a507a,"This paper proposes a new algorithm for goal-directed planning based on Monte Carlo Tree Search (MCTS). The main idea is to use a function approximator to approximate the function of the MCTS tree, and then use the function approximation to compute the reward function. The authors show that the proposed algorithm outperforms the state-of-the-art algorithms on a variety of tasks. The paper also shows that the algorithm can be applied to quantum computing tasks."
1611,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes Recursive Gradient Optimization (RGO), a method for continual learning (CL). The main idea of RGO is to iteratively update the gradient of the optimizer and the feature encoding layer of the neural network. The proposed method is evaluated on the 20-split-CIFAR-100 and mini-ImageNet datasets."
1612,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes Recursive Gradient Optimization (RGO), a method for continual learning (CL). The main idea of RGO is to iteratively update the gradient of the optimizer and the feature encoding layer of the neural network. The proposed method is evaluated on the 20-split-CIFAR-100 and mini-ImageNet datasets."
1613,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes Recursive Gradient Optimization (RGO), a method for continual learning (CL). The main idea of RGO is to iteratively update the gradient of the optimizer and the feature encoding layer of the neural network. The proposed method is evaluated on the 20-split-CIFAR-100 and mini-ImageNet datasets."
1614,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper proposes Recursive Gradient Optimization (RGO), a method for continual learning (CL). The main idea of RGO is to iteratively update the gradient of the optimizer and the feature encoding layer of the neural network. The proposed method is evaluated on the 20-split-CIFAR-100 and mini-ImageNet datasets."
1615,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,This paper studies the problem of model alignment for image-to-image translation and zero-shot image morphing. The authors propose a method to align the latent space of the child model and the parent model in the same latent space. The proposed method is based on the idea of inversion and fine-tuning. The experimental results show that the proposed method outperforms the state-of-the-art methods.
1616,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,This paper studies the problem of model alignment for image-to-image translation and zero-shot image morphing. The authors propose a method to align the latent space of the child model and the parent model in the same latent space. The proposed method is based on the idea of inversion and fine-tuning. The experimental results show that the proposed method outperforms the state-of-the-art methods.
1617,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,This paper studies the problem of model alignment for image-to-image translation and zero-shot image morphing. The authors propose a method to align the latent space of the child model and the parent model in the same latent space. The proposed method is based on the idea of inversion and fine-tuning. The experimental results show that the proposed method outperforms the state-of-the-art methods.
1618,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,This paper studies the problem of model alignment for image-to-image translation and zero-shot image morphing. The authors propose a method to align the latent space of the child model and the parent model in the same latent space. The proposed method is based on the idea of inversion and fine-tuning. The experimental results show that the proposed method outperforms the state-of-the-art methods.
1619,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper studies the Gromov-Wasserstein (GW) distance between two graphs. The main contribution of this paper is to propose a semi-relaxed version of the GW distance, which can be used for graph dictionary learning, partitioning, clustering, and completion tasks. The proposed GW distance is based on the Optimal Transport (OT) metric, and the authors show that it can be computed efficiently. The experimental results show that the proposed method outperforms the state-of-the-art methods."
1620,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper studies the Gromov-Wasserstein (GW) distance between two graphs. The main contribution of this paper is to propose a semi-relaxed version of the GW distance, which can be used for graph dictionary learning, partitioning, clustering, and completion tasks. The proposed GW distance is based on the Optimal Transport (OT) metric, and the authors show that it can be computed efficiently. The experimental results show that the proposed method outperforms the state-of-the-art methods."
1621,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper studies the Gromov-Wasserstein (GW) distance between two graphs. The main contribution of this paper is to propose a semi-relaxed version of the GW distance, which can be used for graph dictionary learning, partitioning, clustering, and completion tasks. The proposed GW distance is based on the Optimal Transport (OT) metric, and the authors show that it can be computed efficiently. The experimental results show that the proposed method outperforms the state-of-the-art methods."
1622,SP:0e13f831c211626195c118487f2fff36a6e293f6,"This paper studies the Gromov-Wasserstein (GW) distance between two graphs. The main contribution of this paper is to propose a semi-relaxed version of the GW distance, which can be used for graph dictionary learning, partitioning, clustering, and completion tasks. The proposed GW distance is based on the Optimal Transport (OT) metric, and the authors show that it can be computed efficiently. The experimental results show that the proposed method outperforms the state-of-the-art methods."
1623,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper studies the problem of imitation learning in the context of Boltzmann rationality, where the goal is to learn a policy that maximizes the Boltzman rationality of the reward function. The authors propose to use a Bayesian inference approach to estimate the policy distribution (BPD) of the rewards and trajectories of the agent, and then use a generative and sequence model to sample from the BPD. They show that this approach can be applied to both imitation learning and imitation learning-based human models. They also show that the proposed method can be used to predict human behavior and human-AI collaboration."
1624,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper studies the problem of imitation learning in the context of Boltzmann rationality, where the goal is to learn a policy that maximizes the Boltzman rationality of the reward function. The authors propose to use a Bayesian inference approach to estimate the policy distribution (BPD) of the rewards and trajectories of the agent, and then use a generative and sequence model to sample from the BPD. They show that this approach can be applied to both imitation learning and imitation learning-based human models. They also show that the proposed method can be used to predict human behavior and human-AI collaboration."
1625,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper studies the problem of imitation learning in the context of Boltzmann rationality, where the goal is to learn a policy that maximizes the Boltzman rationality of the reward function. The authors propose to use a Bayesian inference approach to estimate the policy distribution (BPD) of the rewards and trajectories of the agent, and then use a generative and sequence model to sample from the BPD. They show that this approach can be applied to both imitation learning and imitation learning-based human models. They also show that the proposed method can be used to predict human behavior and human-AI collaboration."
1626,SP:d6d144be11230070ae9395db70b7c7743540bad4,"This paper studies the problem of imitation learning in the context of Boltzmann rationality, where the goal is to learn a policy that maximizes the Boltzman rationality of the reward function. The authors propose to use a Bayesian inference approach to estimate the policy distribution (BPD) of the rewards and trajectories of the agent, and then use a generative and sequence model to sample from the BPD. They show that this approach can be applied to both imitation learning and imitation learning-based human models. They also show that the proposed method can be used to predict human behavior and human-AI collaboration."
1627,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free quantization method for deep neural networks (DNNs). The main idea is to use a Hessian-based optimization objective for quantization of DNNs, where the Hessian is a diagonal sub-item of the weight tensor of the network. The authors propose a novel algorithm to solve this objective, which is based on back-propagation. The main contribution of this paper is to propose a new algorithm, SQuant, which can be applied to the discrete domain. The algorithm is evaluated on a variety of datasets, and compared to the state-of-the-art methods."
1628,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free quantization method for deep neural networks (DNNs). The main idea is to use a Hessian-based optimization objective for quantization of DNNs, where the Hessian is a diagonal sub-item of the weight tensor of the network. The authors propose a novel algorithm to solve this objective, which is based on back-propagation. The main contribution of this paper is to propose a new algorithm, SQuant, which can be applied to the discrete domain. The algorithm is evaluated on a variety of datasets, and compared to the state-of-the-art methods."
1629,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free quantization method for deep neural networks (DNNs). The main idea is to use a Hessian-based optimization objective for quantization of DNNs, where the Hessian is a diagonal sub-item of the weight tensor of the network. The authors propose a novel algorithm to solve this objective, which is based on back-propagation. The main contribution of this paper is to propose a new algorithm, SQuant, which can be applied to the discrete domain. The algorithm is evaluated on a variety of datasets, and compared to the state-of-the-art methods."
1630,SP:401ef5fe2022e926b0321258efac1f369f186ace,"This paper proposes a data-free quantization method for deep neural networks (DNNs). The main idea is to use a Hessian-based optimization objective for quantization of DNNs, where the Hessian is a diagonal sub-item of the weight tensor of the network. The authors propose a novel algorithm to solve this objective, which is based on back-propagation. The main contribution of this paper is to propose a new algorithm, SQuant, which can be applied to the discrete domain. The algorithm is evaluated on a variety of datasets, and compared to the state-of-the-art methods."
1631,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes SegTime, a method for time series segmentation. The proposed method is based on a bi-pass architecture, where the first pass is used to segment the time series into sub-sequences, and the second pass uses a sliding window to classify the sub-segments. The authors show that SegTime is able to achieve state-of-the-art performance on time series classification tasks."
1632,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes SegTime, a method for time series segmentation. The proposed method is based on a bi-pass architecture, where the first pass is used to segment the time series into sub-sequences, and the second pass uses a sliding window to classify the sub-segments. The authors show that SegTime is able to achieve state-of-the-art performance on time series classification tasks."
1633,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes SegTime, a method for time series segmentation. The proposed method is based on a bi-pass architecture, where the first pass is used to segment the time series into sub-sequences, and the second pass uses a sliding window to classify the sub-segments. The authors show that SegTime is able to achieve state-of-the-art performance on time series classification tasks."
1634,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper proposes SegTime, a method for time series segmentation. The proposed method is based on a bi-pass architecture, where the first pass is used to segment the time series into sub-sequences, and the second pass uses a sliding window to classify the sub-segments. The authors show that SegTime is able to achieve state-of-the-art performance on time series classification tasks."
1635,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,This paper proposes a method for improving the faithfulness of graph neural networks (GNNs). The main idea is to use a subgraph level interpretation algorithm to learn a faithful explanation of GNN predictions. The method is based on the idea that GNNs should be able to explain complex interactions between graph nodes. The main contribution of this paper is to propose a method to learn the faithful explanation for GNN. The proposed method is evaluated on both synthetic and real-world datasets.
1636,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,This paper proposes a method for improving the faithfulness of graph neural networks (GNNs). The main idea is to use a subgraph level interpretation algorithm to learn a faithful explanation of GNN predictions. The method is based on the idea that GNNs should be able to explain complex interactions between graph nodes. The main contribution of this paper is to propose a method to learn the faithful explanation for GNN. The proposed method is evaluated on both synthetic and real-world datasets.
1637,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,This paper proposes a method for improving the faithfulness of graph neural networks (GNNs). The main idea is to use a subgraph level interpretation algorithm to learn a faithful explanation of GNN predictions. The method is based on the idea that GNNs should be able to explain complex interactions between graph nodes. The main contribution of this paper is to propose a method to learn the faithful explanation for GNN. The proposed method is evaluated on both synthetic and real-world datasets.
1638,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,This paper proposes a method for improving the faithfulness of graph neural networks (GNNs). The main idea is to use a subgraph level interpretation algorithm to learn a faithful explanation of GNN predictions. The method is based on the idea that GNNs should be able to explain complex interactions between graph nodes. The main contribution of this paper is to propose a method to learn the faithful explanation for GNN. The proposed method is evaluated on both synthetic and real-world datasets.
1639,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer for convolutional neural networks. The proposed method, DiffStride, is based on the idea of learnable strides in the downsampled layer. The authors show that the proposed method can be used to reduce the computational cost and improve the accuracy of ResNet-18 and CIFAR-10 on ImageNet. They also provide a theoretical analysis of their method."
1640,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer for convolutional neural networks. The proposed method, DiffStride, is based on the idea of learnable strides in the downsampled layer. The authors show that the proposed method can be used to reduce the computational cost and improve the accuracy of ResNet-18 and CIFAR-10 on ImageNet. They also provide a theoretical analysis of their method."
1641,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer for convolutional neural networks. The proposed method, DiffStride, is based on the idea of learnable strides in the downsampled layer. The authors show that the proposed method can be used to reduce the computational cost and improve the accuracy of ResNet-18 and CIFAR-10 on ImageNet. They also provide a theoretical analysis of their method."
1642,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes a new downsampling layer for convolutional neural networks. The proposed method, DiffStride, is based on the idea of learnable strides in the downsampled layer. The authors show that the proposed method can be used to reduce the computational cost and improve the accuracy of ResNet-18 and CIFAR-10 on ImageNet. They also provide a theoretical analysis of their method."
1643,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a novel method for improving the robustness of locally smoothed models to random perturbations. The proposed method is based on the idea of local randomized smoothing (LRSM), which is an extension of the recently proposed Local Randomized Smoothing (LRS) method. The main difference between LRSM and LRSSM is that LRSM is a local smoothing method, whereas LRSM uses a global smoothing technique. The authors propose a new certificate for the local smoothed model, which is a weighted average of the local and global smoothed versions of the smoothed version of LRSM. The paper also provides theoretical guarantees for the proposed method. Experiments are conducted on image segmentation and node classification tasks."
1644,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a novel method for improving the robustness of locally smoothed models to random perturbations. The proposed method is based on the idea of local randomized smoothing (LRSM), which is an extension of the recently proposed Local Randomized Smoothing (LRS) method. The main difference between LRSM and LRSSM is that LRSM is a local smoothing method, whereas LRSM uses a global smoothing technique. The authors propose a new certificate for the local smoothed model, which is a weighted average of the local and global smoothed versions of the smoothed version of LRSM. The paper also provides theoretical guarantees for the proposed method. Experiments are conducted on image segmentation and node classification tasks."
1645,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a novel method for improving the robustness of locally smoothed models to random perturbations. The proposed method is based on the idea of local randomized smoothing (LRSM), which is an extension of the recently proposed Local Randomized Smoothing (LRS) method. The main difference between LRSM and LRSSM is that LRSM is a local smoothing method, whereas LRSM uses a global smoothing technique. The authors propose a new certificate for the local smoothed model, which is a weighted average of the local and global smoothed versions of the smoothed version of LRSM. The paper also provides theoretical guarantees for the proposed method. Experiments are conducted on image segmentation and node classification tasks."
1646,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,"This paper proposes a novel method for improving the robustness of locally smoothed models to random perturbations. The proposed method is based on the idea of local randomized smoothing (LRSM), which is an extension of the recently proposed Local Randomized Smoothing (LRS) method. The main difference between LRSM and LRSSM is that LRSM is a local smoothing method, whereas LRSM uses a global smoothing technique. The authors propose a new certificate for the local smoothed model, which is a weighted average of the local and global smoothed versions of the smoothed version of LRSM. The paper also provides theoretical guarantees for the proposed method. Experiments are conducted on image segmentation and node classification tasks."
1647,SP:aacc31e83886c4c997412a1e51090202075eda86,This paper proposes an embedding-model flow (EMF) method for variational inference. EMFs are a bijective transformation of the embedding of a probabilistic model that can be applied to a variety of differentiable models. The authors show that EMFs can be used to approximate the structure of the prior model in a variational architecture. They show that the EMF can capture the multimodality and hierarchical coupling properties of the model. They also show that their EMF is compatible with gated structured layers.
1648,SP:aacc31e83886c4c997412a1e51090202075eda86,This paper proposes an embedding-model flow (EMF) method for variational inference. EMFs are a bijective transformation of the embedding of a probabilistic model that can be applied to a variety of differentiable models. The authors show that EMFs can be used to approximate the structure of the prior model in a variational architecture. They show that the EMF can capture the multimodality and hierarchical coupling properties of the model. They also show that their EMF is compatible with gated structured layers.
1649,SP:aacc31e83886c4c997412a1e51090202075eda86,This paper proposes an embedding-model flow (EMF) method for variational inference. EMFs are a bijective transformation of the embedding of a probabilistic model that can be applied to a variety of differentiable models. The authors show that EMFs can be used to approximate the structure of the prior model in a variational architecture. They show that the EMF can capture the multimodality and hierarchical coupling properties of the model. They also show that their EMF is compatible with gated structured layers.
1650,SP:aacc31e83886c4c997412a1e51090202075eda86,This paper proposes an embedding-model flow (EMF) method for variational inference. EMFs are a bijective transformation of the embedding of a probabilistic model that can be applied to a variety of differentiable models. The authors show that EMFs can be used to approximate the structure of the prior model in a variational architecture. They show that the EMF can capture the multimodality and hierarchical coupling properties of the model. They also show that their EMF is compatible with gated structured layers.
1651,SP:825a254c0725008143b260ead840ae35f9f096d1,This paper studies the problem of learning a generative language model that can be used to represent concepts and entities in a grid world. The authors propose to use GPT-2 and GPT3 as the base models for this task. The main contribution of the paper is to show that GPT2 and 3 can be trained to generate concepts that are similar to the concepts learned by GPT. The paper also shows that the learned concepts are more expressive than the original text. 
1652,SP:825a254c0725008143b260ead840ae35f9f096d1,This paper studies the problem of learning a generative language model that can be used to represent concepts and entities in a grid world. The authors propose to use GPT-2 and GPT3 as the base models for this task. The main contribution of the paper is to show that GPT2 and 3 can be trained to generate concepts that are similar to the concepts learned by GPT. The paper also shows that the learned concepts are more expressive than the original text. 
1653,SP:825a254c0725008143b260ead840ae35f9f096d1,This paper studies the problem of learning a generative language model that can be used to represent concepts and entities in a grid world. The authors propose to use GPT-2 and GPT3 as the base models for this task. The main contribution of the paper is to show that GPT2 and 3 can be trained to generate concepts that are similar to the concepts learned by GPT. The paper also shows that the learned concepts are more expressive than the original text. 
1654,SP:825a254c0725008143b260ead840ae35f9f096d1,This paper studies the problem of learning a generative language model that can be used to represent concepts and entities in a grid world. The authors propose to use GPT-2 and GPT3 as the base models for this task. The main contribution of the paper is to show that GPT2 and 3 can be trained to generate concepts that are similar to the concepts learned by GPT. The paper also shows that the learned concepts are more expressive than the original text. 
1655,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies emergent language experimentation in a sender-receiver navigation game, where the goal is to maximize the entropy of the communication between the sender and the receiver. The authors propose a novel reward function that encourages the sender to communicate with the receiver in a way that maximizes the entropy. The paper also proposes a new reward function to encourage the receiver to communicate more effectively with the sender. Experiments show that the proposed reward function outperforms the baselines."
1656,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies emergent language experimentation in a sender-receiver navigation game, where the goal is to maximize the entropy of the communication between the sender and the receiver. The authors propose a novel reward function that encourages the sender to communicate with the receiver in a way that maximizes the entropy. The paper also proposes a new reward function to encourage the receiver to communicate more effectively with the sender. Experiments show that the proposed reward function outperforms the baselines."
1657,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies emergent language experimentation in a sender-receiver navigation game, where the goal is to maximize the entropy of the communication between the sender and the receiver. The authors propose a novel reward function that encourages the sender to communicate with the receiver in a way that maximizes the entropy. The paper also proposes a new reward function to encourage the receiver to communicate more effectively with the sender. Experiments show that the proposed reward function outperforms the baselines."
1658,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper studies emergent language experimentation in a sender-receiver navigation game, where the goal is to maximize the entropy of the communication between the sender and the receiver. The authors propose a novel reward function that encourages the sender to communicate with the receiver in a way that maximizes the entropy. The paper also proposes a new reward function to encourage the receiver to communicate more effectively with the sender. Experiments show that the proposed reward function outperforms the baselines."
1659,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,This paper proposes an unsupervised cross-lingual learning method for multilingual language models. The proposed method is based on importance-weighted domain alignment (IWDA) and prior shift estimation. The authors show that the proposed method can be used to improve the performance of multilingual models in the presence of distributional shift in class priors and prior shifts. The method is evaluated on a variety of datasets.
1660,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,This paper proposes an unsupervised cross-lingual learning method for multilingual language models. The proposed method is based on importance-weighted domain alignment (IWDA) and prior shift estimation. The authors show that the proposed method can be used to improve the performance of multilingual models in the presence of distributional shift in class priors and prior shifts. The method is evaluated on a variety of datasets.
1661,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,This paper proposes an unsupervised cross-lingual learning method for multilingual language models. The proposed method is based on importance-weighted domain alignment (IWDA) and prior shift estimation. The authors show that the proposed method can be used to improve the performance of multilingual models in the presence of distributional shift in class priors and prior shifts. The method is evaluated on a variety of datasets.
1662,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,This paper proposes an unsupervised cross-lingual learning method for multilingual language models. The proposed method is based on importance-weighted domain alignment (IWDA) and prior shift estimation. The authors show that the proposed method can be used to improve the performance of multilingual models in the presence of distributional shift in class priors and prior shifts. The method is evaluated on a variety of datasets.
1663,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper studies the problem of meta-learning in the context of bootstrapping. The authors propose a new metric for meta-optimization, which is based on the gradients of the meta-learner. The proposed metric is a pseudo-metric, which can be used to evaluate the performance of a model-free meta-learner. They show that the proposed metric can be applied to the case of a greedy Q-learning agent, and show that it can lead to better performance on the Atari ALE benchmark. "
1664,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper studies the problem of meta-learning in the context of bootstrapping. The authors propose a new metric for meta-optimization, which is based on the gradients of the meta-learner. The proposed metric is a pseudo-metric, which can be used to evaluate the performance of a model-free meta-learner. They show that the proposed metric can be applied to the case of a greedy Q-learning agent, and show that it can lead to better performance on the Atari ALE benchmark. "
1665,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper studies the problem of meta-learning in the context of bootstrapping. The authors propose a new metric for meta-optimization, which is based on the gradients of the meta-learner. The proposed metric is a pseudo-metric, which can be used to evaluate the performance of a model-free meta-learner. They show that the proposed metric can be applied to the case of a greedy Q-learning agent, and show that it can lead to better performance on the Atari ALE benchmark. "
1666,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"This paper studies the problem of meta-learning in the context of bootstrapping. The authors propose a new metric for meta-optimization, which is based on the gradients of the meta-learner. The proposed metric is a pseudo-metric, which can be used to evaluate the performance of a model-free meta-learner. They show that the proposed metric can be applied to the case of a greedy Q-learning agent, and show that it can lead to better performance on the Atari ALE benchmark. "
1667,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of MuZero, a model-based reinforcement learning agent, in the context of multi-task, procedural and task generalization. The main contribution of this paper is the introduction of Meta-World, which is a meta-world that aims to study the effect of different factors on the performance of the MuZero agent. The authors show that MuZero is able to generalize better than other model-free agents in terms of both procedural generalization as well as task generalizability. They also show that the proposed method Procgen outperforms the state-of-the-art methods on a variety of tasks."
1668,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of MuZero, a model-based reinforcement learning agent, in the context of multi-task, procedural and task generalization. The main contribution of this paper is the introduction of Meta-World, which is a meta-world that aims to study the effect of different factors on the performance of the MuZero agent. The authors show that MuZero is able to generalize better than other model-free agents in terms of both procedural generalization as well as task generalizability. They also show that the proposed method Procgen outperforms the state-of-the-art methods on a variety of tasks."
1669,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of MuZero, a model-based reinforcement learning agent, in the context of multi-task, procedural and task generalization. The main contribution of this paper is the introduction of Meta-World, which is a meta-world that aims to study the effect of different factors on the performance of the MuZero agent. The authors show that MuZero is able to generalize better than other model-free agents in terms of both procedural generalization as well as task generalizability. They also show that the proposed method Procgen outperforms the state-of-the-art methods on a variety of tasks."
1670,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"This paper studies the generalization ability of MuZero, a model-based reinforcement learning agent, in the context of multi-task, procedural and task generalization. The main contribution of this paper is the introduction of Meta-World, which is a meta-world that aims to study the effect of different factors on the performance of the MuZero agent. The authors show that MuZero is able to generalize better than other model-free agents in terms of both procedural generalization as well as task generalizability. They also show that the proposed method Procgen outperforms the state-of-the-art methods on a variety of tasks."
1671,SP:ba80e35d452d894181d51624183b60541c0f3704,This paper proposes a graph deconvolution network (GDN) for graph learning. The proposed method is based on eigendecomposition-based spectral methods and iterative optimization solutions. The authors show that the proposed method outperforms existing methods on link prediction and edge-weight regression tasks. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the effectiveness of the proposed model.
1672,SP:ba80e35d452d894181d51624183b60541c0f3704,This paper proposes a graph deconvolution network (GDN) for graph learning. The proposed method is based on eigendecomposition-based spectral methods and iterative optimization solutions. The authors show that the proposed method outperforms existing methods on link prediction and edge-weight regression tasks. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the effectiveness of the proposed model.
1673,SP:ba80e35d452d894181d51624183b60541c0f3704,This paper proposes a graph deconvolution network (GDN) for graph learning. The proposed method is based on eigendecomposition-based spectral methods and iterative optimization solutions. The authors show that the proposed method outperforms existing methods on link prediction and edge-weight regression tasks. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the effectiveness of the proposed model.
1674,SP:ba80e35d452d894181d51624183b60541c0f3704,This paper proposes a graph deconvolution network (GDN) for graph learning. The proposed method is based on eigendecomposition-based spectral methods and iterative optimization solutions. The authors show that the proposed method outperforms existing methods on link prediction and edge-weight regression tasks. The experimental results on the Human Connectome Project-Young Adult neuroimaging dataset demonstrate the effectiveness of the proposed model.
1675,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes a method for reward shaping in reinforcement learning. The method is based on a Markov game, where the goal is to learn a policy that maximizes a shaping-reward function. The main idea is to train an agent (controller) that learns a policy to maximize the shaping reward function, and then use the learned policy to guide the shaping agent (shaper) to the optimal policy. The authors show that the proposed method outperforms several baselines on a variety of tasks."
1676,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes a method for reward shaping in reinforcement learning. The method is based on a Markov game, where the goal is to learn a policy that maximizes a shaping-reward function. The main idea is to train an agent (controller) that learns a policy to maximize the shaping reward function, and then use the learned policy to guide the shaping agent (shaper) to the optimal policy. The authors show that the proposed method outperforms several baselines on a variety of tasks."
1677,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes a method for reward shaping in reinforcement learning. The method is based on a Markov game, where the goal is to learn a policy that maximizes a shaping-reward function. The main idea is to train an agent (controller) that learns a policy to maximize the shaping reward function, and then use the learned policy to guide the shaping agent (shaper) to the optimal policy. The authors show that the proposed method outperforms several baselines on a variety of tasks."
1678,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"This paper proposes a method for reward shaping in reinforcement learning. The method is based on a Markov game, where the goal is to learn a policy that maximizes a shaping-reward function. The main idea is to train an agent (controller) that learns a policy to maximize the shaping reward function, and then use the learned policy to guide the shaping agent (shaper) to the optimal policy. The authors show that the proposed method outperforms several baselines on a variety of tasks."
1679,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper studies the problem of robustness against backdoor attacks in vertical federated learning (VFL). The authors propose a new training and inference framework for VFL called Robust Rotation-based Reinforcement Learning (RVFR). The main idea of the proposed method is to train a global model and then train a local model on top of the global model. The local model is then used to train the local model, and the local network is used to update the global network. The authors show that their method is more robust to adversarial and missing feature attacks than previous methods."
1680,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper studies the problem of robustness against backdoor attacks in vertical federated learning (VFL). The authors propose a new training and inference framework for VFL called Robust Rotation-based Reinforcement Learning (RVFR). The main idea of the proposed method is to train a global model and then train a local model on top of the global model. The local model is then used to train the local model, and the local network is used to update the global network. The authors show that their method is more robust to adversarial and missing feature attacks than previous methods."
1681,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper studies the problem of robustness against backdoor attacks in vertical federated learning (VFL). The authors propose a new training and inference framework for VFL called Robust Rotation-based Reinforcement Learning (RVFR). The main idea of the proposed method is to train a global model and then train a local model on top of the global model. The local model is then used to train the local model, and the local network is used to update the global network. The authors show that their method is more robust to adversarial and missing feature attacks than previous methods."
1682,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"This paper studies the problem of robustness against backdoor attacks in vertical federated learning (VFL). The authors propose a new training and inference framework for VFL called Robust Rotation-based Reinforcement Learning (RVFR). The main idea of the proposed method is to train a global model and then train a local model on top of the global model. The local model is then used to train the local model, and the local network is used to update the global network. The authors show that their method is more robust to adversarial and missing feature attacks than previous methods."
1683,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"This paper proposes a method for unsupervised dense retrieval. The authors propose to use contrastive learning to improve the performance of the existing dense retrieval methods. Specifically, the authors propose a contrastive loss to encourage the model to be more dense. The proposed method is evaluated on the BEIR benchmark and the MS MARCO dataset and compared to the existing methods. The results show that the proposed method outperforms the existing models."
1684,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"This paper proposes a method for unsupervised dense retrieval. The authors propose to use contrastive learning to improve the performance of the existing dense retrieval methods. Specifically, the authors propose a contrastive loss to encourage the model to be more dense. The proposed method is evaluated on the BEIR benchmark and the MS MARCO dataset and compared to the existing methods. The results show that the proposed method outperforms the existing models."
1685,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"This paper proposes a method for unsupervised dense retrieval. The authors propose to use contrastive learning to improve the performance of the existing dense retrieval methods. Specifically, the authors propose a contrastive loss to encourage the model to be more dense. The proposed method is evaluated on the BEIR benchmark and the MS MARCO dataset and compared to the existing methods. The results show that the proposed method outperforms the existing models."
1686,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"This paper proposes a method for unsupervised dense retrieval. The authors propose to use contrastive learning to improve the performance of the existing dense retrieval methods. Specifically, the authors propose a contrastive loss to encourage the model to be more dense. The proposed method is evaluated on the BEIR benchmark and the MS MARCO dataset and compared to the existing methods. The results show that the proposed method outperforms the existing models."
1687,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the effect of fine-tuning on accuracy in-distribution (ID) and out-of-domain (OOD) accuracy. The authors compare the performance of linear probing and fine tuning on CIFAR-10, DomainNet, and Breeds-Entity-30 on distribution shift datasets. They show that linear probing performs better than fine tuning and OOD accuracy is better than linear probing. They also show that fine tuning with linear probing can improve accuracy on Cifar-10 and DomainNet."
1688,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the effect of fine-tuning on accuracy in-distribution (ID) and out-of-domain (OOD) accuracy. The authors compare the performance of linear probing and fine tuning on CIFAR-10, DomainNet, and Breeds-Entity-30 on distribution shift datasets. They show that linear probing performs better than fine tuning and OOD accuracy is better than linear probing. They also show that fine tuning with linear probing can improve accuracy on Cifar-10 and DomainNet."
1689,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the effect of fine-tuning on accuracy in-distribution (ID) and out-of-domain (OOD) accuracy. The authors compare the performance of linear probing and fine tuning on CIFAR-10, DomainNet, and Breeds-Entity-30 on distribution shift datasets. They show that linear probing performs better than fine tuning and OOD accuracy is better than linear probing. They also show that fine tuning with linear probing can improve accuracy on Cifar-10 and DomainNet."
1690,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the effect of fine-tuning on accuracy in-distribution (ID) and out-of-domain (OOD) accuracy. The authors compare the performance of linear probing and fine tuning on CIFAR-10, DomainNet, and Breeds-Entity-30 on distribution shift datasets. They show that linear probing performs better than fine tuning and OOD accuracy is better than linear probing. They also show that fine tuning with linear probing can improve accuracy on Cifar-10 and DomainNet."
1691,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper proposes a meta-learning approach for the problem of two-class classification with two-labeled data (L2DNC). The main contribution of this paper is to propose a method to learn a clustering model for the unlabeled data. The proposed method is based on the idea that the clustering of the unlabelled data can be used to train a classifier for the seen-class data, which can then be used for the labeled data. "
1692,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper proposes a meta-learning approach for the problem of two-class classification with two-labeled data (L2DNC). The main contribution of this paper is to propose a method to learn a clustering model for the unlabeled data. The proposed method is based on the idea that the clustering of the unlabelled data can be used to train a classifier for the seen-class data, which can then be used for the labeled data. "
1693,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper proposes a meta-learning approach for the problem of two-class classification with two-labeled data (L2DNC). The main contribution of this paper is to propose a method to learn a clustering model for the unlabeled data. The proposed method is based on the idea that the clustering of the unlabelled data can be used to train a classifier for the seen-class data, which can then be used for the labeled data. "
1694,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"This paper proposes a meta-learning approach for the problem of two-class classification with two-labeled data (L2DNC). The main contribution of this paper is to propose a method to learn a clustering model for the unlabeled data. The proposed method is based on the idea that the clustering of the unlabelled data can be used to train a classifier for the seen-class data, which can then be used for the labeled data. "
1695,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning from offline POMDPs. The main idea is to use a latent variable to model the transition between online and offline experiences, and then use deconfounding to learn a causal transition model. The proposed method is based on the causal framework of do-calculator, and is able to generalize to both interventional and observational data. Experiments on synthetic and real-world problems show that the proposed method outperforms existing methods."
1696,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning from offline POMDPs. The main idea is to use a latent variable to model the transition between online and offline experiences, and then use deconfounding to learn a causal transition model. The proposed method is based on the causal framework of do-calculator, and is able to generalize to both interventional and observational data. Experiments on synthetic and real-world problems show that the proposed method outperforms existing methods."
1697,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning from offline POMDPs. The main idea is to use a latent variable to model the transition between online and offline experiences, and then use deconfounding to learn a causal transition model. The proposed method is based on the causal framework of do-calculator, and is able to generalize to both interventional and observational data. Experiments on synthetic and real-world problems show that the proposed method outperforms existing methods."
1698,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,"This paper proposes a method for learning from offline POMDPs. The main idea is to use a latent variable to model the transition between online and offline experiences, and then use deconfounding to learn a causal transition model. The proposed method is based on the causal framework of do-calculator, and is able to generalize to both interventional and observational data. Experiments on synthetic and real-world problems show that the proposed method outperforms existing methods."
1699,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper studies the problem of generating informative utterances in conversations. The authors propose an end-to-end system that combines a retriever and a generator. The retriever is trained on top of the Wizard of Wikipedia dataset, and the generator is trained to generate the top-10 responses to the retriever’s query. The generator is then used to guide the guide retriever to generate more informative responses. Experiments are conducted on the Wizard-of-Wikipedia dataset and show that the proposed method outperforms the baselines."
1700,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper studies the problem of generating informative utterances in conversations. The authors propose an end-to-end system that combines a retriever and a generator. The retriever is trained on top of the Wizard of Wikipedia dataset, and the generator is trained to generate the top-10 responses to the retriever’s query. The generator is then used to guide the guide retriever to generate more informative responses. Experiments are conducted on the Wizard-of-Wikipedia dataset and show that the proposed method outperforms the baselines."
1701,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper studies the problem of generating informative utterances in conversations. The authors propose an end-to-end system that combines a retriever and a generator. The retriever is trained on top of the Wizard of Wikipedia dataset, and the generator is trained to generate the top-10 responses to the retriever’s query. The generator is then used to guide the guide retriever to generate more informative responses. Experiments are conducted on the Wizard-of-Wikipedia dataset and show that the proposed method outperforms the baselines."
1702,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"This paper studies the problem of generating informative utterances in conversations. The authors propose an end-to-end system that combines a retriever and a generator. The retriever is trained on top of the Wizard of Wikipedia dataset, and the generator is trained to generate the top-10 responses to the retriever’s query. The generator is then used to guide the guide retriever to generate more informative responses. Experiments are conducted on the Wizard-of-Wikipedia dataset and show that the proposed method outperforms the baselines."
1703,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper proposes a new algorithm for influence estimation and influence maximization in graph neural networks (GNNs). The main idea is to use a graph neural network to estimate the influence of each node in a graph, and then use a Q-network to predict the next node in the graph. The authors show that the proposed algorithm is able to achieve better performance than the state-of-the-art in both cases. "
1704,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper proposes a new algorithm for influence estimation and influence maximization in graph neural networks (GNNs). The main idea is to use a graph neural network to estimate the influence of each node in a graph, and then use a Q-network to predict the next node in the graph. The authors show that the proposed algorithm is able to achieve better performance than the state-of-the-art in both cases. "
1705,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper proposes a new algorithm for influence estimation and influence maximization in graph neural networks (GNNs). The main idea is to use a graph neural network to estimate the influence of each node in a graph, and then use a Q-network to predict the next node in the graph. The authors show that the proposed algorithm is able to achieve better performance than the state-of-the-art in both cases. "
1706,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"This paper proposes a new algorithm for influence estimation and influence maximization in graph neural networks (GNNs). The main idea is to use a graph neural network to estimate the influence of each node in a graph, and then use a Q-network to predict the next node in the graph. The authors show that the proposed algorithm is able to achieve better performance than the state-of-the-art in both cases. "
1707,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"This paper studies the problem of active learning for domain adaptation. The authors consider the case where the target domain is different from the source domain, and the goal is to learn a class of functions that can be used for label prediction. The main contribution of this paper is to derive generalization error bounds for active learning strategies that are based on the Rademacher average and the discrepancy between the target and the source domains. The generalization bounds are derived by considering the regularity condition of the loss functions and the Lipschitz distance between the source and target domains. In particular, the authors show that the proposed algorithm Kmedoids can generalize better than existing active learning methods on large data sets."
1708,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"This paper studies the problem of active learning for domain adaptation. The authors consider the case where the target domain is different from the source domain, and the goal is to learn a class of functions that can be used for label prediction. The main contribution of this paper is to derive generalization error bounds for active learning strategies that are based on the Rademacher average and the discrepancy between the target and the source domains. The generalization bounds are derived by considering the regularity condition of the loss functions and the Lipschitz distance between the source and target domains. In particular, the authors show that the proposed algorithm Kmedoids can generalize better than existing active learning methods on large data sets."
1709,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"This paper studies the problem of active learning for domain adaptation. The authors consider the case where the target domain is different from the source domain, and the goal is to learn a class of functions that can be used for label prediction. The main contribution of this paper is to derive generalization error bounds for active learning strategies that are based on the Rademacher average and the discrepancy between the target and the source domains. The generalization bounds are derived by considering the regularity condition of the loss functions and the Lipschitz distance between the source and target domains. In particular, the authors show that the proposed algorithm Kmedoids can generalize better than existing active learning methods on large data sets."
1710,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"This paper studies the problem of active learning for domain adaptation. The authors consider the case where the target domain is different from the source domain, and the goal is to learn a class of functions that can be used for label prediction. The main contribution of this paper is to derive generalization error bounds for active learning strategies that are based on the Rademacher average and the discrepancy between the target and the source domains. The generalization bounds are derived by considering the regularity condition of the loss functions and the Lipschitz distance between the source and target domains. In particular, the authors show that the proposed algorithm Kmedoids can generalize better than existing active learning methods on large data sets."
1711,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper studies the problem of variational approximation for Bayesian neural networks with singular statistical models. In particular, the authors propose to use the generalized gamma mean field variational family to approximate the posterior distribution of the singular model. The authors propose a normalizing flow based on the desingularization map, which is an algebraic-geometrical transformation of the source distribution. They show that the proposed method can be used to estimate the leading order term of the leading-order term for the model evidence. They also provide a theoretical analysis of the effect of affine coupling layers."
1712,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper studies the problem of variational approximation for Bayesian neural networks with singular statistical models. In particular, the authors propose to use the generalized gamma mean field variational family to approximate the posterior distribution of the singular model. The authors propose a normalizing flow based on the desingularization map, which is an algebraic-geometrical transformation of the source distribution. They show that the proposed method can be used to estimate the leading order term of the leading-order term for the model evidence. They also provide a theoretical analysis of the effect of affine coupling layers."
1713,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper studies the problem of variational approximation for Bayesian neural networks with singular statistical models. In particular, the authors propose to use the generalized gamma mean field variational family to approximate the posterior distribution of the singular model. The authors propose a normalizing flow based on the desingularization map, which is an algebraic-geometrical transformation of the source distribution. They show that the proposed method can be used to estimate the leading order term of the leading-order term for the model evidence. They also provide a theoretical analysis of the effect of affine coupling layers."
1714,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"This paper studies the problem of variational approximation for Bayesian neural networks with singular statistical models. In particular, the authors propose to use the generalized gamma mean field variational family to approximate the posterior distribution of the singular model. The authors propose a normalizing flow based on the desingularization map, which is an algebraic-geometrical transformation of the source distribution. They show that the proposed method can be used to estimate the leading order term of the leading-order term for the model evidence. They also provide a theoretical analysis of the effect of affine coupling layers."
1715,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,This paper studies the problem of domain generalization (DG) in the context of regularized ERM (ERM). The authors provide a generalization bound on the Rademacher complexity of the model for the general purpose DG problem. The authors also provide a theoretical analysis of the empirical risk-prediction complexity trade-off of the proposed methods.
1716,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,This paper studies the problem of domain generalization (DG) in the context of regularized ERM (ERM). The authors provide a generalization bound on the Rademacher complexity of the model for the general purpose DG problem. The authors also provide a theoretical analysis of the empirical risk-prediction complexity trade-off of the proposed methods.
1717,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,This paper studies the problem of domain generalization (DG) in the context of regularized ERM (ERM). The authors provide a generalization bound on the Rademacher complexity of the model for the general purpose DG problem. The authors also provide a theoretical analysis of the empirical risk-prediction complexity trade-off of the proposed methods.
1718,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,This paper studies the problem of domain generalization (DG) in the context of regularized ERM (ERM). The authors provide a generalization bound on the Rademacher complexity of the model for the general purpose DG problem. The authors also provide a theoretical analysis of the empirical risk-prediction complexity trade-off of the proposed methods.
1719,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the problem of maximum n-times coverage of a peptide vaccine. The authors consider a multi-set multi-cover problem, where each set of cover is a subset of the total coverage. The problem is formulated as an integer linear programming problem, and the authors propose a sequential greedy optimization algorithm to solve the problem. The proposed algorithm is evaluated on a pan-strain COVID-19 vaccine design, and it is shown that the proposed algorithm achieves better coverage than the baselines."
1720,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the problem of maximum n-times coverage of a peptide vaccine. The authors consider a multi-set multi-cover problem, where each set of cover is a subset of the total coverage. The problem is formulated as an integer linear programming problem, and the authors propose a sequential greedy optimization algorithm to solve the problem. The proposed algorithm is evaluated on a pan-strain COVID-19 vaccine design, and it is shown that the proposed algorithm achieves better coverage than the baselines."
1721,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the problem of maximum n-times coverage of a peptide vaccine. The authors consider a multi-set multi-cover problem, where each set of cover is a subset of the total coverage. The problem is formulated as an integer linear programming problem, and the authors propose a sequential greedy optimization algorithm to solve the problem. The proposed algorithm is evaluated on a pan-strain COVID-19 vaccine design, and it is shown that the proposed algorithm achieves better coverage than the baselines."
1722,SP:b1f622cbc827e880f98de9e99eca498584efe011,"This paper studies the problem of maximum n-times coverage of a peptide vaccine. The authors consider a multi-set multi-cover problem, where each set of cover is a subset of the total coverage. The problem is formulated as an integer linear programming problem, and the authors propose a sequential greedy optimization algorithm to solve the problem. The proposed algorithm is evaluated on a pan-strain COVID-19 vaccine design, and it is shown that the proposed algorithm achieves better coverage than the baselines."
1723,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"This paper proposes a new initialization method for spiking neural networks (SNNs) that is based on the slant asymptotic formula for the response curve of spiking neurons. The authors show that the gradient vanishing of SNNs with weight initialization can be reduced to a slant approximation of the response curves of the neurons, which can then be used as an initialization method. The proposed method is evaluated on MNIST and CIFAR-10 dataset and compared to several other initialization methods. The experimental results show the effectiveness of the proposed method."
1724,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"This paper proposes a new initialization method for spiking neural networks (SNNs) that is based on the slant asymptotic formula for the response curve of spiking neurons. The authors show that the gradient vanishing of SNNs with weight initialization can be reduced to a slant approximation of the response curves of the neurons, which can then be used as an initialization method. The proposed method is evaluated on MNIST and CIFAR-10 dataset and compared to several other initialization methods. The experimental results show the effectiveness of the proposed method."
1725,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"This paper proposes a new initialization method for spiking neural networks (SNNs) that is based on the slant asymptotic formula for the response curve of spiking neurons. The authors show that the gradient vanishing of SNNs with weight initialization can be reduced to a slant approximation of the response curves of the neurons, which can then be used as an initialization method. The proposed method is evaluated on MNIST and CIFAR-10 dataset and compared to several other initialization methods. The experimental results show the effectiveness of the proposed method."
1726,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"This paper proposes a new initialization method for spiking neural networks (SNNs) that is based on the slant asymptotic formula for the response curve of spiking neurons. The authors show that the gradient vanishing of SNNs with weight initialization can be reduced to a slant approximation of the response curves of the neurons, which can then be used as an initialization method. The proposed method is evaluated on MNIST and CIFAR-10 dataset and compared to several other initialization methods. The experimental results show the effectiveness of the proposed method."
1727,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a federated learning algorithm to address the problem of distribution shift in distributed data. The authors propose to use a block-cyclic pattern to deal with distribution shift. The proposed algorithm is based on the Federated Expectation-Maximization algorithm. The main idea of the algorithm is to learn a mixture model of distributions, which is then used to train a network with light-weight branches. The experiments are conducted on image classification, next word prediction and Stack Overflow."
1728,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a federated learning algorithm to address the problem of distribution shift in distributed data. The authors propose to use a block-cyclic pattern to deal with distribution shift. The proposed algorithm is based on the Federated Expectation-Maximization algorithm. The main idea of the algorithm is to learn a mixture model of distributions, which is then used to train a network with light-weight branches. The experiments are conducted on image classification, next word prediction and Stack Overflow."
1729,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a federated learning algorithm to address the problem of distribution shift in distributed data. The authors propose to use a block-cyclic pattern to deal with distribution shift. The proposed algorithm is based on the Federated Expectation-Maximization algorithm. The main idea of the algorithm is to learn a mixture model of distributions, which is then used to train a network with light-weight branches. The experiments are conducted on image classification, next word prediction and Stack Overflow."
1730,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper proposes a federated learning algorithm to address the problem of distribution shift in distributed data. The authors propose to use a block-cyclic pattern to deal with distribution shift. The proposed algorithm is based on the Federated Expectation-Maximization algorithm. The main idea of the algorithm is to learn a mixture model of distributions, which is then used to train a network with light-weight branches. The experiments are conducted on image classification, next word prediction and Stack Overflow."
1731,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a pruning strategy for deep neural networks (DNs) based on the spline interpretation of DNs. Specifically, the authors propose to prune a subset of DN nodes based on their spline partition regions. The authors show that the pruned DN nodes are more likely to have early-bird (EB) phenomenon, and propose to use this phenomenon to guide the pruning of the DN nodes. Experiments on MNIST, CIFAR-10, and ImageNet demonstrate the effectiveness of the proposed pruning method."
1732,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a pruning strategy for deep neural networks (DNs) based on the spline interpretation of DNs. Specifically, the authors propose to prune a subset of DN nodes based on their spline partition regions. The authors show that the pruned DN nodes are more likely to have early-bird (EB) phenomenon, and propose to use this phenomenon to guide the pruning of the DN nodes. Experiments on MNIST, CIFAR-10, and ImageNet demonstrate the effectiveness of the proposed pruning method."
1733,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a pruning strategy for deep neural networks (DNs) based on the spline interpretation of DNs. Specifically, the authors propose to prune a subset of DN nodes based on their spline partition regions. The authors show that the pruned DN nodes are more likely to have early-bird (EB) phenomenon, and propose to use this phenomenon to guide the pruning of the DN nodes. Experiments on MNIST, CIFAR-10, and ImageNet demonstrate the effectiveness of the proposed pruning method."
1734,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"This paper proposes a pruning strategy for deep neural networks (DNs) based on the spline interpretation of DNs. Specifically, the authors propose to prune a subset of DN nodes based on their spline partition regions. The authors show that the pruned DN nodes are more likely to have early-bird (EB) phenomenon, and propose to use this phenomenon to guide the pruning of the DN nodes. Experiments on MNIST, CIFAR-10, and ImageNet demonstrate the effectiveness of the proposed pruning method."
1735,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper studies the problem of fair representation learning from the perspective of fairness measurement. The authors propose a novel algorithm for inner-level optimization and implicit differentiation. Specifically, the authors propose an implicit path alignment algorithm that combines inner optimization with implicit differentiation to find optimal group predictors. They also propose a sufficiency rule for inner optimization and an implicit differentiation algorithm for optimization path alignment. Experiments on classification and regression tasks show that the proposed algorithm outperforms existing methods."
1736,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper studies the problem of fair representation learning from the perspective of fairness measurement. The authors propose a novel algorithm for inner-level optimization and implicit differentiation. Specifically, the authors propose an implicit path alignment algorithm that combines inner optimization with implicit differentiation to find optimal group predictors. They also propose a sufficiency rule for inner optimization and an implicit differentiation algorithm for optimization path alignment. Experiments on classification and regression tasks show that the proposed algorithm outperforms existing methods."
1737,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper studies the problem of fair representation learning from the perspective of fairness measurement. The authors propose a novel algorithm for inner-level optimization and implicit differentiation. Specifically, the authors propose an implicit path alignment algorithm that combines inner optimization with implicit differentiation to find optimal group predictors. They also propose a sufficiency rule for inner optimization and an implicit differentiation algorithm for optimization path alignment. Experiments on classification and regression tasks show that the proposed algorithm outperforms existing methods."
1738,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"This paper studies the problem of fair representation learning from the perspective of fairness measurement. The authors propose a novel algorithm for inner-level optimization and implicit differentiation. Specifically, the authors propose an implicit path alignment algorithm that combines inner optimization with implicit differentiation to find optimal group predictors. They also propose a sufficiency rule for inner optimization and an implicit differentiation algorithm for optimization path alignment. Experiments on classification and regression tasks show that the proposed algorithm outperforms existing methods."
1739,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the problem of offline reinforcement learning via supervised learning (RvS) in the context of temporal difference (TD) learning. In particular, the authors focus on the setting where there is a large amount of suboptimal data and the goal is to learn a policy that maximizes the difference between the value of the learned policy and the policy conditioned on the data. The authors propose to use a value-based approximate dynamic programming algorithm (VAE) to approximate the value function of the policy. The proposed method is evaluated on a variety of offline RL benchmarks and compared to a number of state-of-the-art methods."
1740,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the problem of offline reinforcement learning via supervised learning (RvS) in the context of temporal difference (TD) learning. In particular, the authors focus on the setting where there is a large amount of suboptimal data and the goal is to learn a policy that maximizes the difference between the value of the learned policy and the policy conditioned on the data. The authors propose to use a value-based approximate dynamic programming algorithm (VAE) to approximate the value function of the policy. The proposed method is evaluated on a variety of offline RL benchmarks and compared to a number of state-of-the-art methods."
1741,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the problem of offline reinforcement learning via supervised learning (RvS) in the context of temporal difference (TD) learning. In particular, the authors focus on the setting where there is a large amount of suboptimal data and the goal is to learn a policy that maximizes the difference between the value of the learned policy and the policy conditioned on the data. The authors propose to use a value-based approximate dynamic programming algorithm (VAE) to approximate the value function of the policy. The proposed method is evaluated on a variety of offline RL benchmarks and compared to a number of state-of-the-art methods."
1742,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper studies the problem of offline reinforcement learning via supervised learning (RvS) in the context of temporal difference (TD) learning. In particular, the authors focus on the setting where there is a large amount of suboptimal data and the goal is to learn a policy that maximizes the difference between the value of the learned policy and the policy conditioned on the data. The authors propose to use a value-based approximate dynamic programming algorithm (VAE) to approximate the value function of the policy. The proposed method is evaluated on a variety of offline RL benchmarks and compared to a number of state-of-the-art methods."
1743,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method for learning a hierarchical Bayesian framework for program induction. The proposed method is based on a Bayesian approach to map induction, which is motivated by the observation that the spatial priors of the learned model are strongly correlated with the spatial structure of unobserved spaces. The authors show that the proposed method can be applied to a variety of tasks, such as planning, exploration, and navigation. The method is evaluated on a set of synthetic and real-world tasks, and compared to a number of baselines."
1744,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method for learning a hierarchical Bayesian framework for program induction. The proposed method is based on a Bayesian approach to map induction, which is motivated by the observation that the spatial priors of the learned model are strongly correlated with the spatial structure of unobserved spaces. The authors show that the proposed method can be applied to a variety of tasks, such as planning, exploration, and navigation. The method is evaluated on a set of synthetic and real-world tasks, and compared to a number of baselines."
1745,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method for learning a hierarchical Bayesian framework for program induction. The proposed method is based on a Bayesian approach to map induction, which is motivated by the observation that the spatial priors of the learned model are strongly correlated with the spatial structure of unobserved spaces. The authors show that the proposed method can be applied to a variety of tasks, such as planning, exploration, and navigation. The method is evaluated on a set of synthetic and real-world tasks, and compared to a number of baselines."
1746,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper proposes a method for learning a hierarchical Bayesian framework for program induction. The proposed method is based on a Bayesian approach to map induction, which is motivated by the observation that the spatial priors of the learned model are strongly correlated with the spatial structure of unobserved spaces. The authors show that the proposed method can be applied to a variety of tasks, such as planning, exploration, and navigation. The method is evaluated on a set of synthetic and real-world tasks, and compared to a number of baselines."
1747,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"This paper proposes a differentiable approach for learning a set of probabilistic factors in a graphical model. The key idea is to use a neural network to learn a feature extractor for each component of the graphical model, and then use a belief propagation algorithm to propagate the learned factors to the next part of the model. In particular, the authors propose to use the gradient descent algorithm for the belief propagation. The authors evaluate the proposed method on articulated pose tracking tasks and show that it outperforms existing methods."
1748,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"This paper proposes a differentiable approach for learning a set of probabilistic factors in a graphical model. The key idea is to use a neural network to learn a feature extractor for each component of the graphical model, and then use a belief propagation algorithm to propagate the learned factors to the next part of the model. In particular, the authors propose to use the gradient descent algorithm for the belief propagation. The authors evaluate the proposed method on articulated pose tracking tasks and show that it outperforms existing methods."
1749,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"This paper proposes a differentiable approach for learning a set of probabilistic factors in a graphical model. The key idea is to use a neural network to learn a feature extractor for each component of the graphical model, and then use a belief propagation algorithm to propagate the learned factors to the next part of the model. In particular, the authors propose to use the gradient descent algorithm for the belief propagation. The authors evaluate the proposed method on articulated pose tracking tasks and show that it outperforms existing methods."
1750,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"This paper proposes a differentiable approach for learning a set of probabilistic factors in a graphical model. The key idea is to use a neural network to learn a feature extractor for each component of the graphical model, and then use a belief propagation algorithm to propagate the learned factors to the next part of the model. In particular, the authors propose to use the gradient descent algorithm for the belief propagation. The authors evaluate the proposed method on articulated pose tracking tasks and show that it outperforms existing methods."
1751,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,This paper proposes a graph-based generative model for in silico drug discovery. The proposed method is based on graph neural networks (GNNs) and is able to generate a scaffold of molecules that can then be used for optimization. The method is evaluated on a number of synthetic and real-world tasks and compared to several baselines. The results show that the proposed method outperforms the state-of-the-art methods.
1752,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,This paper proposes a graph-based generative model for in silico drug discovery. The proposed method is based on graph neural networks (GNNs) and is able to generate a scaffold of molecules that can then be used for optimization. The method is evaluated on a number of synthetic and real-world tasks and compared to several baselines. The results show that the proposed method outperforms the state-of-the-art methods.
1753,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,This paper proposes a graph-based generative model for in silico drug discovery. The proposed method is based on graph neural networks (GNNs) and is able to generate a scaffold of molecules that can then be used for optimization. The method is evaluated on a number of synthetic and real-world tasks and compared to several baselines. The results show that the proposed method outperforms the state-of-the-art methods.
1754,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,This paper proposes a graph-based generative model for in silico drug discovery. The proposed method is based on graph neural networks (GNNs) and is able to generate a scaffold of molecules that can then be used for optimization. The method is evaluated on a number of synthetic and real-world tasks and compared to several baselines. The results show that the proposed method outperforms the state-of-the-art methods.
1755,SP:318b3c294a475960c13a4914b035fd3a2ea84661,This paper studies the problem of imitation learning in the presence of adversarial examples. The authors show that the total variation distance between the imitation learner and the adversary can be reduced by a factor of a factor that depends on the number of expert examples. They also show that adversarial imitation learning can be used to improve the performance of reinforcement learning algorithms.
1756,SP:318b3c294a475960c13a4914b035fd3a2ea84661,This paper studies the problem of imitation learning in the presence of adversarial examples. The authors show that the total variation distance between the imitation learner and the adversary can be reduced by a factor of a factor that depends on the number of expert examples. They also show that adversarial imitation learning can be used to improve the performance of reinforcement learning algorithms.
1757,SP:318b3c294a475960c13a4914b035fd3a2ea84661,This paper studies the problem of imitation learning in the presence of adversarial examples. The authors show that the total variation distance between the imitation learner and the adversary can be reduced by a factor of a factor that depends on the number of expert examples. They also show that adversarial imitation learning can be used to improve the performance of reinforcement learning algorithms.
1758,SP:318b3c294a475960c13a4914b035fd3a2ea84661,This paper studies the problem of imitation learning in the presence of adversarial examples. The authors show that the total variation distance between the imitation learner and the adversary can be reduced by a factor of a factor that depends on the number of expert examples. They also show that adversarial imitation learning can be used to improve the performance of reinforcement learning algorithms.
1759,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of reweighting algorithms for fairness in the overparameterized setting. The authors propose a new algorithm ERM, which is an interpolator-based algorithm that is based on the ERM interpolator. The main contribution of this paper is to show that ERM is equivalent to the interpolator in terms of worst-case worst-group test performance. The paper also provides theoretical backing for the proposed algorithm."
1760,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of reweighting algorithms for fairness in the overparameterized setting. The authors propose a new algorithm ERM, which is an interpolator-based algorithm that is based on the ERM interpolator. The main contribution of this paper is to show that ERM is equivalent to the interpolator in terms of worst-case worst-group test performance. The paper also provides theoretical backing for the proposed algorithm."
1761,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of reweighting algorithms for fairness in the overparameterized setting. The authors propose a new algorithm ERM, which is an interpolator-based algorithm that is based on the ERM interpolator. The main contribution of this paper is to show that ERM is equivalent to the interpolator in terms of worst-case worst-group test performance. The paper also provides theoretical backing for the proposed algorithm."
1762,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper studies the problem of reweighting algorithms for fairness in the overparameterized setting. The authors propose a new algorithm ERM, which is an interpolator-based algorithm that is based on the ERM interpolator. The main contribution of this paper is to show that ERM is equivalent to the interpolator in terms of worst-case worst-group test performance. The paper also provides theoretical backing for the proposed algorithm."
1763,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,This paper proposes Rational Inattention (RIRL) model to model human-irrationality in multi-agent reinforcement learning (MARL). The main idea is to model the cost of cognitive information processing as a function of the number of timesteps in the agent’s inattention. The authors propose to use mutual information between timestep dynamics and information channels in order to reduce the information asymmetry between the agent and the environment. The proposed RIRL model is evaluated on a variety of simulated and real-world environments. The results show that the proposed model outperforms the baselines.
1764,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,This paper proposes Rational Inattention (RIRL) model to model human-irrationality in multi-agent reinforcement learning (MARL). The main idea is to model the cost of cognitive information processing as a function of the number of timesteps in the agent’s inattention. The authors propose to use mutual information between timestep dynamics and information channels in order to reduce the information asymmetry between the agent and the environment. The proposed RIRL model is evaluated on a variety of simulated and real-world environments. The results show that the proposed model outperforms the baselines.
1765,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,This paper proposes Rational Inattention (RIRL) model to model human-irrationality in multi-agent reinforcement learning (MARL). The main idea is to model the cost of cognitive information processing as a function of the number of timesteps in the agent’s inattention. The authors propose to use mutual information between timestep dynamics and information channels in order to reduce the information asymmetry between the agent and the environment. The proposed RIRL model is evaluated on a variety of simulated and real-world environments. The results show that the proposed model outperforms the baselines.
1766,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,This paper proposes Rational Inattention (RIRL) model to model human-irrationality in multi-agent reinforcement learning (MARL). The main idea is to model the cost of cognitive information processing as a function of the number of timesteps in the agent’s inattention. The authors propose to use mutual information between timestep dynamics and information channels in order to reduce the information asymmetry between the agent and the environment. The proposed RIRL model is evaluated on a variety of simulated and real-world environments. The results show that the proposed model outperforms the baselines.
1767,SP:100c91da177504d89f1819f4fdce72ebcf848902,This paper proposes a phase-oriented algorithm for audio adversarial attacks. The proposed method is based on phase-based energy dissipation. The authors propose a weighted loss function to improve the imperceptibility of the perturbations. The paper also proposes a new spectral consistency method for the short-time Fourier transform (STFT). The authors evaluate the effectiveness of the proposed method on a variety of adversarial examples.
1768,SP:100c91da177504d89f1819f4fdce72ebcf848902,This paper proposes a phase-oriented algorithm for audio adversarial attacks. The proposed method is based on phase-based energy dissipation. The authors propose a weighted loss function to improve the imperceptibility of the perturbations. The paper also proposes a new spectral consistency method for the short-time Fourier transform (STFT). The authors evaluate the effectiveness of the proposed method on a variety of adversarial examples.
1769,SP:100c91da177504d89f1819f4fdce72ebcf848902,This paper proposes a phase-oriented algorithm for audio adversarial attacks. The proposed method is based on phase-based energy dissipation. The authors propose a weighted loss function to improve the imperceptibility of the perturbations. The paper also proposes a new spectral consistency method for the short-time Fourier transform (STFT). The authors evaluate the effectiveness of the proposed method on a variety of adversarial examples.
1770,SP:100c91da177504d89f1819f4fdce72ebcf848902,This paper proposes a phase-oriented algorithm for audio adversarial attacks. The proposed method is based on phase-based energy dissipation. The authors propose a weighted loss function to improve the imperceptibility of the perturbations. The paper also proposes a new spectral consistency method for the short-time Fourier transform (STFT). The authors evaluate the effectiveness of the proposed method on a variety of adversarial examples.
1771,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper proposes a new method for self-supervised learning. The proposed method, called DirectSet(α) is a linear network with an eigen-decomposition step and a weight decay step. The authors show that the proposed method can achieve better sample complexity than BYOL and DirectCopy on CIFAR-10, STL-10 and ImageNet. "
1772,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper proposes a new method for self-supervised learning. The proposed method, called DirectSet(α) is a linear network with an eigen-decomposition step and a weight decay step. The authors show that the proposed method can achieve better sample complexity than BYOL and DirectCopy on CIFAR-10, STL-10 and ImageNet. "
1773,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper proposes a new method for self-supervised learning. The proposed method, called DirectSet(α) is a linear network with an eigen-decomposition step and a weight decay step. The authors show that the proposed method can achieve better sample complexity than BYOL and DirectCopy on CIFAR-10, STL-10 and ImageNet. "
1774,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"This paper proposes a new method for self-supervised learning. The proposed method, called DirectSet(α) is a linear network with an eigen-decomposition step and a weight decay step. The authors show that the proposed method can achieve better sample complexity than BYOL and DirectCopy on CIFAR-10, STL-10 and ImageNet. "
1775,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a method for learning long-term sequential dependencies. The method is based on a system of multiscale ordinary differential equations (MODE) and time-discretization. The proposed method is evaluated on image classification, keyword spotting and language modeling tasks."
1776,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a method for learning long-term sequential dependencies. The method is based on a system of multiscale ordinary differential equations (MODE) and time-discretization. The proposed method is evaluated on image classification, keyword spotting and language modeling tasks."
1777,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a method for learning long-term sequential dependencies. The method is based on a system of multiscale ordinary differential equations (MODE) and time-discretization. The proposed method is evaluated on image classification, keyword spotting and language modeling tasks."
1778,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"This paper proposes a method for learning long-term sequential dependencies. The method is based on a system of multiscale ordinary differential equations (MODE) and time-discretization. The proposed method is evaluated on image classification, keyword spotting and language modeling tasks."
1779,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper studies the problem of learning geometric deep learning models that are rotation and permutation-equivariant. The authors propose a new method for learning such models. The main idea is to use product reduction to reduce the number of products of a vector to a single vector, and then use the sum of the product of the two vectors as a regularizer. The paper shows that the proposed method can be applied to a variety of geometric models, and that it is able to achieve better performance than the state-of-the-art."
1780,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper studies the problem of learning geometric deep learning models that are rotation and permutation-equivariant. The authors propose a new method for learning such models. The main idea is to use product reduction to reduce the number of products of a vector to a single vector, and then use the sum of the product of the two vectors as a regularizer. The paper shows that the proposed method can be applied to a variety of geometric models, and that it is able to achieve better performance than the state-of-the-art."
1781,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper studies the problem of learning geometric deep learning models that are rotation and permutation-equivariant. The authors propose a new method for learning such models. The main idea is to use product reduction to reduce the number of products of a vector to a single vector, and then use the sum of the product of the two vectors as a regularizer. The paper shows that the proposed method can be applied to a variety of geometric models, and that it is able to achieve better performance than the state-of-the-art."
1782,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper studies the problem of learning geometric deep learning models that are rotation and permutation-equivariant. The authors propose a new method for learning such models. The main idea is to use product reduction to reduce the number of products of a vector to a single vector, and then use the sum of the product of the two vectors as a regularizer. The paper shows that the proposed method can be applied to a variety of geometric models, and that it is able to achieve better performance than the state-of-the-art."
1783,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,This paper proposes a novel approach to solve the order fulfillment problem. The authors propose to use edge-feature-embedded graph attention mechanism to learn the assignment policy for the tripartite graph. The proposed method is evaluated on a synthetic and real-world dataset.
1784,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,This paper proposes a novel approach to solve the order fulfillment problem. The authors propose to use edge-feature-embedded graph attention mechanism to learn the assignment policy for the tripartite graph. The proposed method is evaluated on a synthetic and real-world dataset.
1785,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,This paper proposes a novel approach to solve the order fulfillment problem. The authors propose to use edge-feature-embedded graph attention mechanism to learn the assignment policy for the tripartite graph. The proposed method is evaluated on a synthetic and real-world dataset.
1786,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,This paper proposes a novel approach to solve the order fulfillment problem. The authors propose to use edge-feature-embedded graph attention mechanism to learn the assignment policy for the tripartite graph. The proposed method is evaluated on a synthetic and real-world dataset.
1787,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,This paper proposes a new evaluation metric for dialogue summarization based on Dialogue Context (CODC). The proposed metric is based on a combination of two existing metrics: CIDEr and CODC. The paper also proposes a neural summarization model based on WordNet. The proposed method is evaluated on a variety of natural language generation tasks.
1788,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,This paper proposes a new evaluation metric for dialogue summarization based on Dialogue Context (CODC). The proposed metric is based on a combination of two existing metrics: CIDEr and CODC. The paper also proposes a neural summarization model based on WordNet. The proposed method is evaluated on a variety of natural language generation tasks.
1789,SP:7f7f8245914ecc5b00570916bbcdb6c9b49d26de,This paper proposes a new evaluation metric for dialogue summarization based on Dialogue Context (CODC). The proposed metric is based on a combination of two existing metrics: CIDEr and CODC. The paper also proposes a neural summarization model based on WordNet. The proposed method is evaluated on a variety of natural language generation tasks.
1790,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the problem of learning entity embeddings for attribute embedding. Specifically, the authors propose a new embedding method based on the concept of “Horn rule”, which is an extension of the “attribute-dependent embedding” paper. The main idea of the paper is to learn a set of embedding strategies for monotonic and non-monotonic attribute dependencies. The authors show empirically that the proposed method outperforms existing methods on a variety of datasets."
1791,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the problem of learning entity embeddings for attribute embedding. Specifically, the authors propose a new embedding method based on the concept of “Horn rule”, which is an extension of the “attribute-dependent embedding” paper. The main idea of the paper is to learn a set of embedding strategies for monotonic and non-monotonic attribute dependencies. The authors show empirically that the proposed method outperforms existing methods on a variety of datasets."
1792,SP:e1591b266d6c329c6c07f4e5234253249ab1db8c,"This paper studies the problem of learning entity embeddings for attribute embedding. Specifically, the authors propose a new embedding method based on the concept of “Horn rule”, which is an extension of the “attribute-dependent embedding” paper. The main idea of the paper is to learn a set of embedding strategies for monotonic and non-monotonic attribute dependencies. The authors show empirically that the proposed method outperforms existing methods on a variety of datasets."
1793,SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper presents a set of experiments to evaluate the performance of transformer-based models on a variety of reasoning tasks. The authors show that transformers are able to perform better than the state-of-the-art models on most of the tasks. They also show that the model is able to generalize to new tasks such as logical reasoning, mathematical reasoning, commonsense reasoning, and mathematical reasoning."
1794,SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper presents a set of experiments to evaluate the performance of transformer-based models on a variety of reasoning tasks. The authors show that transformers are able to perform better than the state-of-the-art models on most of the tasks. They also show that the model is able to generalize to new tasks such as logical reasoning, mathematical reasoning, commonsense reasoning, and mathematical reasoning."
1795,SP:794cca5205d667900ceb9a1332b6272320752ef4,"This paper presents a set of experiments to evaluate the performance of transformer-based models on a variety of reasoning tasks. The authors show that transformers are able to perform better than the state-of-the-art models on most of the tasks. They also show that the model is able to generalize to new tasks such as logical reasoning, mathematical reasoning, commonsense reasoning, and mathematical reasoning."
1796,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper studies the problem of compositional generalization, i.e., the generalization of a classifier to a new task. The authors propose a new method, called compositional recursive learner (CRL), which is based on the idea that the compositional structure of the task distribution can be decomposed into shared subproblems, which can then be used to train a compositional learner. The main contribution of the paper is the theoretical analysis of the proposed method."
1797,SP:3a16ffa27e7ef0684e6d0f3ee744787aef108a07,"This paper studies the problem of compositional generalization, i.e., the generalization of a classifier to a new task. The authors propose a new method, called compositional recursive learner (CRL), which is based on the idea that the compositional structure of the task distribution can be decomposed into shared subproblems, which can then be used to train a compositional learner. The main contribution of the paper is the theoretical analysis of the proposed method."
1798,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IPnet), a method for weight pruning and activation pruning in deep neural networks (DNNs). The main idea is to prune the weights and activations of a DNN to reduce the sparsity of the network. The authors propose a method that prunes the activation and weight numbers of the DNN, which is based on the idea of Integral pruning. The proposed method is evaluated on a variety of datasets and compared to other methods."
1799,SP:7f91f3805bd643e3b796e885b00f88a77aa49d15,"This paper proposes Integral Pruning (IPnet), a method for weight pruning and activation pruning in deep neural networks (DNNs). The main idea is to prune the weights and activations of a DNN to reduce the sparsity of the network. The authors propose a method that prunes the activation and weight numbers of the DNN, which is based on the idea of Integral pruning. The proposed method is evaluated on a variety of datasets and compared to other methods."
1800,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a novel method for feature selection based on Generative Adversarial Networks (GANs). The proposed method is based on the idea of knockoff generation, where the generator is trained to generate knockoffs that are close to the original feature distribution, and the discriminator and stability network are trained to discriminate between the generated features and the original features. The authors show that the proposed method outperforms the state-of-the-art methods in terms of False Discovery Rate (FDR) on the MNIST and CIFAR-10 datasets. "
1801,SP:d34277109f713f78abd3b911c7a38baf18c8c8c1,"This paper proposes a novel method for feature selection based on Generative Adversarial Networks (GANs). The proposed method is based on the idea of knockoff generation, where the generator is trained to generate knockoffs that are close to the original feature distribution, and the discriminator and stability network are trained to discriminate between the generated features and the original features. The authors show that the proposed method outperforms the state-of-the-art methods in terms of False Discovery Rate (FDR) on the MNIST and CIFAR-10 datasets. "
1802,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes spatial-Winograd pruning, a method to reduce the spatial-domain sparsity of convolutional neural networks (CNNs) by pruning the ReLU function in the Winograd domain. The main idea is to use the importance factor matrix of the weight importance and the importance of the gradients of the weights in the spatial domain as a proxy for the weight sparsity in the original network. The method is evaluated on CIFAR-10, ImageNet, and Cifar-100."
1803,SP:7bf79b020c2cafaced61f2595ad17e8238c3dc5d,"This paper proposes spatial-Winograd pruning, a method to reduce the spatial-domain sparsity of convolutional neural networks (CNNs) by pruning the ReLU function in the Winograd domain. The main idea is to use the importance factor matrix of the weight importance and the importance of the gradients of the weights in the spatial domain as a proxy for the weight sparsity in the original network. The method is evaluated on CIFAR-10, ImageNet, and Cifar-100."
1804,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed model is based on the adversarially learned mixture model (AMM). The proposed method is evaluated on the MNIST and SVHN datasets.
1805,SP:35e050c84f55f30b5a958128fa5bdaa1cb3f7e90,This paper proposes a generative model for unsupervised or semi-supervised data clustering. The proposed model is based on the adversarially learned mixture model (AMM). The proposed method is evaluated on the MNIST and SVHN datasets.
1806,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes an adaptive variance reduction algorithm for estimating the gradient of a discrete latent variable model with stochastic binary layers. The main contribution of this paper is to propose a variance-reduced estimator, which can be used to reduce the computational complexity of the estimator. The proposed estimator is based on the REINFORCE estimator and can be applied to a variety of tasks such as auto-encoding variational inference, maximum likelihood estimation, and Monte Carlo integration. The paper also proposes an antithetic sampling method for minimizing the variance of the augmented space."
1807,SP:c65ea3a1cc796e65465e8b4dc05ae103316e2cb3,"This paper proposes an adaptive variance reduction algorithm for estimating the gradient of a discrete latent variable model with stochastic binary layers. The main contribution of this paper is to propose a variance-reduced estimator, which can be used to reduce the computational complexity of the estimator. The proposed estimator is based on the REINFORCE estimator and can be applied to a variety of tasks such as auto-encoding variational inference, maximum likelihood estimation, and Monte Carlo integration. The paper also proposes an antithetic sampling method for minimizing the variance of the augmented space."
1808,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a new probabilistic programming language, called MXFusion, which is a combination of Bayesian nonparametric models (BPMs) and probabilism modules. The core idea of the paper is to learn a module that takes as input a random variable and outputs a probability distribution over the random variable, and then uses a variational inference method to estimate the probability distribution of the random variables. The authors show that the proposed method can be applied to a variety of models, including Gaussian process models, Bayesian neural networks, and neural networks trained on MNIST and CIFAR-10."
1809,SP:c54ee7a7d321a487257d2554c7e689967cf0ceaa,"This paper proposes a new probabilistic programming language, called MXFusion, which is a combination of Bayesian nonparametric models (BPMs) and probabilism modules. The core idea of the paper is to learn a module that takes as input a random variable and outputs a probability distribution over the random variable, and then uses a variational inference method to estimate the probability distribution of the random variables. The authors show that the proposed method can be applied to a variety of models, including Gaussian process models, Bayesian neural networks, and neural networks trained on MNIST and CIFAR-10."
1810,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method for pruning large neural networks. The main idea is to learn a saliency criterion for each layer of the network based on the importance of each connection in the network. The proposed method is evaluated on a variety of architectures, including convolutional, residual and recurrent networks. It is shown that the proposed method outperforms existing pruning methods in terms of accuracy and time complexity."
1811,SP:b65eb92fcbea57626721a156be6e6cbbad3c071c,"This paper proposes a method for pruning large neural networks. The main idea is to learn a saliency criterion for each layer of the network based on the importance of each connection in the network. The proposed method is evaluated on a variety of architectures, including convolutional, residual and recurrent networks. It is shown that the proposed method outperforms existing pruning methods in terms of accuracy and time complexity."
1812,SP:986b9781534ffec84619872cd269ad48d235f869,This paper studies the problem of beam search for decoding neural sequence models. The authors propose a new beam search algorithm that is based on the idea that the beam width of the search can be controlled by the number of copies of the input sequence. They show that beam search with a small beam width can achieve better performance than beam search without a large beam width. They also show that the performance of the beam search is affected by the choice of beam width and the amount of data used for beam search.
1813,SP:986b9781534ffec84619872cd269ad48d235f869,This paper studies the problem of beam search for decoding neural sequence models. The authors propose a new beam search algorithm that is based on the idea that the beam width of the search can be controlled by the number of copies of the input sequence. They show that beam search with a small beam width can achieve better performance than beam search without a large beam width. They also show that the performance of the beam search is affected by the choice of beam width and the amount of data used for beam search.
1814,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes a method for model-free reinforcement learning (MRL) based on BackPlay. The main idea of Backplay is to learn a curriculum of demonstrations for a given task, and then use this curriculum to train a policy that maximizes the sample efficiency of the learned policy. The method is evaluated on a variety of tasks, and compared to a number of state-of-the-art methods. The results show that Backplay outperforms these methods in terms of sample efficiency."
1815,SP:b2a8f5c3a417390582f26981fe0c81c16d2bb07d,"This paper proposes a method for model-free reinforcement learning (MRL) based on BackPlay. The main idea of Backplay is to learn a curriculum of demonstrations for a given task, and then use this curriculum to train a policy that maximizes the sample efficiency of the learned policy. The method is evaluated on a variety of tasks, and compared to a number of state-of-the-art methods. The results show that Backplay outperforms these methods in terms of sample efficiency."
1816,SP:426c98718b2dbad640380ec4ccb2b656958389bc,This paper proposes a method for model compression. The main idea is to use Kronecker-factored Approximate Curvature (KCA) method to estimate the Hessian of the compression ratio of a given layer. The paper also proposes a pruning criterion for the pruning of layers. The proposed method is evaluated on ImageNet and VGG16.
1817,SP:426c98718b2dbad640380ec4ccb2b656958389bc,This paper proposes a method for model compression. The main idea is to use Kronecker-factored Approximate Curvature (KCA) method to estimate the Hessian of the compression ratio of a given layer. The paper also proposes a pruning criterion for the pruning of layers. The proposed method is evaluated on ImageNet and VGG16.
1818,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper proposes a method for dissection of GANs. The method is based on segmentation-based network dissection, and is able to disentangle the unit-level, object-level and scene-level representations of GANS. The authors also propose a method to identify the artifact-causing units in GAN models. The proposed method is evaluated on a variety of datasets and models."
1819,SP:b97549a4c1f4b2407f97576fed46c25cbf669009,"This paper proposes a method for dissection of GANs. The method is based on segmentation-based network dissection, and is able to disentangle the unit-level, object-level and scene-level representations of GANS. The authors also propose a method to identify the artifact-causing units in GAN models. The proposed method is evaluated on a variety of datasets and models."
1820,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper studies the problem of catastrophic forgetting in multi-task learning. The authors show that the L-space of a neural network can be viewed as a function of the function distance between the parameters of the network and the function. They show that if the function L distance is smaller than the parameter L distance, then catastrophic forgetting can happen. They also show that when the parameter and function L distances are small, catastrophic forgetting does not happen. "
1821,SP:252c20661ef36f8c32f7412db315747925d3a3d0,"This paper studies the problem of catastrophic forgetting in multi-task learning. The authors show that the L-space of a neural network can be viewed as a function of the function distance between the parameters of the network and the function. They show that if the function L distance is smaller than the parameter L distance, then catastrophic forgetting can happen. They also show that when the parameter and function L distances are small, catastrophic forgetting does not happen. "
1822,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a deep generative Markov model for biological data. The model is based on the Dynamics Modeling Network (DynMoN), which is a generative model of the probability distribution. The authors propose to use dimensionality reduction methods to reduce the dimensionality of trajectories. The proposed model is evaluated on synthetic and real-world data."
1823,SP:f6cb7efaef82aff9849c8e157bfe5db5092a6271,"This paper proposes a deep generative Markov model for biological data. The model is based on the Dynamics Modeling Network (DynMoN), which is a generative model of the probability distribution. The authors propose to use dimensionality reduction methods to reduce the dimensionality of trajectories. The proposed model is evaluated on synthetic and real-world data."
1824,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,This paper proposes a graph Laplacian-based unsupervised learning method for image classification. The proposed method is based on spectral clustering theory and approximate linear connector network. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of image classification tasks.
1825,SP:4828e4160b70ea11e364b48db24cb68cdf86edfc,This paper proposes a graph Laplacian-based unsupervised learning method for image classification. The proposed method is based on spectral clustering theory and approximate linear connector network. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of image classification tasks.
1826,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning permutation-invariant representations for set models. The proposed method is based on the idea of permutation optimisation, which aims to learn a set representation that is permutation invariant to permutations and set transformations. The authors propose to use the Permutation-Optimisation module to optimize the permutation and set representations. The method is evaluated on image classification, number sorting, and visual question answering tasks. The results show that the proposed method outperforms existing methods."
1827,SP:d5f5f6a83f0290415ea94b3740a95360a8fa16e3,"This paper proposes a method for learning permutation-invariant representations for set models. The proposed method is based on the idea of permutation optimisation, which aims to learn a set representation that is permutation invariant to permutations and set transformations. The authors propose to use the Permutation-Optimisation module to optimize the permutation and set representations. The method is evaluated on image classification, number sorting, and visual question answering tasks. The results show that the proposed method outperforms existing methods."
1828,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper proposes a new deep learning method called Projective Subspace Networks (PSN) for learning non-linear embeddings in affine subspaces. The main idea of the method is to learn a projective subspace of the affine representation of the input space, which is then used to train an encoder-decoder network that maps the embedding to the subspace. The authors show that the proposed method outperforms existing methods on a variety of tasks."
1829,SP:cf74c553bae2b1194beaba4df1545d35e66aa5b3,"This paper proposes a new deep learning method called Projective Subspace Networks (PSN) for learning non-linear embeddings in affine subspaces. The main idea of the method is to learn a projective subspace of the affine representation of the input space, which is then used to train an encoder-decoder network that maps the embedding to the subspace. The authors show that the proposed method outperforms existing methods on a variety of tasks."
1830,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,This paper proposes a meta-learning method for distributed machine learning. The proposed method is based on the idea of using context parameters and shared parameters to learn a task-specific loss for each task. The method is evaluated on a variety of tasks and compared to a number of baselines.
1831,SP:d7544bc4a0ae3237daa207e789a522363fb5170d,This paper proposes a meta-learning method for distributed machine learning. The proposed method is based on the idea of using context parameters and shared parameters to learn a task-specific loss for each task. The method is evaluated on a variety of tasks and compared to a number of baselines.
1832,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"This paper studies the problem of privacy-preserving representations in machine learning. The authors propose a new privacy-aware representation learning framework, which is based on the natural information theoretic bounds on the utility-privacy trade-off. The main idea of the paper is to learn a representation that preserves the privacy of both the gender and the subject, and then use a sanitization process to sanitize the representation. The paper is well-written and easy to follow. The experimental results show that the proposed method can achieve state-of-the-art performance on face verification and emotion detection."
1833,SP:8a5e86b6770a3c08f861fbf682296dc3a6c02204,"This paper studies the problem of privacy-preserving representations in machine learning. The authors propose a new privacy-aware representation learning framework, which is based on the natural information theoretic bounds on the utility-privacy trade-off. The main idea of the paper is to learn a representation that preserves the privacy of both the gender and the subject, and then use a sanitization process to sanitize the representation. The paper is well-written and easy to follow. The experimental results show that the proposed method can achieve state-of-the-art performance on face verification and emotion detection."
1834,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation of GANs (PAGAN) to address the issue of training instability in GAN training. The proposed method is based on the observation that the discriminator of a GAN is sensitive to the task difficulty. To address this issue, the authors propose to augment the generator and discriminator with the same amount of parameters. Experiments on MNIST, Fashion-MNIST, and CELEBA demonstrate the effectiveness of the proposed method."
1835,SP:6b0e9a8f0c046a767dce8790489b3e90e12e2c46,"This paper proposes a progressive augmentation of GANs (PAGAN) to address the issue of training instability in GAN training. The proposed method is based on the observation that the discriminator of a GAN is sensitive to the task difficulty. To address this issue, the authors propose to augment the generator and discriminator with the same amount of parameters. Experiments on MNIST, Fashion-MNIST, and CELEBA demonstrate the effectiveness of the proposed method."
1836,SP:c210982ccdd134d4b293dbe144990398eefe1a86,This paper proposes a new method for learning the representations of V1 neurons in the mouse primary visual cortex (V1) using a rotation-equivariant convolutional neural network (RCNN). The proposed method is based on the idea that the rotation of the input image can be viewed as an energy model. The authors show that rotation-invariant RCNN can be used to learn the features of the V1. The experiments are conducted on two-photon images of the mouse V1 and show that the proposed method outperforms the RCNN.
1837,SP:c210982ccdd134d4b293dbe144990398eefe1a86,This paper proposes a new method for learning the representations of V1 neurons in the mouse primary visual cortex (V1) using a rotation-equivariant convolutional neural network (RCNN). The proposed method is based on the idea that the rotation of the input image can be viewed as an energy model. The authors show that rotation-invariant RCNN can be used to learn the features of the V1. The experiments are conducted on two-photon images of the mouse V1 and show that the proposed method outperforms the RCNN.
1838,SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper studies the problem of distributed training of deep neural networks in the presence of adversarial attacks. The authors propose a new algorithm for distributed training, called SIGNSGD, which is based on ADAM. In particular, the authors propose to use the sign gradients of the parameter server to estimate the gradient of the gradient vector of the server. The main contribution of this paper is to show that ADAM converges faster than full-precision, distributed SGD in the parameter regime. The paper also provides a theoretical analysis of the convergence rate of ADAM in both large and mini-batch settings."
1839,SP:f17090812ace9c83d418b17bf165649232c223e3,"This paper studies the problem of distributed training of deep neural networks in the presence of adversarial attacks. The authors propose a new algorithm for distributed training, called SIGNSGD, which is based on ADAM. In particular, the authors propose to use the sign gradients of the parameter server to estimate the gradient of the gradient vector of the server. The main contribution of this paper is to show that ADAM converges faster than full-precision, distributed SGD in the parameter regime. The paper also provides a theoretical analysis of the convergence rate of ADAM in both large and mini-batch settings."
1840,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a generative adversarial network (GAN) for predicting the history dependence of orders in a stock market. The main idea is to use conditional Wasserstein GANs to model the history dependency of orders. The authors propose to use a stochastic process to generate the order stream, and then use a conditional GAN to predict the history of the order. The proposed method is evaluated on both synthetic and real stock market data."
1841,SP:0ceece0754a1fe9c46a978bb2854932905685fa4,"This paper proposes a generative adversarial network (GAN) for predicting the history dependence of orders in a stock market. The main idea is to use conditional Wasserstein GANs to model the history dependency of orders. The authors propose to use a stochastic process to generate the order stream, and then use a conditional GAN to predict the history of the order. The proposed method is evaluated on both synthetic and real stock market data."
1842,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"This paper proposes an unbiased reward estimator-assisted robust reinforcement learning (POE) algorithm for learning policies that are robust to arbitrary errors in noisy environments. The main idea is to use the reward confusion matrix to estimate the observed rewards, and then use the estimated surrogate reward to train a policy that maximizes the expected rewards. The authors show that the proposed PPO algorithm converges to the optimal policy with high sample complexity. They also show that PPOE can be applied to Atari games."
1843,SP:ba66503753b3c57781b435c55c47fc9f69450e65,"This paper proposes an unbiased reward estimator-assisted robust reinforcement learning (POE) algorithm for learning policies that are robust to arbitrary errors in noisy environments. The main idea is to use the reward confusion matrix to estimate the observed rewards, and then use the estimated surrogate reward to train a policy that maximizes the expected rewards. The authors show that the proposed PPO algorithm converges to the optimal policy with high sample complexity. They also show that PPOE can be applied to Atari games."
1844,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the halting time of gradient descent in the weight space of a neural network. The authors consider the case where the width of the network is much larger than the number of steps required to reach a stationary point. They show that if the width is larger than a certain threshold, then gradient descent will converge to the stationary point with a power-law-like relationship between the average step size and the average weight space traversal length. They also provide a theoretical analysis of the effect of overparametrization of the weights."
1845,SP:0e62f75b81b696bf794932d0ceee60e9f665f1da,"This paper studies the halting time of gradient descent in the weight space of a neural network. The authors consider the case where the width of the network is much larger than the number of steps required to reach a stationary point. They show that if the width is larger than a certain threshold, then gradient descent will converge to the stationary point with a power-law-like relationship between the average step size and the average weight space traversal length. They also provide a theoretical analysis of the effect of overparametrization of the weights."
1846,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper proposes an online primal-dual framework for solving online optimization problems. The main idea is to use reinforcement learning (RL) to learn a policy that can be used to solve the problem in an online manner. In particular, the paper proposes to use a reinforcement learning algorithm that learns a policy to solve an online optimization problem in a primal dual manner. The algorithm is based on the idea that the optimal policy can be learned in a way that maximizes the mutual information between the learned policy and the optimal algorithm. The paper shows that the proposed algorithm can be applied to the online knapsack problem, the secretary problem, and the AdWords problem. "
1847,SP:40e210d36298e2eafd06d9dc45312ea4fd586ade,"This paper proposes an online primal-dual framework for solving online optimization problems. The main idea is to use reinforcement learning (RL) to learn a policy that can be used to solve the problem in an online manner. In particular, the paper proposes to use a reinforcement learning algorithm that learns a policy to solve an online optimization problem in a primal dual manner. The algorithm is based on the idea that the optimal policy can be learned in a way that maximizes the mutual information between the learned policy and the optimal algorithm. The paper shows that the proposed algorithm can be applied to the online knapsack problem, the secretary problem, and the AdWords problem. "
1848,SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper studies the effect of inductive biases in deep reinforcement learning (RL) algorithms on the performance of AlphaZero. The authors propose a new algorithm, AlphaZero, that combines domain knowledge, pretuned hyperparameters, and domain knowledge. They show that the proposed algorithm outperforms the state-of-the-art AlphaGo algorithm on a variety of games, including Chess, Shogi, and Atari. They also show that AlphaZero is able to learn better than AlphaGo on Atari and Shogi."
1849,SP:b99732087f5a929ab248acdcd7a943bce8671510,"This paper studies the effect of inductive biases in deep reinforcement learning (RL) algorithms on the performance of AlphaZero. The authors propose a new algorithm, AlphaZero, that combines domain knowledge, pretuned hyperparameters, and domain knowledge. They show that the proposed algorithm outperforms the state-of-the-art AlphaGo algorithm on a variety of games, including Chess, Shogi, and Atari. They also show that AlphaZero is able to learn better than AlphaGo on Atari and Shogi."
1850,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for navigation tasks. The proposed method is based on a combination of two components: a visual-textural co-grounding module and a progress monitor. The main idea is to use the visual grounding module to guide the agent to the next state, while the progress monitor is used to track the progress of the agent. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of benchmarks."
1851,SP:47b0c8a984480eb353b36fd877d9775213fb1a5f,"This paper proposes a self-monitoring agent for navigation tasks. The proposed method is based on a combination of two components: a visual-textural co-grounding module and a progress monitor. The main idea is to use the visual grounding module to guide the agent to the next state, while the progress monitor is used to track the progress of the agent. The authors show that the proposed method outperforms the state-of-the-art methods on a variety of benchmarks."
1852,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,"This paper proposes a neural program synthesis method that combines an encoder-decoder architecture with a decoder-style neural program synthesizer. The encoder encodes the input-output embedding and the decoder decodes the output of the encoder. The authors show that the proposed method can achieve state-of-the-art performance on Karel, FlashFill, and Execution-Guided Synthesis. The proposed method is evaluated on a variety of tasks, including execution-guided synthesis, synthesis ensemble, and synthesizer ensemble."
1853,SP:7e70c97e9b7b182e974b071c93baafef8b11cf90,"This paper proposes a neural program synthesis method that combines an encoder-decoder architecture with a decoder-style neural program synthesizer. The encoder encodes the input-output embedding and the decoder decodes the output of the encoder. The authors show that the proposed method can achieve state-of-the-art performance on Karel, FlashFill, and Execution-Guided Synthesis. The proposed method is evaluated on a variety of tasks, including execution-guided synthesis, synthesis ensemble, and synthesizer ensemble."
1854,SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the stability of batch normalization (BN) in the context of ordinary least squares (ODEs) problems. The authors consider the case where the weights of the OLS problem can be expressed as a function of the learning rate of the weights, and show that BN can be viewed as a variant of gradient descent in the setting of OLS. They show that the convergence of BN is guaranteed by the scaling law and convergence of the gradient descent with respect to the number of weights. They also show that gradient descent converges to a stationary point in the case of arbitrary learning rates."
1855,SP:dc7dfc1eec473800580dba309446871122be6040,"This paper studies the stability of batch normalization (BN) in the context of ordinary least squares (ODEs) problems. The authors consider the case where the weights of the OLS problem can be expressed as a function of the learning rate of the weights, and show that BN can be viewed as a variant of gradient descent in the setting of OLS. They show that the convergence of BN is guaranteed by the scaling law and convergence of the gradient descent with respect to the number of weights. They also show that gradient descent converges to a stationary point in the case of arbitrary learning rates."
1856,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,This paper proposes a variational framework for data noising in Bayesian recurrent neural networks. The main idea is to use a mixture of Gaussians to approximate the unigram distribution of the input and output of the model. The authors propose to use variational smoothing and element-wise smoothing for the variational distribution. Experiments show that the proposed method outperforms existing methods on several benchmark language modeling datasets.
1857,SP:9984d73a1fcfce932cfcafb4d200f70b07723bf3,This paper proposes a variational framework for data noising in Bayesian recurrent neural networks. The main idea is to use a mixture of Gaussians to approximate the unigram distribution of the input and output of the model. The authors propose to use variational smoothing and element-wise smoothing for the variational distribution. Experiments show that the proposed method outperforms existing methods on several benchmark language modeling datasets.
1858,SP:f4a914d3df1a5a21a7365ba78279420f39210884,This paper proposes a method for extracting saliency maps for image classification. The method is based on a classifier-agnostic saliency map extraction method. The proposed method is evaluated on the ImageNet dataset and compared with a few weakly supervised methods. The results show that the proposed method outperforms the weakly-supervised methods.
1859,SP:f4a914d3df1a5a21a7365ba78279420f39210884,This paper proposes a method for extracting saliency maps for image classification. The method is based on a classifier-agnostic saliency map extraction method. The proposed method is evaluated on the ImageNet dataset and compared with a few weakly supervised methods. The results show that the proposed method outperforms the weakly-supervised methods.
1860,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a method for fine-tuning a deep neural network using a knowledge flow method. The method is based on the idea that a teacher network can be used to guide the learning of a student network. The idea is that the student network is trained to be similar to the teacher network, and the teacher model is used to train the student model. The authors show that the proposed method is able to improve the performance of both supervised and reinforcement learning tasks."
1861,SP:df038354c6a7638116a98d150aa4a8f5f2b0a2da,"This paper proposes a method for fine-tuning a deep neural network using a knowledge flow method. The method is based on the idea that a teacher network can be used to guide the learning of a student network. The idea is that the student network is trained to be similar to the teacher network, and the teacher model is used to train the student model. The authors show that the proposed method is able to improve the performance of both supervised and reinforcement learning tasks."
1862,SP:a72072879f7c61270d952f06d9ce995e8150632c,"This paper proposes a method for learning a compact dynamical model with soft-clustering. The proposed method is based on stochastic calculus and is motivated by the information theory inspired approach of information theory. The main idea is to learn a model that compresses the state variables of the dynamical system, and then use the learned model to predict the future state of the system. The authors show that the proposed method can achieve better prediction accuracy than the state-of-the-art methods."
1863,SP:a72072879f7c61270d952f06d9ce995e8150632c,"This paper proposes a method for learning a compact dynamical model with soft-clustering. The proposed method is based on stochastic calculus and is motivated by the information theory inspired approach of information theory. The main idea is to learn a model that compresses the state variables of the dynamical system, and then use the learned model to predict the future state of the system. The authors show that the proposed method can achieve better prediction accuracy than the state-of-the-art methods."
1864,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a variational autoencoder (VAE) model for learning a single neural probabilistic model with stochastic variational Bayes. The proposed VAE model is based on the idea of variational variational inference, which is an extension of the VAE framework. The main difference is that instead of using a single model, the authors propose to use two models, one for each feature and the other for the latent variables. The authors show that the proposed model outperforms the state-of-the-art VAE models on synthetic data and image inpainting tasks."
1865,SP:2b03b7ea1264c2671d29e8fa5f3a828412ea7996,"This paper proposes a variational autoencoder (VAE) model for learning a single neural probabilistic model with stochastic variational Bayes. The proposed VAE model is based on the idea of variational variational inference, which is an extension of the VAE framework. The main difference is that instead of using a single model, the authors propose to use two models, one for each feature and the other for the latent variables. The authors show that the proposed model outperforms the state-of-the-art VAE models on synthetic data and image inpainting tasks."
1866,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,This paper proposes a method to reduce the memory footprint of neural network models. The main idea is to use lower-precision approximations of the activations of intermediate layer activations for back-propagation during the forward pass of the network. The authors propose to use 32-bit floating-point activations to approximate the gradients of gradients during the backward pass. The proposed method is evaluated on CIFAR-10 and ImageNet.
1867,SP:f46f0cb43274fb20cba91ef7318305f668bc6928,This paper proposes a method to reduce the memory footprint of neural network models. The main idea is to use lower-precision approximations of the activations of intermediate layer activations for back-propagation during the forward pass of the network. The authors propose to use 32-bit floating-point activations to approximate the gradients of gradients during the backward pass. The proposed method is evaluated on CIFAR-10 and ImageNet.
1868,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,This paper proposes a generative adversarial network (GAN) for face completion. The main idea is to use conditional vectors to represent the controllable attributes of the filled-in fragments. The proposed method is evaluated on a variety of face completion tasks and compared to several state-of-the-art methods.
1869,SP:6ad33c6fbdee78c13d9190601637e07d20fe024f,This paper proposes a generative adversarial network (GAN) for face completion. The main idea is to use conditional vectors to represent the controllable attributes of the filled-in fragments. The proposed method is evaluated on a variety of face completion tasks and compared to several state-of-the-art methods.
1870,SP:a300122021e93d695af85e158f2b402d21525bc8,This paper studies the effect of partial sum accumulators on the quality of convergence of deep learning models. The authors propose a statistical approach to study the impact of the accumulation precision of partial sums on the convergence rate of the model. The main contribution of this paper is to provide a theoretical analysis of the effect on the performance of partial accumulators. The paper is well-written and easy to follow.
1871,SP:a300122021e93d695af85e158f2b402d21525bc8,This paper studies the effect of partial sum accumulators on the quality of convergence of deep learning models. The authors propose a statistical approach to study the impact of the accumulation precision of partial sums on the convergence rate of the model. The main contribution of this paper is to provide a theoretical analysis of the effect on the performance of partial accumulators. The paper is well-written and easy to follow.
1872,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,This paper studies the asymptotic weight matrix alignment in deep linear networks with linearly separable data. The authors show that gradient flow and gradient descent converge to a linear function when the weight matrices of the network are rank-1 matrices. They also show that the gradient flow converges to the maximum margin solution of the logistic loss with decreasing step size. The paper also shows that gradient descent converges faster than gradient flow with increasing step size when the network is linearly separated.
1873,SP:3a1655a2efdf0246f459b6f82a2948aafc7438a9,This paper studies the asymptotic weight matrix alignment in deep linear networks with linearly separable data. The authors show that gradient flow and gradient descent converge to a linear function when the weight matrices of the network are rank-1 matrices. They also show that the gradient flow converges to the maximum margin solution of the logistic loss with decreasing step size. The paper also shows that gradient descent converges faster than gradient flow with increasing step size when the network is linearly separated.
1874,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper proposes a new hredGAN-based model for multi-turn dialogue. The proposed model is based on a combination of two existing models, namely the persona-based HRED generator (PHRED) and the conditional discriminator (CADE). The main difference between the two models is that the PHRED model is trained with a discriminator, while the CADE model uses an adversarial discriminator. Experiments on the Big Bang Theory and Ubuntu dataset show that the proposed model outperforms the other two models."
1875,SP:868dd531fe7886b0260295d25b75cc6d6d28f12d,"This paper proposes a new hredGAN-based model for multi-turn dialogue. The proposed model is based on a combination of two existing models, namely the persona-based HRED generator (PHRED) and the conditional discriminator (CADE). The main difference between the two models is that the PHRED model is trained with a discriminator, while the CADE model uses an adversarial discriminator. Experiments on the Big Bang Theory and Ubuntu dataset show that the proposed model outperforms the other two models."
1876,SP:017b66d6262427cca551ef50006784498ffc741d,This paper proposes a new collaborative image-drawing game called CoDraw. The game is based on the idea that the goal is to draw a clip art object from a set of clip art objects. The paper proposes to train a neural network on the CoDraw dataset and evaluate the performance of the proposed model on the game. The proposed method is evaluated on a small number of tasks and compared to a number of baselines.
1877,SP:017b66d6262427cca551ef50006784498ffc741d,This paper proposes a new collaborative image-drawing game called CoDraw. The game is based on the idea that the goal is to draw a clip art object from a set of clip art objects. The paper proposes to train a neural network on the CoDraw dataset and evaluate the performance of the proposed model on the game. The proposed method is evaluated on a small number of tasks and compared to a number of baselines.
1878,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a novel approach for learning neural random fields (NRFs) for continuous data. The authors propose to use an auxiliary generator to generate samples from the input space of the original NRF. The auxiliary generator is based on the idea that the gradient of the model can be used to estimate the divergence between the input and the output of the generator. The proposed approach is evaluated on both unsupervised and supervised image generation and semi-supervised classification tasks on CIFAR-10, SVHN, and MNIST."
1879,SP:d5126851b9e75b49522d953ee2b253e3e6c836ba,"This paper proposes a novel approach for learning neural random fields (NRFs) for continuous data. The authors propose to use an auxiliary generator to generate samples from the input space of the original NRF. The auxiliary generator is based on the idea that the gradient of the model can be used to estimate the divergence between the input and the output of the generator. The proposed approach is evaluated on both unsupervised and supervised image generation and semi-supervised classification tasks on CIFAR-10, SVHN, and MNIST."
1880,SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper studies the problem of adversarial training of graph convolutional neural networks (GNNs). The authors propose a meta-gradient method to attack GNNs. The proposed method is based on meta-gradients, which can be viewed as a bilevel optimization problem. The authors show that the proposed method can improve the robustness of GNN models to small perturbations."
1881,SP:0841febf2e95da495b41e12ded491ba5e9633538,"This paper studies the problem of adversarial training of graph convolutional neural networks (GNNs). The authors propose a meta-gradient method to attack GNNs. The proposed method is based on meta-gradients, which can be viewed as a bilevel optimization problem. The authors show that the proposed method can improve the robustness of GNN models to small perturbations."
1882,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper studies the problem of optimizing the cost function of sliced-Wasserstein autoencoder (SWAE) and its variant, Cramer-Wold AutoEncoder (CWAE). In particular, the authors propose to use the maximum mean discrepancy based distance function (MDSF) to optimize the WAE-MMD and CWAE cost functions. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of CWAE and SWAE. The authors show that CWAE can converge faster than SWAE and WAE with the same number of samples. "
1883,SP:beb54248806f7a68beb60167c3dbbd45b34dad83,"This paper studies the problem of optimizing the cost function of sliced-Wasserstein autoencoder (SWAE) and its variant, Cramer-Wold AutoEncoder (CWAE). In particular, the authors propose to use the maximum mean discrepancy based distance function (MDSF) to optimize the WAE-MMD and CWAE cost functions. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of CWAE and SWAE. The authors show that CWAE can converge faster than SWAE and WAE with the same number of samples. "
1884,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper proposes a new method for few-shot learning of coarse-to-fine classification (MCFS) problems. The proposed method, MahiNet, consists of a convolutional neural network (CNN) and a multi-layer perceptron (MLP) with a memory-augmented attention module and a KNN classifier. The CNN is trained with a linear classifier and the MLP is trained using a linear discriminator. The experiments show that the proposed method outperforms the state-of-the-art methods on several MCFS classification tasks."
1885,SP:57538c4cac6a4510a0c79e6da3deffae4d6c3b91,"This paper proposes a new method for few-shot learning of coarse-to-fine classification (MCFS) problems. The proposed method, MahiNet, consists of a convolutional neural network (CNN) and a multi-layer perceptron (MLP) with a memory-augmented attention module and a KNN classifier. The CNN is trained with a linear classifier and the MLP is trained using a linear discriminator. The experiments show that the proposed method outperforms the state-of-the-art methods on several MCFS classification tasks."
1886,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes a neural speed reading model, called Structural-Jump-LSTM, which is a combination of LSTM and agents. The main idea is to combine LSTMs and agents to reduce the number of floating-point operations in the inference time of the model. The authors show that the proposed model can achieve better performance than the vanilla vanilla model on a few standard benchmarks."
1887,SP:ae9b6f7f2bd29ad1d24c4acbe1ecd345fcd6a081,"This paper proposes a neural speed reading model, called Structural-Jump-LSTM, which is a combination of LSTM and agents. The main idea is to combine LSTMs and agents to reduce the number of floating-point operations in the inference time of the model. The authors show that the proposed model can achieve better performance than the vanilla vanilla model on a few standard benchmarks."
1888,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a novel method for adversarial defense of deep convolutional neural networks against small adversarial perturbations. The proposed method is based on the Mahalanobis distance function, which is used to estimate the distance between the features of the network and the decision boundary of the perturbation. The authors show that the proposed method can be applied to both clean and adversarially perturbed data. The method is evaluated on MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14."
1889,SP:9be782b532e64c6aad140531a17fbba1dd3342cd,"This paper proposes a novel method for adversarial defense of deep convolutional neural networks against small adversarial perturbations. The proposed method is based on the Mahalanobis distance function, which is used to estimate the distance between the features of the network and the decision boundary of the perturbation. The authors show that the proposed method can be applied to both clean and adversarially perturbed data. The method is evaluated on MNIST, ISBI ISIC skin lesion, and NIH ChestX-ray14."
1890,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a method for learning a policy that is on-policy temporally consistent. The proposed method, called NADPEx, is based on a stochastic policy model with a global random variable and a conditional distribution. The authors show that the proposed method can be applied to a variety of tasks with sparse rewards and long-term information. The main contribution of this paper is to propose a method that can be used to learn a policy with a temporal consistency policy. The method is evaluated on the Mujoco continuous control task."
1891,SP:b08dc82d5098474ddd68ab13003013ee6e7ba989,"This paper proposes a method for learning a policy that is on-policy temporally consistent. The proposed method, called NADPEx, is based on a stochastic policy model with a global random variable and a conditional distribution. The authors show that the proposed method can be applied to a variety of tasks with sparse rewards and long-term information. The main contribution of this paper is to propose a method that can be used to learn a policy with a temporal consistency policy. The method is evaluated on the Mujoco continuous control task."
1892,SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes a new metric for evaluating the quality of a neural network. The proposed metric is based on the nonlinearity coefficient (NLC), which measures the difference between the gradients of the network and the test error. The authors show that the NLC can be used as a way to evaluate the performance of neural networks. The NLC is shown to be a better metric than the standard gradient-based metric."
1893,SP:304930c105cf036ab48e9653926a5f61879dfea6,"This paper proposes a new metric for evaluating the quality of a neural network. The proposed metric is based on the nonlinearity coefficient (NLC), which measures the difference between the gradients of the network and the test error. The authors show that the NLC can be used as a way to evaluate the performance of neural networks. The NLC is shown to be a better metric than the standard gradient-based metric."
1894,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies feature-wise bias amplification, which is an important problem in machine learning. The authors propose two algorithms to reduce bias amplification in linear models. First, they propose a feature selection algorithm to select features that are moderately predictive of the model’s performance. Second, the authors propose an algorithm to reduce the bias of linear models with bias amplification. Experiments on synthetic and real-world datasets show that the proposed algorithms are effective in reducing bias."
1895,SP:17d8dc884e15131636a8c2490085ce42c05433c1,"This paper studies feature-wise bias amplification, which is an important problem in machine learning. The authors propose two algorithms to reduce bias amplification in linear models. First, they propose a feature selection algorithm to select features that are moderately predictive of the model’s performance. Second, the authors propose an algorithm to reduce the bias of linear models with bias amplification. Experiments on synthetic and real-world datasets show that the proposed algorithms are effective in reducing bias."
1896,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the generalization properties of neural networks from a margin-based perspective. The authors show that the maximum normalized margin of the global minimizer of the weakly-regularized cross-entropy loss can be used to derive generalization error bounds for infinite-width neural networks with two-layer networks and multi-layer feedforward relu networks. In addition, the authors provide a perturbed gradient flow to analyze the global optimizer of infinite-size networks."
1897,SP:2b84207c0015dba126d4ef4a89ef9cc29656f2f8,"This paper studies the generalization properties of neural networks from a margin-based perspective. The authors show that the maximum normalized margin of the global minimizer of the weakly-regularized cross-entropy loss can be used to derive generalization error bounds for infinite-width neural networks with two-layer networks and multi-layer feedforward relu networks. In addition, the authors provide a perturbed gradient flow to analyze the global optimizer of infinite-size networks."
1898,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"This paper proposes a method for generating captions for image captions using GANs. The proposed method is based on the object pathway, which consists of two parts: (1) an object pathway that maps an image to a set of bounding boxes, and (2) an image layout that maps the image to an image background. The object pathway is used to train the discriminator, and the image layout is used as the input to the generator. Experiments are conducted on CLEVR, Multi-MNIST, and MSCOCO datasets, and show the effectiveness of the proposed method."
1899,SP:91459c66bb597751ffce8410e283ce3f094bdd5f,"This paper proposes a method for generating captions for image captions using GANs. The proposed method is based on the object pathway, which consists of two parts: (1) an object pathway that maps an image to a set of bounding boxes, and (2) an image layout that maps the image to an image background. The object pathway is used to train the discriminator, and the image layout is used as the input to the generator. Experiments are conducted on CLEVR, Multi-MNIST, and MSCOCO datasets, and show the effectiveness of the proposed method."
1900,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a local model for policy learning in complex systems. The main idea is to learn a local representation of the dynamics of the system, which can then be used for policy improvement. The proposed method is evaluated on the Sawyer robotic arm manipulation task."
1901,SP:fbfe2c90a70a6adf39fa4d4a3c28f6b5adbc6c06,"This paper proposes a local model for policy learning in complex systems. The main idea is to learn a local representation of the dynamics of the system, which can then be used for policy improvement. The proposed method is evaluated on the Sawyer robotic arm manipulation task."
1902,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"This paper proposes a method for counterfactual evaluation of off-policy experience in reinforcement learning. The proposed method, called Counterfactual Guided Policy Search (CF-GPS), is based on the idea of Importance Sampling (importance sampling), which is an extension of the importance sampling method proposed in [1]. Importance sampling is a technique that is used in the context of model-based policy evaluation and search. The main idea of the paper is to use a causal model to model the causal relationship between the policy and the environment, and then use the causal model as a proxy for policy evaluation. The authors show that the proposed method is able to outperform the state-of-the-art methods on a variety of tasks."
1903,SP:9a4c7d9df6685347e75e0ae72928225b7622a73c,"This paper proposes a method for counterfactual evaluation of off-policy experience in reinforcement learning. The proposed method, called Counterfactual Guided Policy Search (CF-GPS), is based on the idea of Importance Sampling (importance sampling), which is an extension of the importance sampling method proposed in [1]. Importance sampling is a technique that is used in the context of model-based policy evaluation and search. The main idea of the paper is to use a causal model to model the causal relationship between the policy and the environment, and then use the causal model as a proxy for policy evaluation. The authors show that the proposed method is able to outperform the state-of-the-art methods on a variety of tasks."
1904,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,This paper proposes a new metric for evaluating the robustness of a neural network to adversarial attacks. The metric is based on the flat local minima of the loss surface of the parameter space and the input space. The paper also proposes a method to measure the intrinsic adversarial robustness to the adversarial attack. The method is evaluated on a synthetic dataset and on a real-world dataset.
1905,SP:9371d08e2b3a821e40cc9d4757c22f6cdb731b6a,This paper proposes a new metric for evaluating the robustness of a neural network to adversarial attacks. The metric is based on the flat local minima of the loss surface of the parameter space and the input space. The paper also proposes a method to measure the intrinsic adversarial robustness to the adversarial attack. The method is evaluated on a synthetic dataset and on a real-world dataset.
1906,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training method for energy saving. The authors propose a weighted sparse projection and input masking method to reduce the energy consumption of DNNs during training. The proposed method is based on the idea that the energy budget of a DNN is a function of the number of parameters of the network, and the authors propose an approximate algorithm to solve the optimization problem. The paper also provides a theoretical analysis of the proposed method."
1907,SP:6f94f59bc936a11d95ded7309dc2458fee6d2595,"This paper proposes an end-to-end DNN training method for energy saving. The authors propose a weighted sparse projection and input masking method to reduce the energy consumption of DNNs during training. The proposed method is based on the idea that the energy budget of a DNN is a function of the number of parameters of the network, and the authors propose an approximate algorithm to solve the optimization problem. The paper also provides a theoretical analysis of the proposed method."
1908,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper proposes a Bayesian policy optimization algorithm for Bayesian Policy Optimization (BPO). The main idea is to use a policy network to estimate the belief distribution of the observed state, and then use a universal policy to maximize the expected long-term reward. The proposed algorithm is evaluated on a variety of benchmarks and compared to a number of state-of-the-art algorithms."
1909,SP:7f07f3fa8a10b48bb380a7c84bc012ce3541122b,"This paper proposes a Bayesian policy optimization algorithm for Bayesian Policy Optimization (BPO). The main idea is to use a policy network to estimate the belief distribution of the observed state, and then use a universal policy to maximize the expected long-term reward. The proposed algorithm is evaluated on a variety of benchmarks and compared to a number of state-of-the-art algorithms."
1910,SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a novel unsupervised representation learning method for entities in text. The proposed method is based on the Wasserstein distance between entities and the optimal transport map of the histogram (or distribution) of the entities. The authors show that the proposed method can be applied to both supervised (i.e., sentence similarity) as well as un-supervised (e.g., word entailment detection) tasks. The main contribution of the paper is to propose a unified framework for learning the representation of entities and their distributions. The paper is well-written and easy to follow. The experimental results demonstrate the effectiveness of the proposed model."
1911,SP:3823faee83bc07a989934af5495dafd003c27921,"This paper proposes a novel unsupervised representation learning method for entities in text. The proposed method is based on the Wasserstein distance between entities and the optimal transport map of the histogram (or distribution) of the entities. The authors show that the proposed method can be applied to both supervised (i.e., sentence similarity) as well as un-supervised (e.g., word entailment detection) tasks. The main contribution of the paper is to propose a unified framework for learning the representation of entities and their distributions. The paper is well-written and easy to follow. The experimental results demonstrate the effectiveness of the proposed model."
1912,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper proposes a model-based reinforcement learning method for MuJoCo environments. The proposed method consists of a dynamics model and a planner. The dynamics model is used to estimate the state of the environment, and the planner is trained to predict the next state. The authors show that the proposed method outperforms the state-of-the-art model-free RL methods on MuJoco tasks."
1913,SP:9ce5b80147ea2c7d0711ec98e31f4bbb5eac534e,"This paper proposes a model-based reinforcement learning method for MuJoCo environments. The proposed method consists of a dynamics model and a planner. The dynamics model is used to estimate the state of the environment, and the planner is trained to predict the next state. The authors show that the proposed method outperforms the state-of-the-art model-free RL methods on MuJoco tasks."
1914,SP:da14205470819495a3aad69d64de4033749d4d3e,"This paper proposes a new quantization method for neural network quantization. The proposed method is based on the precision highway method, which aims to reduce the accumulated quantization error of convolutional and recurrent neural networks. The main contribution of this paper is to propose a new method for quantization of neural networks with 2-bit and 3-bit precision. The method is evaluated on the ResNet-50 and the LSTM on the CIFAR-10 dataset and compared to the state-of-the-art quantization methods."
1915,SP:da14205470819495a3aad69d64de4033749d4d3e,"This paper proposes a new quantization method for neural network quantization. The proposed method is based on the precision highway method, which aims to reduce the accumulated quantization error of convolutional and recurrent neural networks. The main contribution of this paper is to propose a new method for quantization of neural networks with 2-bit and 3-bit precision. The method is evaluated on the ResNet-50 and the LSTM on the CIFAR-10 dataset and compared to the state-of-the-art quantization methods."
1916,SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a generative adversarial network (GAN) based method for image restoration. The proposed method is motivated by the observation that the posterior probability of the latent variables of a degraded image is highly correlated with the posterior density of the original image. The authors propose to use a GAN to estimate the density of latent variables in the latent space of the degraded image, which is then used to train a density estimation model. The paper shows that the proposed method can be applied to image restoration problems such as denoising, deblurring and super-resolution."
1917,SP:0355b54430b39b52df94014d78289dd6e1e81795,"This paper proposes a generative adversarial network (GAN) based method for image restoration. The proposed method is motivated by the observation that the posterior probability of the latent variables of a degraded image is highly correlated with the posterior density of the original image. The authors propose to use a GAN to estimate the density of latent variables in the latent space of the degraded image, which is then used to train a density estimation model. The paper shows that the proposed method can be applied to image restoration problems such as denoising, deblurring and super-resolution."
1918,SP:2feef921a0563d52fde1c074da754f73e6cabef8,This paper proposes a method for knowledge distillation based on convolutional neural networks (CNNs). The main idea is to add a conv-layer to the student-net layer of the teacher network to reduce the computation cost of the conv-net. The authors show that the proposed method can be used to train a large teacher-student network and a small student network simultaneously. The proposed method is evaluated on a variety of datasets and compared to a number of baselines.
1919,SP:2feef921a0563d52fde1c074da754f73e6cabef8,This paper proposes a method for knowledge distillation based on convolutional neural networks (CNNs). The main idea is to add a conv-layer to the student-net layer of the teacher network to reduce the computation cost of the conv-net. The authors show that the proposed method can be used to train a large teacher-student network and a small student network simultaneously. The proposed method is evaluated on a variety of datasets and compared to a number of baselines.
1920,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper studies the problem of task transfer learning. The authors propose a new metric called H-score, which is based on the asymptotic error probability of the decision function of the source task and the transferred feature of the target task. The proposed metric is evaluated on synthetic and real image data."
1921,SP:ca491b166bd8bf1a7c71657471a2f58b7fd36609,"This paper studies the problem of task transfer learning. The authors propose a new metric called H-score, which is based on the asymptotic error probability of the decision function of the source task and the transferred feature of the target task. The proposed metric is evaluated on synthetic and real image data."
1922,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes a method for learning structural representations for neural machine translation (NMT) models. The main idea is to learn a sequence of discrete codes for each word in a sentence, which are then used to train a model to predict the next word in the sentence. The method is evaluated on a variety of tasks, including sentence generation, sentence structure prediction, and sentence generation. The results show that the proposed method is able to generate sentences that are more diverse than the baseline models."
1923,SP:c6884b04001bd0d43aa47e2d72ebbe2bbc89ab3d,"This paper proposes a method for learning structural representations for neural machine translation (NMT) models. The main idea is to learn a sequence of discrete codes for each word in a sentence, which are then used to train a model to predict the next word in the sentence. The method is evaluated on a variety of tasks, including sentence generation, sentence structure prediction, and sentence generation. The results show that the proposed method is able to generate sentences that are more diverse than the baseline models."
1924,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"This paper proposes a new loss function for GANs. The proposed loss function is based on a relativistic discriminator, which can be viewed as a combination of the original GAN loss function and the gradient penalty. The authors show that this loss function can be applied to both RGANs and RaGANs. They also show that the proposed loss can be used to improve the performance of WGAN-GP and SGAN."
1925,SP:51810c5f8d40d9ec40469349f1612bf2eefe9aad,"This paper proposes a new loss function for GANs. The proposed loss function is based on a relativistic discriminator, which can be viewed as a combination of the original GAN loss function and the gradient penalty. The authors show that this loss function can be applied to both RGANs and RaGANs. They also show that the proposed loss can be used to improve the performance of WGAN-GP and SGAN."
1926,SP:8df1599919dcb3329553e75ffb19059f192542ea,This paper studies the problem of continual learning in the presence of catastrophic forgetting. The authors propose a new method for continual learning based on parameter generation. The main idea is to generate new parameters for the model and use them to train the model. The method is evaluated on a variety of tasks and compared to other continual learning methods.
1927,SP:8df1599919dcb3329553e75ffb19059f192542ea,This paper studies the problem of continual learning in the presence of catastrophic forgetting. The authors propose a new method for continual learning based on parameter generation. The main idea is to generate new parameters for the model and use them to train the model. The method is evaluated on a variety of tasks and compared to other continual learning methods.
1928,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"This paper proposes a new approach for multi-agent reinforcement learning based on Relational Forward Models (RFM). The authors argue that RFM can be used to learn interpretable intermediate representations of discrete entities, which can then be used as intermediate representations for learning agents. The authors show that the proposed approach can be applied to a variety of tasks, including reinforcement learning, imitation learning, and imitation learning. The proposed approach is evaluated on a number of simulated and real-world environments."
1929,SP:1342b6e11d1ccf04ee95b63d8b7a88b184dee43e,"This paper proposes a new approach for multi-agent reinforcement learning based on Relational Forward Models (RFM). The authors argue that RFM can be used to learn interpretable intermediate representations of discrete entities, which can then be used as intermediate representations for learning agents. The authors show that the proposed approach can be applied to a variety of tasks, including reinforcement learning, imitation learning, and imitation learning. The proposed approach is evaluated on a number of simulated and real-world environments."
1930,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a method for inverse reinforcement learning (IRL), where the goal is to learn a reward function that is more expressive than the oracle reward function. The main idea is to use a prior to learn the reward function, and then use the prior to train the policy. The method is evaluated on a variety of tasks, and compared to a number of baselines."
1931,SP:f2f01c7c4fb68c25d6e5ac56cbf79615ed1ee9ee,"This paper proposes a method for inverse reinforcement learning (IRL), where the goal is to learn a reward function that is more expressive than the oracle reward function. The main idea is to use a prior to learn the reward function, and then use the prior to train the policy. The method is evaluated on a variety of tasks, and compared to a number of baselines."
1932,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,This paper proposes a meta-learning approach for few-shot learning. The proposed method is based on the ML-PIP framework. The main idea of the paper is to use a multi-task amortized amortization network to learn the second derivatives of the task-specific parameters of the model. The authors show that the proposed method can be applied to a variety of tasks. Experiments are conducted on ShapeNet and CIFAR-10.
1933,SP:4c2f45c7fd0cac662a33be602985cf360b45fe4d,This paper proposes a meta-learning approach for few-shot learning. The proposed method is based on the ML-PIP framework. The main idea of the paper is to use a multi-task amortized amortization network to learn the second derivatives of the task-specific parameters of the model. The authors show that the proposed method can be applied to a variety of tasks. Experiments are conducted on ShapeNet and CIFAR-10.
1934,SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper proposes a novel CRF-based method for image classification. The proposed method is based on the CRF pairwise potential matrix (CRF), which is a linear-chain CRF with a nonlinear objective function. The authors show that the proposed CRF can be used to approximate the local likelihood of an image. The CRF is trained with batch normalization, and the proposed method outperforms existing CRF methods."
1935,SP:44e0f63ffee15796ba6135463134084bb370627b,"This paper proposes a novel CRF-based method for image classification. The proposed method is based on the CRF pairwise potential matrix (CRF), which is a linear-chain CRF with a nonlinear objective function. The authors show that the proposed CRF can be used to approximate the local likelihood of an image. The CRF is trained with batch normalization, and the proposed method outperforms existing CRF methods."
1936,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a generative adversarial network (GAN) architecture for audio synthesis. The proposed architecture is based on WaveNet, which is an autoregressive model with a global latent conditioning and parallel sampling. The authors show that the proposed GAN can achieve high-fidelity and locally-coherent audio waveforms. The experimental results on the NSynth dataset demonstrate the effectiveness of the proposed architecture."
1937,SP:18be2cb182761b64fa232c1b7d1899882e5bcf15,"This paper proposes a generative adversarial network (GAN) architecture for audio synthesis. The proposed architecture is based on WaveNet, which is an autoregressive model with a global latent conditioning and parallel sampling. The authors show that the proposed GAN can achieve high-fidelity and locally-coherent audio waveforms. The experimental results on the NSynth dataset demonstrate the effectiveness of the proposed architecture."
1938,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper studies the problem of verifying piecewise linear neural networks. The authors propose a new method for verifying the robustness of piecewise-linear neural networks against adversarial attacks. The main contribution of the paper is a novel formulation of the verification problem, which is based on a mixed integer program. The proposed method is evaluated on MNIST and CIFAR-10 datasets and compared to the state-of-the-art methods. The results show that the proposed method can find adversarial perturbations that are bounded by a bounded l-norm."
1939,SP:0c0f078c208600f541a76ecaae49cf9a98588736,"This paper studies the problem of verifying piecewise linear neural networks. The authors propose a new method for verifying the robustness of piecewise-linear neural networks against adversarial attacks. The main contribution of the paper is a novel formulation of the verification problem, which is based on a mixed integer program. The proposed method is evaluated on MNIST and CIFAR-10 datasets and compared to the state-of-the-art methods. The results show that the proposed method can find adversarial perturbations that are bounded by a bounded l-norm."
1940,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper studies the problem of learning a policy that can adaptively adapts to new tasks. The authors propose to use a KL-regularized expected reward objective, which is a regularized version of the KL-divergence objective, as a regularizer to encourage the learner to learn reusable behaviours. They show that this regularizer can be used in combination with a variational autoencoder (VAE) to learn a new policy that is more flexible than the default policy. They also show that the proposed method can be combined with information bottleneck approaches and variational EM algorithms."
1941,SP:dc48dbfb8f4f25d3ceb7be607e8f2e0bc8f99f14,"This paper studies the problem of learning a policy that can adaptively adapts to new tasks. The authors propose to use a KL-regularized expected reward objective, which is a regularized version of the KL-divergence objective, as a regularizer to encourage the learner to learn reusable behaviours. They show that this regularizer can be used in combination with a variational autoencoder (VAE) to learn a new policy that is more flexible than the default policy. They also show that the proposed method can be combined with information bottleneck approaches and variational EM algorithms."
1942,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes a method for sequential instruction understanding for conversational machine comprehension. The proposed method is based on the idea of FLOWQA, which is a combination of CoQA and QuAC. The main difference between the two methods is that FLOW uses an alternating parallel processing structure, while QuAC uses a single-turn model. The method is evaluated on a variety of tasks, including SCONE, QuAC, and Co-QA. The results show that the proposed method outperforms the state of the art."
1943,SP:08a6a48b05e2c00d77a73413cbba52cda08e184c,"This paper proposes a method for sequential instruction understanding for conversational machine comprehension. The proposed method is based on the idea of FLOWQA, which is a combination of CoQA and QuAC. The main difference between the two methods is that FLOW uses an alternating parallel processing structure, while QuAC uses a single-turn model. The method is evaluated on a variety of tasks, including SCONE, QuAC, and Co-QA. The results show that the proposed method outperforms the state of the art."
1944,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,"This paper proposes a generative model for generating static snapshots of code. The model is based on the attentional and pointer network components of a neural network, and is trained on a large-scale dataset with fine-grained edits. The authors show that the model is able to generalize better than existing models on the small-scale datasets."
1945,SP:fbb7bb8b4f75715f139c702750b28e7e87aa0e1f,"This paper proposes a generative model for generating static snapshots of code. The model is based on the attentional and pointer network components of a neural network, and is trained on a large-scale dataset with fine-grained edits. The authors show that the model is able to generalize better than existing models on the small-scale datasets."
1946,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning method for multi-class classification. The proposed method is based on the idea of pairwise similarity prediction, which is a well-studied and well-motivated problem. The authors propose to use a probabilistic graphical model to model the similarity between two classes, and then use a binary classifier to learn a submodule for each class. The main contribution of this paper is to propose a new loss function for the meta-classifier, which aims to improve the performance of the model. The paper is well-written and easy to follow."
1947,SP:dbb06f953788696f65013765f0a4e6967444fa0f,"This paper proposes a meta-learning method for multi-class classification. The proposed method is based on the idea of pairwise similarity prediction, which is a well-studied and well-motivated problem. The authors propose to use a probabilistic graphical model to model the similarity between two classes, and then use a binary classifier to learn a submodule for each class. The main contribution of this paper is to propose a new loss function for the meta-classifier, which aims to improve the performance of the model. The paper is well-written and easy to follow."
1948,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper studies the problem of visual question answering in the context of psycholinguistic question answering. The authors propose a new model, FiLM, which is based on the approximate number system. The main idea is to use a neural network to predict the answer of the question, and then use the answer as a function of the number of words in the question. The model is trained on a set of images from the FiLM dataset, and the authors show that the model is able to answer the question in a way that is consistent with the answer given by the model. The paper is well-written and easy to follow."
1949,SP:c5c84ea1945b79b70521e0b73f762ad643175020,"This paper studies the problem of visual question answering in the context of psycholinguistic question answering. The authors propose a new model, FiLM, which is based on the approximate number system. The main idea is to use a neural network to predict the answer of the question, and then use the answer as a function of the number of words in the question. The model is trained on a set of images from the FiLM dataset, and the authors show that the model is able to answer the question in a way that is consistent with the answer given by the model. The paper is well-written and easy to follow."
1950,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a Bayesian approach for link prediction in relational knowledge graphs. The proposed approach is based on variational inference, where the parameters of the model are variational approximations of the original graph. The authors show that the proposed approach outperforms the state-of-the-art in terms of the number of parameters and the accuracy of the proposed model. The main contribution of the paper is the introduction of a new hyperparameter for the model, which is a combination of the standard Bayesian and variational approach. "
1951,SP:0fb732fe65ef1081b046a6aa6e1972e40cfdc247,"This paper proposes a Bayesian approach for link prediction in relational knowledge graphs. The proposed approach is based on variational inference, where the parameters of the model are variational approximations of the original graph. The authors show that the proposed approach outperforms the state-of-the-art in terms of the number of parameters and the accuracy of the proposed model. The main contribution of the paper is the introduction of a new hyperparameter for the model, which is a combination of the standard Bayesian and variational approach. "
1952,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"This paper proposes an algorithm for online dimension reduction. The algorithm is based on incremental sliced inverse regression (ISIR), which is an extension of SIR. The main difference between ISIR and SIR is that ISIR uses an overlapping technique to reduce the dimensionality of the subspace of significant factors, whereas SIR only uses an overlap technique. The paper also provides a theoretical analysis of the proposed algorithm."
1953,SP:5ff0668b433a190d87d5833d8b2a8ca04daa299c,"This paper proposes an algorithm for online dimension reduction. The algorithm is based on incremental sliced inverse regression (ISIR), which is an extension of SIR. The main difference between ISIR and SIR is that ISIR uses an overlapping technique to reduce the dimensionality of the subspace of significant factors, whereas SIR only uses an overlap technique. The paper also provides a theoretical analysis of the proposed algorithm."
1954,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"This paper proposes a generative-discriminative objective for learning multimodal representations. The proposed objective is composed of two components: (1) modality-specific generative factors, and (2) joint multi-modal features. The generative and discriminative factors are modeled as independent factors, while the joint multi modal features are jointly modeled. The experimental results show that the proposed objective outperforms the baselines on a variety of datasets."
1955,SP:4d5b993c6be6e55bdf98eca9a3b23a1bab5d2499,"This paper proposes a generative-discriminative objective for learning multimodal representations. The proposed objective is composed of two components: (1) modality-specific generative factors, and (2) joint multi-modal features. The generative and discriminative factors are modeled as independent factors, while the joint multi modal features are jointly modeled. The experimental results show that the proposed objective outperforms the baselines on a variety of datasets."
1956,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper proposes a meta-learning approach for multi-speaker neural networks. The main idea is to learn a shared conditional WaveNet core and independent learned embeddings for each speaker, and then use the shared conditional waveNet core to train an independent speaker embedding. The authors show that the proposed method outperforms the baselines in terms of sample naturalness, sample similarity, and voice similarity. "
1957,SP:cae76d3c3da91e50fe29cc3b6e204bb3e0793d7e,"This paper proposes a meta-learning approach for multi-speaker neural networks. The main idea is to learn a shared conditional WaveNet core and independent learned embeddings for each speaker, and then use the shared conditional waveNet core to train an independent speaker embedding. The authors show that the proposed method outperforms the baselines in terms of sample naturalness, sample similarity, and voice similarity. "
1958,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper studies the problem of finding robust estimators for Gaussian and elliptical distributions under the Huber’s-contamination model. In particular, the authors show that the first moment of f-GANs and depth functions can be approximated by f-Learning. The authors also provide a variational lower bound on the total variation distance between the depth function and the f-learning depth function. "
1959,SP:e80d6118fc3b9ff3195fea2f6adac88e59d350c2,"This paper studies the problem of finding robust estimators for Gaussian and elliptical distributions under the Huber’s-contamination model. In particular, the authors show that the first moment of f-GANs and depth functions can be approximated by f-Learning. The authors also provide a variational lower bound on the total variation distance between the depth function and the f-learning depth function. "
1960,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper proposes a method for learning symbolic programs for scene perception. The authors propose a hierarchical, object-based scene representation, which is used to represent the scene as a symbolic program. The proposed method is evaluated on synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art."
1961,SP:861c5336fda684e5bdd8a05f0af10dd442bf5339,"This paper proposes a method for learning symbolic programs for scene perception. The authors propose a hierarchical, object-based scene representation, which is used to represent the scene as a symbolic program. The proposed method is evaluated on synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art."
1962,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a new RNN model called g-LSTM, which is a timing-gated LSTM. The main idea of the proposed model is to learn a curriculum learning schedule for the training of the network. The authors show that the proposed method is able to achieve better convergence rate than the existing state-of-the-art in terms of convergence rate on the vanishing gradient problem. They also show that their model is more memory efficient than LSTMs."
1963,SP:a8df2aa6870a05f8580117f433e07e70a5342930,"This paper proposes a new RNN model called g-LSTM, which is a timing-gated LSTM. The main idea of the proposed model is to learn a curriculum learning schedule for the training of the network. The authors show that the proposed method is able to achieve better convergence rate than the existing state-of-the-art in terms of convergence rate on the vanishing gradient problem. They also show that their model is more memory efficient than LSTMs."
1964,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a plug-and-play detection method for out-of-distribution (OOD) detection. The proposed method is based on the idea of encoder-decoder encoder (ECE) where the encoder encodes the first and second order feature statistics and the decoder decodes the second-order feature statistics. The authors show that the proposed method can detect out of distribution samples from CIFAR-100, TinyImageNet, and DenseNet with a true negative rate of 0.5%. The authors also show that their method can be used to detect in-domain samples from Tiny-ImageNet."
1965,SP:e39bcc2ee6db054f0f1d8e8d04291a78488886ae,"This paper proposes a plug-and-play detection method for out-of-distribution (OOD) detection. The proposed method is based on the idea of encoder-decoder encoder (ECE) where the encoder encodes the first and second order feature statistics and the decoder decodes the second-order feature statistics. The authors show that the proposed method can detect out of distribution samples from CIFAR-100, TinyImageNet, and DenseNet with a true negative rate of 0.5%. The authors also show that their method can be used to detect in-domain samples from Tiny-ImageNet."
1966,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,This paper studies the problem of model recovery for one-hidden-layer neural networks. The authors propose to use gradient descent to estimate the cross entropy of the empirical risk function of a neural network with Gaussian inputs. They show that gradient descent converges to a near-optimal sample complexity with respect to the input dimension. They also provide a global convergence guarantee for gradient descent under the tensor method.
1967,SP:827f95cdefae78e38a9c4b5718fcf294606a1989,This paper studies the problem of model recovery for one-hidden-layer neural networks. The authors propose to use gradient descent to estimate the cross entropy of the empirical risk function of a neural network with Gaussian inputs. They show that gradient descent converges to a near-optimal sample complexity with respect to the input dimension. They also provide a global convergence guarantee for gradient descent under the tensor method.
1968,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a method for removing salient convolutional channels in CNNs. The proposed method is motivated by the observation that salient channels are more computationally efficient than other channels in the network. The authors propose to remove these salient channels by adding a small auxiliary connection to each layer of the network, which is called Feature Boosting and Suppressions (BBS). The authors also propose to use stochastic gradient descent (SGD) to improve the performance of FBS-augmented networks. Experiments on ImageNet classification and ResNet-18 show that FBS outperforms channel pruning and dynamic execution schemes."
1969,SP:2b4a39b997934ccf0e6b5fcb4d1e62253592b05f,"This paper proposes feature boosting and suppression (FBS), a method for removing salient convolutional channels in CNNs. The proposed method is motivated by the observation that salient channels are more computationally efficient than other channels in the network. The authors propose to remove these salient channels by adding a small auxiliary connection to each layer of the network, which is called Feature Boosting and Suppressions (BBS). The authors also propose to use stochastic gradient descent (SGD) to improve the performance of FBS-augmented networks. Experiments on ImageNet classification and ResNet-18 show that FBS outperforms channel pruning and dynamic execution schemes."
1970,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the problem of finding mixed Nash equilibria in GANs from a theoretical perspective. The authors propose a new algorithm that uses a proximal sampling procedure to find the mixed Nash equilibrium of the two players in the game. The algorithm is motivated by the fact that existing proximization methods are limited to the case of pure Nash equilibrium, which is not the case for mixed NE. The main contribution of this paper is to propose a sampling procedure that can be applied to any proximal method. The proposed algorithm is evaluated on a variety of synthetic and real-world problems and compared to Adam, RMSProp and SGD."
1971,SP:2b1813a3cc39d6e1eba546b456bf8d1f9cc8657c,"This paper studies the problem of finding mixed Nash equilibria in GANs from a theoretical perspective. The authors propose a new algorithm that uses a proximal sampling procedure to find the mixed Nash equilibrium of the two players in the game. The algorithm is motivated by the fact that existing proximization methods are limited to the case of pure Nash equilibrium, which is not the case for mixed NE. The main contribution of this paper is to propose a sampling procedure that can be applied to any proximal method. The proposed algorithm is evaluated on a variety of synthetic and real-world problems and compared to Adam, RMSProp and SGD."
1972,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,This paper proposes a method for sequential transfer learning for multi-task learning. The main idea is to train a pretrained network on a single task and then fine-tune the network on multiple tasks. The authors propose to use depth-wise convolutions in the low-parameter layers of the network to improve the transfer learning performance. The experiments show that the proposed method outperforms the logits-only fine-tuning method.
1973,SP:79ece684e3c4aca516b4ec41aa8fcb7d86449784,This paper proposes a method for sequential transfer learning for multi-task learning. The main idea is to train a pretrained network on a single task and then fine-tune the network on multiple tasks. The authors propose to use depth-wise convolutions in the low-parameter layers of the network to improve the transfer learning performance. The experiments show that the proposed method outperforms the logits-only fine-tuning method.
1974,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method for batch normalization (BN) based on batch normalisation (BN). The authors argue that BN is not suitable for multi-modal distributions, and propose a new method called Multi-Modal Batch Normalization (MBMN) to address this issue. The authors show that MBMN outperforms BN on both single-task and multi-task datasets. "
1975,SP:82b8270b33110e50b5914246f3ca75d3bdbffb6e,"This paper proposes a new normalization method for batch normalization (BN) based on batch normalisation (BN). The authors argue that BN is not suitable for multi-modal distributions, and propose a new method called Multi-Modal Batch Normalization (MBMN) to address this issue. The authors show that MBMN outperforms BN on both single-task and multi-task datasets. "
1976,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,This paper studies the lottery ticket hypothesis and proposes a pruning technique to improve the test accuracy of neural networks. The main idea is to prune subnetworks of a neural network based on the lottery tickets hypothesis. The authors show that pruning a subnetwork leads to better test accuracy than training the entire network. They also show that the pruned subnetwork is more effective than training a full network.
1977,SP:034c3bc2b2fe4991f56f168ea7b4b552c500b9ad,This paper studies the lottery ticket hypothesis and proposes a pruning technique to improve the test accuracy of neural networks. The main idea is to prune subnetworks of a neural network based on the lottery tickets hypothesis. The authors show that pruning a subnetwork leads to better test accuracy than training the entire network. They also show that the pruned subnetwork is more effective than training a full network.
1978,SP:08c662296c7cf346f027e462d29184275fd6a102,This paper proposes an extension of the attentive dynamics model (ADM) to Atari games. The authors propose a new state representation for Atari games based on the Arcade Learning Element (ALE) algorithm. The state representation is learned using the actor-critic algorithm and a count-based exploration method. The proposed method is evaluated on MONTEZUMA’s REVENGE and Atari games and compared to other state-of-the-art methods.
1979,SP:08c662296c7cf346f027e462d29184275fd6a102,This paper proposes an extension of the attentive dynamics model (ADM) to Atari games. The authors propose a new state representation for Atari games based on the Arcade Learning Element (ALE) algorithm. The state representation is learned using the actor-critic algorithm and a count-based exploration method. The proposed method is evaluated on MONTEZUMA’s REVENGE and Atari games and compared to other state-of-the-art methods.
1980,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,This paper proposes a generative adversarial network (HyperGAN) that is able to generate adversarial examples with low dimensional noise in the latent space. The proposed method is motivated by the observation that the true parameter distribution of the generated network parameter distribution can be different from the true one. The authors propose to use the KL-divergence between the true and generated network parameters to approximate the distribution of effective parameters of the generative network. They show that the proposed method outperforms the state-of-the-art GANs on MNIST and CIFAR-10 datasets. 
1981,SP:614f742a75039b1509343d53e0fb4a6d4088ab3e,This paper proposes a generative adversarial network (HyperGAN) that is able to generate adversarial examples with low dimensional noise in the latent space. The proposed method is motivated by the observation that the true parameter distribution of the generated network parameter distribution can be different from the true one. The authors propose to use the KL-divergence between the true and generated network parameters to approximate the distribution of effective parameters of the generative network. They show that the proposed method outperforms the state-of-the-art GANs on MNIST and CIFAR-10 datasets. 
1982,SP:230b3e008e687e03a8b914084b93fc81609051c0,"This paper proposes a variational auto encoder (VAE) model for representation learning. The authors propose a differentiable estimator for the ELBO of the variational autoencoder, which is based on reparametrized sampling. The main contribution of the paper is to propose a new representation learning algorithm for VAE. The proposed method is evaluated on a variety of datasets and compared to other VAE models."
1983,SP:230b3e008e687e03a8b914084b93fc81609051c0,"This paper proposes a variational auto encoder (VAE) model for representation learning. The authors propose a differentiable estimator for the ELBO of the variational autoencoder, which is based on reparametrized sampling. The main contribution of the paper is to propose a new representation learning algorithm for VAE. The proposed method is evaluated on a variety of datasets and compared to other VAE models."
1984,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes a method to train a Boltzmann machine that is robust to adversarial attacks. The main idea is to use the mean field description of the Boltzman machine as a pre-trained building block for the pre-training of a feed-forward neural network. The method is evaluated on MNIST and CIFAR-10 datasets and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of adversarial robustness.
1985,SP:153fe1172e689b345729c0c848cfb38bdae0e5f7,This paper proposes a method to train a Boltzmann machine that is robust to adversarial attacks. The main idea is to use the mean field description of the Boltzman machine as a pre-trained building block for the pre-training of a feed-forward neural network. The method is evaluated on MNIST and CIFAR-10 datasets and compared to a few baselines. The results show that the proposed method outperforms the baselines in terms of adversarial robustness.
1986,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,This paper studies the robustness of deep neural networks (DNNs) against adversarial attacks on minimal images. The authors show that DNNs are not invariant to adversarial perturbations. They also show that natural images are more robust to adversarially perturbed images.
1987,SP:40ade446aa4a700cb1519b9115e8d6cdf33db4a4,This paper studies the robustness of deep neural networks (DNNs) against adversarial attacks on minimal images. The authors show that DNNs are not invariant to adversarial perturbations. They also show that natural images are more robust to adversarially perturbed images.
1988,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper proposes a multi-agent reinforcement learning (MARL) algorithm for ad hoc worker teaming. The proposed algorithm is based on a combination of agent modeling and policy learning. The agent model is used to model the agent’s actions, and the policy is learned to maximize the mutual information between the agent and the manager. The policy is trained to maximize mutual information among the agents. The experiments are conducted on two environments: resource collection and crafting."
1989,SP:8ab0bb3eb38958d607fe6b6ebbd921b8abdf149d,"This paper proposes a multi-agent reinforcement learning (MARL) algorithm for ad hoc worker teaming. The proposed algorithm is based on a combination of agent modeling and policy learning. The agent model is used to model the agent’s actions, and the policy is learned to maximize the mutual information between the agent and the manager. The policy is trained to maximize mutual information among the agents. The experiments are conducted on two environments: resource collection and crafting."
1990,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a unified RNN framework for asynchronous time series. The proposed framework is based on the idea that RNNs should be able to handle both sparse and dense features. The authors propose to model the time features at the sequential level, and the encoder output at the static level. The experimental results show that the proposed model can achieve better performance than the baselines."
1991,SP:50a5e5227932ff1196706f53fb82f1785da45e2a,"This paper proposes a unified RNN framework for asynchronous time series. The proposed framework is based on the idea that RNNs should be able to handle both sparse and dense features. The authors propose to model the time features at the sequential level, and the encoder output at the static level. The experimental results show that the proposed model can achieve better performance than the baselines."
1992,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper studies the problem of learning propositional formulas for neural networks with recurrent neural networks. The authors propose a new approach to learn a propositional formula that can be used to train a recurrent neural network. The proposed approach is based on the idea that propositional atoms can be represented as a set of atoms in the input space of a neural network, which can then be used as input to a recurrent network. In particular, the authors show that the proposed approach can be applied to the case where the input is a sequence of atoms and the network is a feedforward network. "
1993,SP:f2c3dd2b485d6307847c759a5609b7ebe24b7058,"This paper studies the problem of learning propositional formulas for neural networks with recurrent neural networks. The authors propose a new approach to learn a propositional formula that can be used to train a recurrent neural network. The proposed approach is based on the idea that propositional atoms can be represented as a set of atoms in the input space of a neural network, which can then be used as input to a recurrent network. In particular, the authors show that the proposed approach can be applied to the case where the input is a sequence of atoms and the network is a feedforward network. "
1994,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper proposes a method for training small networks with random mini-batches. The method is based on the idea of curriculum learning, where the difficulty of a mini-batch is measured by the number of times it has been used to train the network. The authors propose to use the difficulty measure as a way to measure the difficulty and then train a competitive teacher network to improve the performance of the small network. They show that the proposed method outperforms the state-of-the-art self-paced learning method on CIFAR-10 and CIFar-100 datasets."
1995,SP:845ae21e5758a8aabfa610c291fdcc5f61af7748,"This paper proposes a method for training small networks with random mini-batches. The method is based on the idea of curriculum learning, where the difficulty of a mini-batch is measured by the number of times it has been used to train the network. The authors propose to use the difficulty measure as a way to measure the difficulty and then train a competitive teacher network to improve the performance of the small network. They show that the proposed method outperforms the state-of-the-art self-paced learning method on CIFAR-10 and CIFar-100 datasets."
1996,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper studies the generalization properties of PAC-Bayesian neural networks. The authors consider the case where the weight matrices of the network are randomly initialized with small random noise, and the network is trained with a wide training loss minimum. They show that under certain conditions, the network can generalize to a wide range of values of the weights matrices. They also provide a generalization bound on the number of samples required for the network to generalize."
1997,SP:b33a6a1fe4bbae422ba001cbe656f31d07a62025,"This paper studies the generalization properties of PAC-Bayesian neural networks. The authors consider the case where the weight matrices of the network are randomly initialized with small random noise, and the network is trained with a wide training loss minimum. They show that under certain conditions, the network can generalize to a wide range of values of the weights matrices. They also provide a generalization bound on the number of samples required for the network to generalize."
1998,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper proposes a new VAE architecture for discrete latent variable models. The main idea is to train a vector quantized autoencoder (VQ-VAE) on a sequence of discrete latent variables, and then use Expectation Maximization (EM) algorithm to maximize the expected value of the latent variables. The authors show that the proposed method is able to achieve state-of-the-art performance on CIFAR-10 and Transformer-based machine translation models. They also show that their method can be applied to a non-autoregressive machine translation model."
1999,SP:d0533cb69d938d4128d17b1a6d8aeb8d1ca6e3fd,"This paper proposes a new VAE architecture for discrete latent variable models. The main idea is to train a vector quantized autoencoder (VQ-VAE) on a sequence of discrete latent variables, and then use Expectation Maximization (EM) algorithm to maximize the expected value of the latent variables. The authors show that the proposed method is able to achieve state-of-the-art performance on CIFAR-10 and Transformer-based machine translation models. They also show that their method can be applied to a non-autoregressive machine translation model."
2000,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes a multi-view learning framework for sentence representation learning based on the Distributional Hypothesis. The authors propose a generative objective and a discriminative objective to learn sentence representations. The proposed framework consists of a recurrent neural network (RNN) and a linear model. The RNN is trained to generate a sentence representation for each view, while the discriminator is trained on the output of the RNN. Experiments show that the proposed framework outperforms the state-of-the-art methods on several downstream tasks."
2001,SP:60628f7db9cfcac3f0dbe6ce0b2a161310525ba0,"This paper proposes a multi-view learning framework for sentence representation learning based on the Distributional Hypothesis. The authors propose a generative objective and a discriminative objective to learn sentence representations. The proposed framework consists of a recurrent neural network (RNN) and a linear model. The RNN is trained to generate a sentence representation for each view, while the discriminator is trained on the output of the RNN. Experiments show that the proposed framework outperforms the state-of-the-art methods on several downstream tasks."
2002,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,This paper proposes an online distributed optimization method called Anytime Minibatch. The proposed method is based on the dual averaging technique. The authors show that the proposed method converges to the optimal solution in a finite number of minibatch iterations. The paper also provides a theoretical analysis of the proposed algorithm.
2003,SP:f5da908b5f6c19a059d2447b9cda15af5e12dc55,This paper proposes an online distributed optimization method called Anytime Minibatch. The proposed method is based on the dual averaging technique. The authors show that the proposed method converges to the optimal solution in a finite number of minibatch iterations. The paper also provides a theoretical analysis of the proposed algorithm.
2004,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,"This paper proposes a novel intrinsic reward function for reinforcement learning based on peripheral pulse measurements. The proposed method is based on the idea that intrinsic feedback can be used as a reward function to encourage the learner to perform well in the learning stage. The authors propose to use the intrinsic feedback as a task-independent reward function, which can be applied to both sparse and skewed rewards in reinforcement learning settings. Experiments on simulated driving environments show that the proposed method can improve the sample efficiency of the learned reward function."
2005,SP:f167ad4bb1e140f692ec71c8baf0a59bff7bbc6f,"This paper proposes a novel intrinsic reward function for reinforcement learning based on peripheral pulse measurements. The proposed method is based on the idea that intrinsic feedback can be used as a reward function to encourage the learner to perform well in the learning stage. The authors propose to use the intrinsic feedback as a task-independent reward function, which can be applied to both sparse and skewed rewards in reinforcement learning settings. Experiments on simulated driving environments show that the proposed method can improve the sample efficiency of the learned reward function."
2006,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper studies the problem of underfitting in neural process learning. The authors propose a new approach to tackle this problem. The main idea is to train a neural process to predict the conditional distribution of the observed data, and then use the predicted distribution as the input to a regression function. The proposed approach is based on the idea of attention, and the authors show that the proposed approach can improve the performance of the proposed method. "
2007,SP:2db0ece25ebfb4d5e3aa8eb145964ce4be19409f,"This paper studies the problem of underfitting in neural process learning. The authors propose a new approach to tackle this problem. The main idea is to train a neural process to predict the conditional distribution of the observed data, and then use the predicted distribution as the input to a regression function. The proposed approach is based on the idea of attention, and the authors show that the proposed approach can improve the performance of the proposed method. "
2008,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,This paper studies the problem of credit assignment in meta-learning. The authors propose a method to estimate the gradients of meta-policy gradients in the context of the credit assignment problem. The main contribution of this paper is to propose an algorithm that estimates the gradient of the meta-parameterized policy gradient for each task using the statistical distance between the pre-adapted and adapted policies. The proposed method is evaluated on a variety of tasks and compared to a number of baselines.
2009,SP:26535b26a3178050d8aae56b7c9669c9d2408ac8,This paper studies the problem of credit assignment in meta-learning. The authors propose a method to estimate the gradients of meta-policy gradients in the context of the credit assignment problem. The main contribution of this paper is to propose an algorithm that estimates the gradient of the meta-parameterized policy gradient for each task using the statistical distance between the pre-adapted and adapted policies. The proposed method is evaluated on a variety of tasks and compared to a number of baselines.
2010,SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes NeuroSAT, a message passing neural network for solving SAT problems. The main idea is to learn a classifier for satisfiability of the SAT problem, and then use this classifier to train a neural network to solve the problem. The proposed method is evaluated on graph coloring, clique detection, dominating set, and vertex cover problems. It is shown that the proposed method outperforms the state-of-the-art SAT solvers."
2011,SP:be5f2c827605914206f5645087b94a50f59f9214,"This paper proposes NeuroSAT, a message passing neural network for solving SAT problems. The main idea is to learn a classifier for satisfiability of the SAT problem, and then use this classifier to train a neural network to solve the problem. The proposed method is evaluated on graph coloring, clique detection, dominating set, and vertex cover problems. It is shown that the proposed method outperforms the state-of-the-art SAT solvers."
2012,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method for imitation learning in the context of autonomous driving, where the goal is to learn a policy that is robust to perturbations in the environment. The method is based on imitation learning, where a policy is trained to imitate the behavior of an expert, and then the policy is evaluated on a set of simulated trajectories generated by the expert. The main contribution of this paper is the introduction of a new loss function, which is a combination of two losses: (1) a regularization term that encourages the policy to be more robust to collisions, and (2) an adversarial loss that penalizes the policy for being more sensitive to collisions than the expert policy. The authors evaluate the proposed method on simulated driving tasks, and show that it is able to outperform the state-of-the-art in terms of robustness to collisions."
2013,SP:a99fddee87b684b2783ef3a21f8c15c19631953b,"This paper proposes a method for imitation learning in the context of autonomous driving, where the goal is to learn a policy that is robust to perturbations in the environment. The method is based on imitation learning, where a policy is trained to imitate the behavior of an expert, and then the policy is evaluated on a set of simulated trajectories generated by the expert. The main contribution of this paper is the introduction of a new loss function, which is a combination of two losses: (1) a regularization term that encourages the policy to be more robust to collisions, and (2) an adversarial loss that penalizes the policy for being more sensitive to collisions than the expert policy. The authors evaluate the proposed method on simulated driving tasks, and show that it is able to outperform the state-of-the-art in terms of robustness to collisions."
2014,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a proxy model for image classification and language modeling. The proxy model is a combination of a small proxy model and a large target model, which is trained on a large dataset of images and text. The proposed method is evaluated on CIFAR-10, SVHN, and Cifar-100."
2015,SP:f5be102f16ed9ac70a2e9e2580111226fb0d8b71,"This paper proposes a proxy model for image classification and language modeling. The proxy model is a combination of a small proxy model and a large target model, which is trained on a large dataset of images and text. The proposed method is evaluated on CIFAR-10, SVHN, and Cifar-100."
2016,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"This paper studies the problem of planning in continuous control problems. The authors propose a new method, Sequential Monte Carlo Planning (SMCP), which combines the ideas of Bayesian smoothing and sequential Monte Carlo. The main contribution of SMCP is that it can be applied to a variety of continuous control tasks. "
2017,SP:4332dfe46b715595e9f1dd3f6a79b82a646b4c23,"This paper studies the problem of planning in continuous control problems. The authors propose a new method, Sequential Monte Carlo Planning (SMCP), which combines the ideas of Bayesian smoothing and sequential Monte Carlo. The main contribution of SMCP is that it can be applied to a variety of continuous control tasks. "
2018,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,This paper studies the relationship between clean accuracy and adversarial robustness in the context of adversarial training. The authors show that the clean accuracy of adversarially trained models can be better than the robust accuracy of the original model when the input data distribution is semantically similar to the original data distribution. They also show that clean accuracy can be worse than robust accuracy when the distribution is different from the original distribution. 
2019,SP:d3e4e2c267fd9ae536ab1816d5c1ba8e8fec19be,This paper studies the relationship between clean accuracy and adversarial robustness in the context of adversarial training. The authors show that the clean accuracy of adversarially trained models can be better than the robust accuracy of the original model when the input data distribution is semantically similar to the original data distribution. They also show that clean accuracy can be worse than robust accuracy when the distribution is different from the original distribution. 
2020,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper proposes a new normalization technique for batch normalization. The proposed method is based on the idea that the output of the layer weights of a neural network can be transformed to a different output of a different layer. The authors show that the proposed method can be applied to both positive and negative weights of the network. The experimental results on CIFAR-10/100, SVHN, and ILSVRC 2012 ImageNet demonstrate the effectiveness of the proposed technique."
2021,SP:a49fd0479a977c8fb45199210f9ff7dd2c0dabaf,"This paper proposes a new normalization technique for batch normalization. The proposed method is based on the idea that the output of the layer weights of a neural network can be transformed to a different output of a different layer. The authors show that the proposed method can be applied to both positive and negative weights of the network. The experimental results on CIFAR-10/100, SVHN, and ILSVRC 2012 ImageNet demonstrate the effectiveness of the proposed technique."
2022,SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper studies the problem of data denoising (DD) in the context of over-parameterized neural networks. In particular, the authors propose a new algorithm that uses the loss statistics of the model to identify mislabeled examples. The proposed algorithm is based on the idea of the implicit regularization effect of stochastic gradient descent (SGD). The authors show that SGD can be used as a regularizer to reduce the computational overhead of ODD. The authors also show that the proposed algorithm can be applied to both artificial and real-world datasets."
2023,SP:8188f15c8521099305aa8664e05f102ee6cea402,"This paper studies the problem of data denoising (DD) in the context of over-parameterized neural networks. In particular, the authors propose a new algorithm that uses the loss statistics of the model to identify mislabeled examples. The proposed algorithm is based on the idea of the implicit regularization effect of stochastic gradient descent (SGD). The authors show that SGD can be used as a regularizer to reduce the computational overhead of ODD. The authors also show that the proposed algorithm can be applied to both artificial and real-world datasets."
2024,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a new generative model for pretraining language models for downstream NLP tasks. The proposed model is a combination of Hidden Markov Model (HMM) and HMM with a latent memory component. The authors show that the HMM can be used to improve the performance of pretrained language models on downstream tasks. In particular, the authors propose a memory-augmented version of HMM and show that it can recover the downstream performance of the pretrained model. They also provide a theoretical analysis of the effect of head tuning and prompt tuning."
2025,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a new generative model for pretraining language models for downstream NLP tasks. The proposed model is a combination of Hidden Markov Model (HMM) and HMM with a latent memory component. The authors show that the HMM can be used to improve the performance of pretrained language models on downstream tasks. In particular, the authors propose a memory-augmented version of HMM and show that it can recover the downstream performance of the pretrained model. They also provide a theoretical analysis of the effect of head tuning and prompt tuning."
2026,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a new generative model for pretraining language models for downstream NLP tasks. The proposed model is a combination of Hidden Markov Model (HMM) and HMM with a latent memory component. The authors show that the HMM can be used to improve the performance of pretrained language models on downstream tasks. In particular, the authors propose a memory-augmented version of HMM and show that it can recover the downstream performance of the pretrained model. They also provide a theoretical analysis of the effect of head tuning and prompt tuning."
2027,SP:fbf023a772013e6eca62f92982aecf857c16a428,"This paper proposes a new generative model for pretraining language models for downstream NLP tasks. The proposed model is a combination of Hidden Markov Model (HMM) and HMM with a latent memory component. The authors show that the HMM can be used to improve the performance of pretrained language models on downstream tasks. In particular, the authors propose a memory-augmented version of HMM and show that it can recover the downstream performance of the pretrained model. They also provide a theoretical analysis of the effect of head tuning and prompt tuning."
2028,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of transferability of feature embeddings for out-of-distribution (OOD) generalization. The authors propose a new algorithm for learning transferable features. The main idea is to use the total variation and the Wasserstein distance between the feature embedding and the classifier. The proposed algorithm is evaluated on PACS, Rotated MNIST, Office-Home, and Office-MNIST."
2029,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of transferability of feature embeddings for out-of-distribution (OOD) generalization. The authors propose a new algorithm for learning transferable features. The main idea is to use the total variation and the Wasserstein distance between the feature embedding and the classifier. The proposed algorithm is evaluated on PACS, Rotated MNIST, Office-Home, and Office-MNIST."
2030,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of transferability of feature embeddings for out-of-distribution (OOD) generalization. The authors propose a new algorithm for learning transferable features. The main idea is to use the total variation and the Wasserstein distance between the feature embedding and the classifier. The proposed algorithm is evaluated on PACS, Rotated MNIST, Office-Home, and Office-MNIST."
2031,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,"This paper studies the problem of transferability of feature embeddings for out-of-distribution (OOD) generalization. The authors propose a new algorithm for learning transferable features. The main idea is to use the total variation and the Wasserstein distance between the feature embedding and the classifier. The proposed algorithm is evaluated on PACS, Rotated MNIST, Office-Home, and Office-MNIST."
2032,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,This paper studies the problem of learning a Markov reward function for reinforcement learning. The authors propose a polynomial-time algorithm for learning the reward function. The algorithm is based on the idea of partial ordering and partial ordering with partial ordering. They show that the proposed algorithm can be used to learn a reward function that is polynomially time-invariant.
2033,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,This paper studies the problem of learning a Markov reward function for reinforcement learning. The authors propose a polynomial-time algorithm for learning the reward function. The algorithm is based on the idea of partial ordering and partial ordering with partial ordering. They show that the proposed algorithm can be used to learn a reward function that is polynomially time-invariant.
2034,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,This paper studies the problem of learning a Markov reward function for reinforcement learning. The authors propose a polynomial-time algorithm for learning the reward function. The algorithm is based on the idea of partial ordering and partial ordering with partial ordering. They show that the proposed algorithm can be used to learn a reward function that is polynomially time-invariant.
2035,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,This paper studies the problem of learning a Markov reward function for reinforcement learning. The authors propose a polynomial-time algorithm for learning the reward function. The algorithm is based on the idea of partial ordering and partial ordering with partial ordering. They show that the proposed algorithm can be used to learn a reward function that is polynomially time-invariant.
2036,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,This paper studies the problem of generalization in partially observed Markov decision processes (POMDPs). The authors propose an ensemble-based algorithm to solve the epistemic POMDP problem. The proposed algorithm is evaluated on Procgen benchmark.
2037,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,This paper studies the problem of generalization in partially observed Markov decision processes (POMDPs). The authors propose an ensemble-based algorithm to solve the epistemic POMDP problem. The proposed algorithm is evaluated on Procgen benchmark.
2038,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,This paper studies the problem of generalization in partially observed Markov decision processes (POMDPs). The authors propose an ensemble-based algorithm to solve the epistemic POMDP problem. The proposed algorithm is evaluated on Procgen benchmark.
2039,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,This paper studies the problem of generalization in partially observed Markov decision processes (POMDPs). The authors propose an ensemble-based algorithm to solve the epistemic POMDP problem. The proposed algorithm is evaluated on Procgen benchmark.
2040,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a meta-reinforcement learning framework for model-agnostic meta-learning. The main idea is to learn a higher-order derivative of the Hessian matrix of value functions, which can then be used for off-policy evaluation. The proposed method is based on the auto-differentiation library and the authors show that the proposed method outperforms the state-of-the-art baselines."
2041,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a meta-reinforcement learning framework for model-agnostic meta-learning. The main idea is to learn a higher-order derivative of the Hessian matrix of value functions, which can then be used for off-policy evaluation. The proposed method is based on the auto-differentiation library and the authors show that the proposed method outperforms the state-of-the-art baselines."
2042,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a meta-reinforcement learning framework for model-agnostic meta-learning. The main idea is to learn a higher-order derivative of the Hessian matrix of value functions, which can then be used for off-policy evaluation. The proposed method is based on the auto-differentiation library and the authors show that the proposed method outperforms the state-of-the-art baselines."
2043,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"This paper proposes a meta-reinforcement learning framework for model-agnostic meta-learning. The main idea is to learn a higher-order derivative of the Hessian matrix of value functions, which can then be used for off-policy evaluation. The proposed method is based on the auto-differentiation library and the authors show that the proposed method outperforms the state-of-the-art baselines."
2044,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper studies the problem of distributed learning in the presence of communication constraints. The authors propose a new algorithm called MCM, which is based on the idea of perturbation of the gradients of the local models of the global model. The main idea of the algorithm is to perturb the gradient of a local model with respect to the perturbed global model, and then use the gradient to compress the local model and store it in the memory of the central server. The proposed algorithm is shown to converge to the optimal solution in terms of convergence rate."
2045,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper studies the problem of distributed learning in the presence of communication constraints. The authors propose a new algorithm called MCM, which is based on the idea of perturbation of the gradients of the local models of the global model. The main idea of the algorithm is to perturb the gradient of a local model with respect to the perturbed global model, and then use the gradient to compress the local model and store it in the memory of the central server. The proposed algorithm is shown to converge to the optimal solution in terms of convergence rate."
2046,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper studies the problem of distributed learning in the presence of communication constraints. The authors propose a new algorithm called MCM, which is based on the idea of perturbation of the gradients of the local models of the global model. The main idea of the algorithm is to perturb the gradient of a local model with respect to the perturbed global model, and then use the gradient to compress the local model and store it in the memory of the central server. The proposed algorithm is shown to converge to the optimal solution in terms of convergence rate."
2047,SP:54a60315416c6e304f59741490c335fb1e2ce95d,"This paper studies the problem of distributed learning in the presence of communication constraints. The authors propose a new algorithm called MCM, which is based on the idea of perturbation of the gradients of the local models of the global model. The main idea of the algorithm is to perturb the gradient of a local model with respect to the perturbed global model, and then use the gradient to compress the local model and store it in the memory of the central server. The proposed algorithm is shown to converge to the optimal solution in terms of convergence rate."
2048,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of counterfactual invariance of out-of-domain (OOD) models to spurious correlations. The authors propose two regularization schemes to improve the robustness of the model against spurious correlations, and show that the proposed methods are robust to domain shift. They also provide a theoretical analysis of the causal structure of the proposed regularization scheme, which shows that it is invariant to the domain shift and can be used to derive a causal structure that is robust to spurious correlation."
2049,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of counterfactual invariance of out-of-domain (OOD) models to spurious correlations. The authors propose two regularization schemes to improve the robustness of the model against spurious correlations, and show that the proposed methods are robust to domain shift. They also provide a theoretical analysis of the causal structure of the proposed regularization scheme, which shows that it is invariant to the domain shift and can be used to derive a causal structure that is robust to spurious correlation."
2050,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of counterfactual invariance of out-of-domain (OOD) models to spurious correlations. The authors propose two regularization schemes to improve the robustness of the model against spurious correlations, and show that the proposed methods are robust to domain shift. They also provide a theoretical analysis of the causal structure of the proposed regularization scheme, which shows that it is invariant to the domain shift and can be used to derive a causal structure that is robust to spurious correlation."
2051,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"This paper studies the problem of counterfactual invariance of out-of-domain (OOD) models to spurious correlations. The authors propose two regularization schemes to improve the robustness of the model against spurious correlations, and show that the proposed methods are robust to domain shift. They also provide a theoretical analysis of the causal structure of the proposed regularization scheme, which shows that it is invariant to the domain shift and can be used to derive a causal structure that is robust to spurious correlation."
2052,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes Adaptive Pseudo Augmentation (APA), a method for improving the performance of GANs in the low-data regime. APA is based on the idea that the discriminator of a GAN can overfit to a small subset of the generated images. The authors argue that this overfitting is due to the fact that the generator is trained to generate images that are close to the real data distribution. To address this issue, the authors propose to augment the original image with pseudo-images generated by a generator that is trained on the pseudo-image generated by the generator. The proposed method is evaluated on StyleGAN2 and StyleGAN3."
2053,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes Adaptive Pseudo Augmentation (APA), a method for improving the performance of GANs in the low-data regime. APA is based on the idea that the discriminator of a GAN can overfit to a small subset of the generated images. The authors argue that this overfitting is due to the fact that the generator is trained to generate images that are close to the real data distribution. To address this issue, the authors propose to augment the original image with pseudo-images generated by a generator that is trained on the pseudo-image generated by the generator. The proposed method is evaluated on StyleGAN2 and StyleGAN3."
2054,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes Adaptive Pseudo Augmentation (APA), a method for improving the performance of GANs in the low-data regime. APA is based on the idea that the discriminator of a GAN can overfit to a small subset of the generated images. The authors argue that this overfitting is due to the fact that the generator is trained to generate images that are close to the real data distribution. To address this issue, the authors propose to augment the original image with pseudo-images generated by a generator that is trained on the pseudo-image generated by the generator. The proposed method is evaluated on StyleGAN2 and StyleGAN3."
2055,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"This paper proposes Adaptive Pseudo Augmentation (APA), a method for improving the performance of GANs in the low-data regime. APA is based on the idea that the discriminator of a GAN can overfit to a small subset of the generated images. The authors argue that this overfitting is due to the fact that the generator is trained to generate images that are close to the real data distribution. To address this issue, the authors propose to augment the original image with pseudo-images generated by a generator that is trained on the pseudo-image generated by the generator. The proposed method is evaluated on StyleGAN2 and StyleGAN3."
2056,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a method for causal inference in multivariate point processes. The proposed method is based on the idea of Rubin’s framework, which is a formalization of the average treatment effect (ATE) and propensity scores (PE) in the context of multivariate recurrent event streams. The authors show that the proposed method can be applied to both synthetic and real-world event datasets. "
2057,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a method for causal inference in multivariate point processes. The proposed method is based on the idea of Rubin’s framework, which is a formalization of the average treatment effect (ATE) and propensity scores (PE) in the context of multivariate recurrent event streams. The authors show that the proposed method can be applied to both synthetic and real-world event datasets. "
2058,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a method for causal inference in multivariate point processes. The proposed method is based on the idea of Rubin’s framework, which is a formalization of the average treatment effect (ATE) and propensity scores (PE) in the context of multivariate recurrent event streams. The authors show that the proposed method can be applied to both synthetic and real-world event datasets. "
2059,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a method for causal inference in multivariate point processes. The proposed method is based on the idea of Rubin’s framework, which is a formalization of the average treatment effect (ATE) and propensity scores (PE) in the context of multivariate recurrent event streams. The authors show that the proposed method can be applied to both synthetic and real-world event datasets. "
2060,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper proposes a method for improving the resilience of graph neural networks (GNNs) in the presence of abnormal node features. Specifically, the authors propose an adaptive residual network (AIRGNN) that is able to adaptively learn the residual connections in the message passing of GNNs. The authors show that AirGNN can be used to improve the performance of existing GNN models on a variety of graph representation learning tasks. "
2061,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper proposes a method for improving the resilience of graph neural networks (GNNs) in the presence of abnormal node features. Specifically, the authors propose an adaptive residual network (AIRGNN) that is able to adaptively learn the residual connections in the message passing of GNNs. The authors show that AirGNN can be used to improve the performance of existing GNN models on a variety of graph representation learning tasks. "
2062,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper proposes a method for improving the resilience of graph neural networks (GNNs) in the presence of abnormal node features. Specifically, the authors propose an adaptive residual network (AIRGNN) that is able to adaptively learn the residual connections in the message passing of GNNs. The authors show that AirGNN can be used to improve the performance of existing GNN models on a variety of graph representation learning tasks. "
2063,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper proposes a method for improving the resilience of graph neural networks (GNNs) in the presence of abnormal node features. Specifically, the authors propose an adaptive residual network (AIRGNN) that is able to adaptively learn the residual connections in the message passing of GNNs. The authors show that AirGNN can be used to improve the performance of existing GNN models on a variety of graph representation learning tasks. "
2064,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies the problem of variational Bayesian optimistic sampling (VBOS) in the context of stochastic multi-armed bandit problems. The authors propose a variational Thompson sampling algorithm that can be used to sample from the optimistic set of policies in the policy optimization problem. The proposed algorithm is based on the Thompson sampling policy, which is a variant of Thompson sampling. The main contribution of the paper is the analysis of the regret of VBOS in the case where the policy is a convex optimization problem with log-concavity, unimodality, smoothness, and smoothness. In particular, the authors show that the regret can be expressed as a function of the posterior of the policy and the exploration-exploitation trade-off between the exploration and exploitation trade-offs. The paper also provides a theoretical analysis of this problem."
2065,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies the problem of variational Bayesian optimistic sampling (VBOS) in the context of stochastic multi-armed bandit problems. The authors propose a variational Thompson sampling algorithm that can be used to sample from the optimistic set of policies in the policy optimization problem. The proposed algorithm is based on the Thompson sampling policy, which is a variant of Thompson sampling. The main contribution of the paper is the analysis of the regret of VBOS in the case where the policy is a convex optimization problem with log-concavity, unimodality, smoothness, and smoothness. In particular, the authors show that the regret can be expressed as a function of the posterior of the policy and the exploration-exploitation trade-off between the exploration and exploitation trade-offs. The paper also provides a theoretical analysis of this problem."
2066,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies the problem of variational Bayesian optimistic sampling (VBOS) in the context of stochastic multi-armed bandit problems. The authors propose a variational Thompson sampling algorithm that can be used to sample from the optimistic set of policies in the policy optimization problem. The proposed algorithm is based on the Thompson sampling policy, which is a variant of Thompson sampling. The main contribution of the paper is the analysis of the regret of VBOS in the case where the policy is a convex optimization problem with log-concavity, unimodality, smoothness, and smoothness. In particular, the authors show that the regret can be expressed as a function of the posterior of the policy and the exploration-exploitation trade-off between the exploration and exploitation trade-offs. The paper also provides a theoretical analysis of this problem."
2067,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper studies the problem of variational Bayesian optimistic sampling (VBOS) in the context of stochastic multi-armed bandit problems. The authors propose a variational Thompson sampling algorithm that can be used to sample from the optimistic set of policies in the policy optimization problem. The proposed algorithm is based on the Thompson sampling policy, which is a variant of Thompson sampling. The main contribution of the paper is the analysis of the regret of VBOS in the case where the policy is a convex optimization problem with log-concavity, unimodality, smoothness, and smoothness. In particular, the authors show that the regret can be expressed as a function of the posterior of the policy and the exploration-exploitation trade-off between the exploration and exploitation trade-offs. The paper also provides a theoretical analysis of this problem."
2068,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of without-replacement sampling in stochastic gradient descent (SGD) without variance reduction. In particular, the authors propose a new algorithm called Prox-DFinito, which is a variant of Finito with a cyclic order. The authors show that the proposed algorithm can achieve sample-size-independent convergence rates for both convex and strongly convex SGD problems. They also provide a theoretical analysis of the convergence rate of the proposed method."
2069,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of without-replacement sampling in stochastic gradient descent (SGD) without variance reduction. In particular, the authors propose a new algorithm called Prox-DFinito, which is a variant of Finito with a cyclic order. The authors show that the proposed algorithm can achieve sample-size-independent convergence rates for both convex and strongly convex SGD problems. They also provide a theoretical analysis of the convergence rate of the proposed method."
2070,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of without-replacement sampling in stochastic gradient descent (SGD) without variance reduction. In particular, the authors propose a new algorithm called Prox-DFinito, which is a variant of Finito with a cyclic order. The authors show that the proposed algorithm can achieve sample-size-independent convergence rates for both convex and strongly convex SGD problems. They also provide a theoretical analysis of the convergence rate of the proposed method."
2071,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper studies the problem of without-replacement sampling in stochastic gradient descent (SGD) without variance reduction. In particular, the authors propose a new algorithm called Prox-DFinito, which is a variant of Finito with a cyclic order. The authors show that the proposed algorithm can achieve sample-size-independent convergence rates for both convex and strongly convex SGD problems. They also provide a theoretical analysis of the convergence rate of the proposed method."
2072,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the relative entropy policy search (REPS) problem in reinforcement learning. The authors propose a generative access method for policy optimization. The main idea is to use a Markov decision process (MDP) to update the parameters of the policy, and then use a stochastic gradient-based solver to find the optimal regularized policy. The proposed method is shown to have favorable convergence rates compared to the state-of-the-art methods."
2073,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the relative entropy policy search (REPS) problem in reinforcement learning. The authors propose a generative access method for policy optimization. The main idea is to use a Markov decision process (MDP) to update the parameters of the policy, and then use a stochastic gradient-based solver to find the optimal regularized policy. The proposed method is shown to have favorable convergence rates compared to the state-of-the-art methods."
2074,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the relative entropy policy search (REPS) problem in reinforcement learning. The authors propose a generative access method for policy optimization. The main idea is to use a Markov decision process (MDP) to update the parameters of the policy, and then use a stochastic gradient-based solver to find the optimal regularized policy. The proposed method is shown to have favorable convergence rates compared to the state-of-the-art methods."
2075,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper studies the relative entropy policy search (REPS) problem in reinforcement learning. The authors propose a generative access method for policy optimization. The main idea is to use a Markov decision process (MDP) to update the parameters of the policy, and then use a stochastic gradient-based solver to find the optimal regularized policy. The proposed method is shown to have favorable convergence rates compared to the state-of-the-art methods."
2076,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method for learning 3D point cloud representations from 3D data. The method is based on adversarial training, where the network is trained to minimize the distance between points in the 3D space. The authors show that the proposed method is able to achieve better performance than the state-of-the-art methods in terms of representation complexity, spatial smoothness, and adversarial robustness. "
2077,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method for learning 3D point cloud representations from 3D data. The method is based on adversarial training, where the network is trained to minimize the distance between points in the 3D space. The authors show that the proposed method is able to achieve better performance than the state-of-the-art methods in terms of representation complexity, spatial smoothness, and adversarial robustness. "
2078,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method for learning 3D point cloud representations from 3D data. The method is based on adversarial training, where the network is trained to minimize the distance between points in the 3D space. The authors show that the proposed method is able to achieve better performance than the state-of-the-art methods in terms of representation complexity, spatial smoothness, and adversarial robustness. "
2079,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper proposes a method for learning 3D point cloud representations from 3D data. The method is based on adversarial training, where the network is trained to minimize the distance between points in the 3D space. The authors show that the proposed method is able to achieve better performance than the state-of-the-art methods in terms of representation complexity, spatial smoothness, and adversarial robustness. "
2080,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural-network-based method for finding optimal auctions in restricted settings. The main idea is to use a pre-trained neural network to estimate the value of each item in an auction, and then use this value as a proxy for the fairness, diversity, and allocation fairness of the auction. The proposed method, PreferenceNet, is evaluated on a variety of auctions, and is shown to outperform the state-of-the-art methods."
2081,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural-network-based method for finding optimal auctions in restricted settings. The main idea is to use a pre-trained neural network to estimate the value of each item in an auction, and then use this value as a proxy for the fairness, diversity, and allocation fairness of the auction. The proposed method, PreferenceNet, is evaluated on a variety of auctions, and is shown to outperform the state-of-the-art methods."
2082,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural-network-based method for finding optimal auctions in restricted settings. The main idea is to use a pre-trained neural network to estimate the value of each item in an auction, and then use this value as a proxy for the fairness, diversity, and allocation fairness of the auction. The proposed method, PreferenceNet, is evaluated on a variety of auctions, and is shown to outperform the state-of-the-art methods."
2083,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper proposes a neural-network-based method for finding optimal auctions in restricted settings. The main idea is to use a pre-trained neural network to estimate the value of each item in an auction, and then use this value as a proxy for the fairness, diversity, and allocation fairness of the auction. The proposed method, PreferenceNet, is evaluated on a variety of auctions, and is shown to outperform the state-of-the-art methods."
2084,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,This paper studies the problem of user-level differential privacy for linear regression problems. The authors propose an exponential mechanism-based algorithm for this problem. The algorithm is based on the idea of joint differential privacy (JDP) and provides an information-theoretic upper bound on the error of the algorithm. The paper also provides an empirical evaluation of the proposed algorithm.
2085,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,This paper studies the problem of user-level differential privacy for linear regression problems. The authors propose an exponential mechanism-based algorithm for this problem. The algorithm is based on the idea of joint differential privacy (JDP) and provides an information-theoretic upper bound on the error of the algorithm. The paper also provides an empirical evaluation of the proposed algorithm.
2086,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,This paper studies the problem of user-level differential privacy for linear regression problems. The authors propose an exponential mechanism-based algorithm for this problem. The algorithm is based on the idea of joint differential privacy (JDP) and provides an information-theoretic upper bound on the error of the algorithm. The paper also provides an empirical evaluation of the proposed algorithm.
2087,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,This paper studies the problem of user-level differential privacy for linear regression problems. The authors propose an exponential mechanism-based algorithm for this problem. The algorithm is based on the idea of joint differential privacy (JDP) and provides an information-theoretic upper bound on the error of the algorithm. The paper also provides an empirical evaluation of the proposed algorithm.
2088,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for adversarial VQA (AdVQA) based on the Visual Question Answering dataset. The proposed benchmark is designed to evaluate the performance of the state-of-the-art models on a set of human-adversarially generated examples. The paper also proposes a method to train the model on adversarial examples. 
2089,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for adversarial VQA (AdVQA) based on the Visual Question Answering dataset. The proposed benchmark is designed to evaluate the performance of the state-of-the-art models on a set of human-adversarially generated examples. The paper also proposes a method to train the model on adversarial examples. 
2090,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for adversarial VQA (AdVQA) based on the Visual Question Answering dataset. The proposed benchmark is designed to evaluate the performance of the state-of-the-art models on a set of human-adversarially generated examples. The paper also proposes a method to train the model on adversarial examples. 
2091,SP:3925fc528de17b8b2e93808f5440ea0503895b75,This paper proposes a new benchmark for adversarial VQA (AdVQA) based on the Visual Question Answering dataset. The proposed benchmark is designed to evaluate the performance of the state-of-the-art models on a set of human-adversarially generated examples. The paper also proposes a method to train the model on adversarial examples. 
2092,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper proposes a method for modeling the response patterns of MEC neurons. The authors propose a goal-driven model based on a combination of a grid cell-based model and an intermediate place cell model. The grid cell is used to model the spatial response of the MEC cells, while the intermediate place cells are used to represent the spatial behavior of the cells. The proposed method is evaluated on a variety of tasks and compared to a grid-based and a heterogeneous model."
2093,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper proposes a method for modeling the response patterns of MEC neurons. The authors propose a goal-driven model based on a combination of a grid cell-based model and an intermediate place cell model. The grid cell is used to model the spatial response of the MEC cells, while the intermediate place cells are used to represent the spatial behavior of the cells. The proposed method is evaluated on a variety of tasks and compared to a grid-based and a heterogeneous model."
2094,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper proposes a method for modeling the response patterns of MEC neurons. The authors propose a goal-driven model based on a combination of a grid cell-based model and an intermediate place cell model. The grid cell is used to model the spatial response of the MEC cells, while the intermediate place cells are used to represent the spatial behavior of the cells. The proposed method is evaluated on a variety of tasks and compared to a grid-based and a heterogeneous model."
2095,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"This paper proposes a method for modeling the response patterns of MEC neurons. The authors propose a goal-driven model based on a combination of a grid cell-based model and an intermediate place cell model. The grid cell is used to model the spatial response of the MEC cells, while the intermediate place cells are used to represent the spatial behavior of the cells. The proposed method is evaluated on a variety of tasks and compared to a grid-based and a heterogeneous model."
2096,SP:57f9812fa5e7d0c66d412beb035301684d760746,This paper studies the problem of learning behavioral reference policies in the presence of pathological training dynamics. The authors propose a KL-regularized reinforcement learning (KL-RL) algorithm for this problem. The KL-RL algorithm is based on the idea that the policy should be able to learn to imitate the behavior of an expert policy class. The main contribution of this paper is that the authors propose to use a non-parametric behavioral reference policy to train the policy classifier. The proposed method is evaluated on locomotion and dexterous hand manipulation tasks and compared to several state-of-the-art RL algorithms.
2097,SP:57f9812fa5e7d0c66d412beb035301684d760746,This paper studies the problem of learning behavioral reference policies in the presence of pathological training dynamics. The authors propose a KL-regularized reinforcement learning (KL-RL) algorithm for this problem. The KL-RL algorithm is based on the idea that the policy should be able to learn to imitate the behavior of an expert policy class. The main contribution of this paper is that the authors propose to use a non-parametric behavioral reference policy to train the policy classifier. The proposed method is evaluated on locomotion and dexterous hand manipulation tasks and compared to several state-of-the-art RL algorithms.
2098,SP:57f9812fa5e7d0c66d412beb035301684d760746,This paper studies the problem of learning behavioral reference policies in the presence of pathological training dynamics. The authors propose a KL-regularized reinforcement learning (KL-RL) algorithm for this problem. The KL-RL algorithm is based on the idea that the policy should be able to learn to imitate the behavior of an expert policy class. The main contribution of this paper is that the authors propose to use a non-parametric behavioral reference policy to train the policy classifier. The proposed method is evaluated on locomotion and dexterous hand manipulation tasks and compared to several state-of-the-art RL algorithms.
2099,SP:57f9812fa5e7d0c66d412beb035301684d760746,This paper studies the problem of learning behavioral reference policies in the presence of pathological training dynamics. The authors propose a KL-regularized reinforcement learning (KL-RL) algorithm for this problem. The KL-RL algorithm is based on the idea that the policy should be able to learn to imitate the behavior of an expert policy class. The main contribution of this paper is that the authors propose to use a non-parametric behavioral reference policy to train the policy classifier. The proposed method is evaluated on locomotion and dexterous hand manipulation tasks and compared to several state-of-the-art RL algorithms.
2100,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the problem of kernel regression with convolutional neural networks. The authors propose a teacher-student framework for kernel regression, where the teacher is a neural tangent kernel and the student is a linear kernel. The main contribution of this paper is to show that the learning curve exponent of the teacher and student is invariant to the number of filters in the network. The paper also provides a theoretical analysis of the convergence rate of the proposed method."
2101,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the problem of kernel regression with convolutional neural networks. The authors propose a teacher-student framework for kernel regression, where the teacher is a neural tangent kernel and the student is a linear kernel. The main contribution of this paper is to show that the learning curve exponent of the teacher and student is invariant to the number of filters in the network. The paper also provides a theoretical analysis of the convergence rate of the proposed method."
2102,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the problem of kernel regression with convolutional neural networks. The authors propose a teacher-student framework for kernel regression, where the teacher is a neural tangent kernel and the student is a linear kernel. The main contribution of this paper is to show that the learning curve exponent of the teacher and student is invariant to the number of filters in the network. The paper also provides a theoretical analysis of the convergence rate of the proposed method."
2103,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,"This paper studies the problem of kernel regression with convolutional neural networks. The authors propose a teacher-student framework for kernel regression, where the teacher is a neural tangent kernel and the student is a linear kernel. The main contribution of this paper is to show that the learning curve exponent of the teacher and student is invariant to the number of filters in the network. The paper also provides a theoretical analysis of the convergence rate of the proposed method."
2104,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a variational autoencoder (VAE) method for learning latent representations of complex data distributions. The proposed method is based on the uni-modal Gaussian distribution of the latent space of the VAE. The authors propose to use a deterministic auto-encoder to learn the latent representation of the data. The main contribution of the paper is the introduction of an ex-post density estimation step to estimate the latent density of the model. The paper also proposes a new variational training procedure for VAEs. The experimental results show that the proposed method outperforms the state-of-the-art methods on both continuous and discrete domains.
2105,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a variational autoencoder (VAE) method for learning latent representations of complex data distributions. The proposed method is based on the uni-modal Gaussian distribution of the latent space of the VAE. The authors propose to use a deterministic auto-encoder to learn the latent representation of the data. The main contribution of the paper is the introduction of an ex-post density estimation step to estimate the latent density of the model. The paper also proposes a new variational training procedure for VAEs. The experimental results show that the proposed method outperforms the state-of-the-art methods on both continuous and discrete domains.
2106,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a variational autoencoder (VAE) method for learning latent representations of complex data distributions. The proposed method is based on the uni-modal Gaussian distribution of the latent space of the VAE. The authors propose to use a deterministic auto-encoder to learn the latent representation of the data. The main contribution of the paper is the introduction of an ex-post density estimation step to estimate the latent density of the model. The paper also proposes a new variational training procedure for VAEs. The experimental results show that the proposed method outperforms the state-of-the-art methods on both continuous and discrete domains.
2107,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,This paper proposes a variational autoencoder (VAE) method for learning latent representations of complex data distributions. The proposed method is based on the uni-modal Gaussian distribution of the latent space of the VAE. The authors propose to use a deterministic auto-encoder to learn the latent representation of the data. The main contribution of the paper is the introduction of an ex-post density estimation step to estimate the latent density of the model. The paper also proposes a new variational training procedure for VAEs. The experimental results show that the proposed method outperforms the state-of-the-art methods on both continuous and discrete domains.
2108,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes an autoencoder-based method for self-supervised adversarial detection. The proposed method is based on the idea of disentangling represen8 tations of images into disentangled class features and semantic features. The authors also propose a discriminator network to improve the generalization ability of autoencoders. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
2109,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes an autoencoder-based method for self-supervised adversarial detection. The proposed method is based on the idea of disentangling represen8 tations of images into disentangled class features and semantic features. The authors also propose a discriminator network to improve the generalization ability of autoencoders. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
2110,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes an autoencoder-based method for self-supervised adversarial detection. The proposed method is based on the idea of disentangling represen8 tations of images into disentangled class features and semantic features. The authors also propose a discriminator network to improve the generalization ability of autoencoders. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
2111,SP:6232d8738592c9728feddec4462e61903a17d131,This paper proposes an autoencoder-based method for self-supervised adversarial detection. The proposed method is based on the idea of disentangling represen8 tations of images into disentangled class features and semantic features. The authors also propose a discriminator network to improve the generalization ability of autoencoders. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
2112,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,This paper proposes a new method for measuring the brain activity of a language system using fMRI data. The method is based on the observation that the brain is highly sensitive to the syntactic structure of the stimulus words. The authors propose to use a multi-dimensional representation of the input word embedding space to measure the semantic processing load of the language system. They also propose a new complexity metric that measures the complexity of syntactic processing load. They show that the proposed method is able to capture the brain representation of syntax in the fMRI recordings. 
2113,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,This paper proposes a new method for measuring the brain activity of a language system using fMRI data. The method is based on the observation that the brain is highly sensitive to the syntactic structure of the stimulus words. The authors propose to use a multi-dimensional representation of the input word embedding space to measure the semantic processing load of the language system. They also propose a new complexity metric that measures the complexity of syntactic processing load. They show that the proposed method is able to capture the brain representation of syntax in the fMRI recordings. 
2114,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,This paper proposes a new method for measuring the brain activity of a language system using fMRI data. The method is based on the observation that the brain is highly sensitive to the syntactic structure of the stimulus words. The authors propose to use a multi-dimensional representation of the input word embedding space to measure the semantic processing load of the language system. They also propose a new complexity metric that measures the complexity of syntactic processing load. They show that the proposed method is able to capture the brain representation of syntax in the fMRI recordings. 
2115,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,This paper proposes a new method for measuring the brain activity of a language system using fMRI data. The method is based on the observation that the brain is highly sensitive to the syntactic structure of the stimulus words. The authors propose to use a multi-dimensional representation of the input word embedding space to measure the semantic processing load of the language system. They also propose a new complexity metric that measures the complexity of syntactic processing load. They show that the proposed method is able to capture the brain representation of syntax in the fMRI recordings. 
2116,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation based on energy based models (EBMs). The authors propose a pre-trained generative model, StyleGAN, which is a combination of StyleGAN and an EBM. StyleGAN is trained to generate high-resolution images from a latent space of the EBM, while StyleGAN uses an ODE to sample from the latent space. The authors show that StyleGAN outperforms the state-of-the-art in both conditional sampling and sequential editing."
2117,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation based on energy based models (EBMs). The authors propose a pre-trained generative model, StyleGAN, which is a combination of StyleGAN and an EBM. StyleGAN is trained to generate high-resolution images from a latent space of the EBM, while StyleGAN uses an ODE to sample from the latent space. The authors show that StyleGAN outperforms the state-of-the-art in both conditional sampling and sequential editing."
2118,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation based on energy based models (EBMs). The authors propose a pre-trained generative model, StyleGAN, which is a combination of StyleGAN and an EBM. StyleGAN is trained to generate high-resolution images from a latent space of the EBM, while StyleGAN uses an ODE to sample from the latent space. The authors show that StyleGAN outperforms the state-of-the-art in both conditional sampling and sequential editing."
2119,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper proposes a method for controllable image generation based on energy based models (EBMs). The authors propose a pre-trained generative model, StyleGAN, which is a combination of StyleGAN and an EBM. StyleGAN is trained to generate high-resolution images from a latent space of the EBM, while StyleGAN uses an ODE to sample from the latent space. The authors show that StyleGAN outperforms the state-of-the-art in both conditional sampling and sequential editing."
2120,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper studies the problem of federated linear contextual bandits with shared global parameters. In particular, the authors consider the case where the local feature vectors and the raw data are distributed across multiple clients. The authors propose a collaborative algorithm called Fed-PE, which is based on a multi-client G-optimal design. The main contribution of this paper is to provide a tight minimax regret lower bound for disjoint and shared parameter cases, and a tighter bound for the shared parameter case. The proposed algorithm is evaluated on both synthetic and real-world datasets."
2121,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper studies the problem of federated linear contextual bandits with shared global parameters. In particular, the authors consider the case where the local feature vectors and the raw data are distributed across multiple clients. The authors propose a collaborative algorithm called Fed-PE, which is based on a multi-client G-optimal design. The main contribution of this paper is to provide a tight minimax regret lower bound for disjoint and shared parameter cases, and a tighter bound for the shared parameter case. The proposed algorithm is evaluated on both synthetic and real-world datasets."
2122,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper studies the problem of federated linear contextual bandits with shared global parameters. In particular, the authors consider the case where the local feature vectors and the raw data are distributed across multiple clients. The authors propose a collaborative algorithm called Fed-PE, which is based on a multi-client G-optimal design. The main contribution of this paper is to provide a tight minimax regret lower bound for disjoint and shared parameter cases, and a tighter bound for the shared parameter case. The proposed algorithm is evaluated on both synthetic and real-world datasets."
2123,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"This paper studies the problem of federated linear contextual bandits with shared global parameters. In particular, the authors consider the case where the local feature vectors and the raw data are distributed across multiple clients. The authors propose a collaborative algorithm called Fed-PE, which is based on a multi-client G-optimal design. The main contribution of this paper is to provide a tight minimax regret lower bound for disjoint and shared parameter cases, and a tighter bound for the shared parameter case. The proposed algorithm is evaluated on both synthetic and real-world datasets."
2124,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a model-based offline reinforcement learning algorithm called COMBO. The main idea of COMBO is to use a model to estimate the uncertainty of the value function of the policy, which is then used to train a policy that maximizes the expected return of the model. The paper also proposes a policy improvement guarantee for the proposed algorithm. The experimental results show that COMBO outperforms the baselines on a variety of offline RL tasks."
2125,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a model-based offline reinforcement learning algorithm called COMBO. The main idea of COMBO is to use a model to estimate the uncertainty of the value function of the policy, which is then used to train a policy that maximizes the expected return of the model. The paper also proposes a policy improvement guarantee for the proposed algorithm. The experimental results show that COMBO outperforms the baselines on a variety of offline RL tasks."
2126,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a model-based offline reinforcement learning algorithm called COMBO. The main idea of COMBO is to use a model to estimate the uncertainty of the value function of the policy, which is then used to train a policy that maximizes the expected return of the model. The paper also proposes a policy improvement guarantee for the proposed algorithm. The experimental results show that COMBO outperforms the baselines on a variety of offline RL tasks."
2127,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposes a model-based offline reinforcement learning algorithm called COMBO. The main idea of COMBO is to use a model to estimate the uncertainty of the value function of the policy, which is then used to train a policy that maximizes the expected return of the model. The paper also proposes a policy improvement guarantee for the proposed algorithm. The experimental results show that COMBO outperforms the baselines on a variety of offline RL tasks."
2128,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the effect of stochastic differential equations (SDEs) on the evolution of features in deep neural networks. The authors show that SDEs can cause a sharp phase transition in the feature space, leading to the neural collapse of the features. They also show that the vanishing training loss is responsible for this phenomenon. "
2129,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the effect of stochastic differential equations (SDEs) on the evolution of features in deep neural networks. The authors show that SDEs can cause a sharp phase transition in the feature space, leading to the neural collapse of the features. They also show that the vanishing training loss is responsible for this phenomenon. "
2130,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the effect of stochastic differential equations (SDEs) on the evolution of features in deep neural networks. The authors show that SDEs can cause a sharp phase transition in the feature space, leading to the neural collapse of the features. They also show that the vanishing training loss is responsible for this phenomenon. "
2131,SP:ca6f11ed297290e487890660d9a9a088aa106801,"This paper studies the effect of stochastic differential equations (SDEs) on the evolution of features in deep neural networks. The authors show that SDEs can cause a sharp phase transition in the feature space, leading to the neural collapse of the features. They also show that the vanishing training loss is responsible for this phenomenon. "
2132,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a two-stage learning scheme for learning programmatic policies. The first stage learns a program embedding space, which is then used to learn a policy. The second stage learns the policy from the learned embedding. The authors evaluate the proposed method on a variety of tasks and show that it outperforms the baselines."
2133,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a two-stage learning scheme for learning programmatic policies. The first stage learns a program embedding space, which is then used to learn a policy. The second stage learns the policy from the learned embedding. The authors evaluate the proposed method on a variety of tasks and show that it outperforms the baselines."
2134,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a two-stage learning scheme for learning programmatic policies. The first stage learns a program embedding space, which is then used to learn a policy. The second stage learns the policy from the learned embedding. The authors evaluate the proposed method on a variety of tasks and show that it outperforms the baselines."
2135,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper proposes a two-stage learning scheme for learning programmatic policies. The first stage learns a program embedding space, which is then used to learn a policy. The second stage learns the policy from the learned embedding. The authors evaluate the proposed method on a variety of tasks and show that it outperforms the baselines."
2136,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper proposes a method for physics-informed neural network (PINN) training. The main idea is to regularize the loss function of PINN with soft constraints on the empirical loss function. The proposed method is based on the idea of curriculum regularization, which is a regularization term that encourages the PINN to learn a curriculum of PDE-based differential operators. The method is evaluated on a sequence-to-sequence learning task."
2137,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper proposes a method for physics-informed neural network (PINN) training. The main idea is to regularize the loss function of PINN with soft constraints on the empirical loss function. The proposed method is based on the idea of curriculum regularization, which is a regularization term that encourages the PINN to learn a curriculum of PDE-based differential operators. The method is evaluated on a sequence-to-sequence learning task."
2138,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper proposes a method for physics-informed neural network (PINN) training. The main idea is to regularize the loss function of PINN with soft constraints on the empirical loss function. The proposed method is based on the idea of curriculum regularization, which is a regularization term that encourages the PINN to learn a curriculum of PDE-based differential operators. The method is evaluated on a sequence-to-sequence learning task."
2139,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"This paper proposes a method for physics-informed neural network (PINN) training. The main idea is to regularize the loss function of PINN with soft constraints on the empirical loss function. The proposed method is based on the idea of curriculum regularization, which is a regularization term that encourages the PINN to learn a curriculum of PDE-based differential operators. The method is evaluated on a sequence-to-sequence learning task."
2140,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a self-training algorithm for unsupervised domain adaptation (UDA). The proposed method is based on the idea that the source-trained classifier can be used to generate pseudo-labels for unlabeled target data and the target pseudo-label is used to train the classifier. The authors propose to use Tsallis entropy regularization to improve the confidence-friendly regularization of the model. Experiments show that the proposed method outperforms the state-of-the-art in both visual recognition and sentiment analysis benchmarks."
2141,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a self-training algorithm for unsupervised domain adaptation (UDA). The proposed method is based on the idea that the source-trained classifier can be used to generate pseudo-labels for unlabeled target data and the target pseudo-label is used to train the classifier. The authors propose to use Tsallis entropy regularization to improve the confidence-friendly regularization of the model. Experiments show that the proposed method outperforms the state-of-the-art in both visual recognition and sentiment analysis benchmarks."
2142,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a self-training algorithm for unsupervised domain adaptation (UDA). The proposed method is based on the idea that the source-trained classifier can be used to generate pseudo-labels for unlabeled target data and the target pseudo-label is used to train the classifier. The authors propose to use Tsallis entropy regularization to improve the confidence-friendly regularization of the model. Experiments show that the proposed method outperforms the state-of-the-art in both visual recognition and sentiment analysis benchmarks."
2143,SP:cfd501bca783590a78305f0592f537e8f20bce27,"This paper proposes Cycle Self-Training (CST), a self-training algorithm for unsupervised domain adaptation (UDA). The proposed method is based on the idea that the source-trained classifier can be used to generate pseudo-labels for unlabeled target data and the target pseudo-label is used to train the classifier. The authors propose to use Tsallis entropy regularization to improve the confidence-friendly regularization of the model. Experiments show that the proposed method outperforms the state-of-the-art in both visual recognition and sentiment analysis benchmarks."
2144,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured pruning method called DiscriminAtive Masking (DAM) for representation learning. The main idea of DAM is to use a discriminative masking layer to reduce the similarity between features of the original network and the masking layers of the pruned network. The proposed method is evaluated on a variety of tasks, including graph representation learning, graph pruning, recommendation system, and dimensionality reduction."
2145,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured pruning method called DiscriminAtive Masking (DAM) for representation learning. The main idea of DAM is to use a discriminative masking layer to reduce the similarity between features of the original network and the masking layers of the pruned network. The proposed method is evaluated on a variety of tasks, including graph representation learning, graph pruning, recommendation system, and dimensionality reduction."
2146,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured pruning method called DiscriminAtive Masking (DAM) for representation learning. The main idea of DAM is to use a discriminative masking layer to reduce the similarity between features of the original network and the masking layers of the pruned network. The proposed method is evaluated on a variety of tasks, including graph representation learning, graph pruning, recommendation system, and dimensionality reduction."
2147,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper proposes a single-stage structured pruning method called DiscriminAtive Masking (DAM) for representation learning. The main idea of DAM is to use a discriminative masking layer to reduce the similarity between features of the original network and the masking layers of the pruned network. The proposed method is evaluated on a variety of tasks, including graph representation learning, graph pruning, recommendation system, and dimensionality reduction."
2148,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a new neural network architecture, called Raven Progressive Matrices (RPM), which is a combination of a self-attention module and a system of modules. The authors claim that the proposed architecture is able to achieve better generalization performance than the state-of-the-art models on image classification and visual abstract reasoning tasks. The main contribution of the paper is the introduction of the Raven Progressive Matrix module, which is an extension of the original Raven Progressive matrix. The paper also proposes a capacity extension to the RPM module to allow for capacity expansion."
2149,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a new neural network architecture, called Raven Progressive Matrices (RPM), which is a combination of a self-attention module and a system of modules. The authors claim that the proposed architecture is able to achieve better generalization performance than the state-of-the-art models on image classification and visual abstract reasoning tasks. The main contribution of the paper is the introduction of the Raven Progressive Matrix module, which is an extension of the original Raven Progressive matrix. The paper also proposes a capacity extension to the RPM module to allow for capacity expansion."
2150,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a new neural network architecture, called Raven Progressive Matrices (RPM), which is a combination of a self-attention module and a system of modules. The authors claim that the proposed architecture is able to achieve better generalization performance than the state-of-the-art models on image classification and visual abstract reasoning tasks. The main contribution of the paper is the introduction of the Raven Progressive Matrix module, which is an extension of the original Raven Progressive matrix. The paper also proposes a capacity extension to the RPM module to allow for capacity expansion."
2151,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a new neural network architecture, called Raven Progressive Matrices (RPM), which is a combination of a self-attention module and a system of modules. The authors claim that the proposed architecture is able to achieve better generalization performance than the state-of-the-art models on image classification and visual abstract reasoning tasks. The main contribution of the paper is the introduction of the Raven Progressive Matrix module, which is an extension of the original Raven Progressive matrix. The paper also proposes a capacity extension to the RPM module to allow for capacity expansion."
2152,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique for unsupervised pre-training of policies for reinforcement learning. The main idea is to train a policy on a set of tasks and then fine-tune it on a new task. The authors show that the learned policy can transfer better to new tasks than the original policy trained on the same task. They also show that this can be achieved by fine-tuning the weights of the policy."
2153,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique for unsupervised pre-training of policies for reinforcement learning. The main idea is to train a policy on a set of tasks and then fine-tune it on a new task. The authors show that the learned policy can transfer better to new tasks than the original policy trained on the same task. They also show that this can be achieved by fine-tuning the weights of the policy."
2154,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique for unsupervised pre-training of policies for reinforcement learning. The main idea is to train a policy on a set of tasks and then fine-tune it on a new task. The authors show that the learned policy can transfer better to new tasks than the original policy trained on the same task. They also show that this can be achieved by fine-tuning the weights of the policy."
2155,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes Behavior Transfer (BT), a technique for unsupervised pre-training of policies for reinforcement learning. The main idea is to train a policy on a set of tasks and then fine-tune it on a new task. The authors show that the learned policy can transfer better to new tasks than the original policy trained on the same task. They also show that this can be achieved by fine-tuning the weights of the policy."
2156,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes a differentiable surrogate loss function for ranking. The authors propose a continuous, temperature-controlled relaxation of the sorting operator of PiRank, which can be used to compute differentiable surrogates for different ranking metrics. The proposed method is based on the divide-and-conquer extension of NeuralSort, which is a divide and conquer extension of the NeuralSort algorithm. The main contribution of this paper is to introduce a continuous temperature-control relaxation for the sorting operation in PiRank. Experiments show that the proposed method outperforms the state-of-the-art."
2157,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes a differentiable surrogate loss function for ranking. The authors propose a continuous, temperature-controlled relaxation of the sorting operator of PiRank, which can be used to compute differentiable surrogates for different ranking metrics. The proposed method is based on the divide-and-conquer extension of NeuralSort, which is a divide and conquer extension of the NeuralSort algorithm. The main contribution of this paper is to introduce a continuous temperature-control relaxation for the sorting operation in PiRank. Experiments show that the proposed method outperforms the state-of-the-art."
2158,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes a differentiable surrogate loss function for ranking. The authors propose a continuous, temperature-controlled relaxation of the sorting operator of PiRank, which can be used to compute differentiable surrogates for different ranking metrics. The proposed method is based on the divide-and-conquer extension of NeuralSort, which is a divide and conquer extension of the NeuralSort algorithm. The main contribution of this paper is to introduce a continuous temperature-control relaxation for the sorting operation in PiRank. Experiments show that the proposed method outperforms the state-of-the-art."
2159,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"This paper proposes a differentiable surrogate loss function for ranking. The authors propose a continuous, temperature-controlled relaxation of the sorting operator of PiRank, which can be used to compute differentiable surrogates for different ranking metrics. The proposed method is based on the divide-and-conquer extension of NeuralSort, which is a divide and conquer extension of the NeuralSort algorithm. The main contribution of this paper is to introduce a continuous temperature-control relaxation for the sorting operation in PiRank. Experiments show that the proposed method outperforms the state-of-the-art."
2160,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for the optimization of variational quantum entanglement (VQE) ansatz. In particular, the authors propose a curriculum learning method to learn the circuit depth of the VQE. The proposed algorithm is based on the feedback-driven curriculum learning algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art in terms of accuracy and complexity."
2161,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for the optimization of variational quantum entanglement (VQE) ansatz. In particular, the authors propose a curriculum learning method to learn the circuit depth of the VQE. The proposed algorithm is based on the feedback-driven curriculum learning algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art in terms of accuracy and complexity."
2162,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for the optimization of variational quantum entanglement (VQE) ansatz. In particular, the authors propose a curriculum learning method to learn the circuit depth of the VQE. The proposed algorithm is based on the feedback-driven curriculum learning algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art in terms of accuracy and complexity."
2163,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"This paper proposes a reinforcement learning algorithm for the optimization of variational quantum entanglement (VQE) ansatz. In particular, the authors propose a curriculum learning method to learn the circuit depth of the VQE. The proposed algorithm is based on the feedback-driven curriculum learning algorithm. The authors show that the proposed algorithm outperforms the state-of-the-art in terms of accuracy and complexity."
2164,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the problem of few-shot learning in the context of transductive inference. The authors propose a new method for learning the marginal probabilities of the label marginals of the unlabeled query set, which is based on a Dirichlet-distributed random variable. The main idea is to optimize the mutual information between the marginal probability of the query set and the class-balance of the class distribution. The proposed method, called α-divergence, is motivated by the observation that if the class distributions are not uniform, then it is not possible to learn a class-balanced model. To this end, the authors propose to use a simplex-based method to approximate the class balance of the model. They show that the proposed method outperforms the state-of-the-art methods on a variety of few shot benchmarks."
2165,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the problem of few-shot learning in the context of transductive inference. The authors propose a new method for learning the marginal probabilities of the label marginals of the unlabeled query set, which is based on a Dirichlet-distributed random variable. The main idea is to optimize the mutual information between the marginal probability of the query set and the class-balance of the class distribution. The proposed method, called α-divergence, is motivated by the observation that if the class distributions are not uniform, then it is not possible to learn a class-balanced model. To this end, the authors propose to use a simplex-based method to approximate the class balance of the model. They show that the proposed method outperforms the state-of-the-art methods on a variety of few shot benchmarks."
2166,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the problem of few-shot learning in the context of transductive inference. The authors propose a new method for learning the marginal probabilities of the label marginals of the unlabeled query set, which is based on a Dirichlet-distributed random variable. The main idea is to optimize the mutual information between the marginal probability of the query set and the class-balance of the class distribution. The proposed method, called α-divergence, is motivated by the observation that if the class distributions are not uniform, then it is not possible to learn a class-balanced model. To this end, the authors propose to use a simplex-based method to approximate the class balance of the model. They show that the proposed method outperforms the state-of-the-art methods on a variety of few shot benchmarks."
2167,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper studies the problem of few-shot learning in the context of transductive inference. The authors propose a new method for learning the marginal probabilities of the label marginals of the unlabeled query set, which is based on a Dirichlet-distributed random variable. The main idea is to optimize the mutual information between the marginal probability of the query set and the class-balance of the class distribution. The proposed method, called α-divergence, is motivated by the observation that if the class distributions are not uniform, then it is not possible to learn a class-balanced model. To this end, the authors propose to use a simplex-based method to approximate the class balance of the model. They show that the proposed method outperforms the state-of-the-art methods on a variety of few shot benchmarks."
2168,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a new method for patch-by-patch inference for tiny neural networks. The method is based on the idea that the receptive field of the network should be redistributed to reduce the number of FLOPs. The authors show that the proposed method is able to achieve better performance on ImageNet and TinyML compared to existing methods. The proposed method, called MCUNetV2, is evaluated on a variety of image classification tasks and is shown to outperform the state-of-the-art."
2169,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a new method for patch-by-patch inference for tiny neural networks. The method is based on the idea that the receptive field of the network should be redistributed to reduce the number of FLOPs. The authors show that the proposed method is able to achieve better performance on ImageNet and TinyML compared to existing methods. The proposed method, called MCUNetV2, is evaluated on a variety of image classification tasks and is shown to outperform the state-of-the-art."
2170,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a new method for patch-by-patch inference for tiny neural networks. The method is based on the idea that the receptive field of the network should be redistributed to reduce the number of FLOPs. The authors show that the proposed method is able to achieve better performance on ImageNet and TinyML compared to existing methods. The proposed method, called MCUNetV2, is evaluated on a variety of image classification tasks and is shown to outperform the state-of-the-art."
2171,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"This paper proposes a new method for patch-by-patch inference for tiny neural networks. The method is based on the idea that the receptive field of the network should be redistributed to reduce the number of FLOPs. The authors show that the proposed method is able to achieve better performance on ImageNet and TinyML compared to existing methods. The proposed method, called MCUNetV2, is evaluated on a variety of image classification tasks and is shown to outperform the state-of-the-art."
2172,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of designing a mechanism that maximizes the principal’s utility in an unstructured dynamic environment. The authors propose a linear program formulation for the problem, and provide a time complexity bound for the algorithm. They also provide an algorithm for memoryless mechanisms, and show that the proposed algorithm is computationally tractable."
2173,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of designing a mechanism that maximizes the principal’s utility in an unstructured dynamic environment. The authors propose a linear program formulation for the problem, and provide a time complexity bound for the algorithm. They also provide an algorithm for memoryless mechanisms, and show that the proposed algorithm is computationally tractable."
2174,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of designing a mechanism that maximizes the principal’s utility in an unstructured dynamic environment. The authors propose a linear program formulation for the problem, and provide a time complexity bound for the algorithm. They also provide an algorithm for memoryless mechanisms, and show that the proposed algorithm is computationally tractable."
2175,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"This paper studies the problem of designing a mechanism that maximizes the principal’s utility in an unstructured dynamic environment. The authors propose a linear program formulation for the problem, and provide a time complexity bound for the algorithm. They also provide an algorithm for memoryless mechanisms, and show that the proposed algorithm is computationally tractable."
2176,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper proposes a new architecture search method for graph neural networks (GNNs) based on graph structure optimization. The main idea is to use a differentiable architecture search model to find the optimal graph structure for each node in the graph. The proposed method is based on the idea of denoising the graph structure of a graph and then using the denoised graph as a basis for the search procedure. The method is evaluated on several graph datasets and compared to several baselines.
2177,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper proposes a new architecture search method for graph neural networks (GNNs) based on graph structure optimization. The main idea is to use a differentiable architecture search model to find the optimal graph structure for each node in the graph. The proposed method is based on the idea of denoising the graph structure of a graph and then using the denoised graph as a basis for the search procedure. The method is evaluated on several graph datasets and compared to several baselines.
2178,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper proposes a new architecture search method for graph neural networks (GNNs) based on graph structure optimization. The main idea is to use a differentiable architecture search model to find the optimal graph structure for each node in the graph. The proposed method is based on the idea of denoising the graph structure of a graph and then using the denoised graph as a basis for the search procedure. The method is evaluated on several graph datasets and compared to several baselines.
2179,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,This paper proposes a new architecture search method for graph neural networks (GNNs) based on graph structure optimization. The main idea is to use a differentiable architecture search model to find the optimal graph structure for each node in the graph. The proposed method is based on the idea of denoising the graph structure of a graph and then using the denoised graph as a basis for the search procedure. The method is evaluated on several graph datasets and compared to several baselines.
2180,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fair clustering in the context of group membership. The authors propose a new objective function, called the group leximin objective, which is a combination of two existing fairness objectives, namely the group utilitarian objective and the group egalitarian objective. The main contribution of this paper is to provide a lower bound on the cost of the fairness objective and a heuristic algorithm for the egalitarian and utilitarian objectives. The paper also provides an upper bound for the cost for the group fairness objective."
2181,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fair clustering in the context of group membership. The authors propose a new objective function, called the group leximin objective, which is a combination of two existing fairness objectives, namely the group utilitarian objective and the group egalitarian objective. The main contribution of this paper is to provide a lower bound on the cost of the fairness objective and a heuristic algorithm for the egalitarian and utilitarian objectives. The paper also provides an upper bound for the cost for the group fairness objective."
2182,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fair clustering in the context of group membership. The authors propose a new objective function, called the group leximin objective, which is a combination of two existing fairness objectives, namely the group utilitarian objective and the group egalitarian objective. The main contribution of this paper is to provide a lower bound on the cost of the fairness objective and a heuristic algorithm for the egalitarian and utilitarian objectives. The paper also provides an upper bound for the cost for the group fairness objective."
2183,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"This paper studies the problem of fair clustering in the context of group membership. The authors propose a new objective function, called the group leximin objective, which is a combination of two existing fairness objectives, namely the group utilitarian objective and the group egalitarian objective. The main contribution of this paper is to provide a lower bound on the cost of the fairness objective and a heuristic algorithm for the egalitarian and utilitarian objectives. The paper also provides an upper bound for the cost for the group fairness objective."
2184,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,This paper proposes a generative model for graph statistics. The proposed model is based on edge-independent random graph models. The authors show that the proposed model can be used to generate graphs with high triangle density and high accuracy. The paper also provides a theoretical analysis of the overlap between the generated graphs and the original graph. 
2185,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,This paper proposes a generative model for graph statistics. The proposed model is based on edge-independent random graph models. The authors show that the proposed model can be used to generate graphs with high triangle density and high accuracy. The paper also provides a theoretical analysis of the overlap between the generated graphs and the original graph. 
2186,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,This paper proposes a generative model for graph statistics. The proposed model is based on edge-independent random graph models. The authors show that the proposed model can be used to generate graphs with high triangle density and high accuracy. The paper also provides a theoretical analysis of the overlap between the generated graphs and the original graph. 
2187,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,This paper proposes a generative model for graph statistics. The proposed model is based on edge-independent random graph models. The authors show that the proposed model can be used to generate graphs with high triangle density and high accuracy. The paper also provides a theoretical analysis of the overlap between the generated graphs and the original graph. 
2188,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the problem of algorithmic differentiation of nonsmooth problems in deep learning. The authors propose to use double precision (ReLU) instead of ReLU(0) as the default precision for backpropagation. They show that the double precision can be used to improve the test accuracy of deep learning models. They also show that using double precision is equivalent to using batch norm and batch norm-norm-adam. They evaluate their method on MNIST, CIFAR-10, SVHN, and ImageNet."
2189,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the problem of algorithmic differentiation of nonsmooth problems in deep learning. The authors propose to use double precision (ReLU) instead of ReLU(0) as the default precision for backpropagation. They show that the double precision can be used to improve the test accuracy of deep learning models. They also show that using double precision is equivalent to using batch norm and batch norm-norm-adam. They evaluate their method on MNIST, CIFAR-10, SVHN, and ImageNet."
2190,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the problem of algorithmic differentiation of nonsmooth problems in deep learning. The authors propose to use double precision (ReLU) instead of ReLU(0) as the default precision for backpropagation. They show that the double precision can be used to improve the test accuracy of deep learning models. They also show that using double precision is equivalent to using batch norm and batch norm-norm-adam. They evaluate their method on MNIST, CIFAR-10, SVHN, and ImageNet."
2191,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper studies the problem of algorithmic differentiation of nonsmooth problems in deep learning. The authors propose to use double precision (ReLU) instead of ReLU(0) as the default precision for backpropagation. They show that the double precision can be used to improve the test accuracy of deep learning models. They also show that using double precision is equivalent to using batch norm and batch norm-norm-adam. They evaluate their method on MNIST, CIFAR-10, SVHN, and ImageNet."
2192,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method for minimizing information in reinforcement learning (RL) algorithms. The main idea is to learn a latent-space model for the policy, and then use the latent space model to estimate the reward of the policy. The authors show that this method can be used to reduce the information bottleneck in RL algorithms. They also show that the proposed method can achieve better performance than previous methods."
2193,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method for minimizing information in reinforcement learning (RL) algorithms. The main idea is to learn a latent-space model for the policy, and then use the latent space model to estimate the reward of the policy. The authors show that this method can be used to reduce the information bottleneck in RL algorithms. They also show that the proposed method can achieve better performance than previous methods."
2194,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method for minimizing information in reinforcement learning (RL) algorithms. The main idea is to learn a latent-space model for the policy, and then use the latent space model to estimate the reward of the policy. The authors show that this method can be used to reduce the information bottleneck in RL algorithms. They also show that the proposed method can achieve better performance than previous methods."
2195,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"This paper proposes a method for minimizing information in reinforcement learning (RL) algorithms. The main idea is to learn a latent-space model for the policy, and then use the latent space model to estimate the reward of the policy. The authors show that this method can be used to reduce the information bottleneck in RL algorithms. They also show that the proposed method can achieve better performance than previous methods."
2196,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a spectral attention network (SAN) for graph neural networks (GNNs). The main idea is to use the full Laplacian spectrum to encode node features and then use the learned positional encoding (LPE) to extract node features from node features. The proposed model is evaluated on several graph datasets and compared to state-of-the-art GNNs.
2197,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a spectral attention network (SAN) for graph neural networks (GNNs). The main idea is to use the full Laplacian spectrum to encode node features and then use the learned positional encoding (LPE) to extract node features from node features. The proposed model is evaluated on several graph datasets and compared to state-of-the-art GNNs.
2198,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a spectral attention network (SAN) for graph neural networks (GNNs). The main idea is to use the full Laplacian spectrum to encode node features and then use the learned positional encoding (LPE) to extract node features from node features. The proposed model is evaluated on several graph datasets and compared to state-of-the-art GNNs.
2199,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,This paper proposes a spectral attention network (SAN) for graph neural networks (GNNs). The main idea is to use the full Laplacian spectrum to encode node features and then use the learned positional encoding (LPE) to extract node features from node features. The proposed model is evaluated on several graph datasets and compared to state-of-the-art GNNs.
2200,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,This paper studies the problem of two-alternate elections in which the state variable is unknown to the voters. The authors propose a Bayesian Nash equilibrium for this problem. The main contribution of this paper is to show that the Bayes Nash equilibrium can be obtained by considering the private information of the voters and the private signals of the state variables. The paper is well-written and easy to follow. 
2201,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,This paper studies the problem of two-alternate elections in which the state variable is unknown to the voters. The authors propose a Bayesian Nash equilibrium for this problem. The main contribution of this paper is to show that the Bayes Nash equilibrium can be obtained by considering the private information of the voters and the private signals of the state variables. The paper is well-written and easy to follow. 
2202,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,This paper studies the problem of two-alternate elections in which the state variable is unknown to the voters. The authors propose a Bayesian Nash equilibrium for this problem. The main contribution of this paper is to show that the Bayes Nash equilibrium can be obtained by considering the private information of the voters and the private signals of the state variables. The paper is well-written and easy to follow. 
2203,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,This paper studies the problem of two-alternate elections in which the state variable is unknown to the voters. The authors propose a Bayesian Nash equilibrium for this problem. The main contribution of this paper is to show that the Bayes Nash equilibrium can be obtained by considering the private information of the voters and the private signals of the state variables. The paper is well-written and easy to follow. 
2204,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,This paper studies the problem of finding the Hessian of a neural network with overparameterized parameters. The authors consider the case of deep linear networks with rectified and hyperbolic tangent networks. The main contribution of this paper is to provide a theoretical analysis of the rank deficiency of the neural network. The paper is well-written and well-motivated. The theoretical analysis is well motivated and the paper is easy to follow. 
2205,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,This paper studies the problem of finding the Hessian of a neural network with overparameterized parameters. The authors consider the case of deep linear networks with rectified and hyperbolic tangent networks. The main contribution of this paper is to provide a theoretical analysis of the rank deficiency of the neural network. The paper is well-written and well-motivated. The theoretical analysis is well motivated and the paper is easy to follow. 
2206,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,This paper studies the problem of finding the Hessian of a neural network with overparameterized parameters. The authors consider the case of deep linear networks with rectified and hyperbolic tangent networks. The main contribution of this paper is to provide a theoretical analysis of the rank deficiency of the neural network. The paper is well-written and well-motivated. The theoretical analysis is well motivated and the paper is easy to follow. 
2207,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,This paper studies the problem of finding the Hessian of a neural network with overparameterized parameters. The authors consider the case of deep linear networks with rectified and hyperbolic tangent networks. The main contribution of this paper is to provide a theoretical analysis of the rank deficiency of the neural network. The paper is well-written and well-motivated. The theoretical analysis is well motivated and the paper is easy to follow. 
2208,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new disentangled representation metric for linear symmetric-based disentanglement (LSBD) methods. The proposed metric is based on a VAE-based VAE, which is a semi-supervised 6-layer VAE. The authors show that the proposed metric can be used to evaluate the performance of LSBD-VAE and DLSBD. The paper also provides a theoretical analysis of the proposed method."
2209,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new disentangled representation metric for linear symmetric-based disentanglement (LSBD) methods. The proposed metric is based on a VAE-based VAE, which is a semi-supervised 6-layer VAE. The authors show that the proposed metric can be used to evaluate the performance of LSBD-VAE and DLSBD. The paper also provides a theoretical analysis of the proposed method."
2210,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new disentangled representation metric for linear symmetric-based disentanglement (LSBD) methods. The proposed metric is based on a VAE-based VAE, which is a semi-supervised 6-layer VAE. The authors show that the proposed metric can be used to evaluate the performance of LSBD-VAE and DLSBD. The paper also provides a theoretical analysis of the proposed method."
2211,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"This paper proposes a new disentangled representation metric for linear symmetric-based disentanglement (LSBD) methods. The proposed metric is based on a VAE-based VAE, which is a semi-supervised 6-layer VAE. The authors show that the proposed metric can be used to evaluate the performance of LSBD-VAE and DLSBD. The paper also provides a theoretical analysis of the proposed method."
2212,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,This paper proposes an extension of the extended Kalman VAE (EKVAE) framework for deep state-space models (DSSMs). The main idea is to use amortized variational inference and Bayesian filtering/smoothing to improve the performance of DSSMs. The authors also propose a constrained optimisation framework to optimize the evidence lower bound of the DSSM. Empirical results show that the proposed method outperforms the state-of-the-art RNN-based models.
2213,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,This paper proposes an extension of the extended Kalman VAE (EKVAE) framework for deep state-space models (DSSMs). The main idea is to use amortized variational inference and Bayesian filtering/smoothing to improve the performance of DSSMs. The authors also propose a constrained optimisation framework to optimize the evidence lower bound of the DSSM. Empirical results show that the proposed method outperforms the state-of-the-art RNN-based models.
2214,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,This paper proposes an extension of the extended Kalman VAE (EKVAE) framework for deep state-space models (DSSMs). The main idea is to use amortized variational inference and Bayesian filtering/smoothing to improve the performance of DSSMs. The authors also propose a constrained optimisation framework to optimize the evidence lower bound of the DSSM. Empirical results show that the proposed method outperforms the state-of-the-art RNN-based models.
2215,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,This paper proposes an extension of the extended Kalman VAE (EKVAE) framework for deep state-space models (DSSMs). The main idea is to use amortized variational inference and Bayesian filtering/smoothing to improve the performance of DSSMs. The authors also propose a constrained optimisation framework to optimize the evidence lower bound of the DSSM. Empirical results show that the proposed method outperforms the state-of-the-art RNN-based models.
2216,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The method is based on deep inversion (DIC), which is an extension of Deep Inversion (DIR). The main idea of DIC is to train a classifier to predict the class label of a given image, and then to use the predicted class label to generate a set of images that are similar to the original image. DIC can then be used to generate images with different class labels, which are then used to train the classifier."
2217,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The method is based on deep inversion (DIC), which is an extension of Deep Inversion (DIR). The main idea of DIC is to train a classifier to predict the class label of a given image, and then to use the predicted class label to generate a set of images that are similar to the original image. DIC can then be used to generate images with different class labels, which are then used to train the classifier."
2218,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The method is based on deep inversion (DIC), which is an extension of Deep Inversion (DIR). The main idea of DIC is to train a classifier to predict the class label of a given image, and then to use the predicted class label to generate a set of images that are similar to the original image. DIC can then be used to generate images with different class labels, which are then used to train the classifier."
2219,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"This paper proposes a method for generating counterfactual explanations for black-box models. The method is based on deep inversion (DIC), which is an extension of Deep Inversion (DIR). The main idea of DIC is to train a classifier to predict the class label of a given image, and then to use the predicted class label to generate a set of images that are similar to the original image. DIC can then be used to generate images with different class labels, which are then used to train the classifier."
2220,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,This paper studies the problem of causal inference in the context of drug-related offenses. The authors propose a new algorithm for causal inference based on the notion of heterogeneity. The main contribution of this paper is to provide a generalization bound on the generalization error of the proposed algorithm. This bound is based on a theoretical analysis of the causal inference problem. The paper also provides an empirical evaluation of the performance of the algorithm.
2221,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,This paper studies the problem of causal inference in the context of drug-related offenses. The authors propose a new algorithm for causal inference based on the notion of heterogeneity. The main contribution of this paper is to provide a generalization bound on the generalization error of the proposed algorithm. This bound is based on a theoretical analysis of the causal inference problem. The paper also provides an empirical evaluation of the performance of the algorithm.
2222,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,This paper studies the problem of causal inference in the context of drug-related offenses. The authors propose a new algorithm for causal inference based on the notion of heterogeneity. The main contribution of this paper is to provide a generalization bound on the generalization error of the proposed algorithm. This bound is based on a theoretical analysis of the causal inference problem. The paper also provides an empirical evaluation of the performance of the algorithm.
2223,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,This paper studies the problem of causal inference in the context of drug-related offenses. The authors propose a new algorithm for causal inference based on the notion of heterogeneity. The main contribution of this paper is to provide a generalization bound on the generalization error of the proposed algorithm. This bound is based on a theoretical analysis of the causal inference problem. The paper also provides an empirical evaluation of the performance of the algorithm.
2224,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based generative model for image synthesis. The key idea is to use a transformer to generate visual tokens and style tokens, which are then combined with the content tokens to generate the final image. The proposed method is evaluated on FFHQ and LSUN Church."
2225,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based generative model for image synthesis. The key idea is to use a transformer to generate visual tokens and style tokens, which are then combined with the content tokens to generate the final image. The proposed method is evaluated on FFHQ and LSUN Church."
2226,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based generative model for image synthesis. The key idea is to use a transformer to generate visual tokens and style tokens, which are then combined with the content tokens to generate the final image. The proposed method is evaluated on FFHQ and LSUN Church."
2227,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper proposes a token-based generative model for image synthesis. The key idea is to use a transformer to generate visual tokens and style tokens, which are then combined with the content tokens to generate the final image. The proposed method is evaluated on FFHQ and LSUN Church."
2228,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the problem of robust overfitting of min-norm interpolators and max-margin classifiers in the presence of noise. The authors propose ridge regularization to avoid interpolation and show that it can be used to reduce the variance of interpolation. They also provide a theoretical analysis of the robustness of the proposed ridge regularizer. Finally, they provide empirical results on linear regression and classification."
2229,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the problem of robust overfitting of min-norm interpolators and max-margin classifiers in the presence of noise. The authors propose ridge regularization to avoid interpolation and show that it can be used to reduce the variance of interpolation. They also provide a theoretical analysis of the robustness of the proposed ridge regularizer. Finally, they provide empirical results on linear regression and classification."
2230,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the problem of robust overfitting of min-norm interpolators and max-margin classifiers in the presence of noise. The authors propose ridge regularization to avoid interpolation and show that it can be used to reduce the variance of interpolation. They also provide a theoretical analysis of the robustness of the proposed ridge regularizer. Finally, they provide empirical results on linear regression and classification."
2231,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies the problem of robust overfitting of min-norm interpolators and max-margin classifiers in the presence of noise. The authors propose ridge regularization to avoid interpolation and show that it can be used to reduce the variance of interpolation. They also provide a theoretical analysis of the robustness of the proposed ridge regularizer. Finally, they provide empirical results on linear regression and classification."
2232,SP:09f080f47db81b513af26add851822c5c32bb94e,This paper proposes a canonical point autoencoder (CPAE) for 3D point clouds. The proposed method is based on the idea that the canonical surface of a point cloud can be represented as a sphere. The authors also propose a self-supervised part segmentation network. The experimental results show that the proposed method outperforms the state-of-the-art correspondence learning methods.
2233,SP:09f080f47db81b513af26add851822c5c32bb94e,This paper proposes a canonical point autoencoder (CPAE) for 3D point clouds. The proposed method is based on the idea that the canonical surface of a point cloud can be represented as a sphere. The authors also propose a self-supervised part segmentation network. The experimental results show that the proposed method outperforms the state-of-the-art correspondence learning methods.
2234,SP:09f080f47db81b513af26add851822c5c32bb94e,This paper proposes a canonical point autoencoder (CPAE) for 3D point clouds. The proposed method is based on the idea that the canonical surface of a point cloud can be represented as a sphere. The authors also propose a self-supervised part segmentation network. The experimental results show that the proposed method outperforms the state-of-the-art correspondence learning methods.
2235,SP:09f080f47db81b513af26add851822c5c32bb94e,This paper proposes a canonical point autoencoder (CPAE) for 3D point clouds. The proposed method is based on the idea that the canonical surface of a point cloud can be represented as a sphere. The authors also propose a self-supervised part segmentation network. The experimental results show that the proposed method outperforms the state-of-the-art correspondence learning methods.
2236,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,This paper proposes a new method for domain generalization (DG) based on Stochastic Weight Averaging Densely (SWAD). The main idea of SWAD is to minimize the distance between the minima of the loss function of the original DG method and the loss of the new DG method. The main contribution of this paper is to show that SWAD can achieve a flat minima in the case of sharp minima. The paper also provides a theoretical analysis of the generalization gap between SWAD and vanilla SWA.
2237,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,This paper proposes a new method for domain generalization (DG) based on Stochastic Weight Averaging Densely (SWAD). The main idea of SWAD is to minimize the distance between the minima of the loss function of the original DG method and the loss of the new DG method. The main contribution of this paper is to show that SWAD can achieve a flat minima in the case of sharp minima. The paper also provides a theoretical analysis of the generalization gap between SWAD and vanilla SWA.
2238,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,This paper proposes a new method for domain generalization (DG) based on Stochastic Weight Averaging Densely (SWAD). The main idea of SWAD is to minimize the distance between the minima of the loss function of the original DG method and the loss of the new DG method. The main contribution of this paper is to show that SWAD can achieve a flat minima in the case of sharp minima. The paper also provides a theoretical analysis of the generalization gap between SWAD and vanilla SWA.
2239,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,This paper proposes a new method for domain generalization (DG) based on Stochastic Weight Averaging Densely (SWAD). The main idea of SWAD is to minimize the distance between the minima of the loss function of the original DG method and the loss of the new DG method. The main contribution of this paper is to show that SWAD can achieve a flat minima in the case of sharp minima. The paper also provides a theoretical analysis of the generalization gap between SWAD and vanilla SWA.
2240,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,This paper studies the problem of predicting the performance of neural architecture search (NAS) methods. The authors propose a method to estimate the predictive power of predictor-based NAS frameworks based on correlation and rank-based performance measures. The method is based on a combination of two techniques: (1) learning curve extrapolation and (2) weight-sharing. The experiments show that the proposed method outperforms the baselines on a variety of NAS benchmarks.
2241,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,This paper studies the problem of predicting the performance of neural architecture search (NAS) methods. The authors propose a method to estimate the predictive power of predictor-based NAS frameworks based on correlation and rank-based performance measures. The method is based on a combination of two techniques: (1) learning curve extrapolation and (2) weight-sharing. The experiments show that the proposed method outperforms the baselines on a variety of NAS benchmarks.
2242,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,This paper studies the problem of predicting the performance of neural architecture search (NAS) methods. The authors propose a method to estimate the predictive power of predictor-based NAS frameworks based on correlation and rank-based performance measures. The method is based on a combination of two techniques: (1) learning curve extrapolation and (2) weight-sharing. The experiments show that the proposed method outperforms the baselines on a variety of NAS benchmarks.
2243,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,This paper studies the problem of predicting the performance of neural architecture search (NAS) methods. The authors propose a method to estimate the predictive power of predictor-based NAS frameworks based on correlation and rank-based performance measures. The method is based on a combination of two techniques: (1) learning curve extrapolation and (2) weight-sharing. The experiments show that the proposed method outperforms the baselines on a variety of NAS benchmarks.
2244,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the privacy of posterior sampling in the exponential family of Dirichlet posterior sampling. In particular, the authors propose a truncated concentrated differential privacy (tCDP) algorithm for posterior sampling, which is based on truncated differential privacy. The authors show that the proposed tCDP can be used to improve the accuracy and privacy of the posterior sampling algorithm. The paper also provides a theoretical analysis of the privacy guarantee of the proposed algorithm."
2245,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the privacy of posterior sampling in the exponential family of Dirichlet posterior sampling. In particular, the authors propose a truncated concentrated differential privacy (tCDP) algorithm for posterior sampling, which is based on truncated differential privacy. The authors show that the proposed tCDP can be used to improve the accuracy and privacy of the posterior sampling algorithm. The paper also provides a theoretical analysis of the privacy guarantee of the proposed algorithm."
2246,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the privacy of posterior sampling in the exponential family of Dirichlet posterior sampling. In particular, the authors propose a truncated concentrated differential privacy (tCDP) algorithm for posterior sampling, which is based on truncated differential privacy. The authors show that the proposed tCDP can be used to improve the accuracy and privacy of the posterior sampling algorithm. The paper also provides a theoretical analysis of the privacy guarantee of the proposed algorithm."
2247,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper studies the privacy of posterior sampling in the exponential family of Dirichlet posterior sampling. In particular, the authors propose a truncated concentrated differential privacy (tCDP) algorithm for posterior sampling, which is based on truncated differential privacy. The authors show that the proposed tCDP can be used to improve the accuracy and privacy of the posterior sampling algorithm. The paper also provides a theoretical analysis of the privacy guarantee of the proposed algorithm."
2248,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for parallel local clustering based on random walks. The algorithm is based on the idea of random walks, which is an extension of the random walk algorithm proposed by [1]. The main difference is that instead of using random walks for each node in the graph, the authors propose to use a random walk for all nodes. The authors show that the proposed algorithm can be applied to both parallel and non-parallel settings. The experimental results show that their algorithm outperforms the state of the art."
2249,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for parallel local clustering based on random walks. The algorithm is based on the idea of random walks, which is an extension of the random walk algorithm proposed by [1]. The main difference is that instead of using random walks for each node in the graph, the authors propose to use a random walk for all nodes. The authors show that the proposed algorithm can be applied to both parallel and non-parallel settings. The experimental results show that their algorithm outperforms the state of the art."
2250,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for parallel local clustering based on random walks. The algorithm is based on the idea of random walks, which is an extension of the random walk algorithm proposed by [1]. The main difference is that instead of using random walks for each node in the graph, the authors propose to use a random walk for all nodes. The authors show that the proposed algorithm can be applied to both parallel and non-parallel settings. The experimental results show that their algorithm outperforms the state of the art."
2251,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This paper proposes a new algorithm for parallel local clustering based on random walks. The algorithm is based on the idea of random walks, which is an extension of the random walk algorithm proposed by [1]. The main difference is that instead of using random walks for each node in the graph, the authors propose to use a random walk for all nodes. The authors show that the proposed algorithm can be applied to both parallel and non-parallel settings. The experimental results show that their algorithm outperforms the state of the art."
2252,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper studies the problem of model selection for random regular graphs with paramagnetic phase. The authors propose a replica method for model selection based on the 1-LinR method. The main contribution of this paper is to show that the sample complexity of the proposed method is $O(1/\epsilon^2)$, which is a factor of $\mathcal{O}(\sqrt{T})$, where $T$ is the number of samples required to select a model from a set of models. The paper also provides a theoretical analysis of the non-asymptotic behavior of the method."
2253,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper studies the problem of model selection for random regular graphs with paramagnetic phase. The authors propose a replica method for model selection based on the 1-LinR method. The main contribution of this paper is to show that the sample complexity of the proposed method is $O(1/\epsilon^2)$, which is a factor of $\mathcal{O}(\sqrt{T})$, where $T$ is the number of samples required to select a model from a set of models. The paper also provides a theoretical analysis of the non-asymptotic behavior of the method."
2254,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper studies the problem of model selection for random regular graphs with paramagnetic phase. The authors propose a replica method for model selection based on the 1-LinR method. The main contribution of this paper is to show that the sample complexity of the proposed method is $O(1/\epsilon^2)$, which is a factor of $\mathcal{O}(\sqrt{T})$, where $T$ is the number of samples required to select a model from a set of models. The paper also provides a theoretical analysis of the non-asymptotic behavior of the method."
2255,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper studies the problem of model selection for random regular graphs with paramagnetic phase. The authors propose a replica method for model selection based on the 1-LinR method. The main contribution of this paper is to show that the sample complexity of the proposed method is $O(1/\epsilon^2)$, which is a factor of $\mathcal{O}(\sqrt{T})$, where $T$ is the number of samples required to select a model from a set of models. The paper also provides a theoretical analysis of the non-asymptotic behavior of the method."
2256,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the problem of active clustering for k-means clustering, where the goal is to find a cluster that maximizes the similarity between a set of k samples. The authors propose a new algorithm for this problem, which is based on Lloyd-type algorithms and alternating-minimization algorithms. They show that the algorithm is polynomial in the number of queries needed to find the optimal cluster. They also provide a theoretical analysis of the convergence rate of the algorithm."
2257,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the problem of active clustering for k-means clustering, where the goal is to find a cluster that maximizes the similarity between a set of k samples. The authors propose a new algorithm for this problem, which is based on Lloyd-type algorithms and alternating-minimization algorithms. They show that the algorithm is polynomial in the number of queries needed to find the optimal cluster. They also provide a theoretical analysis of the convergence rate of the algorithm."
2258,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the problem of active clustering for k-means clustering, where the goal is to find a cluster that maximizes the similarity between a set of k samples. The authors propose a new algorithm for this problem, which is based on Lloyd-type algorithms and alternating-minimization algorithms. They show that the algorithm is polynomial in the number of queries needed to find the optimal cluster. They also provide a theoretical analysis of the convergence rate of the algorithm."
2259,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This paper studies the problem of active clustering for k-means clustering, where the goal is to find a cluster that maximizes the similarity between a set of k samples. The authors propose a new algorithm for this problem, which is based on Lloyd-type algorithms and alternating-minimization algorithms. They show that the algorithm is polynomial in the number of queries needed to find the optimal cluster. They also provide a theoretical analysis of the convergence rate of the algorithm."
2260,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a model-based reinforcement learning (RL) algorithm for planning. The main idea is to train a model to predict the value function of a policy, which is then used to update the policy in a way that is consistent with the model. The authors show that this approach can be applied to both tabular and function approximation settings. They also show that the proposed algorithm can be used for policy evaluation and control."
2261,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a model-based reinforcement learning (RL) algorithm for planning. The main idea is to train a model to predict the value function of a policy, which is then used to update the policy in a way that is consistent with the model. The authors show that this approach can be applied to both tabular and function approximation settings. They also show that the proposed algorithm can be used for policy evaluation and control."
2262,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a model-based reinforcement learning (RL) algorithm for planning. The main idea is to train a model to predict the value function of a policy, which is then used to update the policy in a way that is consistent with the model. The authors show that this approach can be applied to both tabular and function approximation settings. They also show that the proposed algorithm can be used for policy evaluation and control."
2263,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This paper proposes a model-based reinforcement learning (RL) algorithm for planning. The main idea is to train a model to predict the value function of a policy, which is then used to update the policy in a way that is consistent with the model. The authors show that this approach can be applied to both tabular and function approximation settings. They also show that the proposed algorithm can be used for policy evaluation and control."
2264,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,This paper proposes a sampling method for episodic training of few-shot learning models. The proposed method is based on the idea that the difficulty of each episode should be determined by the number of episodes in the episode. The authors propose a sampling scheme based on a weighted average of the episode difficulty and show that it can be used to estimate the episode sampling distribution. The method is evaluated on a variety of datasets and algorithms.
2265,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,This paper proposes a sampling method for episodic training of few-shot learning models. The proposed method is based on the idea that the difficulty of each episode should be determined by the number of episodes in the episode. The authors propose a sampling scheme based on a weighted average of the episode difficulty and show that it can be used to estimate the episode sampling distribution. The method is evaluated on a variety of datasets and algorithms.
2266,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,This paper proposes a sampling method for episodic training of few-shot learning models. The proposed method is based on the idea that the difficulty of each episode should be determined by the number of episodes in the episode. The authors propose a sampling scheme based on a weighted average of the episode difficulty and show that it can be used to estimate the episode sampling distribution. The method is evaluated on a variety of datasets and algorithms.
2267,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,This paper proposes a sampling method for episodic training of few-shot learning models. The proposed method is based on the idea that the difficulty of each episode should be determined by the number of episodes in the episode. The authors propose a sampling scheme based on a weighted average of the episode difficulty and show that it can be used to estimate the episode sampling distribution. The method is evaluated on a variety of datasets and algorithms.
2268,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of multi-inomial logit (MNL) bandit with unknown parameters. The authors propose a UCB-based algorithm for this problem. The main contribution of this paper is the extension of the UCB algorithm to the multi-parameter bandit setting. In particular, the authors show that the proposed UCB can be extended to the case where the parameters of the bandit are unknown."
2269,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of multi-inomial logit (MNL) bandit with unknown parameters. The authors propose a UCB-based algorithm for this problem. The main contribution of this paper is the extension of the UCB algorithm to the multi-parameter bandit setting. In particular, the authors show that the proposed UCB can be extended to the case where the parameters of the bandit are unknown."
2270,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of multi-inomial logit (MNL) bandit with unknown parameters. The authors propose a UCB-based algorithm for this problem. The main contribution of this paper is the extension of the UCB algorithm to the multi-parameter bandit setting. In particular, the authors show that the proposed UCB can be extended to the case where the parameters of the bandit are unknown."
2271,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"This paper studies the problem of multi-inomial logit (MNL) bandit with unknown parameters. The authors propose a UCB-based algorithm for this problem. The main contribution of this paper is the extension of the UCB algorithm to the multi-parameter bandit setting. In particular, the authors show that the proposed UCB can be extended to the case where the parameters of the bandit are unknown."
2272,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a soft actor-critic (SAC) algorithm for maximum-entropy reinforcement learning. The main idea of SAC is to learn a policy that maximizes the entropy of low-entropic states, and then use this policy to explore new states. The authors show that SAC can be applied to a variety of MDPs."
2273,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a soft actor-critic (SAC) algorithm for maximum-entropy reinforcement learning. The main idea of SAC is to learn a policy that maximizes the entropy of low-entropic states, and then use this policy to explore new states. The authors show that SAC can be applied to a variety of MDPs."
2274,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a soft actor-critic (SAC) algorithm for maximum-entropy reinforcement learning. The main idea of SAC is to learn a policy that maximizes the entropy of low-entropic states, and then use this policy to explore new states. The authors show that SAC can be applied to a variety of MDPs."
2275,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper proposes a soft actor-critic (SAC) algorithm for maximum-entropy reinforcement learning. The main idea of SAC is to learn a policy that maximizes the entropy of low-entropic states, and then use this policy to explore new states. The authors show that SAC can be applied to a variety of MDPs."
2276,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper proposes a method for learning a sequence of continuous-time Markov chains (CMCs) from cross-sectional data. The proposed method is based on approximate likelihood maximization (ELM), which is a well-known method for estimating the likelihood of the next time step of a Markov chain. The authors show that the proposed method can be applied to CMCs in the context of biomedical applications, and show that it outperforms existing methods on both synthetic and real-world datasets."
2277,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper proposes a method for learning a sequence of continuous-time Markov chains (CMCs) from cross-sectional data. The proposed method is based on approximate likelihood maximization (ELM), which is a well-known method for estimating the likelihood of the next time step of a Markov chain. The authors show that the proposed method can be applied to CMCs in the context of biomedical applications, and show that it outperforms existing methods on both synthetic and real-world datasets."
2278,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper proposes a method for learning a sequence of continuous-time Markov chains (CMCs) from cross-sectional data. The proposed method is based on approximate likelihood maximization (ELM), which is a well-known method for estimating the likelihood of the next time step of a Markov chain. The authors show that the proposed method can be applied to CMCs in the context of biomedical applications, and show that it outperforms existing methods on both synthetic and real-world datasets."
2279,SP:19107a648d3d23403a8693b065ee842833a0b893,"This paper proposes a method for learning a sequence of continuous-time Markov chains (CMCs) from cross-sectional data. The proposed method is based on approximate likelihood maximization (ELM), which is a well-known method for estimating the likelihood of the next time step of a Markov chain. The authors show that the proposed method can be applied to CMCs in the context of biomedical applications, and show that it outperforms existing methods on both synthetic and real-world datasets."
2280,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a novel self-supervised pretraining method for document understanding. The proposed method is based on the Transformer architecture. The main idea is to learn a joint representation of words and visual features, which is then used to train a Transformer-based model for document classification. The paper also proposes a self-training procedure for downstream tasks. The experiments show that the proposed method outperforms the state-of-the-art pretraining methods on several document classification tasks."
2281,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a novel self-supervised pretraining method for document understanding. The proposed method is based on the Transformer architecture. The main idea is to learn a joint representation of words and visual features, which is then used to train a Transformer-based model for document classification. The paper also proposes a self-training procedure for downstream tasks. The experiments show that the proposed method outperforms the state-of-the-art pretraining methods on several document classification tasks."
2282,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a novel self-supervised pretraining method for document understanding. The proposed method is based on the Transformer architecture. The main idea is to learn a joint representation of words and visual features, which is then used to train a Transformer-based model for document classification. The paper also proposes a self-training procedure for downstream tasks. The experiments show that the proposed method outperforms the state-of-the-art pretraining methods on several document classification tasks."
2283,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,"This paper proposes a novel self-supervised pretraining method for document understanding. The proposed method is based on the Transformer architecture. The main idea is to learn a joint representation of words and visual features, which is then used to train a Transformer-based model for document classification. The paper also proposes a self-training procedure for downstream tasks. The experiments show that the proposed method outperforms the state-of-the-art pretraining methods on several document classification tasks."
2284,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in data clustering problems. The authors consider the case where the objective is either l-median or k-center, where the l-norm of the objective depends on the number of data points in the dataset. The main contribution of this paper is to propose a local search algorithm for the lp-norm objective. The algorithm is based on a linear programming algorithm, and the authors provide theoretical guarantees for the worst-case guarantee of the algorithm. The paper also provides a theoretical analysis of the performance of the proposed algorithm."
2285,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in data clustering problems. The authors consider the case where the objective is either l-median or k-center, where the l-norm of the objective depends on the number of data points in the dataset. The main contribution of this paper is to propose a local search algorithm for the lp-norm objective. The algorithm is based on a linear programming algorithm, and the authors provide theoretical guarantees for the worst-case guarantee of the algorithm. The paper also provides a theoretical analysis of the performance of the proposed algorithm."
2286,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in data clustering problems. The authors consider the case where the objective is either l-median or k-center, where the l-norm of the objective depends on the number of data points in the dataset. The main contribution of this paper is to propose a local search algorithm for the lp-norm objective. The algorithm is based on a linear programming algorithm, and the authors provide theoretical guarantees for the worst-case guarantee of the algorithm. The paper also provides a theoretical analysis of the performance of the proposed algorithm."
2287,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"This paper studies the problem of individual fairness in data clustering problems. The authors consider the case where the objective is either l-median or k-center, where the l-norm of the objective depends on the number of data points in the dataset. The main contribution of this paper is to propose a local search algorithm for the lp-norm objective. The algorithm is based on a linear programming algorithm, and the authors provide theoretical guarantees for the worst-case guarantee of the algorithm. The paper also provides a theoretical analysis of the performance of the proposed algorithm."
2288,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper studies the memory bottleneck problem of graph partitioning problems. The authors consider the problem of partitioning graphs with O(n) constraints, where the number of nodes in the graph is limited to a certain number. The main contribution of this paper is to provide a theoretical analysis of the memory complexity of the problem, and to provide an approximation ratio of O(N^2/\epsilon^2) for polynomial-time Gaussian sampling-based algorithms. "
2289,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper studies the memory bottleneck problem of graph partitioning problems. The authors consider the problem of partitioning graphs with O(n) constraints, where the number of nodes in the graph is limited to a certain number. The main contribution of this paper is to provide a theoretical analysis of the memory complexity of the problem, and to provide an approximation ratio of O(N^2/\epsilon^2) for polynomial-time Gaussian sampling-based algorithms. "
2290,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper studies the memory bottleneck problem of graph partitioning problems. The authors consider the problem of partitioning graphs with O(n) constraints, where the number of nodes in the graph is limited to a certain number. The main contribution of this paper is to provide a theoretical analysis of the memory complexity of the problem, and to provide an approximation ratio of O(N^2/\epsilon^2) for polynomial-time Gaussian sampling-based algorithms. "
2291,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"This paper studies the memory bottleneck problem of graph partitioning problems. The authors consider the problem of partitioning graphs with O(n) constraints, where the number of nodes in the graph is limited to a certain number. The main contribution of this paper is to provide a theoretical analysis of the memory complexity of the problem, and to provide an approximation ratio of O(N^2/\epsilon^2) for polynomial-time Gaussian sampling-based algorithms. "
2292,SP:cfd6cf88a823729c281059e179788248238a6ed7,This paper proposes a motion-aware unit (MAU) for video prediction. The proposed model consists of two modules: an attention module and a fusion module. The attention module maps the temporal receptive field of the predictive unit to the historical temporal states. The fusion module is used to extract augmented motion information (AMI) from the historical spatial states of the encoders and decoders. The authors also propose an information recalling scheme to improve the encoder and decoder. The experimental results show that the proposed model outperforms the state-of-the-art methods on several video prediction tasks.
2293,SP:cfd6cf88a823729c281059e179788248238a6ed7,This paper proposes a motion-aware unit (MAU) for video prediction. The proposed model consists of two modules: an attention module and a fusion module. The attention module maps the temporal receptive field of the predictive unit to the historical temporal states. The fusion module is used to extract augmented motion information (AMI) from the historical spatial states of the encoders and decoders. The authors also propose an information recalling scheme to improve the encoder and decoder. The experimental results show that the proposed model outperforms the state-of-the-art methods on several video prediction tasks.
2294,SP:cfd6cf88a823729c281059e179788248238a6ed7,This paper proposes a motion-aware unit (MAU) for video prediction. The proposed model consists of two modules: an attention module and a fusion module. The attention module maps the temporal receptive field of the predictive unit to the historical temporal states. The fusion module is used to extract augmented motion information (AMI) from the historical spatial states of the encoders and decoders. The authors also propose an information recalling scheme to improve the encoder and decoder. The experimental results show that the proposed model outperforms the state-of-the-art methods on several video prediction tasks.
2295,SP:cfd6cf88a823729c281059e179788248238a6ed7,This paper proposes a motion-aware unit (MAU) for video prediction. The proposed model consists of two modules: an attention module and a fusion module. The attention module maps the temporal receptive field of the predictive unit to the historical temporal states. The fusion module is used to extract augmented motion information (AMI) from the historical spatial states of the encoders and decoders. The authors also propose an information recalling scheme to improve the encoder and decoder. The experimental results show that the proposed model outperforms the state-of-the-art methods on several video prediction tasks.
2296,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies the problem of approximating the Q function of a neural network with ReLU and polynomial activation functions with two-layer neural networks. In particular, the authors consider the case where the neural network is trained with a generative model and the function class is deterministic. The authors show that the sample complexity of the algorithm is polynomially smaller than the number of samples required to approximate the function in the deterministic setting. The main contribution of this paper is to provide a lower bound on sample complexity in the generative setting. "
2297,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies the problem of approximating the Q function of a neural network with ReLU and polynomial activation functions with two-layer neural networks. In particular, the authors consider the case where the neural network is trained with a generative model and the function class is deterministic. The authors show that the sample complexity of the algorithm is polynomially smaller than the number of samples required to approximate the function in the deterministic setting. The main contribution of this paper is to provide a lower bound on sample complexity in the generative setting. "
2298,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies the problem of approximating the Q function of a neural network with ReLU and polynomial activation functions with two-layer neural networks. In particular, the authors consider the case where the neural network is trained with a generative model and the function class is deterministic. The authors show that the sample complexity of the algorithm is polynomially smaller than the number of samples required to approximate the function in the deterministic setting. The main contribution of this paper is to provide a lower bound on sample complexity in the generative setting. "
2299,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies the problem of approximating the Q function of a neural network with ReLU and polynomial activation functions with two-layer neural networks. In particular, the authors consider the case where the neural network is trained with a generative model and the function class is deterministic. The authors show that the sample complexity of the algorithm is polynomially smaller than the number of samples required to approximate the function in the deterministic setting. The main contribution of this paper is to provide a lower bound on sample complexity in the generative setting. "
2300,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for zero-shot transfer learning, called ooDML, which aims to measure the generalization performance of deep metric learning (DML) methods in the presence of distribution shift. The main contribution of this paper is the introduction of a few-shot DML benchmark, which is designed to evaluate the performance of different DML methods on different distribution shifts. The authors show that the performance on the new benchmark is comparable to the state-of-the-art methods on the standard DML benchmarks. "
2301,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for zero-shot transfer learning, called ooDML, which aims to measure the generalization performance of deep metric learning (DML) methods in the presence of distribution shift. The main contribution of this paper is the introduction of a few-shot DML benchmark, which is designed to evaluate the performance of different DML methods on different distribution shifts. The authors show that the performance on the new benchmark is comparable to the state-of-the-art methods on the standard DML benchmarks. "
2302,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for zero-shot transfer learning, called ooDML, which aims to measure the generalization performance of deep metric learning (DML) methods in the presence of distribution shift. The main contribution of this paper is the introduction of a few-shot DML benchmark, which is designed to evaluate the performance of different DML methods on different distribution shifts. The authors show that the performance on the new benchmark is comparable to the state-of-the-art methods on the standard DML benchmarks. "
2303,SP:cac881243abde92a28c110f5bd84d115ed189bda,"This paper proposes a new benchmark for zero-shot transfer learning, called ooDML, which aims to measure the generalization performance of deep metric learning (DML) methods in the presence of distribution shift. The main contribution of this paper is the introduction of a few-shot DML benchmark, which is designed to evaluate the performance of different DML methods on different distribution shifts. The authors show that the performance on the new benchmark is comparable to the state-of-the-art methods on the standard DML benchmarks. "
2304,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier detection. The proposed approach is based on meta-training, where the meta-learned model is used to select the best model for a given dataset. The meta-learner is trained on the dataset and the model is evaluated on a set of benchmark datasets. The experiments show that the proposed method outperforms the baselines."
2305,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier detection. The proposed approach is based on meta-training, where the meta-learned model is used to select the best model for a given dataset. The meta-learner is trained on the dataset and the model is evaluated on a set of benchmark datasets. The experiments show that the proposed method outperforms the baselines."
2306,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier detection. The proposed approach is based on meta-training, where the meta-learned model is used to select the best model for a given dataset. The meta-learner is trained on the dataset and the model is evaluated on a set of benchmark datasets. The experiments show that the proposed method outperforms the baselines."
2307,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a meta-learning approach for unsupervised outlier detection. The proposed approach is based on meta-training, where the meta-learned model is used to select the best model for a given dataset. The meta-learner is trained on the dataset and the model is evaluated on a set of benchmark datasets. The experiments show that the proposed method outperforms the baselines."
2308,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for prediction-optimization (SPO). The surrogate objective is a combination of the max operator and the gradients of the prediction model. The authors show that the proposed surrogate objective can be used for both soft linear and non-negative hard constraints, and provide theoretical bounds on the convergence rate of the surrogate objective. The proposed method is evaluated on synthetic linear programming and portfolio optimization problems."
2309,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for prediction-optimization (SPO). The surrogate objective is a combination of the max operator and the gradients of the prediction model. The authors show that the proposed surrogate objective can be used for both soft linear and non-negative hard constraints, and provide theoretical bounds on the convergence rate of the surrogate objective. The proposed method is evaluated on synthetic linear programming and portfolio optimization problems."
2310,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for prediction-optimization (SPO). The surrogate objective is a combination of the max operator and the gradients of the prediction model. The authors show that the proposed surrogate objective can be used for both soft linear and non-negative hard constraints, and provide theoretical bounds on the convergence rate of the surrogate objective. The proposed method is evaluated on synthetic linear programming and portfolio optimization problems."
2311,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"This paper proposes a surrogate objective framework for prediction-optimization (SPO). The surrogate objective is a combination of the max operator and the gradients of the prediction model. The authors show that the proposed surrogate objective can be used for both soft linear and non-negative hard constraints, and provide theoretical bounds on the convergence rate of the surrogate objective. The proposed method is evaluated on synthetic linear programming and portfolio optimization problems."
2312,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the expressiveness of Dropout Graph Neural Networks (DropGNNs), which is a generalization of GNNs. In particular, the authors study the expressivity of DropGNN in the context of graph neighborhoods. The authors show that the expressive power of dropout graph neural networks (GNN) is limited by the number of nodes in the graph, and propose to use GNN-based message passing to address this issue. Theoretical analysis is provided to support the theoretical results. Experimental results are provided to demonstrate the effectiveness of the proposed method."
2313,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the expressiveness of Dropout Graph Neural Networks (DropGNNs), which is a generalization of GNNs. In particular, the authors study the expressivity of DropGNN in the context of graph neighborhoods. The authors show that the expressive power of dropout graph neural networks (GNN) is limited by the number of nodes in the graph, and propose to use GNN-based message passing to address this issue. Theoretical analysis is provided to support the theoretical results. Experimental results are provided to demonstrate the effectiveness of the proposed method."
2314,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the expressiveness of Dropout Graph Neural Networks (DropGNNs), which is a generalization of GNNs. In particular, the authors study the expressivity of DropGNN in the context of graph neighborhoods. The authors show that the expressive power of dropout graph neural networks (GNN) is limited by the number of nodes in the graph, and propose to use GNN-based message passing to address this issue. Theoretical analysis is provided to support the theoretical results. Experimental results are provided to demonstrate the effectiveness of the proposed method."
2315,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"This paper studies the expressiveness of Dropout Graph Neural Networks (DropGNNs), which is a generalization of GNNs. In particular, the authors study the expressivity of DropGNN in the context of graph neighborhoods. The authors show that the expressive power of dropout graph neural networks (GNN) is limited by the number of nodes in the graph, and propose to use GNN-based message passing to address this issue. Theoretical analysis is provided to support the theoretical results. Experimental results are provided to demonstrate the effectiveness of the proposed method."
2316,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a new transformer architecture for image classification. The main idea of the proposed architecture is to use a dual-scale transformer architecture, which is able to capture both local and global pattern information. The proposed architecture consists of two modules: an Intra-scale Propagation module and an Inter-Scale Alignment module. The first module is used to propagate the local pattern information, while the second module is the inter-scale alignment module. Experiments on ImageNet-1k and mAP show that the proposed model achieves state-of-the-art results."
2317,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a new transformer architecture for image classification. The main idea of the proposed architecture is to use a dual-scale transformer architecture, which is able to capture both local and global pattern information. The proposed architecture consists of two modules: an Intra-scale Propagation module and an Inter-Scale Alignment module. The first module is used to propagate the local pattern information, while the second module is the inter-scale alignment module. Experiments on ImageNet-1k and mAP show that the proposed model achieves state-of-the-art results."
2318,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a new transformer architecture for image classification. The main idea of the proposed architecture is to use a dual-scale transformer architecture, which is able to capture both local and global pattern information. The proposed architecture consists of two modules: an Intra-scale Propagation module and an Inter-Scale Alignment module. The first module is used to propagate the local pattern information, while the second module is the inter-scale alignment module. Experiments on ImageNet-1k and mAP show that the proposed model achieves state-of-the-art results."
2319,SP:090dc0471d54e237f423034b1e1c46a510202807,"This paper proposes a new transformer architecture for image classification. The main idea of the proposed architecture is to use a dual-scale transformer architecture, which is able to capture both local and global pattern information. The proposed architecture consists of two modules: an Intra-scale Propagation module and an Inter-Scale Alignment module. The first module is used to propagate the local pattern information, while the second module is the inter-scale alignment module. Experiments on ImageNet-1k and mAP show that the proposed model achieves state-of-the-art results."
2320,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a method for learning visual concepts from videos. The key idea is to use an impulse-based differentiable rigid-body simulator (VRDP) to simulate the dynamics of a differentiable physics model, and then use a concept learner to predict the trajectory of the simulated trajectory. The method is evaluated on both synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art."
2321,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a method for learning visual concepts from videos. The key idea is to use an impulse-based differentiable rigid-body simulator (VRDP) to simulate the dynamics of a differentiable physics model, and then use a concept learner to predict the trajectory of the simulated trajectory. The method is evaluated on both synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art."
2322,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a method for learning visual concepts from videos. The key idea is to use an impulse-based differentiable rigid-body simulator (VRDP) to simulate the dynamics of a differentiable physics model, and then use a concept learner to predict the trajectory of the simulated trajectory. The method is evaluated on both synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art."
2323,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper proposes a method for learning visual concepts from videos. The key idea is to use an impulse-based differentiable rigid-body simulator (VRDP) to simulate the dynamics of a differentiable physics model, and then use a concept learner to predict the trajectory of the simulated trajectory. The method is evaluated on both synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art."
2324,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes a new active learning method based on submodular information measures (SIM) for active learning. SIMILAR is a unified active learning framework that combines two existing active learning methods, namely DISTIL and LeARning. The main contribution of this paper is the introduction of a new acquisition function, which can be applied to any active learning algorithm. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2325,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes a new active learning method based on submodular information measures (SIM) for active learning. SIMILAR is a unified active learning framework that combines two existing active learning methods, namely DISTIL and LeARning. The main contribution of this paper is the introduction of a new acquisition function, which can be applied to any active learning algorithm. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2326,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes a new active learning method based on submodular information measures (SIM) for active learning. SIMILAR is a unified active learning framework that combines two existing active learning methods, namely DISTIL and LeARning. The main contribution of this paper is the introduction of a new acquisition function, which can be applied to any active learning algorithm. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2327,SP:c511066c38f9793bacb4986c564eafa36e032f39,"This paper proposes a new active learning method based on submodular information measures (SIM) for active learning. SIMILAR is a unified active learning framework that combines two existing active learning methods, namely DISTIL and LeARning. The main contribution of this paper is the introduction of a new acquisition function, which can be applied to any active learning algorithm. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2328,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"This paper studies the problem of identity tests for ranking data. The authors propose a new test called Uniformly Most Powerful Unbiased (UMPU) test for the asymptotic and non-asymptotic setting. The test is based on the Mallows model, which is a variant of Mallows models. The main contribution of this paper is to provide a sample-optimal non-asymptotically optimal test for this test. The paper is well-written and easy to follow. "
2329,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"This paper studies the problem of identity tests for ranking data. The authors propose a new test called Uniformly Most Powerful Unbiased (UMPU) test for the asymptotic and non-asymptotic setting. The test is based on the Mallows model, which is a variant of Mallows models. The main contribution of this paper is to provide a sample-optimal non-asymptotically optimal test for this test. The paper is well-written and easy to follow. "
2330,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"This paper studies the problem of identity tests for ranking data. The authors propose a new test called Uniformly Most Powerful Unbiased (UMPU) test for the asymptotic and non-asymptotic setting. The test is based on the Mallows model, which is a variant of Mallows models. The main contribution of this paper is to provide a sample-optimal non-asymptotically optimal test for this test. The paper is well-written and easy to follow. "
2331,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,"This paper studies the problem of identity tests for ranking data. The authors propose a new test called Uniformly Most Powerful Unbiased (UMPU) test for the asymptotic and non-asymptotic setting. The test is based on the Mallows model, which is a variant of Mallows models. The main contribution of this paper is to provide a sample-optimal non-asymptotically optimal test for this test. The paper is well-written and easy to follow. "
2332,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a neural radiance field-based method to capture the radiance fields of a human body. The proposed method is based on a parametric human body model, which is trained with a multi-view transformer and a temporal transformer. The model is trained to capture both the temporal and pixel-aligned features of the video. The method is evaluated on the ZJU-MoCap and AIST datasets. The results show that the proposed method outperforms the baselines."
2333,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a neural radiance field-based method to capture the radiance fields of a human body. The proposed method is based on a parametric human body model, which is trained with a multi-view transformer and a temporal transformer. The model is trained to capture both the temporal and pixel-aligned features of the video. The method is evaluated on the ZJU-MoCap and AIST datasets. The results show that the proposed method outperforms the baselines."
2334,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a neural radiance field-based method to capture the radiance fields of a human body. The proposed method is based on a parametric human body model, which is trained with a multi-view transformer and a temporal transformer. The model is trained to capture both the temporal and pixel-aligned features of the video. The method is evaluated on the ZJU-MoCap and AIST datasets. The results show that the proposed method outperforms the baselines."
2335,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"This paper proposes a neural radiance field-based method to capture the radiance fields of a human body. The proposed method is based on a parametric human body model, which is trained with a multi-view transformer and a temporal transformer. The model is trained to capture both the temporal and pixel-aligned features of the video. The method is evaluated on the ZJU-MoCap and AIST datasets. The results show that the proposed method outperforms the baselines."
2336,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for neural architecture search for vision transformers. The main idea is to use a weight-sharing supernet to estimate the E-T error of the search space, which is then used as a proxy for the search dimensions of the network. The authors show that the proposed method is able to outperform the existing methods on ImageNet and VQA tasks."
2337,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for neural architecture search for vision transformers. The main idea is to use a weight-sharing supernet to estimate the E-T error of the search space, which is then used as a proxy for the search dimensions of the network. The authors show that the proposed method is able to outperform the existing methods on ImageNet and VQA tasks."
2338,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for neural architecture search for vision transformers. The main idea is to use a weight-sharing supernet to estimate the E-T error of the search space, which is then used as a proxy for the search dimensions of the network. The authors show that the proposed method is able to outperform the existing methods on ImageNet and VQA tasks."
2339,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This paper proposes a method for neural architecture search for vision transformers. The main idea is to use a weight-sharing supernet to estimate the E-T error of the search space, which is then used as a proxy for the search dimensions of the network. The authors show that the proposed method is able to outperform the existing methods on ImageNet and VQA tasks."
2340,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In particular, the authors consider the case where the label proportions are non-monochromatic, i.e., the labels are not monotone. The authors propose an algorithm that learns a linear threshold function (LFT) that can be expressed as a linear function of a d-dimensional boolean vector. The main contribution of this paper is to provide an algorithmic bound on the complexity of the algorithm. The paper is well-written and easy to follow."
2341,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In particular, the authors consider the case where the label proportions are non-monochromatic, i.e., the labels are not monotone. The authors propose an algorithm that learns a linear threshold function (LFT) that can be expressed as a linear function of a d-dimensional boolean vector. The main contribution of this paper is to provide an algorithmic bound on the complexity of the algorithm. The paper is well-written and easy to follow."
2342,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In particular, the authors consider the case where the label proportions are non-monochromatic, i.e., the labels are not monotone. The authors propose an algorithm that learns a linear threshold function (LFT) that can be expressed as a linear function of a d-dimensional boolean vector. The main contribution of this paper is to provide an algorithmic bound on the complexity of the algorithm. The paper is well-written and easy to follow."
2343,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This paper studies the problem of learning from label proportions (LLP) for linear threshold functions (LTFs). In particular, the authors consider the case where the label proportions are non-monochromatic, i.e., the labels are not monotone. The authors propose an algorithm that learns a linear threshold function (LFT) that can be expressed as a linear function of a d-dimensional boolean vector. The main contribution of this paper is to provide an algorithmic bound on the complexity of the algorithm. The paper is well-written and easy to follow."
2344,SP:2eb193c76355aac08003c9b377895202fd3bd297,This paper proposes a method for learning curve extrapolation for neural architecture search (NAS). The method is based on the singular value decomposition and noise modeling. The authors show that the proposed method outperforms single-fidelity algorithms on NAS-Bench-311 and NAS-bench-NLP11. They also show that their method can be applied to surrogate benchmarks.
2345,SP:2eb193c76355aac08003c9b377895202fd3bd297,This paper proposes a method for learning curve extrapolation for neural architecture search (NAS). The method is based on the singular value decomposition and noise modeling. The authors show that the proposed method outperforms single-fidelity algorithms on NAS-Bench-311 and NAS-bench-NLP11. They also show that their method can be applied to surrogate benchmarks.
2346,SP:2eb193c76355aac08003c9b377895202fd3bd297,This paper proposes a method for learning curve extrapolation for neural architecture search (NAS). The method is based on the singular value decomposition and noise modeling. The authors show that the proposed method outperforms single-fidelity algorithms on NAS-Bench-311 and NAS-bench-NLP11. They also show that their method can be applied to surrogate benchmarks.
2347,SP:2eb193c76355aac08003c9b377895202fd3bd297,This paper proposes a method for learning curve extrapolation for neural architecture search (NAS). The method is based on the singular value decomposition and noise modeling. The authors show that the proposed method outperforms single-fidelity algorithms on NAS-Bench-311 and NAS-bench-NLP11. They also show that their method can be applied to surrogate benchmarks.
2348,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centric method for generating explanations for machine learning models. SimplEx is based on the Integrated Jacobian (IJ) framework. The core idea is to decompose the latent representation of the model into a mixture of corpus latent representations and model representations, and then use the mixture of model representations to generate post-hoc explanations. The proposed method is evaluated on a variety of tasks, including mortality prediction, image classification, and image classification."
2349,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centric method for generating explanations for machine learning models. SimplEx is based on the Integrated Jacobian (IJ) framework. The core idea is to decompose the latent representation of the model into a mixture of corpus latent representations and model representations, and then use the mixture of model representations to generate post-hoc explanations. The proposed method is evaluated on a variety of tasks, including mortality prediction, image classification, and image classification."
2350,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centric method for generating explanations for machine learning models. SimplEx is based on the Integrated Jacobian (IJ) framework. The core idea is to decompose the latent representation of the model into a mixture of corpus latent representations and model representations, and then use the mixture of model representations to generate post-hoc explanations. The proposed method is evaluated on a variety of tasks, including mortality prediction, image classification, and image classification."
2351,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"This paper proposes SimplEx, a user-centric method for generating explanations for machine learning models. SimplEx is based on the Integrated Jacobian (IJ) framework. The core idea is to decompose the latent representation of the model into a mixture of corpus latent representations and model representations, and then use the mixture of model representations to generate post-hoc explanations. The proposed method is evaluated on a variety of tasks, including mortality prediction, image classification, and image classification."
2352,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based architecture for visual feature learning (VLP). The main idea is to learn a visual relation between images and text, and then use a transformer network to learn the inter-modal alignment between the visual relation and the text. The paper also proposes a new metric called Inter-Modality Flow (IMF) to measure the influence of different modalities on VLP performance. Experiments on VQA, Image-Text Retrieval, and Visual Entailment demonstrate the effectiveness of the proposed method."
2353,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based architecture for visual feature learning (VLP). The main idea is to learn a visual relation between images and text, and then use a transformer network to learn the inter-modal alignment between the visual relation and the text. The paper also proposes a new metric called Inter-Modality Flow (IMF) to measure the influence of different modalities on VLP performance. Experiments on VQA, Image-Text Retrieval, and Visual Entailment demonstrate the effectiveness of the proposed method."
2354,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based architecture for visual feature learning (VLP). The main idea is to learn a visual relation between images and text, and then use a transformer network to learn the inter-modal alignment between the visual relation and the text. The paper also proposes a new metric called Inter-Modality Flow (IMF) to measure the influence of different modalities on VLP performance. Experiments on VQA, Image-Text Retrieval, and Visual Entailment demonstrate the effectiveness of the proposed method."
2355,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer-based architecture for visual feature learning (VLP). The main idea is to learn a visual relation between images and text, and then use a transformer network to learn the inter-modal alignment between the visual relation and the text. The paper also proposes a new metric called Inter-Modality Flow (IMF) to measure the influence of different modalities on VLP performance. Experiments on VQA, Image-Text Retrieval, and Visual Entailment demonstrate the effectiveness of the proposed method."
2356,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the problem of information leakage in iterative randomized learning with noisy gradient descent. In particular, the authors study the dynamics of the Rényi differential privacy loss, which is the divergence between the probability distribution of the model and the privacy loss of the algorithm. The authors show that under certain conditions, the optimal utility of the proposed algorithm is $O(1/\epsilon^2)$. The authors also show that the optimal privacy loss can be expressed as a composition of smooth and strongly convex loss functions. "
2357,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the problem of information leakage in iterative randomized learning with noisy gradient descent. In particular, the authors study the dynamics of the Rényi differential privacy loss, which is the divergence between the probability distribution of the model and the privacy loss of the algorithm. The authors show that under certain conditions, the optimal utility of the proposed algorithm is $O(1/\epsilon^2)$. The authors also show that the optimal privacy loss can be expressed as a composition of smooth and strongly convex loss functions. "
2358,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the problem of information leakage in iterative randomized learning with noisy gradient descent. In particular, the authors study the dynamics of the Rényi differential privacy loss, which is the divergence between the probability distribution of the model and the privacy loss of the algorithm. The authors show that under certain conditions, the optimal utility of the proposed algorithm is $O(1/\epsilon^2)$. The authors also show that the optimal privacy loss can be expressed as a composition of smooth and strongly convex loss functions. "
2359,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"This paper studies the problem of information leakage in iterative randomized learning with noisy gradient descent. In particular, the authors study the dynamics of the Rényi differential privacy loss, which is the divergence between the probability distribution of the model and the privacy loss of the algorithm. The authors show that under certain conditions, the optimal utility of the proposed algorithm is $O(1/\epsilon^2)$. The authors also show that the optimal privacy loss can be expressed as a composition of smooth and strongly convex loss functions. "
2360,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper studies the problem of quadratic optimization with reinforcement learning (RL) in the context of Q-learning. The authors propose a new method for solving the problem, called RLQP, which is a combination of RL and QP. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of RL-based QP solvers. The paper also provides a set of experiments to demonstrate the effectiveness of the proposed method."
2361,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper studies the problem of quadratic optimization with reinforcement learning (RL) in the context of Q-learning. The authors propose a new method for solving the problem, called RLQP, which is a combination of RL and QP. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of RL-based QP solvers. The paper also provides a set of experiments to demonstrate the effectiveness of the proposed method."
2362,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper studies the problem of quadratic optimization with reinforcement learning (RL) in the context of Q-learning. The authors propose a new method for solving the problem, called RLQP, which is a combination of RL and QP. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of RL-based QP solvers. The paper also provides a set of experiments to demonstrate the effectiveness of the proposed method."
2363,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"This paper studies the problem of quadratic optimization with reinforcement learning (RL) in the context of Q-learning. The authors propose a new method for solving the problem, called RLQP, which is a combination of RL and QP. The main contribution of this paper is to provide a theoretical analysis of the convergence rate of RL-based QP solvers. The paper also provides a set of experiments to demonstrate the effectiveness of the proposed method."
2364,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,This paper studies the principal components bias in deep linear networks. The authors show that the convergence rate of the network is bounded by the singular value of the principal component. The paper also shows that the PC-bias can be leveraged for early stopping in PCA.
2365,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,This paper studies the principal components bias in deep linear networks. The authors show that the convergence rate of the network is bounded by the singular value of the principal component. The paper also shows that the PC-bias can be leveraged for early stopping in PCA.
2366,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,This paper studies the principal components bias in deep linear networks. The authors show that the convergence rate of the network is bounded by the singular value of the principal component. The paper also shows that the PC-bias can be leveraged for early stopping in PCA.
2367,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,This paper studies the principal components bias in deep linear networks. The authors show that the convergence rate of the network is bounded by the singular value of the principal component. The paper also shows that the PC-bias can be leveraged for early stopping in PCA.
2368,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a new method for detecting and erasing backdoor attacks on deep neural networks (DNNs). The proposed method is based on the dual-task learning approach, where the model is trained to detect and erase backdoor triggers and the target class is used to train a clean model. The authors also propose a two-stage gradient ascent mechanism to improve the performance of the proposed method."
2369,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a new method for detecting and erasing backdoor attacks on deep neural networks (DNNs). The proposed method is based on the dual-task learning approach, where the model is trained to detect and erase backdoor triggers and the target class is used to train a clean model. The authors also propose a two-stage gradient ascent mechanism to improve the performance of the proposed method."
2370,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a new method for detecting and erasing backdoor attacks on deep neural networks (DNNs). The proposed method is based on the dual-task learning approach, where the model is trained to detect and erase backdoor triggers and the target class is used to train a clean model. The authors also propose a two-stage gradient ascent mechanism to improve the performance of the proposed method."
2371,SP:1598bad835a657e56af3261501c671897b7e9ffd,"This paper proposes a new method for detecting and erasing backdoor attacks on deep neural networks (DNNs). The proposed method is based on the dual-task learning approach, where the model is trained to detect and erase backdoor triggers and the target class is used to train a clean model. The authors also propose a two-stage gradient ascent mechanism to improve the performance of the proposed method."
2372,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a generative implicit model for 3D-aware image synthesis. The proposed method is based on a shading-guided implicit model that learns a 3D shape representation and a volume rendering strategy based on surface tracking. The main contribution of this paper is to propose a shading guided implicit model, which can be used to generate 3D radiance fields. The paper also proposes a discriminator for the Gradients of the 3D Radiance Field. The experimental results show that the proposed method can generate realistic 3D shapes."
2373,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a generative implicit model for 3D-aware image synthesis. The proposed method is based on a shading-guided implicit model that learns a 3D shape representation and a volume rendering strategy based on surface tracking. The main contribution of this paper is to propose a shading guided implicit model, which can be used to generate 3D radiance fields. The paper also proposes a discriminator for the Gradients of the 3D Radiance Field. The experimental results show that the proposed method can generate realistic 3D shapes."
2374,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a generative implicit model for 3D-aware image synthesis. The proposed method is based on a shading-guided implicit model that learns a 3D shape representation and a volume rendering strategy based on surface tracking. The main contribution of this paper is to propose a shading guided implicit model, which can be used to generate 3D radiance fields. The paper also proposes a discriminator for the Gradients of the 3D Radiance Field. The experimental results show that the proposed method can generate realistic 3D shapes."
2375,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper proposes a generative implicit model for 3D-aware image synthesis. The proposed method is based on a shading-guided implicit model that learns a 3D shape representation and a volume rendering strategy based on surface tracking. The main contribution of this paper is to propose a shading guided implicit model, which can be used to generate 3D radiance fields. The paper also proposes a discriminator for the Gradients of the 3D Radiance Field. The experimental results show that the proposed method can generate realistic 3D shapes."
2376,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a method for instrumental variable regression (IV) based on a quasi-Bayesian approach. The main idea is to use a kernelized variational autoencoder (KVAE) to model the data generating process, and then use a Bayesian approach to estimate the posterior of the KVAE. The authors show that the proposed method can be applied to a variety of IV regression problems, and show that it outperforms the state-of-the-art in terms of time complexity and accuracy."
2377,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a method for instrumental variable regression (IV) based on a quasi-Bayesian approach. The main idea is to use a kernelized variational autoencoder (KVAE) to model the data generating process, and then use a Bayesian approach to estimate the posterior of the KVAE. The authors show that the proposed method can be applied to a variety of IV regression problems, and show that it outperforms the state-of-the-art in terms of time complexity and accuracy."
2378,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a method for instrumental variable regression (IV) based on a quasi-Bayesian approach. The main idea is to use a kernelized variational autoencoder (KVAE) to model the data generating process, and then use a Bayesian approach to estimate the posterior of the KVAE. The authors show that the proposed method can be applied to a variety of IV regression problems, and show that it outperforms the state-of-the-art in terms of time complexity and accuracy."
2379,SP:4b3dad77d79507c512877867dfea6db87a78682d,"This paper proposes a method for instrumental variable regression (IV) based on a quasi-Bayesian approach. The main idea is to use a kernelized variational autoencoder (KVAE) to model the data generating process, and then use a Bayesian approach to estimate the posterior of the KVAE. The authors show that the proposed method can be applied to a variety of IV regression problems, and show that it outperforms the state-of-the-art in terms of time complexity and accuracy."
2380,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a multi-language autoregressive model for multi-lingual open-domain QA. The authors propose a dense passage retrieval algorithm for the multi-source QA problem. The proposed method is based on the idea of CORA, which is a multilingual auto-encoder-decoder model. The main difference between CORA and the previous work is that the authors propose to train the model on both the source language and the target language simultaneously. The method is evaluated on a variety of open QA benchmarks, and the results show that the proposed method outperforms the existing methods."
2381,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a multi-language autoregressive model for multi-lingual open-domain QA. The authors propose a dense passage retrieval algorithm for the multi-source QA problem. The proposed method is based on the idea of CORA, which is a multilingual auto-encoder-decoder model. The main difference between CORA and the previous work is that the authors propose to train the model on both the source language and the target language simultaneously. The method is evaluated on a variety of open QA benchmarks, and the results show that the proposed method outperforms the existing methods."
2382,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a multi-language autoregressive model for multi-lingual open-domain QA. The authors propose a dense passage retrieval algorithm for the multi-source QA problem. The proposed method is based on the idea of CORA, which is a multilingual auto-encoder-decoder model. The main difference between CORA and the previous work is that the authors propose to train the model on both the source language and the target language simultaneously. The method is evaluated on a variety of open QA benchmarks, and the results show that the proposed method outperforms the existing methods."
2383,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper proposes a multi-language autoregressive model for multi-lingual open-domain QA. The authors propose a dense passage retrieval algorithm for the multi-source QA problem. The proposed method is based on the idea of CORA, which is a multilingual auto-encoder-decoder model. The main difference between CORA and the previous work is that the authors propose to train the model on both the source language and the target language simultaneously. The method is evaluated on a variety of open QA benchmarks, and the results show that the proposed method outperforms the existing methods."
2384,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper studies the problem of out-of-domain generalization of deep neural networks (DNNs) trained with Empirical Risk Minimization (ERM) under distribution shift. The authors propose three measures to evaluate the generalization performance of DNNs trained with ERM: Fisher information, predictive entropy, and maximum mean discrepancy. The proposed measures are based on the Fisher information and predictive entropy measures, and the authors show that the proposed measures can be used to compare the performance of different DNN models trained with different ERM methods."
2385,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper studies the problem of out-of-domain generalization of deep neural networks (DNNs) trained with Empirical Risk Minimization (ERM) under distribution shift. The authors propose three measures to evaluate the generalization performance of DNNs trained with ERM: Fisher information, predictive entropy, and maximum mean discrepancy. The proposed measures are based on the Fisher information and predictive entropy measures, and the authors show that the proposed measures can be used to compare the performance of different DNN models trained with different ERM methods."
2386,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper studies the problem of out-of-domain generalization of deep neural networks (DNNs) trained with Empirical Risk Minimization (ERM) under distribution shift. The authors propose three measures to evaluate the generalization performance of DNNs trained with ERM: Fisher information, predictive entropy, and maximum mean discrepancy. The proposed measures are based on the Fisher information and predictive entropy measures, and the authors show that the proposed measures can be used to compare the performance of different DNN models trained with different ERM methods."
2387,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper studies the problem of out-of-domain generalization of deep neural networks (DNNs) trained with Empirical Risk Minimization (ERM) under distribution shift. The authors propose three measures to evaluate the generalization performance of DNNs trained with ERM: Fisher information, predictive entropy, and maximum mean discrepancy. The proposed measures are based on the Fisher information and predictive entropy measures, and the authors show that the proposed measures can be used to compare the performance of different DNN models trained with different ERM methods."
2388,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,This paper studies the problem of backdoor data poisoning attacks. The authors propose a theoretical framework to study the intrinsic vulnerability of the learning problem for backdoor attacks. They show that there is an intrinsic vulnerability to backdoor attacks in the setting of adversarial training. They also provide a theoretical analysis of the memorization capacity of the learned model and show that it can be used to improve the robustness of backdoor attacks to adversarial attacks. 
2389,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,This paper studies the problem of backdoor data poisoning attacks. The authors propose a theoretical framework to study the intrinsic vulnerability of the learning problem for backdoor attacks. They show that there is an intrinsic vulnerability to backdoor attacks in the setting of adversarial training. They also provide a theoretical analysis of the memorization capacity of the learned model and show that it can be used to improve the robustness of backdoor attacks to adversarial attacks. 
2390,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,This paper studies the problem of backdoor data poisoning attacks. The authors propose a theoretical framework to study the intrinsic vulnerability of the learning problem for backdoor attacks. They show that there is an intrinsic vulnerability to backdoor attacks in the setting of adversarial training. They also provide a theoretical analysis of the memorization capacity of the learned model and show that it can be used to improve the robustness of backdoor attacks to adversarial attacks. 
2391,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,This paper studies the problem of backdoor data poisoning attacks. The authors propose a theoretical framework to study the intrinsic vulnerability of the learning problem for backdoor attacks. They show that there is an intrinsic vulnerability to backdoor attacks in the setting of adversarial training. They also provide a theoretical analysis of the memorization capacity of the learned model and show that it can be used to improve the robustness of backdoor attacks to adversarial attacks. 
2392,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the relationship between the width and depth of neural networks for deep Gaussian processes. The authors show that the width of a neural network can be bounded by the depth of its hidden units, which is a function of the number of hidden units in the network. They also show that depth and width can be controlled by the L2 regularization of the weights of the neural network. Finally, the authors provide a theoretical analysis of the effect of the width on the adaptability of the posterior of a deep GP. "
2393,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the relationship between the width and depth of neural networks for deep Gaussian processes. The authors show that the width of a neural network can be bounded by the depth of its hidden units, which is a function of the number of hidden units in the network. They also show that depth and width can be controlled by the L2 regularization of the weights of the neural network. Finally, the authors provide a theoretical analysis of the effect of the width on the adaptability of the posterior of a deep GP. "
2394,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the relationship between the width and depth of neural networks for deep Gaussian processes. The authors show that the width of a neural network can be bounded by the depth of its hidden units, which is a function of the number of hidden units in the network. They also show that depth and width can be controlled by the L2 regularization of the weights of the neural network. Finally, the authors provide a theoretical analysis of the effect of the width on the adaptability of the posterior of a deep GP. "
2395,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"This paper studies the relationship between the width and depth of neural networks for deep Gaussian processes. The authors show that the width of a neural network can be bounded by the depth of its hidden units, which is a function of the number of hidden units in the network. They also show that depth and width can be controlled by the L2 regularization of the weights of the neural network. Finally, the authors provide a theoretical analysis of the effect of the width on the adaptability of the posterior of a deep GP. "
2396,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the problem of federated learning (FL) in the setting where the number of clients is limited. The authors propose a new algorithm, FedLin, to address the issue of speed-accuracy conflict in FL. The main idea of FedLin is to use gradient sparsification to compress the gradient of the loss function of the clients. The paper shows that FedLin converges linearly to the global minimum with a sub-linear rate, which is faster than existing algorithms. In addition, the authors show that the convergence rate is tight at the compression level."
2397,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the problem of federated learning (FL) in the setting where the number of clients is limited. The authors propose a new algorithm, FedLin, to address the issue of speed-accuracy conflict in FL. The main idea of FedLin is to use gradient sparsification to compress the gradient of the loss function of the clients. The paper shows that FedLin converges linearly to the global minimum with a sub-linear rate, which is faster than existing algorithms. In addition, the authors show that the convergence rate is tight at the compression level."
2398,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the problem of federated learning (FL) in the setting where the number of clients is limited. The authors propose a new algorithm, FedLin, to address the issue of speed-accuracy conflict in FL. The main idea of FedLin is to use gradient sparsification to compress the gradient of the loss function of the clients. The paper shows that FedLin converges linearly to the global minimum with a sub-linear rate, which is faster than existing algorithms. In addition, the authors show that the convergence rate is tight at the compression level."
2399,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper studies the problem of federated learning (FL) in the setting where the number of clients is limited. The authors propose a new algorithm, FedLin, to address the issue of speed-accuracy conflict in FL. The main idea of FedLin is to use gradient sparsification to compress the gradient of the loss function of the clients. The paper shows that FedLin converges linearly to the global minimum with a sub-linear rate, which is faster than existing algorithms. In addition, the authors show that the convergence rate is tight at the compression level."
2400,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper proposes a method for estimating the sliced-Wasserstein distance (SW) between a random vector and a high-dimensional random vector. The method is based on the concentration of measure phenomenon (CPM) phenomenon, which is a well-studied phenomenon in the literature. The main contribution of this paper is to derive a deterministic approximation of the SW using the CPM phenomenon. The authors show that the proposed method is non-asymptotical under the weak dependence condition of the data distribution, and that it can be used for generative modeling."
2401,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper proposes a method for estimating the sliced-Wasserstein distance (SW) between a random vector and a high-dimensional random vector. The method is based on the concentration of measure phenomenon (CPM) phenomenon, which is a well-studied phenomenon in the literature. The main contribution of this paper is to derive a deterministic approximation of the SW using the CPM phenomenon. The authors show that the proposed method is non-asymptotical under the weak dependence condition of the data distribution, and that it can be used for generative modeling."
2402,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper proposes a method for estimating the sliced-Wasserstein distance (SW) between a random vector and a high-dimensional random vector. The method is based on the concentration of measure phenomenon (CPM) phenomenon, which is a well-studied phenomenon in the literature. The main contribution of this paper is to derive a deterministic approximation of the SW using the CPM phenomenon. The authors show that the proposed method is non-asymptotical under the weak dependence condition of the data distribution, and that it can be used for generative modeling."
2403,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper proposes a method for estimating the sliced-Wasserstein distance (SW) between a random vector and a high-dimensional random vector. The method is based on the concentration of measure phenomenon (CPM) phenomenon, which is a well-studied phenomenon in the literature. The main contribution of this paper is to derive a deterministic approximation of the SW using the CPM phenomenon. The authors show that the proposed method is non-asymptotical under the weak dependence condition of the data distribution, and that it can be used for generative modeling."
2404,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method for learning a low-dimensional representation of natural language representations that can be used for NLP tasks. The proposed method is based on a computer vision approach to learn a representation of the feature space of a language model. The method is evaluated on a variety of tasks, including word embeddings, syntactic and semantic tasks, and language tagging tasks. "
2405,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method for learning a low-dimensional representation of natural language representations that can be used for NLP tasks. The proposed method is based on a computer vision approach to learn a representation of the feature space of a language model. The method is evaluated on a variety of tasks, including word embeddings, syntactic and semantic tasks, and language tagging tasks. "
2406,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method for learning a low-dimensional representation of natural language representations that can be used for NLP tasks. The proposed method is based on a computer vision approach to learn a representation of the feature space of a language model. The method is evaluated on a variety of tasks, including word embeddings, syntactic and semantic tasks, and language tagging tasks. "
2407,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper proposes a method for learning a low-dimensional representation of natural language representations that can be used for NLP tasks. The proposed method is based on a computer vision approach to learn a representation of the feature space of a language model. The method is evaluated on a variety of tasks, including word embeddings, syntactic and semantic tasks, and language tagging tasks. "
2408,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,This paper proposes a contrastive self-supervised learning (D2C) method for few-shot conditional image generation. The main idea is to use a diffusion-based prior for the latent representations and contrastive representations for the generation. Experiments show that the proposed method outperforms StyleGAN2 and Diffusion-Decoding models.
2409,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,This paper proposes a contrastive self-supervised learning (D2C) method for few-shot conditional image generation. The main idea is to use a diffusion-based prior for the latent representations and contrastive representations for the generation. Experiments show that the proposed method outperforms StyleGAN2 and Diffusion-Decoding models.
2410,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,This paper proposes a contrastive self-supervised learning (D2C) method for few-shot conditional image generation. The main idea is to use a diffusion-based prior for the latent representations and contrastive representations for the generation. Experiments show that the proposed method outperforms StyleGAN2 and Diffusion-Decoding models.
2411,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,This paper proposes a contrastive self-supervised learning (D2C) method for few-shot conditional image generation. The main idea is to use a diffusion-based prior for the latent representations and contrastive representations for the generation. Experiments show that the proposed method outperforms StyleGAN2 and Diffusion-Decoding models.
2412,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,This paper proposes a new contrastive learning objective for self-supervised learning. The proposed objective is based on the spectral decomposition of a population augmentation graph. The authors show that the proposed objective can be used to improve the generalization of the learned representations. The main contribution of this paper is to provide a generalization bound for the proposed contrastive loss. The paper is well-written and easy to follow.
2413,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,This paper proposes a new contrastive learning objective for self-supervised learning. The proposed objective is based on the spectral decomposition of a population augmentation graph. The authors show that the proposed objective can be used to improve the generalization of the learned representations. The main contribution of this paper is to provide a generalization bound for the proposed contrastive loss. The paper is well-written and easy to follow.
2414,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,This paper proposes a new contrastive learning objective for self-supervised learning. The proposed objective is based on the spectral decomposition of a population augmentation graph. The authors show that the proposed objective can be used to improve the generalization of the learned representations. The main contribution of this paper is to provide a generalization bound for the proposed contrastive loss. The paper is well-written and easy to follow.
2415,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,This paper proposes a new contrastive learning objective for self-supervised learning. The proposed objective is based on the spectral decomposition of a population augmentation graph. The authors show that the proposed objective can be used to improve the generalization of the learned representations. The main contribution of this paper is to provide a generalization bound for the proposed contrastive loss. The paper is well-written and easy to follow.
2416,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,This paper studies the complexity of Bayesian Network Structure Learning (BNSL). The authors show that BNSL can be viewed as a variant of Polytree Learning (PLL) with a non-zero representation. The main contribution of this paper is to provide a lower bound on the parameterized complexity of the Bayesian network structure learning (BNSL) algorithm. The lower bound is based on the fact that the number of nodes in the network can be expressed as a function of the treewidth of the graph. The authors also provide lower bounds on the complexity in terms of the feedback edge set.
2417,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,This paper studies the complexity of Bayesian Network Structure Learning (BNSL). The authors show that BNSL can be viewed as a variant of Polytree Learning (PLL) with a non-zero representation. The main contribution of this paper is to provide a lower bound on the parameterized complexity of the Bayesian network structure learning (BNSL) algorithm. The lower bound is based on the fact that the number of nodes in the network can be expressed as a function of the treewidth of the graph. The authors also provide lower bounds on the complexity in terms of the feedback edge set.
2418,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,This paper studies the complexity of Bayesian Network Structure Learning (BNSL). The authors show that BNSL can be viewed as a variant of Polytree Learning (PLL) with a non-zero representation. The main contribution of this paper is to provide a lower bound on the parameterized complexity of the Bayesian network structure learning (BNSL) algorithm. The lower bound is based on the fact that the number of nodes in the network can be expressed as a function of the treewidth of the graph. The authors also provide lower bounds on the complexity in terms of the feedback edge set.
2419,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,This paper studies the complexity of Bayesian Network Structure Learning (BNSL). The authors show that BNSL can be viewed as a variant of Polytree Learning (PLL) with a non-zero representation. The main contribution of this paper is to provide a lower bound on the parameterized complexity of the Bayesian network structure learning (BNSL) algorithm. The lower bound is based on the fact that the number of nodes in the network can be expressed as a function of the treewidth of the graph. The authors also provide lower bounds on the complexity in terms of the feedback edge set.
2420,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper studies active learning in the streaming setting. The authors propose a new active learning algorithm for binary classification tasks. The main idea is to use a surrogate loss for the loss function of the model, where the surrogate loss is the sum of labeled and weak-labeled points. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling. The experimental results show that the proposed algorithm outperforms the existing active learning algorithms in terms of generalization and label complexity."
2421,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper studies active learning in the streaming setting. The authors propose a new active learning algorithm for binary classification tasks. The main idea is to use a surrogate loss for the loss function of the model, where the surrogate loss is the sum of labeled and weak-labeled points. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling. The experimental results show that the proposed algorithm outperforms the existing active learning algorithms in terms of generalization and label complexity."
2422,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper studies active learning in the streaming setting. The authors propose a new active learning algorithm for binary classification tasks. The main idea is to use a surrogate loss for the loss function of the model, where the surrogate loss is the sum of labeled and weak-labeled points. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling. The experimental results show that the proposed algorithm outperforms the existing active learning algorithms in terms of generalization and label complexity."
2423,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper studies active learning in the streaming setting. The authors propose a new active learning algorithm for binary classification tasks. The main idea is to use a surrogate loss for the loss function of the model, where the surrogate loss is the sum of labeled and weak-labeled points. The proposed algorithm is based on the Margin Algorithm and Uncertainty Sampling. The experimental results show that the proposed algorithm outperforms the existing active learning algorithms in terms of generalization and label complexity."
2424,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper studies the generalization error bounds of neural networks with Kolmogorov complexity. The authors propose a new measure of complexity that measures the complexity of the classifier’s function space in terms of the number of functions in the function space. The main contribution of this paper is to provide a generalization bound for neural networks that is based on the complexity measure of the function. The generalization bounds are based on a measure of KG, which is a function complexity measure that measures how far the network is from the low KG zone of the classification function, and the authors show that this measure can be used as a regularizer for N2N regularization. The paper also provides a theoretical analysis of the KG and its relation to the test accuracy."
2425,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper studies the generalization error bounds of neural networks with Kolmogorov complexity. The authors propose a new measure of complexity that measures the complexity of the classifier’s function space in terms of the number of functions in the function space. The main contribution of this paper is to provide a generalization bound for neural networks that is based on the complexity measure of the function. The generalization bounds are based on a measure of KG, which is a function complexity measure that measures how far the network is from the low KG zone of the classification function, and the authors show that this measure can be used as a regularizer for N2N regularization. The paper also provides a theoretical analysis of the KG and its relation to the test accuracy."
2426,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper studies the generalization error bounds of neural networks with Kolmogorov complexity. The authors propose a new measure of complexity that measures the complexity of the classifier’s function space in terms of the number of functions in the function space. The main contribution of this paper is to provide a generalization bound for neural networks that is based on the complexity measure of the function. The generalization bounds are based on a measure of KG, which is a function complexity measure that measures how far the network is from the low KG zone of the classification function, and the authors show that this measure can be used as a regularizer for N2N regularization. The paper also provides a theoretical analysis of the KG and its relation to the test accuracy."
2427,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper studies the generalization error bounds of neural networks with Kolmogorov complexity. The authors propose a new measure of complexity that measures the complexity of the classifier’s function space in terms of the number of functions in the function space. The main contribution of this paper is to provide a generalization bound for neural networks that is based on the complexity measure of the function. The generalization bounds are based on a measure of KG, which is a function complexity measure that measures how far the network is from the low KG zone of the classification function, and the authors show that this measure can be used as a regularizer for N2N regularization. The paper also provides a theoretical analysis of the KG and its relation to the test accuracy."
2428,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a new regularization term for self-supervised image representation learning. The proposed term is based on the variance of the embedding. The authors show that the proposed term can be applied to a variety of regularization terms, including batch normalization, feature-wise normalization and output quantization. Experiments on several downstream tasks demonstrate the effectiveness of the proposed method."
2429,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a new regularization term for self-supervised image representation learning. The proposed term is based on the variance of the embedding. The authors show that the proposed term can be applied to a variety of regularization terms, including batch normalization, feature-wise normalization and output quantization. Experiments on several downstream tasks demonstrate the effectiveness of the proposed method."
2430,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a new regularization term for self-supervised image representation learning. The proposed term is based on the variance of the embedding. The authors show that the proposed term can be applied to a variety of regularization terms, including batch normalization, feature-wise normalization and output quantization. Experiments on several downstream tasks demonstrate the effectiveness of the proposed method."
2431,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,"This paper proposes a new regularization term for self-supervised image representation learning. The proposed term is based on the variance of the embedding. The authors show that the proposed term can be applied to a variety of regularization terms, including batch normalization, feature-wise normalization and output quantization. Experiments on several downstream tasks demonstrate the effectiveness of the proposed method."
2432,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,This paper proposes a Bayesian model for information-directed reinforcement learning (IDRL) based on a reward model. The model is trained to predict the expected return of a policy in the presence of expert queries. The proposed method is evaluated on a variety of tasks and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of expected return.
2433,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,This paper proposes a Bayesian model for information-directed reinforcement learning (IDRL) based on a reward model. The model is trained to predict the expected return of a policy in the presence of expert queries. The proposed method is evaluated on a variety of tasks and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of expected return.
2434,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,This paper proposes a Bayesian model for information-directed reinforcement learning (IDRL) based on a reward model. The model is trained to predict the expected return of a policy in the presence of expert queries. The proposed method is evaluated on a variety of tasks and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of expected return.
2435,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,This paper proposes a Bayesian model for information-directed reinforcement learning (IDRL) based on a reward model. The model is trained to predict the expected return of a policy in the presence of expert queries. The proposed method is evaluated on a variety of tasks and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of expected return.
2436,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction on a large-scale dataset of neural architectures. The authors propose to use graph neural networks (GNNs) to predict the parameters of a neural network. The method is evaluated on CIFAR-10, ImageNet, and DEEPNETS-1M, and compared to ResNet-50 and ResNet50. The results show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2437,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction on a large-scale dataset of neural architectures. The authors propose to use graph neural networks (GNNs) to predict the parameters of a neural network. The method is evaluated on CIFAR-10, ImageNet, and DEEPNETS-1M, and compared to ResNet-50 and ResNet50. The results show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2438,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction on a large-scale dataset of neural architectures. The authors propose to use graph neural networks (GNNs) to predict the parameters of a neural network. The method is evaluated on CIFAR-10, ImageNet, and DEEPNETS-1M, and compared to ResNet-50 and ResNet50. The results show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2439,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"This paper proposes a method for parameter prediction on a large-scale dataset of neural architectures. The authors propose to use graph neural networks (GNNs) to predict the parameters of a neural network. The method is evaluated on CIFAR-10, ImageNet, and DEEPNETS-1M, and compared to ResNet-50 and ResNet50. The results show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2440,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff between the Wasserstein-2 perception index and the mean squared-error (MSE) distortion. The authors propose a closed form expression for the distortion-perception (DP) function, which can be expressed as a closed-form expression of the closed form of the WASSERSTEIN-2 (Wasserstein) perception index. The paper also proposes a new estimator for the MSE and the global MSE minimizer, which is based on a stochastic transformation of the former and the latter. The main contribution of the paper is to show that the proposed estimator can be viewed as a combination of the two existing estimators. The proposed estimators are shown to be optimal in the Gaussian setting."
2441,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff between the Wasserstein-2 perception index and the mean squared-error (MSE) distortion. The authors propose a closed form expression for the distortion-perception (DP) function, which can be expressed as a closed-form expression of the closed form of the WASSERSTEIN-2 (Wasserstein) perception index. The paper also proposes a new estimator for the MSE and the global MSE minimizer, which is based on a stochastic transformation of the former and the latter. The main contribution of the paper is to show that the proposed estimator can be viewed as a combination of the two existing estimators. The proposed estimators are shown to be optimal in the Gaussian setting."
2442,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff between the Wasserstein-2 perception index and the mean squared-error (MSE) distortion. The authors propose a closed form expression for the distortion-perception (DP) function, which can be expressed as a closed-form expression of the closed form of the WASSERSTEIN-2 (Wasserstein) perception index. The paper also proposes a new estimator for the MSE and the global MSE minimizer, which is based on a stochastic transformation of the former and the latter. The main contribution of the paper is to show that the proposed estimator can be viewed as a combination of the two existing estimators. The proposed estimators are shown to be optimal in the Gaussian setting."
2443,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"This paper studies the perception-distortion tradeoff between the Wasserstein-2 perception index and the mean squared-error (MSE) distortion. The authors propose a closed form expression for the distortion-perception (DP) function, which can be expressed as a closed-form expression of the closed form of the WASSERSTEIN-2 (Wasserstein) perception index. The paper also proposes a new estimator for the MSE and the global MSE minimizer, which is based on a stochastic transformation of the former and the latter. The main contribution of the paper is to show that the proposed estimator can be viewed as a combination of the two existing estimators. The proposed estimators are shown to be optimal in the Gaussian setting."
2444,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a cascaded model architecture for the task of independent modeling of textual features on a textual graph. The authors propose GraphFormers, which is a combination of GraphNets, GNNs, and transformer blocks of language models. In particular, the authors propose to combine GNN, transformer, and text encoder to form a single model. The model is trained with a progressive learning strategy, where the original data is used to train the model, and the manipulated data is fed to the model to improve its performance. The experimental results show that the proposed model achieves state-of-the-art performance on several benchmark datasets."
2445,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a cascaded model architecture for the task of independent modeling of textual features on a textual graph. The authors propose GraphFormers, which is a combination of GraphNets, GNNs, and transformer blocks of language models. In particular, the authors propose to combine GNN, transformer, and text encoder to form a single model. The model is trained with a progressive learning strategy, where the original data is used to train the model, and the manipulated data is fed to the model to improve its performance. The experimental results show that the proposed model achieves state-of-the-art performance on several benchmark datasets."
2446,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a cascaded model architecture for the task of independent modeling of textual features on a textual graph. The authors propose GraphFormers, which is a combination of GraphNets, GNNs, and transformer blocks of language models. In particular, the authors propose to combine GNN, transformer, and text encoder to form a single model. The model is trained with a progressive learning strategy, where the original data is used to train the model, and the manipulated data is fed to the model to improve its performance. The experimental results show that the proposed model achieves state-of-the-art performance on several benchmark datasets."
2447,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"This paper proposes a cascaded model architecture for the task of independent modeling of textual features on a textual graph. The authors propose GraphFormers, which is a combination of GraphNets, GNNs, and transformer blocks of language models. In particular, the authors propose to combine GNN, transformer, and text encoder to form a single model. The model is trained with a progressive learning strategy, where the original data is used to train the model, and the manipulated data is fed to the model to improve its performance. The experimental results show that the proposed model achieves state-of-the-art performance on several benchmark datasets."
2448,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy in the context of mean estimation and stochastic convex optimization. In particular, the authors consider the setting where the user has access to the mean of the algorithm, and the algorithm has to estimate the true mean. The authors provide a lower bound on the privacy cost of the algorithms, and show that it is O(1/n) times faster than the O(n) rate of private mean estimation. They also provide lower bounds on the error scaling of algorithms for mean estimation in the case of arbitrary dimension."
2449,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy in the context of mean estimation and stochastic convex optimization. In particular, the authors consider the setting where the user has access to the mean of the algorithm, and the algorithm has to estimate the true mean. The authors provide a lower bound on the privacy cost of the algorithms, and show that it is O(1/n) times faster than the O(n) rate of private mean estimation. They also provide lower bounds on the error scaling of algorithms for mean estimation in the case of arbitrary dimension."
2450,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy in the context of mean estimation and stochastic convex optimization. In particular, the authors consider the setting where the user has access to the mean of the algorithm, and the algorithm has to estimate the true mean. The authors provide a lower bound on the privacy cost of the algorithms, and show that it is O(1/n) times faster than the O(n) rate of private mean estimation. They also provide lower bounds on the error scaling of algorithms for mean estimation in the case of arbitrary dimension."
2451,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"This paper studies the problem of user-level differential privacy in the context of mean estimation and stochastic convex optimization. In particular, the authors consider the setting where the user has access to the mean of the algorithm, and the algorithm has to estimate the true mean. The authors provide a lower bound on the privacy cost of the algorithms, and show that it is O(1/n) times faster than the O(n) rate of private mean estimation. They also provide lower bounds on the error scaling of algorithms for mean estimation in the case of arbitrary dimension."
2452,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper studies the effect of feature learning on the finite-width/channel limit of deep neural networks (DNNs). The authors propose a self-consistent Gaussian process theory for finite-DNN and feature learning effects. The main contribution of the paper is to prove that the feature learning effect is independent of the width/channel of the DNN. The authors also provide a theoretical analysis of the effect on feature learning in the lazy learning regime. The experimental results on CIFAR-10 and Myrtle5 demonstrate the effectiveness of the proposed theory.
2453,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper studies the effect of feature learning on the finite-width/channel limit of deep neural networks (DNNs). The authors propose a self-consistent Gaussian process theory for finite-DNN and feature learning effects. The main contribution of the paper is to prove that the feature learning effect is independent of the width/channel of the DNN. The authors also provide a theoretical analysis of the effect on feature learning in the lazy learning regime. The experimental results on CIFAR-10 and Myrtle5 demonstrate the effectiveness of the proposed theory.
2454,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper studies the effect of feature learning on the finite-width/channel limit of deep neural networks (DNNs). The authors propose a self-consistent Gaussian process theory for finite-DNN and feature learning effects. The main contribution of the paper is to prove that the feature learning effect is independent of the width/channel of the DNN. The authors also provide a theoretical analysis of the effect on feature learning in the lazy learning regime. The experimental results on CIFAR-10 and Myrtle5 demonstrate the effectiveness of the proposed theory.
2455,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,This paper studies the effect of feature learning on the finite-width/channel limit of deep neural networks (DNNs). The authors propose a self-consistent Gaussian process theory for finite-DNN and feature learning effects. The main contribution of the paper is to prove that the feature learning effect is independent of the width/channel of the DNN. The authors also provide a theoretical analysis of the effect on feature learning in the lazy learning regime. The experimental results on CIFAR-10 and Myrtle5 demonstrate the effectiveness of the proposed theory.
2456,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the problem of learning compositional communication in the presence of noisy signals. The authors propose a novel training framework to train a neural network that learns to predict the compositionality of the input signal. The proposed method is based on the notion of “context independence” and “topographical similarity”, which is an extension of the work of Chen et al. (2018). The authors show that the proposed method outperforms existing methods on a variety of tasks."
2457,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the problem of learning compositional communication in the presence of noisy signals. The authors propose a novel training framework to train a neural network that learns to predict the compositionality of the input signal. The proposed method is based on the notion of “context independence” and “topographical similarity”, which is an extension of the work of Chen et al. (2018). The authors show that the proposed method outperforms existing methods on a variety of tasks."
2458,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the problem of learning compositional communication in the presence of noisy signals. The authors propose a novel training framework to train a neural network that learns to predict the compositionality of the input signal. The proposed method is based on the notion of “context independence” and “topographical similarity”, which is an extension of the work of Chen et al. (2018). The authors show that the proposed method outperforms existing methods on a variety of tasks."
2459,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"This paper studies the problem of learning compositional communication in the presence of noisy signals. The authors propose a novel training framework to train a neural network that learns to predict the compositionality of the input signal. The proposed method is based on the notion of “context independence” and “topographical similarity”, which is an extension of the work of Chen et al. (2018). The authors show that the proposed method outperforms existing methods on a variety of tasks."
2460,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper proposes a two-layer ResMLP architecture for image classification. The main idea is to use a residual network to replace the original ResNet network. The residual network is composed of two channels, one for each image patch and one for the labels. The authors show that the residual network can be used to train ResNet-like models on a self-supervised setup. The proposed method is evaluated on ImageNet and is shown to outperform ResNet."
2461,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper proposes a two-layer ResMLP architecture for image classification. The main idea is to use a residual network to replace the original ResNet network. The residual network is composed of two channels, one for each image patch and one for the labels. The authors show that the residual network can be used to train ResNet-like models on a self-supervised setup. The proposed method is evaluated on ImageNet and is shown to outperform ResNet."
2462,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper proposes a two-layer ResMLP architecture for image classification. The main idea is to use a residual network to replace the original ResNet network. The residual network is composed of two channels, one for each image patch and one for the labels. The authors show that the residual network can be used to train ResNet-like models on a self-supervised setup. The proposed method is evaluated on ImageNet and is shown to outperform ResNet."
2463,SP:9d326254d77a188baf5bde39229c09b3966b5418,"This paper proposes a two-layer ResMLP architecture for image classification. The main idea is to use a residual network to replace the original ResNet network. The residual network is composed of two channels, one for each image patch and one for the labels. The authors show that the residual network can be used to train ResNet-like models on a self-supervised setup. The proposed method is evaluated on ImageNet and is shown to outperform ResNet."
2464,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,This paper studies the problem of contextual search for multi-class classification. The authors propose a new reduction technique based on the Euclidean distance between the nearest neighbor partition of the data points. The proposed reduction technique is based on minimizing the distance between nearest neighbors of the classifier and the nearest classifier. They show that the proposed method can reduce the misclassification rate by a factor of at least 1/\epsilon. 
2465,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,This paper studies the problem of contextual search for multi-class classification. The authors propose a new reduction technique based on the Euclidean distance between the nearest neighbor partition of the data points. The proposed reduction technique is based on minimizing the distance between nearest neighbors of the classifier and the nearest classifier. They show that the proposed method can reduce the misclassification rate by a factor of at least 1/\epsilon. 
2466,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,This paper studies the problem of contextual search for multi-class classification. The authors propose a new reduction technique based on the Euclidean distance between the nearest neighbor partition of the data points. The proposed reduction technique is based on minimizing the distance between nearest neighbors of the classifier and the nearest classifier. They show that the proposed method can reduce the misclassification rate by a factor of at least 1/\epsilon. 
2467,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,This paper studies the problem of contextual search for multi-class classification. The authors propose a new reduction technique based on the Euclidean distance between the nearest neighbor partition of the data points. The proposed reduction technique is based on minimizing the distance between nearest neighbors of the classifier and the nearest classifier. They show that the proposed method can reduce the misclassification rate by a factor of at least 1/\epsilon. 
2468,SP:5c0114535065d5125349f00bafdbccc911461ede,This paper proposes a PAC-learning-based method for visual question answering (VQA). The main idea is to regularize the loss function of the VQA model to encourage the model to transfer knowledge between different tasks. The proposed method is evaluated on the GQA dataset.
2469,SP:5c0114535065d5125349f00bafdbccc911461ede,This paper proposes a PAC-learning-based method for visual question answering (VQA). The main idea is to regularize the loss function of the VQA model to encourage the model to transfer knowledge between different tasks. The proposed method is evaluated on the GQA dataset.
2470,SP:5c0114535065d5125349f00bafdbccc911461ede,This paper proposes a PAC-learning-based method for visual question answering (VQA). The main idea is to regularize the loss function of the VQA model to encourage the model to transfer knowledge between different tasks. The proposed method is evaluated on the GQA dataset.
2471,SP:5c0114535065d5125349f00bafdbccc911461ede,This paper proposes a PAC-learning-based method for visual question answering (VQA). The main idea is to regularize the loss function of the VQA model to encourage the model to transfer knowledge between different tasks. The proposed method is evaluated on the GQA dataset.
2472,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the memory complexity of unbounded-precision RNNs with fixed precision and bounded precision. The authors show that unbounded precision RNN can scale linearly with the number of neurons. They also show that the memory size of bounded precision and unbounded RNN is the same as that of fixed precision. Finally, the authors provide a theoretical analysis of the computational complexity of the memory module of a stacked RNN."
2473,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the memory complexity of unbounded-precision RNNs with fixed precision and bounded precision. The authors show that unbounded precision RNN can scale linearly with the number of neurons. They also show that the memory size of bounded precision and unbounded RNN is the same as that of fixed precision. Finally, the authors provide a theoretical analysis of the computational complexity of the memory module of a stacked RNN."
2474,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the memory complexity of unbounded-precision RNNs with fixed precision and bounded precision. The authors show that unbounded precision RNN can scale linearly with the number of neurons. They also show that the memory size of bounded precision and unbounded RNN is the same as that of fixed precision. Finally, the authors provide a theoretical analysis of the computational complexity of the memory module of a stacked RNN."
2475,SP:40fd96105e77063de4a07d4b36fe19385434c533,"This paper studies the memory complexity of unbounded-precision RNNs with fixed precision and bounded precision. The authors show that unbounded precision RNN can scale linearly with the number of neurons. They also show that the memory size of bounded precision and unbounded RNN is the same as that of fixed precision. Finally, the authors provide a theoretical analysis of the computational complexity of the memory module of a stacked RNN."
2476,SP:3f33489b98ba6145fd4e334669493f15a63455f4,This paper studies the under-covering bias of quantile regression in the setting of linear quantile function estimation. The main contribution of this paper is to provide a theoretical analysis of the under coverage bias in the case where the coverage of the data distribution is low. The analysis is based on the observation that under-completeness of the quantile estimator can be explained by the high-dimensional parameter estimation error of the estimator. The paper also provides an asymptotic analysis of this bias. The theoretical analysis is supported by numerical experiments on simulated and real data.
2477,SP:3f33489b98ba6145fd4e334669493f15a63455f4,This paper studies the under-covering bias of quantile regression in the setting of linear quantile function estimation. The main contribution of this paper is to provide a theoretical analysis of the under coverage bias in the case where the coverage of the data distribution is low. The analysis is based on the observation that under-completeness of the quantile estimator can be explained by the high-dimensional parameter estimation error of the estimator. The paper also provides an asymptotic analysis of this bias. The theoretical analysis is supported by numerical experiments on simulated and real data.
2478,SP:3f33489b98ba6145fd4e334669493f15a63455f4,This paper studies the under-covering bias of quantile regression in the setting of linear quantile function estimation. The main contribution of this paper is to provide a theoretical analysis of the under coverage bias in the case where the coverage of the data distribution is low. The analysis is based on the observation that under-completeness of the quantile estimator can be explained by the high-dimensional parameter estimation error of the estimator. The paper also provides an asymptotic analysis of this bias. The theoretical analysis is supported by numerical experiments on simulated and real data.
2479,SP:3f33489b98ba6145fd4e334669493f15a63455f4,This paper studies the under-covering bias of quantile regression in the setting of linear quantile function estimation. The main contribution of this paper is to provide a theoretical analysis of the under coverage bias in the case where the coverage of the data distribution is low. The analysis is based on the observation that under-completeness of the quantile estimator can be explained by the high-dimensional parameter estimation error of the estimator. The paper also provides an asymptotic analysis of this bias. The theoretical analysis is supported by numerical experiments on simulated and real data.
2480,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based reinforcement learning method for reinforcement learning (RL) based on reinforcement learning. The main idea is to use reinforcement learning to improve the performance of RL-based class-incremental learning (CIL) methods. In particular, the authors propose to use a policy-based RL algorithm to train a policy that can be used to re-train a classifier on a set of pseudo-CIL tasks. The authors show that the proposed method outperforms the baselines on ImageNet-Subset, CIFAR-100, and LUCIR+AANets."
2481,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based reinforcement learning method for reinforcement learning (RL) based on reinforcement learning. The main idea is to use reinforcement learning to improve the performance of RL-based class-incremental learning (CIL) methods. In particular, the authors propose to use a policy-based RL algorithm to train a policy that can be used to re-train a classifier on a set of pseudo-CIL tasks. The authors show that the proposed method outperforms the baselines on ImageNet-Subset, CIFAR-100, and LUCIR+AANets."
2482,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based reinforcement learning method for reinforcement learning (RL) based on reinforcement learning. The main idea is to use reinforcement learning to improve the performance of RL-based class-incremental learning (CIL) methods. In particular, the authors propose to use a policy-based RL algorithm to train a policy that can be used to re-train a classifier on a set of pseudo-CIL tasks. The authors show that the proposed method outperforms the baselines on ImageNet-Subset, CIFAR-100, and LUCIR+AANets."
2483,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"This paper proposes a reinforcement learning-based reinforcement learning method for reinforcement learning (RL) based on reinforcement learning. The main idea is to use reinforcement learning to improve the performance of RL-based class-incremental learning (CIL) methods. In particular, the authors propose to use a policy-based RL algorithm to train a policy that can be used to re-train a classifier on a set of pseudo-CIL tasks. The authors show that the proposed method outperforms the baselines on ImageNet-Subset, CIFAR-100, and LUCIR+AANets."
2484,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper studies the problem of speeding up local gradient descent in stochastic gradient descent (SGD). The authors propose a local SGD method, Local SGD, where the local gradient steps are computed in parallel. The authors show that the convergence rate of the proposed method is linear with respect to the number of local gradients. The main contribution of the paper is the analysis of the variance of local gradient step in SGD. "
2485,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper studies the problem of speeding up local gradient descent in stochastic gradient descent (SGD). The authors propose a local SGD method, Local SGD, where the local gradient steps are computed in parallel. The authors show that the convergence rate of the proposed method is linear with respect to the number of local gradients. The main contribution of the paper is the analysis of the variance of local gradient step in SGD. "
2486,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper studies the problem of speeding up local gradient descent in stochastic gradient descent (SGD). The authors propose a local SGD method, Local SGD, where the local gradient steps are computed in parallel. The authors show that the convergence rate of the proposed method is linear with respect to the number of local gradients. The main contribution of the paper is the analysis of the variance of local gradient step in SGD. "
2487,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"This paper studies the problem of speeding up local gradient descent in stochastic gradient descent (SGD). The authors propose a local SGD method, Local SGD, where the local gradient steps are computed in parallel. The authors show that the convergence rate of the proposed method is linear with respect to the number of local gradients. The main contribution of the paper is the analysis of the variance of local gradient step in SGD. "
2488,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies online lazy gradient descent (OGD) in the strongly convex domain. The main contribution of this paper is to provide an upper bound on the expected regret of OGD with respect to the number of adversarial opponents. In particular, the authors show that OGD can achieve O(\sqrt{N}) regret in the case of simplex and O(log(N)) regret for the case with i.i.d opponents. The authors also provide a lower bound of O(n^2) for the simplex case."
2489,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies online lazy gradient descent (OGD) in the strongly convex domain. The main contribution of this paper is to provide an upper bound on the expected regret of OGD with respect to the number of adversarial opponents. In particular, the authors show that OGD can achieve O(\sqrt{N}) regret in the case of simplex and O(log(N)) regret for the case with i.i.d opponents. The authors also provide a lower bound of O(n^2) for the simplex case."
2490,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies online lazy gradient descent (OGD) in the strongly convex domain. The main contribution of this paper is to provide an upper bound on the expected regret of OGD with respect to the number of adversarial opponents. In particular, the authors show that OGD can achieve O(\sqrt{N}) regret in the case of simplex and O(log(N)) regret for the case with i.i.d opponents. The authors also provide a lower bound of O(n^2) for the simplex case."
2491,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"This paper studies online lazy gradient descent (OGD) in the strongly convex domain. The main contribution of this paper is to provide an upper bound on the expected regret of OGD with respect to the number of adversarial opponents. In particular, the authors show that OGD can achieve O(\sqrt{N}) regret in the case of simplex and O(log(N)) regret for the case with i.i.d opponents. The authors also provide a lower bound of O(n^2) for the simplex case."
2492,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence between Affine-Invariant (AI) geometry and Bures-Wasserstein (BW) geometry for Riemannian optimization problems. The authors show that the affine-invariant AI geometry is non-positively curved, while the BW geometry has non-negative curvature. In particular, the authors prove a linear dependence between the AI metric and the BW metric on the symmetric positive definite (SPD) matrix manifold. They also provide a convergence analysis of the convergence rate of the proposed algorithm."
2493,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence between Affine-Invariant (AI) geometry and Bures-Wasserstein (BW) geometry for Riemannian optimization problems. The authors show that the affine-invariant AI geometry is non-positively curved, while the BW geometry has non-negative curvature. In particular, the authors prove a linear dependence between the AI metric and the BW metric on the symmetric positive definite (SPD) matrix manifold. They also provide a convergence analysis of the convergence rate of the proposed algorithm."
2494,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence between Affine-Invariant (AI) geometry and Bures-Wasserstein (BW) geometry for Riemannian optimization problems. The authors show that the affine-invariant AI geometry is non-positively curved, while the BW geometry has non-negative curvature. In particular, the authors prove a linear dependence between the AI metric and the BW metric on the symmetric positive definite (SPD) matrix manifold. They also provide a convergence analysis of the convergence rate of the proposed algorithm."
2495,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper studies the equivalence between Affine-Invariant (AI) geometry and Bures-Wasserstein (BW) geometry for Riemannian optimization problems. The authors show that the affine-invariant AI geometry is non-positively curved, while the BW geometry has non-negative curvature. In particular, the authors prove a linear dependence between the AI metric and the BW metric on the symmetric positive definite (SPD) matrix manifold. They also provide a convergence analysis of the convergence rate of the proposed algorithm."
2496,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for evaluating NLP models. The proposed framework is based on the Dynaboard framework, which is a combination of Dynascore and Dynabench. The main contribution of this paper is the introduction of a new metric for evaluating the performance of a NLP model. The new metric is a weighted combination of three metrics: throughput, robustness, and memory use. The paper also proposes a utility-based aggregation to evaluate the utility of a model."
2497,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for evaluating NLP models. The proposed framework is based on the Dynaboard framework, which is a combination of Dynascore and Dynabench. The main contribution of this paper is the introduction of a new metric for evaluating the performance of a NLP model. The new metric is a weighted combination of three metrics: throughput, robustness, and memory use. The paper also proposes a utility-based aggregation to evaluate the utility of a model."
2498,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for evaluating NLP models. The proposed framework is based on the Dynaboard framework, which is a combination of Dynascore and Dynabench. The main contribution of this paper is the introduction of a new metric for evaluating the performance of a NLP model. The new metric is a weighted combination of three metrics: throughput, robustness, and memory use. The paper also proposes a utility-based aggregation to evaluate the utility of a model."
2499,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper proposes a new evaluation-as-a-service framework for evaluating NLP models. The proposed framework is based on the Dynaboard framework, which is a combination of Dynascore and Dynabench. The main contribution of this paper is the introduction of a new metric for evaluating the performance of a NLP model. The new metric is a weighted combination of three metrics: throughput, robustness, and memory use. The paper also proposes a utility-based aggregation to evaluate the utility of a model."
2500,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. Specifically, the proposed Neural Dubber is based on the Transformer architecture. The proposed method is evaluated on the chemistry lecture dataset and the LRS2 multi-speaker dataset. "
2501,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. Specifically, the proposed Neural Dubber is based on the Transformer architecture. The proposed method is evaluated on the chemistry lecture dataset and the LRS2 multi-speaker dataset. "
2502,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. Specifically, the proposed Neural Dubber is based on the Transformer architecture. The proposed method is evaluated on the chemistry lecture dataset and the LRS2 multi-speaker dataset. "
2503,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper proposes a neural network model for automatic video dubbing (AVD) task. Specifically, the proposed Neural Dubber is based on the Transformer architecture. The proposed method is evaluated on the chemistry lecture dataset and the LRS2 multi-speaker dataset. "
2504,SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a Transformer-based deep learning framework for collaborative learning for COVID-19 diagnosis of chest X-ray (CXR) images. The proposed method is based on the Transformer architecture, which is a decomposable architecture that can be used for split learning. The method is evaluated on CXR datasets from multiple clients, and the results show that the proposed method outperforms the baselines. "
2505,SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a Transformer-based deep learning framework for collaborative learning for COVID-19 diagnosis of chest X-ray (CXR) images. The proposed method is based on the Transformer architecture, which is a decomposable architecture that can be used for split learning. The method is evaluated on CXR datasets from multiple clients, and the results show that the proposed method outperforms the baselines. "
2506,SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a Transformer-based deep learning framework for collaborative learning for COVID-19 diagnosis of chest X-ray (CXR) images. The proposed method is based on the Transformer architecture, which is a decomposable architecture that can be used for split learning. The method is evaluated on CXR datasets from multiple clients, and the results show that the proposed method outperforms the baselines. "
2507,SP:24ea12428bd675459f0509aa7cee821fa236382e,"This paper proposes a Transformer-based deep learning framework for collaborative learning for COVID-19 diagnosis of chest X-ray (CXR) images. The proposed method is based on the Transformer architecture, which is a decomposable architecture that can be used for split learning. The method is evaluated on CXR datasets from multiple clients, and the results show that the proposed method outperforms the baselines. "
2508,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,This paper proposes a differentiable point-to-mesh layer for Poisson Surface Reconstruction (PSR). The main contribution of this paper is to introduce a differentiability term for the PSR layer. The paper also proposes a method for learning a point cloud representation for PSR. The proposed method is evaluated on a variety of surface reconstruction tasks.
2509,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,This paper proposes a differentiable point-to-mesh layer for Poisson Surface Reconstruction (PSR). The main contribution of this paper is to introduce a differentiability term for the PSR layer. The paper also proposes a method for learning a point cloud representation for PSR. The proposed method is evaluated on a variety of surface reconstruction tasks.
2510,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,This paper proposes a differentiable point-to-mesh layer for Poisson Surface Reconstruction (PSR). The main contribution of this paper is to introduce a differentiability term for the PSR layer. The paper also proposes a method for learning a point cloud representation for PSR. The proposed method is evaluated on a variety of surface reconstruction tasks.
2511,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,This paper proposes a differentiable point-to-mesh layer for Poisson Surface Reconstruction (PSR). The main contribution of this paper is to introduce a differentiability term for the PSR layer. The paper also proposes a method for learning a point cloud representation for PSR. The proposed method is evaluated on a variety of surface reconstruction tasks.
2512,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a supervised inversion method for recovering representations of corrupted images. The proposed method is based on the contrastive objective, which aims to improve the robustness of the representation learned by a pre-trained representation learning network R(x) against the corrupted version A(x). The main idea is to use the forward operator of the labeled data to reconstruct the original image from the corrupted versions of the original images, and then use the backward operator to recover the original representation. Experiments on ImageNet and CLIP show that the proposed method outperforms end-to-end supervised baselines."
2513,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a supervised inversion method for recovering representations of corrupted images. The proposed method is based on the contrastive objective, which aims to improve the robustness of the representation learned by a pre-trained representation learning network R(x) against the corrupted version A(x). The main idea is to use the forward operator of the labeled data to reconstruct the original image from the corrupted versions of the original images, and then use the backward operator to recover the original representation. Experiments on ImageNet and CLIP show that the proposed method outperforms end-to-end supervised baselines."
2514,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a supervised inversion method for recovering representations of corrupted images. The proposed method is based on the contrastive objective, which aims to improve the robustness of the representation learned by a pre-trained representation learning network R(x) against the corrupted version A(x). The main idea is to use the forward operator of the labeled data to reconstruct the original image from the corrupted versions of the original images, and then use the backward operator to recover the original representation. Experiments on ImageNet and CLIP show that the proposed method outperforms end-to-end supervised baselines."
2515,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This paper proposes a supervised inversion method for recovering representations of corrupted images. The proposed method is based on the contrastive objective, which aims to improve the robustness of the representation learned by a pre-trained representation learning network R(x) against the corrupted version A(x). The main idea is to use the forward operator of the labeled data to reconstruct the original image from the corrupted versions of the original images, and then use the backward operator to recover the original representation. Experiments on ImageNet and CLIP show that the proposed method outperforms end-to-end supervised baselines."
2516,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper proposes a reinforcement learning method for the problem of structural credit assignment in neural networks. The main idea is to train a neural network on a finite-horizon reinforcement learning problem, where each node in the network is responsible for assigning a reward to each other. The authors propose a method to train the network using reinforcement learning. The method is based on the idea of backpropagation, and the authors show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2517,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper proposes a reinforcement learning method for the problem of structural credit assignment in neural networks. The main idea is to train a neural network on a finite-horizon reinforcement learning problem, where each node in the network is responsible for assigning a reward to each other. The authors propose a method to train the network using reinforcement learning. The method is based on the idea of backpropagation, and the authors show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2518,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper proposes a reinforcement learning method for the problem of structural credit assignment in neural networks. The main idea is to train a neural network on a finite-horizon reinforcement learning problem, where each node in the network is responsible for assigning a reward to each other. The authors propose a method to train the network using reinforcement learning. The method is based on the idea of backpropagation, and the authors show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2519,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,"This paper proposes a reinforcement learning method for the problem of structural credit assignment in neural networks. The main idea is to train a neural network on a finite-horizon reinforcement learning problem, where each node in the network is responsible for assigning a reward to each other. The authors propose a method to train the network using reinforcement learning. The method is based on the idea of backpropagation, and the authors show that the proposed method is able to achieve better performance than the state-of-the-art methods."
2520,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,This paper proposes a self-supervised predictive loss function for parallel pathways in the mouse visual cortex. The proposed loss function is based on a combination of the loss function of the dorsal and the ventral pathways. The authors show that the proposed method is able to improve the performance of the model compared to the state-of-the-art.
2521,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,This paper proposes a self-supervised predictive loss function for parallel pathways in the mouse visual cortex. The proposed loss function is based on a combination of the loss function of the dorsal and the ventral pathways. The authors show that the proposed method is able to improve the performance of the model compared to the state-of-the-art.
2522,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,This paper proposes a self-supervised predictive loss function for parallel pathways in the mouse visual cortex. The proposed loss function is based on a combination of the loss function of the dorsal and the ventral pathways. The authors show that the proposed method is able to improve the performance of the model compared to the state-of-the-art.
2523,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,This paper proposes a self-supervised predictive loss function for parallel pathways in the mouse visual cortex. The proposed loss function is based on a combination of the loss function of the dorsal and the ventral pathways. The authors show that the proposed method is able to improve the performance of the model compared to the state-of-the-art.
2524,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model for document embedding. The proposed model is based on a knowledge graph, which is a Gaussian-distributed embedding vector with symmetric and asymmetric similarities. The model is trained with a variational inference network and a regularization term. The authors show that the proposed model outperforms the state-of-the-art topic models on a variety of datasets."
2525,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model for document embedding. The proposed model is based on a knowledge graph, which is a Gaussian-distributed embedding vector with symmetric and asymmetric similarities. The model is trained with a variational inference network and a regularization term. The authors show that the proposed model outperforms the state-of-the-art topic models on a variety of datasets."
2526,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model for document embedding. The proposed model is based on a knowledge graph, which is a Gaussian-distributed embedding vector with symmetric and asymmetric similarities. The model is trained with a variational inference network and a regularization term. The authors show that the proposed model outperforms the state-of-the-art topic models on a variety of datasets."
2527,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper proposes TopicNet, a deep hierarchical topic model for document embedding. The proposed model is based on a knowledge graph, which is a Gaussian-distributed embedding vector with symmetric and asymmetric similarities. The model is trained with a variational inference network and a regularization term. The authors show that the proposed model outperforms the state-of-the-art topic models on a variety of datasets."
2528,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,This paper proposes a method for image-level contrastive learning for object detection. The key idea is to use a self-supervised pretext task and a downstream task to align the representations of the two tasks. The proposed method is evaluated on COCO and Mask R-CNN datasets.
2529,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,This paper proposes a method for image-level contrastive learning for object detection. The key idea is to use a self-supervised pretext task and a downstream task to align the representations of the two tasks. The proposed method is evaluated on COCO and Mask R-CNN datasets.
2530,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,This paper proposes a method for image-level contrastive learning for object detection. The key idea is to use a self-supervised pretext task and a downstream task to align the representations of the two tasks. The proposed method is evaluated on COCO and Mask R-CNN datasets.
2531,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,This paper proposes a method for image-level contrastive learning for object detection. The key idea is to use a self-supervised pretext task and a downstream task to align the representations of the two tasks. The proposed method is evaluated on COCO and Mask R-CNN datasets.
2532,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs). The proposed method is based on regression and subproblem selection. The main contribution of this paper is to propose a method for VRPs with linear number of subproblems and exponential number of problems. The method is evaluated on a variety of VRP solvers and variants.
2533,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs). The proposed method is based on regression and subproblem selection. The main contribution of this paper is to propose a method for VRPs with linear number of subproblems and exponential number of problems. The method is evaluated on a variety of VRP solvers and variants.
2534,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs). The proposed method is based on regression and subproblem selection. The main contribution of this paper is to propose a method for VRPs with linear number of subproblems and exponential number of problems. The method is evaluated on a variety of VRP solvers and variants.
2535,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,This paper proposes a learning-augmented local search framework for large-scale vehicle routing problems (VRPs). The proposed method is based on regression and subproblem selection. The main contribution of this paper is to propose a method for VRPs with linear number of subproblems and exponential number of problems. The method is evaluated on a variety of VRP solvers and variants.
2536,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The main idea is to use a Bayesian continual learning approach to train a neural network to learn new tasks. The method is based on the idea of active forgetting (AFEC), which is an extension of the active forgetting technique proposed in [1]. The main difference between AFEC and active forgetting is that AFEC does not require a prior knowledge of the new task, while active forgetting requires access to the previous task. The authors show that the proposed method is able to improve the performance on a variety of continual learning benchmarks, including CIFAR-10 regression tasks, Atari reinforcement learning, and visual classification tasks."
2537,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The main idea is to use a Bayesian continual learning approach to train a neural network to learn new tasks. The method is based on the idea of active forgetting (AFEC), which is an extension of the active forgetting technique proposed in [1]. The main difference between AFEC and active forgetting is that AFEC does not require a prior knowledge of the new task, while active forgetting requires access to the previous task. The authors show that the proposed method is able to improve the performance on a variety of continual learning benchmarks, including CIFAR-10 regression tasks, Atari reinforcement learning, and visual classification tasks."
2538,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The main idea is to use a Bayesian continual learning approach to train a neural network to learn new tasks. The method is based on the idea of active forgetting (AFEC), which is an extension of the active forgetting technique proposed in [1]. The main difference between AFEC and active forgetting is that AFEC does not require a prior knowledge of the new task, while active forgetting requires access to the previous task. The authors show that the proposed method is able to improve the performance on a variety of continual learning benchmarks, including CIFAR-10 regression tasks, Atari reinforcement learning, and visual classification tasks."
2539,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a method for continual learning based on active forgetting. The main idea is to use a Bayesian continual learning approach to train a neural network to learn new tasks. The method is based on the idea of active forgetting (AFEC), which is an extension of the active forgetting technique proposed in [1]. The main difference between AFEC and active forgetting is that AFEC does not require a prior knowledge of the new task, while active forgetting requires access to the previous task. The authors show that the proposed method is able to improve the performance on a variety of continual learning benchmarks, including CIFAR-10 regression tasks, Atari reinforcement learning, and visual classification tasks."
2540,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,This paper studies the convergence of greedy descent algorithms for Zeroth-order optimization (ZO) in the presence of prior information. The authors propose a greedy descent framework that uses finite differences between the gradient estimator and the prior information to estimate the gradient of the ZO algorithm. They also propose an accelerated random search (ARS) algorithm that leverages this prior information in order to speed up the optimization process. They provide convergence analysis for the proposed algorithm and show that it converges faster than existing greedy descent methods.
2541,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,This paper studies the convergence of greedy descent algorithms for Zeroth-order optimization (ZO) in the presence of prior information. The authors propose a greedy descent framework that uses finite differences between the gradient estimator and the prior information to estimate the gradient of the ZO algorithm. They also propose an accelerated random search (ARS) algorithm that leverages this prior information in order to speed up the optimization process. They provide convergence analysis for the proposed algorithm and show that it converges faster than existing greedy descent methods.
2542,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,This paper studies the convergence of greedy descent algorithms for Zeroth-order optimization (ZO) in the presence of prior information. The authors propose a greedy descent framework that uses finite differences between the gradient estimator and the prior information to estimate the gradient of the ZO algorithm. They also propose an accelerated random search (ARS) algorithm that leverages this prior information in order to speed up the optimization process. They provide convergence analysis for the proposed algorithm and show that it converges faster than existing greedy descent methods.
2543,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,This paper studies the convergence of greedy descent algorithms for Zeroth-order optimization (ZO) in the presence of prior information. The authors propose a greedy descent framework that uses finite differences between the gradient estimator and the prior information to estimate the gradient of the ZO algorithm. They also propose an accelerated random search (ARS) algorithm that leverages this prior information in order to speed up the optimization process. They provide convergence analysis for the proposed algorithm and show that it converges faster than existing greedy descent methods.
2544,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that under certain conditions, a pruned neural network is guaranteed to have zero generalization error, and that the generalization of the winning ticket can be guaranteed to be in the convex region of the loss function. The main contribution of this paper is to propose a new algorithm to prune the weights in the hidden layer of a neural network. The algorithm is based on the accelerated gradient descent (AGD) algorithm, which is shown to be faster than the standard SGD algorithm. "
2545,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that under certain conditions, a pruned neural network is guaranteed to have zero generalization error, and that the generalization of the winning ticket can be guaranteed to be in the convex region of the loss function. The main contribution of this paper is to propose a new algorithm to prune the weights in the hidden layer of a neural network. The algorithm is based on the accelerated gradient descent (AGD) algorithm, which is shown to be faster than the standard SGD algorithm. "
2546,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that under certain conditions, a pruned neural network is guaranteed to have zero generalization error, and that the generalization of the winning ticket can be guaranteed to be in the convex region of the loss function. The main contribution of this paper is to propose a new algorithm to prune the weights in the hidden layer of a neural network. The algorithm is based on the accelerated gradient descent (AGD) algorithm, which is shown to be faster than the standard SGD algorithm. "
2547,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper studies the lottery ticket hypothesis (LTH) for pruning multi-layer neural networks. The authors show that under certain conditions, a pruned neural network is guaranteed to have zero generalization error, and that the generalization of the winning ticket can be guaranteed to be in the convex region of the loss function. The main contribution of this paper is to propose a new algorithm to prune the weights in the hidden layer of a neural network. The algorithm is based on the accelerated gradient descent (AGD) algorithm, which is shown to be faster than the standard SGD algorithm. "
2548,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes a new algorithm for generating synthetic data with differential privacy. The algorithm is based on the exponential mechanism (EGM) and the private entropy projection (PEP) algorithms. The authors propose a novel algorithm called PMWPub, which is a combination of GEM and PEP. The main contribution of the paper is that the authors propose an iterative algorithm that can be used to generate synthetic data without releasing any prior information to the public. The paper is well-written and easy to follow. The experimental results show that the proposed algorithm outperforms the existing algorithms."
2549,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes a new algorithm for generating synthetic data with differential privacy. The algorithm is based on the exponential mechanism (EGM) and the private entropy projection (PEP) algorithms. The authors propose a novel algorithm called PMWPub, which is a combination of GEM and PEP. The main contribution of the paper is that the authors propose an iterative algorithm that can be used to generate synthetic data without releasing any prior information to the public. The paper is well-written and easy to follow. The experimental results show that the proposed algorithm outperforms the existing algorithms."
2550,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes a new algorithm for generating synthetic data with differential privacy. The algorithm is based on the exponential mechanism (EGM) and the private entropy projection (PEP) algorithms. The authors propose a novel algorithm called PMWPub, which is a combination of GEM and PEP. The main contribution of the paper is that the authors propose an iterative algorithm that can be used to generate synthetic data without releasing any prior information to the public. The paper is well-written and easy to follow. The experimental results show that the proposed algorithm outperforms the existing algorithms."
2551,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"This paper proposes a new algorithm for generating synthetic data with differential privacy. The algorithm is based on the exponential mechanism (EGM) and the private entropy projection (PEP) algorithms. The authors propose a novel algorithm called PMWPub, which is a combination of GEM and PEP. The main contribution of the paper is that the authors propose an iterative algorithm that can be used to generate synthetic data without releasing any prior information to the public. The paper is well-written and easy to follow. The experimental results show that the proposed algorithm outperforms the existing algorithms."
2552,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a new method for per-pixel mask classification for semantic and instance-level segmentation tasks. The proposed method, MaskFormer, is based on a single global class label prediction model, which is trained to predict the binary masks of binary masks. The method is evaluated on a variety of semantic and panoptic segmentation benchmarks. The results show that the proposed method outperforms the state-of-the-art methods."
2553,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a new method for per-pixel mask classification for semantic and instance-level segmentation tasks. The proposed method, MaskFormer, is based on a single global class label prediction model, which is trained to predict the binary masks of binary masks. The method is evaluated on a variety of semantic and panoptic segmentation benchmarks. The results show that the proposed method outperforms the state-of-the-art methods."
2554,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a new method for per-pixel mask classification for semantic and instance-level segmentation tasks. The proposed method, MaskFormer, is based on a single global class label prediction model, which is trained to predict the binary masks of binary masks. The method is evaluated on a variety of semantic and panoptic segmentation benchmarks. The results show that the proposed method outperforms the state-of-the-art methods."
2555,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a new method for per-pixel mask classification for semantic and instance-level segmentation tasks. The proposed method, MaskFormer, is based on a single global class label prediction model, which is trained to predict the binary masks of binary masks. The method is evaluated on a variety of semantic and panoptic segmentation benchmarks. The results show that the proposed method outperforms the state-of-the-art methods."
2556,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies random undercomplete two-layer ReLU neural networks with smooth activation function. The authors consider the case where the width of the network is subexponential in the dimension and the number of hidden layers is finite. They show that under this setting, gradient descent can be used to approximate the activation function of a subexponentially width random neural network. They also show that this can be done in the overcomplete case."
2557,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies random undercomplete two-layer ReLU neural networks with smooth activation function. The authors consider the case where the width of the network is subexponential in the dimension and the number of hidden layers is finite. They show that under this setting, gradient descent can be used to approximate the activation function of a subexponentially width random neural network. They also show that this can be done in the overcomplete case."
2558,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies random undercomplete two-layer ReLU neural networks with smooth activation function. The authors consider the case where the width of the network is subexponential in the dimension and the number of hidden layers is finite. They show that under this setting, gradient descent can be used to approximate the activation function of a subexponentially width random neural network. They also show that this can be done in the overcomplete case."
2559,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"This paper studies random undercomplete two-layer ReLU neural networks with smooth activation function. The authors consider the case where the width of the network is subexponential in the dimension and the number of hidden layers is finite. They show that under this setting, gradient descent can be used to approximate the activation function of a subexponentially width random neural network. They also show that this can be done in the overcomplete case."
2560,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) based on a variational autoencoder (VAE) framework. The authors propose a score matching objective (SGM) to reduce the variance of the training objective in the LSGM setting. The main contribution of this paper is to propose a new score-matching objective for LSGM. The proposed objective is based on the FID score of the target distribution, and the authors show that the proposed objective can be used to improve the sample quality of LSGM compared to the original SGM. Experiments on CIFAR-10, CelebA-HQ-256, and binarized OMNIGLOT demonstrate the effectiveness of the proposed method."
2561,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) based on a variational autoencoder (VAE) framework. The authors propose a score matching objective (SGM) to reduce the variance of the training objective in the LSGM setting. The main contribution of this paper is to propose a new score-matching objective for LSGM. The proposed objective is based on the FID score of the target distribution, and the authors show that the proposed objective can be used to improve the sample quality of LSGM compared to the original SGM. Experiments on CIFAR-10, CelebA-HQ-256, and binarized OMNIGLOT demonstrate the effectiveness of the proposed method."
2562,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) based on a variational autoencoder (VAE) framework. The authors propose a score matching objective (SGM) to reduce the variance of the training objective in the LSGM setting. The main contribution of this paper is to propose a new score-matching objective for LSGM. The proposed objective is based on the FID score of the target distribution, and the authors show that the proposed objective can be used to improve the sample quality of LSGM compared to the original SGM. Experiments on CIFAR-10, CelebA-HQ-256, and binarized OMNIGLOT demonstrate the effectiveness of the proposed method."
2563,SP:220db9ed147bbe67de5d82778720a1549656e48d,"This paper proposes a score-based generative model (LSGM) based on a variational autoencoder (VAE) framework. The authors propose a score matching objective (SGM) to reduce the variance of the training objective in the LSGM setting. The main contribution of this paper is to propose a new score-matching objective for LSGM. The proposed objective is based on the FID score of the target distribution, and the authors show that the proposed objective can be used to improve the sample quality of LSGM compared to the original SGM. Experiments on CIFAR-10, CelebA-HQ-256, and binarized OMNIGLOT demonstrate the effectiveness of the proposed method."
2564,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper studies the local signal adaptivity (LSA) phenomenon of convolutional neural networks (CNNs) in the presence of high-variance noise. Specifically, the authors show that CNNs trained with stochastic gradient descent (SGD) outperform the neural tangent kernel (NTK) on CIFAR-10 and MNIST images. The main contribution of this paper is to study the LSA phenomenon in the context of CNNs and NTKs. "
2565,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper studies the local signal adaptivity (LSA) phenomenon of convolutional neural networks (CNNs) in the presence of high-variance noise. Specifically, the authors show that CNNs trained with stochastic gradient descent (SGD) outperform the neural tangent kernel (NTK) on CIFAR-10 and MNIST images. The main contribution of this paper is to study the LSA phenomenon in the context of CNNs and NTKs. "
2566,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper studies the local signal adaptivity (LSA) phenomenon of convolutional neural networks (CNNs) in the presence of high-variance noise. Specifically, the authors show that CNNs trained with stochastic gradient descent (SGD) outperform the neural tangent kernel (NTK) on CIFAR-10 and MNIST images. The main contribution of this paper is to study the LSA phenomenon in the context of CNNs and NTKs. "
2567,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper studies the local signal adaptivity (LSA) phenomenon of convolutional neural networks (CNNs) in the presence of high-variance noise. Specifically, the authors show that CNNs trained with stochastic gradient descent (SGD) outperform the neural tangent kernel (NTK) on CIFAR-10 and MNIST images. The main contribution of this paper is to study the LSA phenomenon in the context of CNNs and NTKs. "
2568,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies the convergence of gradient tracking (GT) algorithms for decentralized machine learning in the case of noiseless and stochastic models. The authors show that the convergence rate is O(p-3/2) when the mixing parameter p is small, and O(1/\epsilon) when p is large. They also show that if p is larger than a certain threshold, then the convergence rates are O(2/3/4). The authors also provide a theoretical analysis of this phenomenon."
2569,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies the convergence of gradient tracking (GT) algorithms for decentralized machine learning in the case of noiseless and stochastic models. The authors show that the convergence rate is O(p-3/2) when the mixing parameter p is small, and O(1/\epsilon) when p is large. They also show that if p is larger than a certain threshold, then the convergence rates are O(2/3/4). The authors also provide a theoretical analysis of this phenomenon."
2570,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies the convergence of gradient tracking (GT) algorithms for decentralized machine learning in the case of noiseless and stochastic models. The authors show that the convergence rate is O(p-3/2) when the mixing parameter p is small, and O(1/\epsilon) when p is large. They also show that if p is larger than a certain threshold, then the convergence rates are O(2/3/4). The authors also provide a theoretical analysis of this phenomenon."
2571,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"This paper studies the convergence of gradient tracking (GT) algorithms for decentralized machine learning in the case of noiseless and stochastic models. The authors show that the convergence rate is O(p-3/2) when the mixing parameter p is small, and O(1/\epsilon) when p is large. They also show that if p is larger than a certain threshold, then the convergence rates are O(2/3/4). The authors also provide a theoretical analysis of this phenomenon."
2572,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies optimism-based MAB algorithms with UCB and Thompson Sampling. The authors show that UCB can achieve a regret of $O(\sqrt{n})$ when the mean rewards are small, and $O(log n)$ if the mean reward is large. They also provide a theoretical analysis of the regret of UCB with diffusion scaling."
2573,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies optimism-based MAB algorithms with UCB and Thompson Sampling. The authors show that UCB can achieve a regret of $O(\sqrt{n})$ when the mean rewards are small, and $O(log n)$ if the mean reward is large. They also provide a theoretical analysis of the regret of UCB with diffusion scaling."
2574,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies optimism-based MAB algorithms with UCB and Thompson Sampling. The authors show that UCB can achieve a regret of $O(\sqrt{n})$ when the mean rewards are small, and $O(log n)$ if the mean reward is large. They also provide a theoretical analysis of the regret of UCB with diffusion scaling."
2575,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"This paper studies optimism-based MAB algorithms with UCB and Thompson Sampling. The authors show that UCB can achieve a regret of $O(\sqrt{n})$ when the mean rewards are small, and $O(log n)$ if the mean reward is large. They also provide a theoretical analysis of the regret of UCB with diffusion scaling."
2576,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a new method for cross-domain recommendation (CDR) based on Stein path alignment. Specifically, the authors propose to align the Stein path between the target domain and the source domain to reduce the latent embedding discrepancy between the source and target domains. The proposed method is evaluated on two datasets, Douban and Amazon."
2577,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a new method for cross-domain recommendation (CDR) based on Stein path alignment. Specifically, the authors propose to align the Stein path between the target domain and the source domain to reduce the latent embedding discrepancy between the source and target domains. The proposed method is evaluated on two datasets, Douban and Amazon."
2578,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a new method for cross-domain recommendation (CDR) based on Stein path alignment. Specifically, the authors propose to align the Stein path between the target domain and the source domain to reduce the latent embedding discrepancy between the source and target domains. The proposed method is evaluated on two datasets, Douban and Amazon."
2579,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper proposes a new method for cross-domain recommendation (CDR) based on Stein path alignment. Specifically, the authors propose to align the Stein path between the target domain and the source domain to reduce the latent embedding discrepancy between the source and target domains. The proposed method is evaluated on two datasets, Douban and Amazon."
2580,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for self-attention in vision transformers. The proposed architecture is based on a global filter network (GFNet) that combines 2D discrete Fourier transform, element-wise multiplication, and 2D inverse Fourier transformer. The authors show that the proposed architecture can achieve better accuracy/complexity trade-off compared to CNNs and MLP models on ImageNet and downstream tasks."
2581,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for self-attention in vision transformers. The proposed architecture is based on a global filter network (GFNet) that combines 2D discrete Fourier transform, element-wise multiplication, and 2D inverse Fourier transformer. The authors show that the proposed architecture can achieve better accuracy/complexity trade-off compared to CNNs and MLP models on ImageNet and downstream tasks."
2582,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for self-attention in vision transformers. The proposed architecture is based on a global filter network (GFNet) that combines 2D discrete Fourier transform, element-wise multiplication, and 2D inverse Fourier transformer. The authors show that the proposed architecture can achieve better accuracy/complexity trade-off compared to CNNs and MLP models on ImageNet and downstream tasks."
2583,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper proposes a new architecture for self-attention in vision transformers. The proposed architecture is based on a global filter network (GFNet) that combines 2D discrete Fourier transform, element-wise multiplication, and 2D inverse Fourier transformer. The authors show that the proposed architecture can achieve better accuracy/complexity trade-off compared to CNNs and MLP models on ImageNet and downstream tasks."
2584,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper proposes a new trustworthiness prediction method based on the steep slope loss. The authors show that the proposed method outperforms other trustworthiness predictors on ImageNet and ResNet. The main contribution of the paper is that the authors propose a new loss function for trustworthiness predictions. The proposed loss function is a combination of focal loss, true class probability confidence loss, and cross entropy loss. "
2585,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper proposes a new trustworthiness prediction method based on the steep slope loss. The authors show that the proposed method outperforms other trustworthiness predictors on ImageNet and ResNet. The main contribution of the paper is that the authors propose a new loss function for trustworthiness predictions. The proposed loss function is a combination of focal loss, true class probability confidence loss, and cross entropy loss. "
2586,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper proposes a new trustworthiness prediction method based on the steep slope loss. The authors show that the proposed method outperforms other trustworthiness predictors on ImageNet and ResNet. The main contribution of the paper is that the authors propose a new loss function for trustworthiness predictions. The proposed loss function is a combination of focal loss, true class probability confidence loss, and cross entropy loss. "
2587,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"This paper proposes a new trustworthiness prediction method based on the steep slope loss. The authors show that the proposed method outperforms other trustworthiness predictors on ImageNet and ResNet. The main contribution of the paper is that the authors propose a new loss function for trustworthiness predictions. The proposed loss function is a combination of focal loss, true class probability confidence loss, and cross entropy loss. "
2588,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper proposes a clustering strategy to improve adversarial robustness of deep neural networks against adversarial examples. The proposed method is based on the observation that linearized sub-networks are more robust to adversarial attacks than non-linearized ones. The authors show that the proposed method can be applied to a variety of tasks such as domain adaptation, batch normalization, maximum pooling, and domain adaption. "
2589,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper proposes a clustering strategy to improve adversarial robustness of deep neural networks against adversarial examples. The proposed method is based on the observation that linearized sub-networks are more robust to adversarial attacks than non-linearized ones. The authors show that the proposed method can be applied to a variety of tasks such as domain adaptation, batch normalization, maximum pooling, and domain adaption. "
2590,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper proposes a clustering strategy to improve adversarial robustness of deep neural networks against adversarial examples. The proposed method is based on the observation that linearized sub-networks are more robust to adversarial attacks than non-linearized ones. The authors show that the proposed method can be applied to a variety of tasks such as domain adaptation, batch normalization, maximum pooling, and domain adaption. "
2591,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper proposes a clustering strategy to improve adversarial robustness of deep neural networks against adversarial examples. The proposed method is based on the observation that linearized sub-networks are more robust to adversarial attacks than non-linearized ones. The authors show that the proposed method can be applied to a variety of tasks such as domain adaptation, batch normalization, maximum pooling, and domain adaption. "
2592,SP:c5704a709f318c6e9a5c716e5e7f250acccf46a8,"This paper proposes a clustering strategy to improve adversarial robustness of deep neural networks against adversarial examples. The proposed method is based on the observation that linearized sub-networks are more robust to adversarial attacks than non-linearized ones. The authors show that the proposed method can be applied to a variety of tasks such as domain adaptation, batch normalization, maximum pooling, and domain adaption. "
2593,SP:590b67b1278267e966cf0b31456d981441e61bb1,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on the variational framework and iterative unrolling. The main contribution of this paper is the introduction of a regularizer for the reconstruction problem. The regularizer is a deep neural network that is trained to minimize the expected distortion and Wasserstein distance between the reconstruction operator and the reconstruction network. The authors show that the proposed method can achieve state-of-the-art performance on CT scans.
2594,SP:590b67b1278267e966cf0b31456d981441e61bb1,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on the variational framework and iterative unrolling. The main contribution of this paper is the introduction of a regularizer for the reconstruction problem. The regularizer is a deep neural network that is trained to minimize the expected distortion and Wasserstein distance between the reconstruction operator and the reconstruction network. The authors show that the proposed method can achieve state-of-the-art performance on CT scans.
2595,SP:590b67b1278267e966cf0b31456d981441e61bb1,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on the variational framework and iterative unrolling. The main contribution of this paper is the introduction of a regularizer for the reconstruction problem. The regularizer is a deep neural network that is trained to minimize the expected distortion and Wasserstein distance between the reconstruction operator and the reconstruction network. The authors show that the proposed method can achieve state-of-the-art performance on CT scans.
2596,SP:590b67b1278267e966cf0b31456d981441e61bb1,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on the variational framework and iterative unrolling. The main contribution of this paper is the introduction of a regularizer for the reconstruction problem. The regularizer is a deep neural network that is trained to minimize the expected distortion and Wasserstein distance between the reconstruction operator and the reconstruction network. The authors show that the proposed method can achieve state-of-the-art performance on CT scans.
2597,SP:590b67b1278267e966cf0b31456d981441e61bb1,This paper proposes an end-to-end unrolled reconstruction method for ill-posed inverse problems. The proposed method is based on the variational framework and iterative unrolling. The main contribution of this paper is the introduction of a regularizer for the reconstruction problem. The regularizer is a deep neural network that is trained to minimize the expected distortion and Wasserstein distance between the reconstruction operator and the reconstruction network. The authors show that the proposed method can achieve state-of-the-art performance on CT scans.
2598,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a self-training method for vision-language representation learning. The authors propose a contrastive loss for image and text representations, and a momentum distillation method for pseudo-targeting. The proposed method is evaluated on VQA and NLVR2 tasks, and compared with state-of-the-art methods."
2599,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a self-training method for vision-language representation learning. The authors propose a contrastive loss for image and text representations, and a momentum distillation method for pseudo-targeting. The proposed method is evaluated on VQA and NLVR2 tasks, and compared with state-of-the-art methods."
2600,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a self-training method for vision-language representation learning. The authors propose a contrastive loss for image and text representations, and a momentum distillation method for pseudo-targeting. The proposed method is evaluated on VQA and NLVR2 tasks, and compared with state-of-the-art methods."
2601,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a self-training method for vision-language representation learning. The authors propose a contrastive loss for image and text representations, and a momentum distillation method for pseudo-targeting. The proposed method is evaluated on VQA and NLVR2 tasks, and compared with state-of-the-art methods."
2602,SP:115d679338ab35829dbc594472d13cc02be5ed4c,"This paper proposes a self-training method for vision-language representation learning. The authors propose a contrastive loss for image and text representations, and a momentum distillation method for pseudo-targeting. The proposed method is evaluated on VQA and NLVR2 tasks, and compared with state-of-the-art methods."
2603,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) under the assumption of unrealizability. In particular, the authors consider the case of linear direct method (DM) and show that it is doubly robust to non-parametric consistency of the tile coding estimators. The authors also show that the proposed method can be used to estimate the OPE error in the realizable setting. The paper is well-written and easy to follow."
2604,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) under the assumption of unrealizability. In particular, the authors consider the case of linear direct method (DM) and show that it is doubly robust to non-parametric consistency of the tile coding estimators. The authors also show that the proposed method can be used to estimate the OPE error in the realizable setting. The paper is well-written and easy to follow."
2605,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) under the assumption of unrealizability. In particular, the authors consider the case of linear direct method (DM) and show that it is doubly robust to non-parametric consistency of the tile coding estimators. The authors also show that the proposed method can be used to estimate the OPE error in the realizable setting. The paper is well-written and easy to follow."
2606,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) under the assumption of unrealizability. In particular, the authors consider the case of linear direct method (DM) and show that it is doubly robust to non-parametric consistency of the tile coding estimators. The authors also show that the proposed method can be used to estimate the OPE error in the realizable setting. The paper is well-written and easy to follow."
2607,SP:e5323a171f40c109722a7ea0aebdcd53c151b72d,"This paper studies offline policy evaluation (OPE) in Markov decision processes (MDPs) under the assumption of unrealizability. In particular, the authors consider the case of linear direct method (DM) and show that it is doubly robust to non-parametric consistency of the tile coding estimators. The authors also show that the proposed method can be used to estimate the OPE error in the realizable setting. The paper is well-written and easy to follow."
2608,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,This paper studies the problem of non-smooth stochastic convex optimization. The main contribution of this paper is to extend the gradient clipping algorithm to the case of strongly convex problems. The authors show that the proposed algorithm converges to the optimal value of the objective with high probability. They also provide a lower bound on the complexity of the algorithm.
2609,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,This paper studies the problem of non-smooth stochastic convex optimization. The main contribution of this paper is to extend the gradient clipping algorithm to the case of strongly convex problems. The authors show that the proposed algorithm converges to the optimal value of the objective with high probability. They also provide a lower bound on the complexity of the algorithm.
2610,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,This paper studies the problem of non-smooth stochastic convex optimization. The main contribution of this paper is to extend the gradient clipping algorithm to the case of strongly convex problems. The authors show that the proposed algorithm converges to the optimal value of the objective with high probability. They also provide a lower bound on the complexity of the algorithm.
2611,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,This paper studies the problem of non-smooth stochastic convex optimization. The main contribution of this paper is to extend the gradient clipping algorithm to the case of strongly convex problems. The authors show that the proposed algorithm converges to the optimal value of the objective with high probability. They also provide a lower bound on the complexity of the algorithm.
2612,SP:b45f6966fcc07f3a33f70a57e72507b16fc7bb24,This paper studies the problem of non-smooth stochastic convex optimization. The main contribution of this paper is to extend the gradient clipping algorithm to the case of strongly convex problems. The authors show that the proposed algorithm converges to the optimal value of the objective with high probability. They also provide a lower bound on the complexity of the algorithm.
2613,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes Auto-Correlation, a method for long-term forecasting of time series. The main idea is to decompose the time series into sub-series and use a self-attention mechanism to learn the dependencies between sub-sub-series. The proposed method is based on stochastic process theory. The authors also propose a pre-processing convention for the series decomposition. Experimental results show that the proposed method achieves better performance than the existing Transformer-based models."
2614,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes Auto-Correlation, a method for long-term forecasting of time series. The main idea is to decompose the time series into sub-series and use a self-attention mechanism to learn the dependencies between sub-sub-series. The proposed method is based on stochastic process theory. The authors also propose a pre-processing convention for the series decomposition. Experimental results show that the proposed method achieves better performance than the existing Transformer-based models."
2615,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes Auto-Correlation, a method for long-term forecasting of time series. The main idea is to decompose the time series into sub-series and use a self-attention mechanism to learn the dependencies between sub-sub-series. The proposed method is based on stochastic process theory. The authors also propose a pre-processing convention for the series decomposition. Experimental results show that the proposed method achieves better performance than the existing Transformer-based models."
2616,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes Auto-Correlation, a method for long-term forecasting of time series. The main idea is to decompose the time series into sub-series and use a self-attention mechanism to learn the dependencies between sub-sub-series. The proposed method is based on stochastic process theory. The authors also propose a pre-processing convention for the series decomposition. Experimental results show that the proposed method achieves better performance than the existing Transformer-based models."
2617,SP:a22a893e25ce739dc757861741014764e78aa820,"This paper proposes Auto-Correlation, a method for long-term forecasting of time series. The main idea is to decompose the time series into sub-series and use a self-attention mechanism to learn the dependencies between sub-sub-series. The proposed method is based on stochastic process theory. The authors also propose a pre-processing convention for the series decomposition. Experimental results show that the proposed method achieves better performance than the existing Transformer-based models."
2618,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a new dataset for cryptic crossword puzzles. The authors propose a curriculum-based approach to train a neural language model to solve the cryptic crossword. The main idea is to train the model on a set of cryptic clues, and then use the learned model to generate new clues for the next task. The model is evaluated on a variety of tasks, including unscrambling words, wordplay, and wordplay cipher. "
2619,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a new dataset for cryptic crossword puzzles. The authors propose a curriculum-based approach to train a neural language model to solve the cryptic crossword. The main idea is to train the model on a set of cryptic clues, and then use the learned model to generate new clues for the next task. The model is evaluated on a variety of tasks, including unscrambling words, wordplay, and wordplay cipher. "
2620,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a new dataset for cryptic crossword puzzles. The authors propose a curriculum-based approach to train a neural language model to solve the cryptic crossword. The main idea is to train the model on a set of cryptic clues, and then use the learned model to generate new clues for the next task. The model is evaluated on a variety of tasks, including unscrambling words, wordplay, and wordplay cipher. "
2621,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a new dataset for cryptic crossword puzzles. The authors propose a curriculum-based approach to train a neural language model to solve the cryptic crossword. The main idea is to train the model on a set of cryptic clues, and then use the learned model to generate new clues for the next task. The model is evaluated on a variety of tasks, including unscrambling words, wordplay, and wordplay cipher. "
2622,SP:eeb2c3348de291c5eacac5d9de7b6b84ca030ad5,"This paper presents a new dataset for cryptic crossword puzzles. The authors propose a curriculum-based approach to train a neural language model to solve the cryptic crossword. The main idea is to train the model on a set of cryptic clues, and then use the learned model to generate new clues for the next task. The model is evaluated on a variety of tasks, including unscrambling words, wordplay, and wordplay cipher. "
2623,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper proposes a method to improve the performance of Vision Transformer (ViT) models. The main idea is to use self-attention and residual connections in the ViT to aggregate spatial information. The authors show that the proposed method is able to outperform CNNs and ViT on several image classification tasks. The experiments are conducted on MNIST, CIFAR-10, and ImageNet."
2624,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper proposes a method to improve the performance of Vision Transformer (ViT) models. The main idea is to use self-attention and residual connections in the ViT to aggregate spatial information. The authors show that the proposed method is able to outperform CNNs and ViT on several image classification tasks. The experiments are conducted on MNIST, CIFAR-10, and ImageNet."
2625,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper proposes a method to improve the performance of Vision Transformer (ViT) models. The main idea is to use self-attention and residual connections in the ViT to aggregate spatial information. The authors show that the proposed method is able to outperform CNNs and ViT on several image classification tasks. The experiments are conducted on MNIST, CIFAR-10, and ImageNet."
2626,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper proposes a method to improve the performance of Vision Transformer (ViT) models. The main idea is to use self-attention and residual connections in the ViT to aggregate spatial information. The authors show that the proposed method is able to outperform CNNs and ViT on several image classification tasks. The experiments are conducted on MNIST, CIFAR-10, and ImageNet."
2627,SP:7693974b70806d9b67920b8ddd2335afc4883319,"This paper proposes a method to improve the performance of Vision Transformer (ViT) models. The main idea is to use self-attention and residual connections in the ViT to aggregate spatial information. The authors show that the proposed method is able to outperform CNNs and ViT on several image classification tasks. The experiments are conducted on MNIST, CIFAR-10, and ImageNet."
2628,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) in combinatorial optimization problems. The authors show that TS can be used as an approximation oracle to the exact oracle in CMAB problems. In particular, the authors prove a regret lower bound of $O(1/\sqrt{T})$ for TS with a greedy oracle, and an almost matching regret upper bound for TS without greedy oracles. "
2629,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) in combinatorial optimization problems. The authors show that TS can be used as an approximation oracle to the exact oracle in CMAB problems. In particular, the authors prove a regret lower bound of $O(1/\sqrt{T})$ for TS with a greedy oracle, and an almost matching regret upper bound for TS without greedy oracles. "
2630,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) in combinatorial optimization problems. The authors show that TS can be used as an approximation oracle to the exact oracle in CMAB problems. In particular, the authors prove a regret lower bound of $O(1/\sqrt{T})$ for TS with a greedy oracle, and an almost matching regret upper bound for TS without greedy oracles. "
2631,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) in combinatorial optimization problems. The authors show that TS can be used as an approximation oracle to the exact oracle in CMAB problems. In particular, the authors prove a regret lower bound of $O(1/\sqrt{T})$ for TS with a greedy oracle, and an almost matching regret upper bound for TS without greedy oracles. "
2632,SP:dfd740399e48b946f02efdec823b8975a900f6a3,"This paper studies Thompson Sampling (TS) in combinatorial optimization problems. The authors show that TS can be used as an approximation oracle to the exact oracle in CMAB problems. In particular, the authors prove a regret lower bound of $O(1/\sqrt{T})$ for TS with a greedy oracle, and an almost matching regret upper bound for TS without greedy oracles. "
2633,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in a hedonic game where the goal is to minimize the total error. The authors consider the case where the number of players is bounded by a constant factor. The main contribution of this paper is to provide a theoretical analysis of the optimal (error minimizing) arrangement of players in the game, and provide a constant-factor bound for the optimal solution of the game. The analysis is based on a game theoretic framework, and the authors show that the optimal optimal solution can be found by minimizing the Price of Anarchy."
2634,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in a hedonic game where the goal is to minimize the total error. The authors consider the case where the number of players is bounded by a constant factor. The main contribution of this paper is to provide a theoretical analysis of the optimal (error minimizing) arrangement of players in the game, and provide a constant-factor bound for the optimal solution of the game. The analysis is based on a game theoretic framework, and the authors show that the optimal optimal solution can be found by minimizing the Price of Anarchy."
2635,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in a hedonic game where the goal is to minimize the total error. The authors consider the case where the number of players is bounded by a constant factor. The main contribution of this paper is to provide a theoretical analysis of the optimal (error minimizing) arrangement of players in the game, and provide a constant-factor bound for the optimal solution of the game. The analysis is based on a game theoretic framework, and the authors show that the optimal optimal solution can be found by minimizing the Price of Anarchy."
2636,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in a hedonic game where the goal is to minimize the total error. The authors consider the case where the number of players is bounded by a constant factor. The main contribution of this paper is to provide a theoretical analysis of the optimal (error minimizing) arrangement of players in the game, and provide a constant-factor bound for the optimal solution of the game. The analysis is based on a game theoretic framework, and the authors show that the optimal optimal solution can be found by minimizing the Price of Anarchy."
2637,SP:3ca7fdaba9793a61a1f9d264a551fe895e55dd99,"This paper studies the problem of federated learning in a hedonic game where the goal is to minimize the total error. The authors consider the case where the number of players is bounded by a constant factor. The main contribution of this paper is to provide a theoretical analysis of the optimal (error minimizing) arrangement of players in the game, and provide a constant-factor bound for the optimal solution of the game. The analysis is based on a game theoretic framework, and the authors show that the optimal optimal solution can be found by minimizing the Price of Anarchy."
2638,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The capsule architecture is based on permutation-equivariant attention, and the capsule decompositions of objects are semantically consistent and equivariant. The authors also propose a canonicalization operation for object-centric reasoning. The experimental results show that the proposed method outperforms the state-of-the-art."
2639,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The capsule architecture is based on permutation-equivariant attention, and the capsule decompositions of objects are semantically consistent and equivariant. The authors also propose a canonicalization operation for object-centric reasoning. The experimental results show that the proposed method outperforms the state-of-the-art."
2640,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The capsule architecture is based on permutation-equivariant attention, and the capsule decompositions of objects are semantically consistent and equivariant. The authors also propose a canonicalization operation for object-centric reasoning. The experimental results show that the proposed method outperforms the state-of-the-art."
2641,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The capsule architecture is based on permutation-equivariant attention, and the capsule decompositions of objects are semantically consistent and equivariant. The authors also propose a canonicalization operation for object-centric reasoning. The experimental results show that the proposed method outperforms the state-of-the-art."
2642,SP:17088db004fbf4902c5c3d53e387d1b68f4d69a5,"This paper proposes a self-supervised capsule architecture for 3D point cloud reconstruction. The capsule architecture is based on permutation-equivariant attention, and the capsule decompositions of objects are semantically consistent and equivariant. The authors also propose a canonicalization operation for object-centric reasoning. The experimental results show that the proposed method outperforms the state-of-the-art."
2643,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method for estimating the prediction intervals for nonparametric regression with histograms. The proposed method is based on the notion of conditional coverage, which is defined as the distance between the conditional distribution and the true distribution. The authors show that the proposed method can be used to estimate the conditional coverage and the optimal length of the prediction interval. They also provide a theoretical analysis of the performance of their proposed method."
2644,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method for estimating the prediction intervals for nonparametric regression with histograms. The proposed method is based on the notion of conditional coverage, which is defined as the distance between the conditional distribution and the true distribution. The authors show that the proposed method can be used to estimate the conditional coverage and the optimal length of the prediction interval. They also provide a theoretical analysis of the performance of their proposed method."
2645,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method for estimating the prediction intervals for nonparametric regression with histograms. The proposed method is based on the notion of conditional coverage, which is defined as the distance between the conditional distribution and the true distribution. The authors show that the proposed method can be used to estimate the conditional coverage and the optimal length of the prediction interval. They also provide a theoretical analysis of the performance of their proposed method."
2646,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method for estimating the prediction intervals for nonparametric regression with histograms. The proposed method is based on the notion of conditional coverage, which is defined as the distance between the conditional distribution and the true distribution. The authors show that the proposed method can be used to estimate the conditional coverage and the optimal length of the prediction interval. They also provide a theoretical analysis of the performance of their proposed method."
2647,SP:34cc3466ff7786968f437007b6af7d9ffd4decc7,"This paper proposes a conformal method for estimating the prediction intervals for nonparametric regression with histograms. The proposed method is based on the notion of conditional coverage, which is defined as the distance between the conditional distribution and the true distribution. The authors show that the proposed method can be used to estimate the conditional coverage and the optimal length of the prediction interval. They also provide a theoretical analysis of the performance of their proposed method."
2648,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,This paper studies the problem of invariance in kernel ridge regression. The authors consider the case where the function space is reproducing and the kernel is a kernel. The main contribution of this paper is to propose a new invariance measure based on feature averaging. The proposed measure is based on the difference between the effective dimension of the kernel and the feature average of the function. 
2649,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,This paper studies the problem of invariance in kernel ridge regression. The authors consider the case where the function space is reproducing and the kernel is a kernel. The main contribution of this paper is to propose a new invariance measure based on feature averaging. The proposed measure is based on the difference between the effective dimension of the kernel and the feature average of the function. 
2650,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,This paper studies the problem of invariance in kernel ridge regression. The authors consider the case where the function space is reproducing and the kernel is a kernel. The main contribution of this paper is to propose a new invariance measure based on feature averaging. The proposed measure is based on the difference between the effective dimension of the kernel and the feature average of the function. 
2651,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,This paper studies the problem of invariance in kernel ridge regression. The authors consider the case where the function space is reproducing and the kernel is a kernel. The main contribution of this paper is to propose a new invariance measure based on feature averaging. The proposed measure is based on the difference between the effective dimension of the kernel and the feature average of the function. 
2652,SP:d39075aff611dd54574e7ee1a1aeacce83fdf532,This paper studies the problem of invariance in kernel ridge regression. The authors consider the case where the function space is reproducing and the kernel is a kernel. The main contribution of this paper is to propose a new invariance measure based on feature averaging. The proposed measure is based on the difference between the effective dimension of the kernel and the feature average of the function. 
2653,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model for neural network activations. The main idea is to use a decoder-decoder model to generate a set of latent variables for each layer of a neural network, which are then used to train an ensemble of decoders. The decoder is trained to predict the output of the decoder, which is then used as the input to the ensemble decoder. The authors show that the proposed method can be used for out-of-distribution detection, adversarial example detection, and calibration."
2654,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model for neural network activations. The main idea is to use a decoder-decoder model to generate a set of latent variables for each layer of a neural network, which are then used to train an ensemble of decoders. The decoder is trained to predict the output of the decoder, which is then used as the input to the ensemble decoder. The authors show that the proposed method can be used for out-of-distribution detection, adversarial example detection, and calibration."
2655,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model for neural network activations. The main idea is to use a decoder-decoder model to generate a set of latent variables for each layer of a neural network, which are then used to train an ensemble of decoders. The decoder is trained to predict the output of the decoder, which is then used as the input to the ensemble decoder. The authors show that the proposed method can be used for out-of-distribution detection, adversarial example detection, and calibration."
2656,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model for neural network activations. The main idea is to use a decoder-decoder model to generate a set of latent variables for each layer of a neural network, which are then used to train an ensemble of decoders. The decoder is trained to predict the output of the decoder, which is then used as the input to the ensemble decoder. The authors show that the proposed method can be used for out-of-distribution detection, adversarial example detection, and calibration."
2657,SP:97fac361b69ed5871a60dc40e51900747a453df9,"This paper proposes a generative model for neural network activations. The main idea is to use a decoder-decoder model to generate a set of latent variables for each layer of a neural network, which are then used to train an ensemble of decoders. The decoder is trained to predict the output of the decoder, which is then used as the input to the ensemble decoder. The authors show that the proposed method can be used for out-of-distribution detection, adversarial example detection, and calibration."
2658,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of optimal transport (OT) estimators. The authors consider the case where the optimal transport maps are barycentric projections of the Wasserstein barycenter, and the authors show that the rates of convergence of OT estimators are bounded by the curse of dimensionality. They also provide asymptotic detection thresholds for optimal transport based tests of independence. "
2659,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of optimal transport (OT) estimators. The authors consider the case where the optimal transport maps are barycentric projections of the Wasserstein barycenter, and the authors show that the rates of convergence of OT estimators are bounded by the curse of dimensionality. They also provide asymptotic detection thresholds for optimal transport based tests of independence. "
2660,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of optimal transport (OT) estimators. The authors consider the case where the optimal transport maps are barycentric projections of the Wasserstein barycenter, and the authors show that the rates of convergence of OT estimators are bounded by the curse of dimensionality. They also provide asymptotic detection thresholds for optimal transport based tests of independence. "
2661,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of optimal transport (OT) estimators. The authors consider the case where the optimal transport maps are barycentric projections of the Wasserstein barycenter, and the authors show that the rates of convergence of OT estimators are bounded by the curse of dimensionality. They also provide asymptotic detection thresholds for optimal transport based tests of independence. "
2662,SP:3f10ca1e7f8fef6cb0c5957ec2f0689fb9bed753,"This paper studies the convergence of optimal transport (OT) estimators. The authors consider the case where the optimal transport maps are barycentric projections of the Wasserstein barycenter, and the authors show that the rates of convergence of OT estimators are bounded by the curse of dimensionality. They also provide asymptotic detection thresholds for optimal transport based tests of independence. "
2663,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The authors propose to use infinitely wide convolutional neural networks to train a distributed Kernel-based Meta-Learning framework. The proposed method is evaluated on CIFAR-10, Fashion-MNIST, and Fashion-100 datasets. The experiments show that the proposed method outperforms other distillation methods in terms of test accuracy and training efficiency."
2664,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The authors propose to use infinitely wide convolutional neural networks to train a distributed Kernel-based Meta-Learning framework. The proposed method is evaluated on CIFAR-10, Fashion-MNIST, and Fashion-100 datasets. The experiments show that the proposed method outperforms other distillation methods in terms of test accuracy and training efficiency."
2665,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The authors propose to use infinitely wide convolutional neural networks to train a distributed Kernel-based Meta-Learning framework. The proposed method is evaluated on CIFAR-10, Fashion-MNIST, and Fashion-100 datasets. The experiments show that the proposed method outperforms other distillation methods in terms of test accuracy and training efficiency."
2666,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The authors propose to use infinitely wide convolutional neural networks to train a distributed Kernel-based Meta-Learning framework. The proposed method is evaluated on CIFAR-10, Fashion-MNIST, and Fashion-100 datasets. The experiments show that the proposed method outperforms other distillation methods in terms of test accuracy and training efficiency."
2667,SP:573fbdbe5857c4aace1dfc27e25b8d65a18c9b96,"This paper proposes a distributed kernel-based meta-learning framework for dataset distillation. The authors propose to use infinitely wide convolutional neural networks to train a distributed Kernel-based Meta-Learning framework. The proposed method is evaluated on CIFAR-10, Fashion-MNIST, and Fashion-100 datasets. The experiments show that the proposed method outperforms other distillation methods in terms of test accuracy and training efficiency."
2668,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes a new semi-supervised learning method, called OpenMatch, which learns representations of outliers in unlabeled data. The authors propose a soft-consistency regularization loss to improve the smoothness of the OVA-classifier and a confidence score for outlier detection. The proposed method is evaluated on CIFAR-10 and ImageNet."
2669,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes a new semi-supervised learning method, called OpenMatch, which learns representations of outliers in unlabeled data. The authors propose a soft-consistency regularization loss to improve the smoothness of the OVA-classifier and a confidence score for outlier detection. The proposed method is evaluated on CIFAR-10 and ImageNet."
2670,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes a new semi-supervised learning method, called OpenMatch, which learns representations of outliers in unlabeled data. The authors propose a soft-consistency regularization loss to improve the smoothness of the OVA-classifier and a confidence score for outlier detection. The proposed method is evaluated on CIFAR-10 and ImageNet."
2671,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes a new semi-supervised learning method, called OpenMatch, which learns representations of outliers in unlabeled data. The authors propose a soft-consistency regularization loss to improve the smoothness of the OVA-classifier and a confidence score for outlier detection. The proposed method is evaluated on CIFAR-10 and ImageNet."
2672,SP:9837e0c68887cc1382aefd0ead01f72cde199e0d,"This paper proposes a new semi-supervised learning method, called OpenMatch, which learns representations of outliers in unlabeled data. The authors propose a soft-consistency regularization loss to improve the smoothness of the OVA-classifier and a confidence score for outlier detection. The proposed method is evaluated on CIFAR-10 and ImageNet."
2673,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes Latent Explorer Achiever (LEXA), a method for unsupervised goal reaching. The proposed method is based on the idea that the agent should be able to reach goals that are unseen by the agent. The agent is trained using an image-to-image model, where the agent is given an image of the environment and the goal, and is encouraged to reach the goal as soon as possible. This is achieved by training a policy that is a combination of an exploration policy and an achiever policy. The exploration policy is trained to maximize the likelihood of reaching the goal and the achiever is trained on the unseen goal. Experiments show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation and locomotion tasks."
2674,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes Latent Explorer Achiever (LEXA), a method for unsupervised goal reaching. The proposed method is based on the idea that the agent should be able to reach goals that are unseen by the agent. The agent is trained using an image-to-image model, where the agent is given an image of the environment and the goal, and is encouraged to reach the goal as soon as possible. This is achieved by training a policy that is a combination of an exploration policy and an achiever policy. The exploration policy is trained to maximize the likelihood of reaching the goal and the achiever is trained on the unseen goal. Experiments show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation and locomotion tasks."
2675,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes Latent Explorer Achiever (LEXA), a method for unsupervised goal reaching. The proposed method is based on the idea that the agent should be able to reach goals that are unseen by the agent. The agent is trained using an image-to-image model, where the agent is given an image of the environment and the goal, and is encouraged to reach the goal as soon as possible. This is achieved by training a policy that is a combination of an exploration policy and an achiever policy. The exploration policy is trained to maximize the likelihood of reaching the goal and the achiever is trained on the unseen goal. Experiments show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation and locomotion tasks."
2676,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes Latent Explorer Achiever (LEXA), a method for unsupervised goal reaching. The proposed method is based on the idea that the agent should be able to reach goals that are unseen by the agent. The agent is trained using an image-to-image model, where the agent is given an image of the environment and the goal, and is encouraged to reach the goal as soon as possible. This is achieved by training a policy that is a combination of an exploration policy and an achiever policy. The exploration policy is trained to maximize the likelihood of reaching the goal and the achiever is trained on the unseen goal. Experiments show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation and locomotion tasks."
2677,SP:eb86d33d5d47f1cfe2c66ca2c9f852229e32a32f,"This paper proposes Latent Explorer Achiever (LEXA), a method for unsupervised goal reaching. The proposed method is based on the idea that the agent should be able to reach goals that are unseen by the agent. The agent is trained using an image-to-image model, where the agent is given an image of the environment and the goal, and is encouraged to reach the goal as soon as possible. This is achieved by training a policy that is a combination of an exploration policy and an achiever policy. The exploration policy is trained to maximize the likelihood of reaching the goal and the achiever is trained on the unseen goal. Experiments show that the proposed method outperforms the state-of-the-art on a variety of robotic manipulation and locomotion tasks."
2678,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper studies the lottery ticket hypothesis of deep neural networks. The authors propose a method to learn a low-rank representation of the parameter matrix of a deep neural network. The proposed method is based on the idea of reshaped and rearranged original matrix, which can be used to improve the expressiveness of the network. Experiments are conducted on Transformer models and show that the proposed method can improve the on-task performance."
2679,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper studies the lottery ticket hypothesis of deep neural networks. The authors propose a method to learn a low-rank representation of the parameter matrix of a deep neural network. The proposed method is based on the idea of reshaped and rearranged original matrix, which can be used to improve the expressiveness of the network. Experiments are conducted on Transformer models and show that the proposed method can improve the on-task performance."
2680,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper studies the lottery ticket hypothesis of deep neural networks. The authors propose a method to learn a low-rank representation of the parameter matrix of a deep neural network. The proposed method is based on the idea of reshaped and rearranged original matrix, which can be used to improve the expressiveness of the network. Experiments are conducted on Transformer models and show that the proposed method can improve the on-task performance."
2681,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper studies the lottery ticket hypothesis of deep neural networks. The authors propose a method to learn a low-rank representation of the parameter matrix of a deep neural network. The proposed method is based on the idea of reshaped and rearranged original matrix, which can be used to improve the expressiveness of the network. Experiments are conducted on Transformer models and show that the proposed method can improve the on-task performance."
2682,SP:2db4aba9a370df67f786157f18cbaa4167c6a46d,"This paper studies the lottery ticket hypothesis of deep neural networks. The authors propose a method to learn a low-rank representation of the parameter matrix of a deep neural network. The proposed method is based on the idea of reshaped and rearranged original matrix, which can be used to improve the expressiveness of the network. Experiments are conducted on Transformer models and show that the proposed method can improve the on-task performance."
2683,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,This paper proposes a Transformer-based model for code summarization. The main idea is to learn a positional encoding of the paths in the syntax tree. The authors propose to use a transformer-based architecture to encode the paths. The proposed method is evaluated on code summarisation tasks and compared to several baselines.
2684,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,This paper proposes a Transformer-based model for code summarization. The main idea is to learn a positional encoding of the paths in the syntax tree. The authors propose to use a transformer-based architecture to encode the paths. The proposed method is evaluated on code summarisation tasks and compared to several baselines.
2685,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,This paper proposes a Transformer-based model for code summarization. The main idea is to learn a positional encoding of the paths in the syntax tree. The authors propose to use a transformer-based architecture to encode the paths. The proposed method is evaluated on code summarisation tasks and compared to several baselines.
2686,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,This paper proposes a Transformer-based model for code summarization. The main idea is to learn a positional encoding of the paths in the syntax tree. The authors propose to use a transformer-based architecture to encode the paths. The proposed method is evaluated on code summarisation tasks and compared to several baselines.
2687,SP:0ff862542ada5b664d615c26e7a4a95b6cbe540e,This paper proposes a Transformer-based model for code summarization. The main idea is to learn a positional encoding of the paths in the syntax tree. The authors propose to use a transformer-based architecture to encode the paths. The proposed method is evaluated on code summarisation tasks and compared to several baselines.
2688,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new self-attention module for GANs. The proposed module is based on a multi-layer perceptron. The authors show that the proposed module has linear computational complexity compared to the original self attention module. Moreover, the authors also show that their proposed module can achieve better FID scores than the original module."
2689,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new self-attention module for GANs. The proposed module is based on a multi-layer perceptron. The authors show that the proposed module has linear computational complexity compared to the original self attention module. Moreover, the authors also show that their proposed module can achieve better FID scores than the original module."
2690,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new self-attention module for GANs. The proposed module is based on a multi-layer perceptron. The authors show that the proposed module has linear computational complexity compared to the original self attention module. Moreover, the authors also show that their proposed module can achieve better FID scores than the original module."
2691,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new self-attention module for GANs. The proposed module is based on a multi-layer perceptron. The authors show that the proposed module has linear computational complexity compared to the original self attention module. Moreover, the authors also show that their proposed module can achieve better FID scores than the original module."
2692,SP:727bcd651b11b7d84dd2c2d535cc85402f9117d4,"This paper proposes a new self-attention module for GANs. The proposed module is based on a multi-layer perceptron. The authors show that the proposed module has linear computational complexity compared to the original self attention module. Moreover, the authors also show that their proposed module can achieve better FID scores than the original module."
2693,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors consider the case where the cut size of the graph is larger than the number of nodes in the graph. The main contribution of this paper is to derive a lower bound on the query cost of learning the geodesic convex set. The lower bound is based on the Radon number of the vertices.
2694,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors consider the case where the cut size of the graph is larger than the number of nodes in the graph. The main contribution of this paper is to derive a lower bound on the query cost of learning the geodesic convex set. The lower bound is based on the Radon number of the vertices.
2695,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors consider the case where the cut size of the graph is larger than the number of nodes in the graph. The main contribution of this paper is to derive a lower bound on the query cost of learning the geodesic convex set. The lower bound is based on the Radon number of the vertices.
2696,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors consider the case where the cut size of the graph is larger than the number of nodes in the graph. The main contribution of this paper is to derive a lower bound on the query cost of learning the geodesic convex set. The lower bound is based on the Radon number of the vertices.
2697,SP:41a6753bc56eb16040600666a859294ae36cfa9c,This paper studies the query complexity of learning geodesically convex halfspaces on graphs. The authors consider the case where the cut size of the graph is larger than the number of nodes in the graph. The main contribution of this paper is to derive a lower bound on the query cost of learning the geodesic convex set. The lower bound is based on the Radon number of the vertices.
2698,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,This paper proposes a new video-to-video transfer learning pipeline for temporal action localization (TAL) methods. The main idea is to combine the video encoder and the TAL head in a joint optimization scheme. The proposed method is evaluated on a large action localization dataset and shows promising results compared to the state-of-the-art methods.
2699,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,This paper proposes a new video-to-video transfer learning pipeline for temporal action localization (TAL) methods. The main idea is to combine the video encoder and the TAL head in a joint optimization scheme. The proposed method is evaluated on a large action localization dataset and shows promising results compared to the state-of-the-art methods.
2700,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,This paper proposes a new video-to-video transfer learning pipeline for temporal action localization (TAL) methods. The main idea is to combine the video encoder and the TAL head in a joint optimization scheme. The proposed method is evaluated on a large action localization dataset and shows promising results compared to the state-of-the-art methods.
2701,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,This paper proposes a new video-to-video transfer learning pipeline for temporal action localization (TAL) methods. The main idea is to combine the video encoder and the TAL head in a joint optimization scheme. The proposed method is evaluated on a large action localization dataset and shows promising results compared to the state-of-the-art methods.
2702,SP:e880db33ba8c305ef1808a02325e2d2b7da95e68,This paper proposes a new video-to-video transfer learning pipeline for temporal action localization (TAL) methods. The main idea is to combine the video encoder and the TAL head in a joint optimization scheme. The proposed method is evaluated on a large action localization dataset and shows promising results compared to the state-of-the-art methods.
2703,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies the problem of regularizing M-estimators with gradient-Lipschitz loss functions. The authors consider the case where the design matrix is Gaussian and the noise distribution is arbitrary. In particular, the authors show that under certain conditions, the gradient of the loss function converges to a lower bound on the out-of-sample error of the estimator. They also provide an adaptive criterion for regularizing the M estimator, which is based on the Huber loss and the Elastic-Net penalty. "
2704,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies the problem of regularizing M-estimators with gradient-Lipschitz loss functions. The authors consider the case where the design matrix is Gaussian and the noise distribution is arbitrary. In particular, the authors show that under certain conditions, the gradient of the loss function converges to a lower bound on the out-of-sample error of the estimator. They also provide an adaptive criterion for regularizing the M estimator, which is based on the Huber loss and the Elastic-Net penalty. "
2705,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies the problem of regularizing M-estimators with gradient-Lipschitz loss functions. The authors consider the case where the design matrix is Gaussian and the noise distribution is arbitrary. In particular, the authors show that under certain conditions, the gradient of the loss function converges to a lower bound on the out-of-sample error of the estimator. They also provide an adaptive criterion for regularizing the M estimator, which is based on the Huber loss and the Elastic-Net penalty. "
2706,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies the problem of regularizing M-estimators with gradient-Lipschitz loss functions. The authors consider the case where the design matrix is Gaussian and the noise distribution is arbitrary. In particular, the authors show that under certain conditions, the gradient of the loss function converges to a lower bound on the out-of-sample error of the estimator. They also provide an adaptive criterion for regularizing the M estimator, which is based on the Huber loss and the Elastic-Net penalty. "
2707,SP:f79e91e469a70b219cd4a2116d5f389842f265ec,"This paper studies the problem of regularizing M-estimators with gradient-Lipschitz loss functions. The authors consider the case where the design matrix is Gaussian and the noise distribution is arbitrary. In particular, the authors show that under certain conditions, the gradient of the loss function converges to a lower bound on the out-of-sample error of the estimator. They also provide an adaptive criterion for regularizing the M estimator, which is based on the Huber loss and the Elastic-Net penalty. "
2708,SP:be53bc4c064402489b644332ad9c17743502d73c,This paper proposes a beam-based algorithm for neural abstractive summarization. The proposed method is based on a calibrated beam search algorithm. The authors propose a global scoring mechanism for beam search and a global protocol for the global protocol. They show that the proposed method outperforms the state-of-the-art summarization models in terms of accuracy and inference speed. They also show that their method is more efficient than the state of the art.
2709,SP:be53bc4c064402489b644332ad9c17743502d73c,This paper proposes a beam-based algorithm for neural abstractive summarization. The proposed method is based on a calibrated beam search algorithm. The authors propose a global scoring mechanism for beam search and a global protocol for the global protocol. They show that the proposed method outperforms the state-of-the-art summarization models in terms of accuracy and inference speed. They also show that their method is more efficient than the state of the art.
2710,SP:be53bc4c064402489b644332ad9c17743502d73c,This paper proposes a beam-based algorithm for neural abstractive summarization. The proposed method is based on a calibrated beam search algorithm. The authors propose a global scoring mechanism for beam search and a global protocol for the global protocol. They show that the proposed method outperforms the state-of-the-art summarization models in terms of accuracy and inference speed. They also show that their method is more efficient than the state of the art.
2711,SP:be53bc4c064402489b644332ad9c17743502d73c,This paper proposes a beam-based algorithm for neural abstractive summarization. The proposed method is based on a calibrated beam search algorithm. The authors propose a global scoring mechanism for beam search and a global protocol for the global protocol. They show that the proposed method outperforms the state-of-the-art summarization models in terms of accuracy and inference speed. They also show that their method is more efficient than the state of the art.
2712,SP:be53bc4c064402489b644332ad9c17743502d73c,This paper proposes a beam-based algorithm for neural abstractive summarization. The proposed method is based on a calibrated beam search algorithm. The authors propose a global scoring mechanism for beam search and a global protocol for the global protocol. They show that the proposed method outperforms the state-of-the-art summarization models in terms of accuracy and inference speed. They also show that their method is more efficient than the state of the art.
2713,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a method for self-attention on manifolds. The key idea is to use a regular field of cyclic groups to represent the feature vectors of the intermediate layers. The authors show that this regular field is invariant to the rotation of the coordinate system. The proposed method, Gauge Equivariant Transformer (GET), is evaluated on a variety of image recognition tasks."
2714,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a method for self-attention on manifolds. The key idea is to use a regular field of cyclic groups to represent the feature vectors of the intermediate layers. The authors show that this regular field is invariant to the rotation of the coordinate system. The proposed method, Gauge Equivariant Transformer (GET), is evaluated on a variety of image recognition tasks."
2715,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a method for self-attention on manifolds. The key idea is to use a regular field of cyclic groups to represent the feature vectors of the intermediate layers. The authors show that this regular field is invariant to the rotation of the coordinate system. The proposed method, Gauge Equivariant Transformer (GET), is evaluated on a variety of image recognition tasks."
2716,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a method for self-attention on manifolds. The key idea is to use a regular field of cyclic groups to represent the feature vectors of the intermediate layers. The authors show that this regular field is invariant to the rotation of the coordinate system. The proposed method, Gauge Equivariant Transformer (GET), is evaluated on a variety of image recognition tasks."
2717,SP:4c7d14ab3304cfbf083815aa6e6d9c0e0a5fba6f,"This paper proposes a method for self-attention on manifolds. The key idea is to use a regular field of cyclic groups to represent the feature vectors of the intermediate layers. The authors show that this regular field is invariant to the rotation of the coordinate system. The proposed method, Gauge Equivariant Transformer (GET), is evaluated on a variety of image recognition tasks."
2718,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to use expectation maximization and Metropolis-Hastings algorithm to optimize the KL divergence between the training distribution and the test distribution of the mixture model. The authors show that their method can be applied to both shallow and deep mixture models, and that it outperforms existing methods on both synthetic and real-data datasets."
2719,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to use expectation maximization and Metropolis-Hastings algorithm to optimize the KL divergence between the training distribution and the test distribution of the mixture model. The authors show that their method can be applied to both shallow and deep mixture models, and that it outperforms existing methods on both synthetic and real-data datasets."
2720,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to use expectation maximization and Metropolis-Hastings algorithm to optimize the KL divergence between the training distribution and the test distribution of the mixture model. The authors show that their method can be applied to both shallow and deep mixture models, and that it outperforms existing methods on both synthetic and real-data datasets."
2721,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to use expectation maximization and Metropolis-Hastings algorithm to optimize the KL divergence between the training distribution and the test distribution of the mixture model. The authors show that their method can be applied to both shallow and deep mixture models, and that it outperforms existing methods on both synthetic and real-data datasets."
2722,SP:19cd64baeb7db11b5ec066e6f8ccb4bc576d3588,"This paper proposes a method for unsupervised learning of finite mixture models. The main idea is to use expectation maximization and Metropolis-Hastings algorithm to optimize the KL divergence between the training distribution and the test distribution of the mixture model. The authors show that their method can be applied to both shallow and deep mixture models, and that it outperforms existing methods on both synthetic and real-data datasets."
2723,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for sparse training of deep neural networks. The main idea is to use a variance reduced policy gradient estimator for the forward pass and a chain rule for the backward pass. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2724,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for sparse training of deep neural networks. The main idea is to use a variance reduced policy gradient estimator for the forward pass and a chain rule for the backward pass. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2725,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for sparse training of deep neural networks. The main idea is to use a variance reduced policy gradient estimator for the forward pass and a chain rule for the backward pass. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2726,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for sparse training of deep neural networks. The main idea is to use a variance reduced policy gradient estimator for the forward pass and a chain rule for the backward pass. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2727,SP:aae8847c5e52d14820967ab39770ab4ae16df59c,"This paper proposes a method for sparse training of deep neural networks. The main idea is to use a variance reduced policy gradient estimator for the forward pass and a chain rule for the backward pass. The proposed method is evaluated on MNIST, CIFAR-10, and ImageNet."
2728,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper studies the problem of estimating the mixing time of importance samplers (IS) and Markov chain Monte Carlo (MCMC) in the context of non-equilibrium orbits. In particular, the authors consider the case where the target distribution is a complex distribution, and the authors propose a novel algorithm called NEO-IS, which uses an iterated sampling-importance resampling mechanism to estimate the Hessian of the proposed distribution. The authors show that the proposed algorithm can be used to estimate mixing time estimates for multimodal targets. "
2729,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper studies the problem of estimating the mixing time of importance samplers (IS) and Markov chain Monte Carlo (MCMC) in the context of non-equilibrium orbits. In particular, the authors consider the case where the target distribution is a complex distribution, and the authors propose a novel algorithm called NEO-IS, which uses an iterated sampling-importance resampling mechanism to estimate the Hessian of the proposed distribution. The authors show that the proposed algorithm can be used to estimate mixing time estimates for multimodal targets. "
2730,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper studies the problem of estimating the mixing time of importance samplers (IS) and Markov chain Monte Carlo (MCMC) in the context of non-equilibrium orbits. In particular, the authors consider the case where the target distribution is a complex distribution, and the authors propose a novel algorithm called NEO-IS, which uses an iterated sampling-importance resampling mechanism to estimate the Hessian of the proposed distribution. The authors show that the proposed algorithm can be used to estimate mixing time estimates for multimodal targets. "
2731,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper studies the problem of estimating the mixing time of importance samplers (IS) and Markov chain Monte Carlo (MCMC) in the context of non-equilibrium orbits. In particular, the authors consider the case where the target distribution is a complex distribution, and the authors propose a novel algorithm called NEO-IS, which uses an iterated sampling-importance resampling mechanism to estimate the Hessian of the proposed distribution. The authors show that the proposed algorithm can be used to estimate mixing time estimates for multimodal targets. "
2732,SP:e0aa68ab03a3ef396b0dc4be4190b328d72cfab0,"This paper studies the problem of estimating the mixing time of importance samplers (IS) and Markov chain Monte Carlo (MCMC) in the context of non-equilibrium orbits. In particular, the authors consider the case where the target distribution is a complex distribution, and the authors propose a novel algorithm called NEO-IS, which uses an iterated sampling-importance resampling mechanism to estimate the Hessian of the proposed distribution. The authors show that the proposed algorithm can be used to estimate mixing time estimates for multimodal targets. "
2733,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a method for mini-batch set encoding. The proposed method is based on the idea of Mini-batch Consistency (MBC), which is a property of mini-batches. The authors show that the proposed method can achieve better performance than the state-of-the-art methods on a variety of tasks."
2734,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a method for mini-batch set encoding. The proposed method is based on the idea of Mini-batch Consistency (MBC), which is a property of mini-batches. The authors show that the proposed method can achieve better performance than the state-of-the-art methods on a variety of tasks."
2735,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a method for mini-batch set encoding. The proposed method is based on the idea of Mini-batch Consistency (MBC), which is a property of mini-batches. The authors show that the proposed method can achieve better performance than the state-of-the-art methods on a variety of tasks."
2736,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a method for mini-batch set encoding. The proposed method is based on the idea of Mini-batch Consistency (MBC), which is a property of mini-batches. The authors show that the proposed method can achieve better performance than the state-of-the-art methods on a variety of tasks."
2737,SP:506dca4f64f837e32958c3c43a0c68f194a36bb3,"This paper proposes a method for mini-batch set encoding. The proposed method is based on the idea of Mini-batch Consistency (MBC), which is a property of mini-batches. The authors show that the proposed method can achieve better performance than the state-of-the-art methods on a variety of tasks."
2738,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper studies the problem of self-play in Diplomacy, which is a combinatorial game with multiple equilibria. The authors propose a method to learn a policy proposal network that can be used to train an agent to play Diplomacy. The proposed method is based on the idea of a double oracle step, where the agent is trained to maximize the value of the policy proposed by the proposed policy. The main contribution of this paper is to show that the proposed method outperforms human-data bootstrapped agents on a two-player variant of Diplomacy (DORA) and achieves state-of-the-art performance on Dota 2 and StarCraft."
2739,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper studies the problem of self-play in Diplomacy, which is a combinatorial game with multiple equilibria. The authors propose a method to learn a policy proposal network that can be used to train an agent to play Diplomacy. The proposed method is based on the idea of a double oracle step, where the agent is trained to maximize the value of the policy proposed by the proposed policy. The main contribution of this paper is to show that the proposed method outperforms human-data bootstrapped agents on a two-player variant of Diplomacy (DORA) and achieves state-of-the-art performance on Dota 2 and StarCraft."
2740,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper studies the problem of self-play in Diplomacy, which is a combinatorial game with multiple equilibria. The authors propose a method to learn a policy proposal network that can be used to train an agent to play Diplomacy. The proposed method is based on the idea of a double oracle step, where the agent is trained to maximize the value of the policy proposed by the proposed policy. The main contribution of this paper is to show that the proposed method outperforms human-data bootstrapped agents on a two-player variant of Diplomacy (DORA) and achieves state-of-the-art performance on Dota 2 and StarCraft."
2741,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper studies the problem of self-play in Diplomacy, which is a combinatorial game with multiple equilibria. The authors propose a method to learn a policy proposal network that can be used to train an agent to play Diplomacy. The proposed method is based on the idea of a double oracle step, where the agent is trained to maximize the value of the policy proposed by the proposed policy. The main contribution of this paper is to show that the proposed method outperforms human-data bootstrapped agents on a two-player variant of Diplomacy (DORA) and achieves state-of-the-art performance on Dota 2 and StarCraft."
2742,SP:b2eafdb24fa081ae8b37525d70fb4bc2d54518dc,"This paper studies the problem of self-play in Diplomacy, which is a combinatorial game with multiple equilibria. The authors propose a method to learn a policy proposal network that can be used to train an agent to play Diplomacy. The proposed method is based on the idea of a double oracle step, where the agent is trained to maximize the value of the policy proposed by the proposed policy. The main contribution of this paper is to show that the proposed method outperforms human-data bootstrapped agents on a two-player variant of Diplomacy (DORA) and achieves state-of-the-art performance on Dota 2 and StarCraft."
2743,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,This paper proposes a multi-head attention method for multi-domain sequence modeling. The proposed method is based on the idea that the attention heads should be shared across different domains. The authors show that the proposed method can improve the performance of the model on BLEU and speech recognition tasks.
2744,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,This paper proposes a multi-head attention method for multi-domain sequence modeling. The proposed method is based on the idea that the attention heads should be shared across different domains. The authors show that the proposed method can improve the performance of the model on BLEU and speech recognition tasks.
2745,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,This paper proposes a multi-head attention method for multi-domain sequence modeling. The proposed method is based on the idea that the attention heads should be shared across different domains. The authors show that the proposed method can improve the performance of the model on BLEU and speech recognition tasks.
2746,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,This paper proposes a multi-head attention method for multi-domain sequence modeling. The proposed method is based on the idea that the attention heads should be shared across different domains. The authors show that the proposed method can improve the performance of the model on BLEU and speech recognition tasks.
2747,SP:1ce1cef9988a07ccd2175a718b29ad23bc779429,This paper proposes a multi-head attention method for multi-domain sequence modeling. The proposed method is based on the idea that the attention heads should be shared across different domains. The authors show that the proposed method can improve the performance of the model on BLEU and speech recognition tasks.
2748,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift on the robustness of random feature regression under distribution shift. The authors consider the case where the label distribution of the model is conditional on the covariate of the data distribution. They show that under some conditions, the covariance shift can lead to a lower limiting test error and bias. They also show that the bias can be controlled by the number of samples in the training set. They provide a theoretical analysis of this phenomenon."
2749,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift on the robustness of random feature regression under distribution shift. The authors consider the case where the label distribution of the model is conditional on the covariate of the data distribution. They show that under some conditions, the covariance shift can lead to a lower limiting test error and bias. They also show that the bias can be controlled by the number of samples in the training set. They provide a theoretical analysis of this phenomenon."
2750,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift on the robustness of random feature regression under distribution shift. The authors consider the case where the label distribution of the model is conditional on the covariate of the data distribution. They show that under some conditions, the covariance shift can lead to a lower limiting test error and bias. They also show that the bias can be controlled by the number of samples in the training set. They provide a theoretical analysis of this phenomenon."
2751,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift on the robustness of random feature regression under distribution shift. The authors consider the case where the label distribution of the model is conditional on the covariate of the data distribution. They show that under some conditions, the covariance shift can lead to a lower limiting test error and bias. They also show that the bias can be controlled by the number of samples in the training set. They provide a theoretical analysis of this phenomenon."
2752,SP:69c522cea4a150624bc709e1c12c0f65183c1b2a,"This paper studies the effect of covariate shift on the robustness of random feature regression under distribution shift. The authors consider the case where the label distribution of the model is conditional on the covariate of the data distribution. They show that under some conditions, the covariance shift can lead to a lower limiting test error and bias. They also show that the bias can be controlled by the number of samples in the training set. They provide a theoretical analysis of this phenomenon."
2753,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the problem of meta-learning in the context of Thompson sampling (TS) and contextual bandits (contextual bandits). In particular, the authors consider the case where the prior is misspecified. The authors provide a PAC bound on the total variation distance between the learning horizon and the target domain, and show that this distance is bounded by a universal constant. They also provide a generalization bound for Bayesian POMDPs. Finally, they propose a knowledge gradient algorithm (KG) for contextual bandits."
2754,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the problem of meta-learning in the context of Thompson sampling (TS) and contextual bandits (contextual bandits). In particular, the authors consider the case where the prior is misspecified. The authors provide a PAC bound on the total variation distance between the learning horizon and the target domain, and show that this distance is bounded by a universal constant. They also provide a generalization bound for Bayesian POMDPs. Finally, they propose a knowledge gradient algorithm (KG) for contextual bandits."
2755,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the problem of meta-learning in the context of Thompson sampling (TS) and contextual bandits (contextual bandits). In particular, the authors consider the case where the prior is misspecified. The authors provide a PAC bound on the total variation distance between the learning horizon and the target domain, and show that this distance is bounded by a universal constant. They also provide a generalization bound for Bayesian POMDPs. Finally, they propose a knowledge gradient algorithm (KG) for contextual bandits."
2756,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the problem of meta-learning in the context of Thompson sampling (TS) and contextual bandits (contextual bandits). In particular, the authors consider the case where the prior is misspecified. The authors provide a PAC bound on the total variation distance between the learning horizon and the target domain, and show that this distance is bounded by a universal constant. They also provide a generalization bound for Bayesian POMDPs. Finally, they propose a knowledge gradient algorithm (KG) for contextual bandits."
2757,SP:ff1b7a7a6295e8f40f3b5df5f6950ca9d33603e0,"This paper studies the problem of meta-learning in the context of Thompson sampling (TS) and contextual bandits (contextual bandits). In particular, the authors consider the case where the prior is misspecified. The authors provide a PAC bound on the total variation distance between the learning horizon and the target domain, and show that this distance is bounded by a universal constant. They also provide a generalization bound for Bayesian POMDPs. Finally, they propose a knowledge gradient algorithm (KG) for contextual bandits."
2758,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample complexity of PAC-learning and equivalence-query-learning models. The authors show that the sample/query complexity of both models is exponential in the number of adversarial examples. They also show that adversarial training with on-manifold examples improves the robustness of the model.
2759,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample complexity of PAC-learning and equivalence-query-learning models. The authors show that the sample/query complexity of both models is exponential in the number of adversarial examples. They also show that adversarial training with on-manifold examples improves the robustness of the model.
2760,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample complexity of PAC-learning and equivalence-query-learning models. The authors show that the sample/query complexity of both models is exponential in the number of adversarial examples. They also show that adversarial training with on-manifold examples improves the robustness of the model.
2761,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample complexity of PAC-learning and equivalence-query-learning models. The authors show that the sample/query complexity of both models is exponential in the number of adversarial examples. They also show that adversarial training with on-manifold examples improves the robustness of the model.
2762,SP:3477b64480ed638b1c4e1f8aa73fc2e77666c89a,This paper studies the sample complexity of PAC-learning and equivalence-query-learning models. The authors show that the sample/query complexity of both models is exponential in the number of adversarial examples. They also show that adversarial training with on-manifold examples improves the robustness of the model.
2763,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of model selection for transfer learning. The authors propose a scalable model selection algorithm, called PARC, which is based on the idea that the number of models in a large model bank can be reduced to a small number of small model banks. The main contribution of this paper is to propose a method for model selection that is scalable and efficient. The paper is well-written and easy to follow."
2764,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of model selection for transfer learning. The authors propose a scalable model selection algorithm, called PARC, which is based on the idea that the number of models in a large model bank can be reduced to a small number of small model banks. The main contribution of this paper is to propose a method for model selection that is scalable and efficient. The paper is well-written and easy to follow."
2765,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of model selection for transfer learning. The authors propose a scalable model selection algorithm, called PARC, which is based on the idea that the number of models in a large model bank can be reduced to a small number of small model banks. The main contribution of this paper is to propose a method for model selection that is scalable and efficient. The paper is well-written and easy to follow."
2766,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of model selection for transfer learning. The authors propose a scalable model selection algorithm, called PARC, which is based on the idea that the number of models in a large model bank can be reduced to a small number of small model banks. The main contribution of this paper is to propose a method for model selection that is scalable and efficient. The paper is well-written and easy to follow."
2767,SP:7520cc1203bb06bbe432e7cc679892e95258ed99,"This paper studies the problem of model selection for transfer learning. The authors propose a scalable model selection algorithm, called PARC, which is based on the idea that the number of models in a large model bank can be reduced to a small number of small model banks. The main contribution of this paper is to propose a method for model selection that is scalable and efficient. The paper is well-written and easy to follow."
2768,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for low-dimensional binary codes (LLC) for out-of-distribution (OOD) detection. The proposed method is based on HashNet, which is an existing method for encoding binary codes for image retrieval. The main idea is to use HashNet to encode the binary codes into a lower dimensional representation, which can then be used for OOD detection. In particular, the authors propose to use the lower-dimensional representation as a threshold for the binary code. The authors show that the proposed method outperforms HashNet on the ImageNet-100 retrieval problem."
2769,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for low-dimensional binary codes (LLC) for out-of-distribution (OOD) detection. The proposed method is based on HashNet, which is an existing method for encoding binary codes for image retrieval. The main idea is to use HashNet to encode the binary codes into a lower dimensional representation, which can then be used for OOD detection. In particular, the authors propose to use the lower-dimensional representation as a threshold for the binary code. The authors show that the proposed method outperforms HashNet on the ImageNet-100 retrieval problem."
2770,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for low-dimensional binary codes (LLC) for out-of-distribution (OOD) detection. The proposed method is based on HashNet, which is an existing method for encoding binary codes for image retrieval. The main idea is to use HashNet to encode the binary codes into a lower dimensional representation, which can then be used for OOD detection. In particular, the authors propose to use the lower-dimensional representation as a threshold for the binary code. The authors show that the proposed method outperforms HashNet on the ImageNet-100 retrieval problem."
2771,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for low-dimensional binary codes (LLC) for out-of-distribution (OOD) detection. The proposed method is based on HashNet, which is an existing method for encoding binary codes for image retrieval. The main idea is to use HashNet to encode the binary codes into a lower dimensional representation, which can then be used for OOD detection. In particular, the authors propose to use the lower-dimensional representation as a threshold for the binary code. The authors show that the proposed method outperforms HashNet on the ImageNet-100 retrieval problem."
2772,SP:dcdb9c88f61ac3caf3da8255a7953c753cf048d1,"This paper proposes a method for low-dimensional binary codes (LLC) for out-of-distribution (OOD) detection. The proposed method is based on HashNet, which is an existing method for encoding binary codes for image retrieval. The main idea is to use HashNet to encode the binary codes into a lower dimensional representation, which can then be used for OOD detection. In particular, the authors propose to use the lower-dimensional representation as a threshold for the binary code. The authors show that the proposed method outperforms HashNet on the ImageNet-100 retrieval problem."
2773,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a generalized depthwise-separarable (GDWS) convolutional network that can be used as a pre-trained network for robust model compression. The proposed method is based on a 2D convolution approximator, and the authors show that GDWS can be applied to any 2D network. The authors also provide a theoretical analysis of the performance of the proposed method on CIFAR-10 and ImageNet datasets."
2774,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a generalized depthwise-separarable (GDWS) convolutional network that can be used as a pre-trained network for robust model compression. The proposed method is based on a 2D convolution approximator, and the authors show that GDWS can be applied to any 2D network. The authors also provide a theoretical analysis of the performance of the proposed method on CIFAR-10 and ImageNet datasets."
2775,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a generalized depthwise-separarable (GDWS) convolutional network that can be used as a pre-trained network for robust model compression. The proposed method is based on a 2D convolution approximator, and the authors show that GDWS can be applied to any 2D network. The authors also provide a theoretical analysis of the performance of the proposed method on CIFAR-10 and ImageNet datasets."
2776,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a generalized depthwise-separarable (GDWS) convolutional network that can be used as a pre-trained network for robust model compression. The proposed method is based on a 2D convolution approximator, and the authors show that GDWS can be applied to any 2D network. The authors also provide a theoretical analysis of the performance of the proposed method on CIFAR-10 and ImageNet datasets."
2777,SP:07def8c80d05f86402ce769313480b30cd99af43,"This paper proposes a generalized depthwise-separarable (GDWS) convolutional network that can be used as a pre-trained network for robust model compression. The proposed method is based on a 2D convolution approximator, and the authors show that GDWS can be applied to any 2D network. The authors also provide a theoretical analysis of the performance of the proposed method on CIFAR-10 and ImageNet datasets."
2778,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a graph-based approach for retrosynthesis prediction. The proposed method is based on a graph neural network (GNN) architecture. The main idea is to train a GNN to predict the graph topology of a molecule, and then use the predicted graph as input to a second GNN that predicts the reactivity of the target molecule. The authors show that the proposed method can achieve state-of-the-art results on a synthetic dataset."
2779,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a graph-based approach for retrosynthesis prediction. The proposed method is based on a graph neural network (GNN) architecture. The main idea is to train a GNN to predict the graph topology of a molecule, and then use the predicted graph as input to a second GNN that predicts the reactivity of the target molecule. The authors show that the proposed method can achieve state-of-the-art results on a synthetic dataset."
2780,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a graph-based approach for retrosynthesis prediction. The proposed method is based on a graph neural network (GNN) architecture. The main idea is to train a GNN to predict the graph topology of a molecule, and then use the predicted graph as input to a second GNN that predicts the reactivity of the target molecule. The authors show that the proposed method can achieve state-of-the-art results on a synthetic dataset."
2781,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a graph-based approach for retrosynthesis prediction. The proposed method is based on a graph neural network (GNN) architecture. The main idea is to train a GNN to predict the graph topology of a molecule, and then use the predicted graph as input to a second GNN that predicts the reactivity of the target molecule. The authors show that the proposed method can achieve state-of-the-art results on a synthetic dataset."
2782,SP:9e4d04b22ce4f986aabb747a42f40c827073e39e,"This paper proposes a graph-based approach for retrosynthesis prediction. The proposed method is based on a graph neural network (GNN) architecture. The main idea is to train a GNN to predict the graph topology of a molecule, and then use the predicted graph as input to a second GNN that predicts the reactivity of the target molecule. The authors show that the proposed method can achieve state-of-the-art results on a synthetic dataset."
2783,SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a Bayesian formulation of deconditioning for the reproducing kernel Hilbert space formulation of the downscaling problem. The main idea is to use a conditional mean embedding estimator for multiresolution data. The authors show that this estimator can recover the underlying fine-grained field of the latent field, which can then be used as a decoder for the decoder. They also provide a minimax optimal convergence rate for the proposed estimator."
2784,SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a Bayesian formulation of deconditioning for the reproducing kernel Hilbert space formulation of the downscaling problem. The main idea is to use a conditional mean embedding estimator for multiresolution data. The authors show that this estimator can recover the underlying fine-grained field of the latent field, which can then be used as a decoder for the decoder. They also provide a minimax optimal convergence rate for the proposed estimator."
2785,SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a Bayesian formulation of deconditioning for the reproducing kernel Hilbert space formulation of the downscaling problem. The main idea is to use a conditional mean embedding estimator for multiresolution data. The authors show that this estimator can recover the underlying fine-grained field of the latent field, which can then be used as a decoder for the decoder. They also provide a minimax optimal convergence rate for the proposed estimator."
2786,SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a Bayesian formulation of deconditioning for the reproducing kernel Hilbert space formulation of the downscaling problem. The main idea is to use a conditional mean embedding estimator for multiresolution data. The authors show that this estimator can recover the underlying fine-grained field of the latent field, which can then be used as a decoder for the decoder. They also provide a minimax optimal convergence rate for the proposed estimator."
2787,SP:772277d969c95924755113c86663fb0e009f24cc,"This paper proposes a Bayesian formulation of deconditioning for the reproducing kernel Hilbert space formulation of the downscaling problem. The main idea is to use a conditional mean embedding estimator for multiresolution data. The authors show that this estimator can recover the underlying fine-grained field of the latent field, which can then be used as a decoder for the decoder. They also provide a minimax optimal convergence rate for the proposed estimator."
2788,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method to search for sparse networks with high-order feature interactions in the feature-interaction layer of deep sparse networks (DSNs). The main idea is to use a neural architecture search algorithm to find the best network architecture for a given sparse prediction task. The proposed method is based on a progressive search algorithm, where the search space is distilled into a distilled search space and the search algorithm is used to select the optimal network architecture. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and computation efficiency."
2789,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method to search for sparse networks with high-order feature interactions in the feature-interaction layer of deep sparse networks (DSNs). The main idea is to use a neural architecture search algorithm to find the best network architecture for a given sparse prediction task. The proposed method is based on a progressive search algorithm, where the search space is distilled into a distilled search space and the search algorithm is used to select the optimal network architecture. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and computation efficiency."
2790,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method to search for sparse networks with high-order feature interactions in the feature-interaction layer of deep sparse networks (DSNs). The main idea is to use a neural architecture search algorithm to find the best network architecture for a given sparse prediction task. The proposed method is based on a progressive search algorithm, where the search space is distilled into a distilled search space and the search algorithm is used to select the optimal network architecture. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and computation efficiency."
2791,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method to search for sparse networks with high-order feature interactions in the feature-interaction layer of deep sparse networks (DSNs). The main idea is to use a neural architecture search algorithm to find the best network architecture for a given sparse prediction task. The proposed method is based on a progressive search algorithm, where the search space is distilled into a distilled search space and the search algorithm is used to select the optimal network architecture. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and computation efficiency."
2792,SP:59cfeb59cecac51fecff8f8ceb0266fc6ac22a05,"This paper proposes a method to search for sparse networks with high-order feature interactions in the feature-interaction layer of deep sparse networks (DSNs). The main idea is to use a neural architecture search algorithm to find the best network architecture for a given sparse prediction task. The proposed method is based on a progressive search algorithm, where the search space is distilled into a distilled search space and the search algorithm is used to select the optimal network architecture. The authors show that the proposed method outperforms the state-of-the-art in terms of accuracy and computation efficiency."
2793,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model for transfer learning with noisy labels. In particular, the authors propose a regularized self-labeling method that interpolates between the interpolation between label-reweighting and label-correcting methods, and layer-wise regularization. The authors show that the proposed method can improve the generalization properties of the model under the noise stability condition. They also provide a PAC-Bayes generalization bound for the proposed regularization method. The proposed method is evaluated on image classification tasks and few-shot classification tasks."
2794,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model for transfer learning with noisy labels. In particular, the authors propose a regularized self-labeling method that interpolates between the interpolation between label-reweighting and label-correcting methods, and layer-wise regularization. The authors show that the proposed method can improve the generalization properties of the model under the noise stability condition. They also provide a PAC-Bayes generalization bound for the proposed regularization method. The proposed method is evaluated on image classification tasks and few-shot classification tasks."
2795,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model for transfer learning with noisy labels. In particular, the authors propose a regularized self-labeling method that interpolates between the interpolation between label-reweighting and label-correcting methods, and layer-wise regularization. The authors show that the proposed method can improve the generalization properties of the model under the noise stability condition. They also provide a PAC-Bayes generalization bound for the proposed regularization method. The proposed method is evaluated on image classification tasks and few-shot classification tasks."
2796,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model for transfer learning with noisy labels. In particular, the authors propose a regularized self-labeling method that interpolates between the interpolation between label-reweighting and label-correcting methods, and layer-wise regularization. The authors show that the proposed method can improve the generalization properties of the model under the noise stability condition. They also provide a PAC-Bayes generalization bound for the proposed regularization method. The proposed method is evaluated on image classification tasks and few-shot classification tasks."
2797,SP:23a2171eab71c4fd3754791ca2aac9be87411cdb,"This paper studies the problem of fine-tuning a pre-trained model for transfer learning with noisy labels. In particular, the authors propose a regularized self-labeling method that interpolates between the interpolation between label-reweighting and label-correcting methods, and layer-wise regularization. The authors show that the proposed method can improve the generalization properties of the model under the noise stability condition. They also provide a PAC-Bayes generalization bound for the proposed regularization method. The proposed method is evaluated on image classification tasks and few-shot classification tasks."
2798,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper studies the problem of estimating the value-at-risk (VaR) of a probability distribution under heavy-tailed distributions. The authors consider the case where the probability distribution is a weighted sum of CVaR and mean, where the mean is the sum of VaR and the mean of the VaR. In this case, the authors consider a non-convex optimization problem where the arms of the distribution are assumed to be convex, and the goal is to find an optimal algorithm that maximizes the risk-return trade-off between the two arms. "
2799,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper studies the problem of estimating the value-at-risk (VaR) of a probability distribution under heavy-tailed distributions. The authors consider the case where the probability distribution is a weighted sum of CVaR and mean, where the mean is the sum of VaR and the mean of the VaR. In this case, the authors consider a non-convex optimization problem where the arms of the distribution are assumed to be convex, and the goal is to find an optimal algorithm that maximizes the risk-return trade-off between the two arms. "
2800,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper studies the problem of estimating the value-at-risk (VaR) of a probability distribution under heavy-tailed distributions. The authors consider the case where the probability distribution is a weighted sum of CVaR and mean, where the mean is the sum of VaR and the mean of the VaR. In this case, the authors consider a non-convex optimization problem where the arms of the distribution are assumed to be convex, and the goal is to find an optimal algorithm that maximizes the risk-return trade-off between the two arms. "
2801,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper studies the problem of estimating the value-at-risk (VaR) of a probability distribution under heavy-tailed distributions. The authors consider the case where the probability distribution is a weighted sum of CVaR and mean, where the mean is the sum of VaR and the mean of the VaR. In this case, the authors consider a non-convex optimization problem where the arms of the distribution are assumed to be convex, and the goal is to find an optimal algorithm that maximizes the risk-return trade-off between the two arms. "
2802,SP:f2a77f93bdc0401bbd6162a16fba25b9f90530e2,"This paper studies the problem of estimating the value-at-risk (VaR) of a probability distribution under heavy-tailed distributions. The authors consider the case where the probability distribution is a weighted sum of CVaR and mean, where the mean is the sum of VaR and the mean of the VaR. In this case, the authors consider a non-convex optimization problem where the arms of the distribution are assumed to be convex, and the goal is to find an optimal algorithm that maximizes the risk-return trade-off between the two arms. "
2803,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, an intrinsic inductive bias (IB) based self-attention module for vision transformers. The proposed method is based on a convolutional layer and a spatial pyramid reduction module. The authors claim that the proposed method improves the intrinsic locality of the intrinsic intrinsic bias (IIB) of the transformers, and that it is more robust to scale variance. Experiments on ImageNet and CIFAR-10 demonstrate the effectiveness of the proposed model."
2804,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, an intrinsic inductive bias (IB) based self-attention module for vision transformers. The proposed method is based on a convolutional layer and a spatial pyramid reduction module. The authors claim that the proposed method improves the intrinsic locality of the intrinsic intrinsic bias (IIB) of the transformers, and that it is more robust to scale variance. Experiments on ImageNet and CIFAR-10 demonstrate the effectiveness of the proposed model."
2805,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, an intrinsic inductive bias (IB) based self-attention module for vision transformers. The proposed method is based on a convolutional layer and a spatial pyramid reduction module. The authors claim that the proposed method improves the intrinsic locality of the intrinsic intrinsic bias (IIB) of the transformers, and that it is more robust to scale variance. Experiments on ImageNet and CIFAR-10 demonstrate the effectiveness of the proposed model."
2806,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, an intrinsic inductive bias (IB) based self-attention module for vision transformers. The proposed method is based on a convolutional layer and a spatial pyramid reduction module. The authors claim that the proposed method improves the intrinsic locality of the intrinsic intrinsic bias (IIB) of the transformers, and that it is more robust to scale variance. Experiments on ImageNet and CIFAR-10 demonstrate the effectiveness of the proposed model."
2807,SP:765942c86da1594b33268df6d0d15c682bc7eaa6,"This paper proposes ViTAE, an intrinsic inductive bias (IB) based self-attention module for vision transformers. The proposed method is based on a convolutional layer and a spatial pyramid reduction module. The authors claim that the proposed method improves the intrinsic locality of the intrinsic intrinsic bias (IIB) of the transformers, and that it is more robust to scale variance. Experiments on ImageNet and CIFAR-10 demonstrate the effectiveness of the proposed model."
2808,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-tuning (RIFT), a method for adversarial fine tuning of pre-trained language models. The main idea of RIFT is to use adversarial examples to fine-tune the model. The authors show that the proposed method is robust against word substitution attacks and catastrophic forgetting. Experiments on sentiment analysis and natural language inference show that RIFT outperforms state-of-the-art methods."
2809,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-tuning (RIFT), a method for adversarial fine tuning of pre-trained language models. The main idea of RIFT is to use adversarial examples to fine-tune the model. The authors show that the proposed method is robust against word substitution attacks and catastrophic forgetting. Experiments on sentiment analysis and natural language inference show that RIFT outperforms state-of-the-art methods."
2810,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-tuning (RIFT), a method for adversarial fine tuning of pre-trained language models. The main idea of RIFT is to use adversarial examples to fine-tune the model. The authors show that the proposed method is robust against word substitution attacks and catastrophic forgetting. Experiments on sentiment analysis and natural language inference show that RIFT outperforms state-of-the-art methods."
2811,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-tuning (RIFT), a method for adversarial fine tuning of pre-trained language models. The main idea of RIFT is to use adversarial examples to fine-tune the model. The authors show that the proposed method is robust against word substitution attacks and catastrophic forgetting. Experiments on sentiment analysis and natural language inference show that RIFT outperforms state-of-the-art methods."
2812,SP:5e3572a386f890c5864437985cf63b13844f338f,"This paper proposes Robust Informative Fine-tuning (RIFT), a method for adversarial fine tuning of pre-trained language models. The main idea of RIFT is to use adversarial examples to fine-tune the model. The authors show that the proposed method is robust against word substitution attacks and catastrophic forgetting. Experiments on sentiment analysis and natural language inference show that RIFT outperforms state-of-the-art methods."
2813,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies Anderson mixing (AM) in nonconvex stochastic optimization problems. In particular, the authors study the convergence of Anderson mixing with damped projection and adaptive regularization. The authors propose a preconditioned mixing strategy to improve the convergence rate of SAM. In addition, they propose a variance reduction technique to reduce the variance of the mixing strategy. Experiments on image classification and language modeling show the effectiveness of the proposed method."
2814,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies Anderson mixing (AM) in nonconvex stochastic optimization problems. In particular, the authors study the convergence of Anderson mixing with damped projection and adaptive regularization. The authors propose a preconditioned mixing strategy to improve the convergence rate of SAM. In addition, they propose a variance reduction technique to reduce the variance of the mixing strategy. Experiments on image classification and language modeling show the effectiveness of the proposed method."
2815,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies Anderson mixing (AM) in nonconvex stochastic optimization problems. In particular, the authors study the convergence of Anderson mixing with damped projection and adaptive regularization. The authors propose a preconditioned mixing strategy to improve the convergence rate of SAM. In addition, they propose a variance reduction technique to reduce the variance of the mixing strategy. Experiments on image classification and language modeling show the effectiveness of the proposed method."
2816,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies Anderson mixing (AM) in nonconvex stochastic optimization problems. In particular, the authors study the convergence of Anderson mixing with damped projection and adaptive regularization. The authors propose a preconditioned mixing strategy to improve the convergence rate of SAM. In addition, they propose a variance reduction technique to reduce the variance of the mixing strategy. Experiments on image classification and language modeling show the effectiveness of the proposed method."
2817,SP:167a8b7e0173bffc5f08a9c2f378fe7bdf837da3,"This paper studies Anderson mixing (AM) in nonconvex stochastic optimization problems. In particular, the authors study the convergence of Anderson mixing with damped projection and adaptive regularization. The authors propose a preconditioned mixing strategy to improve the convergence rate of SAM. In addition, they propose a variance reduction technique to reduce the variance of the mixing strategy. Experiments on image classification and language modeling show the effectiveness of the proposed method."
2818,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a stochastic algorithm for solving linear inverse problems with additive white Gaussian noise. The main contribution of this paper is the use of singular value decomposition (SVD) to decompose the posterior score function of the posterior distribution into a degradation operator and an iterative algorithm to solve the linear inverse problem. The proposed algorithm is based on Newton’s method and Langevin dynamics. The authors show that the proposed algorithm can be used for image deblurring, super-resolution and compressive sensing."
2819,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a stochastic algorithm for solving linear inverse problems with additive white Gaussian noise. The main contribution of this paper is the use of singular value decomposition (SVD) to decompose the posterior score function of the posterior distribution into a degradation operator and an iterative algorithm to solve the linear inverse problem. The proposed algorithm is based on Newton’s method and Langevin dynamics. The authors show that the proposed algorithm can be used for image deblurring, super-resolution and compressive sensing."
2820,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a stochastic algorithm for solving linear inverse problems with additive white Gaussian noise. The main contribution of this paper is the use of singular value decomposition (SVD) to decompose the posterior score function of the posterior distribution into a degradation operator and an iterative algorithm to solve the linear inverse problem. The proposed algorithm is based on Newton’s method and Langevin dynamics. The authors show that the proposed algorithm can be used for image deblurring, super-resolution and compressive sensing."
2821,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a stochastic algorithm for solving linear inverse problems with additive white Gaussian noise. The main contribution of this paper is the use of singular value decomposition (SVD) to decompose the posterior score function of the posterior distribution into a degradation operator and an iterative algorithm to solve the linear inverse problem. The proposed algorithm is based on Newton’s method and Langevin dynamics. The authors show that the proposed algorithm can be used for image deblurring, super-resolution and compressive sensing."
2822,SP:fe9c80cc5615705ef844d59b56413779c8d54a06,"This paper proposes a stochastic algorithm for solving linear inverse problems with additive white Gaussian noise. The main contribution of this paper is the use of singular value decomposition (SVD) to decompose the posterior score function of the posterior distribution into a degradation operator and an iterative algorithm to solve the linear inverse problem. The proposed algorithm is based on Newton’s method and Langevin dynamics. The authors show that the proposed algorithm can be used for image deblurring, super-resolution and compressive sensing."
2823,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,This paper proposes a meta-learning method for detecting drug traffickers on Instagram. The proposed method is based on the heterogeneous graph (HG) framework. The authors propose to use a relation-based graph convolutional neural network to learn the node (i.e. user) representations and the sparse connection among entities. The model is trained with a self-supervised module and a knowledge distillation module. Experiments on Instagram show that the proposed method outperforms state-of-the-art methods.
2824,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,This paper proposes a meta-learning method for detecting drug traffickers on Instagram. The proposed method is based on the heterogeneous graph (HG) framework. The authors propose to use a relation-based graph convolutional neural network to learn the node (i.e. user) representations and the sparse connection among entities. The model is trained with a self-supervised module and a knowledge distillation module. Experiments on Instagram show that the proposed method outperforms state-of-the-art methods.
2825,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,This paper proposes a meta-learning method for detecting drug traffickers on Instagram. The proposed method is based on the heterogeneous graph (HG) framework. The authors propose to use a relation-based graph convolutional neural network to learn the node (i.e. user) representations and the sparse connection among entities. The model is trained with a self-supervised module and a knowledge distillation module. Experiments on Instagram show that the proposed method outperforms state-of-the-art methods.
2826,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,This paper proposes a meta-learning method for detecting drug traffickers on Instagram. The proposed method is based on the heterogeneous graph (HG) framework. The authors propose to use a relation-based graph convolutional neural network to learn the node (i.e. user) representations and the sparse connection among entities. The model is trained with a self-supervised module and a knowledge distillation module. Experiments on Instagram show that the proposed method outperforms state-of-the-art methods.
2827,SP:b04caddcb2dc9e9b365a76fdbf3d3eb4efcdffd9,This paper proposes a meta-learning method for detecting drug traffickers on Instagram. The proposed method is based on the heterogeneous graph (HG) framework. The authors propose to use a relation-based graph convolutional neural network to learn the node (i.e. user) representations and the sparse connection among entities. The model is trained with a self-supervised module and a knowledge distillation module. Experiments on Instagram show that the proposed method outperforms state-of-the-art methods.
2828,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the properties of neural networks with ReLU activations. The authors consider the case where the network is trained on a class of functions with a hidden layer, where the hidden layer is a ReLU layer. They show that under certain conditions, the network can be approximated by a linear combination of the ReLU activation and the network parameters. They also show that this can be done by using mixed-integer optimization and polyhedral theory. Finally, they provide upper bounds on the computational complexity of the network."
2829,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the properties of neural networks with ReLU activations. The authors consider the case where the network is trained on a class of functions with a hidden layer, where the hidden layer is a ReLU layer. They show that under certain conditions, the network can be approximated by a linear combination of the ReLU activation and the network parameters. They also show that this can be done by using mixed-integer optimization and polyhedral theory. Finally, they provide upper bounds on the computational complexity of the network."
2830,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the properties of neural networks with ReLU activations. The authors consider the case where the network is trained on a class of functions with a hidden layer, where the hidden layer is a ReLU layer. They show that under certain conditions, the network can be approximated by a linear combination of the ReLU activation and the network parameters. They also show that this can be done by using mixed-integer optimization and polyhedral theory. Finally, they provide upper bounds on the computational complexity of the network."
2831,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the properties of neural networks with ReLU activations. The authors consider the case where the network is trained on a class of functions with a hidden layer, where the hidden layer is a ReLU layer. They show that under certain conditions, the network can be approximated by a linear combination of the ReLU activation and the network parameters. They also show that this can be done by using mixed-integer optimization and polyhedral theory. Finally, they provide upper bounds on the computational complexity of the network."
2832,SP:242da1384f48260d58a0e7949438611c05079197,"This paper studies the properties of neural networks with ReLU activations. The authors consider the case where the network is trained on a class of functions with a hidden layer, where the hidden layer is a ReLU layer. They show that under certain conditions, the network can be approximated by a linear combination of the ReLU activation and the network parameters. They also show that this can be done by using mixed-integer optimization and polyhedral theory. Finally, they provide upper bounds on the computational complexity of the network."
2833,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper studies adversarial training (AT) from the perspective of minimizing the worst-case adversarial loss. In particular, the authors propose a framework for min-max optimization of adversarial adversarial examples. The main contribution of this paper is the introduction of self-adjusting domain weights, which can be used to reduce the difficulty level of attack. The authors also propose a unified framework for attacking model ensembles and crafting attacks. The experimental results show that the proposed method outperforms existing heuristic strategies."
2834,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper studies adversarial training (AT) from the perspective of minimizing the worst-case adversarial loss. In particular, the authors propose a framework for min-max optimization of adversarial adversarial examples. The main contribution of this paper is the introduction of self-adjusting domain weights, which can be used to reduce the difficulty level of attack. The authors also propose a unified framework for attacking model ensembles and crafting attacks. The experimental results show that the proposed method outperforms existing heuristic strategies."
2835,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper studies adversarial training (AT) from the perspective of minimizing the worst-case adversarial loss. In particular, the authors propose a framework for min-max optimization of adversarial adversarial examples. The main contribution of this paper is the introduction of self-adjusting domain weights, which can be used to reduce the difficulty level of attack. The authors also propose a unified framework for attacking model ensembles and crafting attacks. The experimental results show that the proposed method outperforms existing heuristic strategies."
2836,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper studies adversarial training (AT) from the perspective of minimizing the worst-case adversarial loss. In particular, the authors propose a framework for min-max optimization of adversarial adversarial examples. The main contribution of this paper is the introduction of self-adjusting domain weights, which can be used to reduce the difficulty level of attack. The authors also propose a unified framework for attacking model ensembles and crafting attacks. The experimental results show that the proposed method outperforms existing heuristic strategies."
2837,SP:8d5741aedf3125e0e790a58ec3ce81a4e2ea4dcb,"This paper studies adversarial training (AT) from the perspective of minimizing the worst-case adversarial loss. In particular, the authors propose a framework for min-max optimization of adversarial adversarial examples. The main contribution of this paper is the introduction of self-adjusting domain weights, which can be used to reduce the difficulty level of attack. The authors also propose a unified framework for attacking model ensembles and crafting attacks. The experimental results show that the proposed method outperforms existing heuristic strategies."
2838,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of computing sparse tensor principal component analysis (SPCA) in the Wigner form. The authors propose two algorithms for computing sparse PCA. The first algorithm is a polynomial-time algorithm, and the second algorithm is an exponential-time search algorithm. Both algorithms are based on the idea of computing the signal-to-noise ratio of a sparse unit vector, which is a lower bound on the signal to noise ratio of the sparse vector. The lower bound is also extended to the tensor PCA problem, and is shown to be tight."
2839,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of computing sparse tensor principal component analysis (SPCA) in the Wigner form. The authors propose two algorithms for computing sparse PCA. The first algorithm is a polynomial-time algorithm, and the second algorithm is an exponential-time search algorithm. Both algorithms are based on the idea of computing the signal-to-noise ratio of a sparse unit vector, which is a lower bound on the signal to noise ratio of the sparse vector. The lower bound is also extended to the tensor PCA problem, and is shown to be tight."
2840,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of computing sparse tensor principal component analysis (SPCA) in the Wigner form. The authors propose two algorithms for computing sparse PCA. The first algorithm is a polynomial-time algorithm, and the second algorithm is an exponential-time search algorithm. Both algorithms are based on the idea of computing the signal-to-noise ratio of a sparse unit vector, which is a lower bound on the signal to noise ratio of the sparse vector. The lower bound is also extended to the tensor PCA problem, and is shown to be tight."
2841,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of computing sparse tensor principal component analysis (SPCA) in the Wigner form. The authors propose two algorithms for computing sparse PCA. The first algorithm is a polynomial-time algorithm, and the second algorithm is an exponential-time search algorithm. Both algorithms are based on the idea of computing the signal-to-noise ratio of a sparse unit vector, which is a lower bound on the signal to noise ratio of the sparse vector. The lower bound is also extended to the tensor PCA problem, and is shown to be tight."
2842,SP:9dcb74bfdbc4aa1e27f5d2adb6d2abf475e9324d,"This paper studies the problem of computing sparse tensor principal component analysis (SPCA) in the Wigner form. The authors propose two algorithms for computing sparse PCA. The first algorithm is a polynomial-time algorithm, and the second algorithm is an exponential-time search algorithm. Both algorithms are based on the idea of computing the signal-to-noise ratio of a sparse unit vector, which is a lower bound on the signal to noise ratio of the sparse vector. The lower bound is also extended to the tensor PCA problem, and is shown to be tight."
2843,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for multilayer-perceptron (MLP) networks. The proposed SAPE is based on a feedback loop, where each layer of the MLP network is used to learn a frequency-dependent representation of a 3D shape. The authors show that SAPE can be applied to a wide range of applications, including representation learning of occupancy networks, regression of low dimensional signals, and geometric task of mesh transfer. "
2844,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for multilayer-perceptron (MLP) networks. The proposed SAPE is based on a feedback loop, where each layer of the MLP network is used to learn a frequency-dependent representation of a 3D shape. The authors show that SAPE can be applied to a wide range of applications, including representation learning of occupancy networks, regression of low dimensional signals, and geometric task of mesh transfer. "
2845,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for multilayer-perceptron (MLP) networks. The proposed SAPE is based on a feedback loop, where each layer of the MLP network is used to learn a frequency-dependent representation of a 3D shape. The authors show that SAPE can be applied to a wide range of applications, including representation learning of occupancy networks, regression of low dimensional signals, and geometric task of mesh transfer. "
2846,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for multilayer-perceptron (MLP) networks. The proposed SAPE is based on a feedback loop, where each layer of the MLP network is used to learn a frequency-dependent representation of a 3D shape. The authors show that SAPE can be applied to a wide range of applications, including representation learning of occupancy networks, regression of low dimensional signals, and geometric task of mesh transfer. "
2847,SP:660137b0f84e47c06dc2bee1c95b299c67e4cb67,"This paper proposes a spatially adaptive progressive encoding (SAPE) scheme for multilayer-perceptron (MLP) networks. The proposed SAPE is based on a feedback loop, where each layer of the MLP network is used to learn a frequency-dependent representation of a 3D shape. The authors show that SAPE can be applied to a wide range of applications, including representation learning of occupancy networks, regression of low dimensional signals, and geometric task of mesh transfer. "
2848,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies policy extragradient algorithms for zero-sum two-player matrix games with entropy regularization. The authors show that the rate of convergence of these algorithms is linear in the number of state and action spaces, and sublinear in the logarithm factor of the action space. They also show that when the policy is symmetric and multiplicative, the rates of convergence converge to a Nash equilibrium."
2849,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies policy extragradient algorithms for zero-sum two-player matrix games with entropy regularization. The authors show that the rate of convergence of these algorithms is linear in the number of state and action spaces, and sublinear in the logarithm factor of the action space. They also show that when the policy is symmetric and multiplicative, the rates of convergence converge to a Nash equilibrium."
2850,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies policy extragradient algorithms for zero-sum two-player matrix games with entropy regularization. The authors show that the rate of convergence of these algorithms is linear in the number of state and action spaces, and sublinear in the logarithm factor of the action space. They also show that when the policy is symmetric and multiplicative, the rates of convergence converge to a Nash equilibrium."
2851,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies policy extragradient algorithms for zero-sum two-player matrix games with entropy regularization. The authors show that the rate of convergence of these algorithms is linear in the number of state and action spaces, and sublinear in the logarithm factor of the action space. They also show that when the policy is symmetric and multiplicative, the rates of convergence converge to a Nash equilibrium."
2852,SP:b03063fa82d76db341076e5f282176f4c007a202,"This paper studies policy extragradient algorithms for zero-sum two-player matrix games with entropy regularization. The authors show that the rate of convergence of these algorithms is linear in the number of state and action spaces, and sublinear in the logarithm factor of the action space. They also show that when the policy is symmetric and multiplicative, the rates of convergence converge to a Nash equilibrium."
2853,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,This paper proposes a method for super-resolution (SR) browsing on high resolution (HR) screens. The main idea is to use an implicit transformer to encode the pixel values of the image features. The proposed method is evaluated on both compressed and uncompressed SCI1K-compression datasets.
2854,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,This paper proposes a method for super-resolution (SR) browsing on high resolution (HR) screens. The main idea is to use an implicit transformer to encode the pixel values of the image features. The proposed method is evaluated on both compressed and uncompressed SCI1K-compression datasets.
2855,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,This paper proposes a method for super-resolution (SR) browsing on high resolution (HR) screens. The main idea is to use an implicit transformer to encode the pixel values of the image features. The proposed method is evaluated on both compressed and uncompressed SCI1K-compression datasets.
2856,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,This paper proposes a method for super-resolution (SR) browsing on high resolution (HR) screens. The main idea is to use an implicit transformer to encode the pixel values of the image features. The proposed method is evaluated on both compressed and uncompressed SCI1K-compression datasets.
2857,SP:862223b8bd4c275f96c7e41c92daaa2ca2906194,This paper proposes a method for super-resolution (SR) browsing on high resolution (HR) screens. The main idea is to use an implicit transformer to encode the pixel values of the image features. The proposed method is evaluated on both compressed and uncompressed SCI1K-compression datasets.
2858,SP:3751625929b707ced417c3eb10064e4917866048,This paper studies the problem of learning interventional causal models. The authors propose to use sumproduct networks (SPNs) to learn interventional distributions. The main contribution of this paper is to propose a structural causal model for interventional SPNs. The proposed model is based on Pearl’s do-operator. The experimental results show the effectiveness of the proposed model.
2859,SP:3751625929b707ced417c3eb10064e4917866048,This paper studies the problem of learning interventional causal models. The authors propose to use sumproduct networks (SPNs) to learn interventional distributions. The main contribution of this paper is to propose a structural causal model for interventional SPNs. The proposed model is based on Pearl’s do-operator. The experimental results show the effectiveness of the proposed model.
2860,SP:3751625929b707ced417c3eb10064e4917866048,This paper studies the problem of learning interventional causal models. The authors propose to use sumproduct networks (SPNs) to learn interventional distributions. The main contribution of this paper is to propose a structural causal model for interventional SPNs. The proposed model is based on Pearl’s do-operator. The experimental results show the effectiveness of the proposed model.
2861,SP:3751625929b707ced417c3eb10064e4917866048,This paper studies the problem of learning interventional causal models. The authors propose to use sumproduct networks (SPNs) to learn interventional distributions. The main contribution of this paper is to propose a structural causal model for interventional SPNs. The proposed model is based on Pearl’s do-operator. The experimental results show the effectiveness of the proposed model.
2862,SP:3751625929b707ced417c3eb10064e4917866048,This paper studies the problem of learning interventional causal models. The authors propose to use sumproduct networks (SPNs) to learn interventional distributions. The main contribution of this paper is to propose a structural causal model for interventional SPNs. The proposed model is based on Pearl’s do-operator. The experimental results show the effectiveness of the proposed model.
2863,SP:c857ff674ca05c1d949337cb885f056b82d981d6,This paper proposes a generative model based on deep Markov factor analysis (DMFA) to capture nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov property of temporal embeddings and spatial inductive assumptions. The experimental results on synthetic and real fMRI data demonstrate the effectiveness of the proposed model.
2864,SP:c857ff674ca05c1d949337cb885f056b82d981d6,This paper proposes a generative model based on deep Markov factor analysis (DMFA) to capture nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov property of temporal embeddings and spatial inductive assumptions. The experimental results on synthetic and real fMRI data demonstrate the effectiveness of the proposed model.
2865,SP:c857ff674ca05c1d949337cb885f056b82d981d6,This paper proposes a generative model based on deep Markov factor analysis (DMFA) to capture nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov property of temporal embeddings and spatial inductive assumptions. The experimental results on synthetic and real fMRI data demonstrate the effectiveness of the proposed model.
2866,SP:c857ff674ca05c1d949337cb885f056b82d981d6,This paper proposes a generative model based on deep Markov factor analysis (DMFA) to capture nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov property of temporal embeddings and spatial inductive assumptions. The experimental results on synthetic and real fMRI data demonstrate the effectiveness of the proposed model.
2867,SP:c857ff674ca05c1d949337cb885f056b82d981d6,This paper proposes a generative model based on deep Markov factor analysis (DMFA) to capture nonlinear temporal dependencies in functional magnetic resonance imaging (fMRI) data. The proposed model is based on the Markov property of temporal embeddings and spatial inductive assumptions. The experimental results on synthetic and real fMRI data demonstrate the effectiveness of the proposed model.
2868,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes continuous normalizing flows (CNFs), a generative model based on the Moser flow (MF) framework. The main idea is to use a neural network (NN) to estimate the model density, which can then be used as the source of the divergence between the model and the prior density. The authors show that the proposed CNF can be used to approximate the source and prior densities of the ODE solver. They also show that CNFs can approximate the density of a manifold on the implicit surface of the manifold. Finally, the authors provide empirical results on synthetic and real-world datasets."
2869,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes continuous normalizing flows (CNFs), a generative model based on the Moser flow (MF) framework. The main idea is to use a neural network (NN) to estimate the model density, which can then be used as the source of the divergence between the model and the prior density. The authors show that the proposed CNF can be used to approximate the source and prior densities of the ODE solver. They also show that CNFs can approximate the density of a manifold on the implicit surface of the manifold. Finally, the authors provide empirical results on synthetic and real-world datasets."
2870,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes continuous normalizing flows (CNFs), a generative model based on the Moser flow (MF) framework. The main idea is to use a neural network (NN) to estimate the model density, which can then be used as the source of the divergence between the model and the prior density. The authors show that the proposed CNF can be used to approximate the source and prior densities of the ODE solver. They also show that CNFs can approximate the density of a manifold on the implicit surface of the manifold. Finally, the authors provide empirical results on synthetic and real-world datasets."
2871,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes continuous normalizing flows (CNFs), a generative model based on the Moser flow (MF) framework. The main idea is to use a neural network (NN) to estimate the model density, which can then be used as the source of the divergence between the model and the prior density. The authors show that the proposed CNF can be used to approximate the source and prior densities of the ODE solver. They also show that CNFs can approximate the density of a manifold on the implicit surface of the manifold. Finally, the authors provide empirical results on synthetic and real-world datasets."
2872,SP:855dcaa42868a29a14619d63221169495ed5dd54,"This paper proposes continuous normalizing flows (CNFs), a generative model based on the Moser flow (MF) framework. The main idea is to use a neural network (NN) to estimate the model density, which can then be used as the source of the divergence between the model and the prior density. The authors show that the proposed CNF can be used to approximate the source and prior densities of the ODE solver. They also show that CNFs can approximate the density of a manifold on the implicit surface of the manifold. Finally, the authors provide empirical results on synthetic and real-world datasets."
2873,SP:545554de09d17df77d6169a5cc8f36022ecb355c,This paper studies the problem of unsupervised representation learning in the presence of non-identifiability issues. The authors propose an approach to identify the independent causal mechanisms in the generative process. The main contribution of this paper is to provide a theoretical analysis of the problem. The paper is well-written and easy to follow.
2874,SP:545554de09d17df77d6169a5cc8f36022ecb355c,This paper studies the problem of unsupervised representation learning in the presence of non-identifiability issues. The authors propose an approach to identify the independent causal mechanisms in the generative process. The main contribution of this paper is to provide a theoretical analysis of the problem. The paper is well-written and easy to follow.
2875,SP:545554de09d17df77d6169a5cc8f36022ecb355c,This paper studies the problem of unsupervised representation learning in the presence of non-identifiability issues. The authors propose an approach to identify the independent causal mechanisms in the generative process. The main contribution of this paper is to provide a theoretical analysis of the problem. The paper is well-written and easy to follow.
2876,SP:545554de09d17df77d6169a5cc8f36022ecb355c,This paper studies the problem of unsupervised representation learning in the presence of non-identifiability issues. The authors propose an approach to identify the independent causal mechanisms in the generative process. The main contribution of this paper is to provide a theoretical analysis of the problem. The paper is well-written and easy to follow.
2877,SP:545554de09d17df77d6169a5cc8f36022ecb355c,This paper studies the problem of unsupervised representation learning in the presence of non-identifiability issues. The authors propose an approach to identify the independent causal mechanisms in the generative process. The main contribution of this paper is to provide a theoretical analysis of the problem. The paper is well-written and easy to follow.
2878,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The main idea is to use a differentiable transition kernel for AIS, which is differentiable in the sense that it does not require any reparameterization of the target distribution. The authors show that the proposed method can be viewed as a variant of Hamiltonian Annealing (HAMMC) and show that it can be used in combination with AIS. They also provide a tight lower bound on the reparametrization error."
2879,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The main idea is to use a differentiable transition kernel for AIS, which is differentiable in the sense that it does not require any reparameterization of the target distribution. The authors show that the proposed method can be viewed as a variant of Hamiltonian Annealing (HAMMC) and show that it can be used in combination with AIS. They also provide a tight lower bound on the reparametrization error."
2880,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The main idea is to use a differentiable transition kernel for AIS, which is differentiable in the sense that it does not require any reparameterization of the target distribution. The authors show that the proposed method can be viewed as a variant of Hamiltonian Annealing (HAMMC) and show that it can be used in combination with AIS. They also provide a tight lower bound on the reparametrization error."
2881,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The main idea is to use a differentiable transition kernel for AIS, which is differentiable in the sense that it does not require any reparameterization of the target distribution. The authors show that the proposed method can be viewed as a variant of Hamiltonian Annealing (HAMMC) and show that it can be used in combination with AIS. They also provide a tight lower bound on the reparametrization error."
2882,SP:7df49c554d6c9fca370f049279ef7324b6f79de9,"This paper proposes a new method for Annealed Importance Sampling (AIS) based on Hamiltonian MCMC. The main idea is to use a differentiable transition kernel for AIS, which is differentiable in the sense that it does not require any reparameterization of the target distribution. The authors show that the proposed method can be viewed as a variant of Hamiltonian Annealing (HAMMC) and show that it can be used in combination with AIS. They also provide a tight lower bound on the reparametrization error."
2883,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a method for certifiable training of deep neural networks. The main idea is to use a plug-in module for the Lipschitz constant of the weight matrices of the neural network. The method is based on the idea that the weight matrix of a neural network is non-convex if the activation function of the network is not convex. The authors show that this is the case when the activation functions are ReLU, MaxMin, and ReLU-based activation functions. The proposed method is evaluated on MNIST, TinyImageNet, and CIFAR-10."
2884,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a method for certifiable training of deep neural networks. The main idea is to use a plug-in module for the Lipschitz constant of the weight matrices of the neural network. The method is based on the idea that the weight matrix of a neural network is non-convex if the activation function of the network is not convex. The authors show that this is the case when the activation functions are ReLU, MaxMin, and ReLU-based activation functions. The proposed method is evaluated on MNIST, TinyImageNet, and CIFAR-10."
2885,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a method for certifiable training of deep neural networks. The main idea is to use a plug-in module for the Lipschitz constant of the weight matrices of the neural network. The method is based on the idea that the weight matrix of a neural network is non-convex if the activation function of the network is not convex. The authors show that this is the case when the activation functions are ReLU, MaxMin, and ReLU-based activation functions. The proposed method is evaluated on MNIST, TinyImageNet, and CIFAR-10."
2886,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a method for certifiable training of deep neural networks. The main idea is to use a plug-in module for the Lipschitz constant of the weight matrices of the neural network. The method is based on the idea that the weight matrix of a neural network is non-convex if the activation function of the network is not convex. The authors show that this is the case when the activation functions are ReLU, MaxMin, and ReLU-based activation functions. The proposed method is evaluated on MNIST, TinyImageNet, and CIFAR-10."
2887,SP:b0bf070e8d7eefdfc45f236e9ecb9edfb4816e0a,"This paper proposes a method for certifiable training of deep neural networks. The main idea is to use a plug-in module for the Lipschitz constant of the weight matrices of the neural network. The method is based on the idea that the weight matrix of a neural network is non-convex if the activation function of the network is not convex. The authors show that this is the case when the activation functions are ReLU, MaxMin, and ReLU-based activation functions. The proposed method is evaluated on MNIST, TinyImageNet, and CIFAR-10."
2888,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper studies the problem of learning conformal Bayesian predictive intervals for Bayesian posterior predictive distributions. In particular, the authors propose to use add-one-in importance sampling to learn conformal predictive intervals with finite sample frequentist guarantees. The main contribution of this paper is to propose a new approach to compute conformal posterior predictive intervals by re-weighting the posterior samples of model parameters. The authors show that the proposed approach can achieve better performance than existing conformal methods in terms of accuracy and computational efficiency."
2889,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper studies the problem of learning conformal Bayesian predictive intervals for Bayesian posterior predictive distributions. In particular, the authors propose to use add-one-in importance sampling to learn conformal predictive intervals with finite sample frequentist guarantees. The main contribution of this paper is to propose a new approach to compute conformal posterior predictive intervals by re-weighting the posterior samples of model parameters. The authors show that the proposed approach can achieve better performance than existing conformal methods in terms of accuracy and computational efficiency."
2890,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper studies the problem of learning conformal Bayesian predictive intervals for Bayesian posterior predictive distributions. In particular, the authors propose to use add-one-in importance sampling to learn conformal predictive intervals with finite sample frequentist guarantees. The main contribution of this paper is to propose a new approach to compute conformal posterior predictive intervals by re-weighting the posterior samples of model parameters. The authors show that the proposed approach can achieve better performance than existing conformal methods in terms of accuracy and computational efficiency."
2891,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper studies the problem of learning conformal Bayesian predictive intervals for Bayesian posterior predictive distributions. In particular, the authors propose to use add-one-in importance sampling to learn conformal predictive intervals with finite sample frequentist guarantees. The main contribution of this paper is to propose a new approach to compute conformal posterior predictive intervals by re-weighting the posterior samples of model parameters. The authors show that the proposed approach can achieve better performance than existing conformal methods in terms of accuracy and computational efficiency."
2892,SP:f6314bfd897cb996de2eaabf0d3037f41da467f3,"This paper studies the problem of learning conformal Bayesian predictive intervals for Bayesian posterior predictive distributions. In particular, the authors propose to use add-one-in importance sampling to learn conformal predictive intervals with finite sample frequentist guarantees. The main contribution of this paper is to propose a new approach to compute conformal posterior predictive intervals by re-weighting the posterior samples of model parameters. The authors show that the proposed approach can achieve better performance than existing conformal methods in terms of accuracy and computational efficiency."
2893,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the problem of denoising in inverse problems. In particular, the authors consider the case where the denoiser is a convolutional neural network (CNN) with symmetric Jacobians. The authors show that the Lipschitz constant of CNN denoisers can be used to approximate the maximum a posteriori (MAP) and the minimum mean square error (MMSE) estimators of the inverse problem. The paper then proposes a new method to approximate these estimators, which is based on the backtracking step size of the denoiser. The proposed method is evaluated on a variety of image classification tasks."
2894,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the problem of denoising in inverse problems. In particular, the authors consider the case where the denoiser is a convolutional neural network (CNN) with symmetric Jacobians. The authors show that the Lipschitz constant of CNN denoisers can be used to approximate the maximum a posteriori (MAP) and the minimum mean square error (MMSE) estimators of the inverse problem. The paper then proposes a new method to approximate these estimators, which is based on the backtracking step size of the denoiser. The proposed method is evaluated on a variety of image classification tasks."
2895,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the problem of denoising in inverse problems. In particular, the authors consider the case where the denoiser is a convolutional neural network (CNN) with symmetric Jacobians. The authors show that the Lipschitz constant of CNN denoisers can be used to approximate the maximum a posteriori (MAP) and the minimum mean square error (MMSE) estimators of the inverse problem. The paper then proposes a new method to approximate these estimators, which is based on the backtracking step size of the denoiser. The proposed method is evaluated on a variety of image classification tasks."
2896,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the problem of denoising in inverse problems. In particular, the authors consider the case where the denoiser is a convolutional neural network (CNN) with symmetric Jacobians. The authors show that the Lipschitz constant of CNN denoisers can be used to approximate the maximum a posteriori (MAP) and the minimum mean square error (MMSE) estimators of the inverse problem. The paper then proposes a new method to approximate these estimators, which is based on the backtracking step size of the denoiser. The proposed method is evaluated on a variety of image classification tasks."
2897,SP:1e86c162b8e8d652a0590b66aa5f7c363955cc5b,"This paper studies the problem of denoising in inverse problems. In particular, the authors consider the case where the denoiser is a convolutional neural network (CNN) with symmetric Jacobians. The authors show that the Lipschitz constant of CNN denoisers can be used to approximate the maximum a posteriori (MAP) and the minimum mean square error (MMSE) estimators of the inverse problem. The paper then proposes a new method to approximate these estimators, which is based on the backtracking step size of the denoiser. The proposed method is evaluated on a variety of image classification tasks."
2898,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music based on human movements. The method is based on a transformer-based model and a U-net based model. The key idea is to learn skeleton keypoints for each body part in a video, and then use these keypoints to generate a style pattern for the video. The proposed method is evaluated on a variety of video datasets."
2899,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music based on human movements. The method is based on a transformer-based model and a U-net based model. The key idea is to learn skeleton keypoints for each body part in a video, and then use these keypoints to generate a style pattern for the video. The proposed method is evaluated on a variety of video datasets."
2900,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music based on human movements. The method is based on a transformer-based model and a U-net based model. The key idea is to learn skeleton keypoints for each body part in a video, and then use these keypoints to generate a style pattern for the video. The proposed method is evaluated on a variety of video datasets."
2901,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music based on human movements. The method is based on a transformer-based model and a U-net based model. The key idea is to learn skeleton keypoints for each body part in a video, and then use these keypoints to generate a style pattern for the video. The proposed method is evaluated on a variety of video datasets."
2902,SP:da92e936f88b3842ca82c2914413b129ca35890f,"This paper proposes a method for generating music based on human movements. The method is based on a transformer-based model and a U-net based model. The key idea is to learn skeleton keypoints for each body part in a video, and then use these keypoints to generate a style pattern for the video. The proposed method is evaluated on a variety of video datasets."
2903,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes an iterative actor-critic algorithm for offline reinforcement learning. The main idea is to use the on-policy Q estimate of the behavior policy as a regularizer for policy improvement. The proposed algorithm is evaluated on the D4RL benchmark and compared to a one-step algorithm.
2904,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes an iterative actor-critic algorithm for offline reinforcement learning. The main idea is to use the on-policy Q estimate of the behavior policy as a regularizer for policy improvement. The proposed algorithm is evaluated on the D4RL benchmark and compared to a one-step algorithm.
2905,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes an iterative actor-critic algorithm for offline reinforcement learning. The main idea is to use the on-policy Q estimate of the behavior policy as a regularizer for policy improvement. The proposed algorithm is evaluated on the D4RL benchmark and compared to a one-step algorithm.
2906,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes an iterative actor-critic algorithm for offline reinforcement learning. The main idea is to use the on-policy Q estimate of the behavior policy as a regularizer for policy improvement. The proposed algorithm is evaluated on the D4RL benchmark and compared to a one-step algorithm.
2907,SP:0f7ff312a242a553dc9ecf35b421e58fb2d50a26,This paper proposes an iterative actor-critic algorithm for offline reinforcement learning. The main idea is to use the on-policy Q estimate of the behavior policy as a regularizer for policy improvement. The proposed algorithm is evaluated on the D4RL benchmark and compared to a one-step algorithm.
2908,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of matrix recovery from nonsmooth low-rank SVDs. The authors propose an extragradient method to solve the problem. The main contribution of this paper is to study the problem from the perspective of the maximum of smooth functions, which is a natural generalized strict complementarity condition, and to show that the optimal solution can be found by maximizing the max-margin of a smooth function with respect to the maximum number of smooth function initializations. "
2909,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of matrix recovery from nonsmooth low-rank SVDs. The authors propose an extragradient method to solve the problem. The main contribution of this paper is to study the problem from the perspective of the maximum of smooth functions, which is a natural generalized strict complementarity condition, and to show that the optimal solution can be found by maximizing the max-margin of a smooth function with respect to the maximum number of smooth function initializations. "
2910,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of matrix recovery from nonsmooth low-rank SVDs. The authors propose an extragradient method to solve the problem. The main contribution of this paper is to study the problem from the perspective of the maximum of smooth functions, which is a natural generalized strict complementarity condition, and to show that the optimal solution can be found by maximizing the max-margin of a smooth function with respect to the maximum number of smooth function initializations. "
2911,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of matrix recovery from nonsmooth low-rank SVDs. The authors propose an extragradient method to solve the problem. The main contribution of this paper is to study the problem from the perspective of the maximum of smooth functions, which is a natural generalized strict complementarity condition, and to show that the optimal solution can be found by maximizing the max-margin of a smooth function with respect to the maximum number of smooth function initializations. "
2912,SP:0346eba4f587acbe3492d039066f1737360fd870,"This paper studies the problem of matrix recovery from nonsmooth low-rank SVDs. The authors propose an extragradient method to solve the problem. The main contribution of this paper is to study the problem from the perspective of the maximum of smooth functions, which is a natural generalized strict complementarity condition, and to show that the optimal solution can be found by maximizing the max-margin of a smooth function with respect to the maximum number of smooth function initializations. "
2913,SP:d39f1d77d9919f897ccf82958b71be8798523923,This paper proposes a method for estimating the causal effect of a treatment on the outcome of a given treatment. The proposed method is based on the generalized Robinson decomposition of the causal estimand (CATE). The authors show that the proposed method converges to a stationary point with a quasi-oracle convergence guarantee under mild assumptions. The method is evaluated on small-world and molecular graphs.
2914,SP:d39f1d77d9919f897ccf82958b71be8798523923,This paper proposes a method for estimating the causal effect of a treatment on the outcome of a given treatment. The proposed method is based on the generalized Robinson decomposition of the causal estimand (CATE). The authors show that the proposed method converges to a stationary point with a quasi-oracle convergence guarantee under mild assumptions. The method is evaluated on small-world and molecular graphs.
2915,SP:d39f1d77d9919f897ccf82958b71be8798523923,This paper proposes a method for estimating the causal effect of a treatment on the outcome of a given treatment. The proposed method is based on the generalized Robinson decomposition of the causal estimand (CATE). The authors show that the proposed method converges to a stationary point with a quasi-oracle convergence guarantee under mild assumptions. The method is evaluated on small-world and molecular graphs.
2916,SP:d39f1d77d9919f897ccf82958b71be8798523923,This paper proposes a method for estimating the causal effect of a treatment on the outcome of a given treatment. The proposed method is based on the generalized Robinson decomposition of the causal estimand (CATE). The authors show that the proposed method converges to a stationary point with a quasi-oracle convergence guarantee under mild assumptions. The method is evaluated on small-world and molecular graphs.
2917,SP:d39f1d77d9919f897ccf82958b71be8798523923,This paper proposes a method for estimating the causal effect of a treatment on the outcome of a given treatment. The proposed method is based on the generalized Robinson decomposition of the causal estimand (CATE). The authors show that the proposed method converges to a stationary point with a quasi-oracle convergence guarantee under mild assumptions. The method is evaluated on small-world and molecular graphs.
2918,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,This paper studies the problem of causal effect identification. The authors propose a new method based on a combination of matrix equations and graphical criteria to identify the causal effect. The proposed method can be viewed as an extension of existing methods that use matrix equations to identify proxy variables. The main contribution of this paper is the introduction of a matrix-based criteria that can be used as a proxy variable based identification criteria. The paper also provides a theoretical analysis of the proposed method.
2919,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,This paper studies the problem of causal effect identification. The authors propose a new method based on a combination of matrix equations and graphical criteria to identify the causal effect. The proposed method can be viewed as an extension of existing methods that use matrix equations to identify proxy variables. The main contribution of this paper is the introduction of a matrix-based criteria that can be used as a proxy variable based identification criteria. The paper also provides a theoretical analysis of the proposed method.
2920,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,This paper studies the problem of causal effect identification. The authors propose a new method based on a combination of matrix equations and graphical criteria to identify the causal effect. The proposed method can be viewed as an extension of existing methods that use matrix equations to identify proxy variables. The main contribution of this paper is the introduction of a matrix-based criteria that can be used as a proxy variable based identification criteria. The paper also provides a theoretical analysis of the proposed method.
2921,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,This paper studies the problem of causal effect identification. The authors propose a new method based on a combination of matrix equations and graphical criteria to identify the causal effect. The proposed method can be viewed as an extension of existing methods that use matrix equations to identify proxy variables. The main contribution of this paper is the introduction of a matrix-based criteria that can be used as a proxy variable based identification criteria. The paper also provides a theoretical analysis of the proposed method.
2922,SP:eeb42a1e48857f976a647eb8d86d25c9012962d5,This paper studies the problem of causal effect identification. The authors propose a new method based on a combination of matrix equations and graphical criteria to identify the causal effect. The proposed method can be viewed as an extension of existing methods that use matrix equations to identify proxy variables. The main contribution of this paper is the introduction of a matrix-based criteria that can be used as a proxy variable based identification criteria. The paper also provides a theoretical analysis of the proposed method.
2923,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of Dual Curriculum Design (DCD). In particular, the authors propose a method called Prioritized Level Replay (PLR), which is a combination of two existing UED methods, PAIRED and PLR. The main contribution of this paper is the theoretical analysis of the convergence of PLR to Nash equilibria. The authors show that PLR converges to a Nash equilibrium with high probability, and provide theoretical guarantees on the robustness of the proposed method."
2924,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of Dual Curriculum Design (DCD). In particular, the authors propose a method called Prioritized Level Replay (PLR), which is a combination of two existing UED methods, PAIRED and PLR. The main contribution of this paper is the theoretical analysis of the convergence of PLR to Nash equilibria. The authors show that PLR converges to a Nash equilibrium with high probability, and provide theoretical guarantees on the robustness of the proposed method."
2925,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of Dual Curriculum Design (DCD). In particular, the authors propose a method called Prioritized Level Replay (PLR), which is a combination of two existing UED methods, PAIRED and PLR. The main contribution of this paper is the theoretical analysis of the convergence of PLR to Nash equilibria. The authors show that PLR converges to a Nash equilibrium with high probability, and provide theoretical guarantees on the robustness of the proposed method."
2926,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of Dual Curriculum Design (DCD). In particular, the authors propose a method called Prioritized Level Replay (PLR), which is a combination of two existing UED methods, PAIRED and PLR. The main contribution of this paper is the theoretical analysis of the convergence of PLR to Nash equilibria. The authors show that PLR converges to a Nash equilibrium with high probability, and provide theoretical guarantees on the robustness of the proposed method."
2927,SP:db15860d08418f6bc792c2ade2eade32840a12b8,"This paper studies the problem of unsupervised environment design (UED) in the context of Dual Curriculum Design (DCD). In particular, the authors propose a method called Prioritized Level Replay (PLR), which is a combination of two existing UED methods, PAIRED and PLR. The main contribution of this paper is the theoretical analysis of the convergence of PLR to Nash equilibria. The authors show that PLR converges to a Nash equilibrium with high probability, and provide theoretical guarantees on the robustness of the proposed method."
2928,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem with shuffle model. The authors propose a (\epsilon, \delta)-differentially private algorithm to solve the MAB problem. The algorithm is shown to have a lower bound on the suboptimality gap. The regret is also shown to be distribution-independent."
2929,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem with shuffle model. The authors propose a (\epsilon, \delta)-differentially private algorithm to solve the MAB problem. The algorithm is shown to have a lower bound on the suboptimality gap. The regret is also shown to be distribution-independent."
2930,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem with shuffle model. The authors propose a (\epsilon, \delta)-differentially private algorithm to solve the MAB problem. The algorithm is shown to have a lower bound on the suboptimality gap. The regret is also shown to be distribution-independent."
2931,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem with shuffle model. The authors propose a (\epsilon, \delta)-differentially private algorithm to solve the MAB problem. The algorithm is shown to have a lower bound on the suboptimality gap. The regret is also shown to be distribution-independent."
2932,SP:9ed528da4b67f22678303cfd975aafe678db6411,"This paper studies the multi-armed bandit (MAB) problem with shuffle model. The authors propose a (\epsilon, \delta)-differentially private algorithm to solve the MAB problem. The algorithm is shown to have a lower bound on the suboptimality gap. The regret is also shown to be distribution-independent."
2933,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper studies the problem of threshold calibration in the regression setting. The authors propose a method to calibrate the loss function of a threshold-calibrated forecaster with respect to the loss of threshold decisions. The proposed method is based on the idea that the threshold of the loss should be calibrated to be close to the threshold decision of the uncalibrated model. The method is evaluated on a number of real-world problems, including hospital scheduling decisions and resource allocation decisions."
2934,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper studies the problem of threshold calibration in the regression setting. The authors propose a method to calibrate the loss function of a threshold-calibrated forecaster with respect to the loss of threshold decisions. The proposed method is based on the idea that the threshold of the loss should be calibrated to be close to the threshold decision of the uncalibrated model. The method is evaluated on a number of real-world problems, including hospital scheduling decisions and resource allocation decisions."
2935,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper studies the problem of threshold calibration in the regression setting. The authors propose a method to calibrate the loss function of a threshold-calibrated forecaster with respect to the loss of threshold decisions. The proposed method is based on the idea that the threshold of the loss should be calibrated to be close to the threshold decision of the uncalibrated model. The method is evaluated on a number of real-world problems, including hospital scheduling decisions and resource allocation decisions."
2936,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper studies the problem of threshold calibration in the regression setting. The authors propose a method to calibrate the loss function of a threshold-calibrated forecaster with respect to the loss of threshold decisions. The proposed method is based on the idea that the threshold of the loss should be calibrated to be close to the threshold decision of the uncalibrated model. The method is evaluated on a number of real-world problems, including hospital scheduling decisions and resource allocation decisions."
2937,SP:de2523a5fdebda3573f1063447a7818bf3ed6333,"This paper studies the problem of threshold calibration in the regression setting. The authors propose a method to calibrate the loss function of a threshold-calibrated forecaster with respect to the loss of threshold decisions. The proposed method is based on the idea that the threshold of the loss should be calibrated to be close to the threshold decision of the uncalibrated model. The method is evaluated on a number of real-world problems, including hospital scheduling decisions and resource allocation decisions."
2938,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method for centroid approximation to the argmax distribution of random functions. The method is based on the Wasserstein distance between two points in the space of centroid points. The authors show that this distance can be used to estimate the centroid of a random function. The main contribution of this paper is to propose a new objective function for centroids, which is motivated by the fact that it is computationally efficient to compute Wassersteins distances between points in a space of points. "
2939,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method for centroid approximation to the argmax distribution of random functions. The method is based on the Wasserstein distance between two points in the space of centroid points. The authors show that this distance can be used to estimate the centroid of a random function. The main contribution of this paper is to propose a new objective function for centroids, which is motivated by the fact that it is computationally efficient to compute Wassersteins distances between points in a space of points. "
2940,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method for centroid approximation to the argmax distribution of random functions. The method is based on the Wasserstein distance between two points in the space of centroid points. The authors show that this distance can be used to estimate the centroid of a random function. The main contribution of this paper is to propose a new objective function for centroids, which is motivated by the fact that it is computationally efficient to compute Wassersteins distances between points in a space of points. "
2941,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method for centroid approximation to the argmax distribution of random functions. The method is based on the Wasserstein distance between two points in the space of centroid points. The authors show that this distance can be used to estimate the centroid of a random function. The main contribution of this paper is to propose a new objective function for centroids, which is motivated by the fact that it is computationally efficient to compute Wassersteins distances between points in a space of points. "
2942,SP:f55160db59c6f3e85f6e1ea0ec32c1a0982fbc48,"This paper proposes a method for centroid approximation to the argmax distribution of random functions. The method is based on the Wasserstein distance between two points in the space of centroid points. The authors show that this distance can be used to estimate the centroid of a random function. The main contribution of this paper is to propose a new objective function for centroids, which is motivated by the fact that it is computationally efficient to compute Wassersteins distances between points in a space of points. "
2943,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper studies the problem of online multi-objective reinforcement learning in an episodic learning setting, where the goal is to learn a policy that maximizes a reward function over a sequence of transitions. The authors propose a model-based algorithm for this problem, and show that it can achieve a regret bound of $O(1/\epsilon^2)$ with nearly optimal trajectory complexity. In particular, the authors show that the regret bound depends only on the number of transitions and not on the preference of the policy."
2944,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper studies the problem of online multi-objective reinforcement learning in an episodic learning setting, where the goal is to learn a policy that maximizes a reward function over a sequence of transitions. The authors propose a model-based algorithm for this problem, and show that it can achieve a regret bound of $O(1/\epsilon^2)$ with nearly optimal trajectory complexity. In particular, the authors show that the regret bound depends only on the number of transitions and not on the preference of the policy."
2945,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper studies the problem of online multi-objective reinforcement learning in an episodic learning setting, where the goal is to learn a policy that maximizes a reward function over a sequence of transitions. The authors propose a model-based algorithm for this problem, and show that it can achieve a regret bound of $O(1/\epsilon^2)$ with nearly optimal trajectory complexity. In particular, the authors show that the regret bound depends only on the number of transitions and not on the preference of the policy."
2946,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper studies the problem of online multi-objective reinforcement learning in an episodic learning setting, where the goal is to learn a policy that maximizes a reward function over a sequence of transitions. The authors propose a model-based algorithm for this problem, and show that it can achieve a regret bound of $O(1/\epsilon^2)$ with nearly optimal trajectory complexity. In particular, the authors show that the regret bound depends only on the number of transitions and not on the preference of the policy."
2947,SP:ef342e3c6a16e898a49b700a9fd4f0ea6a069dcc,"This paper studies the problem of online multi-objective reinforcement learning in an episodic learning setting, where the goal is to learn a policy that maximizes a reward function over a sequence of transitions. The authors propose a model-based algorithm for this problem, and show that it can achieve a regret bound of $O(1/\epsilon^2)$ with nearly optimal trajectory complexity. In particular, the authors show that the regret bound depends only on the number of transitions and not on the preference of the policy."
2948,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating dialogue response generation from open-ended sentences. The proposed method is based on local explanation of response generation (LERG), which is a model-agnostic explanation of sequence generation models. LERG is trained to generate a sequence of sentences that are consistent with the human response. The authors show that LERG can be used for both explicit and implicit relations between the generated sentences and the human responses. The method is evaluated on the task of dialog response generation."
2949,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating dialogue response generation from open-ended sentences. The proposed method is based on local explanation of response generation (LERG), which is a model-agnostic explanation of sequence generation models. LERG is trained to generate a sequence of sentences that are consistent with the human response. The authors show that LERG can be used for both explicit and implicit relations between the generated sentences and the human responses. The method is evaluated on the task of dialog response generation."
2950,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating dialogue response generation from open-ended sentences. The proposed method is based on local explanation of response generation (LERG), which is a model-agnostic explanation of sequence generation models. LERG is trained to generate a sequence of sentences that are consistent with the human response. The authors show that LERG can be used for both explicit and implicit relations between the generated sentences and the human responses. The method is evaluated on the task of dialog response generation."
2951,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating dialogue response generation from open-ended sentences. The proposed method is based on local explanation of response generation (LERG), which is a model-agnostic explanation of sequence generation models. LERG is trained to generate a sequence of sentences that are consistent with the human response. The authors show that LERG can be used for both explicit and implicit relations between the generated sentences and the human responses. The method is evaluated on the task of dialog response generation."
2952,SP:aa84981dd503ec34d9f06aa6e5f680e267f82b04,"This paper proposes a method for generating dialogue response generation from open-ended sentences. The proposed method is based on local explanation of response generation (LERG), which is a model-agnostic explanation of sequence generation models. LERG is trained to generate a sequence of sentences that are consistent with the human response. The authors show that LERG can be used for both explicit and implicit relations between the generated sentences and the human responses. The method is evaluated on the task of dialog response generation."
2953,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence rate of error compensated gradient compression methods. The authors propose a new algorithm, Katyusha, which combines error compensation and accelerated linear convergence rate. The main contribution of this paper is a theoretical analysis of convergence rate and convergence rate for error compensated algorithms. The theoretical analysis shows that the error compensated algorithm converges linearly to a stationary point. The empirical results show that Katyusha converges faster than other error compensated methods."
2954,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence rate of error compensated gradient compression methods. The authors propose a new algorithm, Katyusha, which combines error compensation and accelerated linear convergence rate. The main contribution of this paper is a theoretical analysis of convergence rate and convergence rate for error compensated algorithms. The theoretical analysis shows that the error compensated algorithm converges linearly to a stationary point. The empirical results show that Katyusha converges faster than other error compensated methods."
2955,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence rate of error compensated gradient compression methods. The authors propose a new algorithm, Katyusha, which combines error compensation and accelerated linear convergence rate. The main contribution of this paper is a theoretical analysis of convergence rate and convergence rate for error compensated algorithms. The theoretical analysis shows that the error compensated algorithm converges linearly to a stationary point. The empirical results show that Katyusha converges faster than other error compensated methods."
2956,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence rate of error compensated gradient compression methods. The authors propose a new algorithm, Katyusha, which combines error compensation and accelerated linear convergence rate. The main contribution of this paper is a theoretical analysis of convergence rate and convergence rate for error compensated algorithms. The theoretical analysis shows that the error compensated algorithm converges linearly to a stationary point. The empirical results show that Katyusha converges faster than other error compensated methods."
2957,SP:965413b1726617006317bbbec55673dd5d21812a,"This paper studies the convergence rate of error compensated gradient compression methods. The authors propose a new algorithm, Katyusha, which combines error compensation and accelerated linear convergence rate. The main contribution of this paper is a theoretical analysis of convergence rate and convergence rate for error compensated algorithms. The theoretical analysis shows that the error compensated algorithm converges linearly to a stationary point. The empirical results show that Katyusha converges faster than other error compensated methods."
2958,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a model of the neuron-astrocyte liquid state machine (NALSM) which is inspired by the neuron and astrocytes in the brain. NALSM is an extension of the LSM model, where the neurons are modeled as neurons and the astrocyses as neurons. The authors show that the proposed model is able to achieve state-of-the-art performance on MNIST, Fashion-MNIST, and CIFAR-10."
2959,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a model of the neuron-astrocyte liquid state machine (NALSM) which is inspired by the neuron and astrocytes in the brain. NALSM is an extension of the LSM model, where the neurons are modeled as neurons and the astrocyses as neurons. The authors show that the proposed model is able to achieve state-of-the-art performance on MNIST, Fashion-MNIST, and CIFAR-10."
2960,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a model of the neuron-astrocyte liquid state machine (NALSM) which is inspired by the neuron and astrocytes in the brain. NALSM is an extension of the LSM model, where the neurons are modeled as neurons and the astrocyses as neurons. The authors show that the proposed model is able to achieve state-of-the-art performance on MNIST, Fashion-MNIST, and CIFAR-10."
2961,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a model of the neuron-astrocyte liquid state machine (NALSM) which is inspired by the neuron and astrocytes in the brain. NALSM is an extension of the LSM model, where the neurons are modeled as neurons and the astrocyses as neurons. The authors show that the proposed model is able to achieve state-of-the-art performance on MNIST, Fashion-MNIST, and CIFAR-10."
2962,SP:27c58dad7fa7743a8ff56fad863aa0dae823dccb,"This paper proposes a model of the neuron-astrocyte liquid state machine (NALSM) which is inspired by the neuron and astrocytes in the brain. NALSM is an extension of the LSM model, where the neurons are modeled as neurons and the astrocyses as neurons. The authors show that the proposed model is able to achieve state-of-the-art performance on MNIST, Fashion-MNIST, and CIFAR-10."
2963,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper proposes a method to tackle the problem of topology imbalance in graph neural networks (GNNs). Specifically, the authors propose a new metric called Totoro, which measures the influence conflict between two classes of nodes in a graph. The authors also propose a label propagation algorithm to mitigate the influence shift phenomenon of node influence shift. Experiments on semi-supervised node classification and topology-imbalance problem show that the proposed method outperforms existing methods."
2964,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper proposes a method to tackle the problem of topology imbalance in graph neural networks (GNNs). Specifically, the authors propose a new metric called Totoro, which measures the influence conflict between two classes of nodes in a graph. The authors also propose a label propagation algorithm to mitigate the influence shift phenomenon of node influence shift. Experiments on semi-supervised node classification and topology-imbalance problem show that the proposed method outperforms existing methods."
2965,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper proposes a method to tackle the problem of topology imbalance in graph neural networks (GNNs). Specifically, the authors propose a new metric called Totoro, which measures the influence conflict between two classes of nodes in a graph. The authors also propose a label propagation algorithm to mitigate the influence shift phenomenon of node influence shift. Experiments on semi-supervised node classification and topology-imbalance problem show that the proposed method outperforms existing methods."
2966,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper proposes a method to tackle the problem of topology imbalance in graph neural networks (GNNs). Specifically, the authors propose a new metric called Totoro, which measures the influence conflict between two classes of nodes in a graph. The authors also propose a label propagation algorithm to mitigate the influence shift phenomenon of node influence shift. Experiments on semi-supervised node classification and topology-imbalance problem show that the proposed method outperforms existing methods."
2967,SP:64ccd697d3c11d7d8947ef1b06c61d94b6a2e575,"This paper proposes a method to tackle the problem of topology imbalance in graph neural networks (GNNs). Specifically, the authors propose a new metric called Totoro, which measures the influence conflict between two classes of nodes in a graph. The authors also propose a label propagation algorithm to mitigate the influence shift phenomenon of node influence shift. Experiments on semi-supervised node classification and topology-imbalance problem show that the proposed method outperforms existing methods."
2968,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery for additive Gaussian noise on a d-dimensional lattice with piece-wise constant signals. The authors propose a DCART-based procedure to recover the partition of the lattice using recursive dyadic partitions. The proposed algorithm is based on the optimal regression tree estimator (ORT) and the partition estimator of optimal regression trees. The algorithm is shown to be NP-hard, and the authors propose an exhaustive search method to find the optimal partition."
2969,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery for additive Gaussian noise on a d-dimensional lattice with piece-wise constant signals. The authors propose a DCART-based procedure to recover the partition of the lattice using recursive dyadic partitions. The proposed algorithm is based on the optimal regression tree estimator (ORT) and the partition estimator of optimal regression trees. The algorithm is shown to be NP-hard, and the authors propose an exhaustive search method to find the optimal partition."
2970,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery for additive Gaussian noise on a d-dimensional lattice with piece-wise constant signals. The authors propose a DCART-based procedure to recover the partition of the lattice using recursive dyadic partitions. The proposed algorithm is based on the optimal regression tree estimator (ORT) and the partition estimator of optimal regression trees. The algorithm is shown to be NP-hard, and the authors propose an exhaustive search method to find the optimal partition."
2971,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery for additive Gaussian noise on a d-dimensional lattice with piece-wise constant signals. The authors propose a DCART-based procedure to recover the partition of the lattice using recursive dyadic partitions. The proposed algorithm is based on the optimal regression tree estimator (ORT) and the partition estimator of optimal regression trees. The algorithm is shown to be NP-hard, and the authors propose an exhaustive search method to find the optimal partition."
2972,SP:ec12f0a05db75ac15ad22b34cdc2a0142bc2c72f,"This paper studies the problem of partition recovery for additive Gaussian noise on a d-dimensional lattice with piece-wise constant signals. The authors propose a DCART-based procedure to recover the partition of the lattice using recursive dyadic partitions. The proposed algorithm is based on the optimal regression tree estimator (ORT) and the partition estimator of optimal regression trees. The algorithm is shown to be NP-hard, and the authors propose an exhaustive search method to find the optimal partition."
2973,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a new method for training deep neural networks for causal inference. The proposed method, Counterfactual Maximum Likelihood Estimation (CMLE), is motivated by the observation that the interventional distribution of the observed confounders is not necessarily the same as the observational distribution. The authors propose to use a general Structural Causal Model (SCM) to model the causal relationship between the observed data and the underlying structure of the model, and propose two algorithms, Implicit CMLE and Explicit CMLE, to estimate the negative log-likelihood of the expected negative log likelihood of the true confounder. Experiments show that the proposed method outperforms the existing CMLE methods on synthetic and real-world datasets."
2974,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a new method for training deep neural networks for causal inference. The proposed method, Counterfactual Maximum Likelihood Estimation (CMLE), is motivated by the observation that the interventional distribution of the observed confounders is not necessarily the same as the observational distribution. The authors propose to use a general Structural Causal Model (SCM) to model the causal relationship between the observed data and the underlying structure of the model, and propose two algorithms, Implicit CMLE and Explicit CMLE, to estimate the negative log-likelihood of the expected negative log likelihood of the true confounder. Experiments show that the proposed method outperforms the existing CMLE methods on synthetic and real-world datasets."
2975,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a new method for training deep neural networks for causal inference. The proposed method, Counterfactual Maximum Likelihood Estimation (CMLE), is motivated by the observation that the interventional distribution of the observed confounders is not necessarily the same as the observational distribution. The authors propose to use a general Structural Causal Model (SCM) to model the causal relationship between the observed data and the underlying structure of the model, and propose two algorithms, Implicit CMLE and Explicit CMLE, to estimate the negative log-likelihood of the expected negative log likelihood of the true confounder. Experiments show that the proposed method outperforms the existing CMLE methods on synthetic and real-world datasets."
2976,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a new method for training deep neural networks for causal inference. The proposed method, Counterfactual Maximum Likelihood Estimation (CMLE), is motivated by the observation that the interventional distribution of the observed confounders is not necessarily the same as the observational distribution. The authors propose to use a general Structural Causal Model (SCM) to model the causal relationship between the observed data and the underlying structure of the model, and propose two algorithms, Implicit CMLE and Explicit CMLE, to estimate the negative log-likelihood of the expected negative log likelihood of the true confounder. Experiments show that the proposed method outperforms the existing CMLE methods on synthetic and real-world datasets."
2977,SP:3c65b3e69a024431cafdc1b4bfbccd432de69faf,"This paper proposes a new method for training deep neural networks for causal inference. The proposed method, Counterfactual Maximum Likelihood Estimation (CMLE), is motivated by the observation that the interventional distribution of the observed confounders is not necessarily the same as the observational distribution. The authors propose to use a general Structural Causal Model (SCM) to model the causal relationship between the observed data and the underlying structure of the model, and propose two algorithms, Implicit CMLE and Explicit CMLE, to estimate the negative log-likelihood of the expected negative log likelihood of the true confounder. Experiments show that the proposed method outperforms the existing CMLE methods on synthetic and real-world datasets."
2978,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper studies the problem of multi-task learning in the multi-objective optimization (MOO) setting. The authors propose a new objective called Conflict-Averse Gradient Descent (CAGrad) that aims to reduce the variance of the average loss function of the task gradients. CAGrad is motivated by the fact that the gradients of different tasks may have conflicting gradients, which can lead to poor convergence. To address this issue, the authors propose two heuristics: (1) conflict-averse gradient descent and (2) multiple gradient descent (MGDA). The authors provide convergence guarantees for both algorithms and show that they converge to a Pareto-stationary point."
2979,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper studies the problem of multi-task learning in the multi-objective optimization (MOO) setting. The authors propose a new objective called Conflict-Averse Gradient Descent (CAGrad) that aims to reduce the variance of the average loss function of the task gradients. CAGrad is motivated by the fact that the gradients of different tasks may have conflicting gradients, which can lead to poor convergence. To address this issue, the authors propose two heuristics: (1) conflict-averse gradient descent and (2) multiple gradient descent (MGDA). The authors provide convergence guarantees for both algorithms and show that they converge to a Pareto-stationary point."
2980,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper studies the problem of multi-task learning in the multi-objective optimization (MOO) setting. The authors propose a new objective called Conflict-Averse Gradient Descent (CAGrad) that aims to reduce the variance of the average loss function of the task gradients. CAGrad is motivated by the fact that the gradients of different tasks may have conflicting gradients, which can lead to poor convergence. To address this issue, the authors propose two heuristics: (1) conflict-averse gradient descent and (2) multiple gradient descent (MGDA). The authors provide convergence guarantees for both algorithms and show that they converge to a Pareto-stationary point."
2981,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper studies the problem of multi-task learning in the multi-objective optimization (MOO) setting. The authors propose a new objective called Conflict-Averse Gradient Descent (CAGrad) that aims to reduce the variance of the average loss function of the task gradients. CAGrad is motivated by the fact that the gradients of different tasks may have conflicting gradients, which can lead to poor convergence. To address this issue, the authors propose two heuristics: (1) conflict-averse gradient descent and (2) multiple gradient descent (MGDA). The authors provide convergence guarantees for both algorithms and show that they converge to a Pareto-stationary point."
2982,SP:c5a59c8d6db0f5491721aaaef182609c360930d3,"This paper studies the problem of multi-task learning in the multi-objective optimization (MOO) setting. The authors propose a new objective called Conflict-Averse Gradient Descent (CAGrad) that aims to reduce the variance of the average loss function of the task gradients. CAGrad is motivated by the fact that the gradients of different tasks may have conflicting gradients, which can lead to poor convergence. To address this issue, the authors propose two heuristics: (1) conflict-averse gradient descent and (2) multiple gradient descent (MGDA). The authors provide convergence guarantees for both algorithms and show that they converge to a Pareto-stationary point."
2983,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of algorithmic concepts. The authors propose to use Occam’s razor to measure the complexity of a given concept in terms of the number of steps needed to learn it. They show that the complexity can be bounded by a linear combination of two factors: (1) the amount of training data required to learn the concept, and (2) the level of complexity of the learned concept. They then propose a simple algorithm for learning the concept and show that it can be learned using a simple GPT-based model. They also show that their algorithm can be used to train a program induction system and a human."
2984,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of algorithmic concepts. The authors propose to use Occam’s razor to measure the complexity of a given concept in terms of the number of steps needed to learn it. They show that the complexity can be bounded by a linear combination of two factors: (1) the amount of training data required to learn the concept, and (2) the level of complexity of the learned concept. They then propose a simple algorithm for learning the concept and show that it can be learned using a simple GPT-based model. They also show that their algorithm can be used to train a program induction system and a human."
2985,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of algorithmic concepts. The authors propose to use Occam’s razor to measure the complexity of a given concept in terms of the number of steps needed to learn it. They show that the complexity can be bounded by a linear combination of two factors: (1) the amount of training data required to learn the concept, and (2) the level of complexity of the learned concept. They then propose a simple algorithm for learning the concept and show that it can be learned using a simple GPT-based model. They also show that their algorithm can be used to train a program induction system and a human."
2986,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of algorithmic concepts. The authors propose to use Occam’s razor to measure the complexity of a given concept in terms of the number of steps needed to learn it. They show that the complexity can be bounded by a linear combination of two factors: (1) the amount of training data required to learn the concept, and (2) the level of complexity of the learned concept. They then propose a simple algorithm for learning the concept and show that it can be learned using a simple GPT-based model. They also show that their algorithm can be used to train a program induction system and a human."
2987,SP:000cbfda2e26fdcfee50a628799a73b6886cfccc,"This paper studies the problem of few-shot learning in the context of algorithmic concepts. The authors propose to use Occam’s razor to measure the complexity of a given concept in terms of the number of steps needed to learn it. They show that the complexity can be bounded by a linear combination of two factors: (1) the amount of training data required to learn the concept, and (2) the level of complexity of the learned concept. They then propose a simple algorithm for learning the concept and show that it can be learned using a simple GPT-based model. They also show that their algorithm can be used to train a program induction system and a human."
2988,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes an information bottleneck to improve the robustness of a model against adversarial examples. The proposed method is based on the idea of Information Bottleneck (IB), which is a technique to extract features from a feature space that are robust to adversarial perturbations. In particular, the authors show that the information bottleneck can be used to extract robust and non-robust features from the feature space. The authors also propose a novel attack mechanism to attack the gradient of features that are not robust."
2989,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes an information bottleneck to improve the robustness of a model against adversarial examples. The proposed method is based on the idea of Information Bottleneck (IB), which is a technique to extract features from a feature space that are robust to adversarial perturbations. In particular, the authors show that the information bottleneck can be used to extract robust and non-robust features from the feature space. The authors also propose a novel attack mechanism to attack the gradient of features that are not robust."
2990,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes an information bottleneck to improve the robustness of a model against adversarial examples. The proposed method is based on the idea of Information Bottleneck (IB), which is a technique to extract features from a feature space that are robust to adversarial perturbations. In particular, the authors show that the information bottleneck can be used to extract robust and non-robust features from the feature space. The authors also propose a novel attack mechanism to attack the gradient of features that are not robust."
2991,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes an information bottleneck to improve the robustness of a model against adversarial examples. The proposed method is based on the idea of Information Bottleneck (IB), which is a technique to extract features from a feature space that are robust to adversarial perturbations. In particular, the authors show that the information bottleneck can be used to extract robust and non-robust features from the feature space. The authors also propose a novel attack mechanism to attack the gradient of features that are not robust."
2992,SP:ba01895bf1aa07a0630b8c41fc0e91effb34b4cf,"This paper proposes an information bottleneck to improve the robustness of a model against adversarial examples. The proposed method is based on the idea of Information Bottleneck (IB), which is a technique to extract features from a feature space that are robust to adversarial perturbations. In particular, the authors show that the information bottleneck can be used to extract robust and non-robust features from the feature space. The authors also propose a novel attack mechanism to attack the gradient of features that are not robust."
2993,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,This paper studies the support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) problems. The authors show that the support vectors of the SVM and MORL can be expressed as a function of the dimension of the input space. They also provide an upper bound on the phase transition of support vectors in the lp case.
2994,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,This paper studies the support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) problems. The authors show that the support vectors of the SVM and MORL can be expressed as a function of the dimension of the input space. They also provide an upper bound on the phase transition of support vectors in the lp case.
2995,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,This paper studies the support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) problems. The authors show that the support vectors of the SVM and MORL can be expressed as a function of the dimension of the input space. They also provide an upper bound on the phase transition of support vectors in the lp case.
2996,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,This paper studies the support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) problems. The authors show that the support vectors of the SVM and MORL can be expressed as a function of the dimension of the input space. They also provide an upper bound on the phase transition of support vectors in the lp case.
2997,SP:ed67b2664359799a11cebb9eaba6da74ff1dd977,This paper studies the support vector machine (SVM) and minimum Euclidean norm least squares regression (MORL) problems. The authors show that the support vectors of the SVM and MORL can be expressed as a function of the dimension of the input space. They also provide an upper bound on the phase transition of support vectors in the lp case.
2998,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the problem of active pure exploration in Markov Decision Processes (MDPs). The main contribution of this paper is to derive a lower bound on the sample complexity of the algorithm for MDPs with non-homogeneous Markov chains. The lower bound is based on the ergodicity assumption of the Markov chain, which is proved to hold in the online setting."
2999,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the problem of active pure exploration in Markov Decision Processes (MDPs). The main contribution of this paper is to derive a lower bound on the sample complexity of the algorithm for MDPs with non-homogeneous Markov chains. The lower bound is based on the ergodicity assumption of the Markov chain, which is proved to hold in the online setting."
3000,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the problem of active pure exploration in Markov Decision Processes (MDPs). The main contribution of this paper is to derive a lower bound on the sample complexity of the algorithm for MDPs with non-homogeneous Markov chains. The lower bound is based on the ergodicity assumption of the Markov chain, which is proved to hold in the online setting."
3001,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the problem of active pure exploration in Markov Decision Processes (MDPs). The main contribution of this paper is to derive a lower bound on the sample complexity of the algorithm for MDPs with non-homogeneous Markov chains. The lower bound is based on the ergodicity assumption of the Markov chain, which is proved to hold in the online setting."
3002,SP:99f226a63902863c429cb7baefab09626d13921e,"This paper studies the problem of active pure exploration in Markov Decision Processes (MDPs). The main contribution of this paper is to derive a lower bound on the sample complexity of the algorithm for MDPs with non-homogeneous Markov chains. The lower bound is based on the ergodicity assumption of the Markov chain, which is proved to hold in the online setting."
3003,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a geometry-based query embedding model for first-order logical (FOL) queries in low-dimensional spaces. The proposed model is based on the Cartesian products of two-dimensional cones, which can be used to represent conjunction, disjunction, and negation operations in the embedding space. Experiments on several benchmark datasets show that the proposed model outperforms state-of-the-art methods."
3004,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a geometry-based query embedding model for first-order logical (FOL) queries in low-dimensional spaces. The proposed model is based on the Cartesian products of two-dimensional cones, which can be used to represent conjunction, disjunction, and negation operations in the embedding space. Experiments on several benchmark datasets show that the proposed model outperforms state-of-the-art methods."
3005,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a geometry-based query embedding model for first-order logical (FOL) queries in low-dimensional spaces. The proposed model is based on the Cartesian products of two-dimensional cones, which can be used to represent conjunction, disjunction, and negation operations in the embedding space. Experiments on several benchmark datasets show that the proposed model outperforms state-of-the-art methods."
3006,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a geometry-based query embedding model for first-order logical (FOL) queries in low-dimensional spaces. The proposed model is based on the Cartesian products of two-dimensional cones, which can be used to represent conjunction, disjunction, and negation operations in the embedding space. Experiments on several benchmark datasets show that the proposed model outperforms state-of-the-art methods."
3007,SP:de4a0f5a464aa3311445cc25c4915cf0c4d975c3,"This paper proposes a geometry-based query embedding model for first-order logical (FOL) queries in low-dimensional spaces. The proposed model is based on the Cartesian products of two-dimensional cones, which can be used to represent conjunction, disjunction, and negation operations in the embedding space. Experiments on several benchmark datasets show that the proposed model outperforms state-of-the-art methods."
3008,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,This paper studies the value iteration (VI) algorithm for stochastic nonlinear systems. The authors propose a linear-time Legendre transform (LTR) based algorithm for value iteration in the conjugate domain. The main contribution of this paper is to provide convergence guarantees for the proposed algorithm. The paper also provides a theoretical analysis of the convergence of the proposed method.
3009,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,This paper studies the value iteration (VI) algorithm for stochastic nonlinear systems. The authors propose a linear-time Legendre transform (LTR) based algorithm for value iteration in the conjugate domain. The main contribution of this paper is to provide convergence guarantees for the proposed algorithm. The paper also provides a theoretical analysis of the convergence of the proposed method.
3010,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,This paper studies the value iteration (VI) algorithm for stochastic nonlinear systems. The authors propose a linear-time Legendre transform (LTR) based algorithm for value iteration in the conjugate domain. The main contribution of this paper is to provide convergence guarantees for the proposed algorithm. The paper also provides a theoretical analysis of the convergence of the proposed method.
3011,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,This paper studies the value iteration (VI) algorithm for stochastic nonlinear systems. The authors propose a linear-time Legendre transform (LTR) based algorithm for value iteration in the conjugate domain. The main contribution of this paper is to provide convergence guarantees for the proposed algorithm. The paper also provides a theoretical analysis of the convergence of the proposed method.
3012,SP:773b5b6d31e6899da395933eb7f9e25a6e50c406,This paper studies the value iteration (VI) algorithm for stochastic nonlinear systems. The authors propose a linear-time Legendre transform (LTR) based algorithm for value iteration in the conjugate domain. The main contribution of this paper is to provide convergence guarantees for the proposed algorithm. The paper also provides a theoretical analysis of the convergence of the proposed method.
3013,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes VideoAudio-Text Transformer (VATT) which is a multi-modal Transformer architecture for audio and video. The proposed method is based on the idea of multimodal contrastive loss, which aims to improve the performance of the Transformer. The authors show that the proposed method outperforms the state-of-the-art vision Transformer on Kinetics-400, ImageNet, and AudioSet. The paper also shows that VATT can be used for waveform-based audio event recognition and video action recognition."
3014,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes VideoAudio-Text Transformer (VATT) which is a multi-modal Transformer architecture for audio and video. The proposed method is based on the idea of multimodal contrastive loss, which aims to improve the performance of the Transformer. The authors show that the proposed method outperforms the state-of-the-art vision Transformer on Kinetics-400, ImageNet, and AudioSet. The paper also shows that VATT can be used for waveform-based audio event recognition and video action recognition."
3015,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes VideoAudio-Text Transformer (VATT) which is a multi-modal Transformer architecture for audio and video. The proposed method is based on the idea of multimodal contrastive loss, which aims to improve the performance of the Transformer. The authors show that the proposed method outperforms the state-of-the-art vision Transformer on Kinetics-400, ImageNet, and AudioSet. The paper also shows that VATT can be used for waveform-based audio event recognition and video action recognition."
3016,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes VideoAudio-Text Transformer (VATT) which is a multi-modal Transformer architecture for audio and video. The proposed method is based on the idea of multimodal contrastive loss, which aims to improve the performance of the Transformer. The authors show that the proposed method outperforms the state-of-the-art vision Transformer on Kinetics-400, ImageNet, and AudioSet. The paper also shows that VATT can be used for waveform-based audio event recognition and video action recognition."
3017,SP:7cd593ccba4830f3383a92ef6266224cc7699706,"This paper proposes VideoAudio-Text Transformer (VATT) which is a multi-modal Transformer architecture for audio and video. The proposed method is based on the idea of multimodal contrastive loss, which aims to improve the performance of the Transformer. The authors show that the proposed method outperforms the state-of-the-art vision Transformer on Kinetics-400, ImageNet, and AudioSet. The paper also shows that VATT can be used for waveform-based audio event recognition and video action recognition."
3018,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"This paper proposes a new self-attention mechanism that can be applied to non-positive semidefinite matrices. The proposed method is based on the Nyström method, which can be used to compute a non-negative matrix with a softmax structure and a Gaussian kernel. The authors show that the proposed method can reduce the computational cost by a factor of $O(1/\epsilon^2)$, which is comparable to full self attention. They also show that their method can be combined with the Skyformer method to reduce the computation cost by an order of magnitude. The experimental results on the Long Range Arena benchmark demonstrate the effectiveness of their method."
3019,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"This paper proposes a new self-attention mechanism that can be applied to non-positive semidefinite matrices. The proposed method is based on the Nyström method, which can be used to compute a non-negative matrix with a softmax structure and a Gaussian kernel. The authors show that the proposed method can reduce the computational cost by a factor of $O(1/\epsilon^2)$, which is comparable to full self attention. They also show that their method can be combined with the Skyformer method to reduce the computation cost by an order of magnitude. The experimental results on the Long Range Arena benchmark demonstrate the effectiveness of their method."
3020,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"This paper proposes a new self-attention mechanism that can be applied to non-positive semidefinite matrices. The proposed method is based on the Nyström method, which can be used to compute a non-negative matrix with a softmax structure and a Gaussian kernel. The authors show that the proposed method can reduce the computational cost by a factor of $O(1/\epsilon^2)$, which is comparable to full self attention. They also show that their method can be combined with the Skyformer method to reduce the computation cost by an order of magnitude. The experimental results on the Long Range Arena benchmark demonstrate the effectiveness of their method."
3021,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"This paper proposes a new self-attention mechanism that can be applied to non-positive semidefinite matrices. The proposed method is based on the Nyström method, which can be used to compute a non-negative matrix with a softmax structure and a Gaussian kernel. The authors show that the proposed method can reduce the computational cost by a factor of $O(1/\epsilon^2)$, which is comparable to full self attention. They also show that their method can be combined with the Skyformer method to reduce the computation cost by an order of magnitude. The experimental results on the Long Range Arena benchmark demonstrate the effectiveness of their method."
3022,SP:97f533426dce73d27768dd7afc2ddf035cf21e61,"This paper proposes a new self-attention mechanism that can be applied to non-positive semidefinite matrices. The proposed method is based on the Nyström method, which can be used to compute a non-negative matrix with a softmax structure and a Gaussian kernel. The authors show that the proposed method can reduce the computational cost by a factor of $O(1/\epsilon^2)$, which is comparable to full self attention. They also show that their method can be combined with the Skyformer method to reduce the computation cost by an order of magnitude. The experimental results on the Long Range Arena benchmark demonstrate the effectiveness of their method."
3023,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,This paper proposes an augmented policy cloning (APC) approach for policy cloning in the policy 3 cloning setting. The main idea is to use an image-based data augmentation technique to improve the feedback-sensitive sensitivity of the policy to perturbations of the expert trajectories. The paper also proposes a data-augmentation technique for the policy cloning problem. The proposed method is evaluated on a variety of policy cloning tasks and compared to the state-of-the-art methods.
3024,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,This paper proposes an augmented policy cloning (APC) approach for policy cloning in the policy 3 cloning setting. The main idea is to use an image-based data augmentation technique to improve the feedback-sensitive sensitivity of the policy to perturbations of the expert trajectories. The paper also proposes a data-augmentation technique for the policy cloning problem. The proposed method is evaluated on a variety of policy cloning tasks and compared to the state-of-the-art methods.
3025,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,This paper proposes an augmented policy cloning (APC) approach for policy cloning in the policy 3 cloning setting. The main idea is to use an image-based data augmentation technique to improve the feedback-sensitive sensitivity of the policy to perturbations of the expert trajectories. The paper also proposes a data-augmentation technique for the policy cloning problem. The proposed method is evaluated on a variety of policy cloning tasks and compared to the state-of-the-art methods.
3026,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,This paper proposes an augmented policy cloning (APC) approach for policy cloning in the policy 3 cloning setting. The main idea is to use an image-based data augmentation technique to improve the feedback-sensitive sensitivity of the policy to perturbations of the expert trajectories. The paper also proposes a data-augmentation technique for the policy cloning problem. The proposed method is evaluated on a variety of policy cloning tasks and compared to the state-of-the-art methods.
3027,SP:a6f1094a4c9f38df38c9710b9dcd6299f430fae2,This paper proposes an augmented policy cloning (APC) approach for policy cloning in the policy 3 cloning setting. The main idea is to use an image-based data augmentation technique to improve the feedback-sensitive sensitivity of the policy to perturbations of the expert trajectories. The paper also proposes a data-augmentation technique for the policy cloning problem. The proposed method is evaluated on a variety of policy cloning tasks and compared to the state-of-the-art methods.
3028,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,This paper proposes a method for training deep neural networks in computer vision settings. The method is based on the idea that the network should be able to learn to discriminate between images that are close to each other and those that are far away from each other. The proposed method is evaluated on a variety of computer vision benchmarks and a simulated robotics environment. The results show that the proposed method outperforms existing methods.
3029,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,This paper proposes a method for training deep neural networks in computer vision settings. The method is based on the idea that the network should be able to learn to discriminate between images that are close to each other and those that are far away from each other. The proposed method is evaluated on a variety of computer vision benchmarks and a simulated robotics environment. The results show that the proposed method outperforms existing methods.
3030,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,This paper proposes a method for training deep neural networks in computer vision settings. The method is based on the idea that the network should be able to learn to discriminate between images that are close to each other and those that are far away from each other. The proposed method is evaluated on a variety of computer vision benchmarks and a simulated robotics environment. The results show that the proposed method outperforms existing methods.
3031,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,This paper proposes a method for training deep neural networks in computer vision settings. The method is based on the idea that the network should be able to learn to discriminate between images that are close to each other and those that are far away from each other. The proposed method is evaluated on a variety of computer vision benchmarks and a simulated robotics environment. The results show that the proposed method outperforms existing methods.
3032,SP:3660d1d4a8e8f281880781ba32df7b678b705f9c,This paper proposes a method for training deep neural networks in computer vision settings. The method is based on the idea that the network should be able to learn to discriminate between images that are close to each other and those that are far away from each other. The proposed method is evaluated on a variety of computer vision benchmarks and a simulated robotics environment. The results show that the proposed method outperforms existing methods.
3033,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,This paper proposes a data augmentation method for off-policy reinforcement learning (RL). The proposed method is based on the observation that the data generated by convolutional neural networks (convNets) can lead to high variance Q-targets. The authors propose to augment the convNets with the data from the ViT model. The experiments on the DeepMind Control Suite show that the proposed method outperforms the state-of-the-art methods.
3034,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,This paper proposes a data augmentation method for off-policy reinforcement learning (RL). The proposed method is based on the observation that the data generated by convolutional neural networks (convNets) can lead to high variance Q-targets. The authors propose to augment the convNets with the data from the ViT model. The experiments on the DeepMind Control Suite show that the proposed method outperforms the state-of-the-art methods.
3035,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,This paper proposes a data augmentation method for off-policy reinforcement learning (RL). The proposed method is based on the observation that the data generated by convolutional neural networks (convNets) can lead to high variance Q-targets. The authors propose to augment the convNets with the data from the ViT model. The experiments on the DeepMind Control Suite show that the proposed method outperforms the state-of-the-art methods.
3036,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,This paper proposes a data augmentation method for off-policy reinforcement learning (RL). The proposed method is based on the observation that the data generated by convolutional neural networks (convNets) can lead to high variance Q-targets. The authors propose to augment the convNets with the data from the ViT model. The experiments on the DeepMind Control Suite show that the proposed method outperforms the state-of-the-art methods.
3037,SP:4c12852373f5f113bd47dce3e2434c5e7d61a202,This paper proposes a data augmentation method for off-policy reinforcement learning (RL). The proposed method is based on the observation that the data generated by convolutional neural networks (convNets) can lead to high variance Q-targets. The authors propose to augment the convNets with the data from the ViT model. The experiments on the DeepMind Control Suite show that the proposed method outperforms the state-of-the-art methods.
3038,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the problem of federated learning (FL) in the non-convex case. The authors propose a stochastic algorithm for FL, which is based on the momentum estimator of the WN’s and the server's update directions. The main contribution of this paper is to provide a theoretical analysis of the trade-off between sample complexity and communication complexity of FL algorithms. The paper also provides an empirical evaluation of the proposed algorithm."
3039,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the problem of federated learning (FL) in the non-convex case. The authors propose a stochastic algorithm for FL, which is based on the momentum estimator of the WN’s and the server's update directions. The main contribution of this paper is to provide a theoretical analysis of the trade-off between sample complexity and communication complexity of FL algorithms. The paper also provides an empirical evaluation of the proposed algorithm."
3040,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the problem of federated learning (FL) in the non-convex case. The authors propose a stochastic algorithm for FL, which is based on the momentum estimator of the WN’s and the server's update directions. The main contribution of this paper is to provide a theoretical analysis of the trade-off between sample complexity and communication complexity of FL algorithms. The paper also provides an empirical evaluation of the proposed algorithm."
3041,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the problem of federated learning (FL) in the non-convex case. The authors propose a stochastic algorithm for FL, which is based on the momentum estimator of the WN’s and the server's update directions. The main contribution of this paper is to provide a theoretical analysis of the trade-off between sample complexity and communication complexity of FL algorithms. The paper also provides an empirical evaluation of the proposed algorithm."
3042,SP:f8ca9d92c45adc4512381035856b445029e3080a,"This paper studies the problem of federated learning (FL) in the non-convex case. The authors propose a stochastic algorithm for FL, which is based on the momentum estimator of the WN’s and the server's update directions. The main contribution of this paper is to provide a theoretical analysis of the trade-off between sample complexity and communication complexity of FL algorithms. The paper also provides an empirical evaluation of the proposed algorithm."
3043,SP:bd3eecb81a17af010f2d3555434990855c1810f2,This paper studies the information-theoretic generalization bound for isotropic noise in Stochastic Gradient Langevin Dynamics (SGLD). The main contribution of this paper is to provide an information-Theoretic bound on the expected gradient covariance of the optimal noise covariance for SGLD. The paper is well-written and easy to follow.
3044,SP:bd3eecb81a17af010f2d3555434990855c1810f2,This paper studies the information-theoretic generalization bound for isotropic noise in Stochastic Gradient Langevin Dynamics (SGLD). The main contribution of this paper is to provide an information-Theoretic bound on the expected gradient covariance of the optimal noise covariance for SGLD. The paper is well-written and easy to follow.
3045,SP:bd3eecb81a17af010f2d3555434990855c1810f2,This paper studies the information-theoretic generalization bound for isotropic noise in Stochastic Gradient Langevin Dynamics (SGLD). The main contribution of this paper is to provide an information-Theoretic bound on the expected gradient covariance of the optimal noise covariance for SGLD. The paper is well-written and easy to follow.
3046,SP:bd3eecb81a17af010f2d3555434990855c1810f2,This paper studies the information-theoretic generalization bound for isotropic noise in Stochastic Gradient Langevin Dynamics (SGLD). The main contribution of this paper is to provide an information-Theoretic bound on the expected gradient covariance of the optimal noise covariance for SGLD. The paper is well-written and easy to follow.
3047,SP:bd3eecb81a17af010f2d3555434990855c1810f2,This paper studies the information-theoretic generalization bound for isotropic noise in Stochastic Gradient Langevin Dynamics (SGLD). The main contribution of this paper is to provide an information-Theoretic bound on the expected gradient covariance of the optimal noise covariance for SGLD. The paper is well-written and easy to follow.
3048,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,This paper proposes a new video compression method based on voxel flows. The proposed method is based on a 3D motion vector field and a motion compensation module. The authors propose a unified polynomial function for motion trajectories and a flow prediction module for multiple-reference-frame predic12 tion. The motion compensation is a weighted 9-trilinear warping and the flow prediction is a multi-reference trajectory prediction module. Experiments show that the proposed method can achieve state-of-the-art performance on MS-SSIM.
3049,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,This paper proposes a new video compression method based on voxel flows. The proposed method is based on a 3D motion vector field and a motion compensation module. The authors propose a unified polynomial function for motion trajectories and a flow prediction module for multiple-reference-frame predic12 tion. The motion compensation is a weighted 9-trilinear warping and the flow prediction is a multi-reference trajectory prediction module. Experiments show that the proposed method can achieve state-of-the-art performance on MS-SSIM.
3050,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,This paper proposes a new video compression method based on voxel flows. The proposed method is based on a 3D motion vector field and a motion compensation module. The authors propose a unified polynomial function for motion trajectories and a flow prediction module for multiple-reference-frame predic12 tion. The motion compensation is a weighted 9-trilinear warping and the flow prediction is a multi-reference trajectory prediction module. Experiments show that the proposed method can achieve state-of-the-art performance on MS-SSIM.
3051,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,This paper proposes a new video compression method based on voxel flows. The proposed method is based on a 3D motion vector field and a motion compensation module. The authors propose a unified polynomial function for motion trajectories and a flow prediction module for multiple-reference-frame predic12 tion. The motion compensation is a weighted 9-trilinear warping and the flow prediction is a multi-reference trajectory prediction module. Experiments show that the proposed method can achieve state-of-the-art performance on MS-SSIM.
3052,SP:19fbd1a381598538662417a4a1885ba4ac04f5f8,This paper proposes a new video compression method based on voxel flows. The proposed method is based on a 3D motion vector field and a motion compensation module. The authors propose a unified polynomial function for motion trajectories and a flow prediction module for multiple-reference-frame predic12 tion. The motion compensation is a weighted 9-trilinear warping and the flow prediction is a multi-reference trajectory prediction module. Experiments show that the proposed method can achieve state-of-the-art performance on MS-SSIM.
3053,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the regret of Online Mirror 1 Descent (OMD) and Exponentiated 7 Gradient (exponential 7 gradients) in the context of online mirror 1 descent (MT-OMD). In particular, the authors show that under certain assumptions on the geometry of the target task, the regret can be bounded by a factor of \sqrt{O}(1/\epsilon^2) where $\eps(1)$ is the distance between the target and the original target task. The authors also show that the regret is bounded by an upper bound on the number of iterations needed to converge to the optimal solution. "
3054,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the regret of Online Mirror 1 Descent (OMD) and Exponentiated 7 Gradient (exponential 7 gradients) in the context of online mirror 1 descent (MT-OMD). In particular, the authors show that under certain assumptions on the geometry of the target task, the regret can be bounded by a factor of \sqrt{O}(1/\epsilon^2) where $\eps(1)$ is the distance between the target and the original target task. The authors also show that the regret is bounded by an upper bound on the number of iterations needed to converge to the optimal solution. "
3055,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the regret of Online Mirror 1 Descent (OMD) and Exponentiated 7 Gradient (exponential 7 gradients) in the context of online mirror 1 descent (MT-OMD). In particular, the authors show that under certain assumptions on the geometry of the target task, the regret can be bounded by a factor of \sqrt{O}(1/\epsilon^2) where $\eps(1)$ is the distance between the target and the original target task. The authors also show that the regret is bounded by an upper bound on the number of iterations needed to converge to the optimal solution. "
3056,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the regret of Online Mirror 1 Descent (OMD) and Exponentiated 7 Gradient (exponential 7 gradients) in the context of online mirror 1 descent (MT-OMD). In particular, the authors show that under certain assumptions on the geometry of the target task, the regret can be bounded by a factor of \sqrt{O}(1/\epsilon^2) where $\eps(1)$ is the distance between the target and the original target task. The authors also show that the regret is bounded by an upper bound on the number of iterations needed to converge to the optimal solution. "
3057,SP:ba790fdcf2deef1a1b5e1961c7c4a28dd0218420,"This paper studies the regret of Online Mirror 1 Descent (OMD) and Exponentiated 7 Gradient (exponential 7 gradients) in the context of online mirror 1 descent (MT-OMD). In particular, the authors show that under certain assumptions on the geometry of the target task, the regret can be bounded by a factor of \sqrt{O}(1/\epsilon^2) where $\eps(1)$ is the distance between the target and the original target task. The authors also show that the regret is bounded by an upper bound on the number of iterations needed to converge to the optimal solution. "
3058,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,This paper proposes a discretization method for the underdamped Langevin diffusion (ULD) problem. The main idea is to use the finite summation of N smooth components in the stronglyconvex potential of the strongly-log-concave distribution. The method is evaluated on synthetic and real-world data.
3059,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,This paper proposes a discretization method for the underdamped Langevin diffusion (ULD) problem. The main idea is to use the finite summation of N smooth components in the stronglyconvex potential of the strongly-log-concave distribution. The method is evaluated on synthetic and real-world data.
3060,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,This paper proposes a discretization method for the underdamped Langevin diffusion (ULD) problem. The main idea is to use the finite summation of N smooth components in the stronglyconvex potential of the strongly-log-concave distribution. The method is evaluated on synthetic and real-world data.
3061,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,This paper proposes a discretization method for the underdamped Langevin diffusion (ULD) problem. The main idea is to use the finite summation of N smooth components in the stronglyconvex potential of the strongly-log-concave distribution. The method is evaluated on synthetic and real-world data.
3062,SP:75f80e4e7836a7575e60de7f055820c6c7065fcb,This paper proposes a discretization method for the underdamped Langevin diffusion (ULD) problem. The main idea is to use the finite summation of N smooth components in the stronglyconvex potential of the strongly-log-concave distribution. The method is evaluated on synthetic and real-world data.
3063,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper studies the problem of continual learning in neural networks. The authors propose a new method for continual learning based on Bayesian weight regularization. The main idea is to use the prior precision of the gradient projection as the weight regularizer to prevent catastrophic forgetting. The proposed method is evaluated on both feedforward and recurrent networks.
3064,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper studies the problem of continual learning in neural networks. The authors propose a new method for continual learning based on Bayesian weight regularization. The main idea is to use the prior precision of the gradient projection as the weight regularizer to prevent catastrophic forgetting. The proposed method is evaluated on both feedforward and recurrent networks.
3065,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper studies the problem of continual learning in neural networks. The authors propose a new method for continual learning based on Bayesian weight regularization. The main idea is to use the prior precision of the gradient projection as the weight regularizer to prevent catastrophic forgetting. The proposed method is evaluated on both feedforward and recurrent networks.
3066,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper studies the problem of continual learning in neural networks. The authors propose a new method for continual learning based on Bayesian weight regularization. The main idea is to use the prior precision of the gradient projection as the weight regularizer to prevent catastrophic forgetting. The proposed method is evaluated on both feedforward and recurrent networks.
3067,SP:22822f378c3fbc15b77eb736194b1ce7f0585072,This paper studies the problem of continual learning in neural networks. The authors propose a new method for continual learning based on Bayesian weight regularization. The main idea is to use the prior precision of the gradient projection as the weight regularizer to prevent catastrophic forgetting. The proposed method is evaluated on both feedforward and recurrent networks.
3068,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper studies the problem of out-of-distribution detection in Normalizing Flows (NFs). The authors propose a new term called volume-change term that is added to the normalizing flow term to encourage the network to be invertible. The authors show that this term is necessary for the network's generalization ability to low-dimensional manifolds and high-dimensional ambient spaces. They also provide a theoretical analysis of the effect of the volume change term on the gradient of the network. Finally, the authors provide an empirical study of the performance of the proposed method on MNIST and CIFAR-10."
3069,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper studies the problem of out-of-distribution detection in Normalizing Flows (NFs). The authors propose a new term called volume-change term that is added to the normalizing flow term to encourage the network to be invertible. The authors show that this term is necessary for the network's generalization ability to low-dimensional manifolds and high-dimensional ambient spaces. They also provide a theoretical analysis of the effect of the volume change term on the gradient of the network. Finally, the authors provide an empirical study of the performance of the proposed method on MNIST and CIFAR-10."
3070,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper studies the problem of out-of-distribution detection in Normalizing Flows (NFs). The authors propose a new term called volume-change term that is added to the normalizing flow term to encourage the network to be invertible. The authors show that this term is necessary for the network's generalization ability to low-dimensional manifolds and high-dimensional ambient spaces. They also provide a theoretical analysis of the effect of the volume change term on the gradient of the network. Finally, the authors provide an empirical study of the performance of the proposed method on MNIST and CIFAR-10."
3071,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper studies the problem of out-of-distribution detection in Normalizing Flows (NFs). The authors propose a new term called volume-change term that is added to the normalizing flow term to encourage the network to be invertible. The authors show that this term is necessary for the network's generalization ability to low-dimensional manifolds and high-dimensional ambient spaces. They also provide a theoretical analysis of the effect of the volume change term on the gradient of the network. Finally, the authors provide an empirical study of the performance of the proposed method on MNIST and CIFAR-10."
3072,SP:26de056be14962312c759be5d284ef235d660f9c,"This paper studies the problem of out-of-distribution detection in Normalizing Flows (NFs). The authors propose a new term called volume-change term that is added to the normalizing flow term to encourage the network to be invertible. The authors show that this term is necessary for the network's generalization ability to low-dimensional manifolds and high-dimensional ambient spaces. They also provide a theoretical analysis of the effect of the volume change term on the gradient of the network. Finally, the authors provide an empirical study of the performance of the proposed method on MNIST and CIFAR-10."
3073,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method to model the dynamics of deep neural networks in the presence of delayed signals. The authors propose to use the Latent Equilibrium (LE) principle, which is a well-known method for modeling physical dynamical systems, to model biological neurons. The main contribution of this paper is to propose a method for learning a model of biological neurons that can be used for inference and inference. The proposed method is based on an energy function that is modeled as a disentangled neuron and synapse dynamics. Theoretical results are provided to show that the proposed method can be applied to a variety of architectures, including fully-connected and convolutional architectures. Experiments are conducted on several datasets to demonstrate the effectiveness of the proposed model."
3074,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method to model the dynamics of deep neural networks in the presence of delayed signals. The authors propose to use the Latent Equilibrium (LE) principle, which is a well-known method for modeling physical dynamical systems, to model biological neurons. The main contribution of this paper is to propose a method for learning a model of biological neurons that can be used for inference and inference. The proposed method is based on an energy function that is modeled as a disentangled neuron and synapse dynamics. Theoretical results are provided to show that the proposed method can be applied to a variety of architectures, including fully-connected and convolutional architectures. Experiments are conducted on several datasets to demonstrate the effectiveness of the proposed model."
3075,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method to model the dynamics of deep neural networks in the presence of delayed signals. The authors propose to use the Latent Equilibrium (LE) principle, which is a well-known method for modeling physical dynamical systems, to model biological neurons. The main contribution of this paper is to propose a method for learning a model of biological neurons that can be used for inference and inference. The proposed method is based on an energy function that is modeled as a disentangled neuron and synapse dynamics. Theoretical results are provided to show that the proposed method can be applied to a variety of architectures, including fully-connected and convolutional architectures. Experiments are conducted on several datasets to demonstrate the effectiveness of the proposed model."
3076,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method to model the dynamics of deep neural networks in the presence of delayed signals. The authors propose to use the Latent Equilibrium (LE) principle, which is a well-known method for modeling physical dynamical systems, to model biological neurons. The main contribution of this paper is to propose a method for learning a model of biological neurons that can be used for inference and inference. The proposed method is based on an energy function that is modeled as a disentangled neuron and synapse dynamics. Theoretical results are provided to show that the proposed method can be applied to a variety of architectures, including fully-connected and convolutional architectures. Experiments are conducted on several datasets to demonstrate the effectiveness of the proposed model."
3077,SP:395dae632dab83f3f61bdf67eabe4d351492798c,"This paper proposes a method to model the dynamics of deep neural networks in the presence of delayed signals. The authors propose to use the Latent Equilibrium (LE) principle, which is a well-known method for modeling physical dynamical systems, to model biological neurons. The main contribution of this paper is to propose a method for learning a model of biological neurons that can be used for inference and inference. The proposed method is based on an energy function that is modeled as a disentangled neuron and synapse dynamics. Theoretical results are provided to show that the proposed method can be applied to a variety of architectures, including fully-connected and convolutional architectures. Experiments are conducted on several datasets to demonstrate the effectiveness of the proposed model."
3078,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph neural network (GNN) based on the idea of Nested Graph Neural Networks (NGNN). The main idea is to learn a subgraph representation for each node in a graph, which is then used to train a GNN for the whole graph. The main difference between GNN and 1-WL is that GNN learns a representation for the entire graph, while 1-wL learns the representation for a specific subgraph. The authors show that the proposed method is able to achieve better performance than GNN on a variety of graph classification tasks."
3079,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph neural network (GNN) based on the idea of Nested Graph Neural Networks (NGNN). The main idea is to learn a subgraph representation for each node in a graph, which is then used to train a GNN for the whole graph. The main difference between GNN and 1-WL is that GNN learns a representation for the entire graph, while 1-wL learns the representation for a specific subgraph. The authors show that the proposed method is able to achieve better performance than GNN on a variety of graph classification tasks."
3080,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph neural network (GNN) based on the idea of Nested Graph Neural Networks (NGNN). The main idea is to learn a subgraph representation for each node in a graph, which is then used to train a GNN for the whole graph. The main difference between GNN and 1-WL is that GNN learns a representation for the entire graph, while 1-wL learns the representation for a specific subgraph. The authors show that the proposed method is able to achieve better performance than GNN on a variety of graph classification tasks."
3081,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph neural network (GNN) based on the idea of Nested Graph Neural Networks (NGNN). The main idea is to learn a subgraph representation for each node in a graph, which is then used to train a GNN for the whole graph. The main difference between GNN and 1-WL is that GNN learns a representation for the entire graph, while 1-wL learns the representation for a specific subgraph. The authors show that the proposed method is able to achieve better performance than GNN on a variety of graph classification tasks."
3082,SP:b937901e3230b14e36975fbab0658a52bdac4977,"This paper proposes a new method for graph neural network (GNN) based on the idea of Nested Graph Neural Networks (NGNN). The main idea is to learn a subgraph representation for each node in a graph, which is then used to train a GNN for the whole graph. The main difference between GNN and 1-WL is that GNN learns a representation for the entire graph, while 1-wL learns the representation for a specific subgraph. The authors show that the proposed method is able to achieve better performance than GNN on a variety of graph classification tasks."
3083,SP:7b8284aa82022ce73802bfc57238b0d82031b226,This paper studies the problem of importance sampling in nested variational inference (NVI). The authors show that nested importance samplers can be used to approximate the KL divergence between the forward and reverse KL divergence of a hidden Markov model. The authors also show that NVI can be applied to amortized inference in hierarchical deep generative models. 
3084,SP:7b8284aa82022ce73802bfc57238b0d82031b226,This paper studies the problem of importance sampling in nested variational inference (NVI). The authors show that nested importance samplers can be used to approximate the KL divergence between the forward and reverse KL divergence of a hidden Markov model. The authors also show that NVI can be applied to amortized inference in hierarchical deep generative models. 
3085,SP:7b8284aa82022ce73802bfc57238b0d82031b226,This paper studies the problem of importance sampling in nested variational inference (NVI). The authors show that nested importance samplers can be used to approximate the KL divergence between the forward and reverse KL divergence of a hidden Markov model. The authors also show that NVI can be applied to amortized inference in hierarchical deep generative models. 
3086,SP:7b8284aa82022ce73802bfc57238b0d82031b226,This paper studies the problem of importance sampling in nested variational inference (NVI). The authors show that nested importance samplers can be used to approximate the KL divergence between the forward and reverse KL divergence of a hidden Markov model. The authors also show that NVI can be applied to amortized inference in hierarchical deep generative models. 
3087,SP:7b8284aa82022ce73802bfc57238b0d82031b226,This paper studies the problem of importance sampling in nested variational inference (NVI). The authors show that nested importance samplers can be used to approximate the KL divergence between the forward and reverse KL divergence of a hidden Markov model. The authors also show that NVI can be applied to amortized inference in hierarchical deep generative models. 
3088,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the packing and integral bounds of Piyavskii-Shubert (PShuber) algorithm for the Lipschitz setting. In particular, the authors show that the packing bound can be expressed as an upper bound on the optimal sample complexity of the algorithm. The authors also provide an instance-dependent lower bound for the PShUber algorithm."
3089,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the packing and integral bounds of Piyavskii-Shubert (PShuber) algorithm for the Lipschitz setting. In particular, the authors show that the packing bound can be expressed as an upper bound on the optimal sample complexity of the algorithm. The authors also provide an instance-dependent lower bound for the PShUber algorithm."
3090,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the packing and integral bounds of Piyavskii-Shubert (PShuber) algorithm for the Lipschitz setting. In particular, the authors show that the packing bound can be expressed as an upper bound on the optimal sample complexity of the algorithm. The authors also provide an instance-dependent lower bound for the PShUber algorithm."
3091,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the packing and integral bounds of Piyavskii-Shubert (PShuber) algorithm for the Lipschitz setting. In particular, the authors show that the packing bound can be expressed as an upper bound on the optimal sample complexity of the algorithm. The authors also provide an instance-dependent lower bound for the PShUber algorithm."
3092,SP:f3792f82b28727a7a198c6eac9511391d2045a5f,"This paper studies the packing and integral bounds of Piyavskii-Shubert (PShuber) algorithm for the Lipschitz setting. In particular, the authors show that the packing bound can be expressed as an upper bound on the optimal sample complexity of the algorithm. The authors also provide an instance-dependent lower bound for the PShUber algorithm."
3093,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper studies uncertainty estimation in deep neural networks (DNNs). The authors propose a method to attack uncertainty estimation methods such as Deep Ensembles, MC-Dropout, and SelectiveNet. The authors show that these methods are vulnerable to adversarial attacks in the black-box regime. They also show that the proposed method can be applied to white-box DNNs."
3094,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper studies uncertainty estimation in deep neural networks (DNNs). The authors propose a method to attack uncertainty estimation methods such as Deep Ensembles, MC-Dropout, and SelectiveNet. The authors show that these methods are vulnerable to adversarial attacks in the black-box regime. They also show that the proposed method can be applied to white-box DNNs."
3095,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper studies uncertainty estimation in deep neural networks (DNNs). The authors propose a method to attack uncertainty estimation methods such as Deep Ensembles, MC-Dropout, and SelectiveNet. The authors show that these methods are vulnerable to adversarial attacks in the black-box regime. They also show that the proposed method can be applied to white-box DNNs."
3096,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper studies uncertainty estimation in deep neural networks (DNNs). The authors propose a method to attack uncertainty estimation methods such as Deep Ensembles, MC-Dropout, and SelectiveNet. The authors show that these methods are vulnerable to adversarial attacks in the black-box regime. They also show that the proposed method can be applied to white-box DNNs."
3097,SP:6e8134eeaf524db765a6186f3de74e936243f8d4,"This paper studies uncertainty estimation in deep neural networks (DNNs). The authors propose a method to attack uncertainty estimation methods such as Deep Ensembles, MC-Dropout, and SelectiveNet. The authors show that these methods are vulnerable to adversarial attacks in the black-box regime. They also show that the proposed method can be applied to white-box DNNs."
3098,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on streaming belief propagation (STREAMBP) approach. The main contribution of this paper is to propose a streaming StSBM based on STREAMBP. The method is evaluated on both synthetic and real data.
3099,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on streaming belief propagation (STREAMBP) approach. The main contribution of this paper is to propose a streaming StSBM based on STREAMBP. The method is evaluated on both synthetic and real data.
3100,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on streaming belief propagation (STREAMBP) approach. The main contribution of this paper is to propose a streaming StSBM based on STREAMBP. The method is evaluated on both synthetic and real data.
3101,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on streaming belief propagation (STREAMBP) approach. The main contribution of this paper is to propose a streaming StSBM based on STREAMBP. The method is evaluated on both synthetic and real data.
3102,SP:c5a5bf6e0bdebf5170c8fe3fedd2f3438e39cd21,This paper proposes a streaming stochastic block model (StSBM) for community detection. The proposed method is based on streaming belief propagation (STREAMBP) approach. The main contribution of this paper is to propose a streaming StSBM based on STREAMBP. The method is evaluated on both synthetic and real data.
3103,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the problem of regularizing linear neural networks (LNNs) in the reverse problem. The authors propose a new regularizer, l2 regularization, to reduce the representation cost of LNNs. The main contribution of this paper is to show that l2-regularization can be combined with other regularizers such as group quasi-norms, k-support-norm, elastic net, and elastic net. The paper is well-written and easy to follow. "
3104,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the problem of regularizing linear neural networks (LNNs) in the reverse problem. The authors propose a new regularizer, l2 regularization, to reduce the representation cost of LNNs. The main contribution of this paper is to show that l2-regularization can be combined with other regularizers such as group quasi-norms, k-support-norm, elastic net, and elastic net. The paper is well-written and easy to follow. "
3105,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the problem of regularizing linear neural networks (LNNs) in the reverse problem. The authors propose a new regularizer, l2 regularization, to reduce the representation cost of LNNs. The main contribution of this paper is to show that l2-regularization can be combined with other regularizers such as group quasi-norms, k-support-norm, elastic net, and elastic net. The paper is well-written and easy to follow. "
3106,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the problem of regularizing linear neural networks (LNNs) in the reverse problem. The authors propose a new regularizer, l2 regularization, to reduce the representation cost of LNNs. The main contribution of this paper is to show that l2-regularization can be combined with other regularizers such as group quasi-norms, k-support-norm, elastic net, and elastic net. The paper is well-written and easy to follow. "
3107,SP:b1163857a6b06047c3531ab762642fcbed6dd294,"This paper studies the problem of regularizing linear neural networks (LNNs) in the reverse problem. The authors propose a new regularizer, l2 regularization, to reduce the representation cost of LNNs. The main contribution of this paper is to show that l2-regularization can be combined with other regularizers such as group quasi-norms, k-support-norm, elastic net, and elastic net. The paper is well-written and easy to follow. "
3108,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper proposes a new method for knowledge graph classification. The method is based on the idea that the relation between two nodes in a knowledge graph is a binary relation, and the goal of the method is to learn a set of logical rules that can be used to represent the relation. The proposed method is evaluated on two datasets, namely NELL-995 and FB-122, and compared to the state-of-the-art methods."
3109,SP:c9c7fc5288e24a54531b7063c028d307279fe2ef,"This paper proposes a new method for knowledge graph classification. The method is based on the idea that the relation between two nodes in a knowledge graph is a binary relation, and the goal of the method is to learn a set of logical rules that can be used to represent the relation. The proposed method is evaluated on two datasets, namely NELL-995 and FB-122, and compared to the state-of-the-art methods."
3110,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,This paper proposes a transformer-based entity linking model for entity linking. The proposed model is based on the Transformer architecture and is trained on both in-domain and out-of-domain data. The model is evaluated on CoNLL and TAC-KBP datasets. The experiments show that the proposed model outperforms the baselines.
3111,SP:f63e4ed39d577b50eab4f4b6d08ef912a69840ef,This paper proposes a transformer-based entity linking model for entity linking. The proposed model is based on the Transformer architecture and is trained on both in-domain and out-of-domain data. The model is evaluated on CoNLL and TAC-KBP datasets. The experiments show that the proposed model outperforms the baselines.
3112,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper studies the problem of training deep neural networks with ensemble active learning. The authors propose a method to search for subsets of training data that can be used to improve the performance of the model. The method is based on the idea that the training data distribution of a deep network is similar to the training distribution of an ensemble of ensembles, and the goal is to find subsets that are close to the distribution of the ensemble. The proposed method is evaluated on CIFAR-10, ImageNet, and ResNet-18 datasets, and compared to other active learning methods. The results show that the proposed method outperforms the other methods."
3113,SP:eaeee88e0717cda8d6f3d8ff83ebe594eba44f29,"This paper studies the problem of training deep neural networks with ensemble active learning. The authors propose a method to search for subsets of training data that can be used to improve the performance of the model. The method is based on the idea that the training data distribution of a deep network is similar to the training distribution of an ensemble of ensembles, and the goal is to find subsets that are close to the distribution of the ensemble. The proposed method is evaluated on CIFAR-10, ImageNet, and ResNet-18 datasets, and compared to other active learning methods. The results show that the proposed method outperforms the other methods."
3114,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper proposes Capsules-Inverted-Attention-Routing (CATR), a routing algorithm for capsule networks. The proposed algorithm is based on the inverted dot-product attention (DOT) mechanism. The authors show that the proposed method outperforms existing routing algorithms on CIFAR-10 and CIFR-100 datasets."
3115,SP:4a1cce61f12c68846c507130bd055b3444ac8101,"This paper proposes Capsules-Inverted-Attention-Routing (CATR), a routing algorithm for capsule networks. The proposed algorithm is based on the inverted dot-product attention (DOT) mechanism. The authors show that the proposed method outperforms existing routing algorithms on CIFAR-10 and CIFR-100 datasets."
3116,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a parallel tempering technique for hyperparameter optimization. The proposed method is based on the observation that the temperature of the hyperparameters in the model-parameter space is correlated with the history of the model. The authors propose to use the temperature as a way to reduce the computational cost of hyperparametrization, and show that the proposed method can be applied to both dropout and learning rate optimization. "
3117,SP:99ca283c579152bc44b19c21392aeb7f6b76231b,"This paper proposes a parallel tempering technique for hyperparameter optimization. The proposed method is based on the observation that the temperature of the hyperparameters in the model-parameter space is correlated with the history of the model. The authors propose to use the temperature as a way to reduce the computational cost of hyperparametrization, and show that the proposed method can be applied to both dropout and learning rate optimization. "
3118,SP:beba754d96cc441712a5413c41e98863c8abf605,This paper studies the problem of machine translation (MT) in the context of reinforcement learning. The authors propose a novel approach to train a GAN-based model for MT. The main contribution of this paper is to propose a new method to train the GAN. The proposed method is based on a combination of two existing methods: Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). The authors show that the proposed method outperforms the state-of-the-art methods on MT.
3119,SP:beba754d96cc441712a5413c41e98863c8abf605,This paper studies the problem of machine translation (MT) in the context of reinforcement learning. The authors propose a novel approach to train a GAN-based model for MT. The main contribution of this paper is to propose a new method to train the GAN. The proposed method is based on a combination of two existing methods: Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). The authors show that the proposed method outperforms the state-of-the-art methods on MT.
3120,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"This paper studies the problem of estimating the Q-value of a policy in the presence of uncertainty. In particular, the authors consider the case where the uncertainty is bounded by a closed-form characterization of the asymptotic variances of Q-values. The authors propose a policy exploration strategy to estimate the uncertainty of the Q values. The exploration strategy is motivated by the fact that the uncertainty can be controlled by the confidence region of the policy, and the authors show that the exploration strategy can lead to better estimates of the uncertainty."
3121,SP:366b68d2490ea7569c74dc66ec0f83daa029ddd9,"This paper studies the problem of estimating the Q-value of a policy in the presence of uncertainty. In particular, the authors consider the case where the uncertainty is bounded by a closed-form characterization of the asymptotic variances of Q-values. The authors propose a policy exploration strategy to estimate the uncertainty of the Q values. The exploration strategy is motivated by the fact that the uncertainty can be controlled by the confidence region of the policy, and the authors show that the exploration strategy can lead to better estimates of the uncertainty."
3122,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,This paper proposes collaborative generated hashing (CGH) to address the cold-start and efficiency issues of Top-k recommendation systems. The authors propose a generative step to generate hash functions for the hash functions. The MDL principle is used to generate compact and informative binary codes. The proposed method is evaluated on two public datasets.
3123,SP:d922459581c3295ff315fda6e59b9f7e9147f22d,This paper proposes collaborative generated hashing (CGH) to address the cold-start and efficiency issues of Top-k recommendation systems. The authors propose a generative step to generate hash functions for the hash functions. The MDL principle is used to generate compact and informative binary codes. The proposed method is evaluated on two public datasets.
3124,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,This paper proposes a method for adversarial inductive transfer learning (AITL) in the context of precision oncology. AITL is based on adversarial domain adaptation (AD) and multi-task learning (MTL). The proposed method is evaluated on a large pre-clinical pharmacogenomics dataset and on a cancer cell line dataset.
3125,SP:c2a5551f229211c9aa4c43686b517fcde82bbccf,This paper proposes a method for adversarial inductive transfer learning (AITL) in the context of precision oncology. AITL is based on adversarial domain adaptation (AD) and multi-task learning (MTL). The proposed method is evaluated on a large pre-clinical pharmacogenomics dataset and on a cancer cell line dataset.
3126,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes an ensemble of dynamics models to improve the sample complexity of model-free policy gradient methods. The authors propose a model-based ensemble of uncertainty aware ensemble of Dynamics models (MBPGE) to estimate the uncertainty of the next state prediction. The ensemble is composed of a stochastic dynamics model and a policy gradient algorithm. The proposed ensemble is evaluated on simulated robotic locomotion tasks and compared to model-only algorithms. The results show that MBPGE outperforms the model based algorithms in terms of sample complexity and learning rates.
3127,SP:a27f975266e990b2ab4a0ab8db1588e945d0300a,This paper proposes an ensemble of dynamics models to improve the sample complexity of model-free policy gradient methods. The authors propose a model-based ensemble of uncertainty aware ensemble of Dynamics models (MBPGE) to estimate the uncertainty of the next state prediction. The ensemble is composed of a stochastic dynamics model and a policy gradient algorithm. The proposed ensemble is evaluated on simulated robotic locomotion tasks and compared to model-only algorithms. The results show that MBPGE outperforms the model based algorithms in terms of sample complexity and learning rates.
3128,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a method to detect adversarial examples in a class-conditional manner. The proposed method is based on the idea of reconstructive adversarial attacks, which are based on class-conditioned reconstructions of the input images. The method is evaluated on CIFAR-10 and ImageNet, where it is shown that the proposed method outperforms the state-of-the-art."
3129,SP:2aaddb6dda434b49487857d99c9d143e2f54d350,"This paper proposes a method to detect adversarial examples in a class-conditional manner. The proposed method is based on the idea of reconstructive adversarial attacks, which are based on class-conditioned reconstructions of the input images. The method is evaluated on CIFAR-10 and ImageNet, where it is shown that the proposed method outperforms the state-of-the-art."
3130,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"This paper studies the initialization of neural networks with neural tangent kernels (NTKs). The authors show that NTKs can be initialized with Edge of Chaos (EOC) initialization, which is a variant of the initialization used in kernel gradient descent. The authors also show that EOC initialization can be extended to NTK initialization. Finally, the authors provide a theoretical analysis of the effect of initialization on NTK depth."
3131,SP:da88bfbe3f59ce1a24522aa5e74c9472b079664a,"This paper studies the initialization of neural networks with neural tangent kernels (NTKs). The authors show that NTKs can be initialized with Edge of Chaos (EOC) initialization, which is a variant of the initialization used in kernel gradient descent. The authors also show that EOC initialization can be extended to NTK initialization. Finally, the authors provide a theoretical analysis of the effect of initialization on NTK depth."
3132,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method for learning sentence embeddings. The method is based on the idea that the embedding of a sentence can be expressed as a set of compositional words, which can be used to encode the semantic meaning of the sentence. The authors propose to use a pre-trained language model to learn the embedded word embedding, which is then used to train a neural network to predict the sentence embedding. The proposed method is evaluated on a variety of datasets and compared to a number of baselines."
3133,SP:dd59b897384c52c20d62be73fc33184c8c226f4b,"This paper proposes a self-supervised method for learning sentence embeddings. The method is based on the idea that the embedding of a sentence can be expressed as a set of compositional words, which can be used to encode the semantic meaning of the sentence. The authors propose to use a pre-trained language model to learn the embedded word embedding, which is then used to train a neural network to predict the sentence embedding. The proposed method is evaluated on a variety of datasets and compared to a number of baselines."
3134,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,This paper proposes a transfer learning method for financial sentiment classification. The main idea is to train a language model on the FinancialPhrasebank dataset and then use the language model to perform transfer learning on the new dataset. The authors show that the proposed method outperforms other transfer learning methods in terms of transfer performance.
3135,SP:980babd58fc2ea5f40bb22b3a9a09737f14f3f18,This paper proposes a transfer learning method for financial sentiment classification. The main idea is to train a language model on the FinancialPhrasebank dataset and then use the language model to perform transfer learning on the new dataset. The authors show that the proposed method outperforms other transfer learning methods in terms of transfer performance.
3136,SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a method for singing voice synthesis. The method is based on a pre-trained voice synthesis model, which is trained to generate a singing voice waveform from a set of musical scores and text lyrics. The authors show that the proposed method is able to achieve state-of-the-art results in terms of training time and inference time. The main contribution of the paper is the introduction of an adversarial network to train the model. "
3137,SP:31c9c3a693922d5c3448e80ade920391dce261f9,"This paper proposes a method for singing voice synthesis. The method is based on a pre-trained voice synthesis model, which is trained to generate a singing voice waveform from a set of musical scores and text lyrics. The authors show that the proposed method is able to achieve state-of-the-art results in terms of training time and inference time. The main contribution of the paper is the introduction of an adversarial network to train the model. "
3138,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,This paper proposes a novel adversarial defense technique for deep neural networks. The proposed method is based on the idea of latent high-order factorization of the network. The authors propose to use randomization to reduce the sparsity of the weights and perturbations in the latent subspace of the neural network. They also propose a new adversarial training method to improve the robustness of the model against adversarial attacks. The experimental results show that the proposed method outperforms the baselines on both image classification and audio classification tasks.
3139,SP:99d41c8285fd0270ff16e915ef03187a0a7005b0,This paper proposes a novel adversarial defense technique for deep neural networks. The proposed method is based on the idea of latent high-order factorization of the network. The authors propose to use randomization to reduce the sparsity of the weights and perturbations in the latent subspace of the neural network. They also propose a new adversarial training method to improve the robustness of the model against adversarial attacks. The experimental results show that the proposed method outperforms the baselines on both image classification and audio classification tasks.
3140,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,"This paper proposes a clustering-based method for spatiotemporal forecasting in the context of time series data. The proposed method is based on a graph attention network and a transformer. The graph attention is used as the encoder-decoder architecture, while the transformer is used for the decoder. The authors also propose a gradient-based clustering method for feature extractors. The experimental results show that the proposed method outperforms the baselines."
3141,SP:762729b64c1c1494de0f7410ea3662da61e93b6d,"This paper proposes a clustering-based method for spatiotemporal forecasting in the context of time series data. The proposed method is based on a graph attention network and a transformer. The graph attention is used as the encoder-decoder architecture, while the transformer is used for the decoder. The authors also propose a gradient-based clustering method for feature extractors. The experimental results show that the proposed method outperforms the baselines."
3142,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper studies the convergence of deep Q-network (DQN) algorithms in a zero-sum Markov game. The main contribution of this paper is a theoretical analysis of the algorithmic and statistical rates of convergence of the action-value function of DQN. The authors show that the algorithm converges at a rate of $O(1/\sqrt{T})$ with respect to the geometric rate of algorithmic error and $O(\sqrt{\frac{T}{T})$. The authors also show that this rate of convergence is bounded by a factor of $\mathcal{O}(T^{-1/2})$, where $T$ is the number of actions and $\frac{1}{T}$ is a function of $T$. "
3143,SP:81d7c60d0d12eb268d7edeebe86422991a1d4997,"This paper studies the convergence of deep Q-network (DQN) algorithms in a zero-sum Markov game. The main contribution of this paper is a theoretical analysis of the algorithmic and statistical rates of convergence of the action-value function of DQN. The authors show that the algorithm converges at a rate of $O(1/\sqrt{T})$ with respect to the geometric rate of algorithmic error and $O(\sqrt{\frac{T}{T})$. The authors also show that this rate of convergence is bounded by a factor of $\mathcal{O}(T^{-1/2})$, where $T$ is the number of actions and $\frac{1}{T}$ is a function of $T$. "
3144,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,This paper proposes a new attention mechanism for natural language translation. The proposed method is based on the Transformer architecture. The main idea is to replace the self-attention in the first phase of Transformer with a weighted sum of the semantics of the target sentence. The authors show that the proposed method outperforms Transformer on the WMT16 English-German translation task and BLEU.
3145,SP:a558ffa1706ef78893528c8c23e2295a79824d2f,This paper proposes a new attention mechanism for natural language translation. The proposed method is based on the Transformer architecture. The main idea is to replace the self-attention in the first phase of Transformer with a weighted sum of the semantics of the target sentence. The authors show that the proposed method outperforms Transformer on the WMT16 English-German translation task and BLEU.
3146,SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper proposes a modular neural network architecture called MAIN. The main idea of the architecture is to use a memoryless controller to store the history of the input and output of each module in a variable length input tape, and then use a random access mechanism to select the modules to be used in the next layer of the network. The proposed architecture is evaluated on a variety of tasks, including reinforcement learning, image classification, and image classification. The results show that the proposed architecture outperforms the state-of-the-art on all tasks."
3147,SP:622b0593972296a95b630a4ece1e959b60fec56c,"This paper proposes a modular neural network architecture called MAIN. The main idea of the architecture is to use a memoryless controller to store the history of the input and output of each module in a variable length input tape, and then use a random access mechanism to select the modules to be used in the next layer of the network. The proposed architecture is evaluated on a variety of tasks, including reinforcement learning, image classification, and image classification. The results show that the proposed architecture outperforms the state-of-the-art on all tasks."
3148,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,This paper proposes a method for quantization of deep neural networks. The main idea is to use Monte Carlo arithmetic (MCDA) to compute the relative standard deviation of the neural network loss. The authors show that MCDA can be used to estimate the loss of significance of the weight parameter sets in the network topology. The paper also provides a theoretical analysis of the performance of the proposed method on CIFAR-10 and ImageNet.
3149,SP:d668cc809e4f6b5f3330cf75cb5f71693a123c07,This paper proposes a method for quantization of deep neural networks. The main idea is to use Monte Carlo arithmetic (MCDA) to compute the relative standard deviation of the neural network loss. The authors show that MCDA can be used to estimate the loss of significance of the weight parameter sets in the network topology. The paper also provides a theoretical analysis of the performance of the proposed method on CIFAR-10 and ImageNet.
3150,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,This paper studies the problem of objective mismatch in model-based reinforcement learning (MBRL). The authors propose a method to address the problem by training a forward dynamics model and a planning scheme. The proposed method is evaluated on a variety of continuous control tasks.
3151,SP:eda1d368aa3b4d806020c4c430a173d1ddd13d0d,This paper studies the problem of objective mismatch in model-based reinforcement learning (MBRL). The authors propose a method to address the problem by training a forward dynamics model and a planning scheme. The proposed method is evaluated on a variety of continuous control tasks.
3152,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper proposes a targeted blackbox transfer-based attack against undefended ImageNet models. The proposed attack is based on the observation that the intermediate feature distributions of CNNs are highly transferable across different networks. The authors propose a new adversarial attack method based on adversarial perturbations to the intermediate features of the network, and show that the proposed method is effective. The paper also provides a theoretical analysis of the transferability of the proposed attack."
3153,SP:63c452f2b2cbfeea0b45831bd7dc1ac26883fd9f,"This paper proposes a targeted blackbox transfer-based attack against undefended ImageNet models. The proposed attack is based on the observation that the intermediate feature distributions of CNNs are highly transferable across different networks. The authors propose a new adversarial attack method based on adversarial perturbations to the intermediate features of the network, and show that the proposed method is effective. The paper also provides a theoretical analysis of the transferability of the proposed attack."
3154,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper proposes a new method for learning policies in reinforcement learning based on Wasserstein distances (WDs). The main idea is to use a dual formulation of the score function of the Wassersteins distance between the agent and the environment, which can be used as a regularizer for policy optimization. The authors show that this dual formulation can be applied to both on-policy and off-policy algorithms, and show that the proposed method outperforms the baselines. "
3155,SP:a7a2ded35804c381603a1196c7f7893fdf796c05,"This paper proposes a new method for learning policies in reinforcement learning based on Wasserstein distances (WDs). The main idea is to use a dual formulation of the score function of the Wassersteins distance between the agent and the environment, which can be used as a regularizer for policy optimization. The authors show that this dual formulation can be applied to both on-policy and off-policy algorithms, and show that the proposed method outperforms the baselines. "
3156,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper studies the problem of adaptive learning rates for deep neural networks. The authors propose a differentiable neural computer (ALI-G) algorithm for learning rates. The main contribution of the paper is to propose a constant hyper-parameterization of the learning rate of ALI, which can be used as a drop-in replacement for SGD. The proposed algorithm is evaluated on the SVHN and CIFAR-10 datasets, and compared to SGD, Bi-LSTM, and other adaptive learning rate methods. "
3157,SP:ef1c6403597c3a6083c1ad4256449325ac99416c,"This paper studies the problem of adaptive learning rates for deep neural networks. The authors propose a differentiable neural computer (ALI-G) algorithm for learning rates. The main contribution of the paper is to propose a constant hyper-parameterization of the learning rate of ALI, which can be used as a drop-in replacement for SGD. The proposed algorithm is evaluated on the SVHN and CIFAR-10 datasets, and compared to SGD, Bi-LSTM, and other adaptive learning rate methods. "
3158,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper proposes a method for incremental grouping of objects in visual scenes. The proposed method is based on the idea of Gestalt cues, which are used to guide the learning of perceptual groups. The method is evaluated on a variety of synthetic visual tasks and compared to a number of baselines."
3159,SP:6e24a1e0aff73db6ae8558f114b644965e287e36,"This paper proposes a method for incremental grouping of objects in visual scenes. The proposed method is based on the idea of Gestalt cues, which are used to guide the learning of perceptual groups. The method is evaluated on a variety of synthetic visual tasks and compared to a number of baselines."
3160,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper studies the effect of weight sparsity on the performance of neural network models. The authors propose a new regularizer called DeepHoyer, which is based on the Hoyer measure. The main idea is to use gradient descent as a regularizer to reduce the sparsity of the weights. The proposed method is evaluated on a variety of image classification tasks and shows that it is effective in reducing sparsity."
3161,SP:7a0db1e8804defc5c04e0f4dd345272c6df1ff77,"This paper studies the effect of weight sparsity on the performance of neural network models. The authors propose a new regularizer called DeepHoyer, which is based on the Hoyer measure. The main idea is to use gradient descent as a regularizer to reduce the sparsity of the weights. The proposed method is evaluated on a variety of image classification tasks and shows that it is effective in reducing sparsity."
3162,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper studies the problem of optimizing strongly convex functions. The main contribution of this paper is to provide a data-dependent O(log T) regret bound for the RMSprop algorithm, which is based on the Adam algorithm. The authors show that the data-dependency of the regret bound depends on the step size and the number of iterations. The paper also provides a theoretical analysis of the convergence rate of the proposed algorithm."
3163,SP:5ec05ac5d72e8e0b39b15a0cd7b2f5a64e861024,"This paper studies the problem of optimizing strongly convex functions. The main contribution of this paper is to provide a data-dependent O(log T) regret bound for the RMSprop algorithm, which is based on the Adam algorithm. The authors show that the data-dependency of the regret bound depends on the step size and the number of iterations. The paper also provides a theoretical analysis of the convergence rate of the proposed algorithm."
3164,SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a new BERT-based model, BlockBERT, which uses sparse block structures to model long-distance dependencies between paragraphs. The authors propose a new attention head for short-term and long-range contextual information. The proposed method is evaluated on two benchmark question answering datasets and compared with RoBERTa and BERT. The BlockBERTs achieves better performance than RoBERTs on both datasets."
3165,SP:9f89501e6319280b4a14b674632a300805aa485c,"This paper proposes a new BERT-based model, BlockBERT, which uses sparse block structures to model long-distance dependencies between paragraphs. The authors propose a new attention head for short-term and long-range contextual information. The proposed method is evaluated on two benchmark question answering datasets and compared with RoBERTa and BERT. The BlockBERTs achieves better performance than RoBERTs on both datasets."
3166,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,This paper studies the effect of pruning on the generalization ability of neural networks. The authors propose a regularization term that encourages the network parameters to be close to each other. They show that this regularization is effective in reducing the instability of the network. They also show that noise can be added to the pruned network to further reduce the instability.
3167,SP:0f04fc2e7966f4ba53909654fc0e8b90fc405f2a,This paper studies the effect of pruning on the generalization ability of neural networks. The authors propose a regularization term that encourages the network parameters to be close to each other. They show that this regularization is effective in reducing the instability of the network. They also show that noise can be added to the pruned network to further reduce the instability.
3168,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"This paper proposes a method for neural architecture search (NAS) that combines reinforcement learning and reinforcement learning. The main idea is to use reinforcement learning to learn the embedding space of a neural architecture, and then use the learned embedding as a pre-training controller to train the architecture network. The proposed method is evaluated on CIFAR-10 and ImageNet."
3169,SP:dba3f5ec3af2a4a67ed4fc36b0f37fe556354177,"This paper proposes a method for neural architecture search (NAS) that combines reinforcement learning and reinforcement learning. The main idea is to use reinforcement learning to learn the embedding space of a neural architecture, and then use the learned embedding as a pre-training controller to train the architecture network. The proposed method is evaluated on CIFAR-10 and ImageNet."
3170,SP:e2e5bebccc76a51df3cb8b64572720da97174604,"This paper studies the problem of homotopy training for neural networks. The main contribution of this paper is to provide a theoretical analysis of HTA for the non-convex case and the convex case. In particular, the authors show that HTA can be used to train neural networks in a low-dimensional coupled system with low dimensionality. The authors also show that the proposed method can be combined with dropout technique to improve the performance of neural networks trained with HTA."
3171,SP:e2e5bebccc76a51df3cb8b64572720da97174604,"This paper studies the problem of homotopy training for neural networks. The main contribution of this paper is to provide a theoretical analysis of HTA for the non-convex case and the convex case. In particular, the authors show that HTA can be used to train neural networks in a low-dimensional coupled system with low dimensionality. The authors also show that the proposed method can be combined with dropout technique to improve the performance of neural networks trained with HTA."
3172,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,This paper proposes a 2-simplicial Transformer architecture for deep reinforcement learning. The main idea is to replace the dot product attention in the original Transformer with tensor products of value vectors. The authors show that the proposed architecture is able to achieve better performance than the state-of-the-art.
3173,SP:5d9517fa62cd97b94ff45f645e100a8ad631e281,This paper proposes a 2-simplicial Transformer architecture for deep reinforcement learning. The main idea is to replace the dot product attention in the original Transformer with tensor products of value vectors. The authors show that the proposed architecture is able to achieve better performance than the state-of-the-art.
3174,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,This paper proposes a method for pose estimation based on conditional variational autoencoders (CVAEs). The proposed method is based on Conditional Variational Autoencoder (CVAE) and uses circular latent representations to represent 2D rotations. The authors show that the proposed method outperforms existing methods on a variety of datasets.
3175,SP:f66721bf3eccf2e36444c2c41303e97745f10f0e,This paper proposes a method for pose estimation based on conditional variational autoencoders (CVAEs). The proposed method is based on Conditional Variational Autoencoder (CVAE) and uses circular latent representations to represent 2D rotations. The authors show that the proposed method outperforms existing methods on a variety of datasets.
3176,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a curriculum learning method for multi-agent reinforcement learning (MARL). The proposed method is based on evolutionary population curriculum learning (EPC), which aims to address the problem of objective misalignment in MARL. The main contribution of this paper is the introduction of a new algorithm, MADDPG, which is able to learn a curriculum for MARL with a much smaller number of agents compared to existing methods. The proposed algorithm is evaluated on a variety of environments, and compared to several baselines."
3177,SP:87dc93d26ad5ad4a8dccde1780b5b127f391cfd6,"This paper proposes a curriculum learning method for multi-agent reinforcement learning (MARL). The proposed method is based on evolutionary population curriculum learning (EPC), which aims to address the problem of objective misalignment in MARL. The main contribution of this paper is the introduction of a new algorithm, MADDPG, which is able to learn a curriculum for MARL with a much smaller number of agents compared to existing methods. The proposed algorithm is evaluated on a variety of environments, and compared to several baselines."
3178,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,This paper proposes a new method for evaluating the quality of images. The method is based on the idea of multi-view multi-image classification (MIMIC). The authors propose a method to evaluate the quality and diversity of images in a single image and then use the results of the MIMIC method to make a decision on which images to classify. The authors also propose a new metric to measure the diversity of a given image. The paper is well-written and easy to follow.
3179,SP:0ea5b3247ce031f25b98cf7d42bd4290020fbed2,This paper proposes a new method for evaluating the quality of images. The method is based on the idea of multi-view multi-image classification (MIMIC). The authors propose a method to evaluate the quality and diversity of images in a single image and then use the results of the MIMIC method to make a decision on which images to classify. The authors also propose a new metric to measure the diversity of a given image. The paper is well-written and easy to follow.
3180,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,"This paper proposes an adaptive noise MCMC algorithm for Bayesian inference. The proposed method is based on the idea of adaptive momentum, which is an extension of momentum-based Bayesian Monte Carlo (MCMC) algorithm. The main difference between the proposed method and MCMC is that the authors propose to use momentum to update the parameters of the posterior distribution instead of the original posterior distribution. The authors also propose a new measure of uncertainty, which can be used to evaluate the performance of the proposed algorithm. Experiments are conducted on Cifar-10 and ImageNet."
3181,SP:9bcb840f867f1a7108aa22a7bb14c348fda52eb0,"This paper proposes an adaptive noise MCMC algorithm for Bayesian inference. The proposed method is based on the idea of adaptive momentum, which is an extension of momentum-based Bayesian Monte Carlo (MCMC) algorithm. The main difference between the proposed method and MCMC is that the authors propose to use momentum to update the parameters of the posterior distribution instead of the original posterior distribution. The authors also propose a new measure of uncertainty, which can be used to evaluate the performance of the proposed algorithm. Experiments are conducted on Cifar-10 and ImageNet."
3182,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,This paper proposes a new metric called Early Bird Ticket (EB) for training deep neural networks. EB tickets can be used to compare the performance of different low-precision training methods on different datasets. The authors show that EB tickets are more efficient than the original training methods. The paper also provides a theoretical analysis of EB tickets and shows that the distance between EB tickets is a function of the connectivity patterns of the network. 
3183,SP:8cf0614f0fbd3756453304703d00776cfc9a4b9f,This paper proposes a new metric called Early Bird Ticket (EB) for training deep neural networks. EB tickets can be used to compare the performance of different low-precision training methods on different datasets. The authors show that EB tickets are more efficient than the original training methods. The paper also provides a theoretical analysis of EB tickets and shows that the distance between EB tickets is a function of the connectivity patterns of the network. 
3184,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"This paper studies the problem of adversarial robustness of deep convolutional neural networks (DNNs). The authors propose a new regularization method for DNNs, called Embedding Regularized Classifier (ER-Classifier). The proposed method is based on the observation that the intrinsic dimension of image data is much smaller than the pixel space dimension. The authors then propose a regularization term that encourages the embedding of the input image to be low-dimensional. Empirically, the authors show that the proposed method improves the robustness against adversarial attacks."
3185,SP:8aeece75c839643a02d2b3b5f3aca7cb76cf1d35,"This paper studies the problem of adversarial robustness of deep convolutional neural networks (DNNs). The authors propose a new regularization method for DNNs, called Embedding Regularized Classifier (ER-Classifier). The proposed method is based on the observation that the intrinsic dimension of image data is much smaller than the pixel space dimension. The authors then propose a regularization term that encourages the embedding of the input image to be low-dimensional. Empirically, the authors show that the proposed method improves the robustness against adversarial attacks."
3186,SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper studies the problem of jointly learning entity recognition and relation extraction from a pre-trained language model. The authors propose a method to jointly train a neural network and an end-to-end model for jointly extracting entities and relations. The main contribution of the paper is to show that the self-attention mechanism of the neural network is recurrence-invariant, which is an important property of deep neural networks. The proposed method is evaluated on NER, relation extraction, and dependency parsers."
3187,SP:efd68097f47dbfdd0208573071686a62240d1b12,"This paper studies the problem of jointly learning entity recognition and relation extraction from a pre-trained language model. The authors propose a method to jointly train a neural network and an end-to-end model for jointly extracting entities and relations. The main contribution of the paper is to show that the self-attention mechanism of the neural network is recurrence-invariant, which is an important property of deep neural networks. The proposed method is evaluated on NER, relation extraction, and dependency parsers."
3188,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper proposes a novel method for learning representations for triplet comparisons. The main idea is to learn a low-level feature representation for each triplet of the triplet, which is then used to train a deep neural network (DNN) to perform triplet comparison. The proposed method is based on Ordinal Embedding (OED), which is an extension of the idea of Euclidean representation learning. The authors show that OED can be applied to both supervised and unsupervised learning problems, and show that the proposed method outperforms the state-of-the-art."
3189,SP:8fd4f3f8615c0a7a76ec7bfe996d2ead803f7828,"This paper proposes a novel method for learning representations for triplet comparisons. The main idea is to learn a low-level feature representation for each triplet of the triplet, which is then used to train a deep neural network (DNN) to perform triplet comparison. The proposed method is based on Ordinal Embedding (OED), which is an extension of the idea of Euclidean representation learning. The authors show that OED can be applied to both supervised and unsupervised learning problems, and show that the proposed method outperforms the state-of-the-art."
3190,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,This paper proposes a meta-learning framework for data value estimation based on reinforcement learning. The main idea is to use a deep neural network to estimate the value of a dataset and then use reinforcement learning to train a predictor model to predict the value. The proposed method is evaluated on a variety of datasets and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of value estimation.
3191,SP:12e7f417a7ef1ccafccff5ffb3f8f11cd2c05b20,This paper proposes a meta-learning framework for data value estimation based on reinforcement learning. The main idea is to use a deep neural network to estimate the value of a dataset and then use reinforcement learning to train a predictor model to predict the value. The proposed method is evaluated on a variety of datasets and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of value estimation.
3192,SP:e2c3374629cfd654b7b35e88507e65646d70470e,This paper studies the Jacobian squared norm of random fully connected ReLU networks. The authors show that the per-layer Jacobian norm of the network is exponentially larger than the depth of the layer. They also show that this norm is not dependent on the network’s architecture or initialization parameters.
3193,SP:e2c3374629cfd654b7b35e88507e65646d70470e,This paper studies the Jacobian squared norm of random fully connected ReLU networks. The authors show that the per-layer Jacobian norm of the network is exponentially larger than the depth of the layer. They also show that this norm is not dependent on the network’s architecture or initialization parameters.
3194,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes a reinforcement learning-based algorithm for code optimization of neural networks. The main idea is to use reinforcement learning to guide the selection of a sequence of samples from the source code of a neural network to be used in the optimization process. The proposed algorithm is based on an adaptive sampling algorithm, which uses a domain-knowledge inspired logic to select samples that are most likely to be useful for the target network. The authors show that the proposed algorithm can reduce the compilation time by a factor of at least 1.5x compared to AutoTVM. They also show that their algorithm is able to reduce the inference time of deep neural networks by an order of magnitude."
3195,SP:4463645f1a9abfbf472935d9eb3342919aa4e0f4,"This paper proposes a reinforcement learning-based algorithm for code optimization of neural networks. The main idea is to use reinforcement learning to guide the selection of a sequence of samples from the source code of a neural network to be used in the optimization process. The proposed algorithm is based on an adaptive sampling algorithm, which uses a domain-knowledge inspired logic to select samples that are most likely to be useful for the target network. The authors show that the proposed algorithm can reduce the compilation time by a factor of at least 1.5x compared to AutoTVM. They also show that their algorithm is able to reduce the inference time of deep neural networks by an order of magnitude."
3196,SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper studies the problem of certified robustness to adversarial perturbations on top-k predictions of a classifier. The authors propose to use randomized smoothing to improve the robustness of the classifier to Gaussian noise. The proposed method is based on Gaussian smoothing, and the authors show that it can achieve tight robustness on the `2 norm of the topk predictions. "
3197,SP:df8483206bb88debeb24b04eb31e016368792a84,"This paper studies the problem of certified robustness to adversarial perturbations on top-k predictions of a classifier. The authors propose to use randomized smoothing to improve the robustness of the classifier to Gaussian noise. The proposed method is based on Gaussian smoothing, and the authors show that it can achieve tight robustness on the `2 norm of the topk predictions. "
3198,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the gradient signal-to-noise ratio (GSNR) of deep neural networks (DNNs). The authors show that GSNR can be used as a measure of the generalization gap between DNNs and other models. The authors also show that under certain conditions, GSNRs can be computed for support vector machines and logistic regression."
3199,SP:84a83ee258d5bc613b7d73045477018b8a56c56d,"This paper studies the gradient signal-to-noise ratio (GSNR) of deep neural networks (DNNs). The authors show that GSNR can be used as a measure of the generalization gap between DNNs and other models. The authors also show that under certain conditions, GSNRs can be computed for support vector machines and logistic regression."
3200,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes a method for solving logical disjunctions (i.e., logical queries) in a vector space. The authors propose a new embedding-based framework, called QUERY2BOX, which is able to handle complex logical queries in a disjunctive normal form. The main contribution of this paper is to introduce a new normal form for logical queries. The proposed method is evaluated on a large number of synthetic and real-world datasets and compared to state-of-the-art methods."
3201,SP:fb726f0fea2ed1a009b3aacf74ac149bcf988cdd,"This paper proposes a method for solving logical disjunctions (i.e., logical queries) in a vector space. The authors propose a new embedding-based framework, called QUERY2BOX, which is able to handle complex logical queries in a disjunctive normal form. The main contribution of this paper is to introduce a new normal form for logical queries. The proposed method is evaluated on a large number of synthetic and real-world datasets and compared to state-of-the-art methods."
3202,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,This paper studies the convergence of SGD for convex and non-convex optimization problems. The authors propose a new unbiased estimator for SGD that is consistent with the empirical risk minimization objective. They show that this unbiased gradient estimator converges faster than the full gradient. They also show that consistent estimators can be used to train SGD on large-scale graphs.
3203,SP:c8bbdbf038ddec801c931ae9399b8c16b08428bc,This paper studies the convergence of SGD for convex and non-convex optimization problems. The authors propose a new unbiased estimator for SGD that is consistent with the empirical risk minimization objective. They show that this unbiased gradient estimator converges faster than the full gradient. They also show that consistent estimators can be used to train SGD on large-scale graphs.
3204,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes a method to prune the width, depth, and kernel size of an OFA network. The authors propose a progressive shrinking algorithm to reduce the width and kernel sizes of OFA networks. The proposed method is evaluated on ImageNet and LPCVC and compared to MobileNetV3 and EfficientNet."
3205,SP:d53ee573b8083ecf891d4d560eb8a54c30c5cb3a,"This paper proposes a method to prune the width, depth, and kernel size of an OFA network. The authors propose a progressive shrinking algorithm to reduce the width and kernel sizes of OFA networks. The proposed method is evaluated on ImageNet and LPCVC and compared to MobileNetV3 and EfficientNet."
3206,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper proposes a neural module network (NMN) based model for answering compositional questions in natural language. The model is based on a probabilistic and differentiable model, which is trained with an auxiliary loss and a heuristically-generated question program. The proposed model is evaluated on the DROP dataset, where it is shown to outperform the state-of-the-art models."
3207,SP:1be944b5f82d33ab1feb5639792a4c06b8f0c85a,"This paper proposes a neural module network (NMN) based model for answering compositional questions in natural language. The model is based on a probabilistic and differentiable model, which is trained with an auxiliary loss and a heuristically-generated question program. The proposed model is evaluated on the DROP dataset, where it is shown to outperform the state-of-the-art models."
3208,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,This paper proposes a data-free method for pruning neural networks. The main idea is to use the connection sensitivity of the network as a saliency criterion for the initialization of the pruned network. The proposed method is evaluated on a variety of image classification tasks. The experiments show that the proposed method outperforms the existing pruning methods in terms of signal propagation properties.
3209,SP:319922e4a316a9b9e76504f806d30ea3bffa3f99,This paper proposes a data-free method for pruning neural networks. The main idea is to use the connection sensitivity of the network as a saliency criterion for the initialization of the pruned network. The proposed method is evaluated on a variety of image classification tasks. The experiments show that the proposed method outperforms the existing pruning methods in terms of signal propagation properties.
3210,SP:d5899cba36329d863513b91c2db57675086abc49,"This paper proposes a new initialization scheme for sparse neural networks. The idea is to train a sparse network with a data-free heuristic, where the weights of each layer are chosen based on a heuristic. The heuristic is based on the fact that sparse networks tend to be more memory efficient than dense ones. The authors show that the heuristic can be applied to a variety of networks, and show that it can be used to train sparse networks with dense and convolutional layers. "
3211,SP:d5899cba36329d863513b91c2db57675086abc49,"This paper proposes a new initialization scheme for sparse neural networks. The idea is to train a sparse network with a data-free heuristic, where the weights of each layer are chosen based on a heuristic. The heuristic is based on the fact that sparse networks tend to be more memory efficient than dense ones. The authors show that the heuristic can be applied to a variety of networks, and show that it can be used to train sparse networks with dense and convolutional layers. "
3212,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,This paper studies the initialization of recurrent neural networks (RNNs) in the presence of training instabilities. The authors propose a new initialization scheme for RNNs based on the Jacobian of the state-to-state Jacobians of the RNN. They show that the proposed initialization scheme is more stable than the original initialization scheme. They also provide a theoretical analysis of the effect of the initialization scheme on the generalization performance of the network.
3213,SP:b05a6a0f05dcc63a7e17233f20c49c465c46d194,This paper studies the initialization of recurrent neural networks (RNNs) in the presence of training instabilities. The authors propose a new initialization scheme for RNNs based on the Jacobian of the state-to-state Jacobians of the RNN. They show that the proposed initialization scheme is more stable than the original initialization scheme. They also provide a theoretical analysis of the effect of the initialization scheme on the generalization performance of the network.
3214,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"This paper proposes a method for post-classification of remote sensing data. The proposed method is based on the siamese network, which is a convolutional neural network with a smoothing operation. The main idea is to use the feature vector of neighboring scene images as a feature vector for the feature vectors of the original image. The authors show that the proposed method outperforms the baseline methods on the disease density estimation task."
3215,SP:7b65eb83b0d3149f788ab11b1ab9057b440ddd57,"This paper proposes a method for post-classification of remote sensing data. The proposed method is based on the siamese network, which is a convolutional neural network with a smoothing operation. The main idea is to use the feature vector of neighboring scene images as a feature vector for the feature vectors of the original image. The authors show that the proposed method outperforms the baseline methods on the disease density estimation task."
3216,SP:99c10e038939aa88fc112db10fe801b42360c8dc,"This paper proposes a self-supervised method for monocular depth estimation. The proposed method is based on semantic segmentation networks, where the semantic labels are learned from a set of category-level patterns, and the geometric representation is learned from pixel-adaptive convolutions. The authors also propose a two-stage training process to deal with the common semantic bias of dynamic objects. The experimental results show that the proposed method outperforms state-of-the-art methods."
3217,SP:99c10e038939aa88fc112db10fe801b42360c8dc,"This paper proposes a self-supervised method for monocular depth estimation. The proposed method is based on semantic segmentation networks, where the semantic labels are learned from a set of category-level patterns, and the geometric representation is learned from pixel-adaptive convolutions. The authors also propose a two-stage training process to deal with the common semantic bias of dynamic objects. The experimental results show that the proposed method outperforms state-of-the-art methods."
3218,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,This paper studies the problem of label-flipping attacks. The authors propose a randomized smoothing method to improve the test-time robustness of a linear classifier. The proposed method is motivated by the fact that the classification error of the classifier can be used as a proxy for the label flipping attack. The experimental results show that the proposed method improves the certified accuracy of the proposed classifier on the Dogfish binary classification task.
3219,SP:e98ec7fd9c27eabd7f5bf3429f984034c2d355a2,This paper studies the problem of label-flipping attacks. The authors propose a randomized smoothing method to improve the test-time robustness of a linear classifier. The proposed method is motivated by the fact that the classification error of the classifier can be used as a proxy for the label flipping attack. The experimental results show that the proposed method improves the certified accuracy of the proposed classifier on the Dogfish binary classification task.
3220,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper studies differential privacy for outlier detection and novelty detection in the context of backdoor poisoning attacks. In particular, the authors propose an extension of differential privacy to the case of backdoor attacks. The authors show that differential privacy can be used to detect backdoor poisoning samples in the presence of outlier samples. They also provide a theoretical analysis of the effect of the differential privacy on novelty detection and backdoor attack detection."
3221,SP:795cdeb7e4f7285f2c1ac9b9a0fbac3039201ed5,"This paper studies differential privacy for outlier detection and novelty detection in the context of backdoor poisoning attacks. In particular, the authors propose an extension of differential privacy to the case of backdoor attacks. The authors show that differential privacy can be used to detect backdoor poisoning samples in the presence of outlier samples. They also provide a theoretical analysis of the effect of the differential privacy on novelty detection and backdoor attack detection."
3222,SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper proposes a method for calibration of uncertainty prediction for regression tasks. The method is based on a histogram-based approach, where the uncertainty is defined as the difference between the predicted uncertainty and the empirical uncertainty. The authors propose a scaling-based calibration method to evaluate the calibration of the uncertainty of the regression uncertainty, and show that the proposed method outperforms other calibration methods on a synthetic problem and a bounding box regression task. "
3223,SP:a5f0e531afd970144169823971d2d039bff752fb,"This paper proposes a method for calibration of uncertainty prediction for regression tasks. The method is based on a histogram-based approach, where the uncertainty is defined as the difference between the predicted uncertainty and the empirical uncertainty. The authors propose a scaling-based calibration method to evaluate the calibration of the uncertainty of the regression uncertainty, and show that the proposed method outperforms other calibration methods on a synthetic problem and a bounding box regression task. "
3224,SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a bidirectional Transformer language model that combines BERT and Tensor-Product Representations (TPRs) to improve the performance of BERT. The proposed model, HUBERT1, is a combination of the BERT model with Tensor Product Representations and TPRs. The main contribution of this paper is the introduction of a general language structure that allows for untangling data-specific semantics from general language structures. Experiments on the GLUE benchmark and HANS dataset show that the proposed model outperforms BERT in terms of performance."
3225,SP:c422afd1df1ac98e23235830585dd0d45513064c,"This paper proposes a bidirectional Transformer language model that combines BERT and Tensor-Product Representations (TPRs) to improve the performance of BERT. The proposed model, HUBERT1, is a combination of the BERT model with Tensor Product Representations and TPRs. The main contribution of this paper is the introduction of a general language structure that allows for untangling data-specific semantics from general language structures. Experiments on the GLUE benchmark and HANS dataset show that the proposed model outperforms BERT in terms of performance."
3226,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"This paper studies the problem of multi-agent reinforcement learning. The authors propose a partial parameter sharing approach to solve the problem. The main idea is to learn a goal-conditioned policy for each agent in a decentralized manner, and then use this policy to guide the other agents to achieve the goal. The proposed method is evaluated on a simulated humanoid navigation task, where the authors show that the proposed method outperforms the baselines."
3227,SP:117b19c4163cb3d08eda6bc7af0d48ed815b519e,"This paper studies the problem of multi-agent reinforcement learning. The authors propose a partial parameter sharing approach to solve the problem. The main idea is to learn a goal-conditioned policy for each agent in a decentralized manner, and then use this policy to guide the other agents to achieve the goal. The proposed method is evaluated on a simulated humanoid navigation task, where the authors show that the proposed method outperforms the baselines."
3228,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,This paper proposes a graph pooling method for graph neural networks (GNNs). The proposed method is based on a point cloud representation of the graph and a graph embedding method. The authors propose to use the spatial distribution of the locally extracted feature vectors for graph down-sampling problem. The experimental results show that the proposed method outperforms state-of-the-art methods.
3229,SP:928640a19b0a0b1e1dc0d1b07cc99e1d51a4d817,This paper proposes a graph pooling method for graph neural networks (GNNs). The proposed method is based on a point cloud representation of the graph and a graph embedding method. The authors propose to use the spatial distribution of the locally extracted feature vectors for graph down-sampling problem. The experimental results show that the proposed method outperforms state-of-the-art methods.
3230,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,"This paper studies the effect of frequency pooling on the accuracy and robustness of convolutional neural networks (CNNs) in the context of shift-equivalent prior of images. In particular, the authors study the impact of frequency-pooling and stride convolution on the performance of CNNs. The authors show that frequency poolings can improve the accuracy, robustness, and accuracy of the CNNs in terms of both accuracy and accuracy. The paper also shows that frequency pools can be used as a down-sampling method for CNNs, which is an interesting idea. "
3231,SP:465adf302cd8b7e6b449271a91d1d2fad844aa4d,"This paper studies the effect of frequency pooling on the accuracy and robustness of convolutional neural networks (CNNs) in the context of shift-equivalent prior of images. In particular, the authors study the impact of frequency-pooling and stride convolution on the performance of CNNs. The authors show that frequency poolings can improve the accuracy, robustness, and accuracy of the CNNs in terms of both accuracy and accuracy. The paper also shows that frequency pools can be used as a down-sampling method for CNNs, which is an interesting idea. "
3232,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,This paper proposes a novel method to improve the generalization ability of deep RL agents. The proposed method is based on a randomized convolutional neural network (RNN) and a Monte Carlo approximation of the inference method. The authors show that the proposed method outperforms existing methods on CoinRun and 3D DeepMind Lab exploration tasks. 
3233,SP:77f0f3779f9bdeb75ea5744ab494942a4943117b,This paper proposes a novel method to improve the generalization ability of deep RL agents. The proposed method is based on a randomized convolutional neural network (RNN) and a Monte Carlo approximation of the inference method. The authors show that the proposed method outperforms existing methods on CoinRun and 3D DeepMind Lab exploration tasks. 
3234,SP:31772a9122ec998c7c829bc4813f6147cdc30145,This paper proposes an explanation method for image similarity models. The main idea is to use the saliency map of an image as an explanation for the model. The proposed method is evaluated on a variety of image classification and attribute recognition tasks.
3235,SP:31772a9122ec998c7c829bc4813f6147cdc30145,This paper proposes an explanation method for image similarity models. The main idea is to use the saliency map of an image as an explanation for the model. The proposed method is evaluated on a variety of image classification and attribute recognition tasks.
3236,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes WaveFlow, an autoregressive flow based generative model for high-fidelity speech synthesis. The proposed model is a combination of WaveNet and WaveGlow, which is a bipartite flow with auxiliary losses. The authors show that WaveFlow can achieve state-of-the-art results on both synthetic and real-time datasets."
3237,SP:50f9dcac485552f2925839151da4dd8d82e35fcc,"This paper proposes WaveFlow, an autoregressive flow based generative model for high-fidelity speech synthesis. The proposed model is a combination of WaveNet and WaveGlow, which is a bipartite flow with auxiliary losses. The authors show that WaveFlow can achieve state-of-the-art results on both synthetic and real-time datasets."
3238,SP:963e85369978dddcd9e3130bc11453696066bbf3,"This paper proposes a GAN-based method for graph generation. The proposed method is based on the Transformer-based GAN (GT-GAN) framework. The main idea of GT-GAN is to learn a translation mapping between the input graph and the target graph, which is then used to train a conditional graph discriminator. The method is evaluated on both synthetic and real-world datasets. The experimental results show the effectiveness of the proposed method."
3239,SP:963e85369978dddcd9e3130bc11453696066bbf3,"This paper proposes a GAN-based method for graph generation. The proposed method is based on the Transformer-based GAN (GT-GAN) framework. The main idea of GT-GAN is to learn a translation mapping between the input graph and the target graph, which is then used to train a conditional graph discriminator. The method is evaluated on both synthetic and real-world datasets. The experimental results show the effectiveness of the proposed method."
3240,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a meta-gated recurrent controller (meta-gating) model for sequence modeling. The proposed model is a combination of meta-learning and meta-activation. The meta-training consists of two steps: (1) meta-attention and (2) meta gating. The first step is to learn a gating mechanism for the meta-controller. The second step consists of training a recurrent encoder-decoder model that takes as input a sequence of tokens and outputs the output of the gating function. The authors evaluate the proposed model on a variety of tasks, including semantic parsing, code generation, machine translation, tree traversal, and logical inference. "
3241,SP:962caffd236630c4079bfc7292403c1cc6861c3b,"This paper proposes a meta-gated recurrent controller (meta-gating) model for sequence modeling. The proposed model is a combination of meta-learning and meta-activation. The meta-training consists of two steps: (1) meta-attention and (2) meta gating. The first step is to learn a gating mechanism for the meta-controller. The second step consists of training a recurrent encoder-decoder model that takes as input a sequence of tokens and outputs the output of the gating function. The authors evaluate the proposed model on a variety of tasks, including semantic parsing, code generation, machine translation, tree traversal, and logical inference. "
3242,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a new self-supervised objective for speech recognition. The proposed objective, Local Prior Matching (LPM), is based on a language model that is trained on unlabeled speech and unpaired text. The authors show that the proposed LPM outperforms the state-of-the-art on both clean and noisy test sets."
3243,SP:d03aa0318f0d24a5b7c7817dfc7fba47ebec11cd,"This paper proposes a new self-supervised objective for speech recognition. The proposed objective, Local Prior Matching (LPM), is based on a language model that is trained on unlabeled speech and unpaired text. The authors show that the proposed LPM outperforms the state-of-the-art on both clean and noisy test sets."
3244,SP:e6af249608633f1776b608852a00946a5c09a357,"This paper proposes a generative adversarial network (GAN) framework for fair and robust model training. The proposed framework consists of three components: a fairness discriminator, a robustness discriminator and an equal opportunity discriminator. The fairness and robustness components are based on the notion of disparate impact and equalized odds, and the equal opportunity component is based on equal opportunity. Experiments are conducted on CIFAR-10 and ImageNet datasets to show the effectiveness of the proposed framework."
3245,SP:e6af249608633f1776b608852a00946a5c09a357,"This paper proposes a generative adversarial network (GAN) framework for fair and robust model training. The proposed framework consists of three components: a fairness discriminator, a robustness discriminator and an equal opportunity discriminator. The fairness and robustness components are based on the notion of disparate impact and equalized odds, and the equal opportunity component is based on equal opportunity. Experiments are conducted on CIFAR-10 and ImageNet datasets to show the effectiveness of the proposed framework."
3246,SP:6306417f5a300629ec856495781515c6af05a363,"This paper proposes a method for learning point cloud representations for point cloud processing. The proposed method is based on the PIC/FLIP scheme, which is inspired by the physics-inspired deep learning approach. The main idea is to use a generalized Eulerian-Lagrangian representation of the particle features and a generalized, high-dimensional force field to represent the flow velocities of the particles. The authors also propose a geometric reservoir to store the information about the flow dynamics of the point cloud. Experiments are conducted on point cloud classification and segmentation problems."
3247,SP:6306417f5a300629ec856495781515c6af05a363,"This paper proposes a method for learning point cloud representations for point cloud processing. The proposed method is based on the PIC/FLIP scheme, which is inspired by the physics-inspired deep learning approach. The main idea is to use a generalized Eulerian-Lagrangian representation of the particle features and a generalized, high-dimensional force field to represent the flow velocities of the particles. The authors also propose a geometric reservoir to store the information about the flow dynamics of the point cloud. Experiments are conducted on point cloud classification and segmentation problems."
3248,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,This paper studies the effect of gradient clipping on the dynamics of iterates in deep networks. The authors show that gradient clipping can lead to a local minimum of the rate of convergence of the gradient descent algorithm. They also show that clipping can improve the robustness of the network to label noise. The main contribution of this paper is to provide a theoretical analysis of the impact of clipping in the context of label noise and show that it can be used as an optimisation lens. 
3249,SP:0561a2174d7334e078a49ae8859a36e4d74f9b5b,This paper studies the effect of gradient clipping on the dynamics of iterates in deep networks. The authors show that gradient clipping can lead to a local minimum of the rate of convergence of the gradient descent algorithm. They also show that clipping can improve the robustness of the network to label noise. The main contribution of this paper is to provide a theoretical analysis of the impact of clipping in the context of label noise and show that it can be used as an optimisation lens. 
3250,SP:414b06d86e132357a54eb844036b78a232571301,This paper proposes a state alignment based imitation learning method for imitation learning. The proposed method is based on the dynamics model and uses a regularized policy update objective to encourage the state alignment between local and global perspectives. Experiments show that the proposed method outperforms the state-of-the-art methods in imitation learning settings.
3251,SP:414b06d86e132357a54eb844036b78a232571301,This paper proposes a state alignment based imitation learning method for imitation learning. The proposed method is based on the dynamics model and uses a regularized policy update objective to encourage the state alignment between local and global perspectives. Experiments show that the proposed method outperforms the state-of-the-art methods in imitation learning settings.
3252,SP:91761d68086330ce378507c152e72218ed7b2196,"This paper proposes a novel method for deep gradient boosting (DGB) based on the input normalization layer (INN). In particular, the authors propose to use pseudo-residual targets as pseudo-reinforcements in the forward pass of SGD. The authors also propose a new normalization procedure for the weight update formula of the base network. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
3253,SP:91761d68086330ce378507c152e72218ed7b2196,"This paper proposes a novel method for deep gradient boosting (DGB) based on the input normalization layer (INN). In particular, the authors propose to use pseudo-residual targets as pseudo-reinforcements in the forward pass of SGD. The authors also propose a new normalization procedure for the weight update formula of the base network. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
3254,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,This paper proposes a new architecture search method for DARTS. The proposed method is based on partially connected DARTS (PC-DARTS). The main idea is to use the edge normalization technique to reduce the number of parameters in the super-net. The experiments on CIFAR-10 and ImageNet show that the proposed method outperforms existing methods.
3255,SP:7709a8b907c5642479e7b6fb0b362efc4ead63ce,This paper proposes a new architecture search method for DARTS. The proposed method is based on partially connected DARTS (PC-DARTS). The main idea is to use the edge normalization technique to reduce the number of parameters in the super-net. The experiments on CIFAR-10 and ImageNet show that the proposed method outperforms existing methods.
3256,SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a novel method for deep reinforcement learning (DRL) based on deep deterministic policy gradients (DDPG). The main idea is to use a differentiable physical simulator to estimate the true gradients of the actor and modify the policy in order to improve the sample efficiency of DRL. The method is evaluated on a variety of 2D robot control tasks with hard contact constraints. The experiments show that the proposed method outperforms the state-of-the-art in terms of sample efficiency and robustness.
3257,SP:724870046e990376990ba9f73d63d331f61788d7,This paper proposes a novel method for deep reinforcement learning (DRL) based on deep deterministic policy gradients (DDPG). The main idea is to use a differentiable physical simulator to estimate the true gradients of the actor and modify the policy in order to improve the sample efficiency of DRL. The method is evaluated on a variety of 2D robot control tasks with hard contact constraints. The experiments show that the proposed method outperforms the state-of-the-art in terms of sample efficiency and robustness.
3258,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"This paper proposes a hierarchical reinforcement learning (RL) framework for learning from trajectories generated by a binary recurrent variational autoencoder (VAE). The main idea is to learn a world graph of trajectories, which is then used to train an RL agent to explore the world graph. The proposed method is evaluated on a maze task, where it is shown that the proposed method outperforms the baselines."
3259,SP:be0202a28bcca68edb0abe4d1c0ba1af265211e3,"This paper proposes a hierarchical reinforcement learning (RL) framework for learning from trajectories generated by a binary recurrent variational autoencoder (VAE). The main idea is to learn a world graph of trajectories, which is then used to train an RL agent to explore the world graph. The proposed method is evaluated on a maze task, where it is shown that the proposed method outperforms the baselines."
3260,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for learning a neural network that can approximate any type of continuous functions. The method is based on the idea of discretizing the network into a set of function blocks, which can then be used to construct a new network. The authors show that the proposed method can be applied to a variety of functions, and that it is able to perform better than the state of the art. "
3261,SP:e8a3a0f77dab336ce50c9dc941f7350173916e04,"This paper proposes a method for learning a neural network that can approximate any type of continuous functions. The method is based on the idea of discretizing the network into a set of function blocks, which can then be used to construct a new network. The authors show that the proposed method can be applied to a variety of functions, and that it is able to perform better than the state of the art. "
3262,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for imitation learning in the multi-task setting, where the goal is to learn a policy that can reach a goal that is similar to the expert policy. The main idea is to use an imitation learning algorithm that learns to imitate the behavior of the expert, and then use the learned policy to reach the goal. The method is evaluated on a variety of tasks, and compared to a number of state-of-the-art methods."
3263,SP:b7f4fda6497a1c20fd57f029be5f1b2e2780e227,"This paper proposes a method for imitation learning in the multi-task setting, where the goal is to learn a policy that can reach a goal that is similar to the expert policy. The main idea is to use an imitation learning algorithm that learns to imitate the behavior of the expert, and then use the learned policy to reach the goal. The method is evaluated on a variety of tasks, and compared to a number of state-of-the-art methods."
3264,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper studies the problem of Byzantine workers in a non-convex optimization problem with Byzantine workers. The main contribution of this paper is to propose a new algorithm, Zeno++, which is able to handle Byzantine workers without Byzantine failures. The algorithm is based on the idea that the candidate gradient of the Byzantine workers can be used to guide the optimization progress of the optimization process. The proposed algorithm is evaluated on a variety of problems and compared to a number of state-of-the-art algorithms."
3265,SP:1c7cf7417825208feac9fe3b3488a51ad1e72270,"This paper studies the problem of Byzantine workers in a non-convex optimization problem with Byzantine workers. The main contribution of this paper is to propose a new algorithm, Zeno++, which is able to handle Byzantine workers without Byzantine failures. The algorithm is based on the idea that the candidate gradient of the Byzantine workers can be used to guide the optimization progress of the optimization process. The proposed algorithm is evaluated on a variety of problems and compared to a number of state-of-the-art algorithms."
3266,SP:d16ed9bd4193d99774840783347137e938955b87,"This paper studies the impact of semantic adversarial attacks on image classification and captioning tasks. The authors propose to use the Lp norm of the image-based visual descriptors as a measure of the adversarial impact, and show that it can be used to estimate the magnitude of adversarial perturbations. The paper also proposes to use semantically aware adversarial examples to improve the performance of adversarially trained models and feature-squeezing and feature compression methods. Experiments are conducted on ImageNet, MSCOCO, and ImageNet-C."
3267,SP:d16ed9bd4193d99774840783347137e938955b87,"This paper studies the impact of semantic adversarial attacks on image classification and captioning tasks. The authors propose to use the Lp norm of the image-based visual descriptors as a measure of the adversarial impact, and show that it can be used to estimate the magnitude of adversarial perturbations. The paper also proposes to use semantically aware adversarial examples to improve the performance of adversarially trained models and feature-squeezing and feature compression methods. Experiments are conducted on ImageNet, MSCOCO, and ImageNet-C."
3268,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a method for few-shot learning of entity recognition in a natural language understanding (NLU) system. The main idea is to use an RL-based controller to train a neural encoder and a memory controller to manage the memory of the encoder. The controller is trained to control the read and write operations of the memory. The authors show that the controller is able to improve the performance on the Stanford Task-Oriented Dialogue dataset.
3269,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a method for few-shot learning of entity recognition in a natural language understanding (NLU) system. The main idea is to use an RL-based controller to train a neural encoder and a memory controller to manage the memory of the encoder. The controller is trained to control the read and write operations of the memory. The authors show that the controller is able to improve the performance on the Stanford Task-Oriented Dialogue dataset.
3270,SP:f4f7dd96b7865fe2d4c6bddf82875f0c9377c3b4,This paper proposes a method for few-shot learning of entity recognition in a natural language understanding (NLU) system. The main idea is to use an RL-based controller to train a neural encoder and a memory controller to manage the memory of the encoder. The controller is trained to control the read and write operations of the memory. The authors show that the controller is able to improve the performance on the Stanford Task-Oriented Dialogue dataset.
3271,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"This paper proposes a method for learning motor primitives that can be used in a hierarchical reinforcement learning setup. The core idea of the method is to learn a latent representation of a set of primitive primitives, and then use the latent representation to train a neural network to generate a new primitive for each primitive. The method is evaluated on a variety of robotic manipulation tasks and compared to a number of baselines."
3272,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"This paper proposes a method for learning motor primitives that can be used in a hierarchical reinforcement learning setup. The core idea of the method is to learn a latent representation of a set of primitive primitives, and then use the latent representation to train a neural network to generate a new primitive for each primitive. The method is evaluated on a variety of robotic manipulation tasks and compared to a number of baselines."
3273,SP:2e9235485b79d0b22ec8b565b19bfa26804ccbe1,"This paper proposes a method for learning motor primitives that can be used in a hierarchical reinforcement learning setup. The core idea of the method is to learn a latent representation of a set of primitive primitives, and then use the latent representation to train a neural network to generate a new primitive for each primitive. The method is evaluated on a variety of robotic manipulation tasks and compared to a number of baselines."
3274,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,This paper proposes a method for transfer learning in reinforcement learning (RL). The main idea is to use a variational inference model to estimate the latent variables of the test dynamics and then use a general algorithm to perform a single episode transfer. The main contribution of this paper is to propose a modular approach that can be applied to a variety of RL algorithms. The proposed method is evaluated on a small set of Mujoco tasks and compared to a number of baselines.
3275,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,This paper proposes a method for transfer learning in reinforcement learning (RL). The main idea is to use a variational inference model to estimate the latent variables of the test dynamics and then use a general algorithm to perform a single episode transfer. The main contribution of this paper is to propose a modular approach that can be applied to a variety of RL algorithms. The proposed method is evaluated on a small set of Mujoco tasks and compared to a number of baselines.
3276,SP:c7c37aeebec7f33c1015f1fa3dd2a36d7b437d1c,This paper proposes a method for transfer learning in reinforcement learning (RL). The main idea is to use a variational inference model to estimate the latent variables of the test dynamics and then use a general algorithm to perform a single episode transfer. The main contribution of this paper is to propose a modular approach that can be applied to a variety of RL algorithms. The proposed method is evaluated on a small set of Mujoco tasks and compared to a number of baselines.
3277,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,This paper proposes a three-head network architecture for AlphaZero. The main idea is to replace the action-value head in AlphaZero with a Monte Carlo tree search (MCTS) head. The authors show that the proposed architecture is able to outperform the two-head architecture in the game of Hex.
3278,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,This paper proposes a three-head network architecture for AlphaZero. The main idea is to replace the action-value head in AlphaZero with a Monte Carlo tree search (MCTS) head. The authors show that the proposed architecture is able to outperform the two-head architecture in the game of Hex.
3279,SP:f2f1aff9a5b91d748b24fee0155367f650401aab,This paper proposes a three-head network architecture for AlphaZero. The main idea is to replace the action-value head in AlphaZero with a Monte Carlo tree search (MCTS) head. The authors show that the proposed architecture is able to outperform the two-head architecture in the game of Hex.
3280,SP:89d6d55107b6180109affe7522265c751640ad96,"This paper proposes a method for policy transfer in reinforcement learning. The main idea is to use warm initialization and imitation to improve policy transfer. The authors show that warm initialization can be used to improve the sample complexity of policy transfer, while imitation can improve the performance of the learned policy. The method is evaluated on a variety of tasks and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of sample complexity."
3281,SP:89d6d55107b6180109affe7522265c751640ad96,"This paper proposes a method for policy transfer in reinforcement learning. The main idea is to use warm initialization and imitation to improve policy transfer. The authors show that warm initialization can be used to improve the sample complexity of policy transfer, while imitation can improve the performance of the learned policy. The method is evaluated on a variety of tasks and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of sample complexity."
3282,SP:89d6d55107b6180109affe7522265c751640ad96,"This paper proposes a method for policy transfer in reinforcement learning. The main idea is to use warm initialization and imitation to improve policy transfer. The authors show that warm initialization can be used to improve the sample complexity of policy transfer, while imitation can improve the performance of the learned policy. The method is evaluated on a variety of tasks and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of sample complexity."
3283,SP:626021101836a635ad2d896bd66951aff31aa846,"This paper proposes a method for scale-equivariant convolutional neural networks. The proposed method is based on steerable filters, which are steerable to the scale of the input image. The authors show that the proposed method can achieve state-of-the-art performance on the MNIST-scale dataset and STL-10 dataset. "
3284,SP:626021101836a635ad2d896bd66951aff31aa846,"This paper proposes a method for scale-equivariant convolutional neural networks. The proposed method is based on steerable filters, which are steerable to the scale of the input image. The authors show that the proposed method can achieve state-of-the-art performance on the MNIST-scale dataset and STL-10 dataset. "
3285,SP:626021101836a635ad2d896bd66951aff31aa846,"This paper proposes a method for scale-equivariant convolutional neural networks. The proposed method is based on steerable filters, which are steerable to the scale of the input image. The authors show that the proposed method can achieve state-of-the-art performance on the MNIST-scale dataset and STL-10 dataset. "
3286,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,This paper proposes a method for 3D shape completion using point clouds. The method is based on a combination of two existing approaches: (1) ScanNet and Matterport3D and (2) KITTI. The proposed method is evaluated on 3D-EPN shape completion dataset and compares favorably to the state-of-the-art methods.
3287,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,This paper proposes a method for 3D shape completion using point clouds. The method is based on a combination of two existing approaches: (1) ScanNet and Matterport3D and (2) KITTI. The proposed method is evaluated on 3D-EPN shape completion dataset and compares favorably to the state-of-the-art methods.
3288,SP:6316f750b8c69e55e61926c34e3ba5acbd7228ad,This paper proposes a method for 3D shape completion using point clouds. The method is based on a combination of two existing approaches: (1) ScanNet and Matterport3D and (2) KITTI. The proposed method is evaluated on 3D-EPN shape completion dataset and compares favorably to the state-of-the-art methods.
3289,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generative impersonation attacks against generative neural networks (GNNs). The authors propose a maximin game to solve the problem, which is based on the maximin theory of information theory. In particular, the authors show that if the source distribution of the GNN is Gaussian, then the attacker can generate a generative adversarial example that is indistinguishable from the target distribution. The authors also show that the optimal attack strategy can be found by maximizing the mutual information between the source and target distributions. The paper is well-written and easy to follow."
3290,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generative impersonation attacks against generative neural networks (GNNs). The authors propose a maximin game to solve the problem, which is based on the maximin theory of information theory. In particular, the authors show that if the source distribution of the GNN is Gaussian, then the attacker can generate a generative adversarial example that is indistinguishable from the target distribution. The authors also show that the optimal attack strategy can be found by maximizing the mutual information between the source and target distributions. The paper is well-written and easy to follow."
3291,SP:270c679b322f69a943bf7f6b938dc1bf663d3c6f,"This paper studies the problem of generative impersonation attacks against generative neural networks (GNNs). The authors propose a maximin game to solve the problem, which is based on the maximin theory of information theory. In particular, the authors show that if the source distribution of the GNN is Gaussian, then the attacker can generate a generative adversarial example that is indistinguishable from the target distribution. The authors also show that the optimal attack strategy can be found by maximizing the mutual information between the source and target distributions. The paper is well-written and easy to follow."
3292,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper studies the problem of defending against PGD attacks on a multi-class classifier. The authors propose a new adversarial defense method based on sensible adversarial learning (SADL) and show that the proposed method is able to achieve 0-1 loss on the Bayes rule of the classifier, which is equivalent to a 1-loss on the standard loss of PGD. In addition, the authors propose an algorithm to train a robust model with sensible adversarially perturbed examples, which can be used to improve the robustness of the model. The proposed method can be applied to a variety of attacks on CIFAR-10 and ImageNet."
3293,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper studies the problem of defending against PGD attacks on a multi-class classifier. The authors propose a new adversarial defense method based on sensible adversarial learning (SADL) and show that the proposed method is able to achieve 0-1 loss on the Bayes rule of the classifier, which is equivalent to a 1-loss on the standard loss of PGD. In addition, the authors propose an algorithm to train a robust model with sensible adversarially perturbed examples, which can be used to improve the robustness of the model. The proposed method can be applied to a variety of attacks on CIFAR-10 and ImageNet."
3294,SP:a7e7619667892806a6f4038cbe4b1c6cd0eec0ed,"This paper studies the problem of defending against PGD attacks on a multi-class classifier. The authors propose a new adversarial defense method based on sensible adversarial learning (SADL) and show that the proposed method is able to achieve 0-1 loss on the Bayes rule of the classifier, which is equivalent to a 1-loss on the standard loss of PGD. In addition, the authors propose an algorithm to train a robust model with sensible adversarially perturbed examples, which can be used to improve the robustness of the model. The proposed method can be applied to a variety of attacks on CIFAR-10 and ImageNet."
3295,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes a method for online knowledge distillation based on adversarial training. The proposed method is based on a cyclic learning scheme, where a discriminator is trained to predict the feature map distribution of the network and a network is trained with the discriminator. The discriminator and the network are trained in a minimax game, where the network is optimized to maximize the class probabilities of the classifier. Experiments show that the proposed method outperforms the state-of-the-art methods."
3296,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes a method for online knowledge distillation based on adversarial training. The proposed method is based on a cyclic learning scheme, where a discriminator is trained to predict the feature map distribution of the network and a network is trained with the discriminator. The discriminator and the network are trained in a minimax game, where the network is optimized to maximize the class probabilities of the classifier. Experiments show that the proposed method outperforms the state-of-the-art methods."
3297,SP:9ca0b8d270e3fea3ba8f88c8f1ba50d8a8f7e4b8,"This paper proposes a method for online knowledge distillation based on adversarial training. The proposed method is based on a cyclic learning scheme, where a discriminator is trained to predict the feature map distribution of the network and a network is trained with the discriminator. The discriminator and the network are trained in a minimax game, where the network is optimized to maximize the class probabilities of the classifier. Experiments show that the proposed method outperforms the state-of-the-art methods."
3298,SP:e43fc8747f823be6497224696adb92d45150b02d,"This paper proposes a new model for sentiment word embeddings. The model is based on a combination of maximum likelihood estimation (MLE) and Bayesian estimation (BAE). The proposed model is evaluated on a variety of word embedding tasks, including semantic and sentiment analysis. The results show that the proposed model outperforms the baseline methods on most of the tasks."
3299,SP:e43fc8747f823be6497224696adb92d45150b02d,"This paper proposes a new model for sentiment word embeddings. The model is based on a combination of maximum likelihood estimation (MLE) and Bayesian estimation (BAE). The proposed model is evaluated on a variety of word embedding tasks, including semantic and sentiment analysis. The results show that the proposed model outperforms the baseline methods on most of the tasks."
3300,SP:e43fc8747f823be6497224696adb92d45150b02d,"This paper proposes a new model for sentiment word embeddings. The model is based on a combination of maximum likelihood estimation (MLE) and Bayesian estimation (BAE). The proposed model is evaluated on a variety of word embedding tasks, including semantic and sentiment analysis. The results show that the proposed model outperforms the baseline methods on most of the tasks."
3301,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method for noise-free training of deep neural networks. The first phase is to train the network on a large set of noisy labels, and the second phase consists of training the network with a small set of clean labels. The authors show that their method is able to outperform the state-of-the-art methods in terms of test error and training time."
3302,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method for noise-free training of deep neural networks. The first phase is to train the network on a large set of noisy labels, and the second phase consists of training the network with a small set of clean labels. The authors show that their method is able to outperform the state-of-the-art methods in terms of test error and training time."
3303,SP:72d32a2ae382f63e055ab3eafcc9276b10fba985,"This paper proposes a two-phase training method for noise-free training of deep neural networks. The first phase is to train the network on a large set of noisy labels, and the second phase consists of training the network with a small set of clean labels. The authors show that their method is able to outperform the state-of-the-art methods in terms of test error and training time."
3304,SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a method for learning representations of actions in reinforcement learning. The main idea is to learn a representation of an action, and then use this representation to train a policy that is able to generalize to unseen actions. The method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines."
3305,SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a method for learning representations of actions in reinforcement learning. The main idea is to learn a representation of an action, and then use this representation to train a policy that is able to generalize to unseen actions. The method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines."
3306,SP:8316872d8b388587bf25f724c80155b25b6cb68e,"This paper proposes a method for learning representations of actions in reinforcement learning. The main idea is to learn a representation of an action, and then use this representation to train a policy that is able to generalize to unseen actions. The method is evaluated on a variety of tasks, and the results show that the proposed method outperforms the baselines."
3307,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes a new method for learning vector word embeddings for word2vec and GloVe. The proposed method is based on the idea that the word embedding matrix can be expressed as a matrix of continuous vectors, which can then be used for inference. The authors show that the proposed method can be used to train a neural network on a large text corpus, and that it is able to achieve better performance than the state-of-the-art."
3308,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes a new method for learning vector word embeddings for word2vec and GloVe. The proposed method is based on the idea that the word embedding matrix can be expressed as a matrix of continuous vectors, which can then be used for inference. The authors show that the proposed method can be used to train a neural network on a large text corpus, and that it is able to achieve better performance than the state-of-the-art."
3309,SP:f534d51192eaacc6cb6bfd365e6d959d9dd498b2,"This paper proposes a new method for learning vector word embeddings for word2vec and GloVe. The proposed method is based on the idea that the word embedding matrix can be expressed as a matrix of continuous vectors, which can then be used for inference. The authors show that the proposed method can be used to train a neural network on a large text corpus, and that it is able to achieve better performance than the state-of-the-art."
3310,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning (IL) in the context of reinforcement learning (RL). The main idea is to train a neural network to imitate the behavior of a human agent in a video game. The method is based on a UCB1 algorithm, which is trained on a dataset of human demonstrations of the agent’s state-action pairs. The authors show that the proposed method outperforms the state-of-the-art IL baseline on StarCraft II."
3311,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning (IL) in the context of reinforcement learning (RL). The main idea is to train a neural network to imitate the behavior of a human agent in a video game. The method is based on a UCB1 algorithm, which is trained on a dataset of human demonstrations of the agent’s state-action pairs. The authors show that the proposed method outperforms the state-of-the-art IL baseline on StarCraft II."
3312,SP:3df499068ffe6c995457c2174f987cb0ae3c2551,"This paper proposes a method for imitation learning (IL) in the context of reinforcement learning (RL). The main idea is to train a neural network to imitate the behavior of a human agent in a video game. The method is based on a UCB1 algorithm, which is trained on a dataset of human demonstrations of the agent’s state-action pairs. The authors show that the proposed method outperforms the state-of-the-art IL baseline on StarCraft II."
3313,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis for sparsified neural networks. The authors propose a method to prune the weights of a sparsity-based neural network using iterative pruning. The method is based on the idea of lottery ticket theory, which is a well-known and well-studied topic in the literature. The main contribution of this paper is to study the effect of the weight magnitude of the sparsity of the network on the performance of the pruned network. The paper is well-written and easy to follow. The experiments on CIFAR-10, ImageNet and ResNet50 show that the proposed method outperforms the one-shot pruning method."
3314,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis for sparsified neural networks. The authors propose a method to prune the weights of a sparsity-based neural network using iterative pruning. The method is based on the idea of lottery ticket theory, which is a well-known and well-studied topic in the literature. The main contribution of this paper is to study the effect of the weight magnitude of the sparsity of the network on the performance of the pruned network. The paper is well-written and easy to follow. The experiments on CIFAR-10, ImageNet and ResNet50 show that the proposed method outperforms the one-shot pruning method."
3315,SP:db15d3cc3e95173ca6d4fd88313d89a739d1c910,"This paper studies the lottery ticket hypothesis for sparsified neural networks. The authors propose a method to prune the weights of a sparsity-based neural network using iterative pruning. The method is based on the idea of lottery ticket theory, which is a well-known and well-studied topic in the literature. The main contribution of this paper is to study the effect of the weight magnitude of the sparsity of the network on the performance of the pruned network. The paper is well-written and easy to follow. The experiments on CIFAR-10, ImageNet and ResNet50 show that the proposed method outperforms the one-shot pruning method."
3316,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes a novel method for low-confidence detection of unknowns in image classification systems. The proposed method is based on the idea of unsupervised product operation (UDN), which is an extension of the product operation in CNNs. The main difference between UDN and CNNs is that UDN uses an information-theoretic regularization strategy to regularize the learning process of UDN. The authors show that the proposed method outperforms the state-of-the-art methods on CIFAR-10, Cifar-100, SVHN, and MNIST datasets."
3317,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes a novel method for low-confidence detection of unknowns in image classification systems. The proposed method is based on the idea of unsupervised product operation (UDN), which is an extension of the product operation in CNNs. The main difference between UDN and CNNs is that UDN uses an information-theoretic regularization strategy to regularize the learning process of UDN. The authors show that the proposed method outperforms the state-of-the-art methods on CIFAR-10, Cifar-100, SVHN, and MNIST datasets."
3318,SP:06d2a46282e34302050e81a1be8a2627acb159ee,"This paper proposes a novel method for low-confidence detection of unknowns in image classification systems. The proposed method is based on the idea of unsupervised product operation (UDN), which is an extension of the product operation in CNNs. The main difference between UDN and CNNs is that UDN uses an information-theoretic regularization strategy to regularize the learning process of UDN. The authors show that the proposed method outperforms the state-of-the-art methods on CIFAR-10, Cifar-100, SVHN, and MNIST datasets."
3319,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a new variational inference method for approximate Bayesian inference. The main idea is to use the Evidence Lower BOund (ELBOund) to approximate the distribution of the variational family. The authors show that the proposed method can be applied to a wide range of variational distributions, and show that it outperforms the existing methods on a variety of benchmarks. The paper is well-written and easy to follow."
3320,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a new variational inference method for approximate Bayesian inference. The main idea is to use the Evidence Lower BOund (ELBOund) to approximate the distribution of the variational family. The authors show that the proposed method can be applied to a wide range of variational distributions, and show that it outperforms the existing methods on a variety of benchmarks. The paper is well-written and easy to follow."
3321,SP:fa3e729469e74cac44745008fe65c01cc97c9820,"This paper proposes a new variational inference method for approximate Bayesian inference. The main idea is to use the Evidence Lower BOund (ELBOund) to approximate the distribution of the variational family. The authors show that the proposed method can be applied to a wide range of variational distributions, and show that it outperforms the existing methods on a variety of benchmarks. The paper is well-written and easy to follow."
3322,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,This paper proposes a novel method to control the variance of the policy gradient of a sequence generation model. The proposed method is based on correlated Monte Carlo (MC) rollouts. The authors propose a policy gradient estimator for contextual generation of categorical sequences. The method is evaluated on a set of binary-tree softmax models and neural program synthesis tasks. The experiments show that the proposed method outperforms the baselines.
3323,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,This paper proposes a novel method to control the variance of the policy gradient of a sequence generation model. The proposed method is based on correlated Monte Carlo (MC) rollouts. The authors propose a policy gradient estimator for contextual generation of categorical sequences. The method is evaluated on a set of binary-tree softmax models and neural program synthesis tasks. The experiments show that the proposed method outperforms the baselines.
3324,SP:8e20d28a2a3a6f8f0b6a29a09a10fb8c7a011e86,This paper proposes a novel method to control the variance of the policy gradient of a sequence generation model. The proposed method is based on correlated Monte Carlo (MC) rollouts. The authors propose a policy gradient estimator for contextual generation of categorical sequences. The method is evaluated on a set of binary-tree softmax models and neural program synthesis tasks. The experiments show that the proposed method outperforms the baselines.
3325,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,This paper proposes a method for online goal recognition. The main idea is to train a discriminator that is able to distinguish between good and bad paths in the goal recognition process. The discriminator is trained by minimizing the worst-case distinctiveness (wcd) between the path and the path of the opponent. The authors show that the discriminator can be trained using a combination of two techniques: (1) a soft-interdiction method that is based on the idea that the goal should be differentiable and (2) a hard action removal method that aims to minimize the wcd between the paths of the two paths. The proposed method is evaluated on a variety of goal recognition tasks and compared to several baselines.
3326,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,This paper proposes a method for online goal recognition. The main idea is to train a discriminator that is able to distinguish between good and bad paths in the goal recognition process. The discriminator is trained by minimizing the worst-case distinctiveness (wcd) between the path and the path of the opponent. The authors show that the discriminator can be trained using a combination of two techniques: (1) a soft-interdiction method that is based on the idea that the goal should be differentiable and (2) a hard action removal method that aims to minimize the wcd between the paths of the two paths. The proposed method is evaluated on a variety of goal recognition tasks and compared to several baselines.
3327,SP:ab51af66e626b1b03bbf0de7a5237370e941925c,This paper proposes a method for online goal recognition. The main idea is to train a discriminator that is able to distinguish between good and bad paths in the goal recognition process. The discriminator is trained by minimizing the worst-case distinctiveness (wcd) between the path and the path of the opponent. The authors show that the discriminator can be trained using a combination of two techniques: (1) a soft-interdiction method that is based on the idea that the goal should be differentiable and (2) a hard action removal method that aims to minimize the wcd between the paths of the two paths. The proposed method is evaluated on a variety of goal recognition tasks and compared to several baselines.
3328,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method for active learning in GANs. The method is based on Coreset-selection, which is a popular method for training GAN models with large mini-batch sizes. The main idea is to select a subset of samples from the training set to be used in the active learning phase. The authors show that the proposed method can reduce the training time and memory usage of the GAN. The proposed method is evaluated on a synthetic dataset and on anomaly detection tasks."
3329,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method for active learning in GANs. The method is based on Coreset-selection, which is a popular method for training GAN models with large mini-batch sizes. The main idea is to select a subset of samples from the training set to be used in the active learning phase. The authors show that the proposed method can reduce the training time and memory usage of the GAN. The proposed method is evaluated on a synthetic dataset and on anomaly detection tasks."
3330,SP:e9d173bdf0b650fd093226cfb4607032c905cf61,"This paper proposes a method for active learning in GANs. The method is based on Coreset-selection, which is a popular method for training GAN models with large mini-batch sizes. The main idea is to select a subset of samples from the training set to be used in the active learning phase. The authors show that the proposed method can reduce the training time and memory usage of the GAN. The proposed method is evaluated on a synthetic dataset and on anomaly detection tasks."
3331,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper studies the relationship between maximum likelihood and RNNs in the information plane of biological neurons. The authors show that maximum likelihood can be used as a proxy for predictive information in biological neurons, and that contrastive loss training can help improve the performance of RNN. They also show that the predictive information can be leveraged for maximum likelihood training and contrastive learning. The paper also shows that the RNN can be trained with noise in the hidden state."
3332,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper studies the relationship between maximum likelihood and RNNs in the information plane of biological neurons. The authors show that maximum likelihood can be used as a proxy for predictive information in biological neurons, and that contrastive loss training can help improve the performance of RNN. They also show that the predictive information can be leveraged for maximum likelihood training and contrastive learning. The paper also shows that the RNN can be trained with noise in the hidden state."
3333,SP:f174ef07670a31a3ce647910c59040a19ea52d7a,"This paper studies the relationship between maximum likelihood and RNNs in the information plane of biological neurons. The authors show that maximum likelihood can be used as a proxy for predictive information in biological neurons, and that contrastive loss training can help improve the performance of RNN. They also show that the predictive information can be leveraged for maximum likelihood training and contrastive learning. The paper also shows that the RNN can be trained with noise in the hidden state."
3334,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper studies the problem of Q-learning in the presence of delusional bias. The authors propose a method to penalize the Q-labels of a greedy policy class. The proposed method is based on the idea that Q-labeling can be used as a way to find a policy class that is more expressive than the original policy class, which is then used to train a Q-value estimator. The method is evaluated on several Atari games."
3335,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper studies the problem of Q-learning in the presence of delusional bias. The authors propose a method to penalize the Q-labels of a greedy policy class. The proposed method is based on the idea that Q-labeling can be used as a way to find a policy class that is more expressive than the original policy class, which is then used to train a Q-value estimator. The method is evaluated on several Atari games."
3336,SP:f87a75fa12ddeb7538c4522d025e679f2c6dd237,"This paper studies the problem of Q-learning in the presence of delusional bias. The authors propose a method to penalize the Q-labels of a greedy policy class. The proposed method is based on the idea that Q-labeling can be used as a way to find a policy class that is more expressive than the original policy class, which is then used to train a Q-value estimator. The method is evaluated on several Atari games."
3337,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model for unsupervised object-oriented scene representation learning. The proposed model is a combination of spatial-attention and scene-mixture approaches. The main contribution of this paper is to combine spatial attention and scene mixture in a unified probabilistic modeling framework. The experimental results show that the proposed model outperforms the state-of-the-art IODINE, GENESIS, and SPAIR."
3338,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model for unsupervised object-oriented scene representation learning. The proposed model is a combination of spatial-attention and scene-mixture approaches. The main contribution of this paper is to combine spatial attention and scene mixture in a unified probabilistic modeling framework. The experimental results show that the proposed model outperforms the state-of-the-art IODINE, GENESIS, and SPAIR."
3339,SP:827b0d2e2e3cf434c02b7f221bb9b2e0388e48b8,"This paper proposes a generative latent variable model for unsupervised object-oriented scene representation learning. The proposed model is a combination of spatial-attention and scene-mixture approaches. The main contribution of this paper is to combine spatial attention and scene mixture in a unified probabilistic modeling framework. The experimental results show that the proposed model outperforms the state-of-the-art IODINE, GENESIS, and SPAIR."
3340,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a method for compressing convolutional neural networks (CNNs). The proposed method is based on a mathematical formulation of the convolution kernel (EHP), which is an extension of depthwise separable convolution (DCP). The authors show that EHP can be used to compress CNNs. The authors also show that the proposed method can be combined with other convolution methods such as depthwise convolution and pointwise convolutions. The experiments show that FALCON outperforms the existing methods on a variety of tasks."
3341,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a method for compressing convolutional neural networks (CNNs). The proposed method is based on a mathematical formulation of the convolution kernel (EHP), which is an extension of depthwise separable convolution (DCP). The authors show that EHP can be used to compress CNNs. The authors also show that the proposed method can be combined with other convolution methods such as depthwise convolution and pointwise convolutions. The experiments show that FALCON outperforms the existing methods on a variety of tasks."
3342,SP:73d7d614378cbb6a8d7347dca790675674e0eadb,"This paper proposes a method for compressing convolutional neural networks (CNNs). The proposed method is based on a mathematical formulation of the convolution kernel (EHP), which is an extension of depthwise separable convolution (DCP). The authors show that EHP can be used to compress CNNs. The authors also show that the proposed method can be combined with other convolution methods such as depthwise convolution and pointwise convolutions. The experiments show that FALCON outperforms the existing methods on a variety of tasks."
3343,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,This paper proposes a new normalization method for training neural networks. The proposed method is based on a combination of batch normalization and group normalization. The authors show that the proposed method can reduce the inference discrepancy between small and medium batch sizes. They also provide a theoretical analysis of the effect of the weight decay regularization.
3344,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,This paper proposes a new normalization method for training neural networks. The proposed method is based on a combination of batch normalization and group normalization. The authors show that the proposed method can reduce the inference discrepancy between small and medium batch sizes. They also provide a theoretical analysis of the effect of the weight decay regularization.
3345,SP:35d45ed014320d8dff22f3531f805d15fa91dafb,This paper proposes a new normalization method for training neural networks. The proposed method is based on a combination of batch normalization and group normalization. The authors show that the proposed method can reduce the inference discrepancy between small and medium batch sizes. They also provide a theoretical analysis of the effect of the weight decay regularization.
3346,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"This paper studies the problem of manual inspection of raw data in federated learning. The authors propose a new algorithm for differentially private federated GANs. The main idea is to train a generative model on the raw data, and then use the generated data to train the GAN. The proposed algorithm is evaluated on text and image datasets, and the authors show that the proposed algorithm outperforms the baselines."
3347,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"This paper studies the problem of manual inspection of raw data in federated learning. The authors propose a new algorithm for differentially private federated GANs. The main idea is to train a generative model on the raw data, and then use the generated data to train the GAN. The proposed algorithm is evaluated on text and image datasets, and the authors show that the proposed algorithm outperforms the baselines."
3348,SP:39d187474524c6b7de1ce6fd811ec53edae0a8fc,"This paper studies the problem of manual inspection of raw data in federated learning. The authors propose a new algorithm for differentially private federated GANs. The main idea is to train a generative model on the raw data, and then use the generated data to train the GAN. The proposed algorithm is evaluated on text and image datasets, and the authors show that the proposed algorithm outperforms the baselines."
3349,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,This paper proposes a method for generating long-range diverse and distinctive behaviors of human skeleton. The proposed method is based on a memory bank that stores motion references and a deep network that synthesizes the skeleton from the memory bank. The method is evaluated on two skeleton datasets and compared to several parametric and non-parametric baselines.
3350,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,This paper proposes a method for generating long-range diverse and distinctive behaviors of human skeleton. The proposed method is based on a memory bank that stores motion references and a deep network that synthesizes the skeleton from the memory bank. The method is evaluated on two skeleton datasets and compared to several parametric and non-parametric baselines.
3351,SP:1931ec4c3cd0dbb411cf1bc0f9776b7e26e3ad78,This paper proposes a method for generating long-range diverse and distinctive behaviors of human skeleton. The proposed method is based on a memory bank that stores motion references and a deep network that synthesizes the skeleton from the memory bank. The method is evaluated on two skeleton datasets and compared to several parametric and non-parametric baselines.
3352,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper studies the problem of hierarchical explanation of neural network predictions. The authors propose two algorithms for hierarchical explanation: Sampling and Contextual Decomposition (SCD) and Occlusion (SOC) which are based on sampling and occlusion. The main contribution of the paper is the introduction of the SCD algorithm and SOC algorithm, which is based on the idea of contextual decomposition. The experiments show that the proposed SCD and SOC algorithms outperform existing hierarchical explanation algorithms in terms of human and metrics evaluation. "
3353,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper studies the problem of hierarchical explanation of neural network predictions. The authors propose two algorithms for hierarchical explanation: Sampling and Contextual Decomposition (SCD) and Occlusion (SOC) which are based on sampling and occlusion. The main contribution of the paper is the introduction of the SCD algorithm and SOC algorithm, which is based on the idea of contextual decomposition. The experiments show that the proposed SCD and SOC algorithms outperform existing hierarchical explanation algorithms in terms of human and metrics evaluation. "
3354,SP:3a09bdf2e5a17d271f890fd28113202afb9ae761,"This paper studies the problem of hierarchical explanation of neural network predictions. The authors propose two algorithms for hierarchical explanation: Sampling and Contextual Decomposition (SCD) and Occlusion (SOC) which are based on sampling and occlusion. The main contribution of the paper is the introduction of the SCD algorithm and SOC algorithm, which is based on the idea of contextual decomposition. The experiments show that the proposed SCD and SOC algorithms outperform existing hierarchical explanation algorithms in terms of human and metrics evaluation. "
3355,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"This paper proposes a method for learning saliency maps for deep convolutional neural networks (CNNs). The proposed method is based on the idea of order equivalence between saliency map order and the scale information of the network. The authors argue that the order of scale information in the network can be used to estimate the scale and layer contributions of a network. To do so, the authors propose a method called Layer Ordered Visualization of Information (LOVI), which is a combination of Grad-CAM and Smooth Grad-AMC++. The method is evaluated on a variety of image classification tasks and compared to the state-of-the-art methods."
3356,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"This paper proposes a method for learning saliency maps for deep convolutional neural networks (CNNs). The proposed method is based on the idea of order equivalence between saliency map order and the scale information of the network. The authors argue that the order of scale information in the network can be used to estimate the scale and layer contributions of a network. To do so, the authors propose a method called Layer Ordered Visualization of Information (LOVI), which is a combination of Grad-CAM and Smooth Grad-AMC++. The method is evaluated on a variety of image classification tasks and compared to the state-of-the-art methods."
3357,SP:9ec1740e58d1b07a6b1c6130ec7e23c370efb701,"This paper proposes a method for learning saliency maps for deep convolutional neural networks (CNNs). The proposed method is based on the idea of order equivalence between saliency map order and the scale information of the network. The authors argue that the order of scale information in the network can be used to estimate the scale and layer contributions of a network. To do so, the authors propose a method called Layer Ordered Visualization of Information (LOVI), which is a combination of Grad-CAM and Smooth Grad-AMC++. The method is evaluated on a variety of image classification tasks and compared to the state-of-the-art methods."
3358,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation based on position modeling. Specifically, the authors propose to model the position of the generated words as a latent variable, and use the latent variable to train the model. The proposed method is evaluated on two tasks: machine translation and paraphrase generation. The results show that the proposed method outperforms the baselines on both tasks."
3359,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation based on position modeling. Specifically, the authors propose to model the position of the generated words as a latent variable, and use the latent variable to train the model. The proposed method is evaluated on two tasks: machine translation and paraphrase generation. The results show that the proposed method outperforms the baselines on both tasks."
3360,SP:7f11fa931f4085f7227cc870eba4a3aac4b1bf42,"This paper proposes a method for non-autoregressive text generation based on position modeling. Specifically, the authors propose to model the position of the generated words as a latent variable, and use the latent variable to train the model. The proposed method is evaluated on two tasks: machine translation and paraphrase generation. The results show that the proposed method outperforms the baselines on both tasks."
3361,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,This paper proposes a generative model analysis method based on random path generative adversarial networks (RPGAN). The main idea is to use random paths in the latent space of the generator network to identify factors of variation in the image generation process. The authors show that the generated images are more interpretable and interpretable than the original image generated by the original GAN. The main contribution of this paper is to propose a novel random path generator network based on RPGAN. 
3362,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,This paper proposes a generative model analysis method based on random path generative adversarial networks (RPGAN). The main idea is to use random paths in the latent space of the generator network to identify factors of variation in the image generation process. The authors show that the generated images are more interpretable and interpretable than the original image generated by the original GAN. The main contribution of this paper is to propose a novel random path generator network based on RPGAN. 
3363,SP:e1c40112901b6ff905ae0e221fd3df4f545acd08,This paper proposes a generative model analysis method based on random path generative adversarial networks (RPGAN). The main idea is to use random paths in the latent space of the generator network to identify factors of variation in the image generation process. The authors show that the generated images are more interpretable and interpretable than the original image generated by the original GAN. The main contribution of this paper is to propose a novel random path generator network based on RPGAN. 
3364,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes a method for learning spherical convolutional networks with anisotropic filters. The main idea is to use a graph representation of the sampled sphere as the input to the network, and then use the graph representation as the output of the network. The authors show that the proposed method is able to achieve better efficiency and rotation equivariance compared to existing methods. "
3365,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes a method for learning spherical convolutional networks with anisotropic filters. The main idea is to use a graph representation of the sampled sphere as the input to the network, and then use the graph representation as the output of the network. The authors show that the proposed method is able to achieve better efficiency and rotation equivariance compared to existing methods. "
3366,SP:1fec5468baaccb4a956399a829b62ac47494a6ac,"This paper proposes a method for learning spherical convolutional networks with anisotropic filters. The main idea is to use a graph representation of the sampled sphere as the input to the network, and then use the graph representation as the output of the network. The authors show that the proposed method is able to achieve better efficiency and rotation equivariance compared to existing methods. "
3367,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,This paper studies the problem of learning invariant representations in the presence of domain adaptation. The authors propose a new benchmark to evaluate the adaptability of weighting representations to different domains. The main contribution of this paper is to provide a bound on the risk of compression of the learned representation under the constraint of invariance. The paper is well-written and easy to follow.
3368,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,This paper studies the problem of learning invariant representations in the presence of domain adaptation. The authors propose a new benchmark to evaluate the adaptability of weighting representations to different domains. The main contribution of this paper is to provide a bound on the risk of compression of the learned representation under the constraint of invariance. The paper is well-written and easy to follow.
3369,SP:ae544fa9abd539e0c2e77fdb5541f5c5194feb9f,This paper studies the problem of learning invariant representations in the presence of domain adaptation. The authors propose a new benchmark to evaluate the adaptability of weighting representations to different domains. The main contribution of this paper is to provide a bound on the risk of compression of the learned representation under the constraint of invariance. The paper is well-written and easy to follow.
3370,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for inferring user interface attribute values from a synthetic dataset. The method is based on imitation learning, where the model is trained to imitate the user interface. The authors show that their method is able to achieve state-of-the-art pixel-level accuracy on a real-world Google Play Store application."
3371,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for inferring user interface attribute values from a synthetic dataset. The method is based on imitation learning, where the model is trained to imitate the user interface. The authors show that their method is able to achieve state-of-the-art pixel-level accuracy on a real-world Google Play Store application."
3372,SP:39126802d517f93bdcbc47708a6aa1ed13bf2800,"This paper proposes a method for inferring user interface attribute values from a synthetic dataset. The method is based on imitation learning, where the model is trained to imitate the user interface. The authors show that their method is able to achieve state-of-the-art pixel-level accuracy on a real-world Google Play Store application."
3373,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning in the context of adversarially trained models. Specifically, the authors study the effect of adversarial training on the robustness of the model. The authors show that robustness to adversarial perturbations can be achieved by using robust feature extractors, which can be used as a regularizer to improve the performance of the network. The paper also studies the impact of the regularizer on the generalization ability of robust models."
3374,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning in the context of adversarially trained models. Specifically, the authors study the effect of adversarial training on the robustness of the model. The authors show that robustness to adversarial perturbations can be achieved by using robust feature extractors, which can be used as a regularizer to improve the performance of the network. The paper also studies the impact of the regularizer on the generalization ability of robust models."
3375,SP:1c5d31363faf2b8c43f2698ad426bfffcc02ad03,"This paper studies the problem of robust transfer learning in the context of adversarially trained models. Specifically, the authors study the effect of adversarial training on the robustness of the model. The authors show that robustness to adversarial perturbations can be achieved by using robust feature extractors, which can be used as a regularizer to improve the performance of the network. The paper also studies the impact of the regularizer on the generalization ability of robust models."
3376,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,This paper proposes a neural iterated learning (NIL) algorithm to learn a structured type of language. The authors propose a probabilistic model of NIL and show that it can be used to train a neural agent to communicate with other agents in a language game. They show that NIL can learn a language that is compositional and expressive. They also show that the proposed method can be applied to a variety of language games.
3377,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,This paper proposes a neural iterated learning (NIL) algorithm to learn a structured type of language. The authors propose a probabilistic model of NIL and show that it can be used to train a neural agent to communicate with other agents in a language game. They show that NIL can learn a language that is compositional and expressive. They also show that the proposed method can be applied to a variety of language games.
3378,SP:06a047ae70a1a25dc6e8f317d6e492e211ad17ce,This paper proposes a neural iterated learning (NIL) algorithm to learn a structured type of language. The authors propose a probabilistic model of NIL and show that it can be used to train a neural agent to communicate with other agents in a language game. They show that NIL can learn a language that is compositional and expressive. They also show that the proposed method can be applied to a variety of language games.
3379,SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes AdVIL, a black-box algorithm for learning a Markov random field (MRF). The main idea is to use a variational distribution to approximate the partition function of the MRF, and then use stochastic gradient descent to minimize the negative log-likelihood of the log partition function. Experiments are conducted on MNIST and CIFAR-10 datasets."
3380,SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes AdVIL, a black-box algorithm for learning a Markov random field (MRF). The main idea is to use a variational distribution to approximate the partition function of the MRF, and then use stochastic gradient descent to minimize the negative log-likelihood of the log partition function. Experiments are conducted on MNIST and CIFAR-10 datasets."
3381,SP:add48154b31c13f48aef740e665f23694fa83681,"This paper proposes AdVIL, a black-box algorithm for learning a Markov random field (MRF). The main idea is to use a variational distribution to approximate the partition function of the MRF, and then use stochastic gradient descent to minimize the negative log-likelihood of the log partition function. Experiments are conducted on MNIST and CIFAR-10 datasets."
3382,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,This paper proposes a method for goal-conditioned reinforcement learning in continuous state spaces. The method is based on the idea of using an indicator reward function to encourage the agent to reach the ground-truth state of the environment. The authors show that the proposed method converges to the ground truth state faster than existing methods that use reward balancing and reward filtering. They also show that their method can be applied to a variety of tasks.
3383,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,This paper proposes a method for goal-conditioned reinforcement learning in continuous state spaces. The method is based on the idea of using an indicator reward function to encourage the agent to reach the ground-truth state of the environment. The authors show that the proposed method converges to the ground truth state faster than existing methods that use reward balancing and reward filtering. They also show that their method can be applied to a variety of tasks.
3384,SP:b875f6417663e43dded41b6a6f1b9ab49ad954a2,This paper proposes a method for goal-conditioned reinforcement learning in continuous state spaces. The method is based on the idea of using an indicator reward function to encourage the agent to reach the ground-truth state of the environment. The authors show that the proposed method converges to the ground truth state faster than existing methods that use reward balancing and reward filtering. They also show that their method can be applied to a variety of tasks.
3385,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper studies the problem of certified robustness verification for neural networks. The authors propose a new method for certifying the robustness of a neural network based on the self-attention layer of a Transformer. The proposed method is based on a simple extension of the interval bound propagation method, which is used in previous work. The main contribution of this paper is to propose a method that can be used to certify robustness bounds for a wide range of neural networks, including Transformers. The method is evaluated on sentiment analysis tasks, and the authors show that the proposed method outperforms the naive method."
3386,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper studies the problem of certified robustness verification for neural networks. The authors propose a new method for certifying the robustness of a neural network based on the self-attention layer of a Transformer. The proposed method is based on a simple extension of the interval bound propagation method, which is used in previous work. The main contribution of this paper is to propose a method that can be used to certify robustness bounds for a wide range of neural networks, including Transformers. The method is evaluated on sentiment analysis tasks, and the authors show that the proposed method outperforms the naive method."
3387,SP:8ae78a6640be13e511242eab64101f74ebc4b30a,"This paper studies the problem of certified robustness verification for neural networks. The authors propose a new method for certifying the robustness of a neural network based on the self-attention layer of a Transformer. The proposed method is based on a simple extension of the interval bound propagation method, which is used in previous work. The main contribution of this paper is to propose a method that can be used to certify robustness bounds for a wide range of neural networks, including Transformers. The method is evaluated on sentiment analysis tasks, and the authors show that the proposed method outperforms the naive method."
3388,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a weakly supervised pretraining objective for self-supervised learning (SSL) based on a zero-shot fact completion task. The proposed objective is motivated by the observation that pretrained language models are not able to generalize well to new tasks. The authors propose to use the fact completion objective to train a pretrained model on a small subset of entity-related question answering datasets and show that the proposed objective can improve the performance of pretrained models on these tasks. In addition, the authors propose a fine-grained entity typing dataset to evaluate the effectiveness of the proposed method."
3389,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a weakly supervised pretraining objective for self-supervised learning (SSL) based on a zero-shot fact completion task. The proposed objective is motivated by the observation that pretrained language models are not able to generalize well to new tasks. The authors propose to use the fact completion objective to train a pretrained model on a small subset of entity-related question answering datasets and show that the proposed objective can improve the performance of pretrained models on these tasks. In addition, the authors propose a fine-grained entity typing dataset to evaluate the effectiveness of the proposed method."
3390,SP:92cb7b1e88f3c8883ae6123c19e1ba24622464e6,"This paper proposes a weakly supervised pretraining objective for self-supervised learning (SSL) based on a zero-shot fact completion task. The proposed objective is motivated by the observation that pretrained language models are not able to generalize well to new tasks. The authors propose to use the fact completion objective to train a pretrained model on a small subset of entity-related question answering datasets and show that the proposed objective can improve the performance of pretrained models on these tasks. In addition, the authors propose a fine-grained entity typing dataset to evaluate the effectiveness of the proposed method."
3391,SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper proposes a general-purpose clustering model for novel category discovery. The proposed model is a combination of self-supervised learning and clustering of unlabelled subsets of the data. The main idea is to learn a joint objective function for the labeled data and unlabeled subsets, and then use this objective function to compute the rank statistics of the unlabelling subsets. The authors show that the proposed model outperforms existing methods on a variety of classification benchmarks."
3392,SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper proposes a general-purpose clustering model for novel category discovery. The proposed model is a combination of self-supervised learning and clustering of unlabelled subsets of the data. The main idea is to learn a joint objective function for the labeled data and unlabeled subsets, and then use this objective function to compute the rank statistics of the unlabelling subsets. The authors show that the proposed model outperforms existing methods on a variety of classification benchmarks."
3393,SP:4395d6f3e197df478eee84e092539dc370babd97,"This paper proposes a general-purpose clustering model for novel category discovery. The proposed model is a combination of self-supervised learning and clustering of unlabelled subsets of the data. The main idea is to learn a joint objective function for the labeled data and unlabeled subsets, and then use this objective function to compute the rank statistics of the unlabelling subsets. The authors show that the proposed model outperforms existing methods on a variety of classification benchmarks."
3394,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,This paper proposes a new method for visual planning (VP) based on semiparametric topological memory (SPTM). The proposed method is based on the idea of hallucinated topology memory (HTM) and uses a contrastive predictive coding (CPC) model to learn the energy function of the connectivity graph of the hallucinated samples. The authors show that the proposed method outperforms the state-of-the-art visual planning methods on several simulated domains.
3395,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,This paper proposes a new method for visual planning (VP) based on semiparametric topological memory (SPTM). The proposed method is based on the idea of hallucinated topology memory (HTM) and uses a contrastive predictive coding (CPC) model to learn the energy function of the connectivity graph of the hallucinated samples. The authors show that the proposed method outperforms the state-of-the-art visual planning methods on several simulated domains.
3396,SP:068c4e93c135968aef2637d2bfcba727a3c0f001,This paper proposes a new method for visual planning (VP) based on semiparametric topological memory (SPTM). The proposed method is based on the idea of hallucinated topology memory (HTM) and uses a contrastive predictive coding (CPC) model to learn the energy function of the connectivity graph of the hallucinated samples. The authors show that the proposed method outperforms the state-of-the-art visual planning methods on several simulated domains.
3397,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes a new algorithm for bias reduction. The proposed algorithm, AFLITE, is based on Adversarial Filters (AFOPTIMUM), which is an iterative greedy algorithm. The main idea of the algorithm is to use the K-nearest-neighbor (KNN) objective to reduce the bias of the target dataset. The KNN objective is a weighted sum of the KL divergence between the target and the KNN. The authors show that the proposed algorithm is able to reduce bias by a factor of 1.5 on several datasets."
3398,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes a new algorithm for bias reduction. The proposed algorithm, AFLITE, is based on Adversarial Filters (AFOPTIMUM), which is an iterative greedy algorithm. The main idea of the algorithm is to use the K-nearest-neighbor (KNN) objective to reduce the bias of the target dataset. The KNN objective is a weighted sum of the KL divergence between the target and the KNN. The authors show that the proposed algorithm is able to reduce bias by a factor of 1.5 on several datasets."
3399,SP:907d92896eda706e1526debb5a87b41bb1e978e0,"This paper proposes a new algorithm for bias reduction. The proposed algorithm, AFLITE, is based on Adversarial Filters (AFOPTIMUM), which is an iterative greedy algorithm. The main idea of the algorithm is to use the K-nearest-neighbor (KNN) objective to reduce the bias of the target dataset. The KNN objective is a weighted sum of the KL divergence between the target and the KNN. The authors show that the proposed algorithm is able to reduce bias by a factor of 1.5 on several datasets."
3400,SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper studies the problem of few-shot classification. The authors propose a new decision metric, which is based on the softmax of the distance between the support examples and the centroid of the prototypical network. The main idea is to use batch normalization to make the centroids of support examples closer to each other, and then use a Gaussian layer to compute the distance to the centropole of the prototype network. They show that the proposed metric can be used to compare the accuracy of different models on the Omniglot and MiniImageNet datasets."
3401,SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper studies the problem of few-shot classification. The authors propose a new decision metric, which is based on the softmax of the distance between the support examples and the centroid of the prototypical network. The main idea is to use batch normalization to make the centroids of support examples closer to each other, and then use a Gaussian layer to compute the distance to the centropole of the prototype network. They show that the proposed metric can be used to compare the accuracy of different models on the Omniglot and MiniImageNet datasets."
3402,SP:82777947d2377efa897c6905261f5375b29a4c19,"This paper studies the problem of few-shot classification. The authors propose a new decision metric, which is based on the softmax of the distance between the support examples and the centroid of the prototypical network. The main idea is to use batch normalization to make the centroids of support examples closer to each other, and then use a Gaussian layer to compute the distance to the centropole of the prototype network. They show that the proposed metric can be used to compare the accuracy of different models on the Omniglot and MiniImageNet datasets."
3403,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes a multi-level framework for graph embedding. The proposed GraphZoom combines graph fusion, node attribute information, and graph topology. The authors evaluate the proposed method on several graph datasets and show that it outperforms the state-of-the-art methods."
3404,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes a multi-level framework for graph embedding. The proposed GraphZoom combines graph fusion, node attribute information, and graph topology. The authors evaluate the proposed method on several graph datasets and show that it outperforms the state-of-the-art methods."
3405,SP:4a6df2b39643f548dab806a0b128fe5a3ce4dadc,"This paper proposes a multi-level framework for graph embedding. The proposed GraphZoom combines graph fusion, node attribute information, and graph topology. The authors evaluate the proposed method on several graph datasets and show that it outperforms the state-of-the-art methods."
3406,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a new method for image compression based on a unified probabilistic model. The model is based on an autoregressive image generation model with a relative predictor. The relative predictor is trained to predict the absolute pixel intensities of the image, and the model is trained on the relative predictor to generate smooth transitions. The authors show that the proposed method outperforms the absolute predictor on unconditional image generation, unconditional image colorization, and super-resolution."
3407,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a new method for image compression based on a unified probabilistic model. The model is based on an autoregressive image generation model with a relative predictor. The relative predictor is trained to predict the absolute pixel intensities of the image, and the model is trained on the relative predictor to generate smooth transitions. The authors show that the proposed method outperforms the absolute predictor on unconditional image generation, unconditional image colorization, and super-resolution."
3408,SP:e33a92e3a6acc668fa2022237e6d947b2eb8bd76,"This paper proposes a new method for image compression based on a unified probabilistic model. The model is based on an autoregressive image generation model with a relative predictor. The relative predictor is trained to predict the absolute pixel intensities of the image, and the model is trained on the relative predictor to generate smooth transitions. The authors show that the proposed method outperforms the absolute predictor on unconditional image generation, unconditional image colorization, and super-resolution."
3409,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper proposes a variational divergence minimization method for estimating the transition operator of a Markov chain. The authors show that the proposed method can be applied to both stationary and empirical distributions. The proposed method is evaluated on several real-world problems, including off-policy policy evaluation and PageRank."
3410,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper proposes a variational divergence minimization method for estimating the transition operator of a Markov chain. The authors show that the proposed method can be applied to both stationary and empirical distributions. The proposed method is evaluated on several real-world problems, including off-policy policy evaluation and PageRank."
3411,SP:4224604c2650710cdf5be3ab8acc67c891944bed,"This paper proposes a variational divergence minimization method for estimating the transition operator of a Markov chain. The authors show that the proposed method can be applied to both stationary and empirical distributions. The proposed method is evaluated on several real-world problems, including off-policy policy evaluation and PageRank."
3412,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper studies the problem of classifying spurious features in natural language processing (NLP) models. The authors propose a new term called “internal coherence” to measure the coherence between the original and manipulated data, which they call “direct or indirect causal effects”. This term is motivated by the observation that classifiers trained on the original or manipulated data are more likely to have spurious features than models trained on original data. They show that the internal coherence of classifiers can be measured by the number of spurious features that are present in the original data compared to the spurious features learned by the classifier. They also provide a theoretical analysis of the relationship between the internal consistency of the classifiers and the causal effects."
3413,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper studies the problem of classifying spurious features in natural language processing (NLP) models. The authors propose a new term called “internal coherence” to measure the coherence between the original and manipulated data, which they call “direct or indirect causal effects”. This term is motivated by the observation that classifiers trained on the original or manipulated data are more likely to have spurious features than models trained on original data. They show that the internal coherence of classifiers can be measured by the number of spurious features that are present in the original data compared to the spurious features learned by the classifier. They also provide a theoretical analysis of the relationship between the internal consistency of the classifiers and the causal effects."
3414,SP:235998cafe7b558b6f6cf6c49b689ce84004af5d,"This paper studies the problem of classifying spurious features in natural language processing (NLP) models. The authors propose a new term called “internal coherence” to measure the coherence between the original and manipulated data, which they call “direct or indirect causal effects”. This term is motivated by the observation that classifiers trained on the original or manipulated data are more likely to have spurious features than models trained on original data. They show that the internal coherence of classifiers can be measured by the number of spurious features that are present in the original data compared to the spurious features learned by the classifier. They also provide a theoretical analysis of the relationship between the internal consistency of the classifiers and the causal effects."
3415,SP:b720eb5b6e44473a9392cc572af89270019d4c42,"This paper studies the frequency and orientation tuning of channels in CNNs for image classification tasks. The authors show that the spatial frequency and the orientation selectivity of CNN channels are highly correlated with human visual perception. They also show that contrast masking thresholds depend on the spatial frequencies and orientation selective filters. Finally, the authors conduct experiments on VGG-16 and CIFAR-10 to demonstrate the effectiveness of the proposed method."
3416,SP:b720eb5b6e44473a9392cc572af89270019d4c42,"This paper studies the frequency and orientation tuning of channels in CNNs for image classification tasks. The authors show that the spatial frequency and the orientation selectivity of CNN channels are highly correlated with human visual perception. They also show that contrast masking thresholds depend on the spatial frequencies and orientation selective filters. Finally, the authors conduct experiments on VGG-16 and CIFAR-10 to demonstrate the effectiveness of the proposed method."
3417,SP:b720eb5b6e44473a9392cc572af89270019d4c42,"This paper studies the frequency and orientation tuning of channels in CNNs for image classification tasks. The authors show that the spatial frequency and the orientation selectivity of CNN channels are highly correlated with human visual perception. They also show that contrast masking thresholds depend on the spatial frequencies and orientation selective filters. Finally, the authors conduct experiments on VGG-16 and CIFAR-10 to demonstrate the effectiveness of the proposed method."
3418,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) for predicting drug-drug interactions (DDIs) from graph neural networks (GNNs). The proposed method is based on the link prediction problem, which is an important problem in the field of DDI prediction. The authors propose to use graph energy network (GEN) to predict the energy function of the graph. The proposed model is evaluated on several real-world DDI datasets and compared to several baselines. The results show that the proposed model outperforms the baselines in terms of PR-AUC."
3419,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) for predicting drug-drug interactions (DDIs) from graph neural networks (GNNs). The proposed method is based on the link prediction problem, which is an important problem in the field of DDI prediction. The authors propose to use graph energy network (GEN) to predict the energy function of the graph. The proposed model is evaluated on several real-world DDI datasets and compared to several baselines. The results show that the proposed model outperforms the baselines in terms of PR-AUC."
3420,SP:e2c726a1c3e3ecbec198c4dd804a4298aacec3ad,"This paper proposes a graph energy neural network (GENN) for predicting drug-drug interactions (DDIs) from graph neural networks (GNNs). The proposed method is based on the link prediction problem, which is an important problem in the field of DDI prediction. The authors propose to use graph energy network (GEN) to predict the energy function of the graph. The proposed model is evaluated on several real-world DDI datasets and compared to several baselines. The results show that the proposed model outperforms the baselines in terms of PR-AUC."
3421,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a method for learning discrete representations of audio segments. The proposed method is based on Gumbel-Softmax and online k-means clustering. The method is evaluated on TIMIT phoneme classification and BERT pre-training.
3422,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a method for learning discrete representations of audio segments. The proposed method is based on Gumbel-Softmax and online k-means clustering. The method is evaluated on TIMIT phoneme classification and BERT pre-training.
3423,SP:1f2a27579404aa165303789fdce9b3ed54f7b0c6,This paper proposes a method for learning discrete representations of audio segments. The proposed method is based on Gumbel-Softmax and online k-means clustering. The method is evaluated on TIMIT phoneme classification and BERT pre-training.
3424,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning method for collaborative filtering. The main idea is to use a neural network to estimate the ranking of a set of actions, and then use an actor network to evaluate the performance of the critic network. The proposed method is evaluated on a variety of collaborative filtering tasks, and compared to a number of baselines."
3425,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning method for collaborative filtering. The main idea is to use a neural network to estimate the ranking of a set of actions, and then use an actor network to evaluate the performance of the critic network. The proposed method is evaluated on a variety of collaborative filtering tasks, and compared to a number of baselines."
3426,SP:f9c5b74b8bea5161d33676d9290d7b9d7e81d7b6,"This paper proposes an actor-critic reinforcement learning method for collaborative filtering. The main idea is to use a neural network to estimate the ranking of a set of actions, and then use an actor network to evaluate the performance of the critic network. The proposed method is evaluated on a variety of collaborative filtering tasks, and compared to a number of baselines."
3427,SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a novel off-policy reinforcement learning algorithm called TD3(∆) that combines TD3 and Model-based Value Expansion. The main idea is to use a truncated version of TD3, which is a variant of TD(\sqrt{T}) with a model-based value expansion. The authors show that the proposed algorithm outperforms TD3 in the function approximation setting and the function-approximation setting. In addition, the authors also show that TD3 can be applied to the tabular case."
3428,SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a novel off-policy reinforcement learning algorithm called TD3(∆) that combines TD3 and Model-based Value Expansion. The main idea is to use a truncated version of TD3, which is a variant of TD(\sqrt{T}) with a model-based value expansion. The authors show that the proposed algorithm outperforms TD3 in the function approximation setting and the function-approximation setting. In addition, the authors also show that TD3 can be applied to the tabular case."
3429,SP:2444a83ae08181b125a325d893789f074d6db8ee,"This paper proposes a novel off-policy reinforcement learning algorithm called TD3(∆) that combines TD3 and Model-based Value Expansion. The main idea is to use a truncated version of TD3, which is a variant of TD(\sqrt{T}) with a model-based value expansion. The authors show that the proposed algorithm outperforms TD3 in the function approximation setting and the function-approximation setting. In addition, the authors also show that TD3 can be applied to the tabular case."
3430,SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a method for continuous reinforcement learning (RL) for dexterous manipulation tasks. The main idea is to train a reinforcement learning agent that learns a reward function for each task, which is trained using a combination of reinforcement learning and reinforcement learning-based reinforcement learning. The reward function is learned using an off-the-shelf reinforcement learning algorithm. The authors show that the proposed method outperforms the baselines on a variety of tasks, and demonstrate that the learned reward function can be used to improve the performance of the agent."
3431,SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a method for continuous reinforcement learning (RL) for dexterous manipulation tasks. The main idea is to train a reinforcement learning agent that learns a reward function for each task, which is trained using a combination of reinforcement learning and reinforcement learning-based reinforcement learning. The reward function is learned using an off-the-shelf reinforcement learning algorithm. The authors show that the proposed method outperforms the baselines on a variety of tasks, and demonstrate that the learned reward function can be used to improve the performance of the agent."
3432,SP:64564b09bd68e7af17845019193825794f08e99b,"This paper proposes a method for continuous reinforcement learning (RL) for dexterous manipulation tasks. The main idea is to train a reinforcement learning agent that learns a reward function for each task, which is trained using a combination of reinforcement learning and reinforcement learning-based reinforcement learning. The reward function is learned using an off-the-shelf reinforcement learning algorithm. The authors show that the proposed method outperforms the baselines on a variety of tasks, and demonstrate that the learned reward function can be used to improve the performance of the agent."
3433,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the relationship between adversarial robustness and generalization. The authors show that adversarially robust generalization is a function of the prediction stability of the model. They also show that the stability of a model is related to the expected robust risk. They then propose an adversarial training algorithm to improve the stability. Finally, the authors conduct experiments on MNIST and Cifar-10 to demonstrate the effectiveness of their method."
3434,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the relationship between adversarial robustness and generalization. The authors show that adversarially robust generalization is a function of the prediction stability of the model. They also show that the stability of a model is related to the expected robust risk. They then propose an adversarial training algorithm to improve the stability. Finally, the authors conduct experiments on MNIST and Cifar-10 to demonstrate the effectiveness of their method."
3435,SP:ee4d59fa9487ecdcd663a4a7833689d1754aac7c,"This paper studies the relationship between adversarial robustness and generalization. The authors show that adversarially robust generalization is a function of the prediction stability of the model. They also show that the stability of a model is related to the expected robust risk. They then propose an adversarial training algorithm to improve the stability. Finally, the authors conduct experiments on MNIST and Cifar-10 to demonstrate the effectiveness of their method."
3436,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a new approach to estimate the advantage of a policy in reinforcement learning. The authors propose to use a combination of promotion focus and prevention focus. The proposed approach is based on the idea that the optimal policy should be the one that maximizes the order statistics of the path ensemble of the agents in a path ensemble. The main difference between the proposed approach and the existing approaches is that the authors argue that the proposed method is more efficient than the previous approaches. The paper is well-written and easy to follow. The experiments are conducted on MuJoCo continuous control, Atari games, and Terrain locomotion."
3437,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a new approach to estimate the advantage of a policy in reinforcement learning. The authors propose to use a combination of promotion focus and prevention focus. The proposed approach is based on the idea that the optimal policy should be the one that maximizes the order statistics of the path ensemble of the agents in a path ensemble. The main difference between the proposed approach and the existing approaches is that the authors argue that the proposed method is more efficient than the previous approaches. The paper is well-written and easy to follow. The experiments are conducted on MuJoCo continuous control, Atari games, and Terrain locomotion."
3438,SP:8d92aa968c590a352cb34c9fa1dbe77dff19519f,"This paper proposes a new approach to estimate the advantage of a policy in reinforcement learning. The authors propose to use a combination of promotion focus and prevention focus. The proposed approach is based on the idea that the optimal policy should be the one that maximizes the order statistics of the path ensemble of the agents in a path ensemble. The main difference between the proposed approach and the existing approaches is that the authors argue that the proposed method is more efficient than the previous approaches. The paper is well-written and easy to follow. The experiments are conducted on MuJoCo continuous control, Atari games, and Terrain locomotion."
3439,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes Precision Gating (PG), a new quantization method for deep neural networks that aims to reduce the computational cost of training. The proposed method is based on the idea of precision gating, which aims to improve the precision of the output of a neural network. The authors show that the proposed method can be applied to a wide range of DNN architectures, including CNNs, RNNs, LSTMs, and ShuffleNet. The method is evaluated on ImageNet, Penn Tree Bank, and CIFAR-10."
3440,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes Precision Gating (PG), a new quantization method for deep neural networks that aims to reduce the computational cost of training. The proposed method is based on the idea of precision gating, which aims to improve the precision of the output of a neural network. The authors show that the proposed method can be applied to a wide range of DNN architectures, including CNNs, RNNs, LSTMs, and ShuffleNet. The method is evaluated on ImageNet, Penn Tree Bank, and CIFAR-10."
3441,SP:2162408ce2a3267724b5f8f0abec41d4dc714220,"This paper proposes Precision Gating (PG), a new quantization method for deep neural networks that aims to reduce the computational cost of training. The proposed method is based on the idea of precision gating, which aims to improve the precision of the output of a neural network. The authors show that the proposed method can be applied to a wide range of DNN architectures, including CNNs, RNNs, LSTMs, and ShuffleNet. The method is evaluated on ImageNet, Penn Tree Bank, and CIFAR-10."
3442,SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a method for unsupervised domain adaptation (UDA) in the context of noisy-to-clean (SD) data. The authors propose a method called Butterfly, which is based on the idea that the label noise of the unlabeled data can be used to improve the performance of the classifier. The main idea of the paper is to use a deep neural network to learn a classifier that is robust to label noise. The proposed method is evaluated on a variety of datasets and compared to several state-of-the-art methods."
3443,SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a method for unsupervised domain adaptation (UDA) in the context of noisy-to-clean (SD) data. The authors propose a method called Butterfly, which is based on the idea that the label noise of the unlabeled data can be used to improve the performance of the classifier. The main idea of the paper is to use a deep neural network to learn a classifier that is robust to label noise. The proposed method is evaluated on a variety of datasets and compared to several state-of-the-art methods."
3444,SP:0c2c9b80c087389168acdd42af15877fb499449b,"This paper proposes a method for unsupervised domain adaptation (UDA) in the context of noisy-to-clean (SD) data. The authors propose a method called Butterfly, which is based on the idea that the label noise of the unlabeled data can be used to improve the performance of the classifier. The main idea of the paper is to use a deep neural network to learn a classifier that is robust to label noise. The proposed method is evaluated on a variety of datasets and compared to several state-of-the-art methods."
3445,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,This paper proposes a model-free reinforcement learning algorithm for image-based RL. The main idea is to use a high-capacity encoder and a low-capacity decoder to learn a latent representation of the task and a control policy. The latent representation is then used to train an off-policy actor-critic algorithm. Experiments show that the proposed algorithm achieves better sample efficiency than the baselines.
3446,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,This paper proposes a model-free reinforcement learning algorithm for image-based RL. The main idea is to use a high-capacity encoder and a low-capacity decoder to learn a latent representation of the task and a control policy. The latent representation is then used to train an off-policy actor-critic algorithm. Experiments show that the proposed algorithm achieves better sample efficiency than the baselines.
3447,SP:f3cc10ce2f77aeb2a6a3bae5631602452c14d403,This paper proposes a model-free reinforcement learning algorithm for image-based RL. The main idea is to use a high-capacity encoder and a low-capacity decoder to learn a latent representation of the task and a control policy. The latent representation is then used to train an off-policy actor-critic algorithm. Experiments show that the proposed algorithm achieves better sample efficiency than the baselines.
3448,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for training deep neural networks (DNNs). The proposed method is based on the observation that dropout can be used to accelerate the training of DNNs by reducing the number of dropout samples. The authors show that the proposed method can be applied to both convolutional layers and dropout layers, and that it can speed up training and generalization. The experiments are conducted on CIFAR-10, Cifar-100, and SVHN datasets."
3449,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for training deep neural networks (DNNs). The proposed method is based on the observation that dropout can be used to accelerate the training of DNNs by reducing the number of dropout samples. The authors show that the proposed method can be applied to both convolutional layers and dropout layers, and that it can speed up training and generalization. The experiments are conducted on CIFAR-10, Cifar-100, and SVHN datasets."
3450,SP:917bc9151a5829e97efd9bd0d0b2a3d1771b3265,"This paper proposes a new dropout technique, called multi-sample dropout, for training deep neural networks (DNNs). The proposed method is based on the observation that dropout can be used to accelerate the training of DNNs by reducing the number of dropout samples. The authors show that the proposed method can be applied to both convolutional layers and dropout layers, and that it can speed up training and generalization. The experiments are conducted on CIFAR-10, Cifar-100, and SVHN datasets."
3451,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,This paper proposes a label sensitive gate (LSG) structure for learning disentangled filters in CNNs. The proposed method is based on sparsity regularization and label sensitive gates. Experiments show that the proposed method outperforms the existing methods in terms of interpretability.
3452,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,This paper proposes a label sensitive gate (LSG) structure for learning disentangled filters in CNNs. The proposed method is based on sparsity regularization and label sensitive gates. Experiments show that the proposed method outperforms the existing methods in terms of interpretability.
3453,SP:5ad4b9e837e08d995b545b0b2734bc8fa4fafc43,This paper proposes a label sensitive gate (LSG) structure for learning disentangled filters in CNNs. The proposed method is based on sparsity regularization and label sensitive gates. Experiments show that the proposed method outperforms the existing methods in terms of interpretability.
3454,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,This paper proposes a new value function factorization method for collaborative multi-agent reinforcement learning. The proposed method is based on information theoretic regularization and communication minimization. The main idea is to learn a decentralized value function that is nearly decomposable and can be used as a regularizer to improve the mutual information between the agents. The authors show that the proposed method outperforms the existing methods on the StarCraft unit micromanagement benchmark.
3455,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,This paper proposes a new value function factorization method for collaborative multi-agent reinforcement learning. The proposed method is based on information theoretic regularization and communication minimization. The main idea is to learn a decentralized value function that is nearly decomposable and can be used as a regularizer to improve the mutual information between the agents. The authors show that the proposed method outperforms the existing methods on the StarCraft unit micromanagement benchmark.
3456,SP:c9a512b6bc59aacbec2d5608284e29a7746172cf,This paper proposes a new value function factorization method for collaborative multi-agent reinforcement learning. The proposed method is based on information theoretic regularization and communication minimization. The main idea is to learn a decentralized value function that is nearly decomposable and can be used as a regularizer to improve the mutual information between the agents. The authors show that the proposed method outperforms the existing methods on the StarCraft unit micromanagement benchmark.
3457,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a method for solving programming puzzles. The main idea is to use a GAN-like algorithm to generate a set of puzzles for each input-output pair, and then use the generated puzzles to train a solver to solve the problem. The authors show that the proposed method, called Troublemaker, outperforms the state-of-the-art solvers on a variety of tasks."
3458,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a method for solving programming puzzles. The main idea is to use a GAN-like algorithm to generate a set of puzzles for each input-output pair, and then use the generated puzzles to train a solver to solve the problem. The authors show that the proposed method, called Troublemaker, outperforms the state-of-the-art solvers on a variety of tasks."
3459,SP:562f1a50f80d760a4be35095cd795cdb0f69a890,"This paper proposes a method for solving programming puzzles. The main idea is to use a GAN-like algorithm to generate a set of puzzles for each input-output pair, and then use the generated puzzles to train a solver to solve the problem. The authors show that the proposed method, called Troublemaker, outperforms the state-of-the-art solvers on a variety of tasks."
3460,SP:627b515cc893ff33914dff255f5d6e136441d2e2,This paper proposes a hierarchical reinforcement learning method for learning policies that can be viewed as a decomposition of actions into lower-level primitives and higher-level meta-policies. The authors propose an information-theoretic mechanism for decentralized decision making and show that the proposed method is able to generalize better than flat and hierarchical policies in terms of generalization. They also show that their method can generalize to new environments.
3461,SP:627b515cc893ff33914dff255f5d6e136441d2e2,This paper proposes a hierarchical reinforcement learning method for learning policies that can be viewed as a decomposition of actions into lower-level primitives and higher-level meta-policies. The authors propose an information-theoretic mechanism for decentralized decision making and show that the proposed method is able to generalize better than flat and hierarchical policies in terms of generalization. They also show that their method can generalize to new environments.
3462,SP:627b515cc893ff33914dff255f5d6e136441d2e2,This paper proposes a hierarchical reinforcement learning method for learning policies that can be viewed as a decomposition of actions into lower-level primitives and higher-level meta-policies. The authors propose an information-theoretic mechanism for decentralized decision making and show that the proposed method is able to generalize better than flat and hierarchical policies in terms of generalization. They also show that their method can generalize to new environments.
3463,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-free reinforcement learning method for multi-pendulum and multi-cheetah environments. The authors propose to use a latent dynamics model to model the dynamics of the pendulum and the cheetah, and then use a multi-step reward prediction model to predict the next state in the latent state space. The proposed method is evaluated on a variety of environments and compared to a number of model-based baselines. The results show that the proposed method outperforms the baselines in terms of sample efficiency."
3464,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-free reinforcement learning method for multi-pendulum and multi-cheetah environments. The authors propose to use a latent dynamics model to model the dynamics of the pendulum and the cheetah, and then use a multi-step reward prediction model to predict the next state in the latent state space. The proposed method is evaluated on a variety of environments and compared to a number of model-based baselines. The results show that the proposed method outperforms the baselines in terms of sample efficiency."
3465,SP:4d135a76ab151dd0adcf92c5ed8d3c717d256520,"This paper proposes a model-free reinforcement learning method for multi-pendulum and multi-cheetah environments. The authors propose to use a latent dynamics model to model the dynamics of the pendulum and the cheetah, and then use a multi-step reward prediction model to predict the next state in the latent state space. The proposed method is evaluated on a variety of environments and compared to a number of model-based baselines. The results show that the proposed method outperforms the baselines in terms of sample efficiency."
3466,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes a method to reduce the size of the BERT model. The authors propose a self-supervised loss to improve the inter-sentence coherence of BERT. The proposed method is evaluated on the GLUE, RACE, and SQuAD benchmarks. The results show that the proposed method outperforms BERT-large on GLUE and RACE."
3467,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes a method to reduce the size of the BERT model. The authors propose a self-supervised loss to improve the inter-sentence coherence of BERT. The proposed method is evaluated on the GLUE, RACE, and SQuAD benchmarks. The results show that the proposed method outperforms BERT-large on GLUE and RACE."
3468,SP:03c61ba3d6fe01bd0bc3469cd408c370527d9d69,"This paper proposes a method to reduce the size of the BERT model. The authors propose a self-supervised loss to improve the inter-sentence coherence of BERT. The proposed method is evaluated on the GLUE, RACE, and SQuAD benchmarks. The results show that the proposed method outperforms BERT-large on GLUE and RACE."
3469,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,This paper proposes a Transformer-based model for language understanding. The proposed model is based on the Transformer architecture. The authors propose to use a spatio-temporal cache to store the hidden states of the hidden state of the model during the training. They also propose a self-attention mechanism to reduce the computational cost. The experiments show that the proposed model outperforms the state-of-the-art models on a variety of tasks.
3470,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,This paper proposes a Transformer-based model for language understanding. The proposed model is based on the Transformer architecture. The authors propose to use a spatio-temporal cache to store the hidden states of the hidden state of the model during the training. They also propose a self-attention mechanism to reduce the computational cost. The experiments show that the proposed model outperforms the state-of-the-art models on a variety of tasks.
3471,SP:5dd50f3e6cef6b82192a1d37b35469dc7fb443ce,This paper proposes a Transformer-based model for language understanding. The proposed model is based on the Transformer architecture. The authors propose to use a spatio-temporal cache to store the hidden states of the hidden state of the model during the training. They also propose a self-attention mechanism to reduce the computational cost. The experiments show that the proposed model outperforms the state-of-the-art models on a variety of tasks.
3472,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"This paper proposes a new method for estimating confidence intervals for Bayesian neural networks. The proposed method is based on the concept of discriminative jackknife (DJ), which is a regularization term that is used to approximate posterior inference. The main contribution of this paper is to propose a regularizer that can be used to estimate the confidence intervals of the model parameters. The regularizer is a combination of the regularizer and the influence functions (HOIFs). The authors show that the proposed regularizer can be applied to the Hessian-vector products, loss gradients, and oracle access. The authors also provide a recursive formula for estimating the HOIFs. Experiments are conducted on MNIST, CIFAR-10, and Fashion MNIST."
3473,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"This paper proposes a new method for estimating confidence intervals for Bayesian neural networks. The proposed method is based on the concept of discriminative jackknife (DJ), which is a regularization term that is used to approximate posterior inference. The main contribution of this paper is to propose a regularizer that can be used to estimate the confidence intervals of the model parameters. The regularizer is a combination of the regularizer and the influence functions (HOIFs). The authors show that the proposed regularizer can be applied to the Hessian-vector products, loss gradients, and oracle access. The authors also provide a recursive formula for estimating the HOIFs. Experiments are conducted on MNIST, CIFAR-10, and Fashion MNIST."
3474,SP:6c8e2dd1d6224dffb95dbf729b159f00bfb05721,"This paper proposes a new method for estimating confidence intervals for Bayesian neural networks. The proposed method is based on the concept of discriminative jackknife (DJ), which is a regularization term that is used to approximate posterior inference. The main contribution of this paper is to propose a regularizer that can be used to estimate the confidence intervals of the model parameters. The regularizer is a combination of the regularizer and the influence functions (HOIFs). The authors show that the proposed regularizer can be applied to the Hessian-vector products, loss gradients, and oracle access. The authors also provide a recursive formula for estimating the HOIFs. Experiments are conducted on MNIST, CIFAR-10, and Fashion MNIST."
3475,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a method for video synthesis and video prediction based on GANs. The proposed method is based on a decomposition of the discriminator into a discriminator and a generative model. The discriminator is trained to maximize the Inception Score, which is a measure of the similarity between the generated video and the original video. The generator is trained on the Kinetics-600 dataset, and the video synthesis is performed on the UCF-101 dataset. Experiments show that the proposed method outperforms the state-of-the-art methods."
3476,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a method for video synthesis and video prediction based on GANs. The proposed method is based on a decomposition of the discriminator into a discriminator and a generative model. The discriminator is trained to maximize the Inception Score, which is a measure of the similarity between the generated video and the original video. The generator is trained on the Kinetics-600 dataset, and the video synthesis is performed on the UCF-101 dataset. Experiments show that the proposed method outperforms the state-of-the-art methods."
3477,SP:057a035c4eeeb5fe985b20d0266126d66d9d243f,"This paper proposes a method for video synthesis and video prediction based on GANs. The proposed method is based on a decomposition of the discriminator into a discriminator and a generative model. The discriminator is trained to maximize the Inception Score, which is a measure of the similarity between the generated video and the original video. The generator is trained on the Kinetics-600 dataset, and the video synthesis is performed on the UCF-101 dataset. Experiments show that the proposed method outperforms the state-of-the-art methods."
3478,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper proposes a meta-learning method for few-shot learning. The main idea is to train a pre-trained classifier on the base class data, and then use the meta-learned classifier to learn a representation of the base classes. The meta-learner is trained on a large-scale dataset, where it is shown to outperform the baseline classifier. The authors also propose a spatial attention map to map the background images to the foreground images. The proposed method is evaluated on several few-shots classification benchmarks."
3479,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper proposes a meta-learning method for few-shot learning. The main idea is to train a pre-trained classifier on the base class data, and then use the meta-learned classifier to learn a representation of the base classes. The meta-learner is trained on a large-scale dataset, where it is shown to outperform the baseline classifier. The authors also propose a spatial attention map to map the background images to the foreground images. The proposed method is evaluated on several few-shots classification benchmarks."
3480,SP:a89ee8eb2f60d9d522993a57d656f0ef726d86d6,"This paper proposes a meta-learning method for few-shot learning. The main idea is to train a pre-trained classifier on the base class data, and then use the meta-learned classifier to learn a representation of the base classes. The meta-learner is trained on a large-scale dataset, where it is shown to outperform the baseline classifier. The authors also propose a spatial attention map to map the background images to the foreground images. The proposed method is evaluated on several few-shots classification benchmarks."
3481,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes a method for learning a generative adversarial network (GAN) that can be trained with constraints on the output of an unconstrained GAN. The proposed method is based on a combination of two techniques: (1) knowledge compilation and (2) hybrid logical-neural constraints. The authors show that the combination of these two techniques can improve the performance of the generator and the discriminator. The main contribution of the paper is the use of knowledge compilation techniques to generate a set of constraints for the generator, which are then used to train the discriminators. The method is evaluated on a variety of synthetic and real-world datasets."
3482,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes a method for learning a generative adversarial network (GAN) that can be trained with constraints on the output of an unconstrained GAN. The proposed method is based on a combination of two techniques: (1) knowledge compilation and (2) hybrid logical-neural constraints. The authors show that the combination of these two techniques can improve the performance of the generator and the discriminator. The main contribution of the paper is the use of knowledge compilation techniques to generate a set of constraints for the generator, which are then used to train the discriminators. The method is evaluated on a variety of synthetic and real-world datasets."
3483,SP:d236f0b38414442af00b9be5e5d39e138f0069a2,"This paper proposes a method for learning a generative adversarial network (GAN) that can be trained with constraints on the output of an unconstrained GAN. The proposed method is based on a combination of two techniques: (1) knowledge compilation and (2) hybrid logical-neural constraints. The authors show that the combination of these two techniques can improve the performance of the generator and the discriminator. The main contribution of the paper is the use of knowledge compilation techniques to generate a set of constraints for the generator, which are then used to train the discriminators. The method is evaluated on a variety of synthetic and real-world datasets."
3484,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new activation function called Adaptive Piecewise Linear units (APL) for deep neural networks. APL is based on the idea of adapting the activation function of ReLUs to be symmetric. The authors show that APL can be used in a variety of architectures, including ResNet-18, Network-in-Network (NIN), and LSTM. They also show that the proposed activation function is robust to adversarial attacks."
3485,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new activation function called Adaptive Piecewise Linear units (APL) for deep neural networks. APL is based on the idea of adapting the activation function of ReLUs to be symmetric. The authors show that APL can be used in a variety of architectures, including ResNet-18, Network-in-Network (NIN), and LSTM. They also show that the proposed activation function is robust to adversarial attacks."
3486,SP:110f0b86431f0a93cf48e08fe445e32172a37eae,"This paper proposes a new activation function called Adaptive Piecewise Linear units (APL) for deep neural networks. APL is based on the idea of adapting the activation function of ReLUs to be symmetric. The authors show that APL can be used in a variety of architectures, including ResNet-18, Network-in-Network (NIN), and LSTM. They also show that the proposed activation function is robust to adversarial attacks."
3487,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,This paper proposes a method to estimate the uncertainty of a black-box classifier using a Dirichlet layer on top of a probabilistic neural network. The method is based on the idea that Dirichlets can be used as a measure of uncertainty in a deep learning model. The authors propose to use a rejection system to evaluate whether a classifier is confident in a given classifier. The proposed method is evaluated on a simulated API based classification task.
3488,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,This paper proposes a method to estimate the uncertainty of a black-box classifier using a Dirichlet layer on top of a probabilistic neural network. The method is based on the idea that Dirichlets can be used as a measure of uncertainty in a deep learning model. The authors propose to use a rejection system to evaluate whether a classifier is confident in a given classifier. The proposed method is evaluated on a simulated API based classification task.
3489,SP:4787aff0fb84beb13cde0d40c32d3a743d8e4082,This paper proposes a method to estimate the uncertainty of a black-box classifier using a Dirichlet layer on top of a probabilistic neural network. The method is based on the idea that Dirichlets can be used as a measure of uncertainty in a deep learning model. The authors propose to use a rejection system to evaluate whether a classifier is confident in a given classifier. The proposed method is evaluated on a simulated API based classification task.
3490,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,This paper studies the convergence rate of blockwise adaptive gradient descent (Adagrad) for online convex learning with nonconvex objective. The main contribution of this paper is to show that blockwise adaptivity is more stable than coordinate-wise adaptive stepsize. The paper also shows that Adam and Nesterov’s accelerated gradient converge faster than blockwise adaptation.
3491,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,This paper studies the convergence rate of blockwise adaptive gradient descent (Adagrad) for online convex learning with nonconvex objective. The main contribution of this paper is to show that blockwise adaptivity is more stable than coordinate-wise adaptive stepsize. The paper also shows that Adam and Nesterov’s accelerated gradient converge faster than blockwise adaptation.
3492,SP:1207bf6cf93737d63e1a7cc1ff3a99bf9d6098f9,This paper studies the convergence rate of blockwise adaptive gradient descent (Adagrad) for online convex learning with nonconvex objective. The main contribution of this paper is to show that blockwise adaptivity is more stable than coordinate-wise adaptive stepsize. The paper also shows that Adam and Nesterov’s accelerated gradient converge faster than blockwise adaptation.
3493,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,This paper proposes a video dataset called CATER for video understanding. CATER is a large-scale video dataset with 3D objects and 3D scenes. The authors claim that the dataset can be used as a diagnostic tool for spatiotemporal video architectures. The proposed dataset is evaluated on a variety of video architectures and shows that it is able to identify the implicit biases of video datasets.
3494,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,This paper proposes a video dataset called CATER for video understanding. CATER is a large-scale video dataset with 3D objects and 3D scenes. The authors claim that the dataset can be used as a diagnostic tool for spatiotemporal video architectures. The proposed dataset is evaluated on a variety of video architectures and shows that it is able to identify the implicit biases of video datasets.
3495,SP:0334d79349e9fb8ca32751b7ad29f82e00a5381c,This paper proposes a video dataset called CATER for video understanding. CATER is a large-scale video dataset with 3D objects and 3D scenes. The authors claim that the dataset can be used as a diagnostic tool for spatiotemporal video architectures. The proposed dataset is evaluated on a variety of video architectures and shows that it is able to identify the implicit biases of video datasets.
3496,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes Boundary-Calibration GANs (BCGANs), a GAN-based method for generating near-boundary data. The main idea is to use the boundary information of the pre-trained classifiers to improve model compatibility. The authors propose a BC-loss to improve the model compatibility between the generator of GAN and the target classifier. The proposed method is evaluated on synthetic and real-world datasets. The experimental results show that the proposed method outperforms the baselines."
3497,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes Boundary-Calibration GANs (BCGANs), a GAN-based method for generating near-boundary data. The main idea is to use the boundary information of the pre-trained classifiers to improve model compatibility. The authors propose a BC-loss to improve the model compatibility between the generator of GAN and the target classifier. The proposed method is evaluated on synthetic and real-world datasets. The experimental results show that the proposed method outperforms the baselines."
3498,SP:b637c75acbe9d0152384b632f2e92a0d248cb720,"This paper proposes Boundary-Calibration GANs (BCGANs), a GAN-based method for generating near-boundary data. The main idea is to use the boundary information of the pre-trained classifiers to improve model compatibility. The authors propose a BC-loss to improve the model compatibility between the generator of GAN and the target classifier. The proposed method is evaluated on synthetic and real-world datasets. The experimental results show that the proposed method outperforms the baselines."
3499,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new method for adversarial training of deep neural networks. The proposed method is based on the minimax robust optimization problem, which is a convex-concave optimization problem. The main contribution of this paper is to propose a novel method to solve the inner problem of minimizing the inner minimization of the robust classifier with a convolutional neural network. The method is evaluated on CIFAR-10 and Cifar-100 datasets and compared to several baselines. The results show that the proposed method outperforms the baselines in terms of classification accuracy and computational efficiency."
3500,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new method for adversarial training of deep neural networks. The proposed method is based on the minimax robust optimization problem, which is a convex-concave optimization problem. The main contribution of this paper is to propose a novel method to solve the inner problem of minimizing the inner minimization of the robust classifier with a convolutional neural network. The method is evaluated on CIFAR-10 and Cifar-100 datasets and compared to several baselines. The results show that the proposed method outperforms the baselines in terms of classification accuracy and computational efficiency."
3501,SP:93616e31fa1dc64d130c0c44cbb73c0412b24a97,"This paper proposes a new method for adversarial training of deep neural networks. The proposed method is based on the minimax robust optimization problem, which is a convex-concave optimization problem. The main contribution of this paper is to propose a novel method to solve the inner problem of minimizing the inner minimization of the robust classifier with a convolutional neural network. The method is evaluated on CIFAR-10 and Cifar-100 datasets and compared to several baselines. The results show that the proposed method outperforms the baselines in terms of classification accuracy and computational efficiency."
3502,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,This paper proposes a method for inverse reinforcement learning (IRL) in Markov Decision Processes (MDPs). The main idea is to use the Maximum Entropy IRL framework to learn a policy that maximizes the Maximum Likelihood Constraint (MCL) of the observed behavior. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in simulated MDPs. 
3503,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,This paper proposes a method for inverse reinforcement learning (IRL) in Markov Decision Processes (MDPs). The main idea is to use the Maximum Entropy IRL framework to learn a policy that maximizes the Maximum Likelihood Constraint (MCL) of the observed behavior. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in simulated MDPs. 
3504,SP:0a66c3540383c76689258d2fffe0571ed944c1e7,This paper proposes a method for inverse reinforcement learning (IRL) in Markov Decision Processes (MDPs). The main idea is to use the Maximum Entropy IRL framework to learn a policy that maximizes the Maximum Likelihood Constraint (MCL) of the observed behavior. The authors show that the proposed method is able to achieve better performance than the state-of-the-art in simulated MDPs. 
3505,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper proposes a novel deep recurrent neural network architecture for contour detection. The proposed architecture, called γ-Net, is based on neural circuits and is able to detect low-level object boundary contours. The authors show that the proposed architecture can detect the orientation-tilting optical illusion, which is a perceptual phenomenon observed in biological visual systems. "
3506,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper proposes a novel deep recurrent neural network architecture for contour detection. The proposed architecture, called γ-Net, is based on neural circuits and is able to detect low-level object boundary contours. The authors show that the proposed architecture can detect the orientation-tilting optical illusion, which is a perceptual phenomenon observed in biological visual systems. "
3507,SP:2a5fba69a6287b87a19bcd745d2e4326bbb723de,"This paper proposes a novel deep recurrent neural network architecture for contour detection. The proposed architecture, called γ-Net, is based on neural circuits and is able to detect low-level object boundary contours. The authors show that the proposed architecture can detect the orientation-tilting optical illusion, which is a perceptual phenomenon observed in biological visual systems. "
3508,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,This paper proposes a conditional random field (CRF) model for object detection. The proposed method is based on a stack of common CNN operations and a context-aware module. Experiments on COCO and ImageNet show that the proposed method outperforms existing methods.
3509,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,This paper proposes a conditional random field (CRF) model for object detection. The proposed method is based on a stack of common CNN operations and a context-aware module. Experiments on COCO and ImageNet show that the proposed method outperforms existing methods.
3510,SP:4f094a3f7eeb302738c2b482fbaca56e34ac6a99,This paper proposes a conditional random field (CRF) model for object detection. The proposed method is based on a stack of common CNN operations and a context-aware module. Experiments on COCO and ImageNet show that the proposed method outperforms existing methods.
3511,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"This paper studies the adversarial vulnerability of batch normalization (BatchNorm) in the presence of adversarial perturbations. The authors propose a new normalization technique called RobustNorm, which aims to improve the robustness of BatchNorm to adversarial attacks. In particular, the authors propose to use mini-batch statistics as normalization statistics for training and inference. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
3512,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"This paper studies the adversarial vulnerability of batch normalization (BatchNorm) in the presence of adversarial perturbations. The authors propose a new normalization technique called RobustNorm, which aims to improve the robustness of BatchNorm to adversarial attacks. In particular, the authors propose to use mini-batch statistics as normalization statistics for training and inference. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
3513,SP:74d63293d2f8a41a14743bfcd8939fca5e804fdb,"This paper studies the adversarial vulnerability of batch normalization (BatchNorm) in the presence of adversarial perturbations. The authors propose a new normalization technique called RobustNorm, which aims to improve the robustness of BatchNorm to adversarial attacks. In particular, the authors propose to use mini-batch statistics as normalization statistics for training and inference. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
3514,SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the generalization properties of over-parameterized deep neural networks under label noise. The authors show that under certain conditions, the network can generalize better than under other conditions. The main contribution of this paper is to provide a generalization bound for the network with a wide neural network and a neural tangent kernel (NTK)."
3515,SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the generalization properties of over-parameterized deep neural networks under label noise. The authors show that under certain conditions, the network can generalize better than under other conditions. The main contribution of this paper is to provide a generalization bound for the network with a wide neural network and a neural tangent kernel (NTK)."
3516,SP:f16d3e61eda162dfee39396abbd594425f47f625,"This paper studies the generalization properties of over-parameterized deep neural networks under label noise. The authors show that under certain conditions, the network can generalize better than under other conditions. The main contribution of this paper is to provide a generalization bound for the network with a wide neural network and a neural tangent kernel (NTK)."
3517,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes a novel convolutional neural tangent kernel (CNTK) method for regression. The proposed method is based on the local average pooling (LAP) operation, which is a variant of Global Average Pooling (GAP). The authors show that the proposed CNTK can achieve better performance than CNN-GP on CIFAR-10 and Fashion-MNIST. The main contribution of this paper is to propose a novel method for data augmentation. The authors also provide a theoretical analysis of the convergence of the proposed method."
3518,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes a novel convolutional neural tangent kernel (CNTK) method for regression. The proposed method is based on the local average pooling (LAP) operation, which is a variant of Global Average Pooling (GAP). The authors show that the proposed CNTK can achieve better performance than CNN-GP on CIFAR-10 and Fashion-MNIST. The main contribution of this paper is to propose a novel method for data augmentation. The authors also provide a theoretical analysis of the convergence of the proposed method."
3519,SP:67335658ec9de6ba3fa352ca4de073ac51f2f703,"This paper proposes a novel convolutional neural tangent kernel (CNTK) method for regression. The proposed method is based on the local average pooling (LAP) operation, which is a variant of Global Average Pooling (GAP). The authors show that the proposed CNTK can achieve better performance than CNN-GP on CIFAR-10 and Fashion-MNIST. The main contribution of this paper is to propose a novel method for data augmentation. The authors also provide a theoretical analysis of the convergence of the proposed method."
3520,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for model-free Q-learning with amortized value estimates (MCTS). The main idea is to use MCTS to estimate the state-action value of an agent, which is then used to train a model-based Q-Learning agent. The proposed method, SAVE, is evaluated on a set of Atari games, and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of Q-value estimates."
3521,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for model-free Q-learning with amortized value estimates (MCTS). The main idea is to use MCTS to estimate the state-action value of an agent, which is then used to train a model-based Q-Learning agent. The proposed method, SAVE, is evaluated on a set of Atari games, and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of Q-value estimates."
3522,SP:4ffab7f7f9fc09fdf59602228d231c6f6330fb98,"This paper proposes a method for model-free Q-learning with amortized value estimates (MCTS). The main idea is to use MCTS to estimate the state-action value of an agent, which is then used to train a model-based Q-Learning agent. The proposed method, SAVE, is evaluated on a set of Atari games, and compared to a number of baselines. The results show that the proposed method outperforms the baselines in terms of Q-value estimates."
3523,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a new network architecture for 3D panning. The proposed network is based on a t-shaped adaptive kernel, which allows for both globally and locally adaptive dilations. The authors show that the proposed network outperforms SOTA and SSIM on the VICLAB STEREO indoors dataset."
3524,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a new network architecture for 3D panning. The proposed network is based on a t-shaped adaptive kernel, which allows for both globally and locally adaptive dilations. The authors show that the proposed network outperforms SOTA and SSIM on the VICLAB STEREO indoors dataset."
3525,SP:ab451cc0ec221864a5da532eceba0f021f30def4,"This paper proposes a new network architecture for 3D panning. The proposed network is based on a t-shaped adaptive kernel, which allows for both globally and locally adaptive dilations. The authors show that the proposed network outperforms SOTA and SSIM on the VICLAB STEREO indoors dataset."
3526,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the capacity-constrained InfoMax (CCIM) for variational auto-encoders (VAEs). The authors propose a variational lower bound for the capacity of the variational network. The authors also propose a new variational autoencoder (VAE) called VIMAE, which is a variant of the one-VAE. Experiments on MNIST and CIFAR-10 demonstrate the effectiveness of the proposed method."
3527,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the capacity-constrained InfoMax (CCIM) for variational auto-encoders (VAEs). The authors propose a variational lower bound for the capacity of the variational network. The authors also propose a new variational autoencoder (VAE) called VIMAE, which is a variant of the one-VAE. Experiments on MNIST and CIFAR-10 demonstrate the effectiveness of the proposed method."
3528,SP:e308cf28f7bd5d8e6c36517e2845298ccd401f5d,"This paper studies the capacity-constrained InfoMax (CCIM) for variational auto-encoders (VAEs). The authors propose a variational lower bound for the capacity of the variational network. The authors also propose a new variational autoencoder (VAE) called VIMAE, which is a variant of the one-VAE. Experiments on MNIST and CIFAR-10 demonstrate the effectiveness of the proposed method."
3529,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,This paper proposes a multiscale approach for network embedding based on random walks. The main idea is to learn a matrices of node-feature pointwise mutual information (MUSAE) between node-features and attribute-neighborhood node embeddings. The authors show that MUSAE is more efficient than Skip-gram (Skip-gram) in terms of the number of nodes and feature vectors. They also show that their method can be applied to the case of disconnected networks.
3530,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,This paper proposes a multiscale approach for network embedding based on random walks. The main idea is to learn a matrices of node-feature pointwise mutual information (MUSAE) between node-features and attribute-neighborhood node embeddings. The authors show that MUSAE is more efficient than Skip-gram (Skip-gram) in terms of the number of nodes and feature vectors. They also show that their method can be applied to the case of disconnected networks.
3531,SP:ef4a0c82cc364b797fba0ba86a91d9945b66a193,This paper proposes a multiscale approach for network embedding based on random walks. The main idea is to learn a matrices of node-feature pointwise mutual information (MUSAE) between node-features and attribute-neighborhood node embeddings. The authors show that MUSAE is more efficient than Skip-gram (Skip-gram) in terms of the number of nodes and feature vectors. They also show that their method can be applied to the case of disconnected networks.
3532,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the Information Bottleneck (IB) objective, which is defined as the phase transition of the Fisher Information Matrix (FIM). The authors propose a new definition of the IB objective that is based on the second-order calculus of variations (SCO) of the second order of variations. The authors also propose a novel algorithm that can be used to estimate the phase transitions in the Fisher information matrix. The proposed algorithm is evaluated on MNIST and CIFAR-10 and compared to the state-of-the-art methods."
3533,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the Information Bottleneck (IB) objective, which is defined as the phase transition of the Fisher Information Matrix (FIM). The authors propose a new definition of the IB objective that is based on the second-order calculus of variations (SCO) of the second order of variations. The authors also propose a novel algorithm that can be used to estimate the phase transitions in the Fisher information matrix. The proposed algorithm is evaluated on MNIST and CIFAR-10 and compared to the state-of-the-art methods."
3534,SP:70d92189aedeb4148b61b987d97a3c15898dd834,"This paper studies the Information Bottleneck (IB) objective, which is defined as the phase transition of the Fisher Information Matrix (FIM). The authors propose a new definition of the IB objective that is based on the second-order calculus of variations (SCO) of the second order of variations. The authors also propose a novel algorithm that can be used to estimate the phase transitions in the Fisher information matrix. The proposed algorithm is evaluated on MNIST and CIFAR-10 and compared to the state-of-the-art methods."
3535,SP:fecfd5e98540e2d146a726f94802d96472455111,This paper proposes an importance sampling advantage estimator for reinforcement learning. The main idea is to use a reward decomposition model to estimate the importance of each state in the reward function. The authors show that the proposed estimator has close-to-zero variance compared to the Monte-Carlo estimator. They also show that their estimator is independent of the time horizon.
3536,SP:fecfd5e98540e2d146a726f94802d96472455111,This paper proposes an importance sampling advantage estimator for reinforcement learning. The main idea is to use a reward decomposition model to estimate the importance of each state in the reward function. The authors show that the proposed estimator has close-to-zero variance compared to the Monte-Carlo estimator. They also show that their estimator is independent of the time horizon.
3537,SP:fecfd5e98540e2d146a726f94802d96472455111,This paper proposes an importance sampling advantage estimator for reinforcement learning. The main idea is to use a reward decomposition model to estimate the importance of each state in the reward function. The authors show that the proposed estimator has close-to-zero variance compared to the Monte-Carlo estimator. They also show that their estimator is independent of the time horizon.
3538,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,This paper proposes a method for infinite horizon off-policy policy evaluation. The main idea is to use a bias-reduced augmentation of the value function to reduce the bias of the density ratio estimation and value function estimation. The proposed method is evaluated on a synthetic dataset and on a real-world dataset.
3539,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,This paper proposes a method for infinite horizon off-policy policy evaluation. The main idea is to use a bias-reduced augmentation of the value function to reduce the bias of the density ratio estimation and value function estimation. The proposed method is evaluated on a synthetic dataset and on a real-world dataset.
3540,SP:f0d84396e0ede7969d3f3f55549d214f20daf1b0,This paper proposes a method for infinite horizon off-policy policy evaluation. The main idea is to use a bias-reduced augmentation of the value function to reduce the bias of the density ratio estimation and value function estimation. The proposed method is evaluated on a synthetic dataset and on a real-world dataset.
3541,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a method for few-shot classification. The main idea is to train a classifier on a large set of base classes and then fine-tune it on a small set of new examples. The method is based on a two-stage framework. First, the classifier is trained on the base class, and then a new task is added to the training set. The authors show that the proposed method outperforms baselines on mini-ImageNet and CUB-200-2011 benchmarks."
3542,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a method for few-shot classification. The main idea is to train a classifier on a large set of base classes and then fine-tune it on a small set of new examples. The method is based on a two-stage framework. First, the classifier is trained on the base class, and then a new task is added to the training set. The authors show that the proposed method outperforms baselines on mini-ImageNet and CUB-200-2011 benchmarks."
3543,SP:73f8dddb09333a739c609cc324a1e813d29f8874,"This paper proposes a method for few-shot classification. The main idea is to train a classifier on a large set of base classes and then fine-tune it on a small set of new examples. The method is based on a two-stage framework. First, the classifier is trained on the base class, and then a new task is added to the training set. The authors show that the proposed method outperforms baselines on mini-ImageNet and CUB-200-2011 benchmarks."
3544,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,This paper proposes a data-driven approach to learn a Navier-Stokes solver for fluid flow problems. The main idea is to train a neural network to estimate the error of the numerical scheme and then use the learned model to correct the error. The proposed method is evaluated on a variety of fluid flow simulation problems and compared to a number of state-of-the-art methods.
3545,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,This paper proposes a data-driven approach to learn a Navier-Stokes solver for fluid flow problems. The main idea is to train a neural network to estimate the error of the numerical scheme and then use the learned model to correct the error. The proposed method is evaluated on a variety of fluid flow simulation problems and compared to a number of state-of-the-art methods.
3546,SP:cde2a84c463cdab9b19fcbdaf1cfe20d0187dcfa,This paper proposes a data-driven approach to learn a Navier-Stokes solver for fluid flow problems. The main idea is to train a neural network to estimate the error of the numerical scheme and then use the learned model to correct the error. The proposed method is evaluated on a variety of fluid flow simulation problems and compared to a number of state-of-the-art methods.
3547,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for online continual learning. The main idea is to use discrete autoencoders (SQM) to learn a latent representation of the learned representation, which can then be used for online compression. The method is based on the idea of episodic memory, which is used to store experience replay. The authors show that the proposed method can be applied to a wide range of compression tasks, including Imagenet and LiDAR. The proposed method is evaluated on a variety of continual learning benchmarks."
3548,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for online continual learning. The main idea is to use discrete autoencoders (SQM) to learn a latent representation of the learned representation, which can then be used for online compression. The method is based on the idea of episodic memory, which is used to store experience replay. The authors show that the proposed method can be applied to a wide range of compression tasks, including Imagenet and LiDAR. The proposed method is evaluated on a variety of continual learning benchmarks."
3549,SP:e6534cd49bdc266dbeb111682ad37ef9d666e31e,"This paper proposes a method for online continual learning. The main idea is to use discrete autoencoders (SQM) to learn a latent representation of the learned representation, which can then be used for online compression. The method is based on the idea of episodic memory, which is used to store experience replay. The authors show that the proposed method can be applied to a wide range of compression tasks, including Imagenet and LiDAR. The proposed method is evaluated on a variety of continual learning benchmarks."
3550,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,This paper proposes a method for multi-label metric learning. The proposed method is based on the Bidirectional Representation Learning (BRL) framework. The main idea of BRL is to use a deep neural network to represent the metric space and then use deep convolutional networks to model the feature data. The experiments show that the proposed method outperforms the state-of-the-art methods.
3551,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,This paper proposes a method for multi-label metric learning. The proposed method is based on the Bidirectional Representation Learning (BRL) framework. The main idea of BRL is to use a deep neural network to represent the metric space and then use deep convolutional networks to model the feature data. The experiments show that the proposed method outperforms the state-of-the-art methods.
3552,SP:4ada8234990b4dbcdecb6bafeb6f509263661ae8,This paper proposes a method for multi-label metric learning. The proposed method is based on the Bidirectional Representation Learning (BRL) framework. The main idea of BRL is to use a deep neural network to represent the metric space and then use deep convolutional networks to model the feature data. The experiments show that the proposed method outperforms the state-of-the-art methods.
3553,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,This paper studies the stability of stochastic gradient descent (SGD) in the context of asynchronous training. The authors show that SGD converges to the minima of a closed-form rule when the learning rate is close to zero. They also show that momentum plays an important role in the stability.
3554,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,This paper studies the stability of stochastic gradient descent (SGD) in the context of asynchronous training. The authors show that SGD converges to the minima of a closed-form rule when the learning rate is close to zero. They also show that momentum plays an important role in the stability.
3555,SP:d81a0edd94cc0b32734c42f1fb65d7070f963f86,This paper studies the stability of stochastic gradient descent (SGD) in the context of asynchronous training. The authors show that SGD converges to the minima of a closed-form rule when the learning rate is close to zero. They also show that momentum plays an important role in the stability.
3556,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes a method for value estimation in reinforcement learning. The main idea is to learn a representation of the reward function, and then use this representation to estimate the value function. The proposed method is evaluated on Atari 2600 games, and it is shown to outperform a number of baselines."
3557,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes a method for value estimation in reinforcement learning. The main idea is to learn a representation of the reward function, and then use this representation to estimate the value function. The proposed method is evaluated on Atari 2600 games, and it is shown to outperform a number of baselines."
3558,SP:05587c2ba9ff9bf3574604a60f614dd807c95e22,"This paper proposes a method for value estimation in reinforcement learning. The main idea is to learn a representation of the reward function, and then use this representation to estimate the value function. The proposed method is evaluated on Atari 2600 games, and it is shown to outperform a number of baselines."
3559,SP:6388fb91f2eaac02d9406672760a237f78735452,This paper proposes a method for adversarial attacks on graph neural networks (GNNs). The main idea is to use reinforcement learning to learn a rewiring operation on the graph. The proposed method is evaluated on several real-world graphs and shows that the proposed method outperforms existing methods.
3560,SP:6388fb91f2eaac02d9406672760a237f78735452,This paper proposes a method for adversarial attacks on graph neural networks (GNNs). The main idea is to use reinforcement learning to learn a rewiring operation on the graph. The proposed method is evaluated on several real-world graphs and shows that the proposed method outperforms existing methods.
3561,SP:6388fb91f2eaac02d9406672760a237f78735452,This paper proposes a method for adversarial attacks on graph neural networks (GNNs). The main idea is to use reinforcement learning to learn a rewiring operation on the graph. The proposed method is evaluated on several real-world graphs and shows that the proposed method outperforms existing methods.
3562,SP:233b12d422d0ac40026efdf7aab9973181902d70,This paper proposes a new unbiased risk estimator for denoising problems. The main idea is to use a piecewise linear representation of the encoder-decoder CNN to estimate the divergence term of the unbiased estimator. The paper also proposes a bootstrap and aggregation scheme to improve the performance of the proposed algorithm.
3563,SP:233b12d422d0ac40026efdf7aab9973181902d70,This paper proposes a new unbiased risk estimator for denoising problems. The main idea is to use a piecewise linear representation of the encoder-decoder CNN to estimate the divergence term of the unbiased estimator. The paper also proposes a bootstrap and aggregation scheme to improve the performance of the proposed algorithm.
3564,SP:233b12d422d0ac40026efdf7aab9973181902d70,This paper proposes a new unbiased risk estimator for denoising problems. The main idea is to use a piecewise linear representation of the encoder-decoder CNN to estimate the divergence term of the unbiased estimator. The paper also proposes a bootstrap and aggregation scheme to improve the performance of the proposed algorithm.
3565,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,This paper proposes a meta-learning method for few-shot classification. The proposed method is based on a variational inference framework and a Bayesian learning framework. The main contribution of the paper is the introduction of a balancing component that balances the distributional difference between the task-dependent and task-specific variables. The balancing component is a combination of the meta-knowledge and the task specific learning. The authors also propose a meta learning objective that combines the variational objective and the balancing component. The experiments show that the proposed method outperforms the baselines.
3566,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,This paper proposes a meta-learning method for few-shot classification. The proposed method is based on a variational inference framework and a Bayesian learning framework. The main contribution of the paper is the introduction of a balancing component that balances the distributional difference between the task-dependent and task-specific variables. The balancing component is a combination of the meta-knowledge and the task specific learning. The authors also propose a meta learning objective that combines the variational objective and the balancing component. The experiments show that the proposed method outperforms the baselines.
3567,SP:978555e8eced096b92b37a91fc16b60f7d99c2b6,This paper proposes a meta-learning method for few-shot classification. The proposed method is based on a variational inference framework and a Bayesian learning framework. The main contribution of the paper is the introduction of a balancing component that balances the distributional difference between the task-dependent and task-specific variables. The balancing component is a combination of the meta-knowledge and the task specific learning. The authors also propose a meta learning objective that combines the variational objective and the balancing component. The experiments show that the proposed method outperforms the baselines.
3568,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a soft Q imitation learning (SQIL) method for reinforcement learning. The main idea of SQIL is to use a regularized version of behavioral cloning (BC) as a regularizer for long-horizon imitation. In particular, the authors propose a sparsity prior to encourage long-term imitation. The authors evaluate SQIL on MuJoCo, Atari, and Box2D."
3569,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a soft Q imitation learning (SQIL) method for reinforcement learning. The main idea of SQIL is to use a regularized version of behavioral cloning (BC) as a regularizer for long-horizon imitation. In particular, the authors propose a sparsity prior to encourage long-term imitation. The authors evaluate SQIL on MuJoCo, Atari, and Box2D."
3570,SP:2395947721c4a337701a7c61cd4ba5c0e38fcc9b,"This paper proposes a soft Q imitation learning (SQIL) method for reinforcement learning. The main idea of SQIL is to use a regularized version of behavioral cloning (BC) as a regularizer for long-horizon imitation. In particular, the authors propose a sparsity prior to encourage long-term imitation. The authors evaluate SQIL on MuJoCo, Atari, and Box2D."
3571,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,This paper proposes a method for learning stable and temporally coherent feature spaces for point clouds. The proposed method is based on super-resolution and truncation techniques. The authors propose a temporal loss function for the temporal loss and a higher time derivative for the time loss function. The temporal loss is a combination of the super resolution method and the truncation method. The method is evaluated on a variety of point cloud datasets. The results show that the proposed method outperforms the existing methods.
3572,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,This paper proposes a method for learning stable and temporally coherent feature spaces for point clouds. The proposed method is based on super-resolution and truncation techniques. The authors propose a temporal loss function for the temporal loss and a higher time derivative for the time loss function. The temporal loss is a combination of the super resolution method and the truncation method. The method is evaluated on a variety of point cloud datasets. The results show that the proposed method outperforms the existing methods.
3573,SP:37a4825aaeb899187b957d9ed9ae657617f4a055,This paper proposes a method for learning stable and temporally coherent feature spaces for point clouds. The proposed method is based on super-resolution and truncation techniques. The authors propose a temporal loss function for the temporal loss and a higher time derivative for the time loss function. The temporal loss is a combination of the super resolution method and the truncation method. The method is evaluated on a variety of point cloud datasets. The results show that the proposed method outperforms the existing methods.
3574,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes Optimal Transport (OT), a method for learning a cost function for a single-cell RNA-seq dataset. The proposed method is based on the Sinkhorn algorithm. The main idea is to use the side information of the cost function to learn a subset correspondence between the subset correspondence and the subset correspondences of the input data. The authors show that the proposed method outperforms the state-of-the-art on a variety of benchmarks."
3575,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes Optimal Transport (OT), a method for learning a cost function for a single-cell RNA-seq dataset. The proposed method is based on the Sinkhorn algorithm. The main idea is to use the side information of the cost function to learn a subset correspondence between the subset correspondence and the subset correspondences of the input data. The authors show that the proposed method outperforms the state-of-the-art on a variety of benchmarks."
3576,SP:af54d04f219d381208c049b8a9c59b8cdd1783e0,"This paper proposes Optimal Transport (OT), a method for learning a cost function for a single-cell RNA-seq dataset. The proposed method is based on the Sinkhorn algorithm. The main idea is to use the side information of the cost function to learn a subset correspondence between the subset correspondence and the subset correspondences of the input data. The authors show that the proposed method outperforms the state-of-the-art on a variety of benchmarks."
3577,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,This paper proposes a method for normal data selection. The authors propose to use a clustering technique to learn the representation of the normal data and then use an autoencoder to estimate the reconstruction error of the original data. The reconstruction error is then used as a scoring function for the scoring function. The proposed method is evaluated on several datasets and compared to several semi-supervised methods.
3578,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,This paper proposes a method for normal data selection. The authors propose to use a clustering technique to learn the representation of the normal data and then use an autoencoder to estimate the reconstruction error of the original data. The reconstruction error is then used as a scoring function for the scoring function. The proposed method is evaluated on several datasets and compared to several semi-supervised methods.
3579,SP:fc86b06a367f6790c76b89ec3bfe4cb8627c540a,This paper proposes a method for normal data selection. The authors propose to use a clustering technique to learn the representation of the normal data and then use an autoencoder to estimate the reconstruction error of the original data. The reconstruction error is then used as a scoring function for the scoring function. The proposed method is evaluated on several datasets and compared to several semi-supervised methods.
3580,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper studies the problem of reward improvement in reinforcement learning. The authors propose PCPO, which is an iterative method to improve the reward function. The first step of PCPO is to train a policy that satisfies the constraint set, and then the second step is to update the policy based on the reward improvement. The paper shows that PCPO converges to the optimal value of the L norm and the Kullback-Leibler divergence."
3581,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper studies the problem of reward improvement in reinforcement learning. The authors propose PCPO, which is an iterative method to improve the reward function. The first step of PCPO is to train a policy that satisfies the constraint set, and then the second step is to update the policy based on the reward improvement. The paper shows that PCPO converges to the optimal value of the L norm and the Kullback-Leibler divergence."
3582,SP:9e831d3595c15ca34cadb3c4a5b02230593b4ccb,"This paper studies the problem of reward improvement in reinforcement learning. The authors propose PCPO, which is an iterative method to improve the reward function. The first step of PCPO is to train a policy that satisfies the constraint set, and then the second step is to update the policy based on the reward improvement. The paper shows that PCPO converges to the optimal value of the L norm and the Kullback-Leibler divergence."
3583,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,This paper studies the problem of embedding word embeddings in word-context co-occurrence space. The authors propose a low-rank transformation of the embedding space that is based on a low rank transformation of a word embedding. The proposed method can be viewed as an extension of the recent work of Zhang et al. (2018) that uses a lowrank transformation to embed the word embedder. The main difference is that the proposed method uses a lower-rank embedding transformation instead of a high-rank one. The paper also proposes to use a lower bound on the distance between two words in the co-computation space. 
3584,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,This paper studies the problem of embedding word embeddings in word-context co-occurrence space. The authors propose a low-rank transformation of the embedding space that is based on a low rank transformation of a word embedding. The proposed method can be viewed as an extension of the recent work of Zhang et al. (2018) that uses a lowrank transformation to embed the word embedder. The main difference is that the proposed method uses a lower-rank embedding transformation instead of a high-rank one. The paper also proposes to use a lower bound on the distance between two words in the co-computation space. 
3585,SP:068a0bb2497373acad5f70e66c61b71465b2de3d,This paper studies the problem of embedding word embeddings in word-context co-occurrence space. The authors propose a low-rank transformation of the embedding space that is based on a low rank transformation of a word embedding. The proposed method can be viewed as an extension of the recent work of Zhang et al. (2018) that uses a lowrank transformation to embed the word embedder. The main difference is that the proposed method uses a lower-rank embedding transformation instead of a high-rank one. The paper also proposes to use a lower bound on the distance between two words in the co-computation space. 
3586,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a novel method for similarity measurement based on Random Projection Trees (RP Trees). The main idea is to use the randomness of the projection vectors of the tree to estimate the distance between two samples. The authors show that the proposed method is faster than the state-of-the-art methods and can be applied to a variety of clustering tasks such as clustering and data mining. 
3587,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a novel method for similarity measurement based on Random Projection Trees (RP Trees). The main idea is to use the randomness of the projection vectors of the tree to estimate the distance between two samples. The authors show that the proposed method is faster than the state-of-the-art methods and can be applied to a variety of clustering tasks such as clustering and data mining. 
3588,SP:b2d099c78b48aab509ab64027ca49e9a47079fc0,This paper proposes a novel method for similarity measurement based on Random Projection Trees (RP Trees). The main idea is to use the randomness of the projection vectors of the tree to estimate the distance between two samples. The authors show that the proposed method is faster than the state-of-the-art methods and can be applied to a variety of clustering tasks such as clustering and data mining. 
3589,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation. The main contribution of this paper is to propose a method to reduce the simulation bias of MCMC and VI. The proposed method is evaluated on MNIST, CIFAR-10, and CelebA datasets."
3590,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation. The main contribution of this paper is to propose a method to reduce the simulation bias of MCMC and VI. The proposed method is evaluated on MNIST, CIFAR-10, and CelebA datasets."
3591,SP:77b8bed08af8be8af0c65a72a6e22cfb02645d02,"This paper proposes a hybrid method for variational inference (VI) and Markov chain Monte Carlo (MCMC) based on gradient-based optimisation. The main contribution of this paper is to propose a method to reduce the simulation bias of MCMC and VI. The proposed method is evaluated on MNIST, CIFAR-10, and CelebA datasets."
3592,SP:64f2744e938bd62cd47c1066dc404a42134953da,"This paper proposes a new method for causal inference based on variational autoencoders (VAEs). The authors propose a new unconfoundedness hypothesis for VAEs, which is based on the idea that the latent confounders of the VAEs can be decomposed into two parts: (1) a latent variable that represents the confounder, and (2) the latent variable which represents the missing data. The authors show that the proposed method can be used to estimate the latent variables of VAEs. The proposed method is evaluated on a number of real-world datasets."
3593,SP:64f2744e938bd62cd47c1066dc404a42134953da,"This paper proposes a new method for causal inference based on variational autoencoders (VAEs). The authors propose a new unconfoundedness hypothesis for VAEs, which is based on the idea that the latent confounders of the VAEs can be decomposed into two parts: (1) a latent variable that represents the confounder, and (2) the latent variable which represents the missing data. The authors show that the proposed method can be used to estimate the latent variables of VAEs. The proposed method is evaluated on a number of real-world datasets."
3594,SP:64f2744e938bd62cd47c1066dc404a42134953da,"This paper proposes a new method for causal inference based on variational autoencoders (VAEs). The authors propose a new unconfoundedness hypothesis for VAEs, which is based on the idea that the latent confounders of the VAEs can be decomposed into two parts: (1) a latent variable that represents the confounder, and (2) the latent variable which represents the missing data. The authors show that the proposed method can be used to estimate the latent variables of VAEs. The proposed method is evaluated on a number of real-world datasets."
3595,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a method for finding compact reinforcement learning (RL) policies. The authors propose to use edge-partitioning (e.g., toeplitz matrices) as a way to search for compact policies. They show that the proposed method outperforms existing methods in terms of performance and compression. They also show that their method can be applied to a variety of RL tasks."
3596,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a method for finding compact reinforcement learning (RL) policies. The authors propose to use edge-partitioning (e.g., toeplitz matrices) as a way to search for compact policies. They show that the proposed method outperforms existing methods in terms of performance and compression. They also show that their method can be applied to a variety of RL tasks."
3597,SP:971d0d94adf5113ee1bef8df9ea7dbd508cf4cbc,"This paper proposes a method for finding compact reinforcement learning (RL) policies. The authors propose to use edge-partitioning (e.g., toeplitz matrices) as a way to search for compact policies. They show that the proposed method outperforms existing methods in terms of performance and compression. They also show that their method can be applied to a variety of RL tasks."
3598,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper proposes a method for learning representations of time-series based on group transformers. The proposed method is based on the idea of affine transformations of a mother filter, which can be viewed as an affine version of the Wavelet transform. The authors propose to use a deep neural network to learn the representations of the wavelet transformers, which are then used to train a neural network. The method is evaluated on a variety of time series datasets, and the results show the effectiveness of the proposed method."
3599,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper proposes a method for learning representations of time-series based on group transformers. The proposed method is based on the idea of affine transformations of a mother filter, which can be viewed as an affine version of the Wavelet transform. The authors propose to use a deep neural network to learn the representations of the wavelet transformers, which are then used to train a neural network. The method is evaluated on a variety of time series datasets, and the results show the effectiveness of the proposed method."
3600,SP:18aaba3423e81e9437b509d1a5e24836ef5635f6,"This paper proposes a method for learning representations of time-series based on group transformers. The proposed method is based on the idea of affine transformations of a mother filter, which can be viewed as an affine version of the Wavelet transform. The authors propose to use a deep neural network to learn the representations of the wavelet transformers, which are then used to train a neural network. The method is evaluated on a variety of time series datasets, and the results show the effectiveness of the proposed method."
3601,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a unified formalism for graph convolutional networks (GCNs) that can be applied to (product of) constant curvature spaces. The authors show that GCNs can be used to represent arbitrary geometries of constant curvatures, and that they are scale-free and non-Euclidean. In particular, they show that a GCN trained on Euclidean GCN can be trained on a graph with gyro-barycentric coordinates. They also show that the curvature of the graph can be expressed as a function of the center of mass of the node."
3602,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a unified formalism for graph convolutional networks (GCNs) that can be applied to (product of) constant curvature spaces. The authors show that GCNs can be used to represent arbitrary geometries of constant curvatures, and that they are scale-free and non-Euclidean. In particular, they show that a GCN trained on Euclidean GCN can be trained on a graph with gyro-barycentric coordinates. They also show that the curvature of the graph can be expressed as a function of the center of mass of the node."
3603,SP:3058e6bc5e8c62af325c214c9e1436d6cdf09204,"This paper proposes a unified formalism for graph convolutional networks (GCNs) that can be applied to (product of) constant curvature spaces. The authors show that GCNs can be used to represent arbitrary geometries of constant curvatures, and that they are scale-free and non-Euclidean. In particular, they show that a GCN trained on Euclidean GCN can be trained on a graph with gyro-barycentric coordinates. They also show that the curvature of the graph can be expressed as a function of the center of mass of the node."
